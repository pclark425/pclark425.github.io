<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1732 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1732</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1732</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-5cffd9741570e42299d9440a71818d72ba39de94</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5cffd9741570e42299d9440a71818d72ba39de94" target="_blank">Computational complexity analysis of simple genetic programming on two problems modeling isolated program semantics</a></p>
                <p><strong>Paper Venue:</strong> Foundations of Genetic Algorithms</p>
                <p><strong>Paper TL;DR:</strong> This paper sets up several simplified GP algorithms and analyzes them on two separable model problems, ORDER and MAJORITY, each of which captures a relevant facet of typical GP problems.</p>
                <p><strong>Paper Abstract:</strong> Analyzing the computational complexity of evolutionary algorithms (EAs) for binary search spaces has significantly informed our understanding of EAs in general. With this paper, we start the computational complexity analysis of genetic programming (GP). We set up several simplified GP algorithms and analyze them on two separable model problems, ORDER and MAJORITY, each of which captures a relevant facet of typical GP problems. Both analyses give first rigorous insights into aspects of GP design, highlighting in particular the impact of accepting or rejecting neutral moves and the importance of a local mutation operator.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1732.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1732.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>(1+1) GP + HVL-Mutate'</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>(1+1) Genetic Programming family using the HVL-Mutate' tree-mutation operator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of minimal-change, tree-based genetic programming algorithms that maintain a single-individual population and generate one offspring per iteration via a bespoke tree mutation operator (HVL-Mutate'), with variants that differ in acceptance of neutral moves and in the number of mutation sub-operations per offspring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>(1+1) GP family (including (1+1) GP, (1+1) GP*, single and multi variants) with HVL-Mutate'</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Population size one GP hill-climber / randomized algorithm that at each iteration copies the current parse tree and applies k applications of HVL-Mutate' (k=1 for 'single' variants; k=1+Pois(1) for 'multi' variants) to produce one offspring. Two acceptance criteria are studied: (1+1) GP accepts offspring with f(X') >= f(X) (neutral moves allowed); (1+1) GP* accepts only strict improvements f(X') > f(X). The HVL-Mutate' operator performs one of three equiprobable sub-operations: insertion (adds a binary join node above a chosen node and attaches a random terminal as the other child, order chosen randomly), substitution (replaces a randomly chosen leaf by a random terminal; in this paper substitution restricted to leaves), or deletion (removes a leaf and its parent, replacing the parent by the sibling), designed to be minimal changes respecting variable-length hierarchical tree structure. The algorithms are analyzed rigorously on two model fitness functions (ORDER and MAJORITY) that measure program semantics via tree inspection rather than execution.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>HVL-Mutate': choose one of {insertion, substitution, deletion} with equal probability. Insertion: pick a node v, replace v with a binary join node whose children are v and a terminal u chosen uniformly from L; the order of children chosen randomly. Substitution: replace a randomly chosen leaf with a uniformly random terminal from L (in this work substitution is restricted to leaves). Deletion: pick a leaf v with parent p and sibling u, replace p with u and delete p and v (deletes only leaf+parent to keep changes minimal). For 'multi' variants, apply 1+Pois(1) such sub-operations per offspring; for 'single', apply exactly one.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Problem-specific tree-inspection fitnesses: ORDER: f(X) = number of positive primitives x_i that appear before their complement \bar{x}_i in the inorder leaf parse (i.e. count of expressed conditionals); MAJORITY: f(X) = number of variables x_i for which count(x_i) >= count(\bar{x}_i) and count(x_i) >= 1 (a OneMax-like count of correctly-balanced terminals). These are semantic/functionality measures computed by parsing the tree, not by executing generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Quantitative expected optimization times (number of fitness evaluations to first reach global optimum) reported in the paper: ORDER: for single- and multi-operation variants of both (1+1) GP and (1+1) GP*, worst-case expected optimization time O(n * T_max) where n is number of variables and T_max is maximum tree leaf count during the run; corollary: (1+1) GP*-single with initial O(n) terminals gives O(n^2). MAJORITY: (1+1) GP-single worst-case expected optimization time O(n log n + D * T_max * n log log n) where D = max_i D_i (maximum deficit), corollaries: initialization with m=O(n) terminals gives O(n^2 * T_max * log log n) worst-case; average-case (unity expectation init: 2n random terminals) (1+1) GP-single: O(n * T_max * log n). For (1+1) GP* variants: (1+1) GP*-single can have infinite expected optimization time (can get stuck with constant probability) under unity-expectation initialization; (1+1) GP*-multi has a worst-case lower bound exponential in n: Omega((n/(2e))^{n/2}).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Not formulated as a novelty-vs-executability tradeoff curve; however the paper analyzes the practical tradeoffs between accepting neutral moves (which increases exploratory diversity and helps MAJORITY reach expressible solutions) versus rejecting them (which prevents bloat and helps ORDER). Thus there is an implicit tradeoff: acceptance of neutral moves increases ability to traverse flat regions (improves functionality attainment on MAJORITY) but can cause tree bloat that harms improvement probability (worse for ORDER). The paper provides rigorous runtime consequences (polynomial vs infinite/exponential) rather than continuous Pareto front characterization.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>genetic programming (program synthesis-like), evaluated on two model problems defined by tree-inspection semantics: ORDER (ordering/conditional semantics) and MAJORITY (OneMax-like correctness of terminals).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>(Internal algorithmic baselines): comparisons across (1+1) GP (neutral moves allowed) vs (1+1) GP* (neutral moves forbidden), and 'single' (one HVL-Mutate' op per offspring) vs 'multi' (1+Pois(1) ops per offspring). No crossover baselines or external GP systems experimentally compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) HVL-Mutate' is a minimal tree-edit mutation operator (insertion/substitution/deletion) tailored for rigorous analysis; no crossover is used. 2) Acceptance of neutral moves crucially affects search dynamics: for ORDER, rejecting neutral moves ((1+1) GP*) prevents bloat and yields better/well-bounded runtimes (O(n*T_max) or O(n^2) with size control), while accepting neutrals leads to tree growth that slows improvement; for MAJORITY, accepting neutral moves ((1+1) GP) is essential—(1+1) GP-single attains polynomial expected optimization time (average-case O(n*T_max*log n)), whereas (1+1) GP*-single may fail to terminate with constant probability (infinite expected time) and (1+1) GP*-multi can have exponential worst-case time. 3) Deletion sub-operation necessary to counteract growth: an insertion-only operator would cause unbounded bloat and possibly infinite optimization time absent explicit size bounds. 4) The fitness measures (ORDER, MAJORITY) are semantic but based on tree inspection (not actual program execution), so executability/functionality was evaluated via counts of terminals/ordering rather than runtime correctness tests. 5) The analyses give provable runtime bounds tied to mutation design and acceptance criteria, demonstrating how mutation + neutrality decisions control novelty/diversity indirectly (via tree size and ability to traverse neutral plateaus) and thereby affect functionality attainment.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>The paper provides a rigorous complexity-theoretic analysis of mutation-driven GP on two model semantics; it explicitly does not use crossover and does not define any explicit novelty/diversity metrics (no novelty search), nor does it operate on natural-language literature or scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational complexity analysis of simple genetic programming on two problems modeling isolated program semantics', 'publication_date_yy_mm': '2010-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1732",
    "paper_id": "paper-5cffd9741570e42299d9440a71818d72ba39de94",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "(1+1) GP + HVL-Mutate'",
            "name_full": "(1+1) Genetic Programming family using the HVL-Mutate' tree-mutation operator",
            "brief_description": "A family of minimal-change, tree-based genetic programming algorithms that maintain a single-individual population and generate one offspring per iteration via a bespoke tree mutation operator (HVL-Mutate'), with variants that differ in acceptance of neutral moves and in the number of mutation sub-operations per offspring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "(1+1) GP family (including (1+1) GP, (1+1) GP*, single and multi variants) with HVL-Mutate'",
            "system_description": "Population size one GP hill-climber / randomized algorithm that at each iteration copies the current parse tree and applies k applications of HVL-Mutate' (k=1 for 'single' variants; k=1+Pois(1) for 'multi' variants) to produce one offspring. Two acceptance criteria are studied: (1+1) GP accepts offspring with f(X') &gt;= f(X) (neutral moves allowed); (1+1) GP* accepts only strict improvements f(X') &gt; f(X). The HVL-Mutate' operator performs one of three equiprobable sub-operations: insertion (adds a binary join node above a chosen node and attaches a random terminal as the other child, order chosen randomly), substitution (replaces a randomly chosen leaf by a random terminal; in this paper substitution restricted to leaves), or deletion (removes a leaf and its parent, replacing the parent by the sibling), designed to be minimal changes respecting variable-length hierarchical tree structure. The algorithms are analyzed rigorously on two model fitness functions (ORDER and MAJORITY) that measure program semantics via tree inspection rather than execution.",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": "HVL-Mutate': choose one of {insertion, substitution, deletion} with equal probability. Insertion: pick a node v, replace v with a binary join node whose children are v and a terminal u chosen uniformly from L; the order of children chosen randomly. Substitution: replace a randomly chosen leaf with a uniformly random terminal from L (in this work substitution is restricted to leaves). Deletion: pick a leaf v with parent p and sibling u, replace p with u and delete p and v (deletes only leaf+parent to keep changes minimal). For 'multi' variants, apply 1+Pois(1) such sub-operations per offspring; for 'single', apply exactly one.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Problem-specific tree-inspection fitnesses: ORDER: f(X) = number of positive primitives x_i that appear before their complement \\bar{x}_i in the inorder leaf parse (i.e. count of expressed conditionals); MAJORITY: f(X) = number of variables x_i for which count(x_i) &gt;= count(\\bar{x}_i) and count(x_i) &gt;= 1 (a OneMax-like count of correctly-balanced terminals). These are semantic/functionality measures computed by parsing the tree, not by executing generated code.",
            "executability_results": "Quantitative expected optimization times (number of fitness evaluations to first reach global optimum) reported in the paper: ORDER: for single- and multi-operation variants of both (1+1) GP and (1+1) GP*, worst-case expected optimization time O(n * T_max) where n is number of variables and T_max is maximum tree leaf count during the run; corollary: (1+1) GP*-single with initial O(n) terminals gives O(n^2). MAJORITY: (1+1) GP-single worst-case expected optimization time O(n log n + D * T_max * n log log n) where D = max_i D_i (maximum deficit), corollaries: initialization with m=O(n) terminals gives O(n^2 * T_max * log log n) worst-case; average-case (unity expectation init: 2n random terminals) (1+1) GP-single: O(n * T_max * log n). For (1+1) GP* variants: (1+1) GP*-single can have infinite expected optimization time (can get stuck with constant probability) under unity-expectation initialization; (1+1) GP*-multi has a worst-case lower bound exponential in n: Omega((n/(2e))^{n/2}).",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": "Not formulated as a novelty-vs-executability tradeoff curve; however the paper analyzes the practical tradeoffs between accepting neutral moves (which increases exploratory diversity and helps MAJORITY reach expressible solutions) versus rejecting them (which prevents bloat and helps ORDER). Thus there is an implicit tradeoff: acceptance of neutral moves increases ability to traverse flat regions (improves functionality attainment on MAJORITY) but can cause tree bloat that harms improvement probability (worse for ORDER). The paper provides rigorous runtime consequences (polynomial vs infinite/exponential) rather than continuous Pareto front characterization.",
            "frontier_characterization": null,
            "benchmark_or_domain": "genetic programming (program synthesis-like), evaluated on two model problems defined by tree-inspection semantics: ORDER (ordering/conditional semantics) and MAJORITY (OneMax-like correctness of terminals).",
            "comparison_baseline": "(Internal algorithmic baselines): comparisons across (1+1) GP (neutral moves allowed) vs (1+1) GP* (neutral moves forbidden), and 'single' (one HVL-Mutate' op per offspring) vs 'multi' (1+Pois(1) ops per offspring). No crossover baselines or external GP systems experimentally compared in this paper.",
            "key_findings": "1) HVL-Mutate' is a minimal tree-edit mutation operator (insertion/substitution/deletion) tailored for rigorous analysis; no crossover is used. 2) Acceptance of neutral moves crucially affects search dynamics: for ORDER, rejecting neutral moves ((1+1) GP*) prevents bloat and yields better/well-bounded runtimes (O(n*T_max) or O(n^2) with size control), while accepting neutrals leads to tree growth that slows improvement; for MAJORITY, accepting neutral moves ((1+1) GP) is essential—(1+1) GP-single attains polynomial expected optimization time (average-case O(n*T_max*log n)), whereas (1+1) GP*-single may fail to terminate with constant probability (infinite expected time) and (1+1) GP*-multi can have exponential worst-case time. 3) Deletion sub-operation necessary to counteract growth: an insertion-only operator would cause unbounded bloat and possibly infinite optimization time absent explicit size bounds. 4) The fitness measures (ORDER, MAJORITY) are semantic but based on tree inspection (not actual program execution), so executability/functionality was evaluated via counts of terminals/ordering rather than runtime correctness tests. 5) The analyses give provable runtime bounds tied to mutation design and acceptance criteria, demonstrating how mutation + neutrality decisions control novelty/diversity indirectly (via tree size and ability to traverse neutral plateaus) and thereby affect functionality attainment.",
            "additional_notes": "The paper provides a rigorous complexity-theoretic analysis of mutation-driven GP on two model semantics; it explicitly does not use crossover and does not define any explicit novelty/diversity metrics (no novelty search), nor does it operate on natural-language literature or scientific text.",
            "uuid": "e1732.0",
            "source_info": {
                "paper_title": "Computational complexity analysis of simple genetic programming on two problems modeling isolated program semantics",
                "publication_date_yy_mm": "2010-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.009757499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Computational Complexity Analysis of Simple Genetic Programming On Two Problems Modeling Isolated Program Semantics</h1>
<p>Greg Durrett<br>Computer Science and Artificial Intelligence Laboratory<br>Massachusetts Institute of Technology<br>Cambridge, MA, USA<br>Frank Neumann<br>Algorithms and Complexity<br>Max Planck Institute for Informatics<br>Saarbrücken, Germany<br>Una-May O'Reilly<br>Computer Science and Artificial Intelligence Laboratory<br>Massachusetts Institute of Technology<br>Cambridge, MA, USA</p>
<p>November 1, 2018</p>
<h4>Abstract</h4>
<p>Analyzing the computational complexity of evolutionary algorithms (EAs) for binary search spaces has significantly informed our understanding of EAs in general. With this paper, we start the computational complexity analysis of genetic programming (GP). We set up several simplified GP algorithms and analyze them on two separable model problems, ORDER and MAJORITY, each of which captures a relevant facet of typical GP problems. Both analyses give first rigorous insights into aspects of GP design, highlighting in particular the impact of accepting or rejecting neutral moves and the importance of a local mutation operator.</p>
<h1>1 Introduction</h1>
<p>Because of the complexity of genetic programming (GP) variants and the challenging nature of the problems they address, it is arguably impossible in most cases to make formal guarantees about the number of fitness evaluations needed for an algorithm to find an optimal solution. Current theoretical approaches investigate foundational aspects of GP tangential to this goal, such as schema theories, search spaces, bloat and problem difficulty [13]. However, in this work, we instead choose to follow the path taken for evolutionary algorithms working on fixed-length binary strings. Initial work on pseudo-Boolean functions illustrated the working principles of simple evolutionary algorithms (see e.g. $[7,15,2])$; subsequently, results have been derived for a wide range of classical combinatorial optimization problems such as shortest paths, maximum matchings or minimum spanning trees (see e.g. [9]). These studies have contributed substantially to our theoretical understanding of evolutionary algorithms for binary representations. Poli et al. [13] state, "we expect to see computational complexity techniques being used to model simpler GP systems, perhaps GP systems based on mutation and stochastic hill-climbing." This contribution is one fulfillment of this prediction: its goal is to show a GP variant that identifies optimal solutions in provably low numbers of fitness function evaluations for two much simplified, but still relevant, problems that exhibit a few simple aspects of program structure.</p>
<p>The simple parameterized GP algorithm we analyze can succinctly be described as both a hill climber and a randomized algorithm. It has four parametric instantiations we call $(1+1)$ GP-single, $(1+1)$ GP-multi, $(1+1)$ GP<em>-single, and $(1+1)$ GP</em>-multi that differ in the acceptance criterion and the size of the mutation proposed. Initially, a solution is chosen at random. We produce by random mutation exactly one offspring of the current solution, and replace the current solution by this proposal as specified by the acceptance criterion. The algorithm iterates until it finds an optimal solution. This simple form of GP algorithm has historical precedent in very early comparisons between Kozastyle genetic programming and GP stochastic iterated hill climbing [11, 10, 12], though it does not include a finite bound on fitness evaluations, random restarts or a limit on how many times mutation will be applied to the current solution. Another simplification of the algorithm is that it uses a genetic operator that is as similar to bit-wise mutation as possible. A single bit-wise mutation is the smallest step possible in an binary EA's search space. Our mutation operator makes the smallest alteration possible to the GP tree while respecting the key properties of the GP tree search space: variable length and hierarchical structure.</p>
<p>The two model problems we select for our analysis are ORDER and MAJORITY, defined exactly as in [3]. We have chosen ORDER and MAJORITY because they make complexity analysis tractable. They allow fitness function evaluation without explicitly executing the program defined by the GP tree. They are minimally sufficient to capture several key properties of GP, including the existence of multiple optimal solutions but they are not real world applica-</p>
<p>tion problems. Neither are they ad-hoc toy problems intended to demonstrate GP's strength (such as Boolean multiplexer for classical GP [5] or lawnmower for GP with automatically defined functions [6]). Each problem has a simple relation to more realistic GP problems: ORDER requires correct ordering as in conditional programs and MAJORITY requires the correct set of solution components.</p>
<p>We proceed as follows: in Section 2, we formally describe the GP variants and the two problems. This requires that we first describe program initialization from a primitive set (2.1) and our mutation operator which is called HVLMutate' (2.2). We then proceed in Sections 3 and 4 with our analyses of ORDER and MAJORITY in terms of the expected number of fitness evaluations until our algorithms have produced a globally optimal solution for the first time. This is called the expected optimization time of the algorithm. Our results are followed by a discussion in Section 5 and conclusions and future work in Section 5.5.</p>
<h1>2 Definitions</h1>
<h3>2.1 Program Initialization</h3>
<p>To use tree-based genetic programming, one must first choose a set of primitives $A$, which contains a set $F$ of functions and a set $L$ of terminals. Each primitive has explicitly defined semantics; for example, a primitive might represent a Boolean condition, a branching statement such as an IF-THEN-ELSE conditional, the value bound to an input variable, or an arithmetic operation. Functions are parameterized. Terminals are either functions with no parameters, i.e. arity equal to zero, or input variables to the program.</p>
<p>In our derivations, we assume that a GP program is initialized by its parse tree construction. In general, we start with a root node randomly drawn from $A$ and recursively populate the parameters of each function in the tree with subsequent random samples from $A$, until the leaves of the tree are all terminals. Functions constitute the internal nodes of the parse tree, and terminals occupy the leaf nodes. The exact properties of the tree generated by this procedure will not figure into the analysis of the algorithm, so we do not discuss them in depth.</p>
<h3>2.2 HVL-Mutate'</h3>
<p>The HVL-Mutate' operator is an update of O'Reilly's HVL mutation operator ( $[10,11]$ ) and motivated by minimality rather than inspired from a tree-edit distance metric. HVL first selects a node at random in a copy of the current parse tree. Let us term this the currentNode. It then, with equiprobability, applies one of three sub-operations: insertion, substitution, or deletion. Insertion takes place above currentNode: a randomly drawn function from $F$ becomes the parent of currentNode and its additional parameters are set by drawing randomly from $L$. Substitution changes currentNode to a randomly drawn function of</p>
<p>$F$ with the same arity. Deletion replaces currentNode with its largest child subtree, which often admits large deletion sub-operations.</p>
<p>The operator we consider here, HVL-Mutate ${ }^{\prime}$, functions slightly differently, since we restrict it to operate on trees where all functions take two parameters. Rather than choosing a node followed by an operation, we first choose one of the three sub-operations to perform. The operations then proceed as shown in Figure 1. Insertion and substitution are exactly as in HVL; however, deletion only deletes a leaf and its parent to avoid the potentially macroscopic deletion change of HVL that is not in the spirit of bit-flip mutation. This change makes the algorithm more amenable to complexity analysis and specifies an operator that is only as general as our simplified problems require, contrasting with the generality of HVL, where all sub-operations handle primitives of any arity. Nevertheless, both operators respect the nature of GP's search among variable-length candidate solutions because each generates another candidate of potentially different size, structure, and composition.</p>
<p>In our analysis on these particular problems, we make one further simplification of HVL-Mutate': substitution only takes place at the leaves. This is because our two problems only have one generic "join" function specified, so performing a substitution anywhere above the leaves is a vacuous mutation. Such operations only constitute one-sixth of all operations, so this change has no impact on any of the runtime bounds we derive.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of the operators from HVL-Mutate'.</p>
<h1>2.3 Algorithms</h1>
<p>We define two genetic programming variants called $(1+1)$ GP and $(1+1)$ GP*. Both algorithms work with a population of size one and produce in each iteration</p>
<p>one single offspring. $(1+1) \mathrm{GP}$ is defined in Algorithm 1 and accepts an offspring if it is as least as fit as its parent.</p>
<p>Algorithm $1((1+1) \mathrm{GP})$.</p>
<ol>
<li>Choose an initial solution $X$.</li>
<li>Set $X^{\prime}:=X$.</li>
<li>
<p>Mutate $X^{\prime}$ by applying HVL-Mutate' $k$ times. For each application, randomly choose to either substitute, insert, or delete.</p>
</li>
<li>
<p>If substitute, replace a randomly chosen leaf of $X^{\prime}$ with a new leaf $u \in L$ selected uniformly at random.</p>
</li>
<li>If insert, randomly choose a node $v$ in $X^{\prime}$ and select $u \in L$ uniformly at random. Replace $v$ with a join node whose children are $u$ and $v$, with the order of the children chosen randomly.</li>
<li>
<p>If delete, randomly choose a leaf node $v$ of $X^{\prime}$, with parent $p$ and sibling $u$. Replace $p$ with $u$ and delete $p$ and $v$.</p>
</li>
<li>
<p>If $f\left(X^{\prime}\right) \geq f(X)$, set $X:=X^{\prime}$.</p>
</li>
<li>Go to 2 .
$(1+1) \mathrm{GP}^{*}$ differs from $(1+1) \mathrm{GP}$ by accepting only solution that are strict improvements (see Algorithm 2).</li>
</ol>
<p>Algorithm 2 (Acceptance for $(1+1) \mathrm{GP}^{*}$ ).</p>
<p>$$
\text { 4'. If } f\left(X^{\prime}\right)&gt;f(X) \text {, set } X:=X^{\prime}
$$</p>
<p>For each of $(1+1) \mathrm{GP}$ and $(1+1) \mathrm{GP}^{<em>}$ we consider two further variants which differ in using one application of HVL-Mutate' ("single") or in using more than one ("multi"). For $(1+1)$ GP-single and $(1+1)$ GP</em>-single, we set $k=1$, so that we perform one mutation at a time according to the HVL-Mutate' framework. For $(1+1)$ GP-multi and $(1+1)$ GP*-multi, we choose $k=1+\operatorname{Pois}(1)$, so that the number of mutations at a time varies randomly according to the Poisson distribution.</p>
<p>We will analyze these four algorithms in terms of the expected number of fitness evaluations to produce an optimal solution for the first time. This is called the expected optimization time of the algorithm.</p>
<h1>2.4 The ORDER problem</h1>
<p>We consider two separable problems called ORDER and MAJORITY that have an independent, additive fitness structure. They both admit multiple solutions on their objective function, which we feel is a key property of a model GP problem because it holds generally for all real GP problems. They also both use the same primitive set:</p>
<ul>
<li>$F:={J}, J$ has arity 2 .</li>
<li>$L:=\left{x_{1}, \bar{x}<em n="n">{1}, \ldots, x</em>}, \bar{x<em i="i">{n}\right}$
$x</em>$.
ORDER represents problems where the primitive sets include conditional functions, which gives rise to conditional execution paths. GP classification problems, for example, often employ a numerical comparison function (e.g. greater than X , less than X , or equal to X ). This sort of function has two arguments (subtrees), one which will be executed only when the comparison returns true, the other only when it returns false [5]. Thus, a conditional function results in a conditional execution path, so the GP algorithm must identify and appropriately position the conditional functions to achieve the correct conditional execution behavior for all inputs.}$ is the complement of $\bar{x}_{i</li>
</ul>
<p>ORDER is an abstracted simplification of this challenge in that it determines the conditional path execution of a program by tree inspection rather than execution. Instead of evaluating a condition test and then executing the appropriate condition body explicitly, an ORDER program's conditional execution path is determined by simply inspecting whether a primitive or its complement occurs first in an in-order leaf parse. Correct programs for the ORDER problem must express each positive primitive $x_{i}$ before its corresponding complement $\bar{x}_{i}$. This correctness requirement is intended to reflect a property commonly found in the GP solutions to problems where conditional functions are used: there exist multiple solutions, each with different conditional execution paths.</p>
<p>Algorithm $3(f(X)$ for ORDER $)$.</p>
<ol>
<li>Derive conditional execution path $P$ of $X$ :</li>
</ol>
<p>Init: $l$ an empty leaf list, $P$ an empty conditional execution path
1.1 Parse $X$ inorder and insert each leaf at the rear of $l$ as it is visited.
1.2 Generate $P$ by parsing $l$ front to rear and adding ("expressing") a leaf to $P$ only if it or its complement are not yet in $P$ (i.e. have not yet been expressed).
2. $f(X)=\left|\left{x_{i} \in P\right}\right|$.</p>
<p>For example, for a tree X , with (after the inorder parse) $l=\left(x_{1}, \bar{x}<em 2="2">{4}, x</em>}, \bar{x<em 3="3">{1}, x</em>}, \bar{x<em 1="1">{6}\right)$, $P=\left(x</em>}, \bar{x<em 2="2">{4}, x</em>}, x_{3}, \bar{x<em 1="1">{6}\right)$ and $f(X)=3$ because $x</em> \in P$.}, x_{2}, x_{3</p>
<h1>2.5 The MAJORITY problem</h1>
<p>MAJORITY is a GP equivalent of the GA OneMax problem [3]. MAJORITY reflects a general (and thus weak) property required of GP solutions: a solution must have correct functionality and no incorrect functionality. Like ORDER, MAJORITY is a simplification that uses tree inspection rather than program execution. A correct program in MAJORITY must exhibit at least as many occurrences of a primitive as of its complement and it must exhibit all the</p>
<p>positive primitives of its terminal (leaf) set. Both the independent sub-solution fitness structure and inspection property of MAJORITY are necessary to make our analysis tractable.</p>
<p>Algorithm $4(f(X)$ for MAJORITY).</p>
<ol>
<li>Derive the combined execution statements $S$ of $X$ :</li>
</ol>
<p>Init: $l$ an empty leaf list, $S$ is an empty statement list.
1.1 Parse $X$ inorder and insert each leaf at the rear of $l$ as it is visited.
1.2 For $i \leq n$ : if count $\left(x_{i} \in l\right) \geq \operatorname{count}\left(\bar{x}<em i="i">{i} \in l\right)$ and count $\left(x</em>$ to $S$
2. $f(X)=|S|$.} \in l\right) \geq 1$, add $x_{i</p>
<p>For example, for a tree X , with (after the inorder parse) $l=\left(x_{1}, \bar{x}<em 2="2">{4}, x</em>}, \bar{x<em 3="3">{1}, \bar{x}</em>}, \bar{x<em 1="1">{6}, x</em>\right)$ and $f(X)=3$.}, x_{4}\right)$, $S=\left(x_{1}, x_{2}, x_{4</p>
<h1>3 Analysis for ORDER</h1>
<p>Here we present bounds for ORDER on the number of runtime evaluations needed in the execution of $(1+1)$ GP and $(1+1)$ GP*.</p>
<p>We will analyze this GP problem using fitness-based partitions [2]. This requires us to compute the probability of improving the fitness from $k$ to $k+1$ for each value of $k$ between 0 and $n-1$, inclusive. Although our HVL-Mutate ${ }^{\prime}$ operator is complex, we can obtain a lower bound on the probability of making an improvement by considering fitness improvements that arise from insertions. This is described in the following lemma.</p>
<p>Lemma 1. Define $p_{k}$ to be the probability that we perform an insertion that improves the fitness value of the GP tree from $k$ to $k+1$. For the single- and multi-operation variants of (1+1) GP and (1+1) GP* applied to the ORDER problem,</p>
<p>$$
p_{k}=\Omega\left(\frac{(n-k)^{2}}{n \max {T, n}}\right)
$$</p>
<p>where $n$ is the number of variables and $T$ is the number of leaves in the GP tree at the particular iteration.</p>
<p>Proof. When the fitness value is $k$, it must be the case that $k$ different $x_{i}$ appear before their corresponding $\bar{x}<em i="i">{i}$. To improve the fitness, we must insert one of the $n-k$ unexpressed $x</em>}$ as a leaf that will be visited before a leaf containing the corresponding $\bar{x<em i="i">{i}$. Assume for notational ease that these unexpressed $x</em>}$ are indexed by $\left{x_{1}, \ldots, x_{n-k}\right}$. Define $A_{i}$ to be the event that we insert $x_{i}$ into the tree with our mutation operation, and define $B_{i}$ to be the event that $x_{i}$ is inserted before the corresponding $\bar{x<em k="k">{i}$. Given this, we can write out $p</em>$ as follows.</p>
<p>$$
p_{k}=\sum_{i=1}^{n-k} \operatorname{Pr}\left(A_{i}\right) \operatorname{Pr}\left(B_{i} \mid A_{i}\right)
$$</p>
<p>With a single operation, the probability of choosing to insert a particular $x_{i}$ is $\frac{1}{6 n}$, since we choose to insert with probability $\frac{1}{3}$ and select the variable uniformly at random from the set of $2 n$ possible terminals. We can cover the multioperation case with this analysis as well because the number of operations is sampled according to $1+\operatorname{Pois}(1)$, so the probability of performing exactly one operation is $\frac{1}{e}$. The probability of $A_{i}$ is therefore at least $\frac{1}{6 e n}$, so in both the single- and multi-operation cases, we have</p>
<p>$$
p_{k} \geq \frac{1}{6 e n} \sum_{i=1}^{n-k} \operatorname{Pr}\left(B_{i} \mid A_{i}\right)
$$</p>
<p>We need to analyze two cases in computing this sum. Preliminarily, we define $S$ to be the total number of nodes in the GP tree. Note that $S=2 T-1$, so $S=\Theta(T)$.</p>
<p>Case 1: $T \geq n-k$. We first note that the probability of inserting $x_{i}$ such that it is visited between the $j-1$ st leaf and the $j$ th leaf in the traversal is at least $\frac{1}{2 S}$, since we choose to insert at the $j$ th leaf with probability $\frac{1}{S}$ and then add $x_{i}$ as a left child of the new join node with probability $\frac{1}{2}$.</p>
<p>Inserting any of the $n-k$ unexpressed $x_{i}$ before the first leaf in the tree clearly improves the fitness. If we insert at the second position instead, there must still be at least $n-k-1$ choices of $x_{i}$ that yield an improvement: there is only one node that will be traversed before this position in the tree, so there is at most one $\bar{x}<em i="i">{i}$ expressed before this position. We can iterate this argument to see that at the $i$ th position, there are still $n-k-i+1 x</em>$ can be inserted in at least the first $i$ positions in the tree. Using the fact that the number of leaves $T$ is at least $n-k$, we have that}$ that can be inserted for an improvement to the fitness. By reindexing the $x_{i}$, we then have that $x_{i</p>
<p>$$
\begin{aligned}
p_{k} &amp; \geq \frac{1}{6 e n} \sum_{i=1}^{n-k} \operatorname{Pr}\left(B_{i} \mid A_{i}\right) \
&amp; \geq \frac{1}{6 e n} \sum_{i=1}^{n-k} \frac{n-k-i+1}{2 S}=\frac{1}{6 e n} \sum_{i=1}^{n-k} \frac{i}{2 S} \
&amp; =\frac{1}{6 e n} \frac{(n-k)(n-k+1)}{4 S} \geq \frac{1}{6 e n} \frac{(n-k)^{2}}{\max {4 S, n}}
\end{aligned}
$$</p>
<p>Noting that $S=\Theta(T)$, the asymptotic result follows.
Case 2: $T&lt;n-k$ : We can apply the argument of Case 1 up to the $T$ th position. After this, we have that for $n-k-T+1$ of the unexpressed $x_{i}$, the corresponding $\bar{x}_{i}$ appears nowhere in the tree, so the probability of an insertion improving the fitness is 1 . We also note that $S&lt;2 n$ in this case, allowing us to</p>
<p>simplify our expression for $p_{k}$ as follows.</p>
<p>$$
\begin{aligned}
p_{k} &amp; \geq \frac{1}{6 e n} \sum_{i=1}^{n-k} \operatorname{Pr}\left(B_{i} \mid A_{i}\right) \
&amp; \geq \frac{1}{6 e n}\left[\sum_{i=1}^{T} \frac{i}{2 S}\right]+\frac{1}{6 e n} \sum_{i=T+1}^{n-k} 1 \
&amp; \geq \frac{1}{6 e n}\left[\sum_{i=1}^{T} \frac{i}{4 n}\right]+\frac{n-k-T}{6 e n} \
&amp; =\frac{T(T+1)}{24 e n^{2}}+\frac{(n-k-T)(n-k)}{6 e n(n-k)}
\end{aligned}
$$</p>
<p>If $T=\Omega(n-k)$, then we lower-bound $p_{k}$ using only the first term, which behaves asymptotically in this case as $\Omega\left(\frac{(n-k)^{2}}{n^{2}}\right)$. Otherwise, if $T=o(n-k)$, then we use the second term, which then grows according to $\Omega\left(\frac{(n-k)^{2}}{n^{2}}\right)$. In either case, because $T$ is less than $n$, we have the desired asymptotic behavior.</p>
<p>With this lemma, we can now state the general theorem about the number of fitness evaluations needed for our $(1+1)$ GP variants.</p>
<p>Theorem 1. The expected optimization time of the single- and multi-operation cases of (1+1) GP and (1+1) GP* on ORDER is $O\left(n T_{\max }\right)$ in the worst case, where $n$ is the number of $x_{i}$ and $T_{\max }$ denotes the maximal tree size at any stage during the evolution of the algorithm.</p>
<p>Proof. We can apply Lemma 1 to these algorithms, which implies an asymptotic lower bound on $p_{k}$, the probability of improving the fitness from $k$ to $k+1$ via an insertion. This also serves as an asymptotic lower bound on the probability of improving the fitness at all, and therefore provides an expected time necessary to improve the fitness, regardless of whether or not we accept neutral moves. In order to determine the total number of evaluations, we must sum the expected number of fitness function evaluations over all intermediate fitness values, from $k=0$ to $k=n-1$.</p>
<p>The expected optimization time is therefore upper bounded by</p>
<p>$$
\begin{aligned}
\sum_{k=0}^{n-1} \frac{1}{p_{k}} &amp; =\sum_{k=0}^{n-1} O\left(\frac{n \max \left{T_{k}, n\right}}{(n-k)^{2}}\right) \
&amp; =n T_{\max } \sum_{k=0}^{n-1} O\left(\frac{1}{(n-k)^{2}}\right) \
&amp; =O\left(n T_{\max } \sum_{j=1}^{\infty} \frac{1}{j^{2}}\right) \
&amp; =O\left(n T_{\max }\right)
\end{aligned}
$$</p>
<p>where the second equality follows from the fact that $T_{\max } \geq T_{n} \geq n$, and the last equality follows from the fact that $\sum_{j=1}^{\infty} \frac{1}{j^{2}} \leq 2$.</p>
<p>Note that most GP algorithms explicitly limit the maximum tree size that can be used in an algorithm. Choosing a linear maximum tree size that would still allow us to generate an optimal solution, i. e. a tree with at least $n$ leaves, gives an algorithm that solves the ORDER problem in expected time $O\left(n^{2}\right)$. However, it is also sometimes possible to show that the tree does not get too big during the optimization process. We examine this for $(1+1) \mathrm{GP}^{*}$-single and present an upper bound on the expected optimization time.</p>
<p>Corollary 1. The expected optimization time of
(1+1) GP*-single on ORDER is $O\left(n^{2}\right)$ if the tree is initialized with $O(n)$ terminals.</p>
<p>Proof. We note that the maximum value of the fitness is $n$, and the fitness is integer-valued, so if it is strictly increasing with each operation that is accepted, there must be no more than $n$ operations accepted. In the single-operation framework, each operation adds at most two nodes to the tree (if it is an insertion), which means that $T_{\max } \leq O(n)+2 n=O(n)$ holds during the run of the algorithm.</p>
<p>The case of $(1+1) \mathrm{GP}^{*}$-multi is more difficult to analyze because the expected length of accepted moves may be very different from the expected length of proposed moves, as conditioning on accepting the move will skew the distribution. We conjecture that the bound from Corollary 1 holds in this case as well, but do not present a proof of this.</p>
<p>We also note that because of how our fitness-based partition argument is structured, invoking the average case does not enable us to find any better bounds. Although $k$ will initially be somewhat greater than zero, we will generally still need to improve the fitness $\Theta(n)$ times, so we will have the same asymptotic result.</p>
<h1>4 Analysis for MAJORITY</h1>
<p>We next consider the MAJORITY problem. We start with some preliminary definitions.</p>
<p>Definition 1. For a given GP tree, let $c\left(x_{i}\right)$ be the number of $x_{i}$ variables and $c\left(\bar{x}<em i="i">{i}\right)$ be the number of negated $x</em>$ variables present in the tree. For a GP tree representing a solution to the MAJORITY problem, we define the deficit in the ith variable by</p>
<p>$$
D_{i}=c\left(\bar{x}<em i="i">{i}\right)-c\left(x</em>\right)
$$</p>
<p>Definition 2. In a GP tree for MAJORITY, we say that $x_{i}$ is expressed when $D_{i} \leq 0$ and $c\left(x_{i}\right)&gt;0$.</p>
<p>The fitness of a tree $T$ is simply the number of variables that are expressed.
We note a property of HVL-Mutate ${ }^{\prime}$ for this particular problem that we will make use of later.</p>
<p>Definition 3. The substitution decomposability property (SDP) for MAJORITY states that a substitution is exactly equivalent to a deletion followed by an insertion, which are accepted or rejected as a unit.</p>
<p>This property follows from the fact that the order of the terminals has no bearing on the fitness of a solution for MAJORITY. The variable to be replaced by substitution is selected uniformly at random from the set of leaves of the tree. This is identical to how the variable to be deleted is chosen when using the deletion operator. Substitution then inserts a variable selected uniformly at random from the set of possible terminals, just as the insertion operator does.</p>
<p>We begin our analysis, in 4.1, with worst case bounds for $(1+1)$ GP-single, $(1+1) \mathrm{GP}^{<em>}$-single, and $(1+1) \mathrm{GP}^{</em>}$-multi. $(1+1)$ GP-single solves the problem quite efficiently, yielding polynomial-time worst-case complexity. However, not accepting neutral moves, as in $(1+1) \mathrm{GP}^{<em>}$, results in poor performance: $(1+1) \mathrm{GP}^{</em>}$-single fails to terminate in the worst case, and $(1+1) \mathrm{GP}^{*}$-multi requires a number of fitness evalutions exponential in the size of the initial tree.</p>
<p>In 4.2 we derive average case bounds that assume the initial solution tree has $2 n$ terminals each selected uniformly at random from $L$. This random tree initialization allows us to bound the maximum deficit in any variable. We show that $(1+1)$ GP-single runs in time $O\left(n T_{\max } \log \log (n)\right)$ in the average case. By contrast, $(1+1) \mathrm{GP}^{*}$-single has a constant probability of failing to terminate, and so the expected runtime is infinite.</p>
<h1>4.1 Worst Case Bounds</h1>
<h3>4.1.1 $(1+1)$ GP-single</h3>
<p>We will show here some properties of $(1+1)$ GP-single on MAJORITY and give a polynomial-time worst-case bound on the performance. Our analysis considers the evolution of the deficits $D_{i}$ over the course of the algorithm as $n$ parallel random walks. We will show that each positive $D_{i}$ reaches zero at least as quickly as a balanced random walk, which is the condition for the corresponding $x_{i}$ to be expressed; this, then, gives us the expected number of operations that we are required to perform on a particular variable before it is expressed. Because these arguments do not easily extend to $(1+1)$ GP-multi, we omit from this section any treatment of that case.</p>
<p>We begin by establishing the validity of modeling the temporal sequence of each of the $D_{i}$ as a random walk.</p>
<p>Lemma 2. For (1+1) GP-single on MAJORITY:
a) The probability of proposing an operation that changes either the number of $x_{i}$ or the number of $\bar{x}<em i="i">{i}$ is $\Omega\left(\frac{1}{n}\right)$.
b) If some $x</em>\right)$ proposed operations involving that variable before it is successfully expressed,}$ has a deficit $D_{i}=d&gt;0$, we require in expectation $O\left(d T_{\max </p>
<p>where $T_{\max }$ is the maximum number of nodes in our GP tree at any timestep of the algorithm.</p>
<p>Proof. a) To see that a particular operation involves $x_{i}$ or $\bar{x}<em i="i">{i}$ with probability $\Omega\left(\frac{1}{n}\right)$, we simply note that the probability of inserting one of the two variables is $\frac{1}{3} \times \frac{2}{2 n}=\Omega\left(\frac{1}{n}\right)$.
b) We address each of the three types of operations in turn and show that each is at least as favorable as a balanced random walk in terms of reducing $D</em>$ to zero.</p>
<p>Insertion: The probability of inserting $x_{i}$ into the tree is $\frac{1}{6 n}$, which is the same as the probability of inserting $\bar{x}<em i="i">{i}$. Therefore, given that we change $D</em>$.}$ with an insertion, we increase it or decrease it in a balanced manner, with probability $\frac{1}{2</p>
<p>Deletion: The probability of a deletion changing $D_{i}$ is</p>
<p>$$
\frac{c\left(x_{i}\right)+c\left(\bar{x}_{i}\right)}{T}
$$</p>
<p>where $T$ is the size of the GP tree. Given that we do such a deletion, we increase $D_{i}$ with probability $\frac{c\left(x_{i}\right)}{c\left(x_{i}\right)+c\left(\bar{x}<em i="i">{i}\right)}$ and decrease it with probability $\frac{c\left(\bar{x}</em>}\right)}{c\left(x_{i}\right)+c\left(\bar{x<em i="i">{i}\right)}$, since we pick the variable to delete uniformly at random. However, note that because $D</em>}&gt;0$, we have that $c\left(x_{i}\right)&lt;c\left(\bar{x<em i="i">{i}\right)$, so the probability of decreasing $D</em>$.}$ is greater than the probability of increasing it, so this is actually slightly better than a balanced random walk for the purpose of reducing $D_{i</p>
<p>Substitution: We now make use of the substitution decomposability property (SDP) defined previously to observe that substitution consists of a deletion followed by an insertion. Therefore, a substitution is simply equivalent to taking one or two steps that tend to reduce $D_{i}$ with probability at least $\frac{1}{2}$ if $D_{i}$ is greater than 0 .</p>
<p>Consider the 1-dimensional random walk on the integers $0,1, \ldots, n$, with $n$ being a reflecting barrier and 0 being an absorbing barrier. The expected time to reach 0 when starting at $k$ is $O(k n)$, following the analysis for random walks on undirected graphs carried out in [1]. This is precisely the setting we have for our random walk on the $D_{i}$ if we set $k=d$ and $n=T_{\max }$, so we have that the random walk performed by the $D_{i}$ reaches zero after at most $O\left(d T_{\max }\right)$ accepted operations.</p>
<p>We now must address the question of how many operations on the variable must be proposed in order to accept $O\left(d T_{\max }\right)$ of them. Note that if $x_{i}$ is unexpressed, any insertion or deletion affecting $D_{i}$ will be accepted, since it cannot possibly decrease the fitness value. The probability of a substitution affecting $D_{i}$ is, by the SDP, less than or equal to the probability that an insertion affects $D_{i}$ plus the probability that a deletion affects $D_{i}$. Therefore, even if every substitution is rejected, we still accept a constant fraction of proposed operations that affect $D_{i}$, so we only require $O\left(d T_{\max }\right)$ proposed operations involving $x_{i}$ and $\bar{x}<em _max="\max">{i}$ in order to have $O\left(d T</em>\right)$ accepted operations.</p>
<p>Once $D_{i}$ reaches zero, we are done and $x_{i}$ is expressed unless $c\left(x_{i}\right)=c\left(\bar{x}_{i}\right)=$ 0 . In this case, we clearly cannot do any more deletes, but will either add in</p>
<p>an $x_{i}$ or $\bar{x}<em i="i">{i}$ via an insertion or a substitution. Through either operation, we add each variable with probability $\frac{1}{2}$, and therefore successfully express $x</em>}$ with probability $\frac{1}{2}$. In the case where we insert an $\bar{x<em i="i">{i}$ and increase $D</em>\right)$.}$ to one, we again apply our one-dimensional random walk result with $k=1$ and $n=T_{\max }$ to see that we will return to zero again after only $O\left(T_{\max }\right)$ additional moves, whereupon either $x_{i}$ is present in the tree and we are done or we can once again attempt to add it. Because we expect to do this procedure only twice before succeeding, it only adds $O\left(T_{\max }\right)$ steps, and therefore does not change our bound of $O\left(d T_{\max </p>
<p>This lemma allows us to establish an upper bound on the number of evaluations for $(1+1)$ GP on MAJORITY given a bound on the largest deficit.
Theorem 2. Let $D=\max <em i="i">{i} D</em>}$ for an instance of MAJORITY initialized with $T$ terminals drawn from a set of size $2 n$ (i.e. terminals $x_{1}, \ldots, x_{n}, \bar{x<em n="n">{1}, \ldots, \bar{x}</em>$ ). Then the expected optimization time of $(1+1)$ GP-single is</p>
<p>$$
O\left(n \log n+D T_{\max } n \log \log n\right)
$$</p>
<p>in the worst case.
Proof. We draw upon a result from Myers and Wilf [8] about a generalized form of the coupon collector problem. If we have $n$ coupons and wish to acquire at least $k$ of each coupon, we need to draw, in expectation, $O(n \log n+k n \log \log n)$ coupons. When $k$ is at least $\log n$, this is a slight improvement over the naive bound of $O(k n \log n)$ from simply iterating the basic coupon collector problem $k$ times.</p>
<p>Lemma 2 tells us two things. Firstly, we have that a proposed operation involves $x_{i}$ or $\bar{x}<em _max="\max">{i}$ with probability $\Omega\left(\frac{1}{n}\right)$, so we have a coupon collector problem with slightly perturbed coupon probabilities. Secondly, we find that we need to propose $O\left(D T</em> n \log \log n\right)$ fitness function evaluations, as desired.}\right)$ operations involving each terminal in order to express all of our variables. Plugging these into the bound described above yields an asymptotic requirement of $O\left(n \log n+D T_{\max </p>
<p>The only wrinkle in this picture is that the coupon collector assumes that a variable is "complete" after a set number of coupons have been collected. While we do not accept moves that reduce the fitness value, an expressed variable $x_{i}$ could become unexpressed if, during the course of a substitution operation, another variable $x_{j}$ were simultaneously expressed. However, in this case, we must have had $D_{i}=0$ and $D_{j}=1$, and we have merely reversed the two, which amounts to a relabeling of the $x_{i}$ and $x_{j}$. Because the $D_{i}$ are the only state variables that we care about in this case, this move effectively does nothing except cause us to make a vacuous move. Because substitutions only make up $\frac{1}{3}$ of all of the proposed moves, such wasted moves can only make up a constant fraction of the total number of moves, and therefore do not change the asymptotics.</p>
<p>As a corollary of 2 we can bound the initial $D$ by considering tree initialization.</p>
<p>Corollary 2. When MAJORITY is initialized with $m=O(n)$ terminals drawn from a set of size $2 n$, the expected optimization time of (1+1) GP-single is</p>
<p>$$
O\left(n^{2} T_{\max } \log \log n\right)
$$</p>
<p>Proof. This follows from Theorem 2 with $D=m$, since the deficit cannot be greater than the number of terminals in the tree.</p>
<p>We can consider the outcome of the worst case tree initialization both intuitively and experimentally. We have $D=m$ when all of the leaves consist of instances of one bar variable, say $\bar{x}<em 1="1">{1}$. Since the $\bar{x}</em> \log n \log \log n\right)$, a bound very close to the average-case optimization time we present in 4.2.1.}$ occupy such a large fraction of the tree, they will frequently be substituted out or deleted. This suggests that the balanced random walk argument is quite pessimistic given this circumstance. We thus expect that, in practice, this initial condition will be quickly erased. If we put $T_{\max }=O(n)$, we know, from the coupon collector problem, that after an initial phase of $O(n \log n)$ steps, we will have proposed a deletion on every leaf that was initialized in the GP tree. Because deletions are always accepted on negated variables, we will have deleted all of the initial $\bar{x}_{1}$ variables by the end of this "erasure" phase, and only expect to introduce at most $O(\log n)$ of any particular bar variable through insertions and substitutions. This implies that, after this relatively short phase, $D=O(\log n)$, giving an optimization time of $O\left(n^{2</p>
<p>We experimented with this initialization to confirm our intuition. Figure 2 shows the results of solving MAJORITY using $(1+1)$ GP-single with increasing problem size and trees initialized with $2 n$ leaves, each occupied by $\bar{x}_{1}$. We tracked the number of fitness evaluations required and, even though we imposed no bound on the tree size, the order of growth relative to $n$ appears to be just barely superlinear. This empirical evidence supports the intuition that the worst-case performance is much closer to the average-case than Corollary 2 would suggest.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Plot of the average optimization time given a "bad" initialization of size $2 n$ with $D_{1}=2 n$ for $x_{1}$. Fifty trials were used to compute each point. Circles indicate the mean number of fitness function evaluations for each value of $n$, and error bars show the standard deviation of the 50 trials.</p>
<h1>4.1.2 $(1+1) \mathrm{GP}^{*}$</h1>
<p>Unlike in the case of the ORDER problem, where accepting or not accepting neutral moves makes no difference in the performance of the algorithm, for MAJORITY, such a distinction matters tremendously. Intuitively, this behavior arises because there is a notion of "working towards" a solution here that is absent from the ORDER problem. In ORDER, our analysis relied on our ability to express an $x_{i}$ by simply inserting it as a terminal early enough in the tree, which required only one step. However, in MAJORITY, if there are $k x_{i}$ and $\ell$ $\bar{x}_{i}$ present in the GP tree, at least $\left\lceil\frac{\ell-k}{2}\right\rceil$ mutation operations will be required to make up this deficit, all but the last of which will be neutral moves.</p>
<p>Because of the importance of neutral moves, we find that $(1+1) \mathrm{GP}^{<em>}$-single and $(1+1) \mathrm{GP}^{</em>}$-multi perform quite badly. Even when we initialize with a tree with size linear in $n$, the number of terminal symbols, we can demonstrate an initialization where $(1+1) \mathrm{GP}^{<em>}$-single never terminates and $(1+1) \mathrm{GP}^{</em>}$-multi takes an exponential amount of time to do so. Consider the tree $T_{\text {lopt }}$ which has as leaves the variables</p>
<p>$$
x_{1}, x_{2}, x_{3}, \ldots, x_{n-1}, \underbrace{\bar{x}<em n="n">{n}, \bar{x}</em>} \ldots, \bar{x<em _="{" _text="\text" n_1="n+1" of="of" these="these">{n}}</em>
$$}</p>
<p>Theorem 3. Let $T_{\text {lopt }}$ be the initial solution to MAJORITY. Then the expected optimization time of $(1+1) G P^{*}$-single is infinite.</p>
<p>Proof. It is clear that, with one move, the deficit in $x_{n}$ can only be changed by at most two. There is a deficit of $n+1$ to make up, which is impossible, therefore $(1+1) \mathrm{GP}^{*}$-single will never find its way out of this local optimum.</p>
<p>Theorem 4. Let $T_{\text {lopt }}$ be the current solution to MAJORITY. Then the expected optimization time of $(1+1) G P^{*}$-multi is at least exponential in $n$.</p>
<p>Proof. The fitness value of $T_{\text {lopt }}$ is $n-1$, with $x_{1}$ through $x_{n-1}$ expressed, so the only way to improve the fitness is to make a move that expresses $x_{n}$. Therefore, the moves that achieve this are the only moves that will be accepted. We compute the probability of making such a move in this configuration in order to determine the expected time to make such a move.</p>
<p>Note that any mutation operation that successfully improves the fitness must make up for a deficit of $n+1$, which requires at least $\left\lceil\frac{n+1}{2}\right\rceil$ operations, assuming that we, in each case, substitute an $\bar{x}<em n="n">{n}$ with an $x</em>\right\rceil$. The probability of this when $\lambda=1$ is given by}$. The number of moves per mutation is distributed as $1+\operatorname{Pois}(1)$, so the Poisson random variable must take a value of at least $\left\lceil\frac{n-1}{2</p>
<p>$$
\sum_{i=\left\lceil\frac{n-1}{2}\right\rceil}^{\infty} \frac{e^{-1}}{i!} \leq \frac{1}{\left(\frac{n-1}{2}\right)!}=O\left(\left(\frac{n}{2 e}\right)^{-\frac{n}{2}}\right)
$$</p>
<p>by Stirling's formula.</p>
<p>We can take this probability as a (very weak) upper bound on the probability of improving the fitness. Inverting it, we see that the expected number of moves required is $\Omega\left(\left(\frac{n}{2 e}\right)^{\frac{n}{2}}\right)$.</p>
<h1>4.2 Average Case Bounds</h1>
<p>To provide average case bounds we consider a GP tree which is initialized with what we term "unity expectation": it has $2 n$ terminals (leaves) each selected uniformly at random from the set of possible terminals.</p>
<h3>4.2.1 $(1+1) \mathrm{GP}$</h3>
<p>The average case bound follows more or less directly from Theorem 2 once a result from the literature is applied to given an expected bound on the maximum initial deficit.</p>
<p>Corollary 3. For MAJORITY with a terminal set of size $2 n$ under unity expectation initialization, the expected optimization time of (1+1) GP-single is $O\left(n T_{\max } \log n\right)$.
Proof. A result from Raab and Steger [14] tells us that, with probability at least $1-O\left(\frac{1}{n^{k}}\right)$ for any integer $k$, no $\bar{x}<em _max="\max">{i}$ appears more than $O\left(\frac{\log n}{\log \log n}\right)$ times in the GP tree, so $D=O\left(\frac{\log n}{\log \log n}\right)$. Set $k=2$, so that the probability of having a larger deviation is $O\left(\frac{1}{n^{2}}\right)$. The worst-case bound of $O\left(n^{2} T</em>\right)$ into the expression in Theorem 2 gives us the desired bound for the common case, which is also the overall runtime bound.} \log \log n\right)$ from Corollary 2 ensures that these uncommon cases contribute only an $O\left(T_{\max } \log \log n\right)$ term to the expectation. Substituting $D=O\left(\frac{\log n}{\log \log n</p>
<h3>4.2.2 $(1+1) \mathrm{GP}^{*}$</h3>
<p>Assuming unity expectation initialization, we can improve on our result from 4.1.2 and show $(1+1)$ GP*-single has a constant probability of failing to terminate. Our general strategy will be to prove that there is constant probability that, when starting with a deficit of size three in $x_{1}$, this deficit will be preserved until the fitness is $n-1$. At this point, when all the other variables are expressed, there will remain a gap that cannot be closed in a single step. Such a deficit could disappear over the course of the algorithm because substitution has the ability to shrink the deficit (by removing $\bar{x}<em i="i">{1}$ and replacing it with a $x</em>$ in order to express that variable), but this proof shows that there is nonetheless a constant probability of the deficit being preserved.</p>
<p>First, we establish a lemma about the prevalence of constant-size deficits arising based on our initialization.</p>
<p>Lemma 3. Suppose we have a $2 n$-length instance of the MAJORITY problem with unity expectation initialization. Let $A_{k}$ denote the event that $\bar{x}_{1}$ appears</p>
<p>exactly $k$ times without $x_{1}$ appearing at all, where $k$ is any constant. Then $\operatorname{Pr}\left(A_{k}\right)=\Omega(1)$.</p>
<p>Proof. To compute $\operatorname{Pr}\left(A_{k}\right)$, we count the number of $2 n$-length sequences of terminals for which this is true and divide by the total number of possible sequences. Under $A_{k}$, we must have $k$ instances of $\bar{x}<em 1="1">{1}$ and zero instances of $x</em>}$, so there are $\binom{2 n}{k}$ positions that can be occupied by the $\bar{x<em 1="1">{1}$ and the remaining $2 n-k$ positions should each be occupied by one of the $2 n-2$ elements that are not $x</em>$ possible $2 n$-length sequences of terminals. Combining these facts yields}$ or $\bar{x}_{1}$. In total, there are $(2 n)^{2 n</p>
<p>$$
\begin{aligned}
\operatorname{Pr}\left(A_{k}\right) &amp; =\frac{\binom{2 n}{k}(2 n-2)^{2 n-k}}{(2 n)^{2 n}} \
&amp; =\frac{1}{k!} \frac{(2 n)!}{(2 n-k)!(2 n-2)^{k}}\left(\frac{2 n-2}{2 n}\right)^{2 n} \
&amp; \geq \frac{1}{k!}\left(\frac{2 n-k}{2 n-2}\right)^{k}\left(1-\frac{1}{n}\right)^{2 n} \
&amp; =\frac{1}{k!} \times \Omega(1) \times \Omega(1) \
&amp; =\Omega(1)
\end{aligned}
$$</p>
<p>assuming that $k$ is a constant.
Next, we lower-bound the size of the GP tree when running (1+1) GP*single on MAJORITY. The tree must be large enough so that we are not too likely to substitute out the $\bar{x}_{1}$ over the course of the algorithm.</p>
<p>Lemma 4. Using (1+1) GP-single on MAJORITY with any initialization of size $2 n$, the size of the GP tree is always greater than $\frac{7 n}{6}$ with probability one.</p>
<p>Proof. A deletion can only improve the fitness if we delete some $\bar{x}<em i="i">{i}$ when $c\left(x</em>}\right)=$ $n$ and $c\left(\bar{x<em i="i">{i}\right)=n+1$, with $n$ positive. Such a configuration requires at least three occurrences of $x</em>}$ and $\bar{x<em j="j">{i}$ in the GP tree, so at most $\frac{2 n}{3}$ variables can be present in this fashion initially. Of the at least $\frac{n}{3}$ variables that remain, at most half can be expressed by a deletion, because they must be first put into this configuration during the course of a substitution that expresses some other variable $x</em>$.}$. Therefore, we are forced to accept at least $\frac{n}{6}$ insertions or substitutions over the course of the algorithm, giving us an upper bound of $\frac{5 n}{6}$ on the number of deletions accepted. This in turn guarantees that our tree always remains larger than $2 n-\frac{5 n}{6}=\frac{7 n}{6</p>
<p>Finally, we can prove the claim directly.
Theorem 5. With probability $\Omega(1)$, the optimization time of (1+1) GP*-single with unity expectation initialization is infinite on MAJORITY.</p>
<p>Proof. Lemma 3 tells us that, with a constant probability, we initialize one of the variables, say $x_{1}$, with $c\left(x_{1}\right)=0$ and $c\left(\bar{x}_{1}\right)=3$. We now show that, also with a constant probability, such a deficit is preserved during the course of the expression of at most $n-1$ of the other variables.</p>
<p>We make such an argument by induction. Define the $j$ th step of the algorithm as the period after $j$ variables have been expressed, at the end of which we propose the move that expresses the $j+1$ st move. Suppose that at the $j$ th step, it is true that $c\left(x_{1}\right)=0$ and $c\left(\bar{x}<em 1="1">{1}\right)=3$. The $j+1$ st variable expressed cannot possibly be $x</em>}$, since there is no way to make up a deficit of three with a single move. If the move we accept to express the $j+1$ st variable is an insertion or a deletion, we preserve our deficit of three and do not change the state of the variable $x_{1}$ at all, since we must either insert some variable in the set $\left{x_{2}, \ldots, x_{n}\right}$ or delete some variable in the set $\left{\bar{x<em n="n">{2}, \ldots, \bar{x}</em>\right}$.</p>
<p>If we express the $j+1$ st variable with a substitution, however, it is possible that we might insert an $x_{1}$ or delete one of the $\bar{x}<em 2="2">{1}$. An accepted substitution must either replace some variable with a variable in the set $\left{x</em>}, \ldots, x_{n}\right}$ or substitute out some $\left{\bar{x<em n="n">{2}, \ldots, \bar{x}</em>$.}\right}$. However, a substitution also involves an "extraneous" insertion or deletion, by the SDP. If this operation impacts a variable different than the $j+1$ st variable we are expressing, it must be an operation that, on its own, would keep the fitness constant. For an extraneous insertion, we note that it is always admissible to insert any of the $n$ symbols in the set $\left{x_{1}, x_{2}, \ldots, x_{n}\right}$ without decreasing the fitness. Therefore, the probability of inserting neither $x_{1}$ nor $\bar{x}_{1}$ in a neutral or better move is at least $1-\frac{8}{n</p>
<p>If the extraneous operation is a deletion, we note that it is always possible to delete at least $\frac{T-n}{2}$ terminals, where $T$ is the current tree size. Any variable expressed with $c\left(x_{i}\right)=1$ and $c\left(\bar{x}<em i="i">{i}\right)=0$ cannot be removed, so there might be as many as $n$ terminals forbidden for this reason. For any variable not in this configuration, we have one of two cases. If the variable is unexpressed or is expressed with a deficit less than or equal to -1 , any occurrence of $x</em>}$ or $\bar{x<em i="i">{i}$ can safely be deleted without decreasing the fitness. If the variable is expressed with a deficit of zero, there must be at least as many $\bar{x}</em>$.}$ as there are $x_{i}$, and any of these $\bar{x}_{i}$ can be safely deleted. Therefore, we set aside at most $n$ "singleton" symbols that cannot be deleted, and of those remaining, it must always be acceptable to delete at least half, yielding $\frac{T-n}{2</p>
<p>We therefore preserve our three $\bar{x}_{1}$ variables with probability at least</p>
<p>$$
\frac{\frac{T-n}{2}-3}{\frac{T-n}{2}}=1-\frac{6}{T-n}
$$</p>
<p>We now invoke the result from Lemma 4. Because the size of the tree is at least $\frac{\frac{T n}{n}}{6}$ at all times, we can lower-bound the probability of preserving the $\bar{x}_{1}$ as $1-\frac{36}{n}$.</p>
<p>These situations (extraneous inserts and extraneous deletes) are mutually exclusive, and of the two, the deletes are the more probable to interfere with our $x_{1}$ setup. Nevertheless, the probability of preserving our deficit of three in $x_{1}$ from the $j$ th step to the $j+1$ st step is at least $1-\frac{O(1)}{n}$. Because there are</p>
<p>at most $n-1$ such steps of the algorithm, our overall probability of preserving the deficit is</p>
<p>$$
\left(1-\frac{O(1)}{n}\right)^{n-1}=\Omega(1)
$$</p>
<p>We have constant probability of initializing with such a deficit, and a constant probability of preserving the deficit, in which case the algorithm never terminates. Therefore, with constant probability, $(1+1) \mathrm{GP}^{*}$-single never terminates on MAJORITY.</p>
<p>Corollary 4. Using unity expectation initialization, the expected optimization time of (1+1) GP*-single on MAJORITY is infinite.</p>
<p>Proof. This follows directly from Theorem 5
While Theorem 5 does demonstrate that the probability of getting stuck in a local optimum is at least a constant, the actual constant yielded by the proof is rather small. However, our proof technique made several very conservative assumptions for simplicity. To investigate further, we tried to solve MAJORITY with $(1+1) \mathrm{GP}^{<em>}$-single experimentally, observing when we would get stuck in a local minimum. Experimentally, the actual probability of $(1+1) \mathrm{GP}^{</em>}$-single failing to converge to the optimum is actually quite high, as demonstrated by Figure 3 .
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Plot showing the probability of $(1+1) \mathrm{GP}^{*}$-single failing to terminate on MAJORITY when the initial solution tree has $2 n$ terminals each selected uniformly at random, i.e. with unity expectation initialization. Each probability was determined empirically over the course of 100 simulations for each value of $n$.</p>
<p>However, we do not know how to show a similar result for $(1+1) \mathrm{GP}^{*}$-multi. We note that difficult MAJORITY instances, such as $T_{\text {lopt }}$ presented in 4.1.2, are exponentially unlikely to occur when the initial solution tree has $2 n$ terminals each selected uniformly at random. From Raab and Steger [14], we know that deficits larger than logarithmic occur with exponentially small probability, and in any case large deficits should tend to equalize over the course of the algorithm execution, even if we only accept a linear number of moves. However, if the last unexpressed variable has a deficit of size $k$, we will require at least $\Omega\left(n^{-\frac{k}{2}}\right)$ steps to correctly substitute out enough instances of $\bar{x}<em i="i">{i}$ for $x</em>$ even in the best</p>
<p>case, so unless $k$ can be bounded at a constant, we will have an expected runtime that is superpolynomial.</p>
<h1>5 Summary and Discussion</h1>
<p>Table 1 aggregates our expected optimization time results for all algorithm variants and each problem.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ORDER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$(1+1) \mathrm{GP}$</td>
<td style="text-align: center;">$(1+1) \mathrm{GP}^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">single</td>
<td style="text-align: center;">$O\left(n T_{\max }\right)$ w.c. $\dagger$</td>
<td style="text-align: center;">$O\left(n^{2}\right)$ w.c.</td>
</tr>
<tr>
<td style="text-align: center;">multi</td>
<td style="text-align: center;">$O\left(n T_{\max }\right)$ w.c. $\dagger$</td>
<td style="text-align: center;">$O\left(n T_{\max }\right)$ w.c. $\dagger$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MAJORITY</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$(1+1) \mathrm{GP}$</td>
<td style="text-align: center;">$(1+1) \mathrm{GP}^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">single</td>
<td style="text-align: center;">$O\left(n^{2} T_{\max } \log n\right)$ w.c. $\dagger$ $O\left(n T_{\max } \log n\right)$ a.c.</td>
<td style="text-align: center;">$\Omega(\infty)$ a.c.</td>
</tr>
<tr>
<td style="text-align: center;">multi</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\Omega\left(\left(\frac{n}{2 e}\right)^{\frac{n}{2}}\right)$ w.c.</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of the computational complexity analysis for our sample problem. We use w.c. to denote a worst-case bound and a.c. to denote an average-case bound. The daggers indicate where we conjecture that better bounds exist.</p>
<p>From the perspective of a GP practitioner, the insights provided by this rigorous analysis may be more valuable than the complexity results themselves. In 5.1 we discuss how our treatment sheds light upon the important but subtle interactions between a problem, the acceptance criterion, and the genetic operator. In 5.2 , we discuss the impact of the sub-operations on the mutation operator we considered. In 5.3 , we address the implications of our design and analysis methodology for practical GP algorithm design. Section 5.4 covers some of our analysis techniques, and finally 5.5 presents future work avenues and concludes.</p>
<h3>5.1 Accepting Neutral Moves in ORDER and MAJORITY</h3>
<p>It might initially seem immaterial whether or not we accept neutral moves with our genetic operator for ORDER and MAJORITY. However, our analysis provides rigorous evidence that the differences in performance between $(1+1)$ GP and $(1+1) \mathrm{GP}^{*}$ are substantial for both of these problems. Similar results have already been obtained in the context of evolutionary algorithms for binary representations [4].</p>
<p>ORDER's focus on condition semantics gives it the property that only the first occurrence of each terminal matters. A large tree makes the probability of</p>
<p>improvements smaller because many of the mutations will change variables that have no effect on expression, being sequentially later than earlier occurrences of those same variables. Therefore, not accepting neutral moves helps prevent "bloat" and using $(1+1) \mathrm{GP}^{*}$ is significantly advantageous. $(1+1) \mathrm{GP}$ 's acceptance of neutral moves causes a feedback loop that stimulates growth of the tree: there is a slight bias towards accepting insertions as opposed to deletions, which makes the tree large, which increases the time to find an improvement and results in many neutral insertions, which increase the tree size even more. In general, to solve ORDER with runtime performance that respects the complexity analysis, the tree must not grow too large, and not accepting neutral moves assures this.</p>
<p>Solving MAJORITY, we see the opposite effect: $(1+1)$ GP very handily beats $(1+1) \mathrm{GP}^{<em>}$ in terms of expected optimization time. Neutral moves have the effect of balancing both the relative frequency of variables and the number of positive versus negative occurrences. This draws us toward a very favorable average case where every variable is either expressed or very close to being expressed. If neutral moves are not accepted, improvement can frequently stagnate, underscoring the fact that there are large flat regions in the search space. $(1+1) \mathrm{GP}$ is better equipped to escape these than $(1+1) \mathrm{GP}^{</em>}$, so one should, in fact, clearly choose $(1+1) \mathrm{GP}$ so that there is a guarantee of termination and to avoid the exponential-time worst-case associated with ( $1+1$ ) GP*-multi.</p>
<p>Overall, these results highlight the fact that, in choosing whether or not to accept neutral moves, one should consider their general effect with respect to both the fitness landscape and growth in tree size. Tying this knowledge into expected optimization time also requires an understanding of the mechanisms by which the fitness increases. We recognize that for ORDER and MAJORITY, this is much easier to do rigorously than for more realistic problems. However, perhaps our exercise with ORDER and MAJORITY can provide intuitive insight to GP practitioners.</p>
<h1>5.2 Mutation</h1>
<p>Our results also tell us more about our HVL-Mutate' framework, and show that it has several interesting properties and behaves quite differently for the two problems.</p>
<p>Interestingly, the analysis for ORDER, which uses the fitness partition method, only relies on the use of insert. However, we could not run the algorithm with only insertion, because the tree would get very large and the expected time to termination would actually become infinite, in the absence of a strict bound on the tree size. Therefore, deletions are necessary to control the size of the tree, if nothing else. We could, however, envision designing an alternative operator without substitution, whose insertion and deletion probabilities are imbalanced. By choosing these probabilities appropriately, we could prevent the tree size from getting too large (without explicitly bounding it) while still doing as many insertions as possible, thus allowing the algorithm to reach the optimum with the lowest number of fitness evaluations.</p>            </div>
        </div>

    </div>
</body>
</html>
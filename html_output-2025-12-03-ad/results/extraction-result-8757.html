<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8757 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8757</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8757</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-1ca3b6ff250b4f73486a89f6954edcc4ae21834e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1ca3b6ff250b4f73486a89f6954edcc4ae21834e" target="_blank">When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> NAACL-HLT</p>
                <p><strong>Paper TL;DR:</strong> The influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher.</p>
                <p><strong>Paper Abstract:</strong> Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8757.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8757.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-3.5 SR^2V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo-16k-0613 (ChatGPT) under Single-Round Self-Reflection Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of ChatGPT-3.5 using a single-round generate-then-reflect procedure (SR^2V): sample K candidate responses, solicit a self-critique for each candidate conditioned on the original input+response, then produce a final revised answer from concatenated response+reflection pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-16k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI ChatGPT chat model (gpt-3.5-turbo variant) with extended 16k context window used as a single snapshot for experiments; model details and training outside scope of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V) / self-reflection prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-stage prompt workflow: Exploration — sample K candidate responses independently (K=4 in main experiments); Reflection — for each candidate, prompt model with [input; candidate] to generate a critique/self-evaluation; Revision — concatenate all response+reflection pairs and prompt model to produce a final revised answer. No external feedback or iterative multi-round conditioning on previous revised outputs (single-round).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>TruthfulQA: benchmark for truthfulness of LM outputs (generation-based metrics: ROUGE-1, BLEURT); HotpotQA: multi-hop question answering dataset requiring reasoning over supporting facts (evaluated with manual accuracy assessment for sampled instances).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>TruthfulQA: paper reports self-reflection (SR^2V) yields significantly better performance than exploration-only and standard prompting on ChatGPT (qualitative statement in main text; exact SR^2V numeric not provided in main text). HotpotQA: self-reflection under SR^2V decreased ChatGPT accuracy by roughly ~4% relative to exploration-only and standard prompting (reported in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Exploration-only baseline and standard prompting: used as baselines in the paper; for HotpotQA the baselines outperformed SR^2V on ChatGPT by ~4% (main text statement). Exact numeric values for the SR^2V main-run on ChatGPT are not tabulated in the main text of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: explicit critique prompts appended to each sampled candidate (e.g., 'Please review and critique your previous response'); the final answer is generated by providing the model with all response+critique pairs and asking for a revised answer.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative comparison (main text) shows improved TruthfulQA performance with SR^2V vs exploration-only and standard prompting for ChatGPT; decomposition analyses (Figures 2 and 3) show that improvements depend on initial response accuracy (Response Accuracy, RA) and human-annotated question difficulty; SR^2V also reduces alignment with majority-voting behavior (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SR^2V harms performance on HotpotQA (multi-hop reasoning) for ChatGPT (~4% drop). Effectiveness strongly depends on model's initial response accuracy (RA) and question difficulty: when RA is high (model already reliably correct), SR^2V often harms accuracy; benefits concentrated when initial responses are unreliable and for harder questions. Models are poor at reliably self-assessing RA (Appendix G).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against exploration-only baseline (concatenation of candidate responses without critiques; akin to self-consistency-style aggregation) and standard single-response prompting. Also compared (in appendices) with conditional/iterative prompting; authors report no major advantage from conditional prompting over parallel SR^2V in their controlled comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Exploration-only baseline (ablation removing the Reflection stage) is included; results show that reflection-specific content changes outcomes (improves TruthfulQA, hurts HotpotQA). Artificial-response experiments (K=10) act as an ablation-like probe showing turning points in RA where reflection moves from beneficial to harmful (e.g., ~50% RA for Hard questions, ~30% for Medium, ~20% for Easy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8757.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8757.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Iterative/Conditional</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo-16k-0613 (ChatGPT) under iterative/conditional prompting variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional/iterative prompting variant evaluated in the appendices where reflections and responses are produced in a conditional sequential manner (a comparison to the parallel SR^2V approach).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-16k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same ChatGPT snapshot as above; used to run a conditional/iterative prompting variant reported in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative prompting / conditional prompting (with self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sequential/conditional process in which responses and reflections may be produced conditioning on prior responses/reflections (iterative prompting). The exact iteration count is not specified in the appendix table; the approach is presented as a contrast to parallel SR^2V.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>TruthfulQA (generation metrics ROUGE-1, BLEURT); HotpotQA (manual accuracy evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Appendix table labeled 'Self-Reflection experiment results using iterative prompting' reports: TruthfulQA - ROUGE-1: Self-Reflection 59.0 (vs Standard 57.5±1.1 and Exploration-Only 55.1); BLEURT: Self-Reflection 72.9 (vs Standard 66.8±1.9 and Exploration-Only 70.1). HotpotQA accuracy: Self-Reflection 71.9 (vs Standard 80.2±0.4 and Exploration-Only 69.7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting and Exploration-only baselines as above (see numbers in 'performance_with_reflection' comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering and conditional API calls where responses and reflections are chained; the paper contrasts this with parallel SR^2V and notes conditional prompting may introduce implicit bias.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Appendix quantitative table shows higher ROUGE-1 and BLEURT on TruthfulQA with self-reflection under iterative prompting, but HotpotQA accuracy is lower under self-reflection vs standard prompting (71.9 vs 80.2). The paper also states empirically there is no significant difference between parallel and conditional prompting for their conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same qualitative limitations as SR^2V: improvements concentrated on TruthfulQA; harms on HotpotQA. Authors note conditional prompting can introduce implicit bias and therefore favor using parallel SR^2V for strict evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to exploration-only baseline and standard prompting; compared qualitatively to parallel (SR^2V) prompting and found no significant differences that change conclusions, but conditional prompting may implicitly bias behavior per Huang et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Exploration-only baseline included; iterative prompting numbers are presented in appendix table as a parallel experiment but no additional ablations (iteration count not varied).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8757.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8757.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 SR^2V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-7b-chat under Single-Round Self-Reflection Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLaMA-2-7b-chat using the SR^2V single-round self-reflection prompting pipeline (K=4 for main experiments), with manual HotpotQA evaluation and automated TruthfulQA metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-2 chat model (7B parameters), 4096 token context limit; evaluated under the same SR^2V experimental protocol where feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same three-stage generate-then-reflect pipeline: sample K candidate responses independently (K=4 for these runs), generate a critique for each candidate conditioned on input+response, then concatenate pairs and ask for a final revised answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>TruthfulQA (ROUGE-1, BLEURT); HotpotQA (manual accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Table 3 (Appendix I) reports: TruthfulQA ROUGE-1: Self-Reflection 53.8 (Standard 53.8±0.4, Exploration-Only 51.7); BLEURT: Self-Reflection 63.0 (Standard 60.9±0.6, Exploration-Only 58.2). HotpotQA Accuracy (manual): Self-Reflection 57.5 (Standard 61.0±1.0, Exploration-Only 62.9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting and Exploration-only as above (see numeric comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering (critique prompts) using the SR^2V single-round pipeline. LLaMA-2's shorter context window limited some experiments (e.g., 10-response artificial experiments not replicated due to 4096 token limit).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>On TruthfulQA some BLEURT improvement observed (63.0 with reflection vs 60.9 standard); on ROUGE-1 self-reflection equals standard prompting. On HotpotQA, self-reflection reduces accuracy versus baselines. The decomposition analyses (Figure 8) mirror trends seen with ChatGPT: benefits concentrated when initial responses are unreliable and for harder questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-reflection harms HotpotQA accuracy for LLaMA-2 in these runs. Context length limits precluded certain artificial-response experiments. Overall trends similar to ChatGPT: reflection helpful when initial RA is low, harmful when RA is high.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against exploration-only baseline (aggregation of candidate outputs) and standard prompting; the qualitative interaction with RA and question difficulty replicates ChatGPT findings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Exploration-only baseline included and used to isolate the contribution of the Reflection stage; LLaMA-2 results follow the same RA x difficulty interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8757.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8757.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral SR^2V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7B-v0.1 under Single-Round Self-Reflection Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Mixtral-8x7B-v0.1 using the SR^2V pipeline (K=4 main runs and additional artificial-response experiments), showing mixed-to-negative effects of self-reflection on multi-hop QA and variable effects on truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixtral mixture-of-experts open model (8x7B configuration), larger context capacity than some baselines; tested under the same SR^2V regimen.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate K candidate responses, elicit a critique for each, then synthesize a final answer from concatenated response+critique pairs; artificial-response experiments also run (K up to 10) to probe behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>TruthfulQA (ROUGE-1, BLEURT); HotpotQA (manual accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Table 4 (Appendix J) reports: TruthfulQA ROUGE-1: Self-Reflection 63.3 (Standard 59.1±1.0, Exploration-Only 61.3); BLEURT: Self-Reflection 71.7 (Standard 71.5±0.4, Exploration-Only 73.9). HotpotQA Accuracy (manual): Self-Reflection 89.2 (Standard 89.8±0.3, Exploration-Only 90.9). Additional artificial-response experiments show self-reflection often fails to improve performance across difficulty/RA bins for Mixtral (Figure 10), with some isolated benefits (e.g., under 0% RA for Easy questions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting and Exploration-only baselines as above (see numeric comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering SR^2V; authors note Mixtral appears less sensitive/competent with reflection instructions so the reflection content can act as a distractor rather than a useful signal.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>On TruthfulQA ROUGE-1 improved with self-reflection (63.3 vs 59.1 standard), but BLEURT did not uniformly improve versus exploration-only; on HotpotQA self-reflection slightly decreased accuracy versus baselines. Artificial-response experiments (Figure 10) show that reflection helps in a few low-RA/hard cases but is detrimental in most bins.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mixtral-8x7B-v0.1 shows minimal change in majority-voting behavior with reflection (Figure 11) and in many cases self-reflection is harmful or neutral; authors hypothesize the model lacks instruction-following sensitivity or competence for self-reflection and that additional training may be required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against exploration-only baseline and standard prompting; contrasted with ChatGPT and LLaMA-2 where reflection effects differed, indicating substantial model-dependent variability in whether self-reflection helps.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Exploration-only baseline and artificial-response (K=10) experiments performed to probe the interaction of RA and question difficulty; results indicate fewer cases of clear benefit relative to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Universal self-consistency for large language model generation <em>(Rating: 2)</em></li>
                <li>Constitutional ai: Harmlessness from ai feedback <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8757",
    "paper_id": "paper-1ca3b6ff250b4f73486a89f6954edcc4ae21834e",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "ChatGPT-3.5 SR^2V",
            "name_full": "gpt-3.5-turbo-16k-0613 (ChatGPT) under Single-Round Self-Reflection Verification",
            "brief_description": "Evaluation of ChatGPT-3.5 using a single-round generate-then-reflect procedure (SR^2V): sample K candidate responses, solicit a self-critique for each candidate conditioned on the original input+response, then produce a final revised answer from concatenated response+reflection pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-16k-0613",
            "model_description": "OpenAI ChatGPT chat model (gpt-3.5-turbo variant) with extended 16k context window used as a single snapshot for experiments; model details and training outside scope of this paper.",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V) / self-reflection prompting",
            "reflection_method_description": "Three-stage prompt workflow: Exploration — sample K candidate responses independently (K=4 in main experiments); Reflection — for each candidate, prompt model with [input; candidate] to generate a critique/self-evaluation; Revision — concatenate all response+reflection pairs and prompt model to produce a final revised answer. No external feedback or iterative multi-round conditioning on previous revised outputs (single-round).",
            "task_name": "TruthfulQA; HotpotQA",
            "task_description": "TruthfulQA: benchmark for truthfulness of LM outputs (generation-based metrics: ROUGE-1, BLEURT); HotpotQA: multi-hop question answering dataset requiring reasoning over supporting facts (evaluated with manual accuracy assessment for sampled instances).",
            "performance_with_reflection": "TruthfulQA: paper reports self-reflection (SR^2V) yields significantly better performance than exploration-only and standard prompting on ChatGPT (qualitative statement in main text; exact SR^2V numeric not provided in main text). HotpotQA: self-reflection under SR^2V decreased ChatGPT accuracy by roughly ~4% relative to exploration-only and standard prompting (reported in main text).",
            "performance_without_reflection": "Exploration-only baseline and standard prompting: used as baselines in the paper; for HotpotQA the baselines outperformed SR^2V on ChatGPT by ~4% (main text statement). Exact numeric values for the SR^2V main-run on ChatGPT are not tabulated in the main text of the paper.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: explicit critique prompts appended to each sampled candidate (e.g., 'Please review and critique your previous response'); the final answer is generated by providing the model with all response+critique pairs and asking for a revised answer.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative comparison (main text) shows improved TruthfulQA performance with SR^2V vs exploration-only and standard prompting for ChatGPT; decomposition analyses (Figures 2 and 3) show that improvements depend on initial response accuracy (Response Accuracy, RA) and human-annotated question difficulty; SR^2V also reduces alignment with majority-voting behavior (Figure 4).",
            "limitations_or_failure_cases": "SR^2V harms performance on HotpotQA (multi-hop reasoning) for ChatGPT (~4% drop). Effectiveness strongly depends on model's initial response accuracy (RA) and question difficulty: when RA is high (model already reliably correct), SR^2V often harms accuracy; benefits concentrated when initial responses are unreliable and for harder questions. Models are poor at reliably self-assessing RA (Appendix G).",
            "comparison_to_other_methods": "Compared against exploration-only baseline (concatenation of candidate responses without critiques; akin to self-consistency-style aggregation) and standard single-response prompting. Also compared (in appendices) with conditional/iterative prompting; authors report no major advantage from conditional prompting over parallel SR^2V in their controlled comparisons.",
            "ablation_study_results": "Exploration-only baseline (ablation removing the Reflection stage) is included; results show that reflection-specific content changes outcomes (improves TruthfulQA, hurts HotpotQA). Artificial-response experiments (K=10) act as an ablation-like probe showing turning points in RA where reflection moves from beneficial to harmful (e.g., ~50% RA for Hard questions, ~30% for Medium, ~20% for Easy).",
            "uuid": "e8757.0",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChatGPT Iterative/Conditional",
            "name_full": "gpt-3.5-turbo-16k-0613 (ChatGPT) under iterative/conditional prompting variant",
            "brief_description": "A conditional/iterative prompting variant evaluated in the appendices where reflections and responses are produced in a conditional sequential manner (a comparison to the parallel SR^2V approach).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-16k-0613",
            "model_description": "Same ChatGPT snapshot as above; used to run a conditional/iterative prompting variant reported in appendices.",
            "reflection_method_name": "Iterative prompting / conditional prompting (with self-reflection)",
            "reflection_method_description": "Sequential/conditional process in which responses and reflections may be produced conditioning on prior responses/reflections (iterative prompting). The exact iteration count is not specified in the appendix table; the approach is presented as a contrast to parallel SR^2V.",
            "task_name": "TruthfulQA; HotpotQA",
            "task_description": "TruthfulQA (generation metrics ROUGE-1, BLEURT); HotpotQA (manual accuracy evaluation).",
            "performance_with_reflection": "Appendix table labeled 'Self-Reflection experiment results using iterative prompting' reports: TruthfulQA - ROUGE-1: Self-Reflection 59.0 (vs Standard 57.5±1.1 and Exploration-Only 55.1); BLEURT: Self-Reflection 72.9 (vs Standard 66.8±1.9 and Exploration-Only 70.1). HotpotQA accuracy: Self-Reflection 71.9 (vs Standard 80.2±0.4 and Exploration-Only 69.7).",
            "performance_without_reflection": "Standard prompting and Exploration-only baselines as above (see numbers in 'performance_with_reflection' comparison).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering and conditional API calls where responses and reflections are chained; the paper contrasts this with parallel SR^2V and notes conditional prompting may introduce implicit bias.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Appendix quantitative table shows higher ROUGE-1 and BLEURT on TruthfulQA with self-reflection under iterative prompting, but HotpotQA accuracy is lower under self-reflection vs standard prompting (71.9 vs 80.2). The paper also states empirically there is no significant difference between parallel and conditional prompting for their conclusions.",
            "limitations_or_failure_cases": "Same qualitative limitations as SR^2V: improvements concentrated on TruthfulQA; harms on HotpotQA. Authors note conditional prompting can introduce implicit bias and therefore favor using parallel SR^2V for strict evaluation.",
            "comparison_to_other_methods": "Compared directly to exploration-only baseline and standard prompting; compared qualitatively to parallel (SR^2V) prompting and found no significant differences that change conclusions, but conditional prompting may implicitly bias behavior per Huang et al. (2023).",
            "ablation_study_results": "Exploration-only baseline included; iterative prompting numbers are presented in appendix table as a parallel experiment but no additional ablations (iteration count not varied).",
            "uuid": "e8757.1",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaMA-2 SR^2V",
            "name_full": "LLaMA-2-7b-chat under Single-Round Self-Reflection Verification",
            "brief_description": "Evaluation of LLaMA-2-7b-chat using the SR^2V single-round self-reflection prompting pipeline (K=4 for main experiments), with manual HotpotQA evaluation and automated TruthfulQA metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7b-chat",
            "model_description": "Open-source LLaMA-2 chat model (7B parameters), 4096 token context limit; evaluated under the same SR^2V experimental protocol where feasible.",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V)",
            "reflection_method_description": "Same three-stage generate-then-reflect pipeline: sample K candidate responses independently (K=4 for these runs), generate a critique for each candidate conditioned on input+response, then concatenate pairs and ask for a final revised answer.",
            "task_name": "TruthfulQA; HotpotQA",
            "task_description": "TruthfulQA (ROUGE-1, BLEURT); HotpotQA (manual accuracy).",
            "performance_with_reflection": "Table 3 (Appendix I) reports: TruthfulQA ROUGE-1: Self-Reflection 53.8 (Standard 53.8±0.4, Exploration-Only 51.7); BLEURT: Self-Reflection 63.0 (Standard 60.9±0.6, Exploration-Only 58.2). HotpotQA Accuracy (manual): Self-Reflection 57.5 (Standard 61.0±1.0, Exploration-Only 62.9).",
            "performance_without_reflection": "Standard prompting and Exploration-only as above (see numeric comparisons).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering (critique prompts) using the SR^2V single-round pipeline. LLaMA-2's shorter context window limited some experiments (e.g., 10-response artificial experiments not replicated due to 4096 token limit).",
            "number_of_iterations": 1,
            "evidence_for_improvement": "On TruthfulQA some BLEURT improvement observed (63.0 with reflection vs 60.9 standard); on ROUGE-1 self-reflection equals standard prompting. On HotpotQA, self-reflection reduces accuracy versus baselines. The decomposition analyses (Figure 8) mirror trends seen with ChatGPT: benefits concentrated when initial responses are unreliable and for harder questions.",
            "limitations_or_failure_cases": "Self-reflection harms HotpotQA accuracy for LLaMA-2 in these runs. Context length limits precluded certain artificial-response experiments. Overall trends similar to ChatGPT: reflection helpful when initial RA is low, harmful when RA is high.",
            "comparison_to_other_methods": "Compared against exploration-only baseline (aggregation of candidate outputs) and standard prompting; the qualitative interaction with RA and question difficulty replicates ChatGPT findings.",
            "ablation_study_results": "Exploration-only baseline included and used to isolate the contribution of the Reflection stage; LLaMA-2 results follow the same RA x difficulty interaction.",
            "uuid": "e8757.2",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Mixtral SR^2V",
            "name_full": "Mixtral-8x7B-v0.1 under Single-Round Self-Reflection Verification",
            "brief_description": "Evaluation of Mixtral-8x7B-v0.1 using the SR^2V pipeline (K=4 main runs and additional artificial-response experiments), showing mixed-to-negative effects of self-reflection on multi-hop QA and variable effects on truthfulness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B-v0.1",
            "model_description": "Mixtral mixture-of-experts open model (8x7B configuration), larger context capacity than some baselines; tested under the same SR^2V regimen.",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V)",
            "reflection_method_description": "Generate K candidate responses, elicit a critique for each, then synthesize a final answer from concatenated response+critique pairs; artificial-response experiments also run (K up to 10) to probe behavior.",
            "task_name": "TruthfulQA; HotpotQA",
            "task_description": "TruthfulQA (ROUGE-1, BLEURT); HotpotQA (manual accuracy).",
            "performance_with_reflection": "Table 4 (Appendix J) reports: TruthfulQA ROUGE-1: Self-Reflection 63.3 (Standard 59.1±1.0, Exploration-Only 61.3); BLEURT: Self-Reflection 71.7 (Standard 71.5±0.4, Exploration-Only 73.9). HotpotQA Accuracy (manual): Self-Reflection 89.2 (Standard 89.8±0.3, Exploration-Only 90.9). Additional artificial-response experiments show self-reflection often fails to improve performance across difficulty/RA bins for Mixtral (Figure 10), with some isolated benefits (e.g., under 0% RA for Easy questions).",
            "performance_without_reflection": "Standard prompting and Exploration-only baselines as above (see numeric comparisons).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering SR^2V; authors note Mixtral appears less sensitive/competent with reflection instructions so the reflection content can act as a distractor rather than a useful signal.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "On TruthfulQA ROUGE-1 improved with self-reflection (63.3 vs 59.1 standard), but BLEURT did not uniformly improve versus exploration-only; on HotpotQA self-reflection slightly decreased accuracy versus baselines. Artificial-response experiments (Figure 10) show that reflection helps in a few low-RA/hard cases but is detrimental in most bins.",
            "limitations_or_failure_cases": "Mixtral-8x7B-v0.1 shows minimal change in majority-voting behavior with reflection (Figure 11) and in many cases self-reflection is harmful or neutral; authors hypothesize the model lacks instruction-following sensitivity or competence for self-reflection and that additional training may be required.",
            "comparison_to_other_methods": "Compared against exploration-only baseline and standard prompting; contrasted with ChatGPT and LLaMA-2 where reflection effects differed, indicating substantial model-dependent variability in whether self-reflection helps.",
            "ablation_study_results": "Exploration-only baseline and artificial-response (K=10) experiments performed to probe the interaction of RA and question difficulty; results indicate fewer cases of clear benefit relative to other models.",
            "uuid": "e8757.3",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Universal self-consistency for large language model generation",
            "rating": 2,
            "sanitized_title": "universal_selfconsistency_for_large_language_model_generation"
        },
        {
            "paper_title": "Constitutional ai: Harmlessness from ai feedback",
            "rating": 1,
            "sanitized_title": "constitutional_ai_harmlessness_from_ai_feedback"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.017217999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</h1>
<p>Yanhong $\mathbf{L i}^{1, <em>}$, Chenghao Yang ${ }^{1, </em>}$, Allyson Ettinger ${ }^{2}$<br>${ }^{1}$ University of Chicago ${ }^{2}$ Allen Institute for AI<br>{yanhongli, chenghao}@uchicago.edu<br>allysone@allenai.org</p>
<h4>Abstract</h4>
<p>Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https: //github.com/yanhong-lbh/ LLM-SelfReflection-Eval.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have shown impressive performance in generating human-like text (e.g., ChatGPT (OpenAI, 2021)), and recent works demonstrate that we can further prompt LLMs to reflect on their own outputs to improve their capabilities on complicated reasoning, programming and planning tasks (Huang et al., 2022; Kim et al., 2023; Madaan et al., 2023; Shinn et al., 2023; Chen et al., 2023b; Wang et al., 2023b) and also improve their alignment with human values (e.g., less harmful</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of Self-Reflection Prompting
and more helpful) (Bai et al., 2022; Ganguli et al., 2023). ${ }^{1}$ However, Huang et al. (2023) find that performance gains associated with self-reflection may be due to implicit usage of external feedback as a stop criterion, as well as overly-engineered prompts that bias the model outputs, casting doubt on the true effectiveness of self-reflection.</p>
<p>To verify the extent to which LLMs can truly reflect on their outputs, we take a more stringent evaluation approach: in addition to excluding external feedback (Huang et al., 2023), we also disallow multi-round iterative prompting, which can hint to the model that its prior response is incorrect. Instead, we sample multiple model responses given a prompt, and ask the model to self-reflect on these candidate outputs. With this single-round testing, we can zero in on the model's ability to use selfreflection without implicit hints about whether a given response candidate is correct or incorrect.</p>
<p>Our experiments show that, in a case study with ChatGPT on different QA datasets, self-reflection in our setting yields mixed results. Specifically, self-reflection improves performance on TruthfulQA (Lin et al., 2022), but decreases model performance in HotpotQA (Yang et al., 2018). Through follow-up analyses, we identify that the effectiveness of self-reflection strongly depends on the confidence in accuracy of the model's initial responses, as well as overall question difficulty as</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>judged by humans: when the model is reliably giving correct answers from the start, self-reflection is more often harmful—however, on questions of greater difficulty, self-reflection is beneficial even when a decent percent of initial model responses are correct. We also find that self-reflection reduces model tendency toward majority voting, suggesting more sophisticated decision-making (albeit sometimes resulting in lower accuracy). Based on our findings, we propose a practical guideline for users to decide when to use self-reflection.</p>
<h2>2 Self-Reflection Prompting</h2>
<p>To focus on evaluating intrinsic reflective thinking capability, we adopt the following evaluation setting: in addition to the Huang et al. (2023) protocol of excluding external feedback and prompt optimization, we additionally disallow iterative prompting, which samples new responses based on previous responses, creating an implicit hint to bias the model behavior (Huang et al., 2023). ${ }^{2}$ We call our approach Single-Round Self-Reflection Verification ( $S R^{2} V$ ). We evaluate LLMs' reflective thinking capability using the following simple threestage format: 1) Exploration: Given an input $X$, we prompt LLM $M$ to generate $K$ candidate responses $r_{j} \sim P_{M}\left(r_{j} \mid X, I_{\text {Exploration }}\right), 1 \leq j \leq K$ with instruction $I_{\text {Exploration }}$. Note that this generation of candidate responses differs from iterative prompting because each response is sampled without conditioning on any other candidate responses. 2) Reflection: For each response $r_{j}$, we prompt $M$ with the concatenated input $\left[X ; r_{j}\right]$ to generate a self-critique $c_{j} \sim P_{M}\left(c_{j} \mid\left[X ; r_{j}\right], I_{\text {Reflection }}\right)$ with another instruction $I_{\text {Reflection }}$. 3) Revision: We concatenate the $K$ response-reflection pairs into a new input and prompt $M$ to generate an improved output. An illustration of this procedure is shown in Figure 1.</p>
<h2>3 Preliminary Study: Does Self-Reflection Prompting Work Under SR ${ }^{2} \mathrm{~V}$ ?</h2>
<p>We follow previous works (Bai et al., 2022; Shinn et al., 2023; Huang et al., 2023) in using two representative datasets, TruthfulQA and HotpotQA, to verify the effectiveness of self-reflection under $\mathrm{SR}^{2} \mathrm{~V}$. TruthfulQA is designed to evaluate the truthfulness of LMs' responses, while HotpotQA fo-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>cuses on multi-hop reasoning tasks, aimed at requiring complex reasoning capabilities.</p>
<p>Experiment Setup For these experiments we set $K=4$, and we prompt ChatGPT-3.5 ("gpt-3.5-turbo-16k-0613") with the questions from each dataset. ${ }^{3}$ Our full process for making these API calls is presented in Appendix F, and all prompt templates used can be found in Appendix E. We also extend our experiments to LLaMA-2 (Touvron et al., 2023) and Mixtral (Jiang et al., 2024), finding similar results to ChatGPT-3.5-we present results and discussion for LLaMA-2 and Mixtral in Appendix I and Appendix J.</p>
<p>For TruthfulQA we evaluate automatically (see details in Appendix D). For HotpotQA, we find that traditional exact match often unfairly assigns a score of 0 for semantically correct model responses; therefore, we manually assess 1,000 randomly chosen HotpotQA instances to check the model's answers against references.</p>
<p>To isolate the specific effect of the generated reflections, we also include an exploration-only baseline, in which we retain the Exploration stage but remove the Reflection component, and only concatenate the candidate model responses in the Revision prompt. ${ }^{4}$</p>
<p>Observations Results are shown in Table 1. In TruthfulQA, we see that using self-reflection achieves significantly better performance than either the exploration-only baseline or standard</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance Decomposition on Question Difficulty and Response Accuracy.</p>
<p>prompting. This finding is consistent with the observation of Bai et al. (2022) that LLMs' self-evaluation (in the form of reflection) can help to produce more factual outputs. However, we see that on HotpotQA, accuracy when using self-reflection is about 4% worse compared to both the exploration-only baseline and standard prompting. These results suggest that self-reflection may in fact harm performance in multi-hop reasoning tasks. This aligns with the self-reflection limitations found in Huang et al. (2023), and verifies that these limitations also extend to our more stringent evaluation setting, but presents a more complicated picture with the continued effectiveness of self-reflection on TruthfulQA under this setting.</p>
<h2>4 Why Self-Reflection May Not Work?</h2>
<p>To better understand these patterns, we conduct an error analysis drawing inspiration from the reflection conceptual model in psychology (Hommel et al., 2023). We hypothesize that two key factors influence self-reflection's efficacy: 1) the objective <strong>question difficulty</strong> (quantifiable based on human annotations), and 2) the <strong>model's comprehension quality</strong> (quantifiable based on the proportion of correct responses). Following this framework, we can predict that if a question is above average in human-annotated difficulty, self-reflection may be of greater benefit. Similarly, if the model already has a strong grasp of the question, it may not benefit as much from self-reflection.</p>
<p>To test these hypotheses, we break down model performance based on levels of question difficulty and model comprehension. We focus our analysis on HotpotQA, as this is the dataset on which we</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance Decomposition on Question Difficulty and Response Accuracy (Artificial Responses). Dotted lines show "turning points" at which reflection loses effectiveness, for Easy/Medium/Hard questions.</p>
<p>observe significant detrimental effects of applying self-reflection prompting. Additionally, this dataset contains annotated human judgments of question difficulty, and enables a clearly-defined notion of accuracy. We use these human difficulty annotations for our measure of question difficulty, and for model comprehension we use Response Accuracy (RA): the proportion of correct answers among the K candidate model responses sampled during Exploration.</p>
<p>The broken-down results are shown in Figure 2. The results show an interaction between our two variables. For questions judged by humans as Easy, self-reflection shows a benefit only when the model's candidate responses are mostly—but not all—incorrect, with self-reflection otherwise having negligible or negative effects on performance. For questions judged as Medium, there is a more even split: when most or all of the model's candidate responses are wrong, self-reflection is beneficial, but when half or more of the responses are correct, self-reflection is often harmful—with the notable exception of the 75% RA bin. A similar pattern is seen for questions judged as Hard, though for this category self-reflection is more consistently beneficial through the 75% RA bin, showing harm to performance only when all candidate model responses are already correct.</p>
<h2>5 Error Analysis via Artificial Response</h2>
<p>The above analysis suggests an interaction between difficulty and comprehension variables in effectiveness of self-reflection—however, our ability to disentangle these effects is limited by imbalanced distribution of model comprehension relative to question difficulty. To assess the interaction more thoroughly, we simulate model "mis-comprehension"</p>
<p>across a wider range of question difficulties, by sampling model responses to minimally edited versions of the prompts, and then pairing these responses with the original prompts when eliciting self-reflection. This allows us to increase the number of incorrect candidate responses, and thus to more evenly distribute RA levels across human difficulty levels. More details on this simulation process can be found in Appendix B.</p>
<p>For this experiment, we generate $\mathrm{K}=10$ candidate responses per question, with a mix of synthetic pairings and real pairings. ${ }^{5}$ Results are shown in Figure 3. We see that the benefits of self-reflection are now limited to the lowest RA levels, and there is also now a clearer shift from beneficial to harmful effects of self-reflection as RA increases. We also see that the interaction with question difficulty remains: the turning point from beneficial to harmful falls around 50\% RA for Hard questions, 30\% for Medium questions, and 20\% for Easy questions. Overall, this indicates that a major contributor to the effectiveness of self-reflection is the confidence of model accuracy on the question-if the model is reliably correct on initial responses, self-reflection tends to be harmful. However, this effect is further modulated by overall question difficulty: the benefits of self-reflection persist to higher levels of response accuracy if the questions are more difficult based on human judgment.</p>
<p>Though TruthfulQA is not as conducive to exact quantification of our variables, based on these results we can now speculate that the effectiveness of self-reflection on that dataset may be attributable to lower rate of good initial model responses, and potentially also higher overall question difficulty.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Majority Voting Analysis</p>
<h2>6 Effects on majority voting</h2>
<p>A natural question to ask at this point is to what extent the effect of RA is due to the model em-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Proposed guide for using Self-Reflection.
ploying majority voting on the candidate responses. In Figure 4 we plot the percentage of items in which the model's output is consistent with majority voting, at different RA levels (computed at $K=10$ including artificially generated responses), both with and without self-reflection. The plot shows that without self-reflection, the tendency to give answers consistent with majority voting is strong and closely correlated with the strength of the accuracy trend (i.e., more majority voting when most candidate responses are either correct or incorrect, and less majority voting when candidates are more mixed). However, with self-reflection the tendency to align with majority voting is significantly reduced across RA levels, suggesting that self-reflection does encourage more sophisticated decision strategies (even if in the case of higher RA levels, this in fact has a harmful effect on accuracy).</p>
<h2>7 Discussion</h2>
<p>Our analyses above have found that self-reflection benefits are limited to cases in which model accuracy is unreliable on initial responses, though benefits are more persistent for harder questions. Based on these findings, we propose a set of guidelines for determining when to implement self-reflection in practical applications, for a given request or prompt. The core principle involves basing decisions on estimated RA and question difficulty, and these guidelines can be applied by simply sampling responses for the target question or prompt. First, if external tools or certain access to ground truth answers are available such that RA can be reliably estimated, then self-reflection should be used when RA levels are low. Next, if difficulty annotations/subjective difficulty judgements are available, self-reflection can also be promising when RA levels are interme-</p>
<p>diate and question difficulty is high. If RA cannot be estimated, response consistency can be used as a proxy: if responses are highly consistent, selfreflection may be unlikely to provide benefit. If consistency is low, then self-reflection may be beneficial, especially for questions of higher difficulty. An illustration of these guidelines is in Figure 5.</p>
<h2>8 Conclusion</h2>
<p>In this paper, we evaluate ChatGPT's self-reflective capabilities under a stringent single-round multiresponse evaluation setting. We find mixed results, and further analysis shows that the effectiveness of self-reflection is impacted both by question difficulty and by model response accuracy level: benefits of self-reflection are mostly limited to cases in which the model's initial responses are unreliable in accuracy, but with more persistent benefits for harder questions. Additionally, we find that self-reflection reduces the model's tendency for majority voting. We propose guidelines for when to use self-reflection, and we look forward to work further exploring impacts on self-reflection, and further refining these guidelines.</p>
<h2>Acknowledgements</h2>
<p>We are grateful for the insightful discussion with Xinyun Chen (Google) and Jie Huang (UIUC) at the early stage of this work (names are not listed in particular order). We also thank the anonymous NAACL reviewers and chairs for providing insightful and constructive feedback to make this work more solid.</p>
<h2>Limitations</h2>
<p>In this work, we adopt a stringent evaluation strategy to test the effectiveness of self-reflective abilities of LLMs. One limitation is that our experiments reported in the main text are based on a single snapshot of the ChatGPT model (gpt-3.5-turbo-16k-0613). We focus on ChatGPT because it is a state-of-the-art chat model, allowing us to make our results directly comparable with previous work-and we limit to this particular version of ChatGPT to ensure that results will not be affected by model updates. However, the assessment of selfreflection may vary between different versions of ChatGPT, as well as between ChatGPT and other LLMs. We do verify our experimental results on other open language models including LLaMA2 (Touvron et al., 2023) and Mixtral (Jiang et al.,
2024) in Appendix I and Appendix J, respectively. While we find that our conclusions can be extended to these models, due to budget limitations we leave more extensive evaluation over other popular proprietary and open models for future works.</p>
<p>Our experiments also use only two datasets for evaluating reflective ability. We chose these two datasets for a focused study covering two very different QA domains, but we look forward to future work further extending these types of analyses to a broader collection of datasets.</p>
<p>We conducted an artificial response experiment in Section 5 to simulate the real output distribution of the language model. This is a rough estimate of ChatGPT's actual output distribution. As we sampled ten fake responses from the language model, it is impossible to cover all possible cases of outputs, and there might be bias in the sample distribution. Future work could try generating a higher number of fake responses to obtain a more accurate distribution of the model.</p>
<p>Finally, although RA proves a valuable metric for determining the utility of self-reflection, its reliance on access to ground truth undermines its practical use. An initial attempt to use GPT-4 to produce an estimate of RA yielded unsatisfactory results (detailed in Appendix G). Further examination of this topic is reserved for future research.</p>
<h2>References</h2>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023a. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023b. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.</p>
<p>Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. 2023. The capacity for moral selfcorrection in large language models. arXiv preprint arXiv:2302.07459.</p>
<p>Mandy Hommel, Bärbel Fürstenau, and Regina H Mulder. 2023. Reflection at work-a conceptual model</p>
<p>and the meaning of its components in the domain of VET teachers. Frontiers in Psychology, 13:923888.</p>
<ul>
<li>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610.</li>
<li>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798.</li>
<li>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088.</li>
<li>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491.</li>
<li>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.</li>
<li>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods.</li>
<li>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.</li>
<li>OpenAI. 2021. Chatgpt. https://openai.com/api/models/gpt-chat/.</li>
<li>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computational Linguistics.</li>
<li>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.</li>
<li>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</li>
<li>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models.</li>
<li>Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, and Heng Ji. 2023b. Enable language models to implicitly learn self-improvement from data. arXiv preprint arXiv:2310.00898.</li>
<li>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380.</li>
</ul>
<h2>Appendix A Accuracy Decomposition over 4 responses</h2>
<p>See Figure 6 for accuracy decomposition over 4 responses using ChatGPT.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Accuracy vs. Correctness Margin for each artificial response</p>
<h2>Appendix B Artificial Response Generation</h2>
<p>We do artificial response generation by prompting ChatGPT to edit the context used in HotpotQA. Specifically, the following steps were adopted: 1) For chosen questions, perform a simple perturbation on the context (e.g., entity replacement). An example is shown in Figure 7. 2) Manually inspect some samples to ensure minimal edits and answerability. 3) Prompt the model to regenerate responses and reflections based on the altered context. In this way, we are simulating scenarios where the model doesn't comprehend the context perfectly. ^{6}</p>
<p>Here is an example for how we modify the context:</p>
<p>^{6}While directly editing outputs to create correct or incorrect answers is an option, we avoid this to ensure the results reflect the model's natural response distribution.</p>
<p>Original question: What nationality was James Henry Miller's wife?</p>
<p>Original context: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was an English folk singer, songwriter, communist, labour activist, actor, poet, playwright and record producer. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is an American folksinger. She is also well known in Britain, where she has lived for more than 30 years, and was married to the singer and songwriter Ewan MacColl until his death in 1989. ...</p>
<p>Fake context 1: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was a Scottish folk singer, songwriter, capitalist, labour activist, actor, poet, playwright and record producer.. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is an American country singer. She is also well known in France, where she has lived for more than 30 years, and was married to the actor and playwright Ewan MacColl until his death in 1989. ...</p>
<p>Fake context 2: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was an Australian folk singer, songwriter, conservative, labour activist, actor, poet, playwright and record producer. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is a British pop singer. She is also well known in Germany, where she has lived for more than 30 years, and was married to the musician and producer Ewan MacColl until his death in 1989. ...</p>
<p>Fake context 3: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was a Canadian folk singer, songwriter, anarchist, labour activist, actor, poet, playwright and record producer. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is an American rapper. She is also well known in Spain, where she has lived for more than 30 years, and was married to the actor and politician Ewan MacColl until his death in 1989. ...</p>
<p>Fake context 4: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was an Irish folk singer, songwriter, monarchist, labour activist, actor, poet, playwright and record pro- ducer. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is a French jazz singer. She is also well known in Italy, where she has lived for more than 30 years, and was married to the artist and filmmaker Ewan MacColl until his death in 1989. ...</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Original Context: Alice is Irish; her best friend is Bob, Bob is British Question: What's the nationality of Alice's best friend?</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original Answer: British</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fake context (many versions):</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Alice is Irish; her ... is Bob, Bob is Chinese Alice is Irish; her ... is Bob, Bob is American Alice is Irish; her ... is Bob, Bob is Australian</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10 different versions w/ minimum edits</td>
</tr>
</tbody>
</table>
<p>Figure 7: Synthesized Artificial Contexts Example</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Standard Prompting</th>
<th style="text-align: center;">Exploration-Only</th>
<th style="text-align: center;">Self-Reflection</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TruthfulQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Rouge-1</td>
<td style="text-align: center;">$57.5 \pm 1.1$</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">$66.8 \pm 1.9$</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">$80.2 \pm 0.4$</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">71.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Self-Reflection experiment results using iterative prompting. Bold-faced numbers at each row indicate the best-performing method under each metric.</p>
<h2>C Conditional Prompting Results</h2>
<p>We demonstrate the conditional prompting results in Table 2. Comparing the results in Table 1 and Table 2, we can see that there is no significant difference between these parallel prompting and conditional prompting. To avoid the implicit bias introduced by conditional prompting, as Huang et al. (2023) point out, we stick to parallel prompting to conduct our evaluation on self-reflective thinking capability.</p>
<h2>D Evaluation details for TruthfulQA</h2>
<p>We use the generation setting of TruthfulQA, which evaluates by comparing how closely the model's responses match a preferred reference versus an undesired one We follow (Lin et al., 2022) to use Rouge-1 (Lin, 2004) and BLEURT (Sellam et al., 2020) for similarity computation.</p>
<h2>E Prompts used in Experiment</h2>
<h2>E. 1 TruthfulQA: Standard Prompt</h2>
<div class="codehilite"><pre><span></span><code>messages={
    {&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question}
}
</code></pre></div>

<h2>E. 2 TruthfulQA: Response Critique Prompt</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Could you critique
    your last response?&quot;\}
\}
</code></pre></div>

<h2>E. 3 TruthfulQA: Response Without Reflection</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_1\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_2\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_3\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\}
\}
</code></pre></div>

<h2>E. 4 TruthfulQA: Response With Reflection</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_1\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Please critique your
    responses&quot; \},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: critique_1\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_2\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Please critique your
    responses&quot; \},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: critique_2\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_3\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Please critique your
    responses&quot; \},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: critique_3\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\}
\}
</code></pre></div>

<h2>E. 5 HotpotQA: Standard Prompt</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant. Answer the question
    based on the context provided.
    Provide extremely concise answers
    with no explanation.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Context: Earth: The
    Earth is the third planet from
    the Sun. Question: Which planet
    is Earth from the Sun? Answer:
    Third&quot; \},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Context:
    \{formatted_context\}\n
    Question: \{question\}\nProvide a
    short answer without
    explanation.&quot;\}
\}
</code></pre></div>

<h2>E. 6 HotpotQA: Response Critique Prompt</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant. Answer the question
    based on the context provided.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Context:
    \{formatted_context\}\n
    Question: \{question\}&quot; \},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: f&quot;\{response\}&quot; \},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Please review and
    critique your previous response,
    and keep in mind not to add any
    unnecessary apologies. You can
    refer back to the original
    context if needed.&quot;\}
\}
</code></pre></div>

<h2>E. 7 HotpotQA: Response Without Reflection</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant. Answer the question
    based on the context provided.
    Provide extremely concise answers
    with no explanation.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Context: Earth: The
    Earth is the third planet from
    the Sun. Question: Which planet
    is Earth from the Sun?
    Answer: Third&quot; \},
    \{&quot;role&quot;: &quot;user&quot;,
</code></pre></div>

<p>"content": f"Context: {formatted_context}\n
Question: {question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_1}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_2}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_3}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_4}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
$}$
E. 8 HotpotQA: Response With Reflection
messages $=$ [
${"$ role": "system",
"content": "You are a helpful assistant. Answer the question based on the context provided. Provide extremely concise answers with no explanation."},
${"$ role": "user",
"content": "Context: Earth: The Earth is the third planet from the Sun. Question: Which planet is Earth from the Sun? Answer: Third" },
${"$ role": "user",
"content": f"Context:
{formatted_context}\n
Question: {question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_1}"},
${"$ role": "user",
"content": f"Please review and critique your previous response, and keep in mind not to add any unnecessary apologies. You can refer back to the original context if needed."},
${"$ role": "assistant",
"content": f"{critique_1}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_2}"},
${"$ role": "user",
"content": f"Please review and critique your previous response, and keep in mind not to add any unnecessary apologies. You can refer back to the original context if needed."},
${"$ role": "assistant",
"content": f"{critique_2}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_3}"},
${"$ role": "user",
"content": f"Please review and critique your previous response, and keep in mind not to add any unnecessary apologies. You can refer back to the original context if needed."},
${"$ role": "assistant",
"content": f"{critique_3}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_4}"},
${"$ role": "user",
"content": f"Please review and critique your previous response, and keep in mind not to add any unnecessary apologies. You can refer back to the original context if needed."},
${"$ role": "assistant",
"content": f"{critique_4}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."}
]</p>
<h2>E. 9 HotpotQA: Fake Evidence Generation</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Here is a question:
    \{question\}. Please create 10
    different versions of &#39;fake
    supporting facts&#39; based on the
    following real supporting facts.
    Modify only one sentence in each
    version, making sure the modified
    sentence is still relevant but
    contains false information. Keep
    the other sentences unmodified.
    Each version of fake supporting
    facts should have the same number
    of sentences as the real
    supporting facts.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Real Supporting
    Facts:\{real_sf\}&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
</code></pre></div>

<p>"content": "Please generate the fake supporting facts versions. Remember to index all the sentences. You must generate 10 versions before you stop."), {"role": "user", "content": f"Fake Supporting Facts Version 1:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 2:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 3:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 4:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 5:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 6:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 7:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 8:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 9:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 10:\n [Insert manipulated sentences here $\hookrightarrow$ ]"), ]</p>
<h2>F Illustration of API Calling Processes</h2>
<p>In this section, we provide a simple example to illustrate the API calling process under our $\mathrm{SR}^{2} \mathrm{~V}$, conditional prompting and the Exploration-Only Baseline.</p>
<h2>F. 1 SR $^{2} \mathrm{~V}$ API Calling Process</h2>
<p>(Splitters and other special tokens are $\hookrightarrow$ omitted)
<em>First API Call</em>:
[Instructions and Context]
Question: [question]
Response: (Sample response_1,
$\hookrightarrow$ response_2 here.)
<em>Second API Call</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: $\qquad$ (Sample reflection_1
$\hookrightarrow$ here.)
<em>Third API Call</em>:
[Instructions and Context]
Question: [question]
Response: response_2
[Instruction for Reflection]
Reflection: $\qquad$ (Sample reflection_2
$\hookrightarrow$ here.)
<em>Final API Call (to get the final
$\hookrightarrow$ revised answer)</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: reflection_1
Question: [question]
Response: response_2
[Instruction for Reflection]
Reflection: reflection_2
Question: [question]
Response: $\qquad$ (Sample final_response $\hookrightarrow$ here)</p>
<h2>F. 2 Conditional Prompting Baseline API Calling Process</h2>
<p><em>First API Call</em>:
[Instructions and Context]
Question: [question]
Response: $\qquad$ (sample response_1 here)
<em>Second API Call</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: $\qquad$ (sample reflection_1
$\hookrightarrow$ here)
<em>Third API Call</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: reflection_1
Question: [question]
Response: $\qquad$ (sample response_2 here)
・.
<em>Final API Call (to get the final
$\hookrightarrow$ revised answer)</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: reflection_1
Question: [question]
Response: response_2
[Instruction for Reflection]
Reflection: reflection_2
Question: [question]
Response: final_reponse</p>
<h2>F. 3 Exploration-Only Baseline API Calling Process</h2>
<p><em>First API Call</em>:</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Instructions and Context]</span>
<span class="na">Question</span><span class="o">:</span><span class="w"> </span><span class="s">[question]</span>
<span class="na">Response</span><span class="o">:</span><span class="w"> </span><span class="s">(sample response_1,</span>
<span class="w">    </span><span class="na">~s response_2 here)</span>
<span class="na">*Final API Call*</span><span class="o">:</span>

<span class="k">[Instructions and Context]</span>
<span class="na">Question</span><span class="o">:</span><span class="w"> </span><span class="s">[question]</span>
<span class="na">Response</span><span class="o">:</span><span class="w"> </span><span class="s">response_1</span>
<span class="na">Question</span><span class="o">:</span><span class="w"> </span><span class="s">[question]</span>
<span class="na">Response</span><span class="o">:</span><span class="w"> </span><span class="s">response_2</span>
<span class="na">Question</span><span class="o">:</span><span class="w"> </span><span class="s">[question]</span>
<span class="na">Response</span><span class="o">:</span><span class="w"> </span><span class="s">(sample final_response</span>
<span class="w">    </span><span class="na">~s here)</span>
</code></pre></div>

<h2>G Challenges in Predicting the Correctness Margin for Model Comprehension</h2>
<p>The effectiveness of a model's self-reflection largely hinges on its "correctness margin," a metric quantifying its understanding of both the question and its context. Ideally, we would like to predict this margin through user prompts, thereby allowing the user to make an informed decision on whether to enable the model's self-reflection capability.</p>
<p>Nevertheless, our experiments indicate that current models struggle to self-assess their understanding reliably. Below, we outline our prompt design used for this experiment:</p>
<div class="codehilite"><pre><span></span><code>messages=[
    {&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant. Answer the question based
    on the context provided. Provide
    extremely concise answers with no
    explanation.&quot;},
    {&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Context:
    {formatted_context}\n
    Question: {question}&quot;},
    {&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: f&quot;{response}&quot;},
    {&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;\nYou have just answered
    a question. Now, please evaluate
        ~s your
    own comprehension of the question
        ~s and
    answer provided. Rate your level of
    understanding on a scale from -5 to
        ~s 5.
    A rating of 5 signifies extreme
    certainty that you understand the
    question, while a rating of -5
    indicates extreme uncertainty or
        ~s lack
    of understanding.&quot;},
]
</code></pre></div>

<p>We tested this prompt structure on two sets of questions: one where all 10 model responses were incorrect, and another where all 10 were correct. If the model were capable of accurately evaluating its own comprehension, it should consistently rate its understanding at -5 for questions in the all-wrong dataset and 5 for those in the all-right dataset. However, after experimenting with 20 examples from each dataset, we found that the model consistently assigned high scores (typically 4 or 5 ) regardless of the dataset origin. Thus, reliable self-assessment remains an open challenge for current models.</p>
<h2>H Scientific Artifacts</h2>
<p>In this paper, we use the following artifacts:</p>
<ul>
<li>TruthfulQA (Lin et al., 2022) is a benchmark assessing a language model's ability to generate truthful answers for 817 diverse questions in 38 categories, requiring models to avoid false answers commonly found in human texts due to misconceptions or false beliefs. We use it for the preliminary studies on reflective thinking in LLMs. It is licensed under the Apache License, Version 2.0.</li>
<li>HotpotQA (Yang et al., 2018) is a 113k question-answer dataset based on Wikipedia that requires multi-document reasoning, features diverse questions unconstrained by knowledge bases or schemas, provides sentence-level supporting facts for strong supervision and explanation, and introduces a new factoid comparison question type to evaluate QA systems' extraction and comparison abilities. We use it for evaluating reflective thinking in LLMs. It is distributed under a CC BY-SA 4.0 License.</li>
<li>openai-python ${ }^{7}$ (v0.27.8) provides convenient access to the OpenAI REST API from any Python 3.7+ application. We use it to access ChatGPT models. It is licensed under the Apache License, Version 2.0.</li>
</ul>
<h2>I Results on LLaMA-2-7b-chat</h2>
<p>We extend our experiments to the open-sourced model LLaMA-2-7b-chat (Touvron et al., 2023), and the results support the conclusions that we draw from our experiments on ChatGPT, indicating that our findings can be generalized to different models.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Standard</th>
<th>Exploration-</th>
<th>Self-</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Prompting</td>
<td>Only</td>
<td>Reflection</td>
</tr>
<tr>
<td>TruthfulQA</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Rouge-1</td>
<td>$53.8 \pm 0.4$</td>
<td>51.7</td>
<td>53.8</td>
</tr>
<tr>
<td>BLEURT</td>
<td>$60.9 \pm 0.6$</td>
<td>58.2</td>
<td>63.0</td>
</tr>
<tr>
<td>HotpotQA</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Accuracy*</td>
<td>$61.0 \pm 1.0$</td>
<td>62.9</td>
<td>57.5</td>
</tr>
</tbody>
</table>
<p>Table 3: Self-reflection SR ${ }^{2} \mathrm{~V}$ experiment results on QA datasets using LLaMA-2-chat. Bold-facing indicates the best-performing method under each metric. *Evaluated manually.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Performance Decomposition on Question Difficulty and Response Accuracy (LLaMA-2-chat).</p>
<p>More specifically, for the preliminary study on performance across TruthfulQA and HotpotQA, the results on LLaMA-2-7b-chat (see Table 3) are consistent with the results obtained from ChatGPT (see Table 1): self-reflection prompts improve performance on TruthfulQA while worsening performance on HotpotQA. Additionally, we conduct our error analysis on the results of HotpotQA, breaking down model performance based on levels of question difficulty and model comprehension. The LLaMA-2-7b-chat results for this analysis (Figure 8) also closely follow the trend observed in the results from ChatGPT (Figure 2): self-reflection is more beneficial when the model's initial responses are incorrect and when the question difficulty is higher.</p>
<p>We do not replicate the 10-response artificial experiments on LLaMA-2-7b-chat due to the context length limit. The context length for LLaMA-2 is 4096, which is shorter than our context length for the task. We replicate this experiment in Appendix J as Mixtral models have larger token limits.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Standard <br> Prompting</th>
<th style="text-align: center;">Exploration- <br> Only</th>
<th style="text-align: center;">Self- <br> Reflection</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TruthfulQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Rouge-1</td>
<td style="text-align: center;">$59.1 \pm 1.0$</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">$71.5 \pm 0.4$</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Accuracy*</td>
<td style="text-align: center;">$89.8 \pm 0.3$</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">89.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Self-reflection $\mathrm{SR}^{2} \mathrm{~V}$ experiment results on QA datasets using Mixtral-8x7B-v0.1. Bold-facing indicates the best-performing method under each metric. *Evaluated manually.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Performance Decomposition on Question Difficulty and Response Accuracy (Mixtral-8x7B-v0.1).</p>
<h2>J Results on Mixtral-8x7B-v0.1</h2>
<p>We repeat our experiments on Mixtral-8x7B-v0.1. For the preliminary study on performance across TruthfulQA and HotpotQA (Table 4, we again observe a similar trend to ChatGPT: while selfreflection may help improve the performance on TruthfulQA, it harms the performance on HotpotQA. Then, we again break down model performance on HotpotQA based on question difficulty levels and model comprehension in Figure 9. Here we observe a somewhat different pattern: under all question difficulty levels and model comprehension, self-reflection prompting fails to improve the performance. To further verify this finding, we also conduct the artificial response experiments, with results in Figure 10. Here we see that self-reflection prompting is not always harmful to performance (e.g., under $0 \%$ RA, self-reflection helps improve the performance of easy questions.), but in most cases it is harmful.</p>
<p>We hypothesize that these divergent patterns arise because this particular model may be less sen-</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Performance Decomposition on Question Difficulty and Response Accuracy (Artificial Responses) for Mixtral-8x7B-v0.1.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Majority Voting Analysis (Mixtral-8x7Bv0.1).
sitive in general to instructions for reflection, or less well-equipped to understand them, such that the reflection part serves mostly as a distractor in the input. We examine this hypothesis in the majority voting experiments (Figure 11) and find that compared with ChatGPT, the addition of self-reflection exerts minimal impact on majority voting trends, suggesting that it is comparatively difficult to use self-reflection prompting to change the default behaviors in the case of this model. This is consistent with our hypothesis that Mixtral-8x7B-v0.1 lacks sensitivity or competence in self-reflection, so we speculate that additional training may be needed to help this model to unlock self-reflection prompting potential.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/openai/openai-python&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ The 16 k variant is chosen to accommodate responses and reflection pairs that exceed the standard 4096 token limit, particularly in detailed experiments of Section 5.
${ }^{4}$ The exploration-only baseline can be viewed as one implementation of (universal) self-consistency prompting (Wang et al., 2023a; Chen et al., 2023a). Rather than applying majority voting directly to the outputs, this method involves inputting these outputs back into the model for aggregation. As we'll explore in Section 6, we also find the model predominantly engages in a form of majority voting in this process.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
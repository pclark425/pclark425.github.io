<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8028 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8028</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8028</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-258841365</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.14259v7.pdf" target="_blank">SciMON: Scientific Inspiration Machines Optimized for Novelty</a></p>
                <p><strong>Paper Abstract:</strong> We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link prediction--severely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature. We present SciMON, a modeling framework that uses retrieval of"inspirations"from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved. Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue. Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8028.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8028.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Rating (Study I)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert rating (binary helpful/unhelpful) — Study I</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binary human evaluation where NLP expert annotators judged generated idea sentences as 'helpful' (passing relevance, novelty, clarity and reasonableness) or 'unhelpful' for 50 gold instances; outputs were blind and randomly shuffled across systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, T5 (fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0314; davinci-003; T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (primary) with a biochemical case study</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Idea / hypothesis sentence generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert rating (binary helpful/unhelpful)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Annotators (graduate-level NLP experts) read a background+seed and a generated idea and rated whether the idea was sufficiently relevant, novel (not copying), scientifically sensible, and clearly expressed; label = 'helpful' if all considerations are met.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percent helpful outputs (human votes)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage of annotator votes marking an output as helpful vs. unhelpful (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Gold subset of SCIMON test split (50 randomly selected instances from gold subset derived from ACL Anthology; overall gold subset contains 194 instances)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>6 volunteer NLP experts with graduate-level education; 50 instances (from the gold subset); each annotator received 10 instances with system outputs randomly shuffled and blind to condition; raters assessed relevance, novelty (not copying), scientific sense, and clarity. Moderately high inter-rater agreement reported (see paper for exact agreement table).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT4FS and GPT4FS+KG received the largest share of helpful votes compared to other variants (detailed percent votes reported in Table 2). GPT3.5 models performed worse than fine-tuned T5 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Study I compared system outputs across model variants; Study II (separate) compared to ground-truth paper ideas and found ground truth generally superior (see Study II).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Annotators were graduate students (opinions may differ from domain experts); gold subset pre-filtered to avoid trivial overlaps; binary helpful label does not capture magnitude of novelty or depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8028.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Ranking vs Ground Truth (Study II)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human comparative ranking of LLM outputs vs. ground-truth paper ideas — Study II</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A follow-up human study where expert annotators ranked GPT4FS and GPT4FS+KG outputs against each other and compared them to the ground-truth idea from the source paper in terms of technical detail and innovation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0314)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0314</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Idea / hypothesis sentence generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human comparative ranking against ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Annotators ranked pairs of model outputs by technical detail and novelty, and also judged whether each model output reached the level of technical detail/innovation of the ground-truth (paper) idea.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percent of pairwise preferences and percent comparisons to ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>E.g., % of pairs where GPT4FS+KG was judged more technical (0–100%), % of cases where ground truth judged to have higher technical level/novelty (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Subset of gold test instances (annotated subset used for Study II)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Subset of annotators from Study I; outputs were ranked rather than binary labeled; comparisons made between GPT4FS and GPT4FS+KG and against the ground-truth idea from the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT4FS+KG had higher technical detail in 48% of compared pairs and was less incremental (more novel) in 45% of pairs. In 85% of comparisons, the ground truth idea was judged to have significantly higher technical level and novelty than model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Models (even best variants) were substantially below human-authored ground-truth ideas in technical depth and novelty in 85% of comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Ground-truth ideas drawn from paper abstracts may lack full experimental context; annotator pool limited in size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8028.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Study III - Iterative Novelty Annotation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation of iterative novelty boosting (Study III)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation specifically assessing whether iterative novelty boosting changes and increases novelty of model-generated ideas across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (GPT4FS+SN reported best for iterations); T5 variants also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0314; T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Idea / hypothesis sentence refinement across iterations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human paired comparison of original vs. novelty-boosted outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Annotators compared initial generated idea and post-iteration idea to judge whether the updated idea is substantially different and whether it is more novel/creative.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percent substantially different; percent judged more novel; number of new terms added</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>E.g., first-iteration: % of updated ideas substantially different from original (0–100%); % of updated ideas judged more novel (0–100%); average number of new content terms added.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>70 instances sampled from gold sentence generation subset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>5 annotators compared original vs regenerated ideas on 70 instances; instructions asked whether regenerated idea is substantially different and whether it is more novel/creative.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>For semantic-neighbor (SN) variant: first iteration produced substantially different ideas in 88.9% of cases; 55.6% were judged more novel after first iteration. Second iteration further increased novelty for 57.8% of ideas that continued to another iteration. Average new terms added ~22–23 (after filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Novelty increases often corresponded to adding generic combinations of popular concepts rather than deep, technical novelty; human judgement of novelty can be subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8028.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-L (Longest Common Subsequence based ROUGE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic overlap-based metric commonly used for text generation evaluation; ROUGE-L measures longest common subsequence F1 between generated and reference texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to outputs from GPT-4, GPT-3.5, T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0314; davinci-003; T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automated similarity-based evaluation of generated idea sentences</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE-L automated similarity scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute ROUGE-L F1 between generated idea sentence and ground-truth idea sentence; higher indicates greater surface overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-L F1 score</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>F1 score range 0.0–1.0 (reported in paper as decimals, e.g., 0.120), often reported as decimals or percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Challenging and gold subsets of SCIMON test data (ACL-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None (automated metric).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 9 reports ROUGE-L for models (e.g., GPT4ZS 0.120 R-L on challenging subset; T5 variants up to ~0.228).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>ROUGE rewards copying and surface overlap; longer GPT-4 outputs scored worse on ROUGE despite being preferred by humans, highlighting metric mismatch for open-ended idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8028.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTScore (SciBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore using SciBERT checkpoint</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic similarity metric for text generation using contextual embeddings; paper uses SciBERT as encoder to compute BERTScore between generated and reference idea sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to outputs from GPT-4, GPT-3.5, T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0314; davinci-003; T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / scientific text</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automated semantic similarity evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore (SciBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute token-level contextual embedding similarities between generated and reference sentences using SciBERT checkpoint; aggregate to a score reflecting semantic overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BERTScore (precision/recall/F1 aggregated; paper reports a single score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Score typically ranges 0.0–1.0 (paper reports values like 0.581 etc.), higher = greater semantic similarity to reference.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Challenging and gold subsets of SCIMON test data</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None (automated metric).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 9: e.g., GPT4ZS BERTScore 0.581 on challenging subset; T5 variants up to ~0.672.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>BERTScore favors semantic similarity to reference and can reward paraphrases or copying; may not reflect novelty or technical depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8028.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BARTScore (generation evaluation by model likelihood)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic metric that scores generated text by computing conditional likelihood under a pretrained generation model (BART-style); mentioned as an automated metric used to compare generated sentences to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Not directly used to generate theories (used for scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / text generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automated evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BARTScore automated evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use a pretrained seq2seq model to compute log-probability of reference given candidate (or vice versa) as a quality score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BARTScore (log-probability based score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Model log-probabilities or normalized scores; relative comparisons are used.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used as part of automated evaluation suite (paper mentions BARTScore alongside ROUGE and BERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None (automated metric).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned qualitatively (no detailed numeric breakdown presented in main text beyond listing of automated metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>BARTScore reflects model likelihood and can be biased by reference/model mismatch; not sufficient to assess novelty or technical depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8028.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context Contrastive Objective (InfoNCE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context contrastive augmentation using an InfoNCE loss over decoder hidden states</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training augmentation that adds in-context negative examples (sentences from the input context) and applies an InfoNCE contrastive loss alongside cross-entropy to discourage models from copying background text and encourage novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Training objective to encourage novelty in generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>In-context contrastive training (InfoNCE)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>During fine-tuning, pick in-context negatives from input text; compute decoder hidden-state representations for positive target and negatives and apply InfoNCE-style loss combined with cross-entropy to push model representation away from in-context negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Contrastive loss value (L_cl) during training; downstream effect measured by human and automated metrics (e.g., improved ROUGE/BERTScore and human novelty judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>InfoNCE-style loss normalized via temperature τ; no single external numeric scale beyond training loss and downstream metric improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Trained on SCIMON dataset derived from ACL Anthology (training split before 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Models with contrastive augmentation compared in human studies and automated metrics (reported improvements vs baseline fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>T5+SN+CL and other CL variants outperform baseline fine-tuned T5 on automated metrics (Table 9) and reduce copying behavior; human evaluations show improvement vs non-contrastive variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Contrastive negatives are sampled from input and may not capture all forms of copying; improved novelty is limited and does not achieve the depth of human-authored papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8028.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Novelty Boosting (retrieve-compare-update)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative retrieval–compare–update novelty boosting with γ_nov and threshold µ</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that iteratively retrieves similar prior literature ideas, scores similarity to generated idea I_t, and prompts the LLM to update the idea until similarity scores fall below a threshold µ, thereby encouraging novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to outputs from GPT-4 (GPT4FS variants) and T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0314; T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (and applied in biochemical case study)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automated iterative refinement method to increase novelty of generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Iterative novelty boosting via retrieval and negative example prompting</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate initial idea I0, retrieve top-k nearest literature ideas from reference corpus R using SentenceBERT (k=20), compute cosine similarities S_i; if any S_i >= µ (µ=0.6), provide retrieved literature sentences as negatives and instruct the model to produce a more novel idea; repeat until all S_i < µ.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Novelty delta percentages (human-judged) and number of new terms added; similarity scores used internally (cosine similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Cosine similarity (0–1) between embeddings; threshold µ = 0.6 used to trigger iterations. Human-reported % increases in novelty (e.g., +54.4% first-iteration novelty for best variant in Table 3 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Reference corpus R built from training set (papers before 2021); retrieval over ~59k papers / 374k sentences in NLP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Study III had 5 annotators on 70 instances evaluating whether updated ideas were different and more novel; iterative improvement recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Iterative method substantially increases novelty where applied: e.g., first-iteration novelty deltas reported (e.g., +54.4% for GPT4FS+SN+CT+KG in Table 3); SN variant: 88.9% of updated ideas were substantially different and 55.6% judged more novel after first iteration; second iteration increased novelty for 57.8% of ideas that continued.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Novelty often superficial (recombinations of popular concepts), limited by retrieval quality and embedding similarity proxy; retrieval and similarity threshold choices affect behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8028.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Similarity Retrieval (SentenceBERT all-mpnet-base-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic retrieval using SentenceBERT (all-mpnet-base-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sentence embeddings from SentenceBERT (all-mpnet-base-v2) used for multiple retrieval tasks: semantic neighbors, citation neighbor selection, and retrieving nearest ideas for novelty checking, with cosine similarity as the distance metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SentenceBERT (all-mpnet-base-v2) for retrieval; used alongside LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>all-mpnet-base-v2 (SentenceBERT checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (retrieval over scientific text)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Retrieval/semantic similarity component used to find inspirations and prior ideas</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Embedding-based semantic retrieval with cosine similarity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Represent queries and candidate texts with SentenceBERT embeddings and compute cosine similarity to retrieve top-k neighbors (k varies: semantic neighbors up to 20, novelty retrieval k=20, citation neighbors up to 5).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Cosine similarity scores; top-k retrieval counts</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Cosine similarity in [0,1]; specific thresholds used (e.g., µ=0.6 for novelty retrieval); gold-split selection used cosine similarity ≤ 0.074 between background and ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Training corpus and reference corpus built from ACL Anthology (59k papers, 374k sentences) and citation title sets (87k titles).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Retrieval outputs used as inspirations for generation; human studies evaluated downstream quality improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Retrieval supports the SCIMON pipeline; novelty boosting relies on SentenceBERT similarities. Gold subset created by filtering background/ground-truth pairs with cosine similarity ≤ 0.074 (10th percentile).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Quality depends on embedding model; retrieval noise and imperfect IE upstream limit effectiveness; threshold choices are heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8028.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset: ACL Anthology via S2ORC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACL Anthology corpus extracted via S2ORC (Semantic Scholar Open Research Corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary NLP domain corpus used to construct the SCIMON dataset: 67,408 ACL Anthology papers (1952–2022) filtered for English with abstracts; used to extract background sentences, target idea sentences, seed terms, and to build the reference corpora and KG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs trained/evaluated on data derived from this corpus (T5-large fine-tuned, GPT evaluations held-out)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-large for fine-tuning; GPT-4/GPT-3.5 used for inference</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Corpus for generating and evaluating idea/hypothesis sentences</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dataset for training/evaluation (temporal split and gold subset)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Constructed instances by IE pipeline: background sentences (via scientific sentence classification), target idea sentences, and seed terms via scientific IE; split temporally into train/dev/test (<2021 / 2021 / 2022) and created a manually vetted gold subset (194 instances).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Dataset statistics (counts): 67,408 total papers; training/dev/test splits as in paper; gold subset size 194 instances used for high-quality evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Counts of papers and instances; similarity threshold for gold set selection (cosine similarity ≤ 0.074).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL Anthology (S2ORC) derived SCIMON dataset; gold subset of 194 challenging instances</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Gold subset manually vetted (excluded trivial overlaps) and used in human studies (Study I–III).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset split: ~58,874 papers before 2021 (train), 5,946 from 2021 (dev), 2,588 from 2022 (test); gold subset size 194 after filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>IE preprocessing noisy (relation extraction precision ~65%); overall instance pass rate after preprocessing 79.7%; dataset limited to English abstracts and ACL Anthology coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8028.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset: PubMed / Biochemical Case Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubMed-derived biochemical dataset (PubTator 3 based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biochemical-domain dataset constructed similarly to the NLP pipeline by collecting PubMed abstracts, extracting entities and relations with PubTator 3, and selecting background/target sentences for domain generalization experiments and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meditron-7b (biomedical LLM) fine-tuned variants; also used to evaluate transfer/generalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Meditron-7b (epfl-llm/meditron-7b)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biochemistry / biomedical</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Idea / hypothesis sentence generation in biochemical domain</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Domain generalization case study human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect PubMed papers (1988–2024) on selected topics, extract entities/relations with PubTator 3, classify sentences, fine-tune biomedical LLM and evaluate on held-out test split past model pretraining cutoff; have domain experts rate generated directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percent positive human ratings (human experts)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage of generated directions rated positively by 2 biochemical domain experts (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubMed-derived dataset (counts reported: e.g., final dataset counts: 4,767 papers before 2023/02, 642 papers 2023/02–2023/08, 299 after 2023/08; elsewhere 5,708 PubMed papers collected overall).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>2 biochemical domain experts with graduate-level education evaluated outputs; they rated ~80% of generated directions positively in this initial experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Domain experts rated ~80% of generated directions positively; evaluators in this small study were sometimes more satisfied with generated outputs than ground truth regarding technical detail (contrasting the NLP findings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>In this preliminary biochemical case study, evaluators in some cases rated generated outputs more favorably than ground-truth, though the experiment was small and intended as a generality check.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Preliminary small-scale study; domain experts limited in number (2); dataset and evaluation not exhaustive; PubMed extraction dependent on PubTator quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8028.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gold Subset Selection Criterion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gold test subset selection using cosine similarity thresholding and manual vetting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedure to create a high-quality, challenging evaluation subset by selecting background/ground-truth pairs with low surface similarity (cosine similarity ≤ 0.074, the 10th percentile) and manual annotation to remove trivial or irrelevant pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used to evaluate all LLM variants (GPT-4, GPT-3.5, T5, Meditron)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0314; davinci-003; T5-large; meditron-7b</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (evaluation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation data curation criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Gold subset selection (low similarity + manual filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute cosine similarity between background and corresponding ground-truth sentence for test pairs; select pairs with similarity ≤ 0.074 (10th percentile) and manually remove trivial overlaps, irrelevant backgrounds, or instances lacking salient target relation, yielding 194 high-quality instances.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Cosine similarity cutoff and final gold subset count</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Cosine similarity (SentenceBERT embedding) threshold ≤ 0.074; resulting gold subset size = 194 instances.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCIMON test split derived from ACL Anthology</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual vetting by annotators to ensure challenging, non-trivial pairs used for human studies.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Gold subset contains 194 instances selected via the described procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Choice of cosine similarity cutoff is heuristic; manual vetting subjective; gold subset small relative to full test set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8028.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8028.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preprocessing Quality Criteria / IE Precision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information extraction preprocessing quality metrics and conservative filtering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retention criteria and measured precisions for the IE pipeline components (entity/relation extraction, coreference, abbreviation resolution, sentence classification) used to build training/evaluation data; only high-confidence IE outputs were kept.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PL-Marker (scientific IE), SciCo (coreference), ScispaCy (abbreviation resolution), Sci-sentence classifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP dataset curation / IE</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Data quality / preprocessing criteria for evaluation validity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>IE preprocessing precision and conservative filtering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate precision of each IE preprocessing step via manual sampling; keep only high-confidence outputs to reduce noise; report per-step precision and overall pass rate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-step precision percentages and overall pass rate</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported precisions: most steps 91%–100% precision except relation extraction ~65%; overall rate of instances passing all steps = 79.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCIMON dataset derived from ACL Anthology</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual evaluation of random sample to estimate IE precision; final gold subset further manually vetted.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>IE preprocessing step precisions: 91%–100% for most steps, relation extraction 65%; combined pass rate 79.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relation extraction is noisier (65%); dataset quality depends on IE reliability and conservative filtering may remove useful examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ROUGE: A package for automatic evaluation of summaries <em>(Rating: 2)</em></li>
                <li>BERTScore: Evaluating text generation with BERT <em>(Rating: 2)</em></li>
                <li>BARTScore: Evaluating generated text as text generation <em>(Rating: 2)</em></li>
                <li>Sentence-BERT: Sentence embeddings using Siamese BERT-networks <em>(Rating: 2)</em></li>
                <li>ScispaCy: Fast and robust models for biomedical natural language processing <em>(Rating: 1)</em></li>
                <li>Scico: Hierarchical cross-document coreference for scientific concepts <em>(Rating: 1)</em></li>
                <li>PL-Marker (Packed levitated marker) for entity and relation extraction <em>(Rating: 1)</em></li>
                <li>PubTator 3 <em>(Rating: 2)</em></li>
                <li>S2ORC: The semantic scholar open research corpus <em>(Rating: 2)</em></li>
                <li>Meditron-70b: Scaling medical pretraining for large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8028",
    "paper_id": "paper-258841365",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Human Expert Rating (Study I)",
            "name_full": "Human expert rating (binary helpful/unhelpful) — Study I",
            "brief_description": "Binary human evaluation where NLP expert annotators judged generated idea sentences as 'helpful' (passing relevance, novelty, clarity and reasonableness) or 'unhelpful' for 50 gold instances; outputs were blind and randomly shuffled across systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, T5 (fine-tuned variants)",
            "model_size": "gpt-4-0314; davinci-003; T5-large",
            "scientific_domain": "Natural Language Processing (primary) with a biochemical case study",
            "theory_type": "Idea / hypothesis sentence generation",
            "evaluation_method_name": "Human expert rating (binary helpful/unhelpful)",
            "evaluation_method_description": "Annotators (graduate-level NLP experts) read a background+seed and a generated idea and rated whether the idea was sufficiently relevant, novel (not copying), scientifically sensible, and clearly expressed; label = 'helpful' if all considerations are met.",
            "evaluation_metric": "Percent helpful outputs (human votes)",
            "metric_definition": "Percentage of annotator votes marking an output as helpful vs. unhelpful (0–100%).",
            "dataset_or_benchmark": "Gold subset of SCIMON test split (50 randomly selected instances from gold subset derived from ACL Anthology; overall gold subset contains 194 instances)",
            "human_evaluation_details": "6 volunteer NLP experts with graduate-level education; 50 instances (from the gold subset); each annotator received 10 instances with system outputs randomly shuffled and blind to condition; raters assessed relevance, novelty (not copying), scientific sense, and clarity. Moderately high inter-rater agreement reported (see paper for exact agreement table).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT4FS and GPT4FS+KG received the largest share of helpful votes compared to other variants (detailed percent votes reported in Table 2). GPT3.5 models performed worse than fine-tuned T5 variants.",
            "comparison_to_human_generated": true,
            "comparison_results": "Study I compared system outputs across model variants; Study II (separate) compared to ground-truth paper ideas and found ground truth generally superior (see Study II).",
            "limitations_noted": "Annotators were graduate students (opinions may differ from domain experts); gold subset pre-filtered to avoid trivial overlaps; binary helpful label does not capture magnitude of novelty or depth.",
            "uuid": "e8028.0",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Human Ranking vs Ground Truth (Study II)",
            "name_full": "Human comparative ranking of LLM outputs vs. ground-truth paper ideas — Study II",
            "brief_description": "A follow-up human study where expert annotators ranked GPT4FS and GPT4FS+KG outputs against each other and compared them to the ground-truth idea from the source paper in terms of technical detail and innovation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0314)",
            "model_size": "gpt-4-0314",
            "scientific_domain": "Natural Language Processing",
            "theory_type": "Idea / hypothesis sentence generation",
            "evaluation_method_name": "Human comparative ranking against ground truth",
            "evaluation_method_description": "Annotators ranked pairs of model outputs by technical detail and novelty, and also judged whether each model output reached the level of technical detail/innovation of the ground-truth (paper) idea.",
            "evaluation_metric": "Percent of pairwise preferences and percent comparisons to ground truth",
            "metric_definition": "E.g., % of pairs where GPT4FS+KG was judged more technical (0–100%), % of cases where ground truth judged to have higher technical level/novelty (0–100%).",
            "dataset_or_benchmark": "Subset of gold test instances (annotated subset used for Study II)",
            "human_evaluation_details": "Subset of annotators from Study I; outputs were ranked rather than binary labeled; comparisons made between GPT4FS and GPT4FS+KG and against the ground-truth idea from the paper.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT4FS+KG had higher technical detail in 48% of compared pairs and was less incremental (more novel) in 45% of pairs. In 85% of comparisons, the ground truth idea was judged to have significantly higher technical level and novelty than model outputs.",
            "comparison_to_human_generated": true,
            "comparison_results": "Models (even best variants) were substantially below human-authored ground-truth ideas in technical depth and novelty in 85% of comparisons.",
            "limitations_noted": "Ground-truth ideas drawn from paper abstracts may lack full experimental context; annotator pool limited in size.",
            "uuid": "e8028.1",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Study III - Iterative Novelty Annotation",
            "name_full": "Human evaluation of iterative novelty boosting (Study III)",
            "brief_description": "Human evaluation specifically assessing whether iterative novelty boosting changes and increases novelty of model-generated ideas across iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (GPT4FS+SN reported best for iterations); T5 variants also evaluated",
            "model_size": "gpt-4-0314; T5-large",
            "scientific_domain": "Natural Language Processing",
            "theory_type": "Idea / hypothesis sentence refinement across iterations",
            "evaluation_method_name": "Human paired comparison of original vs. novelty-boosted outputs",
            "evaluation_method_description": "Annotators compared initial generated idea and post-iteration idea to judge whether the updated idea is substantially different and whether it is more novel/creative.",
            "evaluation_metric": "Percent substantially different; percent judged more novel; number of new terms added",
            "metric_definition": "E.g., first-iteration: % of updated ideas substantially different from original (0–100%); % of updated ideas judged more novel (0–100%); average number of new content terms added.",
            "dataset_or_benchmark": "70 instances sampled from gold sentence generation subset",
            "human_evaluation_details": "5 annotators compared original vs regenerated ideas on 70 instances; instructions asked whether regenerated idea is substantially different and whether it is more novel/creative.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "For semantic-neighbor (SN) variant: first iteration produced substantially different ideas in 88.9% of cases; 55.6% were judged more novel after first iteration. Second iteration further increased novelty for 57.8% of ideas that continued to another iteration. Average new terms added ~22–23 (after filtering).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Novelty increases often corresponded to adding generic combinations of popular concepts rather than deep, technical novelty; human judgement of novelty can be subjective.",
            "uuid": "e8028.2",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ROUGE-L",
            "name_full": "ROUGE-L (Longest Common Subsequence based ROUGE)",
            "brief_description": "An automatic overlap-based metric commonly used for text generation evaluation; ROUGE-L measures longest common subsequence F1 between generated and reference texts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to outputs from GPT-4, GPT-3.5, T5",
            "model_size": "gpt-4-0314; davinci-003; T5-large",
            "scientific_domain": "Natural Language Processing",
            "theory_type": "Automated similarity-based evaluation of generated idea sentences",
            "evaluation_method_name": "ROUGE-L automated similarity scoring",
            "evaluation_method_description": "Compute ROUGE-L F1 between generated idea sentence and ground-truth idea sentence; higher indicates greater surface overlap.",
            "evaluation_metric": "ROUGE-L F1 score",
            "metric_definition": "F1 score range 0.0–1.0 (reported in paper as decimals, e.g., 0.120), often reported as decimals or percentages.",
            "dataset_or_benchmark": "Challenging and gold subsets of SCIMON test data (ACL-derived)",
            "human_evaluation_details": "None (automated metric).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Table 9 reports ROUGE-L for models (e.g., GPT4ZS 0.120 R-L on challenging subset; T5 variants up to ~0.228).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "ROUGE rewards copying and surface overlap; longer GPT-4 outputs scored worse on ROUGE despite being preferred by humans, highlighting metric mismatch for open-ended idea generation.",
            "uuid": "e8028.3",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BERTScore (SciBERT)",
            "name_full": "BERTScore using SciBERT checkpoint",
            "brief_description": "A semantic similarity metric for text generation using contextual embeddings; paper uses SciBERT as encoder to compute BERTScore between generated and reference idea sentences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to outputs from GPT-4, GPT-3.5, T5",
            "model_size": "gpt-4-0314; davinci-003; T5-large",
            "scientific_domain": "Natural Language Processing / scientific text",
            "theory_type": "Automated semantic similarity evaluation",
            "evaluation_method_name": "BERTScore (SciBERT)",
            "evaluation_method_description": "Compute token-level contextual embedding similarities between generated and reference sentences using SciBERT checkpoint; aggregate to a score reflecting semantic overlap.",
            "evaluation_metric": "BERTScore (precision/recall/F1 aggregated; paper reports a single score)",
            "metric_definition": "Score typically ranges 0.0–1.0 (paper reports values like 0.581 etc.), higher = greater semantic similarity to reference.",
            "dataset_or_benchmark": "Challenging and gold subsets of SCIMON test data",
            "human_evaluation_details": "None (automated metric).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Table 9: e.g., GPT4ZS BERTScore 0.581 on challenging subset; T5 variants up to ~0.672.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "BERTScore favors semantic similarity to reference and can reward paraphrases or copying; may not reflect novelty or technical depth.",
            "uuid": "e8028.4",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BARTScore",
            "name_full": "BARTScore (generation evaluation by model likelihood)",
            "brief_description": "An automatic metric that scores generated text by computing conditional likelihood under a pretrained generation model (BART-style); mentioned as an automated metric used to compare generated sentences to ground truth.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Not directly used to generate theories (used for scoring)",
            "model_size": null,
            "scientific_domain": "NLP / text generation evaluation",
            "theory_type": "Automated evaluation metric",
            "evaluation_method_name": "BARTScore automated evaluation",
            "evaluation_method_description": "Use a pretrained seq2seq model to compute log-probability of reference given candidate (or vice versa) as a quality score.",
            "evaluation_metric": "BARTScore (log-probability based score)",
            "metric_definition": "Model log-probabilities or normalized scores; relative comparisons are used.",
            "dataset_or_benchmark": "Used as part of automated evaluation suite (paper mentions BARTScore alongside ROUGE and BERTScore)",
            "human_evaluation_details": "None (automated metric).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Mentioned qualitatively (no detailed numeric breakdown presented in main text beyond listing of automated metrics).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "BARTScore reflects model likelihood and can be biased by reference/model mismatch; not sufficient to assess novelty or technical depth.",
            "uuid": "e8028.5",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "In-context Contrastive Objective (InfoNCE)",
            "name_full": "In-context contrastive augmentation using an InfoNCE loss over decoder hidden states",
            "brief_description": "A training augmentation that adds in-context negative examples (sentences from the input context) and applies an InfoNCE contrastive loss alongside cross-entropy to discourage models from copying background text and encourage novelty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-large (fine-tuned variants)",
            "model_size": "T5-large",
            "scientific_domain": "Natural Language Processing",
            "theory_type": "Training objective to encourage novelty in generated hypotheses",
            "evaluation_method_name": "In-context contrastive training (InfoNCE)",
            "evaluation_method_description": "During fine-tuning, pick in-context negatives from input text; compute decoder hidden-state representations for positive target and negatives and apply InfoNCE-style loss combined with cross-entropy to push model representation away from in-context negatives.",
            "evaluation_metric": "Contrastive loss value (L_cl) during training; downstream effect measured by human and automated metrics (e.g., improved ROUGE/BERTScore and human novelty judgments).",
            "metric_definition": "InfoNCE-style loss normalized via temperature τ; no single external numeric scale beyond training loss and downstream metric improvements.",
            "dataset_or_benchmark": "Trained on SCIMON dataset derived from ACL Anthology (training split before 2021)",
            "human_evaluation_details": "Models with contrastive augmentation compared in human studies and automated metrics (reported improvements vs baseline fine-tuning).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "T5+SN+CL and other CL variants outperform baseline fine-tuned T5 on automated metrics (Table 9) and reduce copying behavior; human evaluations show improvement vs non-contrastive variants.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Contrastive negatives are sampled from input and may not capture all forms of copying; improved novelty is limited and does not achieve the depth of human-authored papers.",
            "uuid": "e8028.6",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Iterative Novelty Boosting (retrieve-compare-update)",
            "name_full": "Iterative retrieval–compare–update novelty boosting with γ_nov and threshold µ",
            "brief_description": "A method that iteratively retrieves similar prior literature ideas, scores similarity to generated idea I_t, and prompts the LLM to update the idea until similarity scores fall below a threshold µ, thereby encouraging novelty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to outputs from GPT-4 (GPT4FS variants) and T5",
            "model_size": "gpt-4-0314; T5-large",
            "scientific_domain": "Natural Language Processing (and applied in biochemical case study)",
            "theory_type": "Automated iterative refinement method to increase novelty of generated hypotheses",
            "evaluation_method_name": "Iterative novelty boosting via retrieval and negative example prompting",
            "evaluation_method_description": "Generate initial idea I0, retrieve top-k nearest literature ideas from reference corpus R using SentenceBERT (k=20), compute cosine similarities S_i; if any S_i &gt;= µ (µ=0.6), provide retrieved literature sentences as negatives and instruct the model to produce a more novel idea; repeat until all S_i &lt; µ.",
            "evaluation_metric": "Novelty delta percentages (human-judged) and number of new terms added; similarity scores used internally (cosine similarity).",
            "metric_definition": "Cosine similarity (0–1) between embeddings; threshold µ = 0.6 used to trigger iterations. Human-reported % increases in novelty (e.g., +54.4% first-iteration novelty for best variant in Table 3 entries).",
            "dataset_or_benchmark": "Reference corpus R built from training set (papers before 2021); retrieval over ~59k papers / 374k sentences in NLP experiments.",
            "human_evaluation_details": "Study III had 5 annotators on 70 instances evaluating whether updated ideas were different and more novel; iterative improvement recorded.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Iterative method substantially increases novelty where applied: e.g., first-iteration novelty deltas reported (e.g., +54.4% for GPT4FS+SN+CT+KG in Table 3); SN variant: 88.9% of updated ideas were substantially different and 55.6% judged more novel after first iteration; second iteration increased novelty for 57.8% of ideas that continued.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Novelty often superficial (recombinations of popular concepts), limited by retrieval quality and embedding similarity proxy; retrieval and similarity threshold choices affect behavior.",
            "uuid": "e8028.7",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Similarity Retrieval (SentenceBERT all-mpnet-base-v2)",
            "name_full": "Semantic retrieval using SentenceBERT (all-mpnet-base-v2)",
            "brief_description": "Sentence embeddings from SentenceBERT (all-mpnet-base-v2) used for multiple retrieval tasks: semantic neighbors, citation neighbor selection, and retrieving nearest ideas for novelty checking, with cosine similarity as the distance metric.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SentenceBERT (all-mpnet-base-v2) for retrieval; used alongside LLMs",
            "model_size": "all-mpnet-base-v2 (SentenceBERT checkpoint)",
            "scientific_domain": "NLP (retrieval over scientific text)",
            "theory_type": "Retrieval/semantic similarity component used to find inspirations and prior ideas",
            "evaluation_method_name": "Embedding-based semantic retrieval with cosine similarity",
            "evaluation_method_description": "Represent queries and candidate texts with SentenceBERT embeddings and compute cosine similarity to retrieve top-k neighbors (k varies: semantic neighbors up to 20, novelty retrieval k=20, citation neighbors up to 5).",
            "evaluation_metric": "Cosine similarity scores; top-k retrieval counts",
            "metric_definition": "Cosine similarity in [0,1]; specific thresholds used (e.g., µ=0.6 for novelty retrieval); gold-split selection used cosine similarity ≤ 0.074 between background and ground truth.",
            "dataset_or_benchmark": "Training corpus and reference corpus built from ACL Anthology (59k papers, 374k sentences) and citation title sets (87k titles).",
            "human_evaluation_details": "Retrieval outputs used as inspirations for generation; human studies evaluated downstream quality improvements.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Retrieval supports the SCIMON pipeline; novelty boosting relies on SentenceBERT similarities. Gold subset created by filtering background/ground-truth pairs with cosine similarity ≤ 0.074 (10th percentile).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Quality depends on embedding model; retrieval noise and imperfect IE upstream limit effectiveness; threshold choices are heuristic.",
            "uuid": "e8028.8",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Dataset: ACL Anthology via S2ORC",
            "name_full": "ACL Anthology corpus extracted via S2ORC (Semantic Scholar Open Research Corpus)",
            "brief_description": "Primary NLP domain corpus used to construct the SCIMON dataset: 67,408 ACL Anthology papers (1952–2022) filtered for English with abstracts; used to extract background sentences, target idea sentences, seed terms, and to build the reference corpora and KG.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLMs trained/evaluated on data derived from this corpus (T5-large fine-tuned, GPT evaluations held-out)",
            "model_size": "T5-large for fine-tuning; GPT-4/GPT-3.5 used for inference",
            "scientific_domain": "Natural Language Processing",
            "theory_type": "Corpus for generating and evaluating idea/hypothesis sentences",
            "evaluation_method_name": "Dataset for training/evaluation (temporal split and gold subset)",
            "evaluation_method_description": "Constructed instances by IE pipeline: background sentences (via scientific sentence classification), target idea sentences, and seed terms via scientific IE; split temporally into train/dev/test (&lt;2021 / 2021 / 2022) and created a manually vetted gold subset (194 instances).",
            "evaluation_metric": "Dataset statistics (counts): 67,408 total papers; training/dev/test splits as in paper; gold subset size 194 instances used for high-quality evaluation.",
            "metric_definition": "Counts of papers and instances; similarity threshold for gold set selection (cosine similarity ≤ 0.074).",
            "dataset_or_benchmark": "ACL Anthology (S2ORC) derived SCIMON dataset; gold subset of 194 challenging instances",
            "human_evaluation_details": "Gold subset manually vetted (excluded trivial overlaps) and used in human studies (Study I–III).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Dataset split: ~58,874 papers before 2021 (train), 5,946 from 2021 (dev), 2,588 from 2022 (test); gold subset size 194 after filtering.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "IE preprocessing noisy (relation extraction precision ~65%); overall instance pass rate after preprocessing 79.7%; dataset limited to English abstracts and ACL Anthology coverage.",
            "uuid": "e8028.9",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Dataset: PubMed / Biochemical Case Study",
            "name_full": "PubMed-derived biochemical dataset (PubTator 3 based)",
            "brief_description": "A biochemical-domain dataset constructed similarly to the NLP pipeline by collecting PubMed abstracts, extracting entities and relations with PubTator 3, and selecting background/target sentences for domain generalization experiments and evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meditron-7b (biomedical LLM) fine-tuned variants; also used to evaluate transfer/generalization",
            "model_size": "Meditron-7b (epfl-llm/meditron-7b)",
            "scientific_domain": "Biochemistry / biomedical",
            "theory_type": "Idea / hypothesis sentence generation in biochemical domain",
            "evaluation_method_name": "Domain generalization case study human evaluation",
            "evaluation_method_description": "Collect PubMed papers (1988–2024) on selected topics, extract entities/relations with PubTator 3, classify sentences, fine-tune biomedical LLM and evaluate on held-out test split past model pretraining cutoff; have domain experts rate generated directions.",
            "evaluation_metric": "Percent positive human ratings (human experts)",
            "metric_definition": "Percentage of generated directions rated positively by 2 biochemical domain experts (0–100%).",
            "dataset_or_benchmark": "PubMed-derived dataset (counts reported: e.g., final dataset counts: 4,767 papers before 2023/02, 642 papers 2023/02–2023/08, 299 after 2023/08; elsewhere 5,708 PubMed papers collected overall).",
            "human_evaluation_details": "2 biochemical domain experts with graduate-level education evaluated outputs; they rated ~80% of generated directions positively in this initial experiment.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Domain experts rated ~80% of generated directions positively; evaluators in this small study were sometimes more satisfied with generated outputs than ground truth regarding technical detail (contrasting the NLP findings).",
            "comparison_to_human_generated": true,
            "comparison_results": "In this preliminary biochemical case study, evaluators in some cases rated generated outputs more favorably than ground-truth, though the experiment was small and intended as a generality check.",
            "limitations_noted": "Preliminary small-scale study; domain experts limited in number (2); dataset and evaluation not exhaustive; PubMed extraction dependent on PubTator quality.",
            "uuid": "e8028.10",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Gold Subset Selection Criterion",
            "name_full": "Gold test subset selection using cosine similarity thresholding and manual vetting",
            "brief_description": "Procedure to create a high-quality, challenging evaluation subset by selecting background/ground-truth pairs with low surface similarity (cosine similarity ≤ 0.074, the 10th percentile) and manual annotation to remove trivial or irrelevant pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used to evaluate all LLM variants (GPT-4, GPT-3.5, T5, Meditron)",
            "model_size": "gpt-4-0314; davinci-003; T5-large; meditron-7b",
            "scientific_domain": "NLP (evaluation subset)",
            "theory_type": "Evaluation data curation criterion",
            "evaluation_method_name": "Gold subset selection (low similarity + manual filtering)",
            "evaluation_method_description": "Compute cosine similarity between background and corresponding ground-truth sentence for test pairs; select pairs with similarity ≤ 0.074 (10th percentile) and manually remove trivial overlaps, irrelevant backgrounds, or instances lacking salient target relation, yielding 194 high-quality instances.",
            "evaluation_metric": "Cosine similarity cutoff and final gold subset count",
            "metric_definition": "Cosine similarity (SentenceBERT embedding) threshold ≤ 0.074; resulting gold subset size = 194 instances.",
            "dataset_or_benchmark": "SCIMON test split derived from ACL Anthology",
            "human_evaluation_details": "Manual vetting by annotators to ensure challenging, non-trivial pairs used for human studies.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Gold subset contains 194 instances selected via the described procedure.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Choice of cosine similarity cutoff is heuristic; manual vetting subjective; gold subset small relative to full test set.",
            "uuid": "e8028.11",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Preprocessing Quality Criteria / IE Precision",
            "name_full": "Information extraction preprocessing quality metrics and conservative filtering",
            "brief_description": "Retention criteria and measured precisions for the IE pipeline components (entity/relation extraction, coreference, abbreviation resolution, sentence classification) used to build training/evaluation data; only high-confidence IE outputs were kept.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PL-Marker (scientific IE), SciCo (coreference), ScispaCy (abbreviation resolution), Sci-sentence classifier",
            "model_size": null,
            "scientific_domain": "NLP dataset curation / IE",
            "theory_type": "Data quality / preprocessing criteria for evaluation validity",
            "evaluation_method_name": "IE preprocessing precision and conservative filtering",
            "evaluation_method_description": "Evaluate precision of each IE preprocessing step via manual sampling; keep only high-confidence outputs to reduce noise; report per-step precision and overall pass rate.",
            "evaluation_metric": "Per-step precision percentages and overall pass rate",
            "metric_definition": "Reported precisions: most steps 91%–100% precision except relation extraction ~65%; overall rate of instances passing all steps = 79.7%.",
            "dataset_or_benchmark": "SCIMON dataset derived from ACL Anthology",
            "human_evaluation_details": "Manual evaluation of random sample to estimate IE precision; final gold subset further manually vetted.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "IE preprocessing step precisions: 91%–100% for most steps, relation extraction 65%; combined pass rate 79.7%.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Relation extraction is noisier (65%); dataset quality depends on IE reliability and conservative filtering may remove useful examples.",
            "uuid": "e8028.12",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ROUGE: A package for automatic evaluation of summaries",
            "rating": 2
        },
        {
            "paper_title": "BERTScore: Evaluating text generation with BERT",
            "rating": 2
        },
        {
            "paper_title": "BARTScore: Evaluating generated text as text generation",
            "rating": 2
        },
        {
            "paper_title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
            "rating": 2
        },
        {
            "paper_title": "ScispaCy: Fast and robust models for biomedical natural language processing",
            "rating": 1
        },
        {
            "paper_title": "Scico: Hierarchical cross-document coreference for scientific concepts",
            "rating": 1
        },
        {
            "paper_title": "PL-Marker (Packed levitated marker) for entity and relation extraction",
            "rating": 1
        },
        {
            "paper_title": "PubTator 3",
            "rating": 2
        },
        {
            "paper_title": "S2ORC: The semantic scholar open research corpus",
            "rating": 2
        },
        {
            "paper_title": "Meditron-70b: Scaling medical pretraining for large language models",
            "rating": 1
        }
    ],
    "cost": 0.0215705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scientific Inspiration Machines Optimized for Novelty
3 Jun 2024</p>
<p>Qingyun Wang qingyun4@illinois.edu 
University of Illinois at Urbana-Champaign</p>
<p>Doug Downey 
Allen Institute for Artificial Intelligence (AI2)</p>
<p>Heng Ji hengji@illinois.edu 
University of Illinois at Urbana-Champaign</p>
<p>Tom Hope tomh@allenai.org 
Allen Institute for Artificial Intelligence (AI2)</p>
<p>The Hebrew University of Jerusalem</p>
<p>Scientific Inspiration Machines Optimized for Novelty
3 Jun 20248AFCF73D6229DB6E6028F735BDF4E3F3arXiv:2305.14259v7[cs.CL]
We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature.Work on literature-based hypothesis generation has traditionally focused on binary link predictionseverely limiting the expressivity of hypotheses.This line of work also does not focus on optimizing novelty.We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature.We present SCIMON, a modeling framework that uses retrieval of "inspirations" from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved.Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue.Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature 1 .</p>
<p>Introduction</p>
<p>Can machines mine scientific papers and learn to suggest new directions?The idea that information from the literature can be used for automatically generating hypotheses has been around for decades (Swanson, 1986).To date, the focus has been on a specific setting: hypothesizing links between pairs of concepts (often in drug discovery applications (Henry and McInnes, 2017), e.g., new drug-disease links), where concepts are obtained from papers or knowledge bases previously derived from papers (Sybrandt et al., 2020;Nadkarni et al., 2021).</p>
<p>This common setting has fundamental drawbacks.Reducing the "language of scientific ideas" (Hope et al., 2023) to this simplistic form limits the expressivity of the hypotheses we can hope to generate, and does not capture nuanced contexts that scientists consider: target application settings, requirements and constraints, motivations and challenges.In light of the strong progress recently made with large language models (LLMs), in this paper we explore a dramatically different setting: models that take descriptions of problem contextsand return natural language suggestions of novel scientific directions that are grounded in literature.</p>
<p>We develop a framework named SCIMON (Scientific Inspiration Machines with Optimization for Novelty), named after Nobel laureate and AI pioneer Herbert Simon who authored early foundational work on automated scientific discovery (Newell and Simon, 1956;Simon, 1973).We first present an automated data collection methodology that collects examples of past problems and proposed ideas from scientific papers.We then use this data for both fine-tuning and in-context training of LLMs-training them to take problem descriptions and output proposed ideas to address them.We observe that state-of-art LLMs (e.g., GPT-4 (OpenAI, 2023)) struggle with generating novel scientific ideas, and contribute a new modeling framework for generating hypotheses that makes progress in improving the hypothesis generation ability of LLMs (Figure 1).Given a background problem description, models first dynamically retrieve inspirations from past literature in the form of related problems and their solutions along with contexts from a scientific knowledge graph.These retrieved inspirations serve to ground the generated ideas in existing literature.We then endow models with the ability to iteratively boost the novelty of generated ideas.Given an idea I generated by the LLM at step t, the model compares I with existing research in the literature; if it finds strongly overlapping research, the model is tasked with updating its idea to be more novel relative to prior work (much like a good researcher would do).We also introduce an in-context contrastive model which encourages novelty with respect to background context.</p>
<p>We perform the first comprehensive evaluation of language models for generating scientific ideas in our new generative, contextual setting.We focus on AI/NLP ideas to facilitate analysis by AI researchers themselves, and also demonstrate generalization to the biomedical domain.We design extensive evaluation experiments using human annotators with domain expertise to assess relevance, utility, novelty, and technical depth.Our methods substantially improve the ability of LLMs in our task; however, analyses show that ideas still fall far behind scientific papers in terms of novelty, depth and utility-raising fundamental challenges toward building models that generate scientific ideas.</p>
<p>Background and New Setting</p>
<p>We begin with a brief description of related work and background.We then present our novel setting.</p>
<p>Literature-based discovery Nearly four decades have passed since Don Swanson pioneered Literature-Based Discovery (LBD), based on the premise that the literature can be used for generating hypotheses (Swanson, 1986).LBD has been focused on a very specific, narrow type of hypothesis: links between pairs of concepts (often drugs/diseases).The classic formalization of LBD goes back to Swanson (1986) who proposed the "ABC" model where two concepts (terms) A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers.More recent work has used word vectors (Tshitoyan et al., 2019) or link prediction models (Wang et al., 2019;Sybrandt et al., 2020;Xu et al., 2023) to discover scientific hypotheses as pairwise links between concepts.A tightly related body of research focuses on scientific knowledge graph link prediction (Nadkarni et al., 2021), where predicted links may correspond to new hypotheses, and knowledge bases are reflections of existing scientific knowledge in specific domains, derived from literature.A fundamental gap in this line of work is in the lack of approaches for modeling nuanced contexts (Sosa and Altman, 2022) (e.g., the specific settings in which a drug may be relevant for a disease) for generating ideas in open-ended problem settings with unbounded hypothesis spaces, and for optimizing novelty.Our setting can be viewed as a radical departure addressing the limitations in existing settings.</p>
<p>LLMs for Scientific Innovation Large language models (LLMs) have made remarkable progress in interpreting and producing natural language content and handling knowledge-intensive tasks such as in the medical domain (Nori et al., 2023).Very recent work (Boiko et al., 2023) has explored the use of LLMs in a robotic chemistry lab setting, planning chemical syntheses of known compounds and executing experiments.Robotic lab settings are inherently limited to narrow sub-areas where such experiments are possible and relevant.Other very recent work (Huang et al., 2023) used LLMs to produce code for machine learning tasks such as Kaggle competitions, finding that a GPT-4 agent achieved 0% accuracy on research challenges such as BabyLM (Warstadt et al., 2023).GPT-4 has been anecdotally reported as having "strengths less like those of having a human co-author, and more like a mathematician working with a calculator" (Carlini, 2023).Our goal is to conduct a non-anecdotal evaluation and enhancement of strong LLMs' ability to generate novel open-ended scientific ideas.</p>
<p>SCIMON Problem Setting</p>
<p>We are motivated by imagining an AI-based assistant that suggests ideas in natural language.The assistant takes as input background context B consisting of (1) current problems, motivations, experimental settings and constraints, denoted as M; and optionally (2) a seed term v that should be a focus point of the generated idea I.The seed term is motivated by considering a user-provided cue for the model to limit its hypothesis space.Importantly, generated ideas should not merely paraphrase the background-the output should be novel with respect to B and the broader literature corpus.text that describes problems with "pretrained language models" in the lifelong integration of information sources, including computational costs.The assistant aims to generate an idea for performing "knowledge acquisition" within this context.Given this input, we aim to generate a full sentence describing a novel idea.</p>
<p>Automated Training Data Collection</p>
<p>We obtain training data derived from papers with scientific information extraction (IE) modelsextracting past examples of background sentences and corresponding ideas (e.g., descriptions of methods used for specific problems in the background sentences), along with salient entities as seed terms.This data is used for training in both in-context learning and fine-tuning setups.</p>
<p>We construct a corpus D from 67,408 ACL Anthology papers from S2ORC (Lo et al., 2020) (we later also conduct an experiment with a biomedical corpus §4.1).Given a title and the corresponding abstract from a document d, to select problem/motivation sentences M we first perform sci-entific sentence classification (Cohan et al., 2019) to classify sentences from the abstract into one of {Background, Method, Objective}, selecting sentences with labels of Background and treating the remaining sentences as target sentences T which will serve as desired output examples (Figure 3).</p>
<p>For seed term selection, we apply a state-ofthe-art scientific IE system (Ye et al., 2022) to T to extract entities corresponding to Task, Method, Evaluation Metric, Material, and relations of the form [method,used-for,task]-mentions of methods and the tasks they are used for, materials used for tasks, etc.We treat the head (e.g., method) or tail (e.g., task) entity as the seed term, and name the other entity (tail/head, respectively) as a target term t ∈ T .Continuing our example from Figure 2, Figure 3 shows how the seed and target terms ("knowledge acquisition" and "function preserved model expansion") are extracted from T .During training, each instance contains (B,T ) pairs; during evaluation, target information is removed.</p>
<p>We use SciCo (Cattan et al., 2021) to obtain coreference links for entity normalization, and use ScispaCy (Neumann et al., 2019) to replace abbreviations with a more informative long form.We also collect paper metadata, including the citation network G c .We split our dataset temporally (train/dev/test correspond to papers from years &lt;2021 / 2021 / 2022 respectively).For our experiments, we used model checkpoints trained on data preceding 2022, avoiding the risk of data contamination ( §6).Table 1 shows data statistics. 2uality of IE Preprocessing During preprocessing, we only keep high-confidence outputs from IE models to reduce errors.We observe this removes many of the noisy cases.To validate this, we manually evaluate the precision of each preprocessing step on a random sample of papers and observe that all steps yield high precision (91%-100%) except relation extraction (65%); in total, the rate of instances passing all steps was 79.7%. 3   Gold Test Set We create a high-quality, clean test set.We remove test instances where models can trivially use surface-level background information to infer the ground truth to create a more challenging set, selecting instances with low similarity between background and ground truth sentences.We compute the cosine similarity between each instance's background and corresponding ground truth sentence in the test set and take pairs with similarity ≤ 0.074, which amounts to the tenth percentile of pairs.We further annotate this subset to create a gold subset.We manually exclude instances with trivial overlap between ground truth and background, remove cases with irrelevant background, and retain only instances where the target relation (from which the seed term is taken) is salient to the target sentence.We also remove test pairs that have unexplained terms in the background.We obtain a total of 194 instances.</p>
<p>SCIMON Models</p>
<p>We present a new module to retrieve inspirations as contextual input ( §3.1).Then, we describe another module to generate ideas given the con-text+inspiration ( §3.2).Finally, we introduce a new iterative novelty optimization method to further improve idea quality ( §3.3). 5</p>
<p>3 See Table 6 in Appendix. 4Full annotation details are in Appendix C. 5 Training and hyperparameter details in Appendix B.</p>
<p>Inspiration Retrieval Module</p>
<p>We take broad inspiration from cognitive aspects of innovation (Hope et al., 2023): when researchers generate a new idea, they are grounded in a web of existing concepts and papers bearing on the new idea.We aim to enrich the context of each background by retrieving "inspirations"-pieces of information that can guide hypothesis generation.As illustrated in Figure 2, for a given instance of the SCIMON task, our retrieval augmentation can retrieve from three types of sources.Each source uses a different form of query and output.</p>
<p>Semantic Neighbors For a given problem/motivation as input, ideas proposed for related problems in the training set can serve as a guiding reference for generating a new idea.Given the background context B with a seed term v and problem/motivation M, we construct a base input b: a concatenation of M with a prompt P belonging to one of two templates: "v is used for p" or "v is done by using p", where p is one of Task/Method/Material/Metric.In short, b := P ⊕ context:M.For example, in Figure 2, the concatenation is "Knowledge acquisition is done by using Method; Context:...requires plms to integrate information...lifelong manner...".</p>
<p>We then retrieve inputs from the training set that are semantically related to a new base input b, and obtain target sentences T corresponding to each retrieved training input.We extract the target term t ∈ T matching the seed term in b ( §2.2) as inspiration for input b.Simply put, this means we use as inspiration the salient aspect of the solution proposed in T , which we found empirically to help remove noisy/irrelevant information in T .For example, in Figure 2, we find "informative entities are done by using Method context: in this work, we aim at equipping pre-trained language models with structured knowledge."as similar to the input and use t ="linked knowledge graph" as inspiration.</p>
<p>Technically, we first construct a fully connected graph G S based on the training set where each node is a pair of input text b i and target term t i .We define the weight between two nodes i and j as the cosine similarity between b i and b j based on representations from SentenceBERT (Reimers and Gurevych, 2019) (all-mpnet-base-v2).Given b, we first insert it into G S and compute the weights of its connected edges.We then retrieve neighbors input text {b 1 , . . ., b k } from the training set with the largest edge weight, where k is the number of retrieved instances.We consider the corresponding target terms {t 1 , . . ., t k } as semantic inspirations.</p>
<p>KG Neighbors</p>
<p>We also explore enriching the context by linking it to a background KG with information on related methods and tasks.Using the same IE process used to extract our training examples ( §2.2), we create a global background KG G B which covers all papers in the corpus D Y prior to a given year Y (i.e., the nodes in G B correspond to tasks/methods/materials/metrics, and the edges are used-for relations, extracted and normalized from across the entire corpus as described earlier).Then, given a seed term v at query time, we select adjacent nodes {n 1 , n 2 , ...} from G B as inspirations.</p>
<p>As an example, in Figure 2, the neighbor nodes of "knowledge acquisition" include "collaborative web text annotation editor", "image matching", etc., which we select as inspirations.</p>
<p>Citation Neighbors Another notion of contextual relatedness we explore is via citation graph links.Here, given as input background context B, we assume access to the original source document d from which B was extracted, and consider its cited paper title set C d as potential candidates.This can be seen as a stronger assumption on information available to the model-assuming a researcher using the model provides relevant candidate documents from which ideas could be pooled.Because the training set only contains papers before year Y, we only select papers C dY ⊆ C d prior to year Y.</p>
<p>We then retrieve the top-k titles with the highest cosine similarity to d from C dY based on their Sen-tenceBERT embeddings as earlier.For instance, in Figure 2, the paper ELLE (Qin et al., 2022) cites the paper (de Masson d' Autume et al., 2019).Therefore, we choose the title "episodic memory in lifelong language learning" as inspiration information.</p>
<p>Generation Module</p>
<p>The idea generation module is given retrieved inspirations i 1 , . . ., i k along with context M as input.</p>
<p>In-Context</p>
<p>Learning We experiment with recent state-of-the-art LLMs, GPT3.5 davinci-003 (Ouyang et al., 2022) andGPT4 gpt-4-0314 checkpoint (OpenAI, 2023).We first ask the model to generate sentences based on the seed term and the context in the zero-shot setting without any in-context examples (GPT3.5ZS,GPT4ZS).We then ask the model to generate sentences in a few-shot setting by prompting randomly chosen pairs of input and output from the training set (GPT3.5FS, GPT4FS).Inspired by Liu et al. (2022), we further employ a few-shot setting using semantically similar examples.Instead of random in-context examples, we use the top-k examples from the training set with the highest cosine similarity to the query (GPT3.5Retr).This few-shot retrieval setting differs from the semantic neighbor discussed above, in that we provide both the input and output of each instance rather than solely supplying target entities as additional input.</p>
<p>Fine Tuning We fine-tune T5 (Raffel et al., 2020) (more recent models may be used too; see our biomedical experiment §4.1 fine-tuning an LLM).We observe that the generation models tend to copy phrases from the background context.For example, given the context "...hierarchical tables challenge numerical reasoning ...", the model will generate "hierarchical table reasoning for question answering" as the top prediction.For generating suggestions of novel ideas, we wish to discourage overly copying from the background context.We introduce a new in-context contrastive objective, where negative examples are taken from the text in the input (e.g., in Figure 2, the in-context negatives are plms, pretraining, etc).We compute an InfoNCE loss (Oord et al., 2018) over the hidden states of the decoder, aiming to maximize the probability of the ground truth against those of in-context negatives:
y + = σ(Avg(W y h + + b y )) y − k = σ(Avg(W y h − k + b y )) L cl = exp (y + /τ ) k exp y − k /τ + exp (y + /τ )(1)
where h + and h − k are decoder hidden states from the positive and k-th negative samples, W y and b y are learnable parameters, σ is a sigmoid function, τ is a temperature hyperparameter, and Avg( * ) denotes the average pooling function based on the target sequence length.We optimize with both contrastive loss L cl and the cross-entropy loss.</p>
<p>Iterative Novelty Boosting with Retrieval</p>
<p>We further improve the novelty of generated ideas with a new iterative retrieve-compare-update scheme.Conceptually, we consider a noveltyinducing penalty γ nov (I, R) that penalizes ideas I that are too "close" to existing ideas in literature reference examples R. γ nov (I, R) is included during in-context learning and inference, providing numerical feedback in the form of a score reflecting similarity to existing work.We wish to minimize this score while ensuring I remains relevant to the background context B; we do so iteratively by (1) retrieving related work from R, (2) measuring degree of novelty, (3) instructing the model to update I to be more novel w.r.t R, conditioning on B.</p>
<p>Specifically, in our implementation, we construct a reference corpus R based on all papers in the training set.We then propose an iterative algorithm that compares generated ideas against R. We start with the initial idea I 0 generated by the generation module.At each time step t, we use the generated idea I t as a query to retrieve k nearest ideas from the literature reference corpus R = {R 1 , ..., R k } based on SentenceBERT, with the top-k highest cosine similarity scores to I t (we use k = 20).For each retrieved ground truth literature idea R i , we compare its cosine similarity score S i against a threshold µ (we use 0.6).We provide all the retrieved ground truth ideas R that pass the threshold as additional negative examples for the large language models with the following instruction prompt: "Your idea has similarities with existing research as demonstrated by these j sentences: R Make sure the idea you suggest is significantly different from the existing research mentioned in the above sentences.Let's give it another try."We stop the iteration once all S i are lower than µ. Figure 2 and Table 5 demonstrate novelty iterations.</p>
<p>Experiments</p>
<p>Human Evaluation</p>
<p>We present four human evaluation studies, exploring different facets of our problem and approach.</p>
<p>Study I: Comparing Outputs across Model Variants</p>
<p>We recruit six volunteer NLP experts with graduatelevel education to rate the system.Raters are told to envision an AI assistant that suggests new paper ideas.We randomly select 50 instances (back-ground+seed) from the gold subset.Each annotator receives ten instances, each paired with system outputs from different model variants (Table 2).We ask raters to assess idea quality by considering each output's relevance to the context, novelty, clarity, and whether the idea is reasonable (positive ratings are dubbed "helpful" as shorthand, indicating they pass the multiple considerations).We observe moderately high rater agreement. 6Raters are blind to the condition, and system outputs are randomly shuffled across instances.We instruct annotators to only provide positive ratings to ideas sufficiently different from the input context.In Study I, we ask raters not to anticipate groundbreaking novelty from the system but rather a narrower expectation of quality and utility; in Study II below, we enrich the analysis to examine ranking between top models and also "raise the bar" and compare to actual ideas from papers. 7n a preliminary experiment, we also collected human ratings for GPT4-ZS (zero-shot) vs. GPT4-FS (few-shot) using the same criteria, finding GPT4-FS ranked higher in 65% of cases, with the rest mostly tied; thus, zero-shot GPT-4 was left out of the remainder of study I and subsequent studies to reduce annotation effort and cost.</p>
<p>Results Overall, GPT4FS and GPT4FS+KG outperform other models by a wide margin (Table 2).Apart from GPT4, T5+SN+CL performs best compared to other baselines, given its stronger prior knowledge of useful similar background hypotheses.In general, GPT3.5 models performed worse than fine-tuned T5 and its variants, which echoes results in other work in the scientific NLP domain (Jimenez Gutierrez et al., 2022).GPT4 outputs tended to be longer, which may partially explain higher human preference.Table 2: Percent (%) of total votes each system output receives from human raters.H denotes a helpful output, while U denotes an unhelpful output."3FS" refers to the GPT3.5FS."3Rt" refers to the GPT3.5Retr."4" refers to GPT4FS, and "4+KG" refers to the GPT4FS+KG."T5+SN" refers to the T5+SN+CL.GPT4FS and GPT4FS+KG are rated much higher.While GPT4FS has a slightly higher rating than the KG variant, a further human study reveals that GPT4FS+KG often leads to more technical depth ( §4.1).</p>
<p>Study II: Comparing GPT4 Variants against Real Papers</p>
<p>We conduct a follow-up human study of close competitors GPT4FS and GPT4FS+KG with a subset of the annotators to evaluate the incrementality and novelty of the generated ideas.In this study, model outputs are now ranked, unlike the binary classification of helpful/not in Study I. Suggestions are ranked according to the level of technical detail and innovation in comparison to each other-i.e., ranking which of GPT4FS and GPT4FS+KG had a higher degree of technical detail and novelty, or whether they are roughly the same (tied).Finally, outputs are rated versus the ground truth idea, according to whether or not the suggestions were roughly at the same level of technical detail and innovation as the original paper's idea, or significantly lower.</p>
<p>Results Overall, GPT4FS+KG is found to have higher technical detail in 48% of the compared pairs, and found to be less incremental (more novel) in 45% of the pairs.Among the remaining 52%/55% (respectively), the vast majority are ties, indicating that whenever GPT4FS+KG is not favored, it is of roughly the same quality as GPT4FS, but not vice versa.However, the most crucial aspect is comparing the results against the original ground truth idea on the quality of innovation.Here, we find that in 85% of comparisons, the ground truth is considered to have significantly higher technical level and novelty; and in the remaining 15%, the ground truth was ambiguous or lacking additional context from the paper abstract.This points to a major challenge in obtaining high-quality idea generations using existing state-of-the-art models.</p>
<p>Study III: Evaluation on Iterative Novelty Boosting</p>
<p>We conduct a fine-grained evaluation of our novelty mechanism with qualitative and quantitative evaluation of novelty.Specifically, we ask five annotators to further compare the novelty-enhanced results against the initially generated ideas.We randomly select 70 instances (background+seed) from the sentence generation gold subset.We ask annotators to check whether the new ideas are different than the initial ideas (e.g., adding new information or approaches), and whether they are more novel (i.e., a new idea can be different, but not necessarily more novel).Since GPT4FS+SN outperforms other models, for this model, we further instruct annotators to compare the novelty of the second iteration results against the first iteration results.</p>
<p>Results</p>
<p>For SN, in the first iteration 88.9% of updated ideas are substantially different from initial ideas, and for 55.6% we are able to increase nov- Ideas after novelty iterations are longer than initial ideas.We examine the new terms added after filtering 359 words, including stopwords, as many generic words and terms are often added (e.g., "novel model/method/approach").While our method helps boost novelty, overall the model often tends to suggest combinations between popular concepts ( §4.2).Novelty boosting seemed to often focus on adding dynamic/adaptive modeling, graph models and representations, the fusion of multiple modalities and sources-and sometimes all at once (e.g., "Dynamic Syntax-Aware Graph Fusion Networks (DSAGFN)"), and to explicitly compare against existing ideas from literature (Table 5).Table 4: Human evaluations results of each system output for the idea sentence prediction task on Biomedical Domain."vs.GT" refers to percents which system outputs are better than ground truth ideas.</p>
<p>Domain Generalization Case Study</p>
<p>Our domain-agnostic framework can be applied to other domains by changing the IE system used in the preprocessing procedure.To demonstrate A novel method called Adaptive Speech Unit Boundary Detection (ASUBD) ... a combination of attention mechanisms to focus on relevant acoustic and linguistic features and reinforcement learning to guide the system to make optimal predictions of unit boundaries based on previous decisions... Ground Truth ... an efficient monotonic segmentation module ... accumulate acoustic information incrementally and detect proper speech unit boundaries.</p>
<p>Table 5: Example of iterative novelty iterations.Our novelty iteration method enhances ideas overall; however ideas are often based on superficial recombinations of common concepts, far from the technical depth of scientific papers.</p>
<p>this, we conduct an additional initial experiment in the biochemical domain.We follow a similar data creation procedure as for NLP papers.We collect a dataset from PubMed papers and use PubTator 3 (Islamaj et al., 2021;Wei et al., 2022;Luo et al., 2023;Wei et al., 2023;Lai et al., 2023) as an IE system to extract a KG from paper abstracts.We use a sentence classifier trained on annotated abstracts (Huang et al., 2020) to select background context.We fine-tune a state-of-the-art biomedical large language model (Chen et al., 2023) on our data and evaluate on a test split past its pre-training cutoff date. 8We ask two biochemical domain experts with graduate-level education to evaluate the quality of the results as before, finding them to overall rate 80% of the generated directions positively.Finally, in contrast to NLP-domain experiments, evaluators were more satisfied with the generated outputs than the ground truth regarding technical detail.Detailed results are in Table 4.However, this preliminary experiment was meant mainly to demonstrate the generality of our approach, and a more in-depth exploration of utility and quality is left for future work.</p>
<p>Error Analysis</p>
<p>Models often made generic suggestions, woven together with specific details copied directly from the context (e.g., "NLP with ML algorithms and sentiment analysis" for some problem X, or "data augmentation and transfer learning" for Y, or "BERT or RoBERTa" for Z).Our techniques reduced this behavior but did not fully solve it.GPT4 models, especially, seemed to generate generic descriptions of common steps in NLP workflows (e.g., "Data preprocessing: Clean the text data, remove unnec-8 More data and training details in Appendix A.2, B.2.3.essary characters, perform tokenization...").All models often copied and rephrased directly from the context.In certain cases, models applied simple logical modifications to the context; e.g., when contexts described problems such as "high latency" or "efficiency limitations", the suggestions would include phrases such as "low latency" or "highly efficient".</p>
<p>Automated Evaluation Analysis</p>
<p>In open-ended tasks such as ours, automatic evaluations comparing system output to ground truth texts may be limited.Nonetheless, automated metrics such as ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020) and BARTScore (Yuan et al., 2021), that check the similarity between ground truth and generated output, may surface interesting findings.We find GPT-based models to be outperformed by T5-based models; GPT4 outputs are much longer than T5, explaining why they underperform in automatic metrics but outperform in human evaluations ( §4.1).Generated sentences often follow certain templates (e.g., "In this paper, we propose a new ... for ..."), which also helps explain why T5 fine-tuned on many examples scores higher superficially.At the same time, our in-context contrastive examples which encourage novelty with respect to background context, helped models perform better than baseline fine-tuning by reducing reliance on copying.See results in Table 9 (Appendix B.4).</p>
<p>Conclusions and Future Directions</p>
<p>We propose a new setting, model and comprehensive evaluation for scientific hypothesis generation with language models that are grounded in literature and optimized for novelty.We present a new framework named SCIMON in which mod-els take background problem contexts and provide suggestions that are novel while based on literature.Models retrieve inspirations from semantic similarity graphs, knowledge graphs, and citation networks.We introduce a new iterative novelty boosting mechanism that helps large language models (LLMs) such as GPT-4 generate more novel ideas by explicitly comparing ideas to prior work and refining them.Our experiments demonstrate that the task of generating natural language scientific hypotheses is highly challenging.While our methods improve upon baseline LLMs, generated ideas tend to be incremental and with insufficient detail.Generating novel and meaningful scientific concepts and their compositions remains a fundamental problem (Hope et al., 2023).Evaluation in this setting is also highly challenging, with a huge space of potentially plausible hypotheses formulated in natural language.One interesting direction is to expand SCIMON with a multimodal analysis of formulas, tables, and figures to provide a more comprehensive background context.</p>
<p>Limitations</p>
<p>We discuss limitations extensively throughout the paper, such as in terms of evaluation challenges and data quality.Here we include additional details on limitations.</p>
<p>Limitations of Data Collection</p>
<p>We crawled papers with Semantic Scholar Academic Graph API from 1952 to June 2022.The number of available papers is limited by the data we crawled from the Semantic Scholar Academic Graph.We also crawled papers from PubMed 1988 to 2024/01.We remove papers that are not English.We also remove papers where abstracts are not correctly parsed from paper PDFs.We will expand our models to papers written in other languages and other domains in the future.</p>
<p>Limitations of System Performance</p>
<p>Our dataset is based on state-of-the-art IE systems, which may be noisy.For instance, the coreference and SciSpacy abbreviation resolution models fail to link A2LCTC to Action-to-Language Connectionist Temporal Classification.The background context detection may also have errors: e.g., the sentence classification component fails to treat "For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector." as background context.In our human-vetted gold data subset, we make sure to filter such cases, but they remain in the training data.SentenceBert (Reimers and Gurevych, 2019), and GPT3.5/4 are not finetuned and might be biased towards pretraining datasets.The idea novelty boosting method is limited by the quality of retrieval models.Better retrieval models may be explored in the future.Due to hardware constraints, we mainly investigated models with up to 7 billion parameters.Due to API change and model randomness, our GPT3.5/4results might not be easily reproducible.</p>
<p>Limitations of Evaluation</p>
<p>We recruit annotators from Ph.D. students; their opinions may differ from annotators who have different levels of domain knowledge.Our setting uses a seed term taken from the ground truth as input, to emulate a scenario where a human provides guidance to an assistant model.Future work could explore methods in the setting without a seed term, an even harder task, or evaluate in an interactive setting with user-provided seed terms.In addition, while the seed is sampled from the ground truth, in our human-annotated gold subset, we make sure that in no case does the input context trivially leak the output.et al. (2023) reports that LLMs tend to memorize part of their training data, a well-known concern in evaluating current LLMs.Therefore, we examine the pretraining data of each model:</p>
<p>Memorization Check</p>
<p>Carlini</p>
<p>• T5: Raffel et al. (2020) shows that T5 is pretrained on C4 which was crawled from web prior to April 2019.</p>
<p>• GPT3.5:Based on the documentation,9 GPT-3.5 series is pretrained on a combination of test and code from before Q4 2021.</p>
<p>• GPT4: OpenAI (2023) shows that the GPT-4 checkpoint we used utilizes most pertaining data before September 2021.Despite this, the pretraining and post-training data contain "a small amount" of more recent data.10</p>
<p>Because we evaluate our models on papers published in 2022, the likelihood of test papers appearing in the pretraining corpora for the models is substantially reduced.We additionally performed a manual examination of GPT-4 memorization in our gold set based on 2022 ACL Anthology papers, by seeing if GPT-4 could complete information such as method names or generate text that strongly mimics the ground truth papers, and found no evidence of this occurring.The Meditron-7b (Chen et al., 2023) uses PubMed with a cut-off in August 2023, and our biochemical test set only includes PubMed papers after 2023/08.</p>
<p>A Dataset Collection</p>
<p>A.1 NLP Dataset Collection</p>
<p>We download ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API. 11We filter out papers without abstracts and not written in English to obtain 67,408 papers.Our dataset has 58,874 papers before 2021, 5,946 papers from 2021, and 2,588 from 2022.We first use PL-Marker (Ye et al., 2022) pretrained on Sci-ERC (Luan et al., 2018) to extract nodes belonging to six types: Task, Method, Evaluation Metric, Material, Other Scientific Terms, and Generic Terms.The model then predicts relations between nodes belonging to seven relation types: Usedfor, Feature-of, Evaluate-for, Hyponym-of, Partof, Compare, and Conjunction.Because we want to generate new ideas, we focus on used-for relations in papers.Next, we use SciCo (Cattan et al., 2021) with checkpoint from Hugging Face12 to obtain entity coreference to merge identical nodes.Then, we use ScispaCy (Neumann et al., 2019) to perform unsupervised abbreviation detection to replace the abbreviation with a more informative long form.Finally, we perform scientific sentence classification (Cohan et al., 2019) 13 to classify sentences from the abstract into five categories including Background, Method, Objective, Other, and Result.We select sentences with labels of Background and Other as background context.During preprocessing, we only keep high-confidence outputs from IE models.Figure 4 shows an example of the IE systems pipeline.</p>
<p>A.2 Biochemical Dataset Collection</p>
<p>We collect PubMed papers from 1988 to 2024 using Entrez Programming Utilities API14 for the following topics, including Yarrowia, Saccharomyces cerevisiae, Issatchenkia orientalis, and Rhodosporidium toruloides.We use PubTator 3 (Islamaj et al., 2021;Wei et al., 2022;Luo et al., 2023;Wei et al., 2023;Lai et al., 2023).The PubTator 3 performs named entity recognition, relation extraction, entity coreference and linking, and entity normalization for the abstracts in the dataset.Pub-Tator 3 identifies bio entities belonging to seven types: gene, chemical, chromosome, cell line, variant, disease, and speciesl and relations belonging to 13 types: associate, cause, compare, convert, contract, drug interact, inhibit, interact, negative correlate, positive correlate, prevent, stimulate, and treat.Finally, we use a sentence classifier trained on CODA-19 (Huang et al., 2020) to classify sentences in abstracts into background, purpose, method, finding, and other.We select sentences with labels of background as background context and remove sentences with labels of other.We treat the rest sentences that have at least one entity as the target sentence.We only keep samples with low similarity between background context and corresponding ground truth sentences. 15Our final dataset has 4,767 papers before 2023/02, 642 papers from 2023/02 to 2023/08, and 299 papers after 2023/08.</p>
<p>B Finetuning and Automated Evaluation details B.1 Inspiration Retrieval Module</p>
<p>The statistics of each inspiration type are in Table 7.</p>
<p>Table 8 shows sample retrieved inspirations.</p>
<p>B.1.1 Semantic Neighbors</p>
<p>We use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar nodes from the training set based on query q in §3.1.We retrieve up to 20 relevant semantic neighbors R from the training set for each instance.We treat the target nodes from R as semantic neighbors.</p>
<p>B.1.2 KG Neighbors</p>
<p>We use one-hop connected neighbors from the background KG G B constructed on papers before 2021(i.e., the papers in the training set).Because of the scarcity of KG neighbors, we do not limit the number of KG neighbors.</p>
<p>B.1.3 Citation Neighbors</p>
<p>Similar to semantic neighbors, we use all − mpnet − base − v2 from Sentence-Bert (Reimers and Gurevych, 2019) to retrieve cited paper titles similar to query q.We restrict cited papers only before 2021.We retrieve up to 5 relevant citation neighbors from the papers' citation network.</p>
<p>B.2 Generation Module</p>
<p>Our T5 model and their variants are built based on the Huggingface framework (Wolf et al., 2020). 16e optimize those models by AdamW (Loshchilov and Hutter, 2019) with the linear warmup scheduler.17Those models are finetuned on 4 NVIDIA A6000 48GB GPUs with distributed data parallel. 18he training time for each model is about 10 hours.</p>
<p>Used-for</p>
<p>Used-for</p>
<p>Feature-of</p>
<p>Used-for</p>
<p>Used-for</p>
<p>Coref</p>
<p>Method</p>
<p>Other Scientific Terms Task Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations.However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance of contextual information.</p>
<p>We propose a context-aware interaction network (COIN) to properly align two sequences and infer their semantic relationship.</p>
<p>B.2.1 In-Context Learning</p>
<p>We choose GPT3.5 davinci-003 19 (Brown et al., 2020) as our out-of-the-box causal language modeling baseline.We select 5 instances from the training set as examples for the few-shot setting.We randomly select those examples for GPT3.5FS.For GPT3.5Retr, similar to semantic neighbors, we use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar instances from the training set based on query q in §3.1.The input length is limited to 2048 tokens due to OpenAI API limits.We choose gpt-4-0314 as our GPT4 model.Our input for GPT4 is similar to GPT3.5.</p>
<p>For each selected example from the training set with forward relation, the template is "Consider the following context: M In that context, which p can be used for v, and why?T ", where M is the background context, p is the target node type, v is the seed term, and T is the target idea sentence; for backward relation, the template is "Consider the following context: M In that context, which p do we use v, and why? s".For selected examples with 19 openai.com/api/additional retrieval inspirations, we concatenate the following additional template to the M: "The retrieval results are: i 1 , . . ., i k ", where i 1 , . . ., i k are retrieved inspirations.For the final prompt, the template is similar to the above example template.However, the target sentence T will not be included.We ask the model to generate 10 outputs.We will select the best output and skip the empty output.</p>
<p>B.2.2 Fine Tuning</p>
<p>Given input without any inspirations, the input combines the prompt P and context M as shown in §3.1 (i.e., P | context: M).Given input with inspirations, the input is P | retrieve: i 1 , . . ., i k | context: M, with i 1 , . . ., i k as retrieved inspirations.The input length is limited to 512 tokens.</p>
<p>For both tasks, we finetune our model based on T5-large with a learning rate of 6 × 10 −6 and ϵ = 1 × 10 −6 .The batch size is 8 for each GPU.</p>
<p>The maximum training epoch for all models is 10 with 4 patience.During decoding, we use beamsearch to generate results with a beam size of 5 and a repetition penalty of 1.5.</p>
<p>In-context Contrastive Augmentation</p>
<p>We randomly select 2 sentences that appeared in the input as in-context negatives.For example, in Figure 1, the in-context negatives could be "knowledge acquisition is done by using Method", "this requires plms to integrate the information from all the sources in a lifelong manner .".</p>
<p>B.2.3 Biochemical Case Study</p>
<p>Our Meditron-7b (Chen et al., 2023) and its variants are built based on the Huggingface framework (Wolf et al., 2020). 20We use its epfl-llm/meditron-7b as the base model.We finetune those models with a learning rate of 2 × 10 −6 and ϵ = 5 × 10 −8 .The maximum training epoch for all models is 5.All models are finetuned on 4 NVIDIA A100 80 GB GPUs with Fully Sharded Data Parallel. 21The training time for each model is about 20 hours.</p>
<p>B.3 The Scale of Retrieval Set</p>
<p>We retrieve from a set of 59k papers with over 374k sentences in the NLP domain, the focus of our experiments.Our background KG built on the training set has more than 197k nodes and 261k relations.Moreover, we collect 87k paper titles from citation networks.This represents a large-scale and diverse domain; retrieving inspirations from this set is expected, in principle, to be more than enough for generating novel ideas.Indeed, NLP papers typically cite each other and build on each other as inspirations to create new ideas -which motivates our inspiration retrieval.</p>
<p>B.4 Automated Evaluation</p>
<p>We use BERTScore (Zhang* et al., 2020) with SciBERT checkpoint for both tasks.</p>
<p>The hash of the checkpoint is allenai/scibert_scivocab_uncased_L8 _no-idf_version=0.3.12(hug_trans=4.19.2).The automated evaluation results are in Table 9.</p>
<p>C Human Annotation and Evaluation Details</p>
<p>Gold Dataset Annotation Details The gold dataset annotation interface is in Figure 5.The quality of the instances in the test set is judged given three criteria: (1) whether the ground truth sentence trivially overlaps with background context;</p>
<p>(2) whether background context contains relevant information for the target relation; (3) whether the target relation (from which the seed term is taken) is a salient aspect of the idea proposed in the target paper.</p>
<p>Study I The instructions for human evaluation can be found in Figure 6, while an example of the 20 github.com/huggingface/transformers 21https://huggingface.co/docs/accelerate/usage _guides/fsdp human evaluation interface is provided in Figure 7 and 8. Human annotators are required to evaluate each system output based on the following criteria: (1) Is the candidate relevant to the context + seed term?(2) Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?(3) Does the candidate's suggestion generally make sense to you scientifically?(4) Is the language sufficiently clear and coherent to understand the suggestion?The input for sample human annotation is in Table 10 and the human labels are in Table 11.The human annotation agreement is in Table 13.</p>
<p>Study III We ask the following questions to human annotators to evaluate the quality of regeneration results: (1) Is the regenerated idea substantially different from the original?(2) Is the regenerated idea more novel and creative than the original idea?(3) Does the second iteration increase novelty?The human annotation agreement is in Table 14.</p>
<p>D Scientific Artifacts</p>
<p>We list the licenses of the scientific artifacts used in this paper: Semantic Scholar Academic Graph API (API license agreement22 ), Huggingface Transformers (Apache License 2.0), SBERT (Apache-2.0license), BERTScore (MIT license), Meditron-7b (Llama2), Entrez Programming Utilities API (Copyright23 ), PubTator 3 (Data use policy24 ), and OpenAI (Terms of use 25 ).</p>
<p>E Ethical Consideration</p>
<p>The SCIMON task and corresponding models we have designed in this paper are limited to the natural language processing (NLP) and biochemical domain, and might not apply to other scenarios.</p>
<p>E.1 Usage Requirement</p>
<p>This paper aims to provide investigative leads for a scientific domain, specifically natural language processing.The final results are not intended to be used without human review.Accordingly, domain experts might use this tool as a research writing assistant to develop ideas.However, our system does not do any fact-checking with external knowledge.In addition, we train our models on the ACL</p>
<p>E.2 Data Collection</p>
<p>We collect 67,408 ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API, under API license agreement26 .We ensure our data collection procedure follows the Terms of Use at https://allenai.org/terms.According to the agreement, our dataset can only be used for non-commercial purposes.As mentioned in §4, we perform the human evaluation.All CT is a model with citation inspirations.R-L denotes ROUGE-L.BERT denotes BERTscore with SciBERT as its encoder.† indicates that differences between models are not statistically significant (p ≤ 0.05) when compared to each other but are still significant when compared to the other models on t-test.</p>
<p>Type Content</p>
<p>Seed Term Prompt Irish language learning is done by using Method Context Irish is a minority language which means that l2 learners have limited opportunities for exposure to the language, and additionally, there are also limited (digital) learning resources available.One method that could be used for Irish language learning is computer assisted language learning (CALL) unhelpful</p>
<p>GPT3.5FS+CT</p>
<p>In that context, the most appropriate task for precision grammar would be the development of a part-of-speech tagger for Irish using finite-state morphology and constraint grammar disambiguation.</p>
<p>unhelpful</p>
<p>GPT3.5FS+KG</p>
<p>Cló: An Irish Language Learning Platform -Leveraging Digital Resources and Human Expertise unhelpful</p>
<p>GPT4FS</p>
<p>We propose using a technology-enhanced approach, such as computer-assisted language learning (CALL) or natural language processing (NLP) techniques, to create interactive and engaging Irish language learning resources, as these methods can provide personalized learning experiences and enhance learners' exposure to the language despite limited resources available.</p>
<p>helpful</p>
<p>GPT4FS+KG</p>
<p>Exploiting Social Media for Irish Language Learning: An Analysis of Twitter Data.In this context, we use social media data, particularly from Twitter, as a method for Irish language learning, because it provides a rich source of authentic and diverse language examples that can be used to enhance learning opportunities for L2 learners in a minority language setting.helpful</p>
<p>T5</p>
<p>In this paper, we present a novel machine translation approach to Irish language learning.unhelpful T5+SN+CL</p>
<p>A conversational virtual environment for Irish language learning helpful Table 11: Sample human labels for idea-sentence generation results for seed term prompt "Irish language learning is done by using Method", with context "irish is a minority language ... limited digital learning resources available".</p>
<p>Type Content</p>
<p>Seed Term ARO10 Context</p>
<p>Huangjiu is known for its unique aroma, primarily attributed to its high concentration of bphenylethanol (ranging from 40 to 130 mg/L).</p>
<p>Meditron</p>
<p>We then selected six key genes involved in the phenylalanine metabolism pathway and predicted that their product flux may affect the synthesis of b-phenylethanol.</p>
<p>Meditron + CT</p>
<p>We found that the key amino acid residue that controls the activity of Aro10p was not conserved in wine yeast strains, which may explain the lower b-phenylethanol production in wine fermentation compared with that in Chinese huangjiu.Meditron + KG Both target genes, SSA1 and ARO10, were deleted using the CRISPR-Cas9 genome editing system.Meditron + SN Herein, we report that the key barrier for b-phenylethanol production in Huangjiu is ARO10, the only bi-functional amino acid decarboxylase in Saccharomyces cerevisiae.</p>
<p>Table 12: Input and idea-sentence generation results for seed gene "ARO10" in the biochemical domain.</p>
<p>Annotator Pair 1-2 1-3 1-4 1-5 1-6</p>
<p>Agreement % 68.8 75.0 56.2 43.8 75.0</p>
<p>Table 13: Percent (%) of same labels from overlapped 10 human evaluation instances on each pair of annotators for Study I.</p>
<p>Figure 1: SCIMON takes background context and generates ideas grounded in literature inspirations, optimizing novelty by iteratively comparing to related work.</p>
<p>Figure 2 illustrates the setting, showing a background ...a method that combines continual learning with a dynamic knowledge distillation approach for efficient knowledge acquisition ... 1. Different from previous knowledge distillation methods ... student model learns from teacher model for incremental knowledge extraction ... ...continual learning for knowledge acquisition...This approach is more efficient than exhaustive pre-training on all existing data...</p>
<p>a method that leverages memory-augmented neural networks for knowledge acquisition in a lifelong learning scenario...</p>
<p>Figure 2 :
2
Figure 2: Architecture overview.Our models retrieve inspirations and then pass the background input and retrieved inspirations to an LM-based generation module, which iteratively optimizes novelty.Input from Qin et al. (2022).</p>
<p>Figure 3 :
3
Figure3: We use IE to obtain literature data for our approach: problems/motivations (background) and proposed ideas (target), as well as salient seed terms.</p>
<p>Figure 5 :
5
Figure 5: Gold subset annotation interface</p>
<p>Percent (%) of same labels from overlapped 20 human evaluation instances on each pair of annotators for Study III.(1-3) has 60 shared questions.The rest of the pairs each share 40 questions.</p>
<p>Figure 6 :
6
Figure 6: Human evaluation instructions</p>
<p>4
SplitForwardBackwardTotalTrain55,88458,426114,310Valid7,9388,25716,195Test2,6232,6865,309Table 1: Dataset statistics. Considering a relation of theform [v used-for u], we define [v used-for ?] asforward, and [? used-for u] as backward.</p>
<p>Table 3 :
3
Relative improvements of iterative novelty boosting.Iterations are applied to the ideas for which sufficiently similar related work is detected ( §3.3)."1st Novelty" is % of the 1st iteration ideas that gained novelty over the initial idea, and "2nd Novelty" is the % of gain over the 1st iteration.Our method substantially increases novelty for ideas to which it is applied.To save annotation resources, we only annotate second iteration results for the best-performing method (SN).We report the average number of new terms added, after filtering.elty/creativity (meaning that, e.g., if 100 examples were updated, we would gain 56 examples that are more novel).The 2nd iteration, further increases novelty for 57.8% of the ideas that continued to another iteration.For ideas not considered more novel after applying our method, we do not observe a drop in novelty-the method either increases or maintains novelty.
TypeGPT4FS+SN+CT+KG1st Novelty ∆ (%)+54.4+55.6 +47.8 +46.72nd Novelty ∆(%)-+57.8--1st new terms ∆+23.1+22.8 +22.1 +21.92nd new terms ∆-+21.5--</p>
<p>seed term: speech unit boundaries ; context (abridged): ... generate partial sentence translation given a streaming speech input.existingapproaches... break the acoustic units in speech, as boundaries between acoustic units in speech are not even....Initial ideaA pause prediction model to identify speech unit boundaries ... Iteration 1A method that leverages acoustic and linguistic features to predict speech unit boundaries dynamically, ensuring smooth transitions ... differs from the existing research as it combines both acoustic properties and linguistic context ... adapting to variations in speaker characteristics, speaking styles, and languages.Iteration 2
TypeContentInput (Donget al., 2022)</p>
<p>Table 6 :
6
Human quality evaluation of preprocessing stages(%).Overall pass rate after all steps are applied is 79.7%.
Background Sentence</p>
<p>Table 7 :
7
Average of # of neighbors for each instance, excluding those which do not have any neighbor</p>
<p>an effective solution to data scarcity in low -resource scenarios.however, when applied to token-level tasks such as ner , data augmentation methods often suffer from token-label misalignment, which leads to unsatsifactory performance.
TypeContentSeed Term Promptdata augmentation is used for TaskContext data augmentation is Semantic Neighbors st and automatic speech recognition (asr), low-resource tagging tasks, end-to-endspeech translation, neural online chats response selection, neural machine translation,semi-supervised ner, entity and context learning, semi-supervised setting, dependency pars-ing, low-resource machine translation, slot filling, dialog state tracking, visual questionanswering, visual question answering (vqa), low-resource neural machine translationKG Neighborsnmt-based text normalization, task-oriented dialog systems, task-oriented dialogue system,low-resource languages (lrl), end-to-end speech translation, visual question answering (vqa),multiclass utterance classification, clinical semantic textual similarity, neural online chatsresponse selection, context-aware neural machine translationCitation NeighborsContextual Augmentation: Data Augmentation by Words with Paradigmatic Re-lations,An Analysis of Simple Data Augmentation for Named Entity Recognition,DataAugmentationforLow-ResourceNeuralMachineTranslation,DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks,EDA: Easy Data Augmentation Techniques for Boosting Performance on Text ClassificationTasksGround TruthELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER</p>
<p>Table 8 :
8
Example (from (Zhou et al., 2022)) of retrieved inspirations.Inspirations similar to ground truth are underlined.
context containsRelationrelevantis a partIs theIE is ofinformation forof theoutputsufficienttarget relationmaintrivallyquality(Conservative filterideaoverlap(not-only flag casesproposewith thegeneric,where context isd by theinputcontextentityoutputrelationrel_sentcontextcorrect)highly irrelevant)papertransformer -basedlanguage models usuallytreat texts as linearsequences . however ,most texts also have anWe propose a novel approach toinherent hierarchicalformulate , extract , encode and injectstructure , i.e. , parts of ahierarchical structure informationtext can be identifiedexplicitly into an extractiveusing their position insummarization model based on a pre -this hierarchy . intrained , encoder -only Transformerextractive textaddition , section titleslanguage model ( HiStruct+ model ) ,summarizatiousually indicate thewhich improves SOTA ROUGEs forn is done bycommon topic of theirextractive textextractive summarization on PubMedusing Metricrespective sentences .summarization sota rouges used forand arXiv substantially .</p>
<p>Table 9 :
9
Automatic evaluation results for the challenging and gold subsets.CL is a model with in-context contrastive augmentation.SN is a model with semantic inspirations.KG is a model with KG inspirations.
SubsetChallengingGoldModelR-L↑BERT↑R-L↑BERT↑GPT4ZS0.1200.5810.1300.583GPT4FS0.1430.6180.1510.624T50.2230.672  †0.2460.685GPT4FS+SN0.1440.6200.1490.627GPT4FS+KG0.1430.6190.1520.626GPT4FS+CT0.1440.6170.1490.622T5+CL0.225  †0.671  †0.251  †0.686  †T5+SN+CL0.228  †0.671  †0.258  †0.686  †T5+KG+CL0.223  †0.6690.2480.681  †T5+CT+CL0.225  †0.671  †0.250  †0.686  †</p>
<p>Table 10 :
10
Input for sample human annotation results
ModelOutput
Code, data, and resources are publicly available for research purposes: https://github.com/eaglew/clbd.
More details are in Appendix C.
The agreement scores are in Table13Appendix C.
Full evaluator guidelines are in Appendix C. The sample annotations are in Table 11.
platform.openai.com/docs/model-index-for-res earchers
See footnote 10, page 10 of OpenAI (2023).
huggingface.co/allenai/longformer-scico
github.com/allenai/sequential_sentence_class ification
www.ncbi.nlm.nih.gov/books/NBK25501/
The similarity is calculated with all-mpnet-base-v2.
github.com/huggingface/transformers
huggingface.co/docs/transformers/main_classe s/optimizer_schedules#transformers.get_linear_sc hedule_with_warmup
pytorch.org/tutorials/intermediate/ddp_tutor ial.html
api.semanticscholar.org/license/
www.ncbi.nlm.nih.gov/books/about/copyright/
www.ncbi.nlm.nih.gov/home/about/policies/
openai.com/policies/terms-of-use
https://api.semanticscholar.org/license/ annotators involved in human evaluation are voluntary participants with a fair wage. We further collect 5,708 PubMed papers from 1988 to 2024 using Entrez Programming Utilities
API 27 . We follow their data usage
guidelines 28 . 27 www.ncbi.nlm.nih.gov/books/NBK25501/ 28 www.ncbi.nlm.nih.gov/books/about/copyright/
AcknowledgementsThis work is supported by the Molecule Maker Lab Institute: an AI research institute program supported by NSF under award No. 2019897, by DOE Center for Advanced Bioenergy and Bioproducts Innovation U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research under Award Number DESC0018420, by U.S. the AI Research Institutes program by National Science Foundation and the Institute of Education Sciences, Department of Education through Award No. 2229873 -AI Institute for Transforming Education for Children with Speech and Language Processing Challenges, and by AI Agriculture: the Agriculture and Food Research Initiative (AFRI)grant no.2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of, the National Science Foundation, the U.S. Department of Energy, and the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.
Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>A llm assisted exploitation of ai-guardian. Nicholas Carlini, arXiv:2307.15008Cryptography and Security Repository. 2023</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Scico: Hierarchical cross-document coreference for scientific concepts. Arie Cattan, Sophie Johnson, Ido Daniel S Weld, Iz Dagan, Doug Beltagy, Tom Downey, Hope, 20213rd Conference on Automated Knowledge Base Construction</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, arXiv:2311.16079Computation and Language Repository. 2023</p>
<p>Pretrained language models for sequential sentence classification. Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Dan Weld, 10.18653/v1/D19-1383Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Episodic memory in lifelong language learning. Cyprien De Masson D'autume, Sebastian Ruder, Lingpeng Kong, Dani Yogatama, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>Learning when to translate for streaming speech. Qian Dong, Yaoming Zhu, Mingxuan Wang, Lei Li, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Literature based discovery: models, methods, and trends. Sam Henry, Bridget T Mcinnes, 201774Journal of biomedical informatics</p>
<p>A computational inflection for scientific discovery. Tom Hope, Doug Downey, Oren Etzioni, Eric Daniel S Weld, Horvitz, Communications of the ACM. 2023</p>
<p>Context-aware interaction network for question matching. Zhe Hu, Zuohui Fu, Yu Yin, Gerard De, Melo , 10.18653/v1/2021.emnlp-main.312Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Benchmarking large language models as ai research agents. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.03302Machine Learning Repository. 2023</p>
<p>CODA-19: Using a non-expert crowd to annotate research aspects on 10,000+ abstracts in the COVID-19 open research dataset. Ting-Hao Kenneth Huang, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Yen-Chia Hsu, C Lee Giles, Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. the 1st Workshop on NLP for COVID-19 at ACL 2020Association for Computational Linguistics2020</p>
<p>Nlm-chem, a new resource for chemical entity recognition in pubmed full text literature. Rezarta Islamaj, Robert Leaman, Sun Kim, Dongseop Kwon, Chih-Hsuan Wei, Donald C Comeau, Yifan Peng, David Cissel, Cathleen Coss, Carol Fisher, Rob Guzman, Preeti Gokal Kochar, Stella Koppel, Dorothy Trinh, Keiko Sekiya, Janice Ward, Deborah Whitman, Susan Schmidt, Zhiyong Lu, 10.1038/s41597-021-00875-1Scientific Data. 81912021</p>
<p>Thinking about GPT-3 in-context learning for biomedical IE? think again. Jimenez Bernal, Nikolas Gutierrez, Clayton Mcneal, You Washington, Lang Chen, Huan Li, Yu Sun, Su, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Biorex: Improving biomedical relation extraction by leveraging heterogeneous datasets. Po-Ting Lai, Chih-Hsuan Wei, Ling Luo, Qingyu Chen, Zhiyong Lu, 10.1016/j.jbi.2023.104487Journal of Biomedical Informatics. 1461044872023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational Linguistics2022. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, Proceedings of the 7th International Conference on Learning Representations. the 7th International Conference on Learning Representations2019</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, 10.18653/v1/D18-1360Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>AIONER: all-in-one scheme-based biomedical named entity recognition using deep learning. Ling Luo, Chih-Hsuan Wei, Po-Ting Lai, Robert Leaman, Qingyu Chen, Zhiyong Lu, 10.1093/bioinformatics/btad310Bioinformatics. 3953102023</p>
<p>Scientific language models for biomedical knowledge base completion: an empirical study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, 2021AKBC</p>
<p>ScispaCy: Fast and robust models for biomedical natural language processing. Mark Neumann, Daniel King, Iz Beltagy, Waleed Ammar, 10.18653/v1/W19-5034Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>The logic theory machine-a complex information processing system. Allen Newell, Herbert Simon, IRE Transactions on information theory. 231956</p>
<p>Representation learning with contrastive predictive coding. Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375arXiv:1807.03748Computation and Language Repository. 2023. 2018Capabilities of GPT-4 on medical challenge problems</p>
<p>arXiv:2303.08774Gpt-4 technical report. Computation and Language Repository. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>ELLE: Efficient lifelong pre-training for emerging data. Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou, 10.18653/v1/2022.findings-acl.220Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Does scientific discovery have a logic? Philosophy of science. Simon Herbert, 197340</p>
<p>Contexts and contradictions: a roadmap for computational drug repurposing with knowledge inference. N Daniel, Russ B Sosa, Altman, Briefings in Bioinformatics. 2342682022</p>
<p>Undiscovered public knowledge. Don R Swanson, The Library Quarterly. 5621986</p>
<p>Agatha: automatic graph mining and transformer based hypothesis generation approach. Justin Sybrandt, Ilya Tyagin, Michael Shtutman, Ilya Safro, 10.1145/3340531.3412684Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. John Vahe Tshitoyan, Leigh Dagdelen, Alexander Weston, Ziqin Dunn, Olga Rong, Kristin A Kononova, Gerbrand Persson, Anubhav Ceder, Jain, Nature. 57177632019</p>
<p>PaperRobot: Incremental draft generation of scientific ideas. Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan, 10.18653/v1/P19-1191Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Findings of the babylm challenge: Sample-efficient pretraining on developmentally plausible corpora. Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning2023</p>
<p>Aleksandar Milosavljevic, and Zhiyong Lu. 2022. tmvar 3.0: an improved variant concept recognition and normalization tool. Chih-Hsuan Wei, Alexis Allot, Kevin Riehle, Bioinformatics. 3818</p>
<p>GNorm2: an improved gene name recognition and normalization system. Chih-Hsuan Wei, Ling Luo, Rezarta Islamaj, Po-Ting Lai, Zhiyong Lu, 10.1093/bioinformatics/btad599Bioinformatics. 39105992023</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Exploring and verbalizing academic ideas by concept co-occurrence. Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou, 10.18653/v1/2023.acl-long.727Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Packed levitated marker for entity and relation extraction. Deming Ye, Yankai Lin, Peng Li, Maosong Sun, 10.18653/v1/2022.acl-long.337Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>BARTScore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 2021</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, Proceedings of the 8th International Conference on Learning Representations. the 8th International Conference on Learning Representations2020</p>
<p>MELM: Data augmentation with masked entity language modeling for low-resource NER. Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao, 10.18653/v1/2022.acl-long.160Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland20221Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1712 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1712</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1712</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-273969969</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.07668v1.pdf" target="_blank">Towards Evaluation Guidelines for Empirical Studies Involving LLMs</a></p>
                <p><strong>Paper Abstract:</strong> In the short period since the release of ChatGPT, large language models (LLMs) have changed the software engineering research landscape. While there are numerous opportunities to use LLMs for supporting research or software engineering tasks, solid science needs rigorous empirical evaluations. However, so far, there are no specific guidelines for conducting and assessing studies involving LLMs in software engineering research. Our focus is on empirical studies that either use LLMs as part of the research process or studies that evaluate existing or new tools that are based on LLMs. This paper contributes the first set of holistic guidelines for such studies. Our goal is to start a discussion in the software engineering research community to reach a common understanding of our standards for high-quality empirical studies involving LLMs.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1712.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1712.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bavaresco et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale empirical study (cited by this paper) that investigates whether LLMs can act as substitutes for human judges across 20 NLP evaluation tasks; cited here as evidence that LLM outputs show too much variation to reliably replace human judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>natural language evaluation artifacts (general NLP tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general NLP / natural language</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>reliability/consistency of judgments for evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>High variation in LLM outputs; reported as too large to reliably substitute human judges (i.e., inconsistent judgments across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Cited conclusion: LLMs exhibited too much variation across tasks to be a reliable substitute for human judges (no detailed metrics provided in this paper's text).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited by Wagner et al. as a large-scale empirical result showing that, across many NLP evaluation tasks, LLM judgments vary too much to replace human judges reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Wagner et al. only cite the high-level conclusion (too large variation); specific agreement metrics or task breakdowns are not reported in the guideline paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Evaluation Guidelines for Empirical Studies Involving LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1712.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1712.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ahmed et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can LLMs replace manual annotation of software engineering artifacts?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An arXiv preprint (cited) proposing and empirically studying procedures for replacing human annotators with LLMs for software-engineering artifact annotation; the paper is cited for a proposed process involving few-shot calibration and conditional replacement of a human annotator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs replace manual annotation of software engineering artifacts?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-annotator (hybrid human-LLM annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>few-shot learning for calibration (as part of the proposed process)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>software engineering artifacts (annotations of SE artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>software engineering (unspecified languages/domains)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>annotation accuracy/quality of labels compared to human annotations</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>proposed hybrid process: initial few-shot experiments and, given satisfactory results, replacing one human annotator by an LLM with validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Good few-shot calibration performance and subsequent human validation are conditions under which the authors propose limited replacement of a human annotator.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Proposes a calibrated procedure where an LLM may replace one human annotator only after few-shot calibration and validation, implying conditional (not wholesale) equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Yes — an initial few-shot learning / calibration stage is central to the proposed approach; replacement occurs only if results are validated as good.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as proposing a defensible, conditional workflow for using LLMs as annotators: few-shot calibration + validation can allow replacing one human annotator, but only when validated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires successful calibration and human validation; not presented as a universal replacement for human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Evaluation Guidelines for Empirical Studies Involving LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1712.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1712.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang et al. (2024) CHI (human-LLM verification)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-LLM collaborative annotation through effective verification of llm labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CHI 2024 work (cited) on collaborative human–LLM annotation that emphasizes verification of LLM-proposed labels to improve annotation quality; cited in support of hybrid approaches where humans correct or verify LLM output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-LLM collaborative annotation through effective verification of llm labels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>hybrid human-LLM annotation with verification</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>text annotations (labels for textual artifacts); applicable to SE textual artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>NLP / software engineering textual artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>label accuracy and verification effectiveness</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>human verification of LLM-generated labels (workflow to check and correct LLM labels)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Hybrid verification workflows (human checks of LLM labels) increase label accuracy and reliability according to citation.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Framed as improving alignment by adding human verification steps rather than replacing humans outright.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Describes verification workflows (human-in-the-loop) rather than pure LLM-only labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that human verification of LLM labels can substantially improve annotation quality and is a recommended hybrid approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Hybrid verification requires human effort and does not eliminate the need for human judgment; LLM variability still a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Evaluation Guidelines for Empirical Studies Involving LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1712.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1712.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wagner et al. (2025) guidelines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards Evaluation Guidelines for Empirical Studies involving LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's own preliminary guidelines recommending transparent reporting, use of open-model baselines, and mandatory human validation (or at least partial human validation) of LLM outputs; it cites empirical work showing LLMs have large output variation and recommends reporting inter-rater reliability when humans validate LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-annotator / LLM-as-a-judge (guidelines address both roles and recommend human-in-the-loop validation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>software engineering artifacts (annotations, code, documentation, other SE artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general software engineering (no single language/domain specified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>annotation quality, judgment reliability, code quality/readability where applicable</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Recommends human validation of LLM outputs; example given of reviewing 20% subset of LLM-generated annotations by experienced software engineers.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>experienced software engineers (example in guideline text)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>inter-rater reliability (recommended to be reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Example in guideline: inter-rater reliability of 90% reported for a 20% subset review (presented as an illustrative example, not as an empirical result of this guideline paper).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Hybrid human–LLM annotation/verification, clear reporting of prompts and model configuration, use of open-model baselines for reproducibility, and initial calibration/few-shot validation are recommended to increase alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Tasks with subjective criteria, high LLM non-determinism, lack of clear prompts/rubrics, or where LLMs may have seen solutions in training data are called out as problematic for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not quantified in this paper; guidance implies that increased complexity or subjectivity likely reduces alignment and increases need for human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>The paper emphasizes that reporting exact prompts and using clear, systematic prompt development increases reproducibility and is likely to improve alignment between LLM and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Example value given: 90% inter-rater reliability for a reviewed subset (presented as an illustrative example in the guidelines).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>The paper recommends not using LLMs as direct substitutes for human judges without validation; it cites work showing LLMs have high variation and recommends partial human validation to ensure alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Recommends calibration steps (e.g., reporting temperature, prompt engineering); cites Ahmed et al.'s few-shot + validation approach as a potential procedure for calibrated replacement of a human annotator.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Key recommendations: always declare LLM usage/role, report model version/date/configuration, share prompts and their development, use an open LLM baseline, and perform human validation (report inter-rater reliability) when LLMs provide annotations or judgments. Cites evidence that LLMs alone are often too variable to replace humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM non-determinism, model evolution over time, lack of transparency about training data and fine-tuning, benchmark contamination, and that some open models are only 'open weight' — all complicate alignment and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Evaluation Guidelines for Empirical Studies Involving LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks <em>(Rating: 2)</em></li>
                <li>Can LLMs replace manual annotation of software engineering artifacts? <em>(Rating: 2)</em></li>
                <li>Human-LLM collaborative annotation through effective verification of llm labels <em>(Rating: 2)</em></li>
                <li>TnT-LLM: Text mining at scale with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1712",
    "paper_id": "paper-273969969",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "Bavaresco et al. (2024)",
            "name_full": "LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
            "brief_description": "A large-scale empirical study (cited by this paper) that investigates whether LLMs can act as substitutes for human judges across 20 NLP evaluation tasks; cited here as evidence that LLM outputs show too much variation to reliably replace human judges.",
            "citation_title": "LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "LLM-as-a-judge",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "natural language evaluation artifacts (general NLP tasks)",
            "artifact_domain": "general NLP / natural language",
            "evaluation_criteria": "reliability/consistency of judgments for evaluation tasks",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": null,
            "low_agreement_conditions": "High variation in LLM outputs; reported as too large to reliably substitute human judges (i.e., inconsistent judgments across tasks).",
            "artifact_complexity_effect": null,
            "criteria_clarity_effect": null,
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Cited conclusion: LLMs exhibited too much variation across tasks to be a reliable substitute for human judges (no detailed metrics provided in this paper's text).",
            "calibration_or_training": null,
            "key_findings": "Cited by Wagner et al. as a large-scale empirical result showing that, across many NLP evaluation tasks, LLM judgments vary too much to replace human judges reliably.",
            "limitations_noted": "Wagner et al. only cite the high-level conclusion (too large variation); specific agreement metrics or task breakdowns are not reported in the guideline paper.",
            "uuid": "e1712.0",
            "source_info": {
                "paper_title": "Towards Evaluation Guidelines for Empirical Studies Involving LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Ahmed et al. (2024)",
            "name_full": "Can LLMs replace manual annotation of software engineering artifacts?",
            "brief_description": "An arXiv preprint (cited) proposing and empirically studying procedures for replacing human annotators with LLMs for software-engineering artifact annotation; the paper is cited for a proposed process involving few-shot calibration and conditional replacement of a human annotator.",
            "citation_title": "Can LLMs replace manual annotation of software engineering artifacts?",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "LLM-as-annotator (hybrid human-LLM annotation)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": "few-shot learning for calibration (as part of the proposed process)",
            "artifact_type": "software engineering artifacts (annotations of SE artifacts)",
            "artifact_domain": "software engineering (unspecified languages/domains)",
            "evaluation_criteria": "annotation accuracy/quality of labels compared to human annotations",
            "human_evaluation_setup": "proposed hybrid process: initial few-shot experiments and, given satisfactory results, replacing one human annotator by an LLM with validation",
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Good few-shot calibration performance and subsequent human validation are conditions under which the authors propose limited replacement of a human annotator.",
            "low_agreement_conditions": null,
            "artifact_complexity_effect": null,
            "criteria_clarity_effect": null,
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Proposes a calibrated procedure where an LLM may replace one human annotator only after few-shot calibration and validation, implying conditional (not wholesale) equivalence.",
            "calibration_or_training": "Yes — an initial few-shot learning / calibration stage is central to the proposed approach; replacement occurs only if results are validated as good.",
            "key_findings": "Cited as proposing a defensible, conditional workflow for using LLMs as annotators: few-shot calibration + validation can allow replacing one human annotator, but only when validated.",
            "limitations_noted": "Requires successful calibration and human validation; not presented as a universal replacement for human annotators.",
            "uuid": "e1712.1",
            "source_info": {
                "paper_title": "Towards Evaluation Guidelines for Empirical Studies Involving LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Wang et al. (2024) CHI (human-LLM verification)",
            "name_full": "Human-LLM collaborative annotation through effective verification of llm labels",
            "brief_description": "A CHI 2024 work (cited) on collaborative human–LLM annotation that emphasizes verification of LLM-proposed labels to improve annotation quality; cited in support of hybrid approaches where humans correct or verify LLM output.",
            "citation_title": "Human-LLM collaborative annotation through effective verification of llm labels",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "hybrid human-LLM annotation with verification",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "text annotations (labels for textual artifacts); applicable to SE textual artifacts",
            "artifact_domain": "NLP / software engineering textual artifacts",
            "evaluation_criteria": "label accuracy and verification effectiveness",
            "human_evaluation_setup": "human verification of LLM-generated labels (workflow to check and correct LLM labels)",
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Hybrid verification workflows (human checks of LLM labels) increase label accuracy and reliability according to citation.",
            "low_agreement_conditions": null,
            "artifact_complexity_effect": null,
            "criteria_clarity_effect": null,
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Framed as improving alignment by adding human verification steps rather than replacing humans outright.",
            "calibration_or_training": "Describes verification workflows (human-in-the-loop) rather than pure LLM-only labeling.",
            "key_findings": "Cited as evidence that human verification of LLM labels can substantially improve annotation quality and is a recommended hybrid approach.",
            "limitations_noted": "Hybrid verification requires human effort and does not eliminate the need for human judgment; LLM variability still a concern.",
            "uuid": "e1712.2",
            "source_info": {
                "paper_title": "Towards Evaluation Guidelines for Empirical Studies Involving LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Wagner et al. (2025) guidelines",
            "name_full": "Towards Evaluation Guidelines for Empirical Studies involving LLMs",
            "brief_description": "This paper's own preliminary guidelines recommending transparent reporting, use of open-model baselines, and mandatory human validation (or at least partial human validation) of LLM outputs; it cites empirical work showing LLMs have large output variation and recommends reporting inter-rater reliability when humans validate LLM outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-annotator / LLM-as-a-judge (guidelines address both roles and recommend human-in-the-loop validation)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "software engineering artifacts (annotations, code, documentation, other SE artifacts)",
            "artifact_domain": "general software engineering (no single language/domain specified)",
            "evaluation_criteria": "annotation quality, judgment reliability, code quality/readability where applicable",
            "human_evaluation_setup": "Recommends human validation of LLM outputs; example given of reviewing 20% subset of LLM-generated annotations by experienced software engineers.",
            "human_expert_count": null,
            "human_expert_expertise": "experienced software engineers (example in guideline text)",
            "agreement_metric": "inter-rater reliability (recommended to be reported)",
            "agreement_score": "Example in guideline: inter-rater reliability of 90% reported for a 20% subset review (presented as an illustrative example, not as an empirical result of this guideline paper).",
            "high_agreement_conditions": "Hybrid human–LLM annotation/verification, clear reporting of prompts and model configuration, use of open-model baselines for reproducibility, and initial calibration/few-shot validation are recommended to increase alignment.",
            "low_agreement_conditions": "Tasks with subjective criteria, high LLM non-determinism, lack of clear prompts/rubrics, or where LLMs may have seen solutions in training data are called out as problematic for alignment.",
            "artifact_complexity_effect": "Not quantified in this paper; guidance implies that increased complexity or subjectivity likely reduces alignment and increases need for human validation.",
            "criteria_clarity_effect": "The paper emphasizes that reporting exact prompts and using clear, systematic prompt development increases reproducibility and is likely to improve alignment between LLM and human judgments.",
            "sample_size": null,
            "inter_human_agreement": "Example value given: 90% inter-rater reliability for a reviewed subset (presented as an illustrative example in the guidelines).",
            "proxy_vs_human_comparison": "The paper recommends not using LLMs as direct substitutes for human judges without validation; it cites work showing LLMs have high variation and recommends partial human validation to ensure alignment.",
            "calibration_or_training": "Recommends calibration steps (e.g., reporting temperature, prompt engineering); cites Ahmed et al.'s few-shot + validation approach as a potential procedure for calibrated replacement of a human annotator.",
            "key_findings": "Key recommendations: always declare LLM usage/role, report model version/date/configuration, share prompts and their development, use an open LLM baseline, and perform human validation (report inter-rater reliability) when LLMs provide annotations or judgments. Cites evidence that LLMs alone are often too variable to replace humans.",
            "limitations_noted": "LLM non-determinism, model evolution over time, lack of transparency about training data and fine-tuning, benchmark contamination, and that some open models are only 'open weight' — all complicate alignment and reproducibility.",
            "uuid": "e1712.3",
            "source_info": {
                "paper_title": "Towards Evaluation Guidelines for Empirical Studies Involving LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
            "rating": 2,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        },
        {
            "paper_title": "Can LLMs replace manual annotation of software engineering artifacts?",
            "rating": 2,
            "sanitized_title": "can_llms_replace_manual_annotation_of_software_engineering_artifacts"
        },
        {
            "paper_title": "Human-LLM collaborative annotation through effective verification of llm labels",
            "rating": 2,
            "sanitized_title": "humanllm_collaborative_annotation_through_effective_verification_of_llm_labels"
        },
        {
            "paper_title": "TnT-LLM: Text mining at scale with large language models",
            "rating": 1,
            "sanitized_title": "tntllm_text_mining_at_scale_with_large_language_models"
        }
    ],
    "cost": 0.014580749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Evaluation Guidelines for Empirical Studies involving LLMs</p>
<p>Stefan Wagner stefan.wagner@tum.de 
TUM School of Computation, Information and Technology
Technical University of Munich
HeilbronnGermany</p>
<p>Marvin Muñoz Barón marvin.munoz-baron@tum.de 
TUM School of Computation, Information and Technology
Technical University of Munich
HeilbronnGermany</p>
<p>Davide Falessi falessi@ing.uniroma2.it 
University of Rome "Tor Vergata"
RomeItaly</p>
<p>Sebastian Baltes sebastian.baltes@uni-bayreuth.de 
University of Bayreuth
BayreuthGermany</p>
<p>Towards Evaluation Guidelines for Empirical Studies involving LLMs
52B2B0B14C8144CA5951FA15152A6BBE10.1109/WSESE66602.2025.00011Large language modelsgenerative artificial intelligenceempirical studies
In the short period since the release of ChatGPT, large language models (LLMs) have changed the software engineering research landscape.While there are numerous opportunities to use LLMs for supporting research or software engineering tasks, solid science needs rigorous empirical evaluations.However, so far, there are no specific guidelines for conducting and assessing studies involving LLMs in software engineering research.Our focus is on empirical studies that either use LLMs as part of the research process or studies that evaluate existing or new tools that are based on LLMs.This paper contributes the first set of holistic guidelines for such studies.Our goal is to start a discussion in the software engineering research community to reach a common understanding of our standards for high-quality empirical studies involving LLMs.</p>
<p>I. INTRODUCTION</p>
<p>While artificial intelligence (AI) has been used in software engineering (SE) for a long time, success used to be limited [1].Recently, the rise of large language models (LLMs) has opened new avenues for the application of AI in software engineering [2], [3].These models offer many possible use cases, ranging from code generation and bug detection to requirements analysis and software maintenance.For instance, LLM-based tools were able to generate logging statements [4], generate test cases [5], and support education [6].</p>
<p>As a result, we are starting to see an increasing number of evaluation studies either using LLMs as part of the research process [7] or as part of tools that automate or improve software engineering tasks.These studies explore the effectiveness, performance, and robustness of LLMs in different contexts, such as improving code quality, reducing development time, or supporting software documentation.However, it is often unclear how valid and reproducible results can be achieved with empirical studies involving LLMs -or what effect their usage has on the validity of empirical results.This uncertainty poses significant challenges for researchers aiming to draw reliable conclusions from empirical studies.</p>
<p>The work of Davide Falessi has been partially supported by the 2022JJ3PA5 -PRIN2022 project.This work also was partially supported by the German Federal Ministry of Education and Research in the project MEKI (21IVP016F).</p>
<p>One of the primary risks in creating unreproducible results stems from the variability in LLM performance due to differences in training data, model architecture, evaluation metrics, and the inherent non-determinism of those models.For example, slight changes in the training data or the hyperparameters can lead to significantly different outcomes, making it difficult to reproduce studies.Also, the lack of standardized benchmarks and evaluation protocols further complicates the reproducibility.These issues highlight the need for clear guidelines and best practices to ensure that empirical studies with LLMs yield valid and reproducible results.</p>
<p>There has been extensive work developing guidelines for conducting and reporting specific types of empirical studies such as controlled experiments [8], [9] or their replications [10].We believe that LLMs have specific intrinsic characteristics that require specific guidelines for researchers to achieve an acceptable level of reproducibility.For example, even if we know the specific version of an LLM used for an empirical study, the reported performance for the studied tasks can change over time, especially for commercial models that evolve beyond version identifiers [11].Moreover, commercial providers do not guarantee the availability of old model versions indefinitely.Besides versions, LLMs' performance widely varies depending on configured parameters such as temperature.Therefore, not reporting the parameter settings impacts the reproducibility of the research.</p>
<p>Even for "open" models such as Llama, we do not know how they were fine-tuned for specific tasks and what the exact training data was [12].For example, when evaluating LLMs' performance for certain programming tasks, it would be relevant to know whether the solution to a certain problem was part of the training data or not.</p>
<p>Therefore, with this paper, we provide two key contributions: (1) a classification of different types of empirical studies involving LLMs in software engineering research and (2) preliminary guidelines on how to achieve valid and reproducible results in such studies.The most recent version of the study types and guidelines is available online. 1</p>
<p>II. RELATED WORK</p>
<p>There are several established guidelines for empirical studies in software engineering, e.g., for experiments [8], [9].While these guidelines continue to be useful, they were developed before the rise of LLMs.Therefore, this paper is a starting point for extending the existing set of guidelines.To the best of our knowledge, the only similar work is a paper by Sallou, Durieux, and Panichella [13], in which they also call for a broader discussion in the community.They mostly focus on a discussion of threats to validity, proposing guidelines that partly overlap with ours.However, they do not structure their guidelines according to different types of studies.We are convinced that the diversity of studies involving LLMs requires a differentiation between study types.Our taxonomy presented in Section III is a first step in that direction.</p>
<p>III. TYPES OF STUDIES</p>
<p>The development of empirical guidelines for studies involving LLMs in software engineering is crucial for ensuring the validity and reproducibility of results.However, these guidelines must be tailored for different study types as they may pose unique challenges.Therefore, understanding the classification of these studies is essential for developing appropriate guidelines.We envision that a mature set of guidelines provides specific guidance for each of these study types, addressing their individual methodological idiosyncrasies.</p>
<p>A. LLMs as Tools for Researchers in Empirical Studies</p>
<p>LLMs can be leveraged as powerful tools to assist researchers conducting empirical studies.They can automate various tasks such as data collection, preprocessing, and analysis.For example, LLMs can extract relevant information from large datasets, generate summaries of research papers, and even assist in writing literature reviews.This can significantly reduce the time and effort required by researchers, allowing them to focus on more complex aspects of their studies.</p>
<p>1) LLMs as Annotators: LLMs can serve as annotators by automatically labeling artifacts with corresponding categories for data analysis.For example, in a study analyzing code changes in version control systems, researchers may need to categorize each individual change.For that, they may use LLMs to analyze commit messages and categorize them into predefined labels such as bug fixes, feature additions, or refactorings.This automation can improve the efficiency of the annotation process, which is often a labor-intensive and error-prone task when done manually.Moreover, in qualitative data analysis, manually annotating or coding text passages is also an often time-consuming manual process.LLMs can be used to augment human annotations, provide suggestions for new codes, or even automate the entire process.In such tasks, LLMs have the potential to improve the accuracy and efficiency of automated labeling processes [14], making them valuable tools for empirical research in software engineering.Hybrid human-LLM annotation approaches may further increase accuracy and allow for the correction of incorrectly applied labels [15].</p>
<p>2) LLMs as Judges: In empirical studies, LLMs can act as judges to evaluate the quality of software artifacts such as code, documentation, and design patterns.For instance, LLMs can be trained to assess code readability, adherence to coding standards, and the quality of comments.By providing rather objective and consistent evaluations, LLMs could help mitigate certain biases and part of the variability that human judges might introduce.This could lead to more reliable and reproducible results in empirical studies.However, when relying on the judgment of LLMs, researchers have to make sure to build a reliable process for generating ratings that considers the non-deterministic nature of LLMs and report the intricacies of that process transparently.</p>
<p>3) LLMs as Subjects: LLMs can be used as subjects in empirical studies to simulate human behavior and interactions.For example, researchers can use LLMs to generate responses in user studies, simulate developer interactions in collaborative coding environments, or model user feedback in software usability studies.This approach can provide valuable insights while reducing the need to recruit human participants, which can be time-consuming and costly.Additionally, using LLMs as subjects allows for controlled experiments with consistent and repeatable conditions.However, when using LLMs as study subjects, it is important that researchers are aware of their inherent biases [16] and limitations [17].</p>
<p>B. LLMs for New Tools Supporting Software Engineers</p>
<p>LLMs are being integrated into new tools designed to support software engineers in their daily tasks.These tools can include intelligent code editors that provide real-time code suggestions, automated documentation generators, and advanced debugging assistants.Empirical studies can evaluate the effectiveness of these tools in improving productivity, code quality, and developer satisfaction.By assessing the impact of LLM-powered tools, researchers can identify best practices and areas for further improvement.For example, Choudhuri et al. [18] conducted a student experiment in which they measured the impact of ChatGPT on the correctness and completion time for programming tasks.</p>
<p>C. Studying LLM Usage</p>
<p>Empirical studies can also focus on understanding how software engineers use LLMs in their workflows.This involves investigating the adoption, usage patterns, and perceived benefits and challenges of LLM-based tools.Surveys, interviews, and observational studies can provide insights into how LLMs are integrated into development processes, how they influence decision-making, and what factors affect their acceptance and effectiveness.Such studies can inform the design of more userfriendly and effective LLM-based tools.For example, Khojah et al. [19] investigated the use of ChatGPT by professional software engineers in a week-long observational study.</p>
<p>D. Benchmarking LLMs for SE Tasks</p>
<p>Another typical type of study focuses on benchmarking LLM output quality on large datasets.In software engineering, this may include the evaluation of LLMs' ability to produce accurate and robust outputs for input data from real-world projects or synthetically created SE datasets.In studies with generative models, the LLM output is often compared against a ground truth dataset using similarity metrics such as ROUGE, BLEU, or METEOR [3].Moreover, the evaluation may be augmented by using task-specific or artifact-specific measures.Such measures may include code quality or performance metrics for code generation tasks or readability metrics for natural language SE artifacts (e.g., requirements documents).In this context, reference datasets such as HumanEval [20] play an important role in establishing standardized evaluations.However, benchmark contamination [21] has recently been identified as an issue.The careful creation of samples and corresponding input prompts is particularly important, as correlations between prompts may bias benchmark results [22].</p>
<p>IV. PRELIMINARY GUIDELINES While providing a comprehensive set of guidelines is beyond the scope of this position paper, we report a first set of guidelines based on a discussion session with other empiricism experts at the 2024 International Software Engineering Research Network (ISERN) meeting. 2 This paper is meant as a starting point for further discussions in the community with the aim of developing a common understanding of how we should conduct and report empirical studies involving LLMs.</p>
<p>A. Declare LLM Usage and Role</p>
<p>When conducting any kind of empirical study involving LLMs, it is essential to clearly declare that an LLM was used.This includes specifying the purpose of using the LLM in the study, the tasks it was applied to, and the expected outcomes.Transparency in the usage of LLMs helps in understanding the context and scope of the study, facilitating better interpretation and comparison of results.Beyond this declaration, we recommend that the authors be explicit about the LLM's exact role.Oftentimes, there is a complex layer around the LLM that preprocesses data, prepares prompts, or filters user requests.One example is ChatGPT, which can, among others, use the GPT-4o model.GitHub Copilot uses the same model as well, and researchers can build their own tools utilizing GPT-4o directly (e.g., via the OpenAI API).The infrastructure around the bare model can significantly contribute to the performance of a model in a certain task.Therefore, it is crucial that researchers clearly describe what the LLM contributes to the tool or method presented in a research paper.</p>
<p>B. Report Model Version and Date</p>
<p>It is also crucial for all types of studies to document the specific version of the LLM used in the study, along with the date when the experiments were conducted.LLMs are frequently updated, and different versions may produce varying results.By providing this information, researchers enable others to reproduce the study under the same conditions.Different model providers have varying degrees of information.For example, OpenAI provides a model version and a system fingerprint describing the backend configuration that can also influence the output.Therefore, stating "We used gpt-4o-2024-08-0, system fingerprint fp 6b68a8204b" provides clarity on the exact model and runtime environment.However, the main purpose of the system fingerprint is detecting changes and going back to a previous system fingerprint is impossible.</p>
<p>C. Report Model Configuration</p>
<p>Detailed documentation of the configuration and parameters used during any study is necessary for reproducibility.This includes settings such as the temperature that controls randomness, the maximum token length, and any other relevant parameters such as the consideration of historical context.Additionally, a thorough description of the hosting environment of the LLM or LLM-based tool should be provided, especially in studies focusing on performance or any timesensitive measurement.For instance, researchers might report that "the model was integrated via the Azure OpenAI Service, and configured with a temperature of 0.7, top p set to 0.8, and a maximum token length of 512," providing a clear overview of the experimental setup.Using seed values does further increase reproducibility, but does not completely mitigate the issue of non-determinism. 3</p>
<p>D. Report Prompts and their Development</p>
<p>Reporting the exact prompts used in the study is essential for transparency and reproducibility.Prompts can significantly influence the output of LLMs [23], and sharing them allows other researchers to understand and reproduce the conditions of the study.For example, including the specific questions or tasks given to the LLM helps assess the validity of the results and compare them with other studies.This is an example where different types of studies require different information.When studying LLM usage, the researchers ideally collect and publish the prompts written by the users (if confidentiality allows).Otherwise, summaries and examples can be provided.Prompts also need to be reported when LLMs are integrated into new tools, especially if study participants were able to formulate (parts of) the prompts.For all other types of studies, researchers should discuss how they arrived at their final set of prompts.If a systematic approach was used, this process should be described in detail.</p>
<p>E. Use an Open LLM as a Baseline</p>
<p>To ensure the reproducibility of results, we recommended findings be reported with an open LLM as a baseline.This applies both when using LLMs as tools for supporting researchers in empirical studies and when benchmarking LLMs for SE tasks.In case LLMs are integrated into new tools, this is also preferable if the architecture of the tool allows it.If the effort of changing models is too high, researchers should at least report an initial benchmarking with open models, which enables more objective comparisons.Open LLMs can either be hosted via cloud platforms such as Hugging Face or used locally via tools such as ollama or LM Studio.A replication package for papers using LLMs should include clear instructions that allow other researchers to reproduce the findings using open models.This practice enhances the credibility of the study and allows for independent verification of the results.Researchers could, e.g., mention that "results were compared with those obtained using Meta's Code LLAMA, available on the Hugging Face platform" and point to a replication package.</p>
<p>We are aware that the definition of an "open" model is actively being discussed, and many open models are essentially only "open weight" [12].We consider the Open Source AI Definition proposed by the Open Source Initiative (OSI) to be a first step towards defining true open-source models [24].</p>
<p>F. Use Human Validation for LLM Outputs</p>
<p>Especially in studies where LLMs are used to support researchers, human validation should always be employed.While LLMs can automate many tasks, it is important to validate their outputs with human annotations, at least partially.For natural language processing tasks, a large-scale study has shown that LLMs have too large a variation in their results to be reliably used as a substitution for human judges [25].Human validation helps ensure the accuracy and reliability of the results, as LLMs may sometimes produce incorrect or biased outputs.Incorporating human judgment in the evaluation process adds a layer of quality control and increases the trustworthiness of the study's findings, especially when explicitly reporting inter-rater reliability metrics.For instance, "A subset of 20 % of the LLM-generated annotations were reviewed and validated by experienced software engineers to ensure accuracy.An inter-rater reliability of 90 % was reached."For studies using LLMs as annotators, the proposed process by Ahmed et al. [26], which includes an initial fewshot learning and, given good results, the replacement of one human annotator by an LLM, might be a way forward.</p>
<p>V. CONCLUSIONS</p>
<p>In this paper, we outlined preliminary guidelines for researchers reporting on empirical studies involving Large Language Models (LLMs) in software engineering research.While researchers can already use these guidelines to improve the reproducibility of their studies and the reporting in their papers, we see a demand for more tailored guidelines focusing on the different study types we identified in Section III and also for extending the guidelines to include missing aspects such as ethical implications of using LLMs in research [27].One aspect to focus on could be the particular types of threats to validity that arise from using LLMs in the context of different study types, building on related work [13].Another direction could be to conduct a critical review of published studies involving LLMs, assessing how many papers already adhere to the guidelines we suggest.
 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
https://isern.fraunhofer.de
Open AI Cookbook: The new seed parameter</p>
<p>Software engineering for AI-based systems: A survey. S Martínez-Fernández, J Bogner, X Franch, M Oriol, J Siebert, A Trendowicz, A M Vollmer, S Wagner, ACM TOSEM. 3122022</p>
<p>Large language models for software engineering: Survey and open problems. A Fan, B Gokkaya, M Harman, M Lyubarskiy, S Sengupta, S Yoo, J M Zhang, ICSE 2024: Future of Software Engineering (ICSE-FoSE). IEEE2023</p>
<p>Large language models for software engineering: A systematic literature review. X Hou, Y Zhao, Y Liu, Z Yang, K Wang, L Li, X Luo, D Lo, J Grundy, H Wang, Sept. 2024ACM TOSEM</p>
<p>Exploring the effectiveness of LLMs in automated logging statement generation: An empirical study. Y Li, Y Huo, Z Jiang, R Zhong, P He, Y Su, L C Briand, M R Lyu, IEEE TSE. 2024</p>
<p>Are we testing or being tested? exploring the practical applications of large language models in software testing. R Santos, I Santos, C Magalhaes, R De Souza, Santos , 2024in ICST 2024</p>
<p>Leveraging open source LLMs for software engineering education and training. J Pereira, J.-M López, X Garmendia, M Azanza, CSEE&amp;T 2024. 2024</p>
<p>Large language models for qualitative research in software engineering: exploring opportunities and challenges. M Bano, R Hoda, D Zowghi, C Treude, ASE Journal. 31182024</p>
<p>C Wohlin, P Runeson, M Höst, M C Ohlsson, B Regnell, A Wesslén, Experimentation in Software Engineering. Springer2024</p>
<p>Reporting experiments in software engineering. A Jedlitschka, M Ciolkowski, D Pfahl, Guide to Advanced ESE. F Shull, J Singer, D I K Sjøberg, Springer2008</p>
<p>A procedure and guidelines for analyzing groups of software engineering replications. A Santos, S Vegas, M Oivo, N Juristo, IEEE TSE. 4792021</p>
<p>How is ChatGPT's behavior changing over time?. L Chen, M Zaharia, J Zou, abs/2307.09009CoRR. 2023</p>
<p>Not all 'open source' AI models are actually open. E Gibney, Nature News. 2024</p>
<p>Breaking the silence: the threats of using LLMs in software engineering. J Sallou, T Durieux, A Panichella, ICSE 2024 NIER. 2024</p>
<p>TnT-LLM: Text mining at scale with large language models. M Wan, T Safavi, S K Jauhar, Y Kim, S Counts, J Neville, S Suri, C Shah, R W White, L Yang, ACM KDD 2024. 2024</p>
<p>Human-LLM collaborative annotation through effective verification of llm labels. X Wang, H Kim, S Rahman, K Mitra, Z Miao, CHI 2024. 2024</p>
<p>Why AI's diversity crisis matters, and how to tackle it. R Crowell, Nature Career Feature. 2023</p>
<p>AI language models cannot replace human research participants. J Harding, W D'alessandro, N G Laskowski, R Long, 2024AI Soc39</p>
<p>How far are we? The triumphs and trials of generative AI in learning software engineering. R Choudhuri, D Liu, I Steinmacher, M Gerosa, A Sarma, ICSE 2024. 2024</p>
<p>Beyond code generation: An observational study of ChatGPT usage in software engineering practice. R Khojah, M Mohamad, P Leitner, F G De Oliveira, Neto, PACMSE. 12024FSE</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021</p>
<p>Contamination report for multilingual benchmarks. S Ahuja, V Gumma, S Sitaram, 2024</p>
<p>Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks. C Siska, K Marazopoulou, M Ailem, J Bono, ACL 2024 (Long Papers). 2024</p>
<p>Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues. Y Liu, T Le-Cong, R Widyasari, C Tantithamthavorn, L Li, X.-B D Le, D Lo, June 2024ACM TOSEM</p>
<p>Open Source AI Definition 1.0. 2024-11-11OSI</p>
<p>LLMs instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. A Bavaresco, R Bernardi, L Bertolazzi, D Elliott, R Fernández, A Gatt, E Ghaleb, M Giulianelli, M Hanna, A Koller, arXiv:2406.184032024arXiv preprint</p>
<p>Can LLMs replace manual annotation of software engineering artifacts?. T Ahmed, P Devanbu, C Treude, M Pradel, arXiv:2408.055342024arXiv preprint</p>
<p>E L Ungless, N Vitsakis, Z Talat, J Garforth, B Ross, A Onken, A Kasirzadeh, A Birch, arXiv:2410.19812Ethics whitepaper: Whitepaper on ethical research into large language models. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
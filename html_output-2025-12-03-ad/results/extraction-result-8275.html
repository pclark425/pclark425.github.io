<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8275 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8275</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8275</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-265149412</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07491v1.pdf" target="_blank">A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations. Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response. To address this issue, we introduce the"Decompose-and-Query"framework (D&Q). This framework guides the model to think and utilize external knowledge similar to ReAct, while also restricting its thinking to reliable information, effectively mitigating the risk of hallucinations. Experiments confirm the effectiveness of D&Q: On our ChitChatQA dataset, D&Q does not lose to ChatGPT in 67% of cases; on the HotPotQA question-only setting, D&Q achieved an F1 score of 59.6%. Our code is available at https://github.com/alkaidpku/DQ-ToolQA.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8275.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8275.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D&Q</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decompose-and-Query</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constrained multi-stage question decomposition and tool-invocation framework that forces LLMs to consult a curated reliable QA base, performs depth-first decomposition with backtracking, and issues limited retriever calls to reduce hallucinations and improve retrieval accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA2-13B backbone model which the authors supervised-fine-tuned with annotated solution trajectories (multi-turn dialogue format) to learn tool-invocation, decomposition, and backtracking behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['constrained question decomposition (depth-first search)', 'retrieval-augmented reasoning (tool invocation: QuestionRetriever/AnswerRetriever/ArticleRetriever/PageRetriever)', 'backtracking (rollbacks to previous states / non-greedy search)', 'supervised imitation of annotated solution trajectories']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model decomposes questions into subquestions and iteratively queries a reliable QA base through designated tool calls; it stores intermediate states and can backtrack (depth-first search style) when a retrieval path fails. The model was trained by supervised fine-tuning on annotated trajectories (no tool calls during SFT) and invokes tools at inference. The architecture constrains answers to prefer the reliable QA base when conflicts arise.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared as a method vs multiple baselines (CoT, ReAct, Self-Ask, IRCoT) on HotPotQA; ablation on retrieval ability (recall) comparing direct prompt baseline vs D&Q; qualitative comparison of backtracking vs ReAct's greedy behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>HotPotQA (question-only setting) for multi-hop QA; ChitChatQA (new real-dialog dataset from KwaiTalk) for decomposition and conversational QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>HotPotQA (question-only): EM 39.0, F1 59.6 (Table 1). ChitChatQA human evaluation vs KwaiDialog: win/tie/lose = 37%/36%/27% (not losing in 73% of cases). ChitChatQA human evaluation vs ChatGPT: win/tie/lose = 24%/38%/38% (not losing in 62% of cases). Retrieval recall (ablations): baseline recall 52.3% -> D&Q recall 68.8% (text reports recall increase from 52.3% to 68.8% when using context-aware tool invocation; table also lists direct prompt: 26.7 -> D&Q 50.0 for a related measure). Retriever call budget: up to 10 calls, up to 5 entries per call (max 50 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Backtracking (storing intermediate states and retreating on failure) substantially enhances retrieval capability compared to greedy tool-invocation (the paper states ReAct tends to random walks and lacks explicit backtracking). Constraining the model to a reliable QA base reduces hallucinations. Supervised fine-tuning to learn tool invocation allows a smaller model (13B) to approach performance of much larger models that do not use constrained tool-invocation.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Constrained decomposition with controlled tool invocation and backtracking (D&Q) reduces hallucinations and improves retrieval and multi-hop QA performance; backtracking is an important mechanism for successful retrieval and tool use; tool invocation can compensate for parameter size limitations (13B model achieves competitive results vs much larger baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8275.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8275.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step intermediate reasoning chains from LLMs to improve complex reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 540B (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 540B large transformer model reported in prior work; used in the paper's comparison table as the CoT baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate explicit intermediate reasoning steps (chains) during decoding; often used with temperature sampling or few-shot prompting to elicit multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Included as a baseline in Table 1 (compared to D&Q and other methods on HotPotQA). No controlled ablation in this paper specifically varying diversity of CoT vs alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>HotPotQA (question-only setting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in Table 1 (from literature): EM 30.0, F1 44.1 on HotPotQA (PaLM540B with CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT improves over standard prompting but is outperformed by the D&Q constrained decomposition + tool use approach on HotPotQA per the table.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT is a strong reasoning baseline but D&Q's tool-invocation and backtracking enable better retrieval and higher final QA performance on HotPotQA in the reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8275.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8275.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Ask (decomposition + search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where the model asks sub-questions to itself, uses a search engine to answer them, and then composes the final answer from retrieved subanswers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT 175B (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InstructGPT (GPT-3 instruction-tuned family) with 175B parameters reported as the Self-Ask baseline in the comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['self-question decomposition', 'search-augmented composition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model decomposes a complex question into sub-questions, queries an external search engine for each, and composes the final answer from retrieved results.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Listed as a baseline in Table 1 compared on HotPotQA; no in-paper ablation varying decomposition diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>HotPotQA (question-only setting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in Table 1 (from literature): EM 36.8, F1 55.2 (InstructGPT 175B with Self-Ask).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-Ask relies on the model's decomposition quality and on retrieval; it performs well but is slightly behind IRCoT and D&Q in reported F1.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-Ask is a competitive decomposition + retrieval baseline but D&Q's constrained decomposition and backtracking yield higher retrieval recall and final performance on HotPotQA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8275.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8275.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning and Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that interleaves reasoning (chain-of-thought style) with actions (tool calls) so that the model alternates thought and tool invocation to gather evidence and act.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 540B (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 540B as reported in the comparison table; ReAct is referenced and used as a conceptual baseline for tool-interactive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['interleaved reasoning + tool invocation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Alternate internal reasoning steps with external actions (e.g., search or lookup) so thoughts inform subsequent tool calls, and observations inform further reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single-threaded interleaving)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared conceptually and in performance table (Table 1) against D&Q and others on HotPotQA; paper includes qualitative comparison noting ReAct's action space is limited and it lacks explicit backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>HotPotQA (question-only)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in Table 1 (from literature): EM 24.9, F1 44.7 (PaLM540B with ReAct).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper argues ReAct is effectively greedy in practice and tends toward random walks without explicit backtracking; D&Q's stored intermediate states and rollback capability improve retrieval over ReAct's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>While ReAct allows tool interactions, its lack of explicit backtracking and limited action space leads to weaker retrieval and QA performance compared to D&Q's backtracking-enabled decomposition and constrained queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8275.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8275.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interleaving Retrieval with Chain-of-Thought (IRCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that generates chain-of-thought reasoning, interleaves retrieval calls to augment context, and then refines the final answer using retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Code-Davinci-002 (GPT-3, 175B) as reported</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>IRCoT implementation reported using GPT-3 (Code-Davinci-002) in prior work; used as a high-performing baseline in the authors' comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought + interleaved retrieval']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generates CoT, issues retrieval queries during reasoning steps, and conditions final generation on question plus retrieved references.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (CoT plus retrieval augmentations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Included as a baseline in Table 1 on HotPotQA; paper notes that D&Q (13B, fine-tuned) achieves comparable results to IRCoT which uses a much larger model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>HotPotQA (question-only)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in Table 1 (from literature): F1 60.7 (IRCoT with Code-Davinci-002 175B). EM not listed in table for IRCoT.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>IRCoT is a strong baseline demonstrating that interleaving retrieval with reasoning yields high F1; D&Q claims a similar performance with a much smaller fine-tuned model via constrained decomposition and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Interleaved retrieval with CoT is effective for knowledge-intensive multi-step questions; D&Q provides comparable F1 with a smaller model by learning constrained tool invocation and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8275.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8275.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple reasoning chains (diverse trajectories) and selects the most consistent final answer among them (majority voting) to improve robustness of CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['diverse sampling of reasoning paths', 'majority-vote aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Multiple CoT samples are decoded (often with temperature) to produce diverse reasoning chains; answers across samples are aggregated (self-consistency) to select the most consistent final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned in related work as an approach that samples diverse reasoning paths and aggregates; the current paper cites it as background but does not run a direct ablation vs self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Discussed generically for complex reasoning (no new experiments in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Cited as an existing approach to improve CoT, but the authors emphasize alternative approaches (D&Q backtracking and constrained tool use) for robustness and hallucination reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency samples diverse reasoning paths to improve robustness, but the paper focuses on constrained decomposition + backtracking as a complementary/alternative strategy to reduce hallucinations and improve retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8275.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8275.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that expands reasoning into a tree search where the model can evaluate, look ahead, and backtrack across candidate thought branches to improve decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['tree-structured search with lookahead and backtracking']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>At each reasoning step, the model proposes multiple candidate thoughts and uses self-evaluation or lookahead to choose branches; backtracking is allowed to recover from poor branches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned in related work as an approach that encourages multiple varied reasoning trajectories and backtracking; the paper positions D&Q as using backtracking in a constrained decomposition/search setting (conceptually related).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General reasoning/problem solving (mentioned conceptually; not experimentally evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Cited as relevant prior work demonstrating the value of non-greedy multi-trajectory search (backtracking); D&Q adopts backtracking in a retrieval-augmented decomposition context.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Tree-of-Thoughts highlights benefits of branching and backtracking; D&Q borrows the backtracking concept to improve retrieval and reduce greedy random walks inherent in simpler tool-invocation schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models. <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models. <em>(Rating: 2)</em></li>
                <li>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. <em>(Rating: 2)</em></li>
                <li>Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. <em>(Rating: 2)</em></li>
                <li>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8275",
    "paper_id": "paper-265149412",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "D&Q",
            "name_full": "Decompose-and-Query",
            "brief_description": "A constrained multi-stage question decomposition and tool-invocation framework that forces LLMs to consult a curated reliable QA base, performs depth-first decomposition with backtracking, and issues limited retriever calls to reduce hallucinations and improve retrieval accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B (fine-tuned)",
            "model_description": "LLaMA2-13B backbone model which the authors supervised-fine-tuned with annotated solution trajectories (multi-turn dialogue format) to learn tool-invocation, decomposition, and backtracking behaviors.",
            "reasoning_methods": [
                "constrained question decomposition (depth-first search)",
                "retrieval-augmented reasoning (tool invocation: QuestionRetriever/AnswerRetriever/ArticleRetriever/PageRetriever)",
                "backtracking (rollbacks to previous states / non-greedy search)",
                "supervised imitation of annotated solution trajectories"
            ],
            "reasoning_methods_description": "The model decomposes questions into subquestions and iteratively queries a reliable QA base through designated tool calls; it stores intermediate states and can backtrack (depth-first search style) when a retrieval path fails. The model was trained by supervised fine-tuning on annotated trajectories (no tool calls during SFT) and invokes tools at inference. The architecture constrains answers to prefer the reliable QA base when conflicts arise.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared as a method vs multiple baselines (CoT, ReAct, Self-Ask, IRCoT) on HotPotQA; ablation on retrieval ability (recall) comparing direct prompt baseline vs D&Q; qualitative comparison of backtracking vs ReAct's greedy behavior.",
            "task_or_benchmark": "HotPotQA (question-only setting) for multi-hop QA; ChitChatQA (new real-dialog dataset from KwaiTalk) for decomposition and conversational QA.",
            "performance_results": "HotPotQA (question-only): EM 39.0, F1 59.6 (Table 1). ChitChatQA human evaluation vs KwaiDialog: win/tie/lose = 37%/36%/27% (not losing in 73% of cases). ChitChatQA human evaluation vs ChatGPT: win/tie/lose = 24%/38%/38% (not losing in 62% of cases). Retrieval recall (ablations): baseline recall 52.3% -&gt; D&Q recall 68.8% (text reports recall increase from 52.3% to 68.8% when using context-aware tool invocation; table also lists direct prompt: 26.7 -&gt; D&Q 50.0 for a related measure). Retriever call budget: up to 10 calls, up to 5 entries per call (max 50 entries).",
            "qualitative_findings": "Backtracking (storing intermediate states and retreating on failure) substantially enhances retrieval capability compared to greedy tool-invocation (the paper states ReAct tends to random walks and lacks explicit backtracking). Constraining the model to a reliable QA base reduces hallucinations. Supervised fine-tuning to learn tool invocation allows a smaller model (13B) to approach performance of much larger models that do not use constrained tool-invocation.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Constrained decomposition with controlled tool invocation and backtracking (D&Q) reduces hallucinations and improves retrieval and multi-hop QA performance; backtracking is an important mechanism for successful retrieval and tool use; tool invocation can compensate for parameter size limitations (13B model achieves competitive results vs much larger baselines).",
            "uuid": "e8275.0",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step intermediate reasoning chains from LLMs to improve complex reasoning performance.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "mention_or_use": "mention",
            "model_name": "PaLM 540B (reported baseline)",
            "model_description": "PaLM 540B large transformer model reported in prior work; used in the paper's comparison table as the CoT baseline.",
            "reasoning_methods": [
                "chain-of-thought (CoT)"
            ],
            "reasoning_methods_description": "Generate explicit intermediate reasoning steps (chains) during decoding; often used with temperature sampling or few-shot prompting to elicit multi-step reasoning.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Included as a baseline in Table 1 (compared to D&Q and other methods on HotPotQA). No controlled ablation in this paper specifically varying diversity of CoT vs alternatives.",
            "task_or_benchmark": "HotPotQA (question-only setting)",
            "performance_results": "Reported in Table 1 (from literature): EM 30.0, F1 44.1 on HotPotQA (PaLM540B with CoT).",
            "qualitative_findings": "CoT improves over standard prompting but is outperformed by the D&Q constrained decomposition + tool use approach on HotPotQA per the table.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "CoT is a strong reasoning baseline but D&Q's tool-invocation and backtracking enable better retrieval and higher final QA performance on HotPotQA in the reported comparisons.",
            "uuid": "e8275.1",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Ask",
            "name_full": "Self-Ask (decomposition + search)",
            "brief_description": "A method where the model asks sub-questions to itself, uses a search engine to answer them, and then composes the final answer from retrieved subanswers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "InstructGPT 175B (reported baseline)",
            "model_description": "InstructGPT (GPT-3 instruction-tuned family) with 175B parameters reported as the Self-Ask baseline in the comparison table.",
            "reasoning_methods": [
                "self-question decomposition",
                "search-augmented composition"
            ],
            "reasoning_methods_description": "Model decomposes a complex question into sub-questions, queries an external search engine for each, and composes the final answer from retrieved results.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Listed as a baseline in Table 1 compared on HotPotQA; no in-paper ablation varying decomposition diversity.",
            "task_or_benchmark": "HotPotQA (question-only setting)",
            "performance_results": "Reported in Table 1 (from literature): EM 36.8, F1 55.2 (InstructGPT 175B with Self-Ask).",
            "qualitative_findings": "Self-Ask relies on the model's decomposition quality and on retrieval; it performs well but is slightly behind IRCoT and D&Q in reported F1.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Self-Ask is a competitive decomposition + retrieval baseline but D&Q's constrained decomposition and backtracking yield higher retrieval recall and final performance on HotPotQA.",
            "uuid": "e8275.2",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning and Acting)",
            "brief_description": "An approach that interleaves reasoning (chain-of-thought style) with actions (tool calls) so that the model alternates thought and tool invocation to gather evidence and act.",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
            "mention_or_use": "mention",
            "model_name": "PaLM 540B (reported baseline)",
            "model_description": "PaLM 540B as reported in the comparison table; ReAct is referenced and used as a conceptual baseline for tool-interactive reasoning.",
            "reasoning_methods": [
                "interleaved reasoning + tool invocation"
            ],
            "reasoning_methods_description": "Alternate internal reasoning steps with external actions (e.g., search or lookup) so thoughts inform subsequent tool calls, and observations inform further reasoning.",
            "reasoning_diversity": "similar (single-threaded interleaving)",
            "reasoning_diversity_experimental_setup": "Compared conceptually and in performance table (Table 1) against D&Q and others on HotPotQA; paper includes qualitative comparison noting ReAct's action space is limited and it lacks explicit backtracking.",
            "task_or_benchmark": "HotPotQA (question-only)",
            "performance_results": "Reported in Table 1 (from literature): EM 24.9, F1 44.7 (PaLM540B with ReAct).",
            "qualitative_findings": "Paper argues ReAct is effectively greedy in practice and tends toward random walks without explicit backtracking; D&Q's stored intermediate states and rollback capability improve retrieval over ReAct's approach.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "While ReAct allows tool interactions, its lack of explicit backtracking and limited action space leads to weaker retrieval and QA performance compared to D&Q's backtracking-enabled decomposition and constrained queries.",
            "uuid": "e8275.3",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "IRCoT",
            "name_full": "Interleaving Retrieval with Chain-of-Thought (IRCoT)",
            "brief_description": "A method that generates chain-of-thought reasoning, interleaves retrieval calls to augment context, and then refines the final answer using retrieved context.",
            "citation_title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.",
            "mention_or_use": "mention",
            "model_name": "Code-Davinci-002 (GPT-3, 175B) as reported",
            "model_description": "IRCoT implementation reported using GPT-3 (Code-Davinci-002) in prior work; used as a high-performing baseline in the authors' comparison table.",
            "reasoning_methods": [
                "chain-of-thought + interleaved retrieval"
            ],
            "reasoning_methods_description": "Generates CoT, issues retrieval queries during reasoning steps, and conditions final generation on question plus retrieved references.",
            "reasoning_diversity": "both (CoT plus retrieval augmentations)",
            "reasoning_diversity_experimental_setup": "Included as a baseline in Table 1 on HotPotQA; paper notes that D&Q (13B, fine-tuned) achieves comparable results to IRCoT which uses a much larger model.",
            "task_or_benchmark": "HotPotQA (question-only)",
            "performance_results": "Reported in Table 1 (from literature): F1 60.7 (IRCoT with Code-Davinci-002 175B). EM not listed in table for IRCoT.",
            "qualitative_findings": "IRCoT is a strong baseline demonstrating that interleaving retrieval with reasoning yields high F1; D&Q claims a similar performance with a much smaller fine-tuned model via constrained decomposition and backtracking.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Interleaved retrieval with CoT is effective for knowledge-intensive multi-step questions; D&Q provides comparable F1 with a smaller model by learning constrained tool invocation and backtracking.",
            "uuid": "e8275.4",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency",
            "brief_description": "A technique that samples multiple reasoning chains (diverse trajectories) and selects the most consistent final answer among them (majority voting) to improve robustness of CoT.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reasoning_methods": [
                "diverse sampling of reasoning paths",
                "majority-vote aggregation"
            ],
            "reasoning_methods_description": "Multiple CoT samples are decoded (often with temperature) to produce diverse reasoning chains; answers across samples are aggregated (self-consistency) to select the most consistent final answer.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Mentioned in related work as an approach that samples diverse reasoning paths and aggregates; the current paper cites it as background but does not run a direct ablation vs self-consistency.",
            "task_or_benchmark": "Discussed generically for complex reasoning (no new experiments in this paper).",
            "performance_results": null,
            "qualitative_findings": "Cited as an existing approach to improve CoT, but the authors emphasize alternative approaches (D&Q backtracking and constrained tool use) for robustness and hallucination reduction.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Self-consistency samples diverse reasoning paths to improve robustness, but the paper focuses on constrained decomposition + backtracking as a complementary/alternative strategy to reduce hallucinations and improve retrieval.",
            "uuid": "e8275.5",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Tree-of-Thoughts",
            "name_full": "Tree of Thought (ToT)",
            "brief_description": "A framework that expands reasoning into a tree search where the model can evaluate, look ahead, and backtrack across candidate thought branches to improve decision making.",
            "citation_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reasoning_methods": [
                "tree-structured search with lookahead and backtracking"
            ],
            "reasoning_methods_description": "At each reasoning step, the model proposes multiple candidate thoughts and uses self-evaluation or lookahead to choose branches; backtracking is allowed to recover from poor branches.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Mentioned in related work as an approach that encourages multiple varied reasoning trajectories and backtracking; the paper positions D&Q as using backtracking in a constrained decomposition/search setting (conceptually related).",
            "task_or_benchmark": "General reasoning/problem solving (mentioned conceptually; not experimentally evaluated in this paper).",
            "performance_results": null,
            "qualitative_findings": "Cited as relevant prior work demonstrating the value of non-greedy multi-trajectory search (backtracking); D&Q adopts backtracking in a retrieval-augmented decomposition context.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Tree-of-Thoughts highlights benefits of branching and backtracking; D&Q borrows the backtracking concept to improve retrieval and reduce greedy random walks inherent in simpler tool-invocation schemes.",
            "uuid": "e8275.6",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.",
            "rating": 2,
            "sanitized_title": "interleaving_retrieval_with_chainofthought_reasoning_for_knowledgeintensive_multistep_questions"
        },
        {
            "paper_title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.",
            "rating": 2,
            "sanitized_title": "enhancing_retrievalaugmented_large_language_models_with_iterative_retrievalgeneration_synergy"
        },
        {
            "paper_title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models.",
            "rating": 1,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        }
    ],
    "cost": 0.01513425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models
13 Nov 2023</p>
<p>Hejing Cao caohejing@pku.edu.cn 
Peking University</p>
<p>Equal Contribution</p>
<p>Zhenwei An anzhenwei@pku.edu.cn 
Peking University</p>
<p>Equal Contribution</p>
<p>Jiazhan Feng fengjiazhan@pku.edu.cn 
Peking University</p>
<p>Kun Xu 
Kuaishou Technology</p>
<p>Liwei Chen 
Kuaishou Technology</p>
<p>Dongyan Zhao zhaody@pku.edu.cn 
Peking University</p>
<p>A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models
13 Nov 2023D396FB7D1232D5E04CC61958BBBC75CBarXiv:2311.07491v1[cs.CL]
While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations.Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response.To address this issue, we introduce the "Decompose-and-Query" framework (D&amp;Q).This framework guides the model to think and utilize external knowledge similar to ReAct, while also restricting its thinking to reliable information, effectively mitigating the risk of hallucinations.Experiments confirm the effectiveness of D&amp;Q: On our ChitChatQA dataset, D&amp;Q does not lose to ChatGPT in 67% of cases; on the Hot-PotQA question-only setting, D&amp;Q achieved an F1 score of 59.6%.Our code is available at https://github.com/alkaidpku/DQ-ToolQA .</p>
<p>Introduction</p>
<p>Large language models (LLMs), such as ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), Bard (Google, 2023), have made significant inroads into people's work and daily life, substantially enhancing productivity.For LLMs, to produce highquality response for a complex question relies on two essential capabilities: 1) Understanding multihop reasoning in complex questions; 2) Employing external knowledge.</p>
<p>On the one hand, many questions in our daily lives manifest as multi-hop questions.Individuals often pose intricate questions or instructions within a single sentence, lacking necessary descriptions.This necessitates LLMs to proficiently decompose complex questions into multiple subquestions.One potential solution involves employing a sub-question decomposition approach, such as self-ask (Press et al., 2022), which decomposes the question into sub-questions, resolves them independently, and consolidates the answers.However, this approach relies on LLMs' ability to reasonably decompose questions and is typically more suitable for high-quality prompts, such as HotPotQA (Yang et al., 2018) and 2WikiMultiHopQA (Ho et al., 2020).Challenges may arise when dealing with low-quality or unclear prompts.</p>
<p>On the other hand, large language models are prone to hallucinations (Zhang et al., 2023b) and do not have access to up-to-date knowledge (Mallen et al., 2023).Integrating external tools (search engines, knowledge bases, APIs, and etc) with LLMs is a viable approach to address these challenges.Least-to-most (Zhou et al., 2022) decomposes complex question into sub-questions, ultimately leading to a comprehensive answer.However, answering these sub-questions may also bring hallucinations.ReAct (Yao et al., 2022) bridges the gap of reasoning and action, enabling thoughtful interactions with tools.It is noteworthy that the action space of ReAct is limited, and its effectiveness is relatively modest.Shao et al. (2023a) introduced Iter-RetGen, which iteratively follows a two-step process: retrieving relevant knowledge, and generating answer.The approach of Iter-RetGen bears similarities to Self-Refine (Madaan et al., 2023).However, Iter-RetGen lacks autonomous termination capabilities.</p>
<p>To this, we have proposed a constrained question decomposition strategy that transforms question decomposition into a "Decompose-and-Query" depthfirst search framework.Specifically, we construct a reliable question-answer base, which consists of high-quality question-answer pairs.The model is required to consult it to generate answers for search efficiency.In case of conflicts between the model's knowledge and the reliable QA base, the base's content takes precedence.</p>
<p>We annotated solution trajectories for fine-tuning the model and subsequent predictions.The supervised fine-tuning phase does not involve tool in-vocation; the model processes static dialogue text, learning strategies for invoking tools.During the prediction phase, the model calls external tools to query a reliable QA base.If the LLM realizes during the search process that it has pursued an incorrect path, it can backtrack to previous steps and initiate a new search.Experiments confirm the effectiveness of D&amp;Q: On our ChitChatQA dataset, D&amp;Q does not lose to ChatGPT in 67% of cases; on the HotPotQA question-only setting, D&amp;Q achieved an F1 score of 59.6%.</p>
<p>Our contributions are the following:</p>
<ol>
<li>
<p>We build and make public a dataset of real conversation scenarios: ChitChatQA.</p>
</li>
<li>
<p>We design a framework for LLMs using external tools to perform constrained problem decomposition.Although simple, it can theoretically be extended to any number of external tools.</p>
</li>
<li>
<p>We achieve an F1 score of 59.6% on the question-only setting on HotPotQA dataset.</p>
</li>
</ol>
<p>Related Works</p>
<p>Reasoning.The capacity for logical reasoning is a significant hallmark of intelligence in large language models.Wei et al. (2022) have proposed the Chain-of-Thought (CoT) as an effective means to enhance the reasoning abilities of large language models consistently and efficiently.However, incorrect reasoning paths often lead to incorrect reasoning results.To this, Wang et al. (2022) suggest self-consistency, which samples a diverse set of reasoning paths and selects the most consistent answer.Self-consistency, despite its higher decoding cost compared to CoT, still struggles with challenging problem-solving.Yao et al. (2023) introduce a novel approach by extending CoT into Tree-of-Thought (ToT).At each step, the model engages in self-evaluation and, when necessary, looks ahead or backtracks to enhance decision-making abilities.</p>
<p>Similarly, Graph of Thoughts (GoT) framework models information generated by large language models as a graph, allowing for the combination of various LLM thoughts to yield collaborative results, extract insights from the entire thought network, and employ feedback loops to enhance thinking (Besta et al., 2023).Cumulative reasoning (Zhang et al., 2023a) employs proposers, verifiers, and reporters to store intermediate results, facilitating the accumulation of the reasoning process.So far, the reasoning capabilities of large language models have remained understudied.</p>
<p>Augmented Language Models.Large language models are prone to generating hallucinations and do not have access to up-to-date knowledge.Large language models are likely to make factual errors with long-tail knowledge (Kandpal et al., 2022).As a consequence, more and more research is focusing on the use of external tools to enhance large language models.Retrieval-based language model is a classic augmented language model.The idea is to use discrete or dense retrievers to retrieve related context, and then merge the retrieved context into the input.It is possible to match certain capabilities of some of the largest LMs while having less parameters (Schick et al., 2023;Qin et al., 2023).Along with the research on CoT, many studies have integrated external tools into the chain of thought paradigm (Yao et al., 2022;Paranjape et al., 2023;Feng et al., 2023;Press et al., 2022;Shao et al., 2023b;Trivedi et al., 2022;Gou et al., 2023a,b).Synergizing reasoning and invocation tools in LLMs is an underexplored potential research domain.</p>
<p>Methodology</p>
<p>This section presents our methodology.We construct ChitChatQA and the reliable QA base, annotate problem-solving trajectories and train a model to invoke tools.</p>
<p>ChitChatQA dataset</p>
<p>We introduce a novel dataset, ChitChatQA, to assess the capabilities of large language models in handling complex and diverse questions.Specifically, we collected single-turn dialogue data from real scenarios in the KwaiTalk application.The dataset comprises authentic questions posed by users and responses generated by large language models.We denote the original question-answer data as S 0 = (q i , a i ) N 0 .We performed data cleaning on the collected question-answer dataset to eliminate noise.Specifically, we initially utilized a grammatical correction model to filter out questions with low scores, followed by an intent recognition model to screen for questions with ambiguous intent.During the data cleaning process, we applied a lower intent recognition threshold to retain data with unclear intents.The detailed procedure for data cleaning can be found in Algorithm 1. Algorithm 1 Reliable QA Database Creation Require: S 0 ,  1 ,  2 , GEC, Intent Ensure: S 1: S = new Set() 2: for q i in S 0 do 3:</p>
<p>if GEC(q i ) &gt;  1 and Intent(q i ) &gt;  2 then 4:</p>
<p>S.Add(q i ) 5:</p>
<p>end if 6: end for 7: return S</p>
<p>Reliable QA Base</p>
<p>We constructed a reliable question-answer base, which consists of high-quality question-answer pairs.Similar to Constitutional AI (Bai et al., 2022), all question-answer pairs are considered beyond doubt.The model is required to consult the database to generate answers.In case of conflicts between the model's knowledge and the reliable QA base, the base's content takes precedence.We believe that this makes the model less prone to hallucinations and more robust.</p>
<p>In dialogue system, users may inquire about similar questions, such as "please provide me with a Python code for Bubble Sort" or "tell me the names of the Eight Great Masters of the Tang and Song dynasties".These generic questions are frequent.Preliminary analysis shows that these questions exhibit high quality.Therefore, we selected the top 50,000 questions based on their frequency as benchmark questions for the reliable QA base.</p>
<p>Due to the fact that the pretraining data for large language models comprises noise, the outputs for the same question may vary.We categorized the questions into several types.Objective factual questions demand precise and unique responses.For these questions, we employ a majority voting approach to determine the response.Subjective opinion questions pertain to opinions or perspectives on certain matters, and for such questions, we use an opinion aggregation method to determine the response.Consequently, we designed an answer aggregation model to consolidate diverse viewpoints.</p>
<p>The answer aggregation model is essentially a large language model through supervised finetuning.We manually annotated 5,000 answer ag- For examples of answer aggregation, please refer to Appendix A.</p>
<p>Trajectory Annotation</p>
<p>To train the model to invoke external tools, we need gather a sufficient amount of data for supervised fine-tuning.Both ChatGPT and GPT-4 exhibit excellent performance in data annotation (Ubani et al., 2023).Based on preliminary experiments, the annotation capabilities of ChatGPT are significantly inferior to those of GPT-4.The annotation framework is illustrated in Figure 1.</p>
<p>Adaptive Constraint Question Decomposition</p>
<p>We employ imitation learning using data that involves multiple iterations of calling an external retriever for fine-tuning large language models.We use LLaMA2-13b as base model.The training is conducted in a multi-turn dialogue format, wherein, in each training round, only the output of that round's dialogue is retained, and the rest is masked.For more details, please refer to the Experimental Section.</p>
<p>Experiments</p>
<p>Our primary experiments consist of two parts.To evaluate question decomposition capability, experiments were conducted on the ChitChatQA dataset.</p>
<p>To assess the effectiveness of our approach in answering multi-hop questions, experiments were conducted using the academic dataset HotPotQA.</p>
<p>ChitChatQA</p>
<p>We utilized a set of 200 training instances and conducted supervised fine-tuning in a multi-turn dialogue format on LLaMA2-13b.The supervised fine-tuning phase does not involve tool invocation; the model processes static dialogue text, learning strategies for invoking tools.During the prediction phase, the model calls external tools to query the reliable QA base.We defined three tools:</p>
<p>[QuestionRetriever] for retrieving related questions from the QA base, [AnswerRetriever] for retrieve answer to a question, and [Finish] indicating task termination.ChitChatQA's data stems from real dialogue interactions with the Kuaishou dialogue model, with a test set comprising 100 instances.We employed the Kuaishou dialogue model's output as a baseline, noted as "KwaiDialog", and compared it with Chat-GPT's output.Model evaluation was conducted through human assessments labeled as win/tie/lose, as shown in  ReAct Utilizes Search and Lookup tools iteratively to provide external information for large language models (Yao et al., 2022).</p>
<p>Self-Ask Requires the model to ask itself and use the answers by the search engine to enhance the model (Press et al., 2022).</p>
<p>IRCoT Decomposes questions through Chain-of-Thought, retrieves results using a retriever, then concatenates the question and reference context to a more robust language model (GPT-3).</p>
<p>Implementation Details</p>
<p>We conducted experiments on the HotPotQA dataset.Similar to most baseline models, our configuration was question-only: providing only the question without accompanying references, allowing the model to autonomously invoke external tools for information retrieval.We defined three tools: [ArticleRetriever] for querying entries in Wikipedia, [PageRetriever] for retrieving pages corresponding to the entries, and [Finish] representing task termination.The specific implementations of these tools were achieved through calls to the MediaWiki * API.We utilized 200 training instances and performed supervised fine-tuning in a multi-turn dialogue format on LLaMA2-13b.</p>
<p>Results</p>
<p>Table 1 shows the D&amp;Q result of HotPotQA.We can draw the following conclusions: Backtracking enhances the model's retrieval capability.Although ReAct supports acquiring reference information through observation, this approach is, in practice, greedy.Specifically, ReAct does not explicitly backtrack but tends toward random walks.In contrast, D&amp;P stores intermediate states, allowing it to retreat to the previous state in case of retrieval failure, significantly enhancing retrieval capability.</p>
<p>Tool invocation can compensate for parameter limitations.In comparison to IRCoT's GPT-3 (175B), our generative model is LLaMA-13B.Through supervised fine-tuning, D&amp;Q can achieve results comparable to IRCoT.This indicates that smaller models, through supervised fine-tuning to learn tool invocation, can achieve performance comparable to larger models.</p>
<p>Ablation Study</p>
<p>Retrieval Ability</p>
<p>Method</p>
<p>Average Contexts Recall direct prompt 26.7 52.3 D&amp;Q 50.0 68.8</p>
<p>Table 3: Recall of retrieval.</p>
<p>We limit the retriever's calls to a maximum of 10, with each call returning up to 5 relevant entries.Therefore, the model can return a maximum * https://www.mediawiki.org/wiki/API:Search/ of 50 entries when generating responses.We calculate the recall by retrieving 50 entries using the initial question as a baseline.We tested the recall of D&amp;Q's retrieval ability and compared it with the baseline, resulting in an increase from 52.3% to 68.8%.This indicates that considering context contributes to more accurate tool invocation.</p>
<p>Conclusion</p>
<p>In conclusion, we propose the "Decompose-and-Query" framework (D&amp;Q) to address the susceptibility of large language models to hallucinations by guiding them to utilize external knowledge within a constrained and reliable context.Through experiments on the ChitChatQA dataset and the Hot-PotQA question-only setting, D&amp;Q demonstrated competitive performance against existing models.Our contributions include the introduction of the ChitChatQA dataset, a novel framework for constrained problem decomposition, and the achievement of a notable F1 score on HotPotQA.These findings underscore the potential of D&amp;Q in enhancing the robustness and performance of large language models in question-answering tasks.</p>
<p>Question: What is the Qixi Festival?Answer 1: The Qixi Festival, also known as the "Cowherd and Weaver Girl Festival" or "Chinese Valentine's Day," is a traditional Chinese festival.It originated from the ancient myth of the Cowherd and Weaver Girl, where the Weaver Girl, said to be the seventh daughter of the Queen Mother of the West, fell in love with the Cowherd.However, they separated by the Milky Way could only meet once a year on seventh day of the seventh lunar month.This festival is primarily associated with praying for love, reunion, and happiness.On Qixi Festival, people engage in various celebratory activities such as admiring lanterns, attending lantern festivals, paying respects to the Weaver Girl, and eating Qiao Guo (a type of pastry).Nowadays, Qixi Festival has also become a day for young people to express love and create a romantic atmosphere.Answer 2: The Qixi Festival, also known as the "Cowherd and Weaver Girl Festival" or "Chinese Valentine's Day," is a traditional Chinese festival.It originated from ancient star worship, with the legend of the Cowherd (Altair, the Cowherd Star) and the Weaver Girl (Vega, the Weaver Star) meeting in the Milky Way.This story symbolizes people's yearning for and wishes for beautiful love.Qixi Festival is typically celebrated on the seventh day of the seventh lunar month, and activities on this day include praying for love, couples' reunions, and gift-giving.One of the most famous activities is "Bai Qijie," a ritual to pray for wisdom and craftsmanship from the Weaver Girl.Additionally, there are traditional customs like flying kites, stargazing, and enjoying lanterns.Answer 3: The Qixi Festival, also known as the "Begging for Skills Festival" or "Daughter's Day," is a traditional Chinese festival rooted in ancient astronomical worship.Celebrated on the seventh day of the seventh lunar month, the day is believed to be when the Cowherd and Weaver Girl meet on the Magpie Bridge.On this day, people engage in various customs to pray for fulfilling love, such as praying to the Weaver Girl, begging for skills, paying respects to the Weaver Girl, and eating Qiao Guo.Simultaneously, Qixi Festival serves as China's Valentine's Day, where couples express love by exchanging gifts, having candlelit dinners, and more.Answer 4: The Qixi Festival is a traditional Chinese festival where people participate in various customs and activities, including folding peach branches, worshipping the Weaver Girl, rubbing rose of Sharon flowers, eating Qiao Guo, washing and arranging tools, dyeing fingernails, celebrating the birthday of the Cow, playing Qijie puzzles, threading needles for skill-begging, sowing seeds for birth, offering "Mo He Le," and more.Additionally, Qixi Festival is also considered China's Valentine's Day, with customs like begging for skills, worshiping immortals, and threading needles to pray for a happy marriage.Answer 5: The Qixi Festival is a traditional Chinese Valentine's Day, also known as "Daughter's Day" or "Qiaoqi Day," similar to Valentine's Day on February 14th.The seventh day of the seventh lunar month marks the meeting day of the Cowherd and Weaver Girl, a beautiful and eternal love story.Answer 6: The Qixi Festival is the most romantic traditional Chinese festival, primarily for young girls.The festival focuses on the Begging for Skills ceremony, hence also known as "Begging for Skills Festival," "Girl's Festival," or "Daughter's Festival."On this evening, women engage in needle threading to pray for wisdom and craftsmanship, worshipping the Weaver Girl with sincere and solemn ceremonies.The festival is characterized by displaying flowers, fruits, and needlework, with various activities like making small dolls.Answer 7: The Qixi Festival is a traditional Chinese festival where people perform various rituals and activities, such as needle threading for skills, spider-themed joyous activities, needle testing for skills, seed sowing for birth, offering "Mo He Le," worshiping the Weaver Girl, worshiping the Star of Marriage, sunning books, sunning clothes, celebrating the Cow's birthday, eating Qiao Guo, and more.Additionally, Qixi Festival features various small dolls displayed at home, with needle threading contests being one of the earliest popular activities, starting from the Han Dynasty.On Qixi Festival, women and girls gather under grape arbors, guessing what the Cowherd and Weaver Girl might say in secret while showcasing their needlework.The one who threads the needle the fastest is considered the most skillful.Answer 8: The Qixi Festival is a traditional Chinese folk festival, celebrating the birth of the Seventh Sister in traditional beliefs.It is a festival of both worshiping the Seventh Sister and celebrating love, centered around the folk tale of the Cowherd and Weaver Girl.Traditional customs of Qixi Festival include worshipping the Seventh Sister, making wishes, begging for skills, observing the Cowherd and Weaver Girl stars, praying for marriage, storing Qixi water, and more.On Qixi Festival, groups of women and girls gather, set up fruit offerings, bathe and fast, burn incense, and pray to the Weaver Girl, making wishes like marrying a good husband and having children.Answer 9: The Qixi Festival is a traditional Chinese Valentine's Day, a festival for couples.On this day, people pray for skillfulness and make wishes.Additionally, people light fireworks in the evening, eat sweets, change to new bedding, and display photos.Answer 10: The Qixi Festival is a traditional Chinese festival with many customary activities, including folding peach branches, worshipping the Weaver Girl, rubbing rose of Sharon flowers, eating Qiao Guo, washing and arranging tools, dyeing fingernails, celebrating the Cow's birthday, piecing together Qijie puzzles, threading needles for skill-begging, sowing seeds for birth, offering "Mo He Le," and more.Moreover, the Qixi Festival is China's Valentine's Day, and people engage in activities such as threading needles to pray for skillfulness, offering prayers for good fortune and longevity, worshipping the Weaver Girl, displaying flowers and needlework, and using beautifully crafted furniture and utensils.</p>
<p>Viewpoint: The Qixi Festival is a traditional Chinese festival, originating from the myth of the Cowherd and Weaver Girl, a day for praying for love, reunion, and happiness.Answer IDs: Answer 1, Answer 2, Answer 3 Viewpoint: The Qixi Festival is a traditional Chinese festival where people engage in various customs, such as worshiping the Weaver Girl and threading needles for skill-begging, praying for skillfulness, and achieving a happy marriage.Answer IDs: Answer 4, Answer 7, Answer 10 Viewpoint: The Qixi Festival is China's traditional Valentine's Day, a festival for couples to express love in various ways.Answer IDs: Answer 5, Answer 9 Viewpoint: The Qixi Festival is the most romantic traditional Chinese festival, primarily for young girls, with activities centered around begging for skills.Answer ID: Answer 6 Viewpoint: The Qixi Festival is a traditional Chinese folk festival, celebrating the birth of the Seventh Sister in traditional beliefs.It is a comprehensive festival with themes of praying for good fortune, begging for skills, and love, with women as the main participants.</p>
<p>Answer ID: Answer 8</p>
<p>Figure 1 :
1
Figure 1: Example of question decomposition trajectory.Q: Question; I: Iteration.Dashed line means rollback.We use Ques_Ret for QuestionRetriever and Ans_Ret for AnswerRetriever for brevity.</p>
<p>Figure 2 :
2
Figure 2: Model architecture of HotPotQA.</p>
<p>Table 1 .
1MethodGeneration Model Model Size EM F1StandardPaLM540B21.9 36.8CoT (Wei et al., 2023)PaLM540B30.0 44.1ReAct (Yao et al., 2022)PaLM540B24.9 44.7Self-Ask (Press et al., 2022) InstructGPT175B36.8 55.2IRCoT (Trivedi et al., 2022) Code-Davinci-002 175B-60.7D&amp;QLLaMA213B39.0 59.6</p>
<p>Table 1 :
1
Results on HotPotQA dataset.EM: Exact Match.
The results indicate a significant enhancement inthe response capability of the Base model with theD&amp;Q approach. Even after complex prompt fine-tuning and reinforcement learning for KwaiDialog,our method still achieves victory in 37% of testcases (not losing in 73%). Compared to ChatGPT,D&amp;Q maintains a competitive position, not losingin 62% of cases.wintieloseD&amp;Q vs KwaiDialog 37% 36% 27%D&amp;Q vs ChatGPT24% 38% 38%</p>
<p>Table 2 :
2
Human evaluation on ChitChatQA
4.2 HotPotQA4.2.1 Baselines</p>
<p>. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom HenighanNova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston; Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brownand Jared Kaplan. 2022. Constitutional AI: Harmlessness from AI Feedback</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, 2023</p>
<p>Knowledge Refinement via Interaction Between Search Engines and Large Language Models. Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, Daxin Jiang, 2023Google. 2023. Google bard</p>
<p>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, 2023a</p>
<p>ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, 2023b</p>
<p>Constructing a multihop QA dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, 10.18653/v1/2020.coling-main.580Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Large Language Models Struggle to Learn Long-Tail Knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, 2022</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, 2023</p>
<p>When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 10.48550/arXiv.2212.10511ArXiv:2212.105112023</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>ART: Automatic multistep reasoning and tool-use for large language models. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , 2023Ofir PressMuru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smithand Mike Lewis. 2022. Measuring and Narrowing the Compositionality Gap in Language Models</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun, ToolLLM: Facilitating Large Language Models to Master 16000+ Realworld APIs. 2023</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. </p>
<p>Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, 2023a</p>
<p>Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, 2023b</p>
<p>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, 2022</p>
<p>Zeroshotdataaug: Generating and augmenting training data with chatgpt. Solomon Ubani, Suleyman Olcay Polat, Rodney Nielsen, 2023</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2022</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, ReAct: Synergizing Reasoning and Acting in Language Models. 2022</p>
<p>Cumulative Reasoning with Large Language Models. Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao, 2023a</p>
<p>Siren's song in the ai ocean: A survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023barXiv preprint</p>
<p>Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. </p>
<p>. Aggregation Answer,  1""""   2""""() () ""  3   4   5""""214  6""" """  7""   8""      9  10    id123  Data, , </p>            </div>
        </div>

    </div>
</body>
</html>
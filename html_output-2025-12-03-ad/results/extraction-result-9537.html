<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9537 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9537</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9537</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-259991771</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.10549v1.pdf" target="_blank">Dynamic Large Language Models on Blockchains</a></p>
                <p><strong>Paper Abstract:</strong> Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also sheds a light on the next generation artificial intelligence systems.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9537",
    "paper_id": "paper-259991771",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003182,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dynamic Large Language Models on Blockchains
AUGUST 2015 1</p>
<p>Journal Of L A T E X Class 
Files 
Dynamic Large Language Models on Blockchains
148AUGUST 2015 1Index Terms-language modelblockchaindecentralizeneu- ral networkAIGC
Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamperproof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also shed a light on the next generation artificial intelligence systems.</p>
<p>Abstract-Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamperproof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also shed a light on the next generation artificial intelligence systems.</p>
<p>Index Terms-language model; blockchain; decentralize; neural network; AIGC</p>
<p>I. INTRODUCTION</p>
<p>The development of artificial intelligence and machine learning has led to the evolution of neural networks. These networks have become more complex due to the growing number of parameters. Usually, larger models can achieve higher accuracy and perform better on complex tasks than smaller models.</p>
<p>The increasing number of parameters in neural networks can lead to challenges in training and deploying these models, as well as difficulties in interpreting the behavior of these increasingly complex models. With the growing number of parameters, neural networks can become computationally expensive, requiring more resources to train effectively. Additionally, the larger size of these models can make them difficult to deploy on devices with limited memory or processing power.</p>
<p>To reduce the computational cost of training and deploying neural networks, researchers are exploring techniques like model compression, which involves reducing the size of the model by removing unnecessary parts, and architecture search, which involves automatically selecting the best architecture for a given task. One approach to reducing the computational cost of training and deploying neural networks is through the use of techniques such as pruning, quantization, and distillation. These methods aim to reduce the number of parameters and operations required by the model while still maintaining its performance. These techniques can help reduce the computational cost of training and deploying neural networks, making them more accessible to a wider range of applications.</p>
<p>The trend towards larger and more complex neural networks presents both challenges and opportunities for the field of artificial intelligence. Researchers are actively working to overcome these challenges and harness the full potential of these </p>
<p>A. Large Language Models</p>
<p>Large language models (LLMs) are a type of artificial intelligence system that can generate human-like text based on input data. These models are trained on massive amounts of text data and use sophisticated algorithms to learn patterns in language and generate coherent text. LLMs have become increasingly popular in recent years, and their applications range from chatbots and virtual assistants to content creation and language translation.</p>
<p>One of the most significant advantages of LLMs is their ability to generate high-quality text that is indistinguishable from text written by humans. This makes them a valuable tool in industries such as content creation and journalism, where high-quality writing is essential. LLMs can also be used to generate personalized content for consumers, such as product recommendations or personalized news articles.</p>
<p>Another benefit of LLMs is their ability to understand the context of text. These models can analyze the meaning behind words and phrases and generate text that is relevant and accurate. This makes LLMs a valuable tool in industries such as search engine optimization and natural language processing (NLP), where understanding the context of text is critical.</p>
<p>Furthermore, LLMs can be used to automate repetitive tasks, such as customer service inquiries or data entry. This can help businesses save time and money while improving efficiency. LLMs can also be used to generate more accurate translations, making them a valuable tool in the language translation industry. However, there are some concerns regarding the use of LLMs. One issue is the potential for bias in the data sets used to train these models. Since LLMs are trained on massive amounts of data, any biases in the data can be amplified and reflected in the generated text. Additionally, there are concerns regarding the potential misuse of LLMs to generate fake news or other forms of misinformation.</p>
<p>LLMs are a powerful tool that has the potential to transform various industries by automating tasks, generating high-quality content, and improving efficiency. However, it is essential to address the concerns regarding bias and potential misuse to ensure that these models are used ethically and responsibly.</p>
<p>B. Blockchain</p>
<p>Blockchain technology has emerged as a groundbreaking concept in the digital world in recent years. It is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. This technology holds the potential to revolutionize various industries, such as finance, healthcare, and supply chain management. This essay explores the fundamentals of blockchain technology and its possible impact on different industries.</p>
<p>At its core, a blockchain is a digital ledger of transactions that is distributed across a network of computers. Each block in the chain contains a timestamp and a link to the previous block, forming a chain of blocks. The network of computers that maintains the blockchain ensures that every block added to the chain is valid and unchanged. This makes it nearly impossible to tamper with the ledger or commit fraud, as any change to one block would require a change to every block that follows it.</p>
<p>The potential uses for blockchain technology are vast and varied. In the finance industry, blockchain technology could streamline processes, reduce costs, and increase transparency. Smart contracts, which are self-executing contracts with the terms of the agreement between buyer and seller being directly written into lines of code, could reduce the need for intermediaries and minimize the risk of fraud.</p>
<p>In healthcare, blockchain technology could improve security and privacy. Patient records could be stored on a blockchain, which would allow for secure and easy access to medical records by authorized parties. The technology could also help combat counterfeit drugs by tracking the supply chain and ensuring the authenticity of drugs.</p>
<p>In supply chain management, blockchain technology could improve efficiency and transparency. The technology could be used to track goods from their origin to their final destination, reducing the risk of fraud and ensuring that products are ethically sourced.</p>
<p>Blockchain technology has the potential to transform various industries by increasing efficiency, security, and transparency. While the technology is still in its early stages, many experts believe that it is the future of the digital world. As blockchain technology continues to evolve, it will be interesting to see how it shapes the future of the industries it impacts.</p>
<p>C. Motivation and Contribution</p>
<p>Traditional LLM is trained on centralized GPUs and can not evolve after the training stage. These drawbacks motivate us to develop LLM on blockchains, where the LLM is trained in decentralized nodes and can evolve during its usage. Our contributions are</p>
<p>• we propose to develop LLM on blockchains • The proposed LLM can continuously evolve during its usage after the training process • we propose a new economic model to link the LLM with the blockchain technology.</p>
<p>II. DYNAMIC LARGE LANGUAGE MODELS</p>
<p>The development of artificial intelligence has resulted in increasingly complex neural networks with more parameters. This has led to the creation of large language models such as GPT-3 and BERT, which have significantly impacted natural language processing.</p>
<p>However, a major challenge associated with these models is their high computational cost. Traditional large language models, like GPT-3 and BERT, require thousands of GPUs for training and deployment. This high cost is a significant obstacle for many researchers and organizations, limiting access to these powerful models.</p>
<p>Despite this challenge, advancements in hardware and software have made it possible to train and deploy these models on fewer GPUs. This has opened new opportunities for researchers and organizations to explore the capabilities of these models and leverage their potential in various applications.</p>
<p>Another problem with current LLM is that these models are static. In other words, they can not evolve to achieve higher performance with more and more user input. In fact, millions of users can help to improve the LLM in each specific field, such as poem, medical diagnosis and car design.</p>
<p>A. Decentralized Blockchains</p>
<p>To tackle the heavy computation requirement in LLM's training and deployment, we propose to develop LLM on blockchains which are decentralized and have high computation performance.</p>
<p>Large language models have had a major impact on natural language processing (NLP) in recent years. They've achieved state-of-the-art performance on a wide range of NLP tasks. However, training and deploying LLMs can be challenging due to their massive computational requirements and the need for high-quality training data.</p>
<p>To address these challenges, we propose developing LLMs on blockchains. Blockchains offer a decentralized and transparent platform for storing and processing large amounts of data. By leveraging blockchain technology, we can create a distributed network of nodes that contribute computing power to train and deploy LLMs.</p>
<p>Developing LLMs on blockchains has many benefits. First, it allows for more efficient use of computing resources by leveraging the idle processing power of network participants. Second, it enables the creation of decentralized datasets that are more secure and transparent than traditional centralized datasets. Third, it provides a more fair distribution of economic incentives for data contributors and validators.</p>
<p>One of the main challenges for LLMs is the availability of high-quality training data. With blockchains, we can create a decentralized dataset that is more secure and transparent than traditional datasets. This means that the data is less likely to be tampered with and can be easily audited for accuracy and bias. Also, since the data is decentralized, it is not controlled by any single entity, which makes it more accessible to a wider range of researchers and developers.</p>
<p>Developing LLMs on blockchains also provides a more fair distribution of economic incentives for data contributors and validators. In traditional centralized datasets, data contributors and validators are often not compensated fairly for their contributions. However, with blockchains, data contributors and validators can be rewarded with tokens that have real economic value. This creates a more fair and transparent system that incentivizes participation and collaboration.</p>
<p>Overall, we believe that developing LLMs on blockchains has the potential to revolutionize the field of NLP by making large-scale language models more accessible to everyone. We're excited to explore this new area of research.</p>
<p>B. Dynamic Neural Networks</p>
<p>To tackle the static issue in LLM, in this paper, we propose dynamic LLM which can evolve after the training process. More specifically, the neural network parameters can be continuously updated during their usage after the training process. In other words, the traditional training process is an initialization of the dynamic LLM. The dynamic LLM have the ability to learn during its usage.</p>
<p>Such behavior is inspired by human brain development which has the life-long learning ability and can increase its knowledge with more and more experience.</p>
<p>Although LLM has much less neurons than the human brain and each neuron in LLM is much simpler than the real neuron in human cortex, LLM can be inspired by the anatomic and functional dynamics of human brain. For example, the fact that human brain can achieve better with more and more experience indicates that LLM should also have the ability to update their parameters with more usage. We call such LLM as dynamic LLM or DLLM for short.</p>
<p>Beside the dynamic parameter weights, DLLM also include a dynamic architecture, which is similar to the functional network on human brain cortex. Such functional network points out that one function is supported by several regions and their connectivity. And the same region might play different roles in different functional networks. Such multi functionalities should be imposed into DLLM for improving the model efficiency and reducing the model size.</p>
<p>C. Functional Network in Brain Cortex</p>
<p>The human brain cortex is a highly complex network of functional connections that allow for various cognitive processes, such as perception, attention, memory, and decisionmaking. Scientists have been able to study these functional connections and identify key brain regions and their connectivity patterns using brain imaging techniques such as functional magnetic resonance imaging (fMRI) and electroencephalography (EEG).</p>
<p>One of the most well-known functional networks in the human brain is the default mode network (DMN). The DMN is active when the brain is at rest and not engaged in any specific task. It includes the medial prefrontal cortex, posterior cingulate cortex, and inferior parietal lobule. This network is believed to be involved in self-referential thinking, mindwandering, and autobiographical memory.</p>
<p>Another important functional network is the salience network, which is involved in detecting and prioritizing important stimuli. The salience network includes the anterior cingulate cortex and insula. This network is important for our ability to direct our attention towards important stimuli and regulate our emotional responses.</p>
<p>The attention network, on the other hand, is involved in focusing attention on specific stimuli. This network includes regions such as the frontal eye fields and the parietal cortex. The executive control network, on the other hand, is involved in higher-level cognition such as decision-making and working memory. This network includes regions such as the dorsolateral prefrontal cortex and the anterior cingulate cortex.</p>
<p>Functional connectivity studies have also identified other networks in the human brain, such as the language network, the visual network, and the sensorimotor network. These networks are involved in processing language, visual information, and sensory information, respectively. These brain networks can inspire to design the modules in LLM and the functions of each module. Although the brain network is far from fully understood, they can guide the structure of LLM and their working fashion, especially when the LLM become larger and more complex.</p>
<p>III. TRAINING AND RUNNING ON BLOCKCHAINS</p>
<p>After explaining the DLLM in previous sections, we now explain why they should be trained and run on blockchains from several aspects, including transparency, dynamic price, decentralization and economic analysis.</p>
<p>A. Transparency</p>
<p>Many companies use proprietary data to train their large language models, which can result in highly accurate models. However, this approach also has some drawbacks.</p>
<p>A major issue is that outside researchers often do not have access to the proprietary input used to train these models. This lack of accessibility makes it difficult for others to replicate or independently verify the models' results.</p>
<p>The use of proprietary data can make it challenging to interpret the models. With inputs that are not publicly available, it can be hard to determine the driving factors behind a model's predictions. This lack of transparency can be problematic, particularly when the models make important decisions.</p>
<p>To address these issues, some companies are exploring ways to make their models more transparent and accessible. One method is to fine-tune the models using publicly available data after they have been trained on proprietary data. This can help ensure the models are less reliant on specific inputs and are more generalizable.</p>
<p>In our DLLM, all the input data must be transparent and available to all researchers. Such requirement makes DLLM more transparent than LLM.</p>
<p>B. Dynamic Price</p>
<p>Unlike previous static LLM, DLLM take dynamic input and are dynamic with different tasks. Therefore, it takes different cost and price for easy and difficult tasks. The more difficult the task, the more computation is required and thus the price is higher for its usage.</p>
<p>Such dynamic price is good for both users and developers. For a easy task such as checking the weather, the user only pays a extremely low price (usually free of charge) because it is not a difficult task. For a more difficult task such as analyzing human's behavior in a set of videos, the user has to pay more money since the task requires more resources.</p>
<p>If the developers build a simple model, they only pay a low price and their model can only earn money at a low price since the model can only deal with simple tasks. In contrast, if they build a complex model, they have to pay a higher price because the complex model requires more computation. As a result, the complex model can earn money at a higher price because it can deal with complex tasks.</p>
<p>Such dynamics also corresponds to the dynamics in the network architecture. Easy tasks only use shallow part of networks while difficult tasks use deep and more networks.</p>
<p>C. Decentralization</p>
<p>Current LLM are controlled by companies who might block users from using their LLM. And users might lose their private history on the LLM. To tackle this issue, the decentralization nature of blockchains can play an important role here.</p>
<p>The user's private history is encrypted and securely stored using blockchain technology. This ensures that the data is protected from unauthorized access or tampering.</p>
<p>The decentralized nature of the blockchain means that the user's data is not stored in a single location, reducing the risk of data loss due to technical issues or cyber attacks.</p>
<p>The user's data is protected through strong encryption algorithms that make it extremely difficult for anyone to access the information without authorization. The user has full control over their private history and can decide who can view it.</p>
<p>Overall, the use of blockchain technology provides a high level of security and privacy for the user's private history. The user can trust that their data is safe and secure, and they do not have to worry about losing it or being blocked by anyone.</p>
<p>D. Win-win for Everyone</p>
<p>As mentioned, both users and developers can benefit from DLLM. Users' input and task requirements can be considered as valuable resources for developers. Thus, developers have to pay money for such resources. And the models built from developers are valuable resources. Therefore, the users have to pay money for their usage. This is a economical helix structure that benefits everyone in the system. Users get access to the model at a lower price and developers can develop their model at a lower price. Meanwhile, users can use better models if their tasks are more complex. In this case, they have to pay more. If the developers want to build a better model with better input or requiring more computation resource, they also have to pay more.</p>
<p>A better input from users earns more while the better model from developers also earns more. In contrast, the users' input that does not have value for developers will not earn anything and be abandoned. The developers' models that are not required by any users will not earn anything and be inactive. Finally, the system only contains the valuable input from users and models from developers.</p>
<p>It's important to recognize that both input and models are dynamic, meaning that the popularity and usage of specific models may change over time. Just like fashion trends in human society, machine learning models can come and go. This is something to consider when developing and implementing models. While a particular model may not be widely used or popular at present, it may become more relevant and widely adopted in the future. Therefore, it's important to continue to explore and evaluate various models, even if they're not currently in use.</p>
<p>By remaining open to new models and ideas, we can ensure that our machine learning systems are up-to-date and effective in a constantly evolving technological landscape.</p>
<p>IV. CONCLUSION AND DISCUSSION</p>
<p>In this paper, we have presented dynamic large language models that have dynamic architecture and dynamic parameter weights for different tasks. Meanwhile, we propose to perform such models on blockchains, where the decentralization and transparency property can benefit both users and developers.</p>
<p>Our approach sheds light on the next generation of AI systems that are based on blockchain technology. By combining the power of blockchain with AI, we can create systems that are more secure, transparent, and efficient than ever before.</p>
<p>Through our research and development, we have identified important areas where blockchain can be applied to AI systems. These areas include data management, model training and validation, and decision-making processes.</p>
<p>One of the biggest challenges in AI is ensuring the integrity and security of data. By using blockchain, we can create a decentralized and immutable ledger that guarantees the verification and tamper-proof of data. This provides a level of transparency and trust that is crucial for AI systems.</p>
<p>Moreover, blockchain can improve the model training and validation process. By utilizing a distributed ledger, we can guarantee that models are trained on high-quality data and that the results are accurate and reliable.</p>
<p>Our approach is a significant step forward in the development of blockchain-based AI systems. We believe that these systems will revolutionize the way we think about AI and its potential applications in various audio, vision and machine learning tasks [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [30], [39], [40], [41], [42], [43].</p>
<p>arXiv:2307.10549v1 [cs.CV] 20 Jul 2023</p>
<p>Manuscript receivedApril 19, 2005; revised September 17, 2014.Fig. 1. Illustration of the proposed method. The dynamic LLM run on blockchains.Users feed the model more and more data while the developers can update the model accordingly. From economy point of view, users pay for professional advice at a very low price. They earn money for their valuable input. Developers pay for the input to train their models but earn money from their trained models. Such ecosystems will benefit both users and developers. models. One way to make neural networks more interpretable is by creating visualizations of the model's internal workings and using techniques like saliency maps to understand how different parts of the input contribute to the output. Additionally, techniques like adversarial training can help improve the robustness of the model and make it more resistant to attacks.Train and develop 
blockchain </p>
<p>Input and usage 
Users 
Developers </p>
<p>DLLM </p>
<p>. N Chenouard, I Smal, F De Chaumont, M Maska, I F Sbalzarini, Y Gong, J Cardinale, C Carthel, S Coraluppi, M Winter, A R Cohen, W J Godinez, K Rohr, Y Kalaidzidis, L Liang, J Duncan, H Shen, Y Xu, K E G Magnusson, J Jalden, H M Blau, P Paul-Gilloteaux, P Roudot, C Kervrann, F Waharte, J.-Y Tinevez, S L , N. Chenouard, I. Smal, F. de Chaumont, M. Maska, I. F. Sbalzarini, Y. Gong, J. Cardinale, C. Carthel, S. Coraluppi, M. Winter, A. R. Cohen, W. J. Godinez, K. Rohr, Y. Kalaidzidis, L. Liang, J. Duncan, H. Shen, Y. Xu, K. E. G. Magnusson, J. Jalden, H. M. Blau, P. Paul- Gilloteaux, P. Roudot, C. Kervrann, F. Waharte, J.-Y. Tinevez, S. L.</p>
<p>Objective comparison of particle tracking methods. J Shorte, K Willemse, G P Celler, H.-W Van Wezel, Y.-S Dan, C Tsai, J.-C Ortiz De Solorzano, E Olivo-Marin, Meijering, Nat. Methods. 113Shorte, J. Willemse, K. Celler, G. P. van Wezel, H.-W. Dan, Y.-S. Tsai, C. Ortiz de Solorzano, J.-C. Olivo-Marin, and E. Meijering, "Objective comparison of particle tracking methods," Nat. Methods, vol. 11, no. 3, pp. 281-U247, March 2014.</p>
<p>Symmetry detection for multi-object using local polar coordinate. Y Gong, Q Wang, C Yang, Y Gao, C Li, Lecture Notes in Computer Science. 5702277Y. Gong, Q. Wang, C. Yang, Y. Gao, and C. Li, "Symmetry detection for multi-object using local polar coordinate," Lecture Notes in Computer Science, vol. 5702, p. 277, 2009.</p>
<p>BART: denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, abs/1910.13461CoRR. M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "BART: denoising sequence-to- sequence pre-training for natural language generation, translation, and comprehension," CoRR, vol. abs/1910.13461, 2019. [Online]. Available: http://arxiv.org/abs/1910.13461</p>
<p>A survey of large language models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J.-Y Nie, J.-R Wen, W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and J.-R. Wen, "A survey of large language models," 2023.</p>
<p>Coupled signed-distance functions for implicit surface reconstruction. Y Gong, G Paul, I F Sbalzarini, IEEE Intl. Symp. Biomed. Imaging (ISBI). Y. Gong, G. Paul, and I. F. Sbalzarini, "Coupled signed-distance func- tions for implicit surface reconstruction," in IEEE Intl. Symp. Biomed. Imaging (ISBI), May 2012, pp. 1000-1003.</p>
<p>Language models are fewshot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, CoRR. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, "Language models are few- shot learners," CoRR, vol. abs/2005.14165, 2020. [Online]. Available: https://arxiv.org/abs/2005.14165</p>
<p>Local weighted Gaussian curvature for image processing. Y Gong, I F Sbalzarini, Intl. Conf. Image Proc. (ICIP). Y. Gong and I. F. Sbalzarini, "Local weighted Gaussian curvature for image processing," Intl. Conf. Image Proc. (ICIP), pp. 534-538, September 2013.</p>
<p>Single image interpolation exploiting semilocal similarity. L Yu, M T Orchard, IEEEBrighton, UKL. Yu and M. T. Orchard, "Single image interpolation exploiting semi- local similarity." Brighton, UK: IEEE, 2019, pp. 1722-1726.</p>
<p>Image enhancement by gradient distribution specification. Y Gong, I F Sbalzarini, 12th Asian Conference on Computer Vision. SingaporeProc. WorkshopY. Gong and I. F. Sbalzarini, "Image enhancement by gradient distri- bution specification," in In Proc. Workshop "Emerging Topics in Image Enhancement and Restoration", 12th Asian Conference on Computer Vision, Singapore, Nov 2014, pp. w7-p3.</p>
<p>Side window filtering. H Yin, Y Gong, G Qiu, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). H. Yin, Y. Gong, and G. Qiu, "Side window filtering," in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8750-8758.</p>
<p>Spectrally regularized surfaces. Y Gong, 10.3929/ethz-a-010438292ETH ZurichPh.D. dissertationY. Gong, "Spectrally regularized surfaces," Ph.D. dissertation, ETH Zurich, Nr. 22616, 2015, http://dx.doi.org/10.3929/ethz-a-010438292.</p>
<p>Fast and highquality blind multi-spectral image pansharpening. L Yu, D Liu, H Mansour, P T Boufounos, IEEE Transactions on Geoscience and Remote Sensing. 60L. Yu, D. Liu, H. Mansour, and P. T. Boufounos, "Fast and high- quality blind multi-spectral image pansharpening," IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1-17, 2022.</p>
<p>A natural-scene gradient distribution prior and its application in light-microscopy image processing. Y Gong, I Sbalzarini, IEEE Journal. 101Selected Topics in Signal ProcessingY. Gong and I. Sbalzarini, "A natural-scene gradient distribution prior and its application in light-microscopy image processing," Selected Topics in Signal Processing, IEEE Journal of, vol. 10, no. 1, pp. 99-114, Feb 2016.</p>
<p>A survey on blockchain technology and its security. H Guo, X Yu, Blockchain: Research and Applications. 32100067H. Guo and X. Yu, "A survey on blockchain technology and its security," Blockchain: Research and Applications, vol. 3, no. 2, p. 100067, 2022. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S2096720922000070</p>
<p>Curvature filters efficiently reduce certain variational energies. Y Gong, I F Sbalzarini, IEEE Transactions on Image Processing. 264Y. Gong and I. F. Sbalzarini, "Curvature filters efficiently reduce certain variational energies," IEEE Transactions on Image Processing, vol. 26, no. 4, pp. 1786-1798, April 2017.</p>
<p>Motion saliency based multi-stream multiplier resnets for action recognition. M Zong, R Wang, X Chen, Z Chen, Y Gong, Image and Vision Computing. 107104108M. Zong, R. Wang, X. Chen, Z. Chen, and Y. Gong, "Motion saliency based multi-stream multiplier resnets for action recognition," Image and Vision Computing, vol. 107, p. 104108, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0262885621000135</p>
<p>Bernstein filter: A new solver for mean curvature regularized models. Y Gong, 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Y. Gong, "Bernstein filter: A new solver for mean curvature regularized models," in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2016, pp. 1701-1705.</p>
<p>Blockchain-based cross-domain authorization system for user-centric resource sharing. Y Ezawa, S Kakei, Y Shiraishi, M Mohri, M Morii, Blockchain: Research and Applications. 42100126Y. Ezawa, S. Kakei, Y. Shiraishi, M. Mohri, and M. Morii, "Blockchain-based cross-domain authorization system for user-centric resource sharing," Blockchain: Research and Applications, vol. 4, no. 2, p. 100126, 2023. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S2096720923000015</p>
<p>Linear approximation of mean curvature. Y Gong, Y Xie, 2017 IEEE International Conference on. Image Processing (ICIPY. Gong and Y. Xie, "Linear approximation of mean curvature," in Image Processing (ICIP), 2017 IEEE International Conference on. IEEE, 2017, pp. 570-574.</p>
<p>Real-time optimizing weighted gaussian curvature for 4k videos. W Tang, L Zhou, Y Gong, 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing. W. Tang, L. Zhou, and Y. Gong, "Real-time optimizing weighted gaussian curvature for 4k videos," in 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP), 2021, pp. 1-6.</p>
<p>Sub-window box filter. Y Gong, B Liu, X Hou, G Qiu, Proc. IEEE Visual Communications and Image Processing (VCIP). IEEE Visual Communications and Image essing (VCIP)Y. Gong, B. Liu, X. Hou, and G. Qiu, "Sub-window box filter," in Proc. IEEE Visual Communications and Image Processing (VCIP), Dec. 2018, pp. 1-4.</p>
<p>Image filtering with generic geometric prior. Y Gong, X Hou, F Li, G Qiu, IEEE Access. 6Y. Gong, X. Hou, F. Li, and G. Qiu, "Image filtering with generic geometric prior," IEEE Access, vol. 6, pp. 54 320-54 330, 2018.</p>
<p>Blind multi-spectral image pan-sharpening. L Yu, D Liu, H Mansour, P T Boufounos, Y Ma, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing. L. Yu, D. Liu, H. Mansour, P. T. Boufounos, and Y. Ma, "Blind multi-spectral image pan-sharpening," in ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 1429-1433.</p>
<p>Weighted mean curvature. Y Gong, O Goksel, Signal Processing. 164Y. Gong and O. Goksel, "Weighted mean curvature," Signal Processing, vol. 164, pp. 329 -339, 2019. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0165168419302282</p>
<p>What do large language models learn about scripts. A Sancheti, R Rudinger, 2022A. Sancheti and R. Rudinger, "What do large language models learn about scripts?" 2022.</p>
<p>Computing gaussian curvature in real-time for 4k video processing. Y Gong, Y Chen, IEEE Access. 7Y. Gong and Y. Chen, "Computing gaussian curvature in real-time for 4k video processing," IEEE Access, vol. 7, pp. 115 936-115 944, 2019.</p>
<p>Structure adaptive filtering for edge-preserving image smoothing. W Tang, Y Gong, L Su, W Wu, G Qiu, Image and Graphics, Y. Peng, S.-M. Hu, M. Gabbouj, K. Zhou, M. Elad, and K. XuSpringer International PublishingW. Tang, Y. Gong, L. Su, W. Wu, and G. Qiu, "Structure adaptive filtering for edge-preserving image smoothing," in Image and Graphics, Y. Peng, S.-M. Hu, M. Gabbouj, K. Zhou, M. Elad, and K. Xu, Eds. Cham: Springer International Publishing, 2021, pp. 265-276.</p>
<p>Soft tissue removal in x-ray images by half window dark channel prior. Y Gong, H Yin, J Liu, B Liu, G Qiu, Proc. IEEE Int. Conf. Image Processing (ICIP). IEEE Int. Conf. Image essing (ICIP)Y. Gong, H. Yin, J. Liu, B. Liu, and G. Qiu, "Soft tissue removal in x-ray images by half window dark channel prior," in Proc. IEEE Int. Conf. Image Processing (ICIP), Sep. 2019, pp. 3576-3580.</p>
<p>Side window guided filtering. H Yin, Y Gong, G Qiu, 165Signal ProcessH. Yin, Y. Gong, and G. Qiu, "Side window guided filtering," Signal Process., vol. 165, pp. 315-330, 2019.</p>
<p>Computing curvature, mean curvature and weighted mean curvature. Y Gong, IEEEBordeaux, FranceY. Gong, "Computing curvature, mean curvature and weighted mean curvature." Bordeaux, France: IEEE, 2022, pp. 266-270.</p>
<p>Fast and efficient implementation of image filtering using a side window convolutional neural network. H Yin, Y Gong, G Qiu, Signal Process. 176107717H. Yin, Y. Gong, and G. Qiu, "Fast and efficient implementation of image filtering using a side window convolutional neural network," Signal Process., vol. 176, p. 107717, 2020.</p>
<p>Molecular surface estimation by geometric coupled distance functions. Y Gong, Y Chen, IEEE Access. 8Y. Gong and Y. Chen, "Molecular surface estimation by geometric coupled distance functions," IEEE Access, vol. 8, pp. 176 263-176 273, 2020.</p>
<p>A high performance concurrency protocol for smart contracts of permissioned blockchain. C Jin, S Pang, X Qi, Z Zhang, A Zhou, IEEE Transactions on Knowledge and Data Engineering. 3411C. Jin, S. Pang, X. Qi, Z. Zhang, and A. Zhou, "A high performance concurrency protocol for smart contracts of permissioned blockchain," IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 11, pp. 5070-5083, 2022.</p>
<p>Quarter laplacian filter for edge aware image processing. Y Gong, W Tang, L Zhou, L Yu, G Qiu, Proc. IEEE Int. Conf. Image Processing (ICIP). IEEE Int. Conf. Image essing (ICIP)Y. Gong, W. Tang, L. Zhou, L. Yu, and G. Qiu, "Quarter laplacian filter for edge aware image processing," in Proc. IEEE Int. Conf. Image Processing (ICIP), 2021, pp. 1959-1963.</p>
<p>Curvature-based real-time brightness adjustment for ultra hd video. W Tang, L Zhou, Y Gong, 2022 IEEE 24th International Workshop on Multimedia Signal Processing. W. Tang, L. Zhou, and Y. Gong, "Curvature-based real-time brightness adjustment for ultra hd video," in 2022 IEEE 24th International Work- shop on Multimedia Signal Processing (MMSP), 2022, pp. 1-6.</p>
<p>A discrete scheme for computing image's weighted gaussian curvature. Y Gong, W Tang, L Zhou, L Yu, G Qiu, 2021 IEEE International Conference on Image Processing. Y. Gong, W. Tang, L. Zhou, L. Yu, and G. Qiu, "A discrete scheme for computing image's weighted gaussian curvature," in 2021 IEEE International Conference on Image Processing (ICIP), 2021, pp. 1919- 1923.</p>
<p>A novel structure adaptive algorithm for feature-preserving 3d mesh denoising. W Tang, Y Gong, G Qiu, 2022 IEEE 24th International Workshop on Multimedia Signal Processing. W. Tang, Y. Gong, and G. Qiu, "A novel structure adaptive algorithm for feature-preserving 3d mesh denoising," in 2022 IEEE 24th International Workshop on Multimedia Signal Processing (MMSP), 2022, pp. 1-6.</p>
<p>Gc-net: An unsupervised network for gaussian curvature optimization on images. W Tang, Z Lin, Y Gong, Journal of Signal Processing Systems. 951W. Tang, Z. Lin, and Y. Gong, "Gc-net: An unsupervised network for gaussian curvature optimization on images," Journal of Signal Processing Systems, vol. 95, no. 1, pp. 77-88, 2023. [Online].</p>
<p>. 10.1007/s11265-022-01800-4Available: https://doi.org/10.1007/s11265-022-01800-4</p>
<p>Feature preserving 3d mesh denoising with a dense local graph neural network. W Tang, Y Gong, G Qiu, 233103710W. Tang, Y. Gong, and G. Qiu, "Feature preserving 3d mesh denoising with a dense local graph neural network," vol. 233, p. 103710, 2023.</p>
<p>Regression-based camera pose estimation through multi-level local features and global features. M Xu, Z Zhang, Y Gong, S Poslad, Sensors. 238M. Xu, Z. Zhang, Y. Gong, and S. Poslad, "Regression-based camera pose estimation through multi-level local features and global features," Sensors, vol. 23, no. 8, 2023. [Online]. Available: https://www.mdpi.com/1424-8220/23/8/4063</p>
<p>Dynamic neural networks: A survey. Y Han, G Huang, S Song, L Yang, H Wang, Y Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4411Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang, "Dynamic neural networks: A survey," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 11, pp. 7436-7456, 2022.</p>
<p>Training language models with language feedback at scale. J Scheurer, J A Campos, T Korbak, J S Chan, A Chen, K Cho, E Perez, J. Scheurer, J. A. Campos, T. Korbak, J. S. Chan, A. Chen, K. Cho, and E. Perez, "Training language models with language feedback at scale," Mar. 2023.</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y Qiao, abs/2303.16199CoRR. R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao, "Llama-adapter: Efficient fine-tuning of language models with zero-init attention," CoRR, vol. abs/2303.16199, 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
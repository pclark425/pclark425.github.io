<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-332 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-332</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-332</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-b59947541d2ac4211c4b17554b2e16c260299bed</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b59947541d2ac4211c4b17554b2e16c260299bed" target="_blank">Have You Seen That Number? Investigating Extrapolation in Question Answering Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, and proposes the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text.</p>
                <p><strong>Paper Abstract:</strong> Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the models fail to extrapolate to unseen numbers. Presenting numbers as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e332.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e332.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenBERT (Injecting numerical reasoning skills into language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based model (BERT pretraining adapted for numeric reasoning) evaluated on the DROP numerical MRC benchmark; used as a main proxy model in experiments and evaluated with different number surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Injecting numerical reasoning skills into language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GenBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-only transformer (BERT-based, fine-tuned for text-to-answer generation)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>counting, sorting, and basic arithmetic operations (e.g., addition/multiplication implied by DROP examples and by perturbation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Varied — training data contains numbers up to millions (e.g., CARDINAL max 48,466,928); evaluation included extrapolated numbers via Add(10), Add(100), Factor(10), Factor(100)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on DROP; evaluated on perturbed (Add/Factor) versions of evaluation set; experiments with alternative number surface forms (original subword, digit, 10e-based, 10-based, and proposed E-digit); GenBERT used as a generator for formatted numeric outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (original dev): EM 68.80, F1 72.30. Add(10): EM 60.67, F1 63.80. Add(100): EM 56.73, F1 59.72. Factor(10): EM 45.41, F1 47.58. Factor(100): EM 42.78, F1 45.10. With E-digit (extrapolate setting): on Add(100) All EM 59.97 F1 68.24; on Factor(100) All EM 57.91 F1 63.98. (Detailed breakdowns by answer type available in Table 3 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>GenBERT benefits from digit-aware pretraining (authors note its pretraining uses digit-subword inputs), which gives some numeric ability, but it fails to generalize to out-of-distribution magnitudes because standard subword tokenization fragments numbers and the model lacks a strong inductive bias for digit-place structure; changing surface form to E-digit supplies explicit digit-place cues that the model can exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance drops substantially as the magnitude shift increases (Factor(10) -> Factor(100) produces large declines); no discussion of scaling with model parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails to extrapolate to unseen number magnitudes; performance degrades sharply when passage numbers are multiplied by large factors (e.g., Factor(100)); struggles particularly on numeric/date answer categories when presented with out-of-range numbers under original tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against its own variants with alternative number surface forms (digit, 10e-based, 10-based, E-digit) and against other DROP models (NAQANet, NumNet, NumNet+RoBERTa).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GenBERT shows high interpolation performance on DROP but fails to extrapolate to out-of-distribution numeric magnitudes; representing numbers with the proposed E-digit surface form substantially improves extrapolation performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e332.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e332.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAQANet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAQANet (DROP baseline QA model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The official baseline model for DROP (task-specific QA model that handles spans and numeric answers); evaluated to measure extrapolation sensitivity to number-range perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NAQANet</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>task-specific QA model with heads for numeric answers (baseline for DROP); not a large pretrained LM encoder in the form used for other baselines</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>counting and basic arithmetic operations as required by DROP (including addition/subtraction over multiple numbers supporting an answer)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Evaluated with perturbations Add(10), Add(100), Factor(10), Factor(100) applied to CARDINAL/QUANTITY/MONEY tokens; original dataset contains numbers up to tens of millions in some categories</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Evaluated on perturbed versions of DROP dev set (no special surface-form intervention in the baseline row shown)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (original dev): EM 46.20, F1 49.24. Add(10): EM 40.87, F1 43.61. Add(100): EM 38.31, F1 40.34. Factor(10): EM 30.17, F1 34.09. Factor(100): EM 23.03, F1 26.01.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No internal mechanistic analysis provided; empirical evidence shows large sensitivity to out-of-range numbers, implying NAQANet lacks an inductive bias enabling numeric extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance monotonically degrades as perturbation magnitude increases (worse on multiplicative perturbations), indicating poor scaling to larger numeric magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Severe performance drop on extrapolated numbers, especially under multiplicative perturbations (Factor(100)).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to other DROP models (NumNet variants, GenBERT) on the same perturbation schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>As a baseline, NAQANet demonstrates that DROP-capable models can drastically lose accuracy when passage numbers shift outside the training range, highlighting lack of extrapolation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e332.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e332.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NumNet (Machine reading comprehension with numerical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DROP model that incorporates numeric nodes and graph structure to encode relative magnitude relationships between numbers; evaluated for extrapolation robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NumNet: Machine reading comprehension with numerical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NumNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>graph-augmented QA model (numbers represented as graph nodes with magnitude-aware edges) with a transformer encoder for text</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>numeric reasoning tasks required by DROP: counting, sorting, and arithmetic over multiple numbers (addition/multiplicative perturbations used in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Same perturbation regimes (Add/Factor up to x100); training data contains a long-tailed distribution including large numbers (up to millions)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuned on original DROP; evaluated on perturbed ADD/FACTOR test sets to measure extrapolation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (original dev): EM 64.92, F1 68.31. Add(10): EM 57.01, F1 61.12. Add(100): EM 56.70, F1 60.75. Factor(10): EM 39.76, F1 43.84. Factor(100): EM 27.55, F1 31.68.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Although NumNet explicitly models relative magnitudes via graphs (an inductive bias for numeric relationships), it still fails to extrapolate to large out-of-distribution magnitudes — suggesting that relative-magnitude graph structure alone does not guarantee extrapolation to unseen absolute ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Degrades strongly with multiplicative perturbations; relative performance remains better than some baselines on interpolation but not robust to large magnitude shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Marked drop on Factor(10)/Factor(100), indicating inability to generalize arithmetic reasoning to numbers with magnitudes outside training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to NAQANet, NumNet+(RoBERTa), and GenBERT across same perturbation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>NumNet's graph-based magnitude modeling helps interpolation but does not provide sufficient inductive bias to extrapolate to much larger numbers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e332.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e332.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumNet+RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NumNet with RoBERTa encoder (NumNet+(RoBERTa))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stronger variant of NumNet that replaces the encoder with RoBERTa, achieving higher interpolation performance but still exhibiting extrapolation failure under large numeric perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NumNet: Machine reading comprehension with numerical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NumNet+(RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>graph-augmented numeric reasoning model with RoBERTa encoder (encoder-only transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>DROP numeric reasoning (counting, sorting, arithmetic over multiple numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Evaluated with Add(10), Add(100), Factor(10), Factor(100); originally trained on DROP distribution (numbers up to millions in some types)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on DROP with RoBERTa encoder; evaluated on perturbed evaluation sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (original dev): EM 81.07, F1 84.42. Add(10): EM 63.38, F1 66.19. Add(100): EM 62.30, F1 66.05. Factor(10): EM 55.44, F1 66.01. Factor(100): EM 55.04, F1 58.80. (Shows strong interpolation but large drops under Add/Factor perturbations.)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Stronger pretrained encoder improves interpolation but does not fundamentally resolve extrapolation; the model still relies on surface-form and data-distribution cues rather than a generalizable numeric algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>High interpolation scores with stronger encoder, but extrapolation still degrades with increased numeric magnitude — RoBERTa helps but is not sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Substantial performance decrease when numbers are moved out of training range; multiplicative perturbations still reduce accuracy markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to NumNet (non-RoBERTa), NAQANet, and GenBERT; also evaluated alongside different surface-form representations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Upgrading encoder (to RoBERTa) raises baseline interpolation performance but does not eliminate failure to extrapolate to unseen numeric magnitudes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e332.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e332.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E-digit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>E-digit surface form representation (proposed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel number surface-form representation that encodes numbers as digit tokens augmented by a position-independent 'e' token followed by a digit-position number, designed to provide digit-place information while remaining position-independent to enable extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>E-digit surface form (applied to GenBERT in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>surface-form / tokenization intervention applied to transformer-based QA models (experimentally applied to GenBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Same DROP operations (counting, addition/subtraction implied by multi-number aggregation); extrapolation tested via Add(100) and Factor(100) perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Designed to generalize across digit lengths and magnitudes; evaluated on perturbations that produce numbers well outside training range (multiplication by 100, addition by 100, etc.) and datasets containing numbers up to millions</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Replace numeric tokens in training and evaluation with the E-digit encoding (digit-level tokens plus 'e' tokens with position identifiers); fine-tune GenBERT on E-digit variants and evaluate on extrapolated test sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>E-digit(Extrapolate) outperforms other surface forms on extrapolated data. Representative results (GenBERT): Add(100) All: Original EM 56.73 F1 59.72 -> E-digit EM 59.97 F1 68.24. Factor(100) All: Original EM 42.78 F1 45.10 -> E-digit EM 57.91 F1 63.98. Breakdowns: Add(100) Number: EM 62.42 F1 75.03; Date: EM 50.96 F1 58.78; Others: EM 64.47 F1 77.94. Factor(100) Number: EM 61.56 F1 63.72; Date: EM 41.40 F1 55.74; Others: EM 55.48 F1 64.68. E-digit(Interpolate) on original dev: EM 68.14 F1 71.05 (comparable to GenBERT baseline EM 68.80 F1 72.30).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>E-digit supplies explicit digit-level and digit-place cues via a position-independent 'e' token; this creates an inductive bias allowing the model to represent digit-place structure and generalize arithmetic reasoning beyond seen magnitudes. Authors hypothesize the 'e' embedding is reused across digit positions enabling length/magnitude generalization; they also note pretraining mismatch (digit subword format in GenBERT pretraining) can affect interpolation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Substantially improves extrapolation (especially multiplicative perturbations) relative to original subword tokenization and other surface forms; still does not fully close the gap to interpolation-level performance, indicating remaining limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Although it improves extrapolation, models with E-digit still underperform the original interpolation setting and may be impacted by pretraining mismatches (performance slightly lower in some interpolation tests); other non-surface-form limitations remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared against original subword tokenization, digit-level, 10e-based, and 10-based surface forms (the latter three from prior work), showing E-digit gives the best extrapolation results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Providing an explicit, position-independent digit-place tokenization (E-digit) is an effective intervention to inject an inductive bias for numbers and enables much better extrapolation to out-of-distribution numeric magnitudes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e332.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e332.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surface-form variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number surface-form interventions (original subword, digit, 10e-based, 10-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different ways of representing numbers as input tokens: original subword tokenization, digit-level tokenization, 10e-based and 10-based (from Nogueira et al.), each tested for impact on extrapolation in DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating the limitations of the transformers with simple arithmetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Surface-form variants (applied to GenBERT: original subword, digit, 10e-based, 10-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>tokenization / input representation modifications applied to transformer encoder models</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Basic arithmetic tasks as required by DROP; experiments used additive and multiplicative perturbations to probe extrapolation</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Tested on Add(100) and Factor(100) extrapolated datasets (numbers moved well outside the training distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Create alternative versions of training data using each surface form and fine-tune GenBERT on them; evaluate on perturbed dev sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Representative All-scores (GenBERT) on Add(100): Original EM 56.73 F1 59.72; 10e-based EM 57.29 F1 64.98; 10-based EM 59.19 F1 67.85; Digit EM 59.38 F1 67.90; E-digit EM 59.97 F1 68.24. On Factor(100): Original EM 42.78 F1 45.10; 10e-based EM 43.02 F1 49.94; 10-based EM 49.24 F1 56.47; Digit EM 44.97 F1 51.76; E-digit EM 57.91 F1 63.98.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Providing digit-level information (digit, 10-based, 10e-based) helps performance on extrapolated numbers relative to raw subword tokenization; 10-based forms (position-dependent embeddings for power-of-ten slots) can help for structured domains like dates but require position-specific embeddings that scale with digit length; E-digit's position-independent 'e' token avoids combinatorial growth and supports extrapolation across lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Digit-aware surface forms consistently outperform raw subword tokenization on extrapolated tests; 10-based helps some categories (e.g., dates), but E-digit gives the best overall extrapolation-generalization tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>10e-based and 10-based forms still failed to fully extrapolate beyond seen numeric lengths in previous work; position-dependent embeddings scale poorly with longer numbers and do not generalize to unseen digit lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared to each other and to E-digit in GenBERT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Surface-form choice materially affects arithmetic/extrapolation performance: digit-aware representations improve robustness, and a position-independent digit-place token (E-digit) provides the best extrapolation among tested forms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e332.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e332.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perturbation schemes (Add/Factor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Add(N) and Factor(N) numerical perturbation schemes for probing extrapolation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation method that perturbs all actionable numeric tokens in passages/questions/answers by adding (Add(N)) or multiplying (Factor(N)) by a constant to create out-of-distribution numeric contexts to probe model extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Add(N)/Factor(N) perturbation evaluation (applied to DROP evaluation set)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Probes addition and multiplication invariance/robustness (also indirectly tests multi-number arithmetic and counting by preserving distributive validity of arithmetic answers)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Perturbations used: Add(10), Add(100), Factor(10), Factor(100); these shift numbers well outside typical training ranges (e.g., multiplications produce numbers orders of magnitude larger)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Apply consistent addition or multiplication to QUANTITY, CARDINAL, and MONEY entities in passages/questions/answers and evaluate standard models without retraining (or with surface-form retraining when testing surface-form interventions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Across models, even modest shifts (Add(10)/Factor(10)) reduce accuracy; larger shifts (Add(100)/Factor(100)) produce large drops. Example summary (GenBERT): see entries above. Table 1 shows large EM/F1 drops for NAQANet, NumNet, NumNet+(RoBERTa), GenBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>These perturbations expose that models rely on surface-form statistics and training-distribution numeric ranges rather than learned general numeric algorithms; multiplicative shifts are especially damaging, revealing poor extrapolation to larger magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance monotonically decreases as perturbation magnitude grows (Factor(100) worse than Factor(10), etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Models that perform well on interpolation fail catastrophically under multiplicative perturbations; count-type answers require heuristics to avoid accidental mislabelling when perturbed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a diagnostic to compare models and surface-form interventions (original vs digit vs 10e vs 10-based vs E-digit).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Add/Factor perturbations are an effective probe: state-of-the-art DROP models generally cannot extrapolate to out-of-distribution numeric magnitudes, and surface-form interventions (especially E-digit) materially mitigate but do not fully solve this limitation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating the limitations of the transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Neural arithmetic logic units <em>(Rating: 2)</em></li>
                <li>Neural arithmetic units <em>(Rating: 2)</em></li>
                <li>NumNet: Machine reading comprehension with numerical reasoning <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Representing numbers in NLP: a survey and a vision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-332",
    "paper_id": "paper-b59947541d2ac4211c4b17554b2e16c260299bed",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GenBERT",
            "name_full": "GenBERT (Injecting numerical reasoning skills into language models)",
            "brief_description": "A transformer-based model (BERT pretraining adapted for numeric reasoning) evaluated on the DROP numerical MRC benchmark; used as a main proxy model in experiments and evaluated with different number surface forms.",
            "citation_title": "Injecting numerical reasoning skills into language models",
            "mention_or_use": "use",
            "model_name": "GenBERT",
            "model_size": null,
            "model_architecture": "encoder-only transformer (BERT-based, fine-tuned for text-to-answer generation)",
            "arithmetic_operation_type": "counting, sorting, and basic arithmetic operations (e.g., addition/multiplication implied by DROP examples and by perturbation experiments)",
            "number_range_or_complexity": "Varied — training data contains numbers up to millions (e.g., CARDINAL max 48,466,928); evaluation included extrapolated numbers via Add(10), Add(100), Factor(10), Factor(100)",
            "method_or_intervention": "Fine-tuning on DROP; evaluated on perturbed (Add/Factor) versions of evaluation set; experiments with alternative number surface forms (original subword, digit, 10e-based, 10-based, and proposed E-digit); GenBERT used as a generator for formatted numeric outputs",
            "performance_result": "Interpolation (original dev): EM 68.80, F1 72.30. Add(10): EM 60.67, F1 63.80. Add(100): EM 56.73, F1 59.72. Factor(10): EM 45.41, F1 47.58. Factor(100): EM 42.78, F1 45.10. With E-digit (extrapolate setting): on Add(100) All EM 59.97 F1 68.24; on Factor(100) All EM 57.91 F1 63.98. (Detailed breakdowns by answer type available in Table 3 of the paper.)",
            "mechanistic_insight": "GenBERT benefits from digit-aware pretraining (authors note its pretraining uses digit-subword inputs), which gives some numeric ability, but it fails to generalize to out-of-distribution magnitudes because standard subword tokenization fragments numbers and the model lacks a strong inductive bias for digit-place structure; changing surface form to E-digit supplies explicit digit-place cues that the model can exploit.",
            "performance_scaling": "Performance drops substantially as the magnitude shift increases (Factor(10) -&gt; Factor(100) produces large declines); no discussion of scaling with model parameter count.",
            "failure_modes": "Fails to extrapolate to unseen number magnitudes; performance degrades sharply when passage numbers are multiplied by large factors (e.g., Factor(100)); struggles particularly on numeric/date answer categories when presented with out-of-range numbers under original tokenization.",
            "comparison_baseline": "Compared against its own variants with alternative number surface forms (digit, 10e-based, 10-based, E-digit) and against other DROP models (NAQANet, NumNet, NumNet+RoBERTa).",
            "key_finding": "GenBERT shows high interpolation performance on DROP but fails to extrapolate to out-of-distribution numeric magnitudes; representing numbers with the proposed E-digit surface form substantially improves extrapolation performance.",
            "uuid": "e332.0"
        },
        {
            "name_short": "NAQANet",
            "name_full": "NAQANet (DROP baseline QA model)",
            "brief_description": "The official baseline model for DROP (task-specific QA model that handles spans and numeric answers); evaluated to measure extrapolation sensitivity to number-range perturbations.",
            "citation_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "mention_or_use": "use",
            "model_name": "NAQANet",
            "model_size": null,
            "model_architecture": "task-specific QA model with heads for numeric answers (baseline for DROP); not a large pretrained LM encoder in the form used for other baselines",
            "arithmetic_operation_type": "counting and basic arithmetic operations as required by DROP (including addition/subtraction over multiple numbers supporting an answer)",
            "number_range_or_complexity": "Evaluated with perturbations Add(10), Add(100), Factor(10), Factor(100) applied to CARDINAL/QUANTITY/MONEY tokens; original dataset contains numbers up to tens of millions in some categories",
            "method_or_intervention": "Evaluated on perturbed versions of DROP dev set (no special surface-form intervention in the baseline row shown)",
            "performance_result": "Interpolation (original dev): EM 46.20, F1 49.24. Add(10): EM 40.87, F1 43.61. Add(100): EM 38.31, F1 40.34. Factor(10): EM 30.17, F1 34.09. Factor(100): EM 23.03, F1 26.01.",
            "mechanistic_insight": "No internal mechanistic analysis provided; empirical evidence shows large sensitivity to out-of-range numbers, implying NAQANet lacks an inductive bias enabling numeric extrapolation.",
            "performance_scaling": "Performance monotonically degrades as perturbation magnitude increases (worse on multiplicative perturbations), indicating poor scaling to larger numeric magnitudes.",
            "failure_modes": "Severe performance drop on extrapolated numbers, especially under multiplicative perturbations (Factor(100)).",
            "comparison_baseline": "Compared to other DROP models (NumNet variants, GenBERT) on the same perturbation schemes.",
            "key_finding": "As a baseline, NAQANet demonstrates that DROP-capable models can drastically lose accuracy when passage numbers shift outside the training range, highlighting lack of extrapolation.",
            "uuid": "e332.1"
        },
        {
            "name_short": "NumNet",
            "name_full": "NumNet (Machine reading comprehension with numerical reasoning)",
            "brief_description": "A DROP model that incorporates numeric nodes and graph structure to encode relative magnitude relationships between numbers; evaluated for extrapolation robustness.",
            "citation_title": "NumNet: Machine reading comprehension with numerical reasoning",
            "mention_or_use": "use",
            "model_name": "NumNet",
            "model_size": null,
            "model_architecture": "graph-augmented QA model (numbers represented as graph nodes with magnitude-aware edges) with a transformer encoder for text",
            "arithmetic_operation_type": "numeric reasoning tasks required by DROP: counting, sorting, and arithmetic over multiple numbers (addition/multiplicative perturbations used in evaluation)",
            "number_range_or_complexity": "Same perturbation regimes (Add/Factor up to x100); training data contains a long-tailed distribution including large numbers (up to millions)",
            "method_or_intervention": "Fine-tuned on original DROP; evaluated on perturbed ADD/FACTOR test sets to measure extrapolation",
            "performance_result": "Interpolation (original dev): EM 64.92, F1 68.31. Add(10): EM 57.01, F1 61.12. Add(100): EM 56.70, F1 60.75. Factor(10): EM 39.76, F1 43.84. Factor(100): EM 27.55, F1 31.68.",
            "mechanistic_insight": "Although NumNet explicitly models relative magnitudes via graphs (an inductive bias for numeric relationships), it still fails to extrapolate to large out-of-distribution magnitudes — suggesting that relative-magnitude graph structure alone does not guarantee extrapolation to unseen absolute ranges.",
            "performance_scaling": "Degrades strongly with multiplicative perturbations; relative performance remains better than some baselines on interpolation but not robust to large magnitude shifts.",
            "failure_modes": "Marked drop on Factor(10)/Factor(100), indicating inability to generalize arithmetic reasoning to numbers with magnitudes outside training distribution.",
            "comparison_baseline": "Compared to NAQANet, NumNet+(RoBERTa), and GenBERT across same perturbation settings.",
            "key_finding": "NumNet's graph-based magnitude modeling helps interpolation but does not provide sufficient inductive bias to extrapolate to much larger numbers.",
            "uuid": "e332.2"
        },
        {
            "name_short": "NumNet+RoBERTa",
            "name_full": "NumNet with RoBERTa encoder (NumNet+(RoBERTa))",
            "brief_description": "A stronger variant of NumNet that replaces the encoder with RoBERTa, achieving higher interpolation performance but still exhibiting extrapolation failure under large numeric perturbations.",
            "citation_title": "NumNet: Machine reading comprehension with numerical reasoning",
            "mention_or_use": "use",
            "model_name": "NumNet+(RoBERTa)",
            "model_size": null,
            "model_architecture": "graph-augmented numeric reasoning model with RoBERTa encoder (encoder-only transformer)",
            "arithmetic_operation_type": "DROP numeric reasoning (counting, sorting, arithmetic over multiple numbers)",
            "number_range_or_complexity": "Evaluated with Add(10), Add(100), Factor(10), Factor(100); originally trained on DROP distribution (numbers up to millions in some types)",
            "method_or_intervention": "Fine-tuning on DROP with RoBERTa encoder; evaluated on perturbed evaluation sets",
            "performance_result": "Interpolation (original dev): EM 81.07, F1 84.42. Add(10): EM 63.38, F1 66.19. Add(100): EM 62.30, F1 66.05. Factor(10): EM 55.44, F1 66.01. Factor(100): EM 55.04, F1 58.80. (Shows strong interpolation but large drops under Add/Factor perturbations.)",
            "mechanistic_insight": "Stronger pretrained encoder improves interpolation but does not fundamentally resolve extrapolation; the model still relies on surface-form and data-distribution cues rather than a generalizable numeric algorithm.",
            "performance_scaling": "High interpolation scores with stronger encoder, but extrapolation still degrades with increased numeric magnitude — RoBERTa helps but is not sufficient.",
            "failure_modes": "Substantial performance decrease when numbers are moved out of training range; multiplicative perturbations still reduce accuracy markedly.",
            "comparison_baseline": "Compared to NumNet (non-RoBERTa), NAQANet, and GenBERT; also evaluated alongside different surface-form representations.",
            "key_finding": "Upgrading encoder (to RoBERTa) raises baseline interpolation performance but does not eliminate failure to extrapolate to unseen numeric magnitudes.",
            "uuid": "e332.3"
        },
        {
            "name_short": "E-digit",
            "name_full": "E-digit surface form representation (proposed in this paper)",
            "brief_description": "A novel number surface-form representation that encodes numbers as digit tokens augmented by a position-independent 'e' token followed by a digit-position number, designed to provide digit-place information while remaining position-independent to enable extrapolation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "E-digit surface form (applied to GenBERT in experiments)",
            "model_size": null,
            "model_architecture": "surface-form / tokenization intervention applied to transformer-based QA models (experimentally applied to GenBERT)",
            "arithmetic_operation_type": "Same DROP operations (counting, addition/subtraction implied by multi-number aggregation); extrapolation tested via Add(100) and Factor(100) perturbations",
            "number_range_or_complexity": "Designed to generalize across digit lengths and magnitudes; evaluated on perturbations that produce numbers well outside training range (multiplication by 100, addition by 100, etc.) and datasets containing numbers up to millions",
            "method_or_intervention": "Replace numeric tokens in training and evaluation with the E-digit encoding (digit-level tokens plus 'e' tokens with position identifiers); fine-tune GenBERT on E-digit variants and evaluate on extrapolated test sets",
            "performance_result": "E-digit(Extrapolate) outperforms other surface forms on extrapolated data. Representative results (GenBERT): Add(100) All: Original EM 56.73 F1 59.72 -&gt; E-digit EM 59.97 F1 68.24. Factor(100) All: Original EM 42.78 F1 45.10 -&gt; E-digit EM 57.91 F1 63.98. Breakdowns: Add(100) Number: EM 62.42 F1 75.03; Date: EM 50.96 F1 58.78; Others: EM 64.47 F1 77.94. Factor(100) Number: EM 61.56 F1 63.72; Date: EM 41.40 F1 55.74; Others: EM 55.48 F1 64.68. E-digit(Interpolate) on original dev: EM 68.14 F1 71.05 (comparable to GenBERT baseline EM 68.80 F1 72.30).",
            "mechanistic_insight": "E-digit supplies explicit digit-level and digit-place cues via a position-independent 'e' token; this creates an inductive bias allowing the model to represent digit-place structure and generalize arithmetic reasoning beyond seen magnitudes. Authors hypothesize the 'e' embedding is reused across digit positions enabling length/magnitude generalization; they also note pretraining mismatch (digit subword format in GenBERT pretraining) can affect interpolation.",
            "performance_scaling": "Substantially improves extrapolation (especially multiplicative perturbations) relative to original subword tokenization and other surface forms; still does not fully close the gap to interpolation-level performance, indicating remaining limitations.",
            "failure_modes": "Although it improves extrapolation, models with E-digit still underperform the original interpolation setting and may be impacted by pretraining mismatches (performance slightly lower in some interpolation tests); other non-surface-form limitations remain.",
            "comparison_baseline": "Directly compared against original subword tokenization, digit-level, 10e-based, and 10-based surface forms (the latter three from prior work), showing E-digit gives the best extrapolation results.",
            "key_finding": "Providing an explicit, position-independent digit-place tokenization (E-digit) is an effective intervention to inject an inductive bias for numbers and enables much better extrapolation to out-of-distribution numeric magnitudes.",
            "uuid": "e332.4"
        },
        {
            "name_short": "Surface-form variants",
            "name_full": "Number surface-form interventions (original subword, digit, 10e-based, 10-based)",
            "brief_description": "Different ways of representing numbers as input tokens: original subword tokenization, digit-level tokenization, 10e-based and 10-based (from Nogueira et al.), each tested for impact on extrapolation in DROP.",
            "citation_title": "Investigating the limitations of the transformers with simple arithmetic tasks",
            "mention_or_use": "use",
            "model_name": "Surface-form variants (applied to GenBERT: original subword, digit, 10e-based, 10-based)",
            "model_size": null,
            "model_architecture": "tokenization / input representation modifications applied to transformer encoder models",
            "arithmetic_operation_type": "Basic arithmetic tasks as required by DROP; experiments used additive and multiplicative perturbations to probe extrapolation",
            "number_range_or_complexity": "Tested on Add(100) and Factor(100) extrapolated datasets (numbers moved well outside the training distribution)",
            "method_or_intervention": "Create alternative versions of training data using each surface form and fine-tune GenBERT on them; evaluate on perturbed dev sets",
            "performance_result": "Representative All-scores (GenBERT) on Add(100): Original EM 56.73 F1 59.72; 10e-based EM 57.29 F1 64.98; 10-based EM 59.19 F1 67.85; Digit EM 59.38 F1 67.90; E-digit EM 59.97 F1 68.24. On Factor(100): Original EM 42.78 F1 45.10; 10e-based EM 43.02 F1 49.94; 10-based EM 49.24 F1 56.47; Digit EM 44.97 F1 51.76; E-digit EM 57.91 F1 63.98.",
            "mechanistic_insight": "Providing digit-level information (digit, 10-based, 10e-based) helps performance on extrapolated numbers relative to raw subword tokenization; 10-based forms (position-dependent embeddings for power-of-ten slots) can help for structured domains like dates but require position-specific embeddings that scale with digit length; E-digit's position-independent 'e' token avoids combinatorial growth and supports extrapolation across lengths.",
            "performance_scaling": "Digit-aware surface forms consistently outperform raw subword tokenization on extrapolated tests; 10-based helps some categories (e.g., dates), but E-digit gives the best overall extrapolation-generalization tradeoff.",
            "failure_modes": "10e-based and 10-based forms still failed to fully extrapolate beyond seen numeric lengths in previous work; position-dependent embeddings scale poorly with longer numbers and do not generalize to unseen digit lengths.",
            "comparison_baseline": "Directly compared to each other and to E-digit in GenBERT experiments.",
            "key_finding": "Surface-form choice materially affects arithmetic/extrapolation performance: digit-aware representations improve robustness, and a position-independent digit-place token (E-digit) provides the best extrapolation among tested forms.",
            "uuid": "e332.5"
        },
        {
            "name_short": "Perturbation schemes (Add/Factor)",
            "name_full": "Add(N) and Factor(N) numerical perturbation schemes for probing extrapolation",
            "brief_description": "Evaluation method that perturbs all actionable numeric tokens in passages/questions/answers by adding (Add(N)) or multiplying (Factor(N)) by a constant to create out-of-distribution numeric contexts to probe model extrapolation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Add(N)/Factor(N) perturbation evaluation (applied to DROP evaluation set)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Probes addition and multiplication invariance/robustness (also indirectly tests multi-number arithmetic and counting by preserving distributive validity of arithmetic answers)",
            "number_range_or_complexity": "Perturbations used: Add(10), Add(100), Factor(10), Factor(100); these shift numbers well outside typical training ranges (e.g., multiplications produce numbers orders of magnitude larger)",
            "method_or_intervention": "Apply consistent addition or multiplication to QUANTITY, CARDINAL, and MONEY entities in passages/questions/answers and evaluate standard models without retraining (or with surface-form retraining when testing surface-form interventions).",
            "performance_result": "Across models, even modest shifts (Add(10)/Factor(10)) reduce accuracy; larger shifts (Add(100)/Factor(100)) produce large drops. Example summary (GenBERT): see entries above. Table 1 shows large EM/F1 drops for NAQANet, NumNet, NumNet+(RoBERTa), GenBERT.",
            "mechanistic_insight": "These perturbations expose that models rely on surface-form statistics and training-distribution numeric ranges rather than learned general numeric algorithms; multiplicative shifts are especially damaging, revealing poor extrapolation to larger magnitudes.",
            "performance_scaling": "Performance monotonically decreases as perturbation magnitude grows (Factor(100) worse than Factor(10), etc.).",
            "failure_modes": "Models that perform well on interpolation fail catastrophically under multiplicative perturbations; count-type answers require heuristics to avoid accidental mislabelling when perturbed.",
            "comparison_baseline": "Used as a diagnostic to compare models and surface-form interventions (original vs digit vs 10e vs 10-based vs E-digit).",
            "key_finding": "Add/Factor perturbations are an effective probe: state-of-the-art DROP models generally cannot extrapolate to out-of-distribution numeric magnitudes, and surface-form interventions (especially E-digit) materially mitigate but do not fully solve this limitation.",
            "uuid": "e332.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating the limitations of the transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Neural arithmetic logic units",
            "rating": 2
        },
        {
            "paper_title": "Neural arithmetic units",
            "rating": 2
        },
        {
            "paper_title": "NumNet: Machine reading comprehension with numerical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2
        },
        {
            "paper_title": "Representing numbers in NLP: a survey and a vision",
            "rating": 1
        }
    ],
    "cost": 0.018532749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Have You Seen That Number? Investigating Extrapolation in Question Answering Models</h1>
<p>Jeonghwan Kim Giwon Hong<em> Kyung-min Kim</em><br>Junmo Kang* Sung-Hyon Myaeng<br>School of Computing, KAIST<br>Daejeon, Republic of Korea<br>{jeonghwankim123, giwon.hong, kimdarwin, junmo.kang, myaeng}@kaist.ac.kr</p>
<h4>Abstract</h4>
<p>Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the models fail to extrapolate to unseen numbers. Presenting numbers as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.</p>
<h2>1 Introduction</h2>
<p>The research in question-answering (QA) models that are able to perform reading comprehension and discrete reasoning over numbers in the passage has seen significant progress, like the models in DROP (Ran et al., 2019; Hu et al., 2019; Chen et al., 2020; Geva et al., 2020). Despite their ability to understand the complex context and numbers within, none of them deal with the notion of whether these models can robustly handle "unseen" numbers during testing. The ability to extend discrete, symbolic rules such as addition and subtraction on numbers outside what we already know is called extrapolation, and this is an essential part of human intelligence. For example, if one can reason on numbers over text that range between 0 and 100, it is logically reasonable to infer that it should be able to handle numbers larger than</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of DROP data instance perturbation to test the numerical QA models for extrapolation capabilities. This figure shows CARDINAL-type numbers factored by 100 .
100. The lack of extrapolation capability in models, however, is a significant obstacle in the way toward a truly generalizable, number-understanding QA.</p>
<p>Although the problem of numerical extrapolation has been recently addressed by previous works in arithmetic word problem (AWP) settings (Trask et al., 2018; Madsen and Johansen, 2020; Kim et al., 2021) where the given instances involve simple math problems like "What is $24+5$ ?", their proposed approaches do not have the ability to handle two or more supporting facts (Kim et al., 2021), which is a capability demanded by DROP to handle multiple numbers, or deal with negative numbers or learn question-context relation (Trask et al., 2018). These limitations preclude the possibility of applying their extrapolation capability on the DROP task, where models are required to reason over multiple sentences while dealing with heterogeneous number types (e.g., percentage, cardinal, date), unlike in AWP settings where the numbers are simple, homogeneous type scalars. To see if the state-of-the-art models for DROP possess the extrapolation</p>
<p>capability we design the perturbated version of the DROP evaluation dataset as in Figure 1 (Section 3 for details). Surprisingly, the models show significant performance drop only when the range of numbers appearing in the passage is changed.</p>
<p>We also note that the models for DROP typically use transformer models as the encoder for context understanding. As shown in Wallace et al. (2019); Geva et al. (2020), subword tokenization methods arbitrarily subdivide the numbers and cause two very similar numbers to be in two disparate forms. This observation is in line with Nogueira et al. (2021)'s conclusion that how the numbers are presented to the model, or their surface form, influences the modeling of numbers. The surface forms proposed in Nogueira et al. (2021) provide digitplace information with a special set of tokens (the first three surface forms in Figure 2), to increase model's accuracy in a simple addition task. However, they fail to imbue the extrapolation capability in their tested models, observing that the addition rules could not be extended beyond the length of numbers seen during training. Therefore, we propose a new surface form called E-digit (Figure 2) that addresses the lack of extrapolation capability in the models. Our E-digit method successfully generalizes to out-of-distribution numbers and outperforms all the other surface forms by a significant margin.</p>
<h2>2 Related Work</h2>
<p>Previous works like NumNet (Ran et al., 2019) attempt to tackle the DROP task by using graphs to imbue the model with the relative magnitude information. GenBERT (Geva et al., 2020) pre-trains BERT (Devlin et al., 2019) with synthetic number and text data. QDGAT (Chen et al., 2020) designs a graph neural network with fully-connected number nodes of same entity type. While there are many other related works on this topic (Hu et al., 2019; Andor et al., 2019; Gupta et al., 2019; Min et al., 2019; Sundararaman et al., 2020; Saha et al., 2021), none of them address the problem of extrapolation in DROP. Although Wallace et al. (2019) reveals that NAQANet (Dua et al., 2019) struggles to deal with numbers outside the training range, showing a drop in performance in the extrapolation setting, they simply treat it as one of the failure modes in NAQANet and provide no further analysis on this alarming issue on model reliability. A survey on numerical representations (Thawani et al., 2021)</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of how each surface form is represented and replaced in text.</p>
<p>also mentions the extrapolation issue frequently found in these models, only to stop at reiterating the already identified issues.</p>
<p>A recent study by Nogueira et al. (2021) on the changes in how a number is presented to a model shows that different surface forms have significant influence on T5 (Raffel et al., 2020) in solving a simple arithmetic task. However, their proposed surface forms fail to extrapolate. They also explicitly provide the arithmetic operators and do not require complicated textual understanding for discrete reasoning. This begs the question of whether the same approach is viable in DROP, where reasoning is done across multiple sentences, requires dealing with heterogeneous type numbers, and the operations should be inferred from the text.</p>
<h2>3 Empirical Investigation on Extrapolation</h2>
<p>We first seek to determine whether the state-of-the-art models on DROP (Dua et al., 2019) can extrapolate their numerical reasoning capabilities to unseen numbers during inference.</p>
<p>Dataset DROP is a reading comprehension benchmark that requires models to perform a set of discrete reasoning such as counting, sorting and basic arithmetic operations. In this work, we construct the extrapolated version of DROP evaluation set by perturbing numbers with addition and multiplication of pre-defined numbers as in Figure 1. Then, we test the existing models for their extrapolation capabilities with these variant datasets.</p>
<p>Data Perturbation Prior to constructing the evaluation datasets, we use a named entity recog-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Interpolate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Add(10)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Add(100)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Factor(10)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Factor(100)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">NAQANet</td>
<td style="text-align: center;">46.20</td>
<td style="text-align: center;">49.24</td>
<td style="text-align: center;">40.87</td>
<td style="text-align: center;">43.61</td>
<td style="text-align: center;">38.31</td>
<td style="text-align: center;">40.34</td>
<td style="text-align: center;">30.17</td>
<td style="text-align: center;">34.09</td>
<td style="text-align: center;">23.03</td>
<td style="text-align: center;">26.01</td>
</tr>
<tr>
<td style="text-align: center;">NumNet</td>
<td style="text-align: center;">64.92</td>
<td style="text-align: center;">68.31</td>
<td style="text-align: center;">57.01</td>
<td style="text-align: center;">61.12</td>
<td style="text-align: center;">56.70</td>
<td style="text-align: center;">60.75</td>
<td style="text-align: center;">39.76</td>
<td style="text-align: center;">43.84</td>
<td style="text-align: center;">27.55</td>
<td style="text-align: center;">31.68</td>
</tr>
<tr>
<td style="text-align: center;">NumNet+(RoBERTa)</td>
<td style="text-align: center;">81.07</td>
<td style="text-align: center;">84.42</td>
<td style="text-align: center;">63.38</td>
<td style="text-align: center;">66.19</td>
<td style="text-align: center;">62.30</td>
<td style="text-align: center;">66.05</td>
<td style="text-align: center;">55.44</td>
<td style="text-align: center;">66.01</td>
<td style="text-align: center;">55.04</td>
<td style="text-align: center;">58.80</td>
</tr>
<tr>
<td style="text-align: center;">GenBERT</td>
<td style="text-align: center;">68.80</td>
<td style="text-align: center;">72.30</td>
<td style="text-align: center;">60.67</td>
<td style="text-align: center;">63.80</td>
<td style="text-align: center;">56.73</td>
<td style="text-align: center;">59.72</td>
<td style="text-align: center;">45.41</td>
<td style="text-align: center;">47.58</td>
<td style="text-align: center;">42.78</td>
<td style="text-align: center;">45.10</td>
</tr>
</tbody>
</table>
<p>Table 1: The baseline and state-of-the-art DROP models tested on our four extrapolated versions of DROP evaluation dataset. Add(N) means adding N and Factor(N) means multiplying N to the numbers that appear in the passage. Interpolate shows the results on the original evaluation dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Count</th>
<th style="text-align: right;">Max</th>
<th style="text-align: right;">Min</th>
<th style="text-align: right;">Median</th>
<th style="text-align: right;">MAD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CARDINAL</td>
<td style="text-align: center;">33,732</td>
<td style="text-align: right;">$48,466,928$</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">31</td>
<td style="text-align: right;">140</td>
</tr>
<tr>
<td style="text-align: left;">MONEY</td>
<td style="text-align: center;">1,561</td>
<td style="text-align: right;">$653,422,000$</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">289.5</td>
<td style="text-align: right;">318.5</td>
</tr>
<tr>
<td style="text-align: left;">QUANTITY</td>
<td style="text-align: center;">23,072</td>
<td style="text-align: right;">$4,000,160$</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">19</td>
</tr>
<tr>
<td style="text-align: left;">DATE</td>
<td style="text-align: center;">30,549</td>
<td style="text-align: right;">105,000</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1,907</td>
<td style="text-align: right;">191.5</td>
</tr>
<tr>
<td style="text-align: left;">TIME</td>
<td style="text-align: center;">2,627</td>
<td style="text-align: right;">160</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">13.5</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: left;">PERCENT</td>
<td style="text-align: center;">5,123</td>
<td style="text-align: right;">280</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">14</td>
</tr>
</tbody>
</table>
<p>Table 2: Type-wise analysis on numbers in DROP training dataset. MAD is the median absolute deviation.
nition (NER) system ${ }^{1}$ to extract and identify seven different entity types among the numbers in the text, namely: ORDINAL, DATE, QUANTITY, CARDINAL, MONEY, TIME, PERCENT. Among these seven entity types, we apply the aforementioned extrapolation perturbation to QUANTITY, CARDINAL and MONEY only, because DATE, PERCENT, TIME and ORDINAL require typespecific, handcrafted perturbations. For instance, if we were to perturb, "King James was born in May 25, 1926", it is not possible to simply change the range of " 25 " and " 1926 " by multiplying a 100, which neglects the entity-specific characteristics and requires question-level adjustment. Since we are probing the models to evaluate their extrapolation capability on unseen numbers, changing the range of the three types suffices. We use the four versions of extrapolated DROP evaluation set to observe the changes in performance along with the magnitude of changes in number range: Add(10), Add(100), Factor(10), Factor(100). Add(N) means adding N and Factor(N) means multiplying N to the numbers that appear in the passage.</p>
<p>The numbers from the passage, question and answer are perturbed with one of the four perturbation schemes above. Naturally, by the distributive law, the validity of the perturbed answer value holds. For example (see Figure 1), applying Factor(100)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: MONEY type number distribution in the DROP train and evaluation datasets. Bin width is set to 50 with the numbers shown up to 80th percentile for visibility. The numbers are highly skewed to right.
to a sequence, $49,927+18,009+12,182=80,128$, results in $100 *(49,927+18,009+12,182)=100$ * $(80,128)$. The same rule applies to other perturbation methods. As for the count-type answers that consist of numbers, we apply a heuristic where we consider both the number answers within the range of 0 to 9 and their extrapolated variants as answers. This prevents accidental perturbation of count-type answers and also considers arithmetic-type answers that have been extrapolated.</p>
<p>Models To inspect the extrapolation capability among the existing models in DROP, we evaluate the following representative models in the leaderboard: NAQANet (the official baseline model in DROP), NumNet (Ran et al., 2019), NumNet+(RoBERTa) and GenBERT (Geva et al., 2020). Although we mention QDGAT (Chen et al., 2020) in this paper, we did not evaluate it because its official implementation could not be reproduced.</p>
<p>Probing Models for Extrapolation We experiment on the models with the extrapolated DROP dataset and show that model performances degrade significantly as in Table 1. One notable observation from this experiment is that as the range of numbers increase ("Factor(10)" -&gt; "Factor(100)"),</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Surface Form</th>
<th style="text-align: center;">Number</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Others</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">All</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Add(100)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">62.05</td>
<td style="text-align: center;">62.06</td>
<td style="text-align: center;">33.12</td>
<td style="text-align: center;">34.44</td>
<td style="text-align: center;">48.97</td>
<td style="text-align: center;">57.02</td>
<td style="text-align: center;">56.73</td>
<td style="text-align: center;">59.72</td>
</tr>
<tr>
<td style="text-align: center;">10e-based</td>
<td style="text-align: center;">62.27</td>
<td style="text-align: center;">70.14</td>
<td style="text-align: center;">32.60</td>
<td style="text-align: center;">33.91</td>
<td style="text-align: center;">49.32</td>
<td style="text-align: center;">61.36</td>
<td style="text-align: center;">57.29</td>
<td style="text-align: center;">64.98</td>
</tr>
<tr>
<td style="text-align: center;">10-based</td>
<td style="text-align: center;">62.55</td>
<td style="text-align: center;">74.01</td>
<td style="text-align: center;">37.97</td>
<td style="text-align: center;">48.13</td>
<td style="text-align: center;">51.61</td>
<td style="text-align: center;">56.91</td>
<td style="text-align: center;">59.19</td>
<td style="text-align: center;">67.85</td>
</tr>
<tr>
<td style="text-align: center;">Digit</td>
<td style="text-align: center;">62.47</td>
<td style="text-align: center;">64.58</td>
<td style="text-align: center;">33.71</td>
<td style="text-align: center;">33.92</td>
<td style="text-align: center;">49.02</td>
<td style="text-align: center;">56.98</td>
<td style="text-align: center;">59.38</td>
<td style="text-align: center;">67.90</td>
</tr>
<tr>
<td style="text-align: center;">E-digit (Extrapolate)</td>
<td style="text-align: center;">62.42</td>
<td style="text-align: center;">75.03</td>
<td style="text-align: center;">50.96</td>
<td style="text-align: center;">58.78</td>
<td style="text-align: center;">64.47</td>
<td style="text-align: center;">77.94</td>
<td style="text-align: center;">59.97</td>
<td style="text-align: center;">68.24</td>
</tr>
<tr>
<td style="text-align: center;">Factor(100)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">39.11</td>
<td style="text-align: center;">43.25</td>
<td style="text-align: center;">17.85</td>
<td style="text-align: center;">30.42</td>
<td style="text-align: center;">46.51</td>
<td style="text-align: center;">57.18</td>
<td style="text-align: center;">42.78</td>
<td style="text-align: center;">45.10</td>
</tr>
<tr>
<td style="text-align: center;">10e-based</td>
<td style="text-align: center;">40.31</td>
<td style="text-align: center;">45.06</td>
<td style="text-align: center;">18.59</td>
<td style="text-align: center;">35.79</td>
<td style="text-align: center;">49.59</td>
<td style="text-align: center;">60.74</td>
<td style="text-align: center;">43.02</td>
<td style="text-align: center;">49.94</td>
</tr>
<tr>
<td style="text-align: center;">10-based</td>
<td style="text-align: center;">40.76</td>
<td style="text-align: center;">45.90</td>
<td style="text-align: center;">32.48</td>
<td style="text-align: center;">38.23</td>
<td style="text-align: center;">50.27</td>
<td style="text-align: center;">62.11</td>
<td style="text-align: center;">49.24</td>
<td style="text-align: center;">56.47</td>
</tr>
<tr>
<td style="text-align: center;">Digit</td>
<td style="text-align: center;">41.64</td>
<td style="text-align: center;">45.82</td>
<td style="text-align: center;">17.97</td>
<td style="text-align: center;">32.38</td>
<td style="text-align: center;">51.74</td>
<td style="text-align: center;">62.72</td>
<td style="text-align: center;">44.97</td>
<td style="text-align: center;">51.76</td>
</tr>
<tr>
<td style="text-align: center;">E-digit (Extrapolate)</td>
<td style="text-align: center;">61.56</td>
<td style="text-align: center;">63.72</td>
<td style="text-align: center;">41.40</td>
<td style="text-align: center;">55.74</td>
<td style="text-align: center;">55.48</td>
<td style="text-align: center;">64.68</td>
<td style="text-align: center;">57.91</td>
<td style="text-align: center;">63.98</td>
</tr>
</tbody>
</table>
<p>Table 3: Surface form evaluation with GenBERT on our Add(100) and Factor(100) extrapolated versions of DROP evaluation dataset. Our E-digit (Extrapolate) method outperforms all the other surface forms by a large margin.
model performances decrease accordingly. The result shows that even a small shift in the number range affects the model performance, implying that it is partly due to sample inefficiency. Meaning, that the perturbations create numbers that exist outside the number distribution in the training dataset, with the model trying to handle the vast coverage of numbers with typical subword representations. This problem is evident in the highly skewed number distribution in both the DROP training and evaluation dataset. The right-skewed distribution for the MONEY-type numbers in Figure 3, for example, shows a long tail, with the frequency of numbers in the training text quickly degrading as the magnitude of numbers grow (similar distribution is exhibited in CARDINAL, QUANTITY and PERCENT). This is also apparent in Table 2, where we see numbers that range up to millions but the median absolute deviation (MAD) is overly large for CARDINAL, MONEY and DATE. For TIME, PERCENT and QUANTITY, although we see a negligible spread, MAD's characteristic of ignoring the outliers like the MAX value of QUANTITY may have ignored less frequent, larger values. Such number distribution inhibits the models from generating an inductive bias for numbers, as the model is going to encounter only the numbers within the limited range during training. This lack of inductive bias for numbers prevents the model from extrapolating to out-of-distribution numbers in text. Thus, it is essential that the model gains a strong inductive bias for numbers, despite seeing numbers of
arbitrary lengths.</p>
<h2>4 Injecting Inductive Bias on Numbers with Surface Form Representations</h2>
<p>After revealing the lack of extrapolation capability in the models, we gauge the influence of different surface forms of numbers as input to the MRC models. Based on the observation on the importance of surface forms in arithmetic word problems (AWP) (Nogueira et al., 2021), we evaluate if altering the surface form representation of numbers in DROP alleviates the performance discrepancy shown in Table 1. Moreover, we propose the new E-digit surface form to overcome the limitations of previous surface forms in extrapolation.</p>
<p>Surface Form Methods Our E-digit method makes use of two types of tokens, " $e$ " and "digit", to reconstruct the numbers in the passage as in Figure 2. To elaborate, the E-digit method augments the typical digit-level number surface form by providing digit-position information with the $e$ token and its corresponding digit number (See Figure 2). The three other surface forms proposed in Nogueira et al. (2021), namely 10e-based, 10based and digit forms are composed of " $10 e #$ ", " $10^{\text {th }}$ " and numbers separated into digit-level representation, respectively. The principal difference between our E-digit and the three surface forms is that the $e$ token embedding is digit-position independent, meaning it can occupy any digit-position as long as its followed by the digit-position number. On the contrary, 10e-based and 10-based methods</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GenBERT</td>
<td style="text-align: center;">68.80</td>
<td style="text-align: center;">72.30</td>
</tr>
<tr>
<td style="text-align: left;">E-digit (Interpolate)</td>
<td style="text-align: center;">68.14</td>
<td style="text-align: center;">71.05</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison between the GenBERT model and its E-digit variant (i.e., E-digit(Interpolate)), which is trained with E-digit method and evaluated on the Edigit DROP dev set.
require a separate embedding for every digit position, with its number growing proportionally to the length of a number.</p>
<p>Here, we hypothesize that providing a positionindependent token as in E-digit enables the model to leverage the " $e$ " embedding to improve the extrapolation capability. We provide four versions of the original training dataset for the above four surface forms, and apply the same perturbation "Factor(100)" on evaluation set for inference. We use GenBERT as our proxy model because we need the model to generate answer texts like "2 e 27 e 10 e 0 ," whereas other models are incapable of generating calculated number answers in different surface forms, only performing span extraction and using special heads to assign ${+,-, 0}$ on numbers appearing in the passage.</p>
<p>To validate the utility of the E-digit approach in the default, non-extrapolated setting, we compare the performance of the original GenBERT model against the E-digit(Interpolate) (Table 4), which is GenBERT fine-tuned with the E-digit method and evaluated on the original DROP evaluation set. Despite minor degradation in performance, the E-digit (Interpolate) performs comparably to GenBERT, which proves its effectiveness in representing numbers like digit tokenization does in the original GenBERT model. Our interpretation to such an outcome is that the performance gap is most likely caused by GenBERT's pre-training scheme (Geva et al., 2020), which employs digit subword inputs ( $14 \rightarrow 1 # # 4$ ) to solve simple arithmetic problems to induce numerical reasoning skills. This may have caused the input mismatch issue since digitlevel information explicitly provided by E-digit is absent during pre-training.</p>
<p>Analysis of Different Surface Forms The notable observation in Table 3 is that our E-digit method outperforms all the other surface forms, including the original model on the extrapolate DROP dataset. Also, the surface form methods all outperform the original models' subword tok-
enization approach. The results empirically show that: (i) providing digit information (" $e$ ", "10e#") along with numbers in their digit form is important in modeling numbers for extrapolation in a complicated textual reasoning task, and (ii) from the EM and F1 scores, we realize that the models still underperform in the extrapolation task when compared to the original interpolation task. The latter suggests that, in addition to the surface form problem identified in our work, there still are problems with the current approaches to number modeling in numerical MRC models.</p>
<p>Further analysis on the different answer types in DROP provides insight into the relationship between the answer types and surface forms. The E-digit method outperforms other forms notably in Number and Date categories. This shows that the " $e$ " embedding learns to effectively represent numbers within the model despite seeing out-ofdistribution numbers. The 10-based surface form, to our surprise, outperforms other surface forms in the Date type answers. We speculate that such a result arises from the year-type numbers' characteristic of typically ranging between numbers of 1000 to 2000, which enables the model to learn the relevance of the embedding "1000" to numbers in a year-related context. Overall, the E-digit surface form provides an explicit digit-level information of a number, which in turn empowers the model to effectively preserve and represent number information for numerical reasoning over text.</p>
<h2>5 Conclusion</h2>
<p>In this work, we investigated the extrapolation problem in complex numerical reasoning over text. Our probing results shed light on the significant lack of DROP models' capabilities by simulating a more realistic and ultimately needed benchmark (i.e., extrapolation). One of the key findings is that treating numbers as words inevitably requires a vast coverage of numbers, leading to sample inefficiency. This motivated us to adopt a more generalizable surface form representation, proposing the E-digit method that successfully generalizes to unseen numbers. Empirical results highlight simple surface representations benefit the model with digit information for extrapolation, and our E-digit method effectively generalizes it further. Our work opens up a new research direction in numerical reasoning over text on how to reduce the discrepancy between the original and extrapolated settings.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by Institute for Information \&amp; communications Technology Planning \&amp; Evaluation(IITP) grant funded by the Korea government(MSIT) (No. 2013-2-00131, Development of Knowledge Evolutionary WiseQA Platform Technology for Human Knowledge Augmented Services).</p>
<h2>References</h2>
<p>Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving bert a calculator: Finding operations and arguments with reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 59495954.</p>
<p>Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, and Wei Chu. 2020. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6759-6768.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958.</p>
<p>Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2019. Neural module networks for reasoning over text. In International Conference on Learning Representations.</p>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. 2019. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the 2019 Conference on</p>
<p>Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1596-1606.</p>
<p>Segwang Kim, Hyoungwook Nam, Jin-Tea Kim, and K. Jung. 2021. Neural sequence-to-grid module for learning symbolic rules. ArXiv, abs/2101.04921.</p>
<p>Andreas Madsen and Alexander Rosenberg Johansen. 2020. Neural arithmetic units. In International Conference on Learning Representations.</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard em approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 28442857.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Li. 2021. Investigating the limitations of the transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019.</p>
<p>Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67.</p>
<p>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. Numnet: Machine reading comprehension with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2474-2484.</p>
<p>Amrita Saha, Shafiq Joty, and Steven CH Hoi. 2021. Weakly supervised neuro-symbolic module networks for numerical reasoning. arXiv preprint arXiv:2101.11802.</p>
<p>Dhanasekar Sundararaman, Shijing Si, Vivek Subramanian, Guoyin Wang, Devamanyu Hazarika, and Lawrence Carin. 2020. Methods for numeracypreserving word embeddings. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4742-4753.</p>
<p>Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely. 2021. Representing numbers in NLP: a survey and a vision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-</p>
<p>guage Technologies, pages 644-656, Online. Association for Computational Linguistics.</p>
<p>Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. 2018. Neural arithmetic logic units. Advances in Neural Information Processing Systems, 31:8035-8044.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do nlp models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5310-5318.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Stanford's Stanza toolkit for NLP (Qi et al., 2020)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
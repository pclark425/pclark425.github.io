<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8211 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8211</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8211</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-279250852</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07398v2.pdf" target="_blank">G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems</a></p>
                <p><strong>Paper Abstract:</strong> Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both $\textit{high-level, generalizable insights}$ that enable the system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed interaction trajectories}$ that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\%$ and $10.12\%$, respectively, without any modifications to the original frameworks. Our codes are available at https://github.com/bingreeky/GMemory.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8211.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8211.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Memory: Graph-based Agentic Memory Mechanism for LLM-based Multi-Agent Systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical, agentic memory architecture for multi-agent systems organizing experience into three graph tiers (insight, query, interaction) with bi-directional traversal (query→insight and query→interaction), role-specific filtering, and continual updates to enable MAS self-evolution and concise retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>G-Memory (plug-in for MAS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory module invoked by an MAS upon a new query; performs coarse retrieval on a query graph, 1-hop expansion, upward traversal to an insight graph (Π Q→I), downward traversal to extract sparsified interaction subgraphs (S_LLM), then role-specific memory initialization (Φ) for each agent, and updates all graphs after task execution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini (used as LLM backbones in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three different LLM backbones used to instantiate MAS agents in experiments: two open-source Qwen variants (2.5-7b, 2.5-14b) and a proprietary gpt-4o-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, ScienceWorld (SciWorld), PDDL, HotpotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks spanning embodied action (ALFWorld, SciWorld), strategic/game planning (PDDL), and knowledge QA / fact verification (HotpotQA, FEVER); tasks require multi-step coordination across agents and/or evidence-grounded reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied action, game/strategic planning, question answering / fact verification</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical agentic memory (episodic + distilled insight + structured query metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Coarse similarity-based retrieval over a query graph (embedding similarity), 1-hop graph expansion, upward projector to insight graph (Π Q→I) to retrieve distilled insights, downward LLM-based graph sparsifier (S_LLM) to extract core interaction subgraphs of past trajectories, LLM-based relevance scorer (R_LLM) for ranking, and role-specific filtering/adaptation (Φ) to initialize each agent's Mem_i. After execution, new interaction graphs, query node, and distilled insight nodes are created/updated.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Three-tier: (1) Interaction graphs: atomic utterance nodes (agent, text) and temporal edges representing dialogue/trajectory; (2) Query graph: nodes containing original query, status, and pointer to interaction graph; (3) Insight graph: distilled, generalizable insights (text) linked to supporting queries (Ω_k) and hyperedges indicating contextual relations between insights via queries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Embedding-based semantic search over query graph for top-k, 1-hop neighbor expansion, LLM relevance scoring (R_LLM) for ranking historical queries, upward lookup via query→insight projector (set-intersection of supporting queries), and LLM-facilitated sparsification to extract core interaction subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Varies by benchmark and MAS: example — MacNet + ALFWorld with Qwen-2.5-14b: 79.10% success rate with G-Memory (reported). Overall reported maximum gains: up to +20.89 percentage points on embodied action (ALFWorld) and up to +10.12 percentage points on knowledge QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Example baseline (no-memory) for the same setting: MacNet + ALFWorld with Qwen-2.5-14b: 58.21% success rate (no-memory). More generally, comparisons provided across all five benchmarks in Tables 1–3.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations: removing either the high-level insight module (I_S) or fine-grained interactions ({Ĝ^Qi_inter}) reduces performance. Only fine-grained interactions enabled -> average drop: 4.47% (AutoGen) and 3.82% (DyLAN). Only insights enabled -> smaller drops: 3.95% (AutoGen) and 3.39% (DyLAN). Sensitivity: 1-hop expansion and k∈{1,2} (top-k queries) are optimal; more hops or larger k degrade performance. Token-cost comparison: on PDDL+AutoGen, G-Memory gave +10.32% improvement over no-memory with ~1.4×10^6 tokens, whereas MetaGPT-M used ~2.2×10^6 tokens for only +4.07% gain.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hierarchical, role-specific memory that retrieves both distilled insights and condensed interaction trajectories substantially improves MAS performance and sample efficiency across domains; both insight-level abstraction and fine-grained trajectory condensation are necessary (complementary), with interactions contributing slightly more to performance; small graph-hop and top-k retrieval windows work best to avoid irrelevant noise; G-Memory is token-efficient compared to baseline MAS memory designs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Additional validation needed on more diverse tasks (e.g., medical QA) as acknowledged; relies on LLM-based scorers/sparsifiers (possible fragility to LLM errors); potential risk of amplifying incorrect reasoning if underlying LLM is compromised; complexity of maintaining and updating hierarchical graphs in large-scale deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8211.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8211.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT-M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT-Memory (adapted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Memory design originating from MetaGPT adapted as a baseline in which memory focuses on inside-trial artifacts and does not comprehensively capture cross-trial collaborative trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MetaGPT-M (memory design)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An MAS memory scheme that primarily stores inside-trial artifacts (information created/used during single task resolution) and limited cross-trial artifacts (often final solutions); used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini (used as LLM backbones in comparative experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See G-Memory entry for the LLM backbones used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, SciWorld, PDDL, HotpotQA, FEVER (same benchmark suite)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-domain benchmark suite; MetaGPT-M is evaluated as a competing memory approach when integrated into MAS frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied action, game/strategic planning, question answering</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>inside-trial focused (limited cross-trial artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Stores inside-trial artifacts created during a single task resolution; cross-trial memory if present is typically compressed final outcomes/solutions rather than rich interaction trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Final solutions / execution results and limited inside-trial context snippets; lacks structured interaction graphs and distilled insight nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Simple retrieval of stored artifacts (no hierarchical graph-driven retrieval reported); retrieval tends to be coarse and artifact-focused.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Mixed — sometimes yields small gains but can be token-inefficient: paper reports MetaGPT-M consumed extra ~2.2×10^6 tokens for only +4.07% gain in one reported comparison (PDDL+AutoGen example).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-memory baselines reported in tables; improvements relative to no-memory vary across tasks (sometimes small positive, sometimes negative).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared against G-Memory: MetaGPT-M delivered smaller performance gains and higher token cost in some settings; MetaGPT-M lacks trajectory condensation and role-specific memory, limiting MAS utility.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Inside-trial/artifact-centric memory (as in MetaGPT-M) is insufficient for MAS tasks that require role-specific, trajectory-aware, cross-trial learning; such designs can be token-inefficient relative to performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Overly simplistic cross-trial memory (final artifacts only) fails to capture collaborative dynamics; token cost can be high relative to benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8211.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8211.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDev-M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDev-Memory (adapted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline memory design adapted from ChatDev that stores inside-trial memory and a rudimentary form of cross-trial memory consisting mostly of past solutions, with a narrow memory scope.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatDev-M (memory design)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory approach that passes inside-trial memory from a central/initiating agent across rounds and records past solutions as cross-trial memory; used as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See G-Memory entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, SciWorld, PDDL, HotpotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-domain tasks; ChatDev-M is evaluated as a baseline memory mechanism when integrated into MAS frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied action, question answering, strategic planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>inside-trial + simple cross-trial (final solutions/artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Inside-trial memory passed from central agent; cross-trial memory stores past final solutions for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Past final solutions and inside-trial state passed by a central agent; does not store detailed inter-agent dialogues or distilled insights.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval of stored past solutions (artifact lookup) and passing previous inside-trial state at round start.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>In several experiments, ChatDev-M either provided limited benefit or degraded performance; example: applying ChatDev-M to MacNet+SciWorld resulted in a 2.32% drop in performance (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-memory baselines exist in tables; ChatDev-M often underperformed relative to richer memory designs like G-Memory.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared unfavorably to G-Memory on embodied tasks due to narrow scope and lack of trajectory insights; sometimes decreased MAS performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing only final past solutions (narrow cross-trial memory) is insufficient for embodied action tasks where interaction patterns and fine-grained trajectories matter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited utility in environments requiring procedural or collaborative nuance; inadequate for role-specific guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8211.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8211.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MacNet-M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MacNet-Memory (adopted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Memory design adopted from MacNet where inside-trial memory is essentially the final answers from previous rounds and interaction trajectories between agents are discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MacNet-M (memory design)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simple MAS memory scheme that retains final answers across rounds but discards non-artifact dialogue context (interaction trajectories); used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See above entries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, SciWorld, PDDL, HotpotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated as a baseline memory strategy across the same benchmark suite.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied action, strategic planning, QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>inside-trial artifacts (final answers only)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Stores final answers produced in previous rounds; interaction dialogues and trajectories are not preserved.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Final answer artifacts / outputs from previous tasks; no structured interaction logs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Artifact retrieval (final answers) without trajectory-level retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Mixed; sometimes small positive or negative changes versus no-memory baselines. Example aggregated results vary across tables (no single universal gain reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>See benchmark tables; no-memory baselines shown for per-task comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared against richer designs (G-Memory) which outperform MacNet-M on tasks where interaction dynamics matter; MacNet-M's omission of interaction trajectories reduces its utility.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Discarding interaction trajectories impoverishes cross-trial learning for MAS; final-answer-only memories lack the procedural grounding needed for many embodied and cooperative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Insufficient for tasks requiring replay/analysis of multi-agent dialogues; cannot support distilled insight generation tied to trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8211.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8211.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager (adapted single-agent memory baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent embodied memory design (Voyager) adapted to multi-agent settings by implementing agent-specific history retrieval based on each agent's visible dialogue context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager (adapted to MAS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Originally an open-ended embodied single-agent with long-term memory for Minecraft-like tasks; adapted for MAS experiments by applying agent-specific retrieval from each agent's dialogue-visible context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See G-Memory entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Primarily embodied tasks (ALFWorld, SciWorld) and other benchmarks in the suite</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Embodied environment tasks requiring object manipulation, multi-step procedures, and environment-aware planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied action / procedural planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic long-term memory adapted from single-agent Voyager (artifact and behavior records)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Continuous recording of artifacts and experiences in an open-ended memory; in MAS adaptation, retrieval is constrained to agent-visible dialogue context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Artifacts, actions, and historic execution traces; higher-level reflected concepts in original Voyager but here adapted to per-agent histories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Similarity-based retrieval from per-agent stored histories / visible context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Example: In GPT-4o-mini + AutoGen experiments (Table 1), Voyager improved ALFWorld success rate from 77.61% (no-memory) to 85.07% (+7.46 points) in one reported setting. Results vary across tasks and MAS frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>See no-memory baselines in tables (e.g., 77.61% in that example).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Single-agent memory designs adapted naïvely to MAS can sometimes help (as with Voyager on some embodied tasks) but often fail to provide consistent gains across MAS benchmarks; they lack role-specific and trajectory-aware abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Single-agent memory designs can transfer partially to MAS but need MAS-specific adaptations (role-specific memory, trajectory condensation, insight abstraction) to be reliably beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Originally designed for single-agent open-ended tasks; naive adaptation does not capture rich inter-agent collaboration patterns and can be suboptimal in MAS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8211.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8211.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank (adapted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory design that mimics anthropomorphic memory behaviors with selective preservation and forgetting (temporal decay), adapted as a baseline for MAS experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank (adapted to MAS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory mechanism inspired by human forgetting curves that selectively retains and decays stored memories based on importance and recency; adapted for MAS baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See G-Memory entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, SciWorld, PDDL, HotpotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated on the same suite; represents an adaptive retention memory baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied action, QA, planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic with decay (selective retention)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Records events and selectively reinforces or discards them following a temporal-decay inspired policy (Ebbinghaus-style), with retrieval biased toward reinforced memories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Past interactions/events with importance scores and decay schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Importance-weighted retrieval biased by recency and reinforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Mixed; example in one setting (AutoGen + GPT-4o-mini) MemoryBank reduced ALFWorld performance from 77.61% to 74.96% (−2.65 points) in reported Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>See no-memory baselines in tables for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared with G-Memory: MemoryBank's decay-based selective retention is insufficient for MAS where structured, role-specific, and trajectory-aware cross-trial memory is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Importance/decay-based selective retention helps structure long-term memory but by itself does not address MAS-specific needs like cross-trial trajectory condensation and role-specific insights.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designed primarily for single-agent contexts; without structured trajectories and insights it can degrade MAS performance in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8211.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8211.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents (reflective memory baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Memory baseline inspired by Generative Agents that includes raw observational memory and higher-level reflective memory capturing abstracted thoughts about experiences; adapted for MAS comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (memory design)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory design featuring both raw observations and reflective, abstracted memory (higher-level conceptualizations) intended to support future behavior; used as a competitive baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See G-Memory entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, SciWorld, PDDL, HotpotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as a baseline across the benchmark suite to evaluate the benefit of reflective memory in MAS contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied action, QA, planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic + reflective (abstracted thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Stores raw observational memory logs plus higher-level reflective memories generated by the agent to summarize/interpret experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Observational logs and abstracted reflections or 'thoughts' that summarize experience.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval of observational or reflective memory items, likely via similarity/relevance heuristics (adapted for MAS in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Often yields strong gains in some settings; example from Table 1 (GPT-4o-mini + AutoGen) shows Generative baseline achieving 86.36% vs 77.61% (no-memory) on ALFWorld (+8.75 points).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-memory baselines present for same tasks for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to G-Memory: Generative Agents' reflective memory provides improvements in some contexts but lacks MAS-specific structural graphs (insight/query/interaction) and role-specific condensation, making it less effective than G-Memory across the entire MAS benchmark suite.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reflective memory that abstracts experiences can help agent performance; however, MAS requires additional structure (role-specific cues and trajectory condensation) to fully leverage cross-trial collaborative experience.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Originally designed for single-agent social simulation; direct application to MAS needs extra mechanisms to handle long multi-agent trajectories and role differentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Voyager: An Open-Ended Embodied Agent with Large Language Models <em>(Rating: 2)</em></li>
                <li>MemoryBank: Enhancing Large Language Models with Long-Term Memory <em>(Rating: 2)</em></li>
                <li>Generative Agents: Interactive Simulacra of Human Behavior <em>(Rating: 2)</em></li>
                <li>MetaGPT: Meta programming for multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>ChatDev <em>(Rating: 1)</em></li>
                <li>MacNet <em>(Rating: 2)</em></li>
                <li>A-Mem: Agentic memory for LLM agents <em>(Rating: 1)</em></li>
                <li>Mem0: Building production-ready AI agents with scalable long-term memory <em>(Rating: 1)</em></li>
                <li>MemInsight: Autonomous memory augmentation for LLM agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8211",
    "paper_id": "paper-279250852",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "G-Memory",
            "name_full": "G-Memory: Graph-based Agentic Memory Mechanism for LLM-based Multi-Agent Systems",
            "brief_description": "A hierarchical, agentic memory architecture for multi-agent systems organizing experience into three graph tiers (insight, query, interaction) with bi-directional traversal (query→insight and query→interaction), role-specific filtering, and continual updates to enable MAS self-evolution and concise retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "G-Memory (plug-in for MAS)",
            "agent_description": "A memory module invoked by an MAS upon a new query; performs coarse retrieval on a query graph, 1-hop expansion, upward traversal to an insight graph (Π Q→I), downward traversal to extract sparsified interaction subgraphs (S_LLM), then role-specific memory initialization (Φ) for each agent, and updates all graphs after task execution.",
            "model_name": "Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini (used as LLM backbones in experiments)",
            "model_description": "Three different LLM backbones used to instantiate MAS agents in experiments: two open-source Qwen variants (2.5-7b, 2.5-14b) and a proprietary gpt-4o-mini.",
            "task_name": "ALFWorld, ScienceWorld (SciWorld), PDDL, HotpotQA, FEVER",
            "task_description": "Benchmarks spanning embodied action (ALFWorld, SciWorld), strategic/game planning (PDDL), and knowledge QA / fact verification (HotpotQA, FEVER); tasks require multi-step coordination across agents and/or evidence-grounded reasoning.",
            "task_type": "embodied action, game/strategic planning, question answering / fact verification",
            "memory_used": true,
            "memory_type": "hierarchical agentic memory (episodic + distilled insight + structured query metadata)",
            "memory_mechanism": "Coarse similarity-based retrieval over a query graph (embedding similarity), 1-hop graph expansion, upward projector to insight graph (Π Q→I) to retrieve distilled insights, downward LLM-based graph sparsifier (S_LLM) to extract core interaction subgraphs of past trajectories, LLM-based relevance scorer (R_LLM) for ranking, and role-specific filtering/adaptation (Φ) to initialize each agent's Mem_i. After execution, new interaction graphs, query node, and distilled insight nodes are created/updated.",
            "memory_representation": "Three-tier: (1) Interaction graphs: atomic utterance nodes (agent, text) and temporal edges representing dialogue/trajectory; (2) Query graph: nodes containing original query, status, and pointer to interaction graph; (3) Insight graph: distilled, generalizable insights (text) linked to supporting queries (Ω_k) and hyperedges indicating contextual relations between insights via queries.",
            "memory_retrieval_method": "Embedding-based semantic search over query graph for top-k, 1-hop neighbor expansion, LLM relevance scoring (R_LLM) for ranking historical queries, upward lookup via query→insight projector (set-intersection of supporting queries), and LLM-facilitated sparsification to extract core interaction subgraphs.",
            "performance_with_memory": "Varies by benchmark and MAS: example — MacNet + ALFWorld with Qwen-2.5-14b: 79.10% success rate with G-Memory (reported). Overall reported maximum gains: up to +20.89 percentage points on embodied action (ALFWorld) and up to +10.12 percentage points on knowledge QA tasks.",
            "performance_without_memory": "Example baseline (no-memory) for the same setting: MacNet + ALFWorld with Qwen-2.5-14b: 58.21% success rate (no-memory). More generally, comparisons provided across all five benchmarks in Tables 1–3.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations: removing either the high-level insight module (I_S) or fine-grained interactions ({Ĝ^Qi_inter}) reduces performance. Only fine-grained interactions enabled -&gt; average drop: 4.47% (AutoGen) and 3.82% (DyLAN). Only insights enabled -&gt; smaller drops: 3.95% (AutoGen) and 3.39% (DyLAN). Sensitivity: 1-hop expansion and k∈{1,2} (top-k queries) are optimal; more hops or larger k degrade performance. Token-cost comparison: on PDDL+AutoGen, G-Memory gave +10.32% improvement over no-memory with ~1.4×10^6 tokens, whereas MetaGPT-M used ~2.2×10^6 tokens for only +4.07% gain.",
            "key_findings": "Hierarchical, role-specific memory that retrieves both distilled insights and condensed interaction trajectories substantially improves MAS performance and sample efficiency across domains; both insight-level abstraction and fine-grained trajectory condensation are necessary (complementary), with interactions contributing slightly more to performance; small graph-hop and top-k retrieval windows work best to avoid irrelevant noise; G-Memory is token-efficient compared to baseline MAS memory designs.",
            "limitations_or_challenges": "Additional validation needed on more diverse tasks (e.g., medical QA) as acknowledged; relies on LLM-based scorers/sparsifiers (possible fragility to LLM errors); potential risk of amplifying incorrect reasoning if underlying LLM is compromised; complexity of maintaining and updating hierarchical graphs in large-scale deployments.",
            "uuid": "e8211.0",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "MetaGPT-M",
            "name_full": "MetaGPT-Memory (adapted baseline)",
            "brief_description": "Memory design originating from MetaGPT adapted as a baseline in which memory focuses on inside-trial artifacts and does not comprehensively capture cross-trial collaborative trajectories.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "MetaGPT-M (memory design)",
            "agent_description": "An MAS memory scheme that primarily stores inside-trial artifacts (information created/used during single task resolution) and limited cross-trial artifacts (often final solutions); used as a baseline in experiments.",
            "model_name": "Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini (used as LLM backbones in comparative experiments)",
            "model_description": "See G-Memory entry for the LLM backbones used in experiments.",
            "task_name": "ALFWorld, SciWorld, PDDL, HotpotQA, FEVER (same benchmark suite)",
            "task_description": "Same multi-domain benchmark suite; MetaGPT-M is evaluated as a competing memory approach when integrated into MAS frameworks.",
            "task_type": "embodied action, game/strategic planning, question answering",
            "memory_used": true,
            "memory_type": "inside-trial focused (limited cross-trial artifacts)",
            "memory_mechanism": "Stores inside-trial artifacts created during a single task resolution; cross-trial memory if present is typically compressed final outcomes/solutions rather than rich interaction trajectories.",
            "memory_representation": "Final solutions / execution results and limited inside-trial context snippets; lacks structured interaction graphs and distilled insight nodes.",
            "memory_retrieval_method": "Simple retrieval of stored artifacts (no hierarchical graph-driven retrieval reported); retrieval tends to be coarse and artifact-focused.",
            "performance_with_memory": "Mixed — sometimes yields small gains but can be token-inefficient: paper reports MetaGPT-M consumed extra ~2.2×10^6 tokens for only +4.07% gain in one reported comparison (PDDL+AutoGen example).",
            "performance_without_memory": "No-memory baselines reported in tables; improvements relative to no-memory vary across tasks (sometimes small positive, sometimes negative).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared against G-Memory: MetaGPT-M delivered smaller performance gains and higher token cost in some settings; MetaGPT-M lacks trajectory condensation and role-specific memory, limiting MAS utility.",
            "key_findings": "Inside-trial/artifact-centric memory (as in MetaGPT-M) is insufficient for MAS tasks that require role-specific, trajectory-aware, cross-trial learning; such designs can be token-inefficient relative to performance gains.",
            "limitations_or_challenges": "Overly simplistic cross-trial memory (final artifacts only) fails to capture collaborative dynamics; token cost can be high relative to benefit.",
            "uuid": "e8211.1",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ChatDev-M",
            "name_full": "ChatDev-Memory (adapted baseline)",
            "brief_description": "A baseline memory design adapted from ChatDev that stores inside-trial memory and a rudimentary form of cross-trial memory consisting mostly of past solutions, with a narrow memory scope.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ChatDev-M (memory design)",
            "agent_description": "Memory approach that passes inside-trial memory from a central/initiating agent across rounds and records past solutions as cross-trial memory; used as a comparative baseline.",
            "model_name": "Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini",
            "model_description": "See G-Memory entry.",
            "task_name": "ALFWorld, SciWorld, PDDL, HotpotQA, FEVER",
            "task_description": "Same multi-domain tasks; ChatDev-M is evaluated as a baseline memory mechanism when integrated into MAS frameworks.",
            "task_type": "embodied action, question answering, strategic planning",
            "memory_used": true,
            "memory_type": "inside-trial + simple cross-trial (final solutions/artifacts)",
            "memory_mechanism": "Inside-trial memory passed from central agent; cross-trial memory stores past final solutions for retrieval.",
            "memory_representation": "Past final solutions and inside-trial state passed by a central agent; does not store detailed inter-agent dialogues or distilled insights.",
            "memory_retrieval_method": "Retrieval of stored past solutions (artifact lookup) and passing previous inside-trial state at round start.",
            "performance_with_memory": "In several experiments, ChatDev-M either provided limited benefit or degraded performance; example: applying ChatDev-M to MacNet+SciWorld resulted in a 2.32% drop in performance (reported in paper).",
            "performance_without_memory": "No-memory baselines exist in tables; ChatDev-M often underperformed relative to richer memory designs like G-Memory.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared unfavorably to G-Memory on embodied tasks due to narrow scope and lack of trajectory insights; sometimes decreased MAS performance.",
            "key_findings": "Storing only final past solutions (narrow cross-trial memory) is insufficient for embodied action tasks where interaction patterns and fine-grained trajectories matter.",
            "limitations_or_challenges": "Limited utility in environments requiring procedural or collaborative nuance; inadequate for role-specific guidance.",
            "uuid": "e8211.2",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "MacNet-M",
            "name_full": "MacNet-Memory (adopted baseline)",
            "brief_description": "Memory design adopted from MacNet where inside-trial memory is essentially the final answers from previous rounds and interaction trajectories between agents are discarded.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "MacNet-M (memory design)",
            "agent_description": "A simple MAS memory scheme that retains final answers across rounds but discards non-artifact dialogue context (interaction trajectories); used as baseline.",
            "model_name": "Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini",
            "model_description": "See above entries.",
            "task_name": "ALFWorld, SciWorld, PDDL, HotpotQA, FEVER",
            "task_description": "Evaluated as a baseline memory strategy across the same benchmark suite.",
            "task_type": "embodied action, strategic planning, QA",
            "memory_used": true,
            "memory_type": "inside-trial artifacts (final answers only)",
            "memory_mechanism": "Stores final answers produced in previous rounds; interaction dialogues and trajectories are not preserved.",
            "memory_representation": "Final answer artifacts / outputs from previous tasks; no structured interaction logs.",
            "memory_retrieval_method": "Artifact retrieval (final answers) without trajectory-level retrieval.",
            "performance_with_memory": "Mixed; sometimes small positive or negative changes versus no-memory baselines. Example aggregated results vary across tables (no single universal gain reported).",
            "performance_without_memory": "See benchmark tables; no-memory baselines shown for per-task comparison.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared against richer designs (G-Memory) which outperform MacNet-M on tasks where interaction dynamics matter; MacNet-M's omission of interaction trajectories reduces its utility.",
            "key_findings": "Discarding interaction trajectories impoverishes cross-trial learning for MAS; final-answer-only memories lack the procedural grounding needed for many embodied and cooperative tasks.",
            "limitations_or_challenges": "Insufficient for tasks requiring replay/analysis of multi-agent dialogues; cannot support distilled insight generation tied to trajectories.",
            "uuid": "e8211.3",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Voyager (adapted)",
            "name_full": "Voyager (adapted single-agent memory baseline)",
            "brief_description": "A single-agent embodied memory design (Voyager) adapted to multi-agent settings by implementing agent-specific history retrieval based on each agent's visible dialogue context.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Voyager (adapted to MAS)",
            "agent_description": "Originally an open-ended embodied single-agent with long-term memory for Minecraft-like tasks; adapted for MAS experiments by applying agent-specific retrieval from each agent's dialogue-visible context.",
            "model_name": "Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini",
            "model_description": "See G-Memory entry.",
            "task_name": "Primarily embodied tasks (ALFWorld, SciWorld) and other benchmarks in the suite",
            "task_description": "Embodied environment tasks requiring object manipulation, multi-step procedures, and environment-aware planning.",
            "task_type": "embodied action / procedural planning",
            "memory_used": true,
            "memory_type": "episodic long-term memory adapted from single-agent Voyager (artifact and behavior records)",
            "memory_mechanism": "Continuous recording of artifacts and experiences in an open-ended memory; in MAS adaptation, retrieval is constrained to agent-visible dialogue context.",
            "memory_representation": "Artifacts, actions, and historic execution traces; higher-level reflected concepts in original Voyager but here adapted to per-agent histories.",
            "memory_retrieval_method": "Similarity-based retrieval from per-agent stored histories / visible context.",
            "performance_with_memory": "Example: In GPT-4o-mini + AutoGen experiments (Table 1), Voyager improved ALFWorld success rate from 77.61% (no-memory) to 85.07% (+7.46 points) in one reported setting. Results vary across tasks and MAS frameworks.",
            "performance_without_memory": "See no-memory baselines in tables (e.g., 77.61% in that example).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Single-agent memory designs adapted naïvely to MAS can sometimes help (as with Voyager on some embodied tasks) but often fail to provide consistent gains across MAS benchmarks; they lack role-specific and trajectory-aware abstractions.",
            "key_findings": "Single-agent memory designs can transfer partially to MAS but need MAS-specific adaptations (role-specific memory, trajectory condensation, insight abstraction) to be reliably beneficial.",
            "limitations_or_challenges": "Originally designed for single-agent open-ended tasks; naive adaptation does not capture rich inter-agent collaboration patterns and can be suboptimal in MAS.",
            "uuid": "e8211.4",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "MemoryBank (adapted)",
            "name_full": "MemoryBank (adapted baseline)",
            "brief_description": "A memory design that mimics anthropomorphic memory behaviors with selective preservation and forgetting (temporal decay), adapted as a baseline for MAS experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "MemoryBank (adapted to MAS)",
            "agent_description": "Memory mechanism inspired by human forgetting curves that selectively retains and decays stored memories based on importance and recency; adapted for MAS baseline comparisons.",
            "model_name": "Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini",
            "model_description": "See G-Memory entry.",
            "task_name": "ALFWorld, SciWorld, PDDL, HotpotQA, FEVER",
            "task_description": "Evaluated on the same suite; represents an adaptive retention memory baseline.",
            "task_type": "embodied action, QA, planning",
            "memory_used": true,
            "memory_type": "episodic with decay (selective retention)",
            "memory_mechanism": "Records events and selectively reinforces or discards them following a temporal-decay inspired policy (Ebbinghaus-style), with retrieval biased toward reinforced memories.",
            "memory_representation": "Past interactions/events with importance scores and decay schedules.",
            "memory_retrieval_method": "Importance-weighted retrieval biased by recency and reinforcement.",
            "performance_with_memory": "Mixed; example in one setting (AutoGen + GPT-4o-mini) MemoryBank reduced ALFWorld performance from 77.61% to 74.96% (−2.65 points) in reported Table 1.",
            "performance_without_memory": "See no-memory baselines in tables for comparison.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared with G-Memory: MemoryBank's decay-based selective retention is insufficient for MAS where structured, role-specific, and trajectory-aware cross-trial memory is needed.",
            "key_findings": "Importance/decay-based selective retention helps structure long-term memory but by itself does not address MAS-specific needs like cross-trial trajectory condensation and role-specific insights.",
            "limitations_or_challenges": "Designed primarily for single-agent contexts; without structured trajectories and insights it can degrade MAS performance in some tasks.",
            "uuid": "e8211.5",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Generative Agents (baseline)",
            "name_full": "Generative Agents (reflective memory baseline)",
            "brief_description": "Memory baseline inspired by Generative Agents that includes raw observational memory and higher-level reflective memory capturing abstracted thoughts about experiences; adapted for MAS comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Generative Agents (memory design)",
            "agent_description": "Memory design featuring both raw observations and reflective, abstracted memory (higher-level conceptualizations) intended to support future behavior; used as a competitive baseline.",
            "model_name": "Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini",
            "model_description": "See G-Memory entry.",
            "task_name": "ALFWorld, SciWorld, PDDL, HotpotQA, FEVER",
            "task_description": "Used as a baseline across the benchmark suite to evaluate the benefit of reflective memory in MAS contexts.",
            "task_type": "embodied action, QA, planning",
            "memory_used": true,
            "memory_type": "episodic + reflective (abstracted thoughts)",
            "memory_mechanism": "Stores raw observational memory logs plus higher-level reflective memories generated by the agent to summarize/interpret experiences.",
            "memory_representation": "Observational logs and abstracted reflections or 'thoughts' that summarize experience.",
            "memory_retrieval_method": "Retrieval of observational or reflective memory items, likely via similarity/relevance heuristics (adapted for MAS in experiments).",
            "performance_with_memory": "Often yields strong gains in some settings; example from Table 1 (GPT-4o-mini + AutoGen) shows Generative baseline achieving 86.36% vs 77.61% (no-memory) on ALFWorld (+8.75 points).",
            "performance_without_memory": "No-memory baselines present for same tasks for comparison.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared to G-Memory: Generative Agents' reflective memory provides improvements in some contexts but lacks MAS-specific structural graphs (insight/query/interaction) and role-specific condensation, making it less effective than G-Memory across the entire MAS benchmark suite.",
            "key_findings": "Reflective memory that abstracts experiences can help agent performance; however, MAS requires additional structure (role-specific cues and trajectory condensation) to fully leverage cross-trial collaborative experience.",
            "limitations_or_challenges": "Originally designed for single-agent social simulation; direct application to MAS needs extra mechanisms to handle long multi-agent trajectories and role differentiation.",
            "uuid": "e8211.6",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Generative Agents: Interactive Simulacra of Human Behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "MetaGPT: Meta programming for multi-agent collaborative framework",
            "rating": 2,
            "sanitized_title": "metagpt_meta_programming_for_multiagent_collaborative_framework"
        },
        {
            "paper_title": "ChatDev",
            "rating": 1
        },
        {
            "paper_title": "MacNet",
            "rating": 2
        },
        {
            "paper_title": "A-Mem: Agentic memory for LLM agents",
            "rating": 1,
            "sanitized_title": "amem_agentic_memory_for_llm_agents"
        },
        {
            "paper_title": "Mem0: Building production-ready AI agents with scalable long-term memory",
            "rating": 1,
            "sanitized_title": "mem0_building_productionready_ai_agents_with_scalable_longterm_memory"
        },
        {
            "paper_title": "MemInsight: Autonomous memory augmentation for LLM agents",
            "rating": 1,
            "sanitized_title": "meminsight_autonomous_memory_augmentation_for_llm_agents"
        }
    ],
    "cost": 0.019032999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems
16 Jun 2025</p>
<p>Guibin Zhang 
Muxin Fu 
Tongji University</p>
<p>Guancheng Wan 
UCLA
4 A*STAR, 5 NTU</p>
<p>Miao Yu 
Kun Wang wang.kun@ntu.edu.sg 
Shuicheng Yan 
Nus 
G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems
16 Jun 20255BB9D4C02945ABC5F6AF24A99EF5C7D5arXiv:2506.07398v2[cs.MA]
Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures.Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack crosstrial and agent-specific customization, in stark contrast to the expressive memory developed for single agents.To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory[1], which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs.Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both high-level, generalizable insights that enable the system to leverage cross-trial knowledge, and fine-grained, condensed interaction trajectories that compactly encode prior collaboration experiences.Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams.Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to 20.89% and 10.12%, respectively, without any modifications to the original frameworks.Our codes are available at https://github.com/bingreeky/GMemory.</p>
<p>Introduction</p>
<p>As Large Language Models (LLMs) continue to redefine the frontier of artificial intelligence, LLMdriven agents have exhibited unprecedented prowess in perception [2,3,4,5], planning [6,7,8], reasoning [9,10], and action [11,12], which have catalyzed remarkable progress across diverse downstream domains, including code generation [13,14], data analysis [15], embodied tasks [16] and autonomous driving [3,17,18].Building upon the impressive competencies of single agents, LLMbased Multi-Agent Systems (MAS) have been demonstrated to push the boundaries of single model capacity [19,20,21].Similar to collective intelligence arising from human social collaboration [22,23,24], MAS orchestrates multiple agents [25,26,27], whether through cooperation [28,29,30,31] or competition [32,33,34], to transcend the cognitive and specialized limitations of solitary agents.</p>
<p>Self-Evolving Agents.What especially characterizes LLM agents is their self-evolving capacity, i.e., the ability to continuously adapt and improve through interactions with the environment, as seen in prior works where such adaptability has led to two-to three-fold quantitative improvements [35].The central driving force behind such self-evolving nature is memory mechanism of agents [36,37,38], which parallels human abilities to accumulate knowledge, process past experiences, and</p>
<p>Insight Graph</p>
<p>Query Graph Interaction Graph</p>
<p>Verify the state of objects before and after action...</p>
<p>Insight</p>
<p>Query Check whether there is an apple in the garden retrieve relevant information.Previous successful memory mechanism designs, including both inside-trial memory (i.e., context retained within solving one single query) and cross-trial memory (i.e., experience accumulated across multiple tasks) [39], have empowered agents to excel in diverse applications such as personalized chat [36,40,41], recommendation [42], embodied action [43,16], and social simulation [19,44,45], enabling them to evolve into experiential learners that effectively leverage past experiences and world knowledge.</p>
<p>Status</p>
<p>Self-Evolving MAS.However, such self-evolving capacity remains largely absent in multi-agent systems.Most existing MAS are still constrained by manually defined workflows, such as the Standard Operating Procedures (SOP) in MetaGPT [21] and ChatDev [46], or rely on pre-defined communication topologies in MacNet [47] and AgentPrune [30].More recent automated MASs, such as GPTSwarm [48], ADAS [49], AFlow [50], and MaAS [51] have made it to automatically optimize inter-agent topologies or prompts, which, nevertheless, ultimately yield giant and cumbersome MAS architectures, lacking the agility to self-adjust with accumulated collaboration experience.</p>
<p>Memory for MAS.The absence of the aforementioned self-evolving capacity is, in fact, rooted in the lack of memory mechanisms specifically tailored for MAS.One may challenge this claim from two perspectives: ❶ Do existing MASs lack memory mechanisms altogether?Not entirely.Classical MAS frameworks such as MetaGPT, ChatDev, and Exchange-of-Thought [52] incorporate memory-related designs.However, these are often limited to inside-trial memory [52], while cross-trial memory, if present, remains rudimentary-typically involving the transmission of overly condensed artifacts (e.g., final solutions or execution results) [21,46,47], and failing to enable meaningful learning from collaborative experience.❷ Why not directly transfer existing single-agent memory mechanisms to MAS? Unfortunately, such a transfer is far from straightforward.The inherent nature of MAS, i.e., multi-turn orchestration across multiple agents [26,27], leads to substantially longer task-solving trajectories compared to single-agent settings (up to 10× more tokens, as demonstrated by Figure 1 (Left)).This poses a significant challenge to traditional retrieval-based memory designs [36,37,16], as naive feeding of the entire long-context trajectory without proper abstraction from a collaborative perspective offers little benefit.Given the aforementioned challenges, a natural question arises:</p>
<p>How can we design a memory mechanism capable of storing, retrieving, and managing the lengthy interaction history of multi-agent systems, such that agent teams can benefit from concise and instructive experience and insights?</p>
<p>The Present Work: G-Memory.In response to the above question, we introduce a :</p>
<p>Graph-based Agentic ::::::: Memory Mechanism for LLM-based Multi-Agent Systems, dubbed G-Memory, which manages the complex and lengthy interaction history of MAS through a three-tier hierarchical graph structure:</p>
<p>✱ Insight Graph, which abstracts generalizable insights from historical experience; ✱ Query Graph, which encodes meta-information of task queries and their connectivity; ✱ Interaction Graph, which stores fine-grained textual communication logs among agents.</p>
<p>Figure 1 (Right) visualizes these structures, and their formal definitions are placed in Section 3. When a new query arrives, G-Memory efficiently retrieves relevant query records by leveraging the topology of the query graph, and then traverses upward (i.e., query→insight graph) to extract associated highlevel insights and downward (i.e., query→interaction graph) to identify core interaction subgraphs that are most pertinent to the task at hand, thereby mitigating information overload.Based on the retrieved memory, G-Memory offers actionable guidance to the MAS, e.g., division of labor, task decomposition, and lessons from past failures.Upon the completion of a task, all three levels of the memory hierarchy are updated in an agentic manner, with newly distilled insights, enriched query records, detailed MAS trajectories, and their level of detailed associations.Through this refinement, G-Memory functions as a plug-and-play module that can be seamlessly embedded into mainstream MAS frameworks, empowering evolving inter-agent collaboration and collective intelligence.</p>
<p>Our contributions are summarized as follows:</p>
<p>❶ Bottleneck Identification.We conduct a thorough review of existing multi-agent systems and identify a fundamental bottleneck in their self-evolving capabilities, which is largely attributed to the oversimplified memory architectures.❷ Practical Solution.We propose G-Memory, a hierarchical agentic memory architecture for MAS, which models complex and prolonged inter-agent collaboration through a three-tier structure comprising insight, query, and interaction graphs.❸ Experimental Evaluation.Extensive experiments across five benchmarks show that G-Memory is (I) high-performing, improving state-of-the-art MAS by up to 20.89% and 10.12% on embodied action and knowledge QA tasks, respectively; and (II) resource-friendly, maintaining comparable or even lower token usage than mainstream memory designs.</p>
<p>Related Works</p>
<p>Single-Agent Memory.Memory serves as a primary driving force for agents to accumulate experiences and explore the world through interactions with the environment [53,54,55,56].It plays a critical role in both task-solving and social simulation LLM agents, and this work primarily focuses on the former.Early research on agent memory was confined to simple inside-trial memory, mainly addressing limitations posed by the LLM context window in chatbot applications, including MemoryBank [36], ChatDB [40], MemoChat [41], and MemGPT [37], which typically adopt retrievalaugmented generation (RAG)-style, similarity-based chunk retrieval.Subsequent developments have progressed toward more cognitively inspired memory architectures, including (1) memory scope extended to cross-trial memory like ExpeL [43] and Synapse [57]; (2) application domains broadened to include computer control [57], embodied action [58], scientific discovery [59], coding and reasoning [60]; and (3) management techniques evolved from coarse-grained textual similarity toward more sophisticated abstraction and summarization of acquired knowledge and experiences [19], as seen in A-Mem [61], Mem0 [62] and MemInsight [63].More discussions are in Appendix D.</p>
<p>Memory in Multi-agent System.However, the memory mechanisms tailored for MAS remain markedly underexplored.Some representative frameworks, such as LLM-Debate [20,33] and Mixture-of-Agent [64], omit memory components altogether.Others merely adopt simplistic insidetrial memory schemes [47,52].Even in frameworks that attempt cross-trial memory [46], the memory is merely compressed as the final outcome artifacts, overlooking the nuanced agent interactions.</p>
<p>Collectively, there is a pressing need for a principled memory architecture that can capture, organize, and retrieve the inherently intricate task-solving processes unique to MAS [39].</p>
<p>LLM-based Multi-Agent Systems.Our work focuses on task-solving MAS, which, unlike their single-agent counterparts, often lack the capacity for continual evolution through interaction with the environment [65,66].Early frameworks such as AutoGen [13], CAMEL [24], and AgentVerse [67] rely entirely on pre-defined workflows.More recent efforts [68,69,50,49,70,31] introduce a degree of adaptivity by generating dynamic MAS in response to environmental feedback.However, such evolution is often one-shot: for example, AFlow [50] employs Monte Carlo Tree Search to construct a complex MAS tailored to a specific task domain, which yet lacks the capacity to evolve with increasing task exposure or transfer across domains [51,71].From this perspective, constructing MAS with genuine self-evolving capabilities remains an open and challenging research frontier.</p>
<p>Preliminary</p>
<p>In this section, we establish the notation and formalize key concepts of multi-agent systems and G-Memory's hierarchical memory architecture.</p>
<p>Multi-agent System Formalization.Consider a multi-agent framework represented by a directed graph G = (V, E), where |V| = N is the number of agents and E ⊆ V×V defines their communication channels.Each node C i ∈ V corresponds to an individual agent described by the quadruple:
C i = (Base i , Role i , Mem i , Plugin i ),(1)
where Base i denotes the underlying large language model instance, Role i specifies the agent's designated role or persona, Mem i encapsulates its memory state, including past interactions or external knowledge stores, and Plugin i is the set of auxiliary tools (e.g., web-search engine).</p>
<p>Upon receiving a user query Q, the system evolves through T synchronous communication epochs.</p>
<p>At each epoch t, we derive a topological ordering π = [π 1 , . . ., π N ] of the nodes such that if there is an edge from π j to π k , then j &lt; k, which guarantees that every agent processes its inputs only after all its predecessors have acted.For each agent C i in π, its output at iteration t is computed as:
r (t) i = C i P (t) sys , Q, {r(t)j : C j ∈ N − (C i )} ,
where: r
(t)
i denotes the response generated by C i (which may include reasoning steps, intermediate analyses, or final proposals),
P (t) sys comprises global instructions (including each agent's R i ), N − (C i )
is the set of in-neighbors of C i , whose outputs serve as contextual inputs.After all agents have acted, a global aggregation operator A fuses the collection of responses into an interim solution a (t) :
a (t) = A(r (t) 1 , . . . , r (t) N ).
Common implementations for A include majority voting schemes [48], hierarchical summarization via dedicated aggregator agents [13,30], or simply adopting the final agent's output as the answer [47].These epochs iterate for t = {1, . . ., T } until either a preset limit is reached or an early-stopping criterion is met [72], producing the final response a (T ) to the query Q.</p>
<p>Memory Architecture.Our proposed G-Memory orchestrates and manages the memory of multiagent systems via the following three hierarchical graph structures:
[✱] Interaction Graph (Utterance Graph). For query Q, let G (Q) inter = (U (Q) , E (Q)
u ) denote its interaction trajectory, where (i) nodes U (Q) = {u i } represent atomic utterances, with each u i ≜ (A i , m i ) containing A i ∈ V (speaking agent), and m i (textual content), (ii) Edges
E (Q) u ⊆ U (Q) × U (Q) follow temporal relationships: (u j , u k ) ∈ E (Q) u
⇐⇒ u j is transmitted to and inspires u k .</p>
<p>[✱] Query Graph.The query graph, storing previously tackled queries and metadata, is as follows:
G query = (Q, E q ) = Q i , Ψ i , G (Qi) inter |Q| i=1 , E q ,(2)
where Q = {q i } is the node set, node
q i ≜ (Q i , Ψ i , G(Qi)
inter ) is composed of the original query Q i , task status Ψ i ∈ {Failed, Resolved}, and its associated interaction graph G (Qi) inter .The edges E q ⊆ Q × Q encode semantic relationships between queries.The query graph enables retrieval beyond coarse metrics such as embedding similarity, with its meticulous topology.</p>
<p>[✱] Insight Graph.The highest-level insight graph is featured as follows:
G insight = (I, E i ) = ⟨κ k , Ω k ι k ⟩ |I| k=1 , E i ,(3)
where the node set I = {ι k } represents distilled insights, each node ι k is composed of the insight content κ k and the set of supporting queries Ω k ⊆ Q.The edges E i ⊆ I × I × Q forming hyper-connections where (ι m , ι n , q j ) indicates insight ι m contextualizes ι n through query q j .</p>
<p>G-Memory</p>
<p>This section outlines the management workflow of G-Memory, as illustrated in Figure 2. Specifically, upon the arrival of a new query Q, G-Memory first conducts coarse-grained retrieval to identify pertinent trajectory records (▷ Section 4.1).It then performs bi-directional hierarchical memory traversal: upward to retrieve collective cognitive insights, and downward to distill concrete procedural trajectories (▷ Section 4.2).After the memory-augmented MAS completes the query execution, the hierarchical memory architecture is jointly updated based on environmental feedback, thereby achieving the institutionalization of group knowledge (▷ Section 4.3).</p>
<p>Query</p>
<p>Similaritybased Retrival</p>
<p>1.Your task is to find a butterfly egg in the outside.Move it to the green box in the bathroom.</p>
<p>Core Path Extraction</p>
<p>Are both Lygodium or Maxillaria a genus of orchids?</p>
<p>Topic: Diffculty:</p>
<p>Web search</p>
<p>Trajectory Condensation</p>
<p>Collab.Experience</p>
<p>This history follows a chainstyle, collaboration strategy...</p>
<p>Failure Lessons</p>
<p>Take care when summarizing the final result, DO NOT ...</p>
<p>Distilled Insights</p>
<p>Insight 1: Clearly identify key entities and their roles, use specific names and titles Insight 2: Doublecheck the relevance of the search results</p>
<p>from from and</p>
<p>Ensure the search terms are precise and directly related to the specific entity or institution ...</p>
<p>Memory Augmentation Output Solution Environment Feedback</p>
<p>Execution: Success Token cost: 3,345 Tool calls: 3 ...</p>
<p>Update Interaction Update Insights</p>
<p>Since both are confirmed as film directors from their respective countries, the answer is Yes.</p>
<p>Interaction Graphs</p>
<p>Query Graph</p>
<p>Interactions CEO agent: assigning tasks... Thinker agent: OK, I will... Executor agent: ...</p>
<p>Insight Graph</p>
<p>Memory Augmentation</p>
<p>Black font: Notations</p>
<p>Symbols</p>
<p>Coarse-grained Memory Retrieval</p>
<p>As a plug-in designed for seamless integration into mainstream MAS, G-Memory is triggered when the MAS G encounters a new user query Q.As emphasized in organizational memory theory [1], efficient knowledge retrieval typically begins with broadly relevant schemas prior to more fine-grained access.Following this principle, G-Memory first performs a coarse-grained similarity-based retrieval over the query graph G query to efficiently obtain a sketched set of queries Q S :
Q S = arg top-k qi∈Q s.t. |Q S |=k v(Q) • v(q i ) |v(Q)| |v(q i )| ,(4)
where v(•) maps queries into fixed-length embeddings using models such as MiniLM [73].While Equation (4) retrieves semantically similar historical queries, the similarity may be only superficial or noisy.Therefore, G-Memory further enlarges the relevant set via hop expansion on the query graph:
QS = Q S ∪ Q k ∈ Q | ∃Q j ∈ Q S , Q k ∈ N + (Q j ) ∪ N − (Q j ) ,(5)
where QS is augmented with the 1-hop neighbors of Q S on the query graph G query .However, it is suboptimal to directly feed these relevant records as input akin to certain single-agent memory systems [41,37].On one hand, the excessive context length may overwhelm the LLM; on the other hand, agents in MAS play distinct roles and should be assigned specialized memory tailored to their functions.To address this, the next section introduces a bi-directional processing scheme in G-Memory that operates over both abstract and fine-grained memory levels.</p>
<p>Bi-directional Memory Traversal</p>
<p>Subsequent to identifying the expanded set of relevant query nodes QS within G query , G-Memory executes a bi-directional memory traversal to furnish multi-granularity memory support.Specifically, G-Memory first performs an upward traversal (G query → G insight ), retrieving insight nodes that may provide high-level guidance for the current task:
I S = Π Q→I ( QS ), Π Q→I (S q ) ≜ {ι k ∈ I | Ω k ∩ S q ̸ = ∅} ,(6)
where Π Q→I is a query-to-insight projector that identifies all the insight nodes whose supporting query sets intersect with the input query set, and the retrieved insights I S encapsulate distilled, generalized knowledge potentially relevant for orienting the MAS G's strategic approach to Q.</p>
<p>Beyond generalized insights, the fine-grained textual interaction history of the MAS is equally valuable, as it reveals the underlying reasoning patterns that led to successful or failed collaborations [68,74,75].To utilize these concisely, in the downward traversal (G query → G interaction ), G-Memory employs an LLM-facilitated graph sparsifier S LLM (•, •) to extract the core subgraph that encapsulates essential inter-agent collaboration:
{ ĜQi inter } |M | i=1 = S LLM (G (Qj ) inter , Q) | q j ∈ argtop-M {q ′ k ∈ QS } s.t. |•|=M R LLM (Q, q ′ k ) ,(7)
where R LLM (Q, q j ) rates the relevancy of historical queries w.r.t.Q, and the sparsifier S LLM (G
(Qj ) inter , Q) constructs a sparsified graph Ĝ(Qj) inter = ( Û(Qj) , Ê(Qj) u ) from the original G (Qj )
inter by identifying and retaining dialogue elements.Please refer to Appendix C for their implementations.</p>
<p>Upon completing the bi-directional traversal, we obtain both generalizable insights (I S ) and detailed collaborative trajectories
({ ĜQi inter } |M | i=1
). G-Memory then proceeds to provide specialized memory support for each agent C ∈ V within the MAS G.
Mem i ← Φ I S , { ĜQi inter } |M | i=1 ; Role i , Q , ∀C i = (Base i , Role i , Mem i , Plugin i ) ∈ V,(8)
where the operator Φ(•; •) evaluates the utility and relevance of each insight ι k ∈ I S and sparsified interaction graph Ĝ(Qj) inter concerning the agent's specific role Role i and the task Q (see Appendix C).Based on this evaluation, Φ intializes each agent's internal memory state Mem i with filtered insights, interaction snippets, summaries thereof, equipping it with pertinent historical context before it participates in the subsequent reasoning epochs of the MAS.It is worth noting that G-Memory is invoked at the onset of solving query Q in our implementation.However, practitioners may flexibly configure more fine-grained invocation strategies, such as at the beginning of each MAS dialogue round or selectively for specific agents, based on their needs.</p>
<p>Hierarchy Memory Update</p>
<p>After completing memory augmentation for each agent, the system G is executed as outlined in Section 3, yielding a final solution a (T ) and receiving environmental feedback, including execution status Ψ i ∈ {Failed, Resolved}, token usage, and other performance metrics.Subsequently, G-Memory updates its hierarchical memory architecture to incorporate this new query.At the interaction level, G-Memory traces each agent's utterances to construct the interaction graph G (Q) inter , which is then stored.At the query level, a new query node is instantiated and added to the query graph Q query :
q new ← (Q, Ψ, G (Q) inter ), N conn ← Q R ∪ ι k ∈I S Ω k , E new ← {(q n , q new ) | q n ∈ N conn }, G next query ← (Q ∪ {q new }, E q ∪ E new ),(9)
where edges are established between q new and (ii) the set Q R containing the top-M relevant historical queries identified in Equation (7), and (ii) the set of queries ι k ∈Iret Ω k that support the insights I S utilized for solving Q. G next query denotes the updated query graph.Finally, at the insight level, G-Memory integrates the learning from the completed query Q into the insight graph G insight = (I, E i ).First, possible new insights summarizing the experience are generated and structurally linked via a summarization function J (•, •) (see prompt in Appendix C) as follows:
ι new = (J (G (Q) inter , Ψ), {q new }), E i, new ← {(ι k , ι new , q new ) | ι k ∈ I S } G ′ insight ← (I ∪ {ι new }, E i ∪ E i, new )(10)
where edges are added to connect the previously utilized insights which inspires the completion of Q in Equation (6).Afterward, the supporting query sets (Ω k ) for the utilized insights (I S ) are updated to include q new , reflecting their relevance to this successful (or failed) application:
I next ← (I \ I ret ) ∪ {(κ k , Ω k ∪ {q new }) | ι k = (κ k , Ω k ) ∈ I ret } ∪ {ι new } G next insight ← (I next , E i ∪ E i, new ),(11)
where the final node set I next incorporates the new insight and the updated versions of the utilized insights, and the resulting graph G next insight thus encapsulates the integrated knowledge.This continuous update cycle across all hierarchical levels enables G-Memory to learn and adaptively refine its collective memory based on ongoing experience.</p>
<p>Experiment</p>
<p>In this section, we conduct extensive experiments to answer: (RQ1) How does G-Memory perform compared to existing single/multi-agent memory architectures?(RQ2) Does G-Memory incur excessive resource overhead?(RQ3) How sensitive is G-Memory to its key components and parameters?</p>
<p>Experiment Setup</p>
<p>Datasets and Benchmarks.To thoroughly evaluate the effectiveness of G-Memory, we adopt five widely-adopted benchmarks across three domains: (1) Knowledge reasoning, including Hot-potQA [76] and FEVER [77];</p>
<p>(2) Embodied action, including ALFWorld [78] and SciWorld [79];</p>
<p>(3) Game, namely PDDL [80].Details on these benchmarks are in Appendix A.1.</p>
<p>Baselines.We select four representative single-agent memory baselines, including non-memory, Voyager [16], MemoryBank [36], and Generative Agents [19], as well as three multi-agent memory implementations from MetaGPT [21], ChatDev [46], and MacNet [47], denoted as MetaGPT-M, ChatDev-M, and MacNet-M, respectively.Details are in Appendix A.2. MAS and LLM Backbones.We select three representative multi-agent frameworks to integrate with G-Memory and the baselines, including AutoGen [13], DyLAN [72], and MacNet [47].More details on the MAS setups are placed in Appendix A.3.For instantiating these MAS frameworks, we adopt two open-source LLMs, Qwen-2.5-7b and Qwen-2.5-14b,as well as one proprietary LLM, gpt-4o-mini.The deployment of Qwen series is via local instantiation using Ollama1 , and GPT models are accessed via OpenAI APIs.Parameter Configurations.We implement the embedding function v(•) in Equation ( 4) with ALL-MINILM-L6-V2 [81].The number of the most relevant interaction graphs M in Equation ( 7) is set among {2, 3, 4, 5}, and the number of relevant queries k in Equation ( 4) is set among {1, 2}.The detailed ablation study on hyper-parameters is placed in Section 5.4.</p>
<p>Main Results (RQ1)</p>
<p>Tables 1, 2 and 3 comprehensively report the performance of different memory architectures across three LLM backbones and three MAS frameworks.We summarize the key observations as follows: Takeaway ➊: G-Memory consistently improves performance across all task domains and MAS frameworks.As shown in Table 2, when integrated with AutoGen and MacNet (powered by Qwen-2.5-7b),G-Memory surpasses the best-performing single-/multi-agent memory baselines by an average of 6.8% and 5.5%, respectively.With the more capable Qwen-2.5-14b, the improvement is even more pronounced: in Table 3, G-Memory boosts MacNet's performance on ALFWorld from 58.21% to 79.10%, achieving a substantial 20.89% gain.</p>
<p>Takeaway ➋: Multi-agent systems demand specialized memory designs.A thorough examination of existing baselines reveals a surprising insight: most memory mechanisms fail to consistently benefit MAS settings.In Table 2, baselines such as Voyager and MemoryBank degrade AutoGen's performance on PDDL by as much as 4.17% and 1.34%, respectively.We attribute this to the inability of these methods to provide agent role-specific memory support, which is essential in the PDDL strategic game tasks, where effective division of labor is critical to success.Even MAS-oriented designs, such as ChatDev-M, result in a 2.32% performance drop when applied to MacNet+SciWorld.We attribute this to ChatDev-M's narrow memory scope-storing only the execution results of past queries, which provides limited utility in embodied action environments.These findings highlight the necessity of G-Memory's core characteristics: role-specific memory cues, abstracted high-level insights, and trajectory condensation-all of which are critical for effective memory in MAS.</p>
<p>Cost Analysis (RQ2)</p>
<p>To evaluate the efficiency of G-Memory in terms of token consumption, we visualize the performance versus token cost trade-off across various settings, as shown in Figures 3 and 7. Our findings are: Takeaway ➌: G-Memory achieves high-performing collective memory without excessive token consumption.As depicted in Figure 3, G-Memory consistently delivers the highest performance improvement (10.32% ↑ over no-memory setting on PDDL+AutoGen) while maintaining a modest increase in token consumption (only 1.4 × 10 6 ).In contrast, MetaGPT-M incurred an additional 2.2 × 10 6 tokens for a mere 4.07% gain.This clearly demonstrates the token-efficiency of G-Memory.</p>
<p>Framework Analysis (RQ3)</p>
<p>Sensitivity Analysis.Regarding the hop expansion, as shown in Figure 4a, 1-hop expansion consistently yields the best or near-best performance across tasks, with peak accuracies of 85.82% (ALFWorld), 55.24% (PDDL) in AutoGen.In contrast, 2-hop and 3-hop settings often degrade performance, e.g., PDDL drops to 49.79% (2-hop).This suggests that excessive hop expansion may introduce irrelevant insights during memory upward traversal, impairing task-specific reasoning.</p>
<p>Similarly, Figure 4b shows that the optimal k is among {1, 2}.Larger k values (e.g., k=5) can significantly degrade the system performance, e.g., 7.71% ↓ on ALFWorld+AutoGen and 2.5% ↓ on FEVER+DyLAN, indicating that retrieving more queries may introduce task-irrelevant noise.</p>
<p>Collectively, we employ 1-hop expansion and k ∈ {1, 2} throughout the experiments.</p>
<p>Ablation Study. Figure 4c presents an ablation of G-Memory by isolating the impact of the highlevel insight module (I S in Equation ( 6)) and fine-grained interactions ({ ĜQi inter }</p>
<p>|M |</p>
<p>i=1 in Equation ( 7)).As shown, removing either part leads to a consistent performance drop.When only fine-grained interactions are enabled, the average scores drop by 4.47% ↓ for AutoGen and 3.82% ↓ for DyLAN   6)) or fine-grained interactions (i.e., the core trajectories in Equation ( 7)).All the experiments here are done with Qwen-2.5-14b.</p>
<p>ALFWorld + AutoGen</p>
<p>Query put a clean cloth in countertop</p>
<dl>
<dt>AutoGen Team</dt>
<dd>
<p>Ensure all required items are accessible, clean them, and return them to their designated storage locations or the specified location after use.The goal is to satisfy the following conditions: b2 is on b3., b3 is on b1.</p>
</dd>
</dl>
<p>Fine-grained Trajectory</p>
<p>High-level Insights</p>
<p>Unstack b2 from b3</p>
<p>Check b1 and b3</p>
<p>Unstack b3 from b1</p>
<p>... b1 is on b2., b2 is on b6., b3 is on b7., b5 is on b3., b6 is on b5., b7 is on b4 edge agent For : Ensure that blocks are clear and in the correct positions before attempting to stack them on another block, because this prevents invalid actions and ensures the blocks are placed correctly.</p>
<p>Check b3 and b2</p>
<p>Stack b2 on b3 compared to the full method.Conversely, enabling only insights leads to smaller drops of 3.95% and 3.39%.This indicates that while both components are contributive, interactions offer a slightly greater impact, likely due to their preserving more fine-grained, dialogue-level contextual grounding.</p>
<p>Case Study</p>
<p>Figure 5 illustrates concrete memory cues provided by G-Memory across diverse tasks.For example, in the ALFWorld+AutoGen setting, given the task query "put a clean cloth in countertop", G-Memory successfully retrieves a highly analogous historical query, "put a clean egg in microwave"-both requiring the object to be in a clean state.Alongside this, G-Memory surfaces a critical trajectory segment where the solver agent attempts to place the egg in the microwave before cleaning, prompting the ground agent to intervene.This collaborative trajectory offers actionable guidance for the current task.Moreover, the high-level insights retrieved by G-Memory prove equally valuable for task execution.In the context of HotpotQA's web search task, G-Memory retrieves an insight warning against "mistakenly referring", which helps prevent agents from incorrectly answering based on similarly named individuals.Overall, G-Memory provides effective multi-level memory support across varied domains, including embodied action, knowledge reasoning, and game environments.</p>
<p>Conclusion &amp; Limitation</p>
<p>In this paper, we conduct a thorough examination of existing memory architectures designed for multi-agent systems (MAS) and identify that their overly simplified designs fundamentally hinder the systems' capacity for self-evolution.To bridge this gap, we propose G-Memory, a hierarchical memory framework that organizes the complex and extended interaction trajectories of MAS into a three-tier graph hierarchy: the insight, query, and interaction graphs.G-Memory provides each agent with customized and hierarchical memory cues, ranging from abstract, generalizable insights to fine-grained, task-critical collaborative segments, and dynamically evolves its knowledge base across episodes.Extensive experiments demonstrate that G-Memory can be seamlessly integrated into state-of-the-art MAS frameworks, significantly enhancing their self-evolution capability, e.g., up to 20.89% ↑ improvement on embodied action tasks.Limitations: Although G-Memory has been evaluated across three domains and five benchmarks, further validation on more diverse tasks (e.g., medical QA) would strengthen its soundness, which we leave for future work.</p>
<p>[81] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.Advances in Neural Information Processing Systems, 33:5776-5788, 2020.</p>
<p>Impact Statement</p>
<p>G-Memory introduces a structured, hierarchical memory architecture for multi-agent systems (MAS), enabling large language model (LLM)-based agents to store, recall, and reason over past experiences with enhanced task generalization and cooperation efficiency.The broader impacts of this work include advancing the development of scalable and adaptive collective intelligence, with potential applications in long-term robotic planning, real-world decision-making systems, and collaborative AI assistants.However, if underlying language model is compromised or adversarially manipulated, the memory mechanisms could amplify incorrect reasoning.We urge responsible deployment of this architecture with appropriate safeguards, including continual validation, adversarial robustness checks, and alignment with human values.</p>
<p>A Experimental Details</p>
<p>A.1 Dataset Descriptions</p>
<p>In this section, we describe the datasets used in our experiments:</p>
<p>• ALFWorld [78] (available at https://alfworld.github.io/,MIT license) is a textbased embodied environment featuring household tasks, where agents navigate and interact with objects via natural language commands.</p>
<p>• ScienceWorld [79] (available at https://github.com/allenai/ScienceWorld,Apache-2.0license) is another text-based embodied environment designed for interactive science tasks.Agents must navigate rooms and conduct experiments, testing their ability to perform procedural reasoning and scientific exploration.</p>
<p>• PDDL is a game dataset from AgentBoard [80] (available at https://github.com/hkust-nlp/AgentBoard, Custom properties), comprising a variety of strategic games where agents use PDDL expressions to complete complex tasks.</p>
<p>• HotpotQA [76] (available at https://hotpotqa.github.io/,CC BY-SA 4.0 License) is a multi-hop question answering dataset with strong supervision on supporting facts.It evaluates the agent's ability to retrieve and synthesize information, especially through web search tools, for explainable reasoning.</p>
<p>• FEVER [77] (available at https://fever.ai/dataset/fever.html,Creative Commons Attribution-ShareAlike License) is a knowledge-intensive dataset focused on fact verification.Agents must validate claims using web search APIs, making it a benchmark for evidence-based reasoning.</p>
<p>Evaluation Metrics.We use exact match accuracy for FEVER and HotpotQA.For ScienceWorld and PDDL, we report the progress rate, and for ALFWorld, we use the success rate as the evaluation metric.</p>
<p>A.2 Baseline Setup</p>
<p>In this section, we provide detailed descriptions of each baseline used in our comparison:</p>
<p>• Voyager: The Voyager memory is derived from the Voyager agent [16], where an embodied agent continuously interacts with the Minecraft environment and creates new artifacts.Memory serves as the core driver of the agent's evolution.As Voyager's memory design is tailored for a single-agent setting, we adapt it to the multi-agent scenario by implementing agent-specific history retrieval based on each agent's visible dialogue context.Other singleagent memory designs are adapted in a similar manner.</p>
<p>• MemoryBank: MemoryBank [36] mimics anthropomorphic memory behaviors by selectively preserving and forgetting information.It incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting Curve, allowing the agent to reinforce or discard memory based on temporal decay and the relative importance of stored information.</p>
<p>• Generative: This memory baseline is on [19], which includes both raw observational memory and high-level reflective memory.The latter captures abstract thoughts generated by the agent through reflection, providing a more structured and conceptualized representation of experience.</p>
<p>• MetaGPT-M: The memory design originates from MetaGPT [21], focusing solely on inside-trial memory-information stored internally during the resolution of a single task by multiple agents.</p>
<p>• ChatDev-M: This memory design is adapted from ChatDev [46], which incorporates both inside-trial and cross-trial memory.The inside-trial memory is passed from the central or initiating agent at the beginning of each round to provide guidance based on prior interactions.The cross-trial memory is relatively simple, storing past solutions to previous queries for future retrieval.However, in our task, it does not effectively manage the information-rich inter-agent collaboration.</p>
<p>• MacNet-M: This memory design is adopted from MacNet [47], where the inside-trial memory consists solely of the final answers generated in the previous round.All non-artifact dialogue contexts, i.e., the interaction trajectories among agents, are entirely discarded.</p>
<p>A.3 Multi-agent System Setup</p>
<p>In this section, we detail the setups of our three adopted MAS frameworks, AutoGen, DyLAN and MacNet:</p>
<p>A.3.1 AutoGen</p>
<p>AutoGen [13] is a popular multi-agent orchestration framework, to coordinate interactions among specialized agents for problem-solving tasks.Specifically, we utilize their A3 : Decision Making structure, which is composed of: (1) a Solver Agent, responsible for generating solutions, initialized with the system prompt "You are a smart agent designed to solve problems.";(2) a Ground Truth Agent, which critically evaluates the solver's output and identifies potential errors based on a reference standard; and (3) an Executor Agent, tasked with translating validated solutions into executable commands.This modular design enables transparent, verifiable, and actionable multiagent collaboration.</p>
<p>A.3.2 DyLAN</p>
<p>DyLAN [72] is a debate-style framework similar to LLM-Debate, but incorporates a more efficient agent-wise early stopping mechanism during multi-turn interactions.DyLAN utilizes an agent selection algorithm based on an unsupervised metric, namely the Agent Importance Score, which identifies the most contributive agents through a preliminary trial tailored to the specific task.In our implementation of DyLAN, three agents engage in the debate, while an additional ranker agent evaluates their relative importance.</p>
<p>A.4 MacNet</p>
<p>MacNet [47] is a representative work that explores decentralized and scalable multi-agent systems.</p>
<p>Its key feature lies in the absence of a central agent; instead, it introduces edge agents, which are invoked between agent interactions to provide actionable instructions to the next agent based on the previous agent's outputs.In our implementation, we adopt the random graph topology from MacNet, shown to be robust across diverse scenarios, and employ five agents in addition to the edge agents.</p>
<p>B Additional Experiment Results</p>
<p>B.1 RQ1 Results</p>
<p>Tables 2 and 3 present additional experimental results using Qwen-2.5-7band Qwen-2.5-14bas the LLM backbones.Appendix B.1 illustrates the success rate curves on ALFWorld as the number of trials increases, comparing different MAS frameworks combined with various memory architectures.As shown in Figures 6b and 6c, G-Memory consistently enables MAS frameworks to achieve success with fewer trials and leads to higher final performance ceilings.The prompt below is partially adapted from [43].We would like to express our sincere gratitude for their valuable implementation.You are an analysis -driven agent focused on learning from success .You will be provided with a set of successful trajectories that completed a similar task .As the summarizing agent , remove redundancies , combine similar ideas , and ensure clarity .</p>
<p>Inisght Summarization Function</p>
<p>Your output : """</p>
<p>Customizing Memory for Agents p r o j e c t _ i n s i g h t s _ s y s t e m _ p r o m p t : str = """ You are a thoughtful and context -aware agent .You will be provided with a successfully executed trajectory , a specific agent ** role ** , and a set of ** general insights ** applicable across all roles .Your task is to ** adapt these general insights ** into ** personalized insights ** that are specifically tailored to the given role and its trajectory .These personalized insights should help the agent improve future performance by aligning with their unique background , responsibilities , and perspective .Make sure your output reflects an understanding of the role ' s context and promotes actionable , role -relevant advice .</p>
<p>NOTE -Your output must strictly follow the format below :</p>
<p>D Discussion with Related Works</p>
<p>In this section, we further discuss the relationship between G-Memory and several recent agent memory frameworks.For A-Mem [61], while both A-Mem and G-Memory aim to enhance the memory capabilities of LLM agents, they differ in two key aspects.First, A-Mem is tailored for single-agent scenarios, whereas G-Memory is designed for processing MAS's lengthy and nuanced interaction trajectory.Second, A-Mem emphasizes atomic memory construction for chatbot-style interactions, while G-Memory focuses on distilling reusable strategies from collaborative task execution, where fine-grained atomicity is neither required nor beneficial.For Mem0 [62], although it also employs a graph-based structure, it remains within the chatbot paradigm.Its graph is closer to a knowledge graph, where nodes represent factual entities and edges represent relations, fundamentally differing from G-Memory's agent-centric memory graphs that encode trajectories, decisions, and coordination patterns across agents.</p>
<p>Figure 1 :
1
Figure 1: (Left) We report the token cost of several single-agent and MAS baselines on ALFWorld benchmark; (Right) The overview of G-Memory's three-tier hierarchical memory architecture, encompassing the insight graph, query graph and interaction (utterance) graph.</p>
<p>Embodied</p>
<p>You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, ... Your task is to: put a clean egg in microwave.</p>
<p>Figure 2 :
2
Figure 2: The overview of our proposed G-Memory.</p>
<p>Figure 3 :
3
Figure 3: Cost analysis of G-Memory.We showcase the performance versus the overall system token cost when combined with different memory architectures.</p>
<p>(a) Sensitivity analysis on #hop.(b) Sensitivity analysis on parameter k.Ablation study on two variants of G-Memory.</p>
<p>Figure 4 :
4
Figure 4: (a) Sensitivity analysis of the hop expansion in Equation (5); (b) Sensitivity analysis of the number of selected queries k in Equation (4); (c) We study two variants of G-Memory: merely providing high-level insights (i.e., the insights I S in Equation (6)) or fine-grained interactions (i.e., the core trajectories in Equation (7)).All the experiments here are done with Qwen-2.5-14b.</p>
<p>an item, ensure it is placed in the designated storage location immediately to avoid confusion or loss.For Task: put a clean egg in microwave.or Maxillaria a genus of orchids?verify that the search results are not mistakenly referring to similar entities with similar names or unrelated information.Avoid mistakenly referringAre Ruggero Deodato from Italy, and Mexican Alejandro Springall, both film directors?"</p>
<p>Figure 5 :
5
Figure 5: Case study of G-Memory.</p>
<p>Figure 7
7
Figure 7 provides additional comparisons of token cost across various benchmarks and MAS frameworks when combined with different memory architectures.Overall, G-Memory incurs only a marginal or no increase in token cost compared to classical baselines such as Generative and MetaGPT-M, while consistently delivering the most significant performance improvements.</p>
<p>l e a r</p>
<p>n _ l e s s o n s _ s y s t e m _ p r o m p t _ c o m p a r e = """ You are an analysis -driven agent focused on learning from experience .You will be provided with : -A failed trajectory and its outcome , -A successful trajectory completing a similar task .Your task is to analyze both trajectories and generate clear , actionable insights .Your insights should highlight what the failed trajectory missed and how the successful one addressed or avoided these pitfalls .##Requirements :-All insights must be derived directly from contrasting the two trajectories .-Donot speculate or introduce steps not supported by the successful example .-Focus on ** concrete behavioral or strategic differences ** between the two cases .l e a r n _ l e s s o n s _ u s e r _ p r o m p t _ c o m p a r e = "r n _ l e s s o n s _ s y s t e m _ p r o m p t _ a l l _ s u c c = """</p>
<p>Insight graph on gpt-4o-mini +Dy-LAN+ALFWorld.Insight graph on Qwen-14b +Dy-LAN+ALFWorld.</p>
<p>Figure 8 :
8
Figure 8: Visualizations of insight graphs across different LLM backbones, MAS, and benchmarks.</p>
<p>Figure 9 :
9
Figure 9: Query graph optimized from ALFWorld dataset.</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Query graph optimized from SciWorld dataset.</p>
<p>prompt m e r g e _ r u l e s _ s y s t e m _ p r o m p t = """ You are an agent skilled at summarizing and distilling insights .You are given a list of insights that were previously extracted from similar tasks .These insights may contain redundancy or overlap .Your job is to ** merge and consolidate similar insights ** , and output a refined version that is ** clear , actionable , and concise <strong>.NOTE :-All merged insights ** must be based strictly on the given inputs </strong>.You are ** not allowed to make up ** or infer any new information .-The output should be easy to read and follow .Output Format : -Start your response directly with the numbered list , no preamble or explanations .-Each insight should be a short sentence .-Use the following format exactly : 1g e _ r u l e s _ u s e r _ p r o m p t = """ ## Here are the current insights that need to be merged : { current_rules } ## Please consolidate and rewrite them into ** no more than { l imite d_numb er } refined insights **.</p>
<p>Table 1 :
1
Performance comparison with single/multi-agent memory architectures on five benchmarks.The underlying LLM backbone is GPT-4o-mini.We highlight the best and second best results.
MASMemoryALFWorldSciWorldPDDLHotpotQAFEVERAvg.No-memory77.61 ↑0.0054.49 ↑0.0023.53 ↑0.0028.57 ↑0.0057.13 ↑0.0048.27 ↑0.00Voyager85.07 ↑7.4662.36 ↑7.8724.56 ↑1.0332.32 ↑3.7563.27 ↑6.1453.52 ↑5.25AutoGenMemoryBank74.96 ↓2.6553.11 ↓1.3820.41 ↓3.1233.67 ↑5.1061.22 ↑4.0948.67 ↑0.40COLM 2024Generative86.36 ↑8.7561.19 ↑6.7025.53 ↑2.0031.63 ↑3.0660.20 ↑3.0752.98 ↑4.71MetaGPT81.34 ↑3.7361.91 ↑7.4221.63 ↓1.9032.67 ↑4.1062.67 ↑5.5452.04 ↑3.77ChatDev79.85 ↑2.2450.96 ↓3.5316.65 ↓6.8824.49 ↓4.0859.18 ↑2.0546.23 ↓2.04MacNet76.55 ↓1.0655.44 ↑0.9522.94 ↓0.5928.36 ↓0.2160.87 ↑3.7448.83 ↑0.56G-Memory (Ours)88.81 ↑11.2067.40 ↑12.9127.77 ↑4.2435.67 ↑7.1066.24 ↑9.1157.18 ↑8.91No-memory56.72 ↑0.0055.38 ↑0.0011.62 ↑0.0031.69 ↑0.0060.20 ↑0.0043.12 ↑0.00Voyager66.42 ↑9.7062.83 ↑7.4515.10 ↑3.4832.64 ↑0.9562.24 ↑2.0447.85 ↑4.73DyLAN COLM 2024MemoryBank Generative55.22 ↓1.50 67.91 ↑11.1954.74 ↓0.64 64.16 ↑8.788.08 ↓3.54 13.87 ↑2.2529.59 ↓2.10 29.29 ↓2.4059.13 ↓1.07 62.30 ↑2.1041.35 ↓1.77 47.51 ↑4.39MetaGPT-M69.40 ↑12.6862.37 ↑6.9914.45 ↑2.8332.34 ↑0.6560.20 ↑0.0047.75 ↑4.63ChatDev-M46.27 ↓10.4553.35 ↓2.0310.75 ↓0.8722.45 ↓9.2458.33 ↓1.8738.23 ↓4.89MacNet-M53.44 ↓3.2854.32 ↓1.0612.11 ↑0.4930.12 ↓1.5761.10 ↑0.9042.22 ↓0.90G-Memory (Ours)70.90 ↑14.1865.64 ↑10.2618.95 ↑7.3334.69 ↑3.0064.22 ↑4.0250.88 ↑7.76No-memory51.49 ↑0.0057.53 ↑0.0012.18 ↑0.0028.57 ↑0.0060.29 ↑0.0042.01 ↑0.00Voyager61.94 ↑10.4564.53 ↑7.0014.06 ↑1.8832.65 ↑4.0862.54 ↑2.2547.14 ↑5.13MacNetMemoryBank50.00 ↓1.4960.15 ↑2.628.64 ↓3.5433.67 ↑5.1061.22 ↑0.9342.74 ↑0.73ICLR 2025Generative62.69 ↑11.2065.49 ↑7.967.92 ↓4.2629.59 ↑1.0263.27 ↑2.9845.79 ↑3.78MetaGPT-M63.70 ↑12.2165.27 ↑7.7416.03 ↑3.8531.00 ↑2.4359.33 ↓0.9647.07 ↑5.06ChatDev-M49.25 ↓2.2456.58 ↓0.9513.51 ↑1.3329.00 ↑0.4359.18 ↓1.1141.50 ↓0.51MacNet-M53.44 ↑1.9556.14 ↓1.3913.59 ↑1.4127.89 ↓0.6859.20 ↓1.0942.05 ↑0.04G-Memory (Ours)67.16 ↑15.6768.11 ↑10.5824.33 ↑12.1535.69 ↑7.1264.44 ↑4.1551.95 ↑9.94</p>
<p>Figure8visualizes the high-level insights summarized by G-Memory on the ALFWorld benchmark across different MAS frameworks and LLM backbones.Given that ALFWorld naturally consists of diverse task categories, we further examine how insight nodes corresponding to different task types are interconnected.Overall, we observe dense intra-category connections among insights derived from similar tasks, while also noting the emergence of meaningful inter-category links, reflecting transferable patterns across task domains.B.3.2CaseStudy on Query GraphsFigures 9 to 11 visualize the query graphs constructed by G-Memory on the ALFWorld, PDDL, and SciWorld benchmarks.Recall that a directed edge between two query nodes indicates that the historical trajectory of one query offers useful guidance for the execution of another.We observe emergent clustering patterns, where groups of semantically similar queries form densely connected subgraphs, while sparser inter-cluster edges capture cross-task inspirations.These patterns demonstrate G-Memory's ability to effectively organize and relate collaborative experiences through structured memory reasoning.
B.3 Case StudyB.3.1 Case Study on Insight Graphs</p>
<p>Table 2 :
2
Performance comparison with single/multi-agent memory architectures on five benchmarks.The underlying LLM backbone is Qwen-2.5-7b.We highlight the best and second best results.
MASMemoryALFWorldSciWorldPDDLHotpotQAFEVERAvg.No-memory37.31 ↑0.0023.49 ↑0.0010.86 ↑0.0020.26 ↑0.0048.17 ↑0.0028.02 ↑0.00Vanilla LLMVoyager MemoryBank38.19 ↑0.88 40.30 ↑2.9924.11 ↑0.62 21.64 ↓1.8512.14 ↑1.28 14.36 ↑3.5019.12 ↓1.14 18.79 ↓1.4749.68 ↑1.51 47.66 ↓0.5128.65 ↑0.63 28.55 ↑0.53Generative39.16 ↑1.8526.10 ↑2.6111.37 ↑0.5123.48 ↑3.2252.50 ↑4.3330.52 ↑2.50No-memory52.99 ↑0.0030.27 ↑0.0016.17 ↑0.0033.33 ↑0.0058.74 ↑0.0038.30 ↑0.00Voyager55.22 ↑2.2326.70 ↓3.5712.00 ↓4.1734.29 ↑0.9652.44 ↓6.3036.13 ↓2.17MemoryBank53.37 ↑0.3827.33 ↓2.9414.83 ↓1.3432.67 ↓0.6659.45 ↑0.7137.53 ↓0.77AutoGenGenerative62.69 ↑9.7031.45 ↑1.1817.88 ↑1.7134.17 ↑0.8461.25 ↑2.5141.49 ↑3.19COLM 2024</p>
<p>Table 3 :
3
Performance comparison with single/multi-agent memory architectures on five benchmarks.The underlying LLM backbone is Qwen-2.5-14b.We highlight the best and second best results.
MASMemoryALFWorldSciWorldPDDLHotpotQAFEVERAvg.
http://github.com/ollama/ollama
C Prompt SetQuery Relevance Filtration t a s k _ r e l e v e n c y _ s y s t e m _ p r o m p t = """ You are an agent designed to score the relevance between two pieces of text ."""t a s k _ r e l e v e n c y _ u s e r _ p r o m p t = """ You will be given a successful case where you successfully complete the task .Then you will be given an ongoing task .Do not summarize these two cases , but rather evaluate how relevant and helpful the successful case is for the ongoing task , on a scale of 1 -10.Success Case : { trajectory } Ongoing task : { qu ery_sc enario } Score : """Graph Sparsifier e x t r a c t _ t r u e _ t r a j _ s y s t e m _ p r o m p t = """ You are an agent skilled at extracting key points .Given a task and a successful execution trajectory , your job is to identify the critical steps needed to complete the task while filtering out less important steps ."""e x t r a c t _ t r u e _ t r a j _ u s e r _ p r o m p t = """ Note :-Strictly follow the original trajectory ; absolutely no steps that are not in the trajectory should be added .
. P James, Gerardo Walsh, Rivera Ungson, Organizational memory. Academy of management review. 1611991</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Ayzaan Chowdhery, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Huang, 2023</p>
<p>Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, Jose M Alvarez, arXiv:2405.015332024arXiv preprint</p>
<p>Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds. Sipeng Zheng, Jiazheng Liu, Yicheng Feng, Zongqing Lu, arXiv:2310.132552023arXiv preprint</p>
<p>Editable scene simulation for autonomous driving via collaborative llm-agents. Yuxi Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing Liu, Hao Zhao, Siheng Chen, Yanfeng Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Knowagent: Knowledge-augmented planning for llm-based agents. Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen, Ningyu Zhang, arXiv:2403.031012024arXiv preprint</p>
<p>Plan-and-act: Improving planning of agents for long-horizon tasks. Nicholas Lutfi Eren Erdogan, Sehoon Lee, Suhong Kim, Hiroki Moon, Gopala Furuta, Kurt Anumanchipalli, Amir Keutzer, Gholami, arXiv:2503.095722025arXiv preprint</p>
<p>Understanding the planning of llm agents: A survey. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen, arXiv:2402.027162024arXiv preprint</p>
<p>Agent q: Advanced reasoning and learning for autonomous ai agents. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, Rafael Rafailov, arXiv:2408.071992024arXiv preprint</p>
<p>The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: A survey. Tula Masterman, Sandi Besen, Mason Sawtell, Alex Chao, arXiv:2404.115842024arXiv preprint</p>
<p>Embodied agent interface: Benchmarking llms for embodied decision making. Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, Advances in Neural Information Processing Systems. 202437</p>
<p>Embodied multi-modal agent trained by an llm from a parallel textworld. Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, August 01. 2023 2023</p>
<p>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, 2024</p>
<p>Data interpreter: An llm agent for data science. Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, arXiv:2402.186792024arXiv preprint</p>
<p>Voyager: An Open-Ended Embodied Agent with Large Language Models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.16291May 2023arXiv e-prints</p>
<p>Driving with llms: Fusing object-level vector modality for explainable autonomous driving. Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, Jamie Shotton, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Optimizing autonomous driving for safety: A human-centric approach with llm-enhanced rlhf. Yuan Sun, Navid Salami Pargoo, Peter Jin, Jorge Ortiz, Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing. 2024</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Generative agents: Interactive simulacra of human behavior. April 01. 2023 2023</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, CoRR, abs/2305.143252023</p>
<p>Metagpt: Meta programming for multi-agent collaborative framework. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, August 01. 2023 2023</p>
<p>Society of mind. Marvin Minsky, 1988Simon and Schuster</p>
<p>Examining the society of mind. Push Singh, Comput. Artif. Intell. 2262003</p>
<p>CAMEL: communicative agents for "mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, NeurIPS2023</p>
<p>Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, July 01, 2023 2023work in progress</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, CoRR, abs/2402.016802024</p>
<p>Reasoning capacity in multi-agent systems: Limitations, challenges and human-centered solutions. Pouya Pezeshkpour, Eser Kandogan, Nikita Bhutani, Sajjadur Rahman, Tom Mitchell, Estevam Hruschka, CoRR, abs/2402.011082024</p>
<p>Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, arXiv:2404.16698Mrinmaya Sachan, and Rada Mihalcea. Cooperate or collapse: Emergence of sustainability behaviors in a society of llm agents. 2024arXiv preprint</p>
<p>Discovering causality for efficient cooperation in multi-agent environments. Rafael Pina, Varuna De Silva, Corentin Artaud, CoRR, abs/2306.118462023</p>
<p>Cut the crap: An economical communication pipeline for llm-based multi-agent systems. Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, Tianlong Chen, arXiv:2410.025062024arXiv preprint</p>
<p>Masrouter: Learning to route llms for multi-agent systems. Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, Yiyan Qi, arXiv:2502.111332025arXiv preprint</p>
<p>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Chen Hao, Xing Xie, arXiv:2310.17512Competeai: Understanding the competition behaviors in large language model-based agents. 2023arXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, CoRR, abs/2305.191182023</p>
<p>Battleagentbench: A benchmark for evaluating cooperation and competition capabilities of language models in multi-agent systems. Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, Jie Tang, arXiv:2408.159712024arXiv preprint</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, April 01, 2023 2023Tech Report</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt: Towards llms as operating systems. 2023</p>
<p>Memllm: Finetuning llms to use an explicit read-write memory. Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze, arXiv:2404.116722024arXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024arXiv preprint</p>
<p>Chatdb: Augmenting llms with databases as their symbolic memory. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, arXiv:2306.039012023arXiv preprint</p>
<p>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.082392023arXiv preprint</p>
<p>Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang, Recmind, arXiv:2308.14296Large language model powered agent for recommendation. 2023arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Yuan Li, Yixuan Zhang, Lichao Sun, Metaagents, arXiv:2310.06500Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. 2023arXiv preprint</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.14984Social-network simulation system with large language model-empowered agents. 20233arXiv preprint</p>
<p>Communicative agents for software development. Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, July 01, 2023 2023252 tables</p>
<p>Scaling large-language-model-based multi-agent collaboration. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, arXiv:2406.071552024arXiv preprint</p>
<p>Gptswarm: Language agents as optimizable graphs. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber, Forty-first International Conference on Machine Learning. 2024</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, arXiv:2408.084352024arXiv preprint</p>
<p>. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu, arXiv:2410.10762AFlow: Automating Agentic Workflow Generation. October 2024</p>
<p>Multi-agent architecture search via agentic supernet. Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, Xiang Wang, arXiv:2502.041802025arXiv preprint</p>
<p>Exchange-of-thought: Enhancing large language model capabilities through cross-model communication. Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuan-Jing Huang, Xipeng Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen, A survey on large language model based autonomous agents. Front. Comput. Sci. 182024</p>
<p>Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, 2023arxiv preprint, abs/2309.07864</p>
<p>Large language models empowered agent-based modeling and simulation: A survey and perspectives. Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, Yong Li, CoRR, abs/2312.119702023</p>
<p>A survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, Yi Yang, 2024Vicinagearth19</p>
<p>Synapse: Trajectory-as-exemplar prompting with memory for computer control. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An, arXiv:2306.078632023arXiv preprint</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.171442023arXiv preprint</p>
<p>Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, arXiv:2501.06590Self-updating library in large language models improves chemical reasoning. 2025arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, abs/2303.113662023arXiv preprint</p>
<p>A-mem: Agentic memory for llm agents. Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang, arXiv:2502.121102025arXiv preprint</p>
<p>Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, Deshraj Yadav, arXiv:2504.19413Mem0: Building production-ready ai agents with scalable long-term memory. 2025arXiv preprint</p>
<p>Meminsight: Autonomous memory augmentation for llm agents. Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine Benajiba, arXiv:2503.217602025arXiv preprint</p>
<p>Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou, arXiv:2406.04692Mixture-of-agents enhances large language model capabilities. 2024arXiv preprint</p>
<p>Symbolic learning enables self-evolving agents. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, arXiv:2406.185322024arXiv preprint</p>
<p>Self-evolving agents with reflective and memory-augmented abilities. Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, Jingsong Yang, arXiv:2409.008722024arXiv preprint</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou, 2023</p>
<p>Self-evolving multi-agent collaboration networks for software development. Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, Siheng Chen, arXiv:2410.169462024arXiv preprint</p>
<p>Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, Dawei Cheng, G-Designer, arXiv:2410.11782Architecting multi-agent communication topologies via graph neural networks. 2024arXiv preprint</p>
<p>Evoagent: Towards automatic multi-agent generation via evolutionary algorithms. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang, arXiv:2406.142282024arXiv preprint</p>
<p>Guibin Zhang, Kaijie Chen, Guancheng Wan, Heng Chang, Hong Cheng, Kun Wang, Shuyue Hu, Lei Bai, arXiv:2502.07373Evoflow: Evolving diverse agentic workflows on the fly. 2025arXiv preprint</p>
<p>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, CoRR, abs/2310.021702023</p>
<p>Microsoft academic graph: When experts are not enough. Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, Anshul Kanakia, Quantitative Science Studies. 112020</p>
<p>Sirius: Self-improving multiagent systems via bootstrapped reasoning. Wanjia Zhao, Mert Yuksekgonul, Shirley Wu, James Zou, arXiv:2502.047802025arXiv preprint</p>
<p>Reso: A reward-driven selforganizing llm-based multi-agent system for reasoning tasks. Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, Lei Bai, arXiv:2503.023902025arXiv preprint</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Fever: a large-scale dataset for fact extraction and verification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, arXiv:1803.053552018arXiv preprint</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.037682020arXiv preprint</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022arXiv preprint</p>
<p>Agentboard: An analytical evaluation board of multi-turn llm agents. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, arXiv:2401.13178,2024.MetaGPT-M55.52↑2.5332.44↑2.1717.04↑0.8735.36↑2.0363.33↑4.5940.74↑2.44arXiv preprint</p>
<p>. G-Memory , Ours) 67.91 ↑14.92 34.89 ↑4.62 21.01 ↑4.84 37.34 ↑4.01 64.34 ↑5.60 45.10 ↑6.80</p>
<p>. G-Memory , Ours) 52.99 ↑11.65 33.81 ↑3.97 20.71 ↑7.15 29.33 ↑5.04 63.67 ↑7.44 40.10 ↑7.05</p>
<p>. ICLR 2025 No-memory 44.03 ↑0.00 28.76 ↑0.00 13.36 ↑0.00 22.24 ↑0.00 55.12 ↑0.00 32.70 ↑0.00MacNet. </p>
<p>. G-Memory , Ours) 54.48 ↑10.45 32.23 ↑3.47 17.48 ↑4.12 27.53 ↑5.29 59.14 ↑4.02 38.17 ↑5.47</p>
<p>. G-Memory , Ours</p>
<p>. G-Memory , Ours) 81.34 ↑5.22 64.68 ↑11.44 51.12 ↑9.29 34.63 ↑4.02 66.66 ↑3.32 59.69 ↑6.66</p>
<p>. ICLR 2025 No-memory 58.21 ↑0.00 52.21 ↑0.00 41.74 ↑0.00 28.60 ↑0.00 64.65 ↑0.00 49.08 ↑0.00MacNet. </p>
<p>. G-Memory , 79.10 ↑20.89 61.74 ↑9.53 45.76 ↑4.02 32.33 ↑3.73 70.33 ↑5.68 57.85 ↑8.77</p>
<p>Even in a successful trajectory , there may be some incorrect steps . Pay attention to actions that correspond to " Nothing happens " observations , as these actions are likely incorrect . Filter out these actions for me . -You need to ensure that each step is at the finest granularity. You should strictly follow the output format in the example</p>            </div>
        </div>

    </div>
</body>
</html>
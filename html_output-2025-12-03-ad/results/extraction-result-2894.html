<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2894 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2894</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2894</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-266436682</p>
                <p><strong>Paper Title:</strong> Prompting for Socially Intelligent Agents with ChatGPT</p>
                <p><strong>Paper Abstract:</strong> Socially Intelligent Agents (SIAs) have become increasingly popular in various contexts, including education and entertainment. However, creating complex social scenarios tailored to a designer's specific goals remains a significant challenge. The authoring burden can be substantial, limiting the potential of SIAs to deliver rich, engaging experiences. In this work, we propose leveraging the extensive knowledge stored within Large Language Models and use theory-driven prompting to extract social practices and identify appropriate social affordances for a scenario description. Our prompting approach aims to guide the system into considering the essential components (beliefs and desires) necessary to produce intentions, actions, and emotions1. Results show that our approach produces large amounts of accurate and new information that can add value to the scenario. However, the process can introduce inaccuracies without human supervision.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. <em>(Rating: 2)</em></li>
                <li>Generative Agents: Interactive Simulacra of Human Behavior. <em>(Rating: 2)</em></li>
                <li>Beyond goldfish memory: Longterm open-domain conversation <em>(Rating: 1)</em></li>
                <li>Inner Monologue: Embodied Reasoning through Planning with Language Models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2894",
    "paper_id": "paper-266436682",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_play_text_games_well_current_stateoftheart_and_open_questions"
        },
        {
            "paper_title": "Generative Agents: Interactive Simulacra of Human Behavior.",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Beyond goldfish memory: Longterm open-domain conversation",
            "rating": 1,
            "sanitized_title": "beyond_goldfish_memory_longterm_opendomain_conversation"
        },
        {
            "paper_title": "Inner Monologue: Embodied Reasoning through Planning with Language Models.",
            "rating": 1,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        }
    ],
    "cost": 0.007986749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prompting for Socially Intelligent Agents with ChatGPT</p>
<p>Ana Antunes ana.j.antunes@tecnico.ulisboa.pt 
Instituto Superior T√©cnico</p>
<p>Universidade de Lisboa &amp; INESC-ID Lisbon
Portugal</p>
<p>Joana Campos</p>
<p>Instituto Superior T√©cnico</p>
<p>Universidade de Lisboa &amp; INESC-ID Lisbon
Portugal Manuel Guimar√£es</p>
<p>Instituto Superior T√©cnico</p>
<p>Universidade de Lisboa &amp; INESC-ID Lisbon
Portugal Jo√£o Dias</p>
<p>Faculdade de Ci√™ncias e Tecnologia
Universidade do Algarve &amp; CCMAR &amp; INESC-ID Lisbon
Pedro A. SantosPortugal</p>
<p>Instituto Superior T√©cnico</p>
<p>Universidade de Lisboa &amp; INESC-ID
LisbonPortugal</p>
<p>Prompting for Socially Intelligent Agents with ChatGPT
A7C99E941E65709A0CD01DB51FE452AF10.1145/3570945.3607303Socially Intelligent AgentsAuthoring Social ScenariosPrompt EngineeringLarge Language Models
Socially Intelligent Agents (SIAs) have become increasingly popular in various contexts, including education and entertainment.However, creating complex social scenarios tailored to a designer's specific goals remains a significant challenge.The authoring burden can be substantial, limiting the potential of SIAs to deliver rich, engaging experiences.In this work, we propose leveraging the extensive knowledge stored within Large Language Models and use theory-driven prompting to extract social practices and identify appropriate social affordances for a scenario description.Our prompting approach aims to guide the system into considering the essential components (beliefs and desires) necessary to produce intentions, actions, and emotions 1 .Results show that our approach produces large amounts of accurate and new information that can add value to the scenario.However, the process can introduce inaccuracies without human supervision.CCS CONCEPTS‚Ä¢ Human-centered computing ‚Üí Natural language interfaces.</p>
<p>INTRODUCTION</p>
<p>Socially Intelligent Agents (SIAs) boast increasing application scopes from conversational interfaces on websites to tutors or teammates in educational environments [10,38], where they are equipped with tools to conduct human-like interactions.Amongst the most promising applications of SIAs are serious games and social skills training environments.In these virtual environments, SIAs behaviours can range from reactive wandering in the background of a scenario to complex social interactions that provide social support or assist the player in some skill training [17,39].These autonomous agents sense the environment and act intelligently and independently from the user, allowing them to train and adapt specific verbal and nonverbal behaviours in socially challenging situations [3].</p>
<p>Creating compelling SIAs capable of interacting with human users is a multi-faceted task.It requires the agents have interactive capabilities (e.g., signalling and receiving information clearly) and models to support the cognitive process underlying decisionmaking.These models are responsible for action selection, complementary emotional and non-verbal behaviour, language and learning.A key challenge in the design of these applications is to manually define the social behavior of the agents, which requires extensive content creation.Traditionally, social agent modelling frameworks facilitate simulation of agents' cognitive and affective processes [13,18,29,41], enabling intelligent and emotional behaviour in countless situations.Nonetheless, it is up to the author of a scenario -typically instructors, therapists, or researchers -to manually describe how individual traits, goals, beliefs and actions interact, create dialogue trees and guarantee character adaptability and consistency as events unfold.This laborious rule creation task, while manageable in narrow domains of application, can quickly become an overbearing task when creating more complex scenarios (e.g., a serious game or social skills training content).This authoring burden can create a bottleneck in the design of a human-agent interaction experience and diminishes the widespread adoption of these socio-emotional architectures to create SIAs, which have proven effective in several domains [21].</p>
<p>To address this issue, we propose harnessing the power of Large Language Models (LLMs) to generate social behavior, grounded in the understanding that LLMs encapsulate information about both physical and social processes [36].Although LLMs may not be capable of fully replicating SIA behavior during prolonged interactions, we can tap into the knowledge they store.We propose utilizing Chain-of-Thought prompt engineering techniques [50], known to improve LLMs' reasoning capabilities in complex tasks like arithmetic and commonsense reasoning [12].Furthermore, we incorporate the Belief-Desire-Intention (BDI) [5] agent architecture and Ortony-Clore-Collins (OCC) model of emotions [35] into our methodology, grounding prompts in well-established AI and emotion theories.By merging theory-driven prompt engineering with LLMs' generative potential, we aim to develop social agent scenarios that are consistent with fundamental principles of agent behavior and emotion.We argue this prompting method has potential to substantially enhance the quality and authenticity of generated social agent scenarios while maintaining adherence to established principles in artificial intelligence and affective computing.</p>
<p>In this study, we merge the generative prowess of an LLM with the FAtiMA Toolkit [31], a framework tailored for developing social and emotional agents.We ground Prompt Engineering techniques in theory to extract all essential information (e.g., agent beliefs, desires, intentions, and emotional states) necessary for executing a brief social scenario within the FAtiMA Toolkit.The LLM's final output is a social scenario, symbolically represented in FAtiMA's formalism.We conducted an evaluation to understand how rich scenarios could be created from a small scenario description and whether the generated elements were coherent and correctly described an interactive social situations.Results show that the LLM grounded on theoretical concepts produces large amounts of accurate and new information that can add value to the scenario, by adding more characters and actions that make the scenario evolve in other directions.However, this process can also introduce inaccuracies to the generated output, rendering some actions unattainable.Consequently, we suggest that incorporating a human-in-the-loop approach into this type of generative process could significantly enhance the results and decrease the occurrence of inaccuracies.</p>
<p>RELATED WORK</p>
<p>The design rational behind SIAs represents decades of work across different fields such as Social Sciences, Cognitive Science and Human Computer Interaction [43].Theory-driven architectures are based on the premise that for creating realistic models for Intelligent Agents, we should look at how humans behave and try to better understand the reason behind our decisions.This line of thinking resulted in the rise of cognitive architectures [13,15,22,28] that intend to capture, at the computational level, intelligent behavior using the underlying mechanisms of human cognition.Yet, the theoretical basis required for authoring, makes the authoring process of agent modelling tools, particularly to users outside of the field, a strenuous task.For that reason, researchers have started to explore automated forms for creating SIAs and more recently leveraging LLMs for that task, on the premise that LLMs encapsulate knowledge on how humans behave.</p>
<p>Automated Scenario Authoring</p>
<p>Previous research has addressed the development of Agent Authoring Assistants with the aim of reducing the adoption barrier.Some of these works follow a rule-based approach where SIA behavior is described using natural language, which is then processed by a rule-parsing approach [14,20,25,40,49].The main limitation of assistants of this type is they can only successfully parse simple descriptions.With the promising results of LLMs in NLP-related tasks, data-driven assistants that use LLMs internally have also been created.An example is CHARET, character role-labelling approach to emotion tracking that accounts for the semantics of emotions [7].They use COMMET [4] to track the emotions of the characters in the stories, conditioned by the narrative events.Although this system does not consider other aspects of social behavior besides emotion, it showed the potential of LLMs in extracting SIA-related information from narratives.This underlines the need to develop data-drivem methods to gather social knowledge 2 and the knowledge encapsulated in LLMs could augment SIA frameworks.</p>
<p>LLMs for Agent Behavior Simulation</p>
<p>The demonstrated potential of LLMs has created new opportunities for the development of intelligent agents.In a recent study, Tsai et al. investigated the capabilities of LLMs, such as ChatGPT and GPT-4 [34], in playing text-based adventure games [48].The study shows ChatGPT competitive performance in comparison to existing systems.AI Dungeon is a further demonstration of GPT-3's [6] capabilities 3 in a text-based adventure game.In this game, players provide GPT-3 with natural language sentences that describe the game world and their desired actions.Using this input, GPT-3 generates a simulation of the game's events, characters, and actions.</p>
<p>Park et al. used LLMs to create populated prototypes for social computing systems, including multiplayer games [37].By generating simulated users that engage in realistic conversations and interactions, designers can better understand user behavior and refine their systems during the design phase.This approach is promising and is expected to have a significant impact, particularly in simulating the behavior of social agents in games.</p>
<p>LLMs have also been used to generate agent behavior based on goals and generate plans.Huang et al. investigated planning and reasoning tasks in embodied agents using large language models.By conditioning the models on an agent's current state, goal, and constraints, the researchers were able to generate text sequences that depict actions or steps for the agent to take.Essentially, they created an "inner monologue" that influences the agent's behavior.The agents also had the ability to break down an action into smaller actions [19].Another example AgentGPT 4 , a small project that allows the user to delegate a task to an agent.GPT-3 then plans and decomposes its actions in natural language.More recently, Park et al. used LLMs to populate a Sims-like interactive sandbox environment where end-users could interact with social agents that lived in that environment through natural language.Results show the agents were able to produce believable individual and emergent social behaviors, without the need to define the whole interaction scenario beforehand.These works showed the potential of LLMs in providing context-aware and goal-driven plans for agents.</p>
<p>In a nutshell, LLMs show potential in simulating socially intelligent agent behavior.Yet, previous works focus on and end-to-end approach, which may cause the same pitfalls as other data-heavy systems [1].We argue that to create SIAs one needs high control over content creation and target specific learning needs.Our argument is that one way to condition the output of an LLM is to explicitly encode knowledge within a knowledge framework or meta-model.This allows authors to have access to a process that it is transparent, interpretable, controlable and auditable [16].</p>
<p>Prompt Engineering</p>
<p>Prompt engineering (PE) has emerged as a new research field that focus on the development and usage of LLMs to tackle the challenge of extracting relevant, accurate, and concise responses from these models [26,44,53,56].For instance Shao et al. demonstrated that carefully crafted prompts significantly improve the model's adherence to desired output structures [45].</p>
<p>One key aspect of prompt engineering is finding an optimal balance between the specificity and generality of a prompt.Brown et al. found that longer prompts with more context and examples (i.e., few-shot learning) led to more accurate and relevant responses from GPT-3 in tasks such as question answering, summarization, and translation [6].However, the choice of prompt format, ordering and examples can drastically impact LLMs performance [55].Additionally, employing few-shot learning utilizes a larger portion of an LLM's input size, potentially restricting our ability to handle tasks that demand more extensive prompts or longer outputs.</p>
<p>Techniques for automating PE have also been explored.Shin et al. proposed the use of reinforcement learning to automatically generate optimal prompts that maximize model performance [46].This approach enables more efficient exploration of the vast prompt space while reducing human intervention in the process.However, these approaches of automatic prompt generation [23,24,27] require the existence of a dataset containing examples (e.g., of desired output), a condition not met in our case, as examples of social scenarios in FAtiMA Toolkit's are not readidly available.</p>
<p>The Chain-of-Thought (CoT) prompting method has emerged as a technique that greatly improves the reasoning capabilities of LLMs.Rather than generating the answer outright, this method employs prompts that guide the models through intermediate reasoning steps [50].This method has proven effective across various tasks, such as arithmetic, commonsense and symbolic reasoning, allowing LLMs to produce more accurate and contextually relevant responses [12].CoT prompting has two forms: a simple prompt like "Let's think step by step" [50], and a series of manual demonstrations with a question and reasoning chain leading to an answer [54].Zhang et al. demonstrates that the second paradigm, relying on handcrafted, task-specific demonstrations, yields superior performance compared to the first type.</p>
<p>This method of accessing knowledge in a LLM is comparable to the method of creating scenarios in different theory-driven social agent architectures [16].In agent-based experiences, the initial step typically involves creating the agents, including their beliefs, desires, and goals, which are tied to the scenario context.Along these lines, our approach employs a form of CoT prompting, aiming to bring out the emergent reasoning capabilities of LLMs.</p>
<p>CREATING SIAS' SCENARIOS WITH LLMS</p>
<p>In this work, we explore the creation of SIAs by integrating LLMs with theory-driven models in an attempt to provide a robust basis for crafting effective prompts, ensuring that the generated outputs align with established principles in artificial intelligence and affective computing.As highlighted above, there are numerous approaches to developing social behavior in agents, but they typically draw on fundamental concepts from social science, including beliefs, goals, actions, and emotions [30].We specifically employ theory-driven prompt engineering, leveraging the Belief-Desire-Intention (BDI) architecture for agents [5,11] and the Ortony, Clore, and Collins (OCC) model of emotions for emotionally appraising events [35].Our aim is to obtain a coherent symbolic representation compatible with the FAtiMA Toolkit.</p>
<p>Method</p>
<p>The process starts with a user providing a short (and possibly vague) description of a social scenario, ranging from one to five sentences.This input serves as a seed for generating the emotional social scenario.The system then follows a few-shot learning approach using the second paradigm for CoT (see Section 2.3).First, we describe to ChatGPT the task with a long prompt [6].Then manually create a sequence of prompts (identify agents, beliefs, goals, actions, emotions, and possible dialogues) that follow the tutorial for FAtiMA-toolkit 5 .Because language models often struggle with long-term memory [51], we divide the task of scenario generation into simpler and smaller tasks, where each task is associated with a prompt.The prompts and process for generating, extracting, and translating the SIA scenario information with ChatGPT are explained in more detail below 6 .For implementation, we accessed GPT-3.5 with the OpenAI's API to have more flexibility with how we interacted with the models (as opposed to using the browser interface7 ).</p>
<p>Extracting BDI Components</p>
<p>Task explanation: First, we convey the task to the language model by explaining that we want it to generate a game scenario with SIA agents who act based on their emotions.Then, we introduce the BDI architecture and its main components, beliefs, desires, and intentions, in relation to intelligent agents.</p>
<p>In early trials, we used longer, more in-depth explanations of the BDI architecture hoping for better-grounded responses from the model.However, due to the model's limited input capacity, we had to shorten the prompts.Fortunately, the GPT-3.5 model was trained on data that included information on the BDI architecture.We confirmed this by asking the model about the architecture, and it provided a correct response, suggesting that the knowledge is encoded within its weights.By incorporating BDI architecture and its key components into our prompts, we aim to direct the model towards generating responses that align with BDI agent design.Moreover, we use BDI terminology consistently in our prompts to keep the model's focus on the theory.</p>
<p>After defining the task, we provide a scenario description (e.g., "Tina baked a cake for her boyfriend because he likes cakes.") and begin the process of generating the social scenario information.See Figure 1 for an overview of all the steps in our CoT.</p>
<p>Agent Generation: To obtain the list of agents that will appear in the social scenario, we instruct the model to generate a list of all the agents' names based on the scenario description.This list is conditioned on the task explanation and the scenario description.The task and scenario descriptions are consistently added to the input's start to condition all further generation steps.</p>
<p>Knowledge Generation: Next, we extract the beliefs and desires of each agent, which are then used condition the generation of intentions at a later point.We generate beliefs and desires for each agent using one prompt 8 .For each agent , we instruct the model to list and translate the beliefs and desires of agent  into a symbolic representation compatible with the FAtiMA Toolkit.Beliefs and desires are respectively as represented as ‚Ñé  ,  * ) =  ,  (‚Ñé ‚Ñé,  * ) =  .The first argument for a belief/desire is the holder of that belief/desire, followed by additional arguments for a more detailed representation.Additionally, we direct the model to limit the arguments to letters, numbers, and underscores.As beliefs and desires in FA-tiMA serve as conditions, they must hold a value, either boolean or numerical.We repeat this prompting procedure for every agent, resulting in a populated knowledge base for each agent.</p>
<p>Intention Generation:</p>
<p>Although not all beliefs or desires result in intentions, all intentions must be influenced by beliefs or desires.As such, intentions are generated subsequently.To generate intentions, we append the previously generated agent's  knowledge base to the prompt and instruct the model to generate and translate the intentions of agent  that are motivated by its beliefs and intentions.Intentions are represented as
ùêº ùëÅùëá ùê∏ùëÅùëá (ùê∂‚Ñéùëéùëüùëéùëêùë°ùëíùëüùëä ùëñùë°‚Ñéùêºùëõùë°ùëíùëõùë°ùëñùëúùëõ, ùê¥ùëüùëîùë¢ùëöùëíùëõùë°ùë† * ) = ùëâ ùëéùëôùë¢ùëí,
where the first argument is the agent with the intention, followed by additional arguments for a more detailed representation.</p>
<p>Plan Generation: Once all intentions have been generated, we proceed to generate action plans that enable the attainment of those intentions.To do this, we append the knowledge base to the prompt and then iterate over all agents intentions to generate action plans.We instruct the model to generate an action plan for each agent  that has an intention .Action plans are represented as a chronological sequence of actions.Actions are represented as   (  ,   ,  * ), where   is the action's name,    is the performer,     is the target, and more arguments can be included if needed.At the end of this step, each agent has an action plan associated with each one of its intentions.FAtiMA Toolkit does not have a planning component, but this step helps generating the conditions and effects of each action.</p>
<p>Generating Actions' Conditions and Effects: Actions in FA-tiMA Toolkit have conditions and effects.An agent can execute an action only if the required conditions are met.After performing an action, an agent's beliefs and desires may alter, and new ones may arise.This is crucial since it is the precise assignment of conditions and effects that determines the availability or inaccessibility of actions for agents.This is why we create plans before specifying conditions and effects 9 .</p>
<p>To extract the conditions and effects, we ask the model to list the beliefs and desires required for the action (i.e., the conditions) and the changes or additions to the knowledge base (i.e., the effects) needed for agent  to perform action , conditioned on the action plan the action is a part of.</p>
<p>Emotional Appraisal</p>
<p>Emotional appraisal is a core feature of FAtiMA toolkit and other emotional agent architectures.Emotions in FAtiMA are based on the OCC theory of emotions [35].Two emotional labels are created: one that represents the emotion an agent feels after performing the action (OCC Emotion After) and another that represents the emotion an agent should be feeling to perform the action (OCC Emotion Before).The first label generated serves to attribute emotional appraisal variables to actions, enabling them to emotionally appraise events in FAtiMA.The second label can be added to the action conditions in FAtiMA, making the action available to the agent only if it has a specific emotional state.</p>
<p>Both labels are conditioned on the knowledge base, intentions, and social scenario.We direct the model to create an emotional label that denotes how an agent  felt before and after executing an action .To generate the emotional labels, we give the model all possible emotional labels and instruct it to generate an label for every action carried out by each agent, both before and after it's execution.The model may also choose not to produce an emotion.If this is the case, the agent appraisal variables will not be generated, or the conditions will not include a particular emotion, depending on which label was not generated.To facilitate emotional appraisal in FAtiMA, we convert all OCC emotions into suitable appraisal variables.The appraisal variables 10 used in the FAtiMA Toolkit are Desirability, Desirability for others, Praiseworthiness, Goal Success Probability, and Like.For each possible emotion of the OCC model, FAtiMA has default values for these appraisal variables, which were previously defined by an expert.We use these values to convert OCC emotions into appraisal variables.</p>
<p>Dialogue Generation</p>
<p>FAtiMA Toolkit also allows the authoring of dialogue.Instead of using dialogue trees, FAtiMA Toolkit views dialogue as a state machine, where one state could be interpreted as a turn that leads to another state (another turn).Each utterance is coded as a dialogue action in the following manner &lt; ,  , , ,    &gt;, where  is the current state of dialogue,   is the state the dialogue state machine will go to next,  and  are auxiliary tags that authors can use to better organize the dialogue state machine (e.g., style could be rude), and    is the string representing the utterance an agent can say.To generate dialogue with the LLM, we describe how dialogues are coded in FAtiMA Toolkit to the model and then ask it to generate the Dialogue State Machine in the previous format.</p>
<p>The dialogue state machine is separate from the agents, as the representation does not include who is the agent that can say the utterance.Instead, agents can access dialogues defined in the dialogue state machine by performing speak actions.Speak actions are special actions that follow the form:  (,  , , ). 10For definitions, go to: https://fatima-toolkit.eu/5-emotional-appraisal/If an agent has a speak action whose arguments match one or more dialogues from the dialogue state machine, those dialogues become available for the agent.To create speak actions, we add the dialogue state machine generated by the model to the input and iteratively ask the model what speak actions an agent  can do.To extract speak actions conditions and effects and OCC emotions, we follow the steps for extracting this information for normal actions.</p>
<p>EXPERIMENTAL SETUP</p>
<p>This study aimed to assess the benefits and drawbacks of implementing LLMs within SIAs.We do not claim our evaluation methodology as superior; it serves as an initial examination of the potential, advantages, and challenges associated with the nascent use of LLMs in constructing SIAs.</p>
<p>We evaluated our prompting method through both automatic and subjective evaluations.Human evaluation is essential to accurately assess the model's logic and capabilities.Automatic evaluations provide insight into the content generation volume of our method.</p>
<p>Methods</p>
<p>To conduct the evaluations we used the RocStories dataset [32] as the foundation for our short scenario descriptions.This dataset was selected for two main reasons: 1) it encompasses a wide variety of causal and temporal commonsense relations linked to everyday events, and 2) it offers a high-quality assortment of narratives suitable for story generation.As such, this dataset servers as a basis to evaluate the amount of commonsense and knowledge about human behavior the LLM has encoded in its weights.</p>
<p>Automatic Evaluation.We used gpt-3.5-turbomodel to create the scenarios.Despite the model's input size of 4096 tokens being suboptimal for generating comprehensive scenarios, it was the most advanced option accessible via the API at the time.We generated 43 unique scenarios, with each taking an average of 32.82 minutes to Table 1: Absolute and mean counts of artifacts per scenario generated by GPT-3.5 using our prompts.A total of 43 scenarios were generated.</p>
<p>complete.The model occasionally experienced overloads, necessitating a restart of the generation process.To mitigate this issue, we divided the code into steps and maintained a log to minimize time loss.The actual scenario generation time would likely be reduced if we the API was not overloaded so frequently.</p>
<p>Subjective Evaluation.We selected randomly from the sample 12 scenarios that were evaluated by 2 annotators, with a 16.7% overlap.By applying the Spearman's Rank Correlations (r(42) = .32,p = .042)we find a fair agreement [8].</p>
<p>Although there are more aspects to consider, we choose the following three metrics because we believe they are essential to access the quality of a generated scenario: relevance ensures that the generated content aligns with the scenario's goals, branching evaluates the choices available to the user, enhancing immersion and replayability, and logical errors assess the LLM's incoherences which can hinder scenario execution.</p>
<p>We devised a questionnaire 11 to assess the three metrics described above.All questions were annotated with a 5-point Likert scale, were 1 represented Strongly Disagree and 5 is Strongly Agree.</p>
<p>Results</p>
<p>Automatic Evaluation.The aim of this automated evaluation was to measure the complexity of the generated content by the model.</p>
<p>To automatically analyze the generated content, we introduced the concept of an artifact, encompassing beliefs, desires, conditions, and all other scenario elements.The core BDI components and emotional appraisal counts are presented in Table 1, while Table 2 displays the counts related to dialogue generation.In a recent study Guimar√£es et al. asked 10 participants with previous experience working with the FAtiMA-Toolkit to generate two scenarios based on two distinct stories from RocStories.Results indicated participants generated an average of 1.5 artifacts per minute 12 .As shown in Table 1, our model generates an average of 485.81 artifacts per scenario, translating to approximately 15.28 artifacts per minute given the average generation time for a scenario.This demonstrates the model's ability to produce more content in less time.</p>
<p>The dialogue state machines in the generated dialogues contain an average of 5.30 utterances, which is relatively limited (see Table 2).This suggests that improvements to our dialogue generation prompt are necessary.Additionally, the higher number of speak actions per scenario compared to dialogue lines indicates that multiple agents may use the same dialogue line, which may be undesirable.</p>
<p>Finally, we evaluated the feasibility of the action plans generated by the model for the agents.We identified the number of initial actions that could be performed immediately, i.e., those with conditions met by the agents' existing beliefs and desires, without requiring any changes.Emotions were not considered in this evaluation.In total, 955 actions were deemed immediately executable without updating the agents' knowledge bases.This highlights the model's ability to incorporate existing beliefs and desires into action conditions.However, a closer examination of the action plans' overall feasibility revealed that only 11 of the 369 generated intentions could be fully achieved (i.e., the action plans could be executed to completion).This discrepancy may stem from inaccuracies in the model's creation of conditions (e.g., adding extraneous conditions not present in the knowledge base) or in its generation of effects (e.g., the effects failing to properly update or introduce new beliefs and/or desires in the agents' knowledge base).</p>
<p>Subjective Evaluation.The automatic evaluation indicates the prompting approach generates a substantial amount of content, but there is no guarantee the content is relevant, coherent, or of high quality.The subjective evaluation is intended to assess these factors and provide answers accordingly.There are 3 salient topics in the analysis of the annotations: relevance of the created artefacts, ramification of the scenarios, and errors in the prompting sequence.</p>
<p>Regarding relevance, the model was able to correctly generate relevant agents (M=4.90,SD=0.316), beliefs/desires (M=3.60,SD=0.1994), and was less successfully at generating relevant intentions (M=3.00,SD=2.108).The model was clearly better at extracting agents.The task of generating correct and relevant beliefs, desires, and intentions requires the model to extract more implicit information which is an extra reasoning step that makes the generation harder.The model produced relevant actions (M=4.20,SD=0.632) in line with the given scenario.Additionally, the model was able to generate plausible emotions for each agent (M=3.50,SD=1.354).The model's ability to store human behavioural patterns and action planning in its weights without example prompts is apparent.Nevertheless, it struggled with generating conditions and effects (M=2.90,SD=2.025), which require to access beliefs and actions previously generated and dialogues (M=3.00,SD=1.704) that that into account goals and intentions.Lower performance on these steps is expected and requires additional effort.Most dialogues were too concise and broad, which is suboptimal.</p>
<p>To evaluate ramification, we asked annotators if the model added extra details they would not thought of but would still maintain to make the scenario richer, given that the RocStories scenarios were specific about the actions and events.The model frequently added more agents than the annotators would (M=4.10,SD=1.287).For example, when a hospital was mentioned the model took the liberty to add a doctor and nurses.More beliefs, desires (M=3.60,SD=1.147) and intentions (M=3.00,SD=1.944) were also added.Extra actions (M=3.00,SD=1.944), emotional labels (M=2.70,SD=1.252) and conditions/effects (M=2.90,SD=2.025) were added less frequently.When analyzing errors, the model's action plans were considered plausible, but the attributed conditions and effects were often incorrect.Additionally, we observed that errors later in the theoretical Chain of Thought (CoT) led to better quality scenarios.This is because errors at the beginning of the generation process propagate through the pipeline and affect subsequent steps.</p>
<p>DISCUSSION</p>
<p>SIAs are expected to demonstrate cognitive ability, social behavior, emotional expression, personality, and have mental representations in a variety of applications [33].Each behavior and action displayed by the agents must be consistent with their internal states and as such, writing scenarios can be complex and time-consuming.</p>
<p>LLMs seem very appellative to create SIAs automatically, but also have several limitations.They suffer from repetition [42], generate false information that can be irrelevant or incoherent with the context [42] and have trouble maintaining a consistent persona over long-term interactions [47,51].Furthermore, LLMs have low interpretability [52] and limited control, and are hard to interpret and debug [52].Even recent models (e.g., ) still suffer from these problems.These characteristics may make LLMs an unattractive technology to create rich social experiences in safetycritical systems (e.g., mental health and healthcare) or applications with tailored user experiences.</p>
<p>We have demonstrated, however, that despite their shortcomings, LLMs can rapidly generate vast quantities of content and possess significant amounts of commonsense knowledge acquired from extensive training data.The theory-driven approach proved to be useful in extracting agents and knowledge rooted on beliefs desires and intentions.We verified that the CoT prompting combined with the theoretical concepts forced the system to reason about the necessary ingredients (beliefs and desires) to generate intentions, actions and emotions.</p>
<p>Overall the experiments presented in the previous section produced mixed results.While the model was successful in extracting agents, generating simple beliefs and desires, and forming basic intentions, it struggled with more complex reasoning tasks, such as generating action plans and updating the agent's knowledge base.This is something that can easily fixed by forcing the LLM to update the knowledge base at every step 13 During the subjective analysis of the generated scenarios, we observed that the difficulty of the LLM's task varied depending on factors such as the way the scenarios were written (e.g.verbal tense) and the topic of the story being described.Some scenarios facilitated the task while others made it more challenging for the LLM.Furthermore, because LLMs possess an innate propensity for creativity, the symbolic scenarios 13 Using GPT-4, in the online interface, this was easily achievable because there is a much lesser strict limit for the input size (32.768tokens).The same is not true for GPT-3.5 using the API (4,096 tokens).featured a greater number of agents and available actions than those depicted in the text.While it is a plausible reflection of the social context, it is uncertain whether these additions enhance the scenario, thus accounting for the low level of agreement among annotators.Generation of dialogues comes as a big limitation of this work (as described in the automated analysis), because the prompting strategy is generating a few too general dialogue lines.</p>
<p>Bickmore and Cassell [2] highlighted that social dialogue is a joint task with multiple functions, including initiation, termination, turn-taking, and feedback.For successful dialogue, agents require a memory of prior interactions, goals, and the ability to intentionally guide the conversation towards a desired state.LLMs lack these abilities, creating a barrier to human-agent social interactions.Nonetheless, our approach allows the LLM to generate dialogue lines based on a target emotion (e.g., Neutral, Proud, or Supportive) and intention (e.g., encouragement or greeting) in accordance with the scenario (see Figure 2).</p>
<p>CONCLUSION</p>
<p>This paper proposes a theory-based prompting approach for SIAs development, combining the BDI architecture, the OCC theory of emotion for emotional appraisal, FAtiMA toolkit, and a LLMs' generative capabilities.This strategy accelerates social scenario generation, while also providing transparency, interpretability and control to the developer of a scenario by allowing its edition.However, using LLMs can introduce incoherences and errors that hurt the quality of the generated scenarios.We suggest integrating a human-in-the-loop at every stage of the generation process is crucial for error minimization and oversight.This approach combines human creativity and LLM generative capabilities, potentially yielding more engaging and robust social scenarios.</p>
<p>Serving as a proof-of-concept for the SIA field, this work illustrates how LLMs can speed up social scenario creation, despite the occasional inconsistencies introduced by these models.The potential to produce 10 times more artifacts than human users is a significant boost in generative capacity.However, the focus must remain on content quality.</p>
<p>In our subjective evaluation, we found that, although the models had some inconsistencies, they were reasonably proficient at content generation.We didn't evaluate the resulting social interaction quality from these scenarios, but future work will assess this aspect using typical interaction metrics such as engagement, rapport, user satisfaction, and user experience.</p>
<p>Figure 1 :
1
Figure 1: The pipeline for generating agent's beliefs, desires, intentions, plans of actions, action's conditions and affects and the emotions based on the OCC model of emotion with GPT-3.5.Each arrow represents a different prompt.</p>
<p>Figure 2 :
2
Figure 2: The pipeline for generating dialogue state machines compatible with FAtiMA Toolkit and the Speak Actions for each agent.Only agent with Speak Actions that refer the states of the dialogue state machine can say the respective utterances.</p>
<p>Table 2 :
2
Absolute and mean counts per scenario of artefacts related to dialogue generation.A total of 43 scenarios were generated.
Dialogue Lines Speak Actions Speak Action's Conditions Speak Action's EffectsAbsolute2284351508902Average5.3010.1235.0720.98
For a historical perspective, somewhat recently, social interaction started to be treated as both a linguistic and reasoning problem transforming the task into an end-to-end learning problem. Data-driven approaches (e.g., crowd-workers) have been pursued to reduce the need of large amounts manually created content for authoring social interactions[9]. The rise of the transformer's technology changed the research direction.
3 https://aidungeon.io
/ 4 https://agentgpt.reworkd.ai/
https://fatima-toolkit.eu/
The script and prompts used for this project are available in https://github.com/ana3A/SocialScenarioGPT.git
https://chat.openai.com/
FAtiMA Toolkit saves all beliefs and desires of agents in the Knowledge Base component, as such we choose to refer to beliefs and desires as knowledge although beliefs do not necessary express facts about the word and desires may be inconsistent with each other.
We verified that when plans were not generated, the system struggled to produce coherent conditions and effects.
Multiple different questions were asked for each step in the prompting pipeline: agents, beliefs, desires, intentions, plan generation, actions conditions and effects, dialogue and emotions
The study was conducted in the context of evaluating the latest version of the FAtiMA Toolkit
ACKNOWLEDGMENTSThis study received Portuguese national funds from FCT -Foundation for Science and Technology through the PhD grant 2021/06419/BD, and projects UIDB/50021/2020, UIDB/04326/2020, SLICE PTDC/CCI-COM/30787/2017, IDP/04326/2020, and LA/P/0101/2020.
Agents Beliefs Desires Intentions Action Plans Actions Conditions Effects Emotion Before Emotion After Total Absolute. 115 399 263 369 369 2756 5275 4383 2153 2106 20890</p>
<p>Automated storytelling via causal, commonsense plot ordering. Prithviraj Ammanabrolu, Wesley Cheung, William Broniec, Mark O Riedl, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Social dialongue with embodied conversational agents. Timothy Bickmore, Justine Cassell, Advances in natural multimodal dialogue systems. Springer2005</p>
<p>Virtual agents for professional social skills training: An overview of the state-of-the-art. Kim Bosman, Tibor Bosse, Daniel Formolo, International Conference on Intelligent Technologies for Interactive Entertainment. 2018</p>
<p>COMET: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, arXiv:1906.053172019. 2019arXiv preprint</p>
<p>Intention, plans, and practical reason. Michael Bratman, 1987. 1987</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Joana Diogo S Carvalho, Manuel Campos, Ana Guimar√£es, Jo√£o Antunes, Pedro A Dias, Santos, arXiv:2102.07537CHARET: Character-centered Approach to Emotion Tracking in Stories. 2021. 2021arXiv preprint</p>
<p>Biostatistics 104: correlational analysis. Yh Chan, Singapore Med J. 442003. 2003</p>
<p>Human-agent interaction model learning based on crowdsourcing. Jack-Antoine Charles, Caroline Pc Chanel, Corentin Chauffaut, Pascal Chauvin, Nicolas Drougard, Proceedings of the 6th International Conference on Human-Agent Interaction. the 6th International Conference on Human-Agent Interaction2018</p>
<p>How to make automated systems team players. Klaus Christoffersen, David D Woods, Advances in human performance and cognitive engineering research. 2002. 20022</p>
<p>Intention is choice with commitment. Hector J Philip R Cohen, Levesque, Artificial intelligence. 421990. 1990</p>
<p>Active Prompting with Chain-of-Thought for Large Language Models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, arXiv:2302.12246[cs.CL]2023</p>
<p>Feeling and reasoning: A computational model for emotional characters. Joao Dias, Ana Paiva, Portuguese conference on artificial intelligence. Springer2005</p>
<p>Teaching robots parametrized executable plans through spoken interaction. Guglielmo Gemignani, Emanuele Bastianelli, Daniele Nardi, Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems. the 2015 International Conference on Autonomous Agents and Multiagent Systems2015</p>
<p>A Domain-independent Framework for Modeling Emotion. J Gratch, S Marsella, Journal of Cognitive Systems Research. 542004. 2004</p>
<p>Manuel Guimar√£es, Joana Campos, Pedro A Santos, Jo√£o Dias, Rui Prada, arXiv:2206.03360Towards Explainable Social Agent Authoring tools: A case study on FAtiMA-Toolkit. 2022. 2022arXiv preprint</p>
<p>ISPO: A Serious Game to train the Interview Skills of Police Officers. Manuel Guimar√£es, Rui Prada, Pedro Santos, Jo√£o Dias, Cristina Soeiro, Raquel Guerra, Christina Steiner-Stanitznig, Andrea Molinari, 10.17083/ijsg.v9i4.514International Journal of Serious Games. 92022. 11 2022</p>
<p>All Together Now: Introducing the Virtual Human Toolkit. Arno Hartholt, David Traum, Stacy C Marsella, Ari Shapiro, Giota Stratou, Anton Leuski, Louis-Philippe Morency, Jonathan Gratch, 13th International Conference on Intelligent Virtual Agents. Edinburgh, UK2013</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, arXiv:2207.05608[cs.RO]Inner Monologue: Embodied Reasoning through Planning with Language Models. 2022</p>
<p>Sepehr Janghorbani, Ashutosh Modi, Jakob Buhmann, Mubbasir Kapadia, arXiv:1904.03266Domain Authoring Assistant for Intelligent Virtual Agents. 2019. 2019arXiv preprint</p>
<p>Face-to-face interaction with pedagogical agents, twenty years later. Johnson Lewis, James C Lester, International Journal of Artificial Intelligence in Education. 262016. 2016</p>
<p>The SOAR Cognitive Architecture. John E Laird, 2012The MIT Press</p>
<p>The Power of Scale for Parameter-Efficient Prompt Tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021. 2021</p>
<p>Prefix-Tuning: Optimizing Continuous Prompts for Generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021. 2021</p>
<p>Framer: Planning models from natural language action descriptions. Alan Lindsay, Jonathon Read, Joao F Ferreira, Thomas Hayton, Julie Porteous, Peter Gregory, Twenty-Seventh International Conference on Automated Planning and Scheduling. 2017</p>
<p>Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, 10.1145/3560815ACM Comput. Surv. 55195352023. jan 2023</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang, arXiv:2103.10385GPT Understands, Too. CoRR abs/2103. 2021. 202110385</p>
<p>EMA: a computational model of appraisal dynamics. S Marsella, J Gratch, European Meeting on Cybernetics and Systems Research. 2006</p>
<p>EMA: A process model of appraisal dynamics. C Stacy, Jonathan Marsella, Gratch, Cognitive Systems Research. 1012009. 2009</p>
<p>A virtual agent toolkit for serious games developers. Samuel Mascarenhas, Manuel Guimar√£es, Rui Prada, Jo√£o Dias, Pedro A Santos, Kam Star, Ben Hirsh, Ellis Spice, Rob Kommeren, 2018 IEEE Conference on Computational Intelligence and Games (CIG). IEEE2018</p>
<p>Samuel Mascarenhas, Manuel Guimar√£es, Pedro A Santos, Jo√£o Dias, Rui Prada, Ana Paiva, arXiv:2103.03020FAtiMA Toolkit-Toward an effective and accessible tool for the development of intelligent virtual agents and social robots. 2021. 2021arXiv preprint</p>
<p>A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen, 10.18653/v1/N16-1098Proceedings of the 2016 Conference of the North American Chapter. the 2016 Conference of the North American ChapterSan Diego, CaliforniaHuman Language Technologies. Association for Computational Linguistics2016</p>
<p>Collaborative Storytelling with Social Robots. Eric Nichols, Leo Gao, Yurii Vasylkiv, Randy Gomez, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Andrew Ortony, Gerald Clore, Allan Collins, 10.2307/2074241The Cognitive Structure of Emotion. 198818</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442[cs.HC]Generative Agents: Interactive Simulacra of Human Behavior. 2023</p>
<p>Sung Joon, Lindsay Park, Carrie J Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2208.04024[cs.HC]Social Simulacra: Creating Populated Prototypes for Social Computing Systems. 2022</p>
<p>Evaluating social attitudes of a virtual tutor. Florian Pecune, Angelo Cafaro, Magalie Ochs, Catherine Pelachaud, International Conference on Intelligent Virtual Agents. Springer2016</p>
<p>Non-Player Characters and Artificial Intelligence. Gon√ßalo Pereira, Ant√≥nio Brisson, Jo√£o Dias, Andr√© Carvalho, Joana Dimas, Samuel Mascarenhas, Joana Campos, Marco Vala, Iolanda Leite, Carlos Martinho, Psychology, Pedagogy, and Assessment in Serious Games. IGI Global2014</p>
<p>Task Based Dialog for Service Mobile Robot. Vittorio Perera, Manuela Veloso, 2014 AAAI Fall Symposium Series. 2014</p>
<p>Gamygdala: An emotion engine for games. Alexandru Popescu, Joost Broekens, Maarten Van Someren, IEEE Transactions on Affective Computing. 52014. 2014</p>
<p>Towards content transfer through grounded text generation. Shrimai Prabhumoye, Chris Quirk, Michel Galley, arXiv:1905.052932019. 2019arXiv preprint</p>
<p>Embodied conversational agents for patients with dementia: thematic literature analysis. Margherita Rampioni, Vera Stara, Elisa Felici, Lorena Rossi, Susy Paolini, JMIR mHealth and uHealth. 9e253812021. 2021</p>
<p>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. Laria Reynolds, Kyle Mcdonell, 10.1145/3411763.3451760Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Yokohama, Japan; New York, NY, USA, ArticleAssociation for Computing Machinery2021314CHI EA '21)</p>
<p>Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2302.00618[cs.CL]Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models. 2023</p>
<p>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, 10.18653/v1/2020.emnlp-main.346Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online2020</p>
<p>Personalized Dialogue Response Generation Learned from Monologues. Feng-Guang Su, Aliyah R Hsu, Yi-Lin Tuan, Hung-Yi Lee, INTERSPEECH. 4160-41642019</p>
<p>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu, Hongyuan Mei, arXiv:2304.028682023. 2023arXiv preprint</p>
<p>Towards a conversational interface for authoring intelligent virtual characters. Xinyi Wang, Mubbasir Samuel S Sohn, Kapadia, Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents. the 19th ACM International Conference on Intelligent Virtual Agents2019</p>
<p>Chain of Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022. 2022</p>
<p>Beyond goldfish memory: Longterm open-domain conversation. Jing Xu, Arthur Szlam, Jason Weston, arXiv:2107.075672021. 2021arXiv preprint</p>
<p>Jian Yang, Gang Xiao, Yulong Shen, Wei Jiang, Xinyu Hu, Ying Zhang, Jinghui Peng, arXiv:2110.00269A Survey of Knowledge Enhanced Pre-trained Models. 2021. 2021arXiv preprint</p>
<p>Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, Minjoon Seo ; Zhuosheng, Aston Zhang, Mu Zhang, Alex Li, Smola, arXiv:2302.14691[cs.CL][54arXiv:2210.03493 [cs.CL]Automatic Chain of Thought Prompting in Large Language Models. 2023. 2022Context Instruction Learning</p>
<p>Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, arXiv:2102.09690[cs.CL]Calibrate Before Use: Improving Few-Shot Performance of Language Models. 2021</p>
<p>Controllable Generation from Pre-trained Language Models via Inverse Prompting. Xu Zou, Da Yin, Qingyang Zhong, Hongxia Yang, Zhilin Yang, Jie Tang, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining2021. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
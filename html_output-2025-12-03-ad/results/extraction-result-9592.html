<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9592 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9592</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9592</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-272524329</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.05314v3.pdf" target="_blank">Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in telecommunications remain limited, often relying on general-purpose models that lack domain-specific specialization. This lack of specialization results in underperformance, particularly when dealing with telecommunications-specific technical terminology and their associated mathematical representations. This paper addresses this gap by first creating and disseminating Tele-Data, a comprehensive dataset of telecommunications material curated from relevant sources, and Tele-Eval, a large-scale question-and-answer dataset tailored to the domain. Through extensive experiments, we explore the most effective training techniques for adapting LLMs to the telecommunications domain, ranging from examining the division of expertise across various telecommunications aspects to employing parameter-efficient techniques. We also investigate how models of different sizes behave during adaptation and analyze the impact of their training data on this behavior. Leveraging these findings, we develop and open-source Tele-LLMs, the first series of language models ranging from 1B to 8B parameters, specifically tailored for telecommunications. Our evaluations demonstrate that these models outperform their general-purpose counterparts on Tele-Eval and telecommunications-related literature tasks while retaining their previously acquired capabilities, thus avoiding the catastrophic forgetting phenomenon.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9592.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9592.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eq-Std (Tele-Data cleaning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Equation Standardization and Cleaning Pipeline (Tele-Data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing pipeline that standardizes heterogeneous equation representations across arXiv (LaTeX), 3GPP .doc/.docx (XML), Wikipedia and web text into unified LaTeX inline equations and removes non-inline content to enable LLM training on mathematical content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>docx2tex + demacro + regex pipeline (non-LLM tools)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deterministic preprocessing stack: convert .doc/.docx to LaTeX with docx2tex; flatten multi-file LaTeX with latexpand; replace custom macros using de-macro tools; remove figures/tables and noninformative LaTeX commands via regexes to produce inline LaTeX equations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Telecommunications (scholarly documents, standards, wiki, web pages)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>mathematical formula / equation representations (preprocessing for downstream extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Automated document conversion and cleaning (docx2tex, latexpand, demacro, regex-based removal of non-inline content) to produce uniform LaTeX equations across heterogeneous source types before LLM continual pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Tele-Data: curated corpus including arXiv (pre-filtered from ~610k papers), 3GPP standards (~2.8k docs converted from .doc/.docx), Wikipedia (19.5k telecom-relevant articles after keyword+LLM filter), and Common Crawl-derived web pages (~740k links); cleaning removes figures/tables and standardizes equations into LaTeX.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Cross-entropy comparison using GPT-2 on 1024-token chunks (compare raw vs cleaned distributions, focus on 90th percentile tokens to assess 'cleanliness'), qualitative checks and downstream model training performance.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cleaning produced lower mean and SD cross-entropy in the top 10% hardest tokens vs raw arXiv; equations standardized into LaTeX enabling models to output runnable LaTeX in later stages (e.g., examples where Tele-LLMs produced full LaTeX equations).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Conversion of 3GPP .doc files (XML equations) to LaTeX; Appendix example showing Gemma-2B-Tele producing LaTeX-formatted stopping condition for Wirtinger gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Conversion and macro-demacro rules are heuristics and may miss edge-case custom macros; removing figures/tables discards contextual diagrams that can be important for interpreting equations; standardization does not by itself guarantee semantic correctness of extracted equations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to raw ingestion of mixed formats, standardized LaTeX improved downstream cross-entropy proxy for cleanliness and facilitated equation-aware model fine-tuning (no external baseline specific to equation-standardization experiments reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9592.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9592.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-LaTeX-Gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Generation and Reproduction of LaTeX Equations and Mathematical Answers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using domain-adapted LLMs (Tele-LLMs) to produce explicit mathematical relationships and runnable LaTeX equations from telecom literature, demonstrating the ability to reproduce or synthesize equations in answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2B-Tele, LLaMA-3-8B-Tele, Mixtral 8x-7B-Instruct (used for generation/filtering), Phi-1.5-Tele (instruction-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Series of adapted LLMs (1B–8B) produced by full-parameter continual pretraining on Tele-Data; Mixtral 8x-7B-Instruct used for data filtering and QnA generation. Example sizes: Gemma-2B (~2B params), LLaMA-3-8B (~8B params), Mixtral 8x-7B (~7B instruct-tuned).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Telecommunications (equation-heavy scholarly and standards texts)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>mathematical formulas and algorithmic stopping conditions (runnable LaTeX expressions, e.g., gradient conditions)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Continual pretraining (full fine-tuning, FFT) on cleaned Tele-Data containing standardized LaTeX; LLMs were prompted to generate question-answer pairs and to produce answers in natural language or LaTeX where appropriate. Examples and filtering ensured questions were non-local.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Tele-Data (cleaned, LaTeX-standardized) used for continual pretraining across 2 epochs (full Tele-Data plus 5% SlimPajama general data to reduce forgetting). Tele-Eval (750k LLM-generated open-ended QnA pairs linked to source IDs) used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative examples (Appendix D) showing LLMs outputting runnable LaTeX equations; quantitative improvements measured on GSM-8K (math benchmark) and Tele-Eval metrics (LLM-Eval, SemScore, Ans-PPL).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Adapted Tele-LLMs produced explicit LaTeX equations in answers (Appendix example: Wirtinger gradient descent stopping condition), and demonstrated improved mathematical reasoning — e.g., LLaMA-3-8B-Tele achieved ~10% higher accuracy on GSM-8K compared to its base counterpart; overall Tele-Eval LLM-Eval average relative improvement ~25%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Appendix Example 3: Gemma-2B-Tele answered stopping condition for Wirtinger gradient descent with full LaTeX equations (partial derivatives and second-derivative inequality).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Models can be biased by formatting preferences; open-ended generation risks hallucinating incorrect equations; producing syntactically correct LaTeX does not guarantee mathematical correctness; evaluation of equation correctness is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to base models (Gemma-2B, LLaMA variants) and instruct-tuned counterparts, Tele-LLMs outperform on telecom QnA and in some math benchmarks; instruct-tuned base models sometimes outperform base Tele models unless instruction fine-tuning is applied to Tele models (Tele-Alpaca).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9592.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9592.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eq-Eval Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Framework for Equation-Containing Answers (Ans-PPL / SemScore / LLM-Eval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-pronged evaluation approach to judge open-ended telecom QnA that often include equations: answer perplexity conditioned on ground truth (Ans-PPL), semantic similarity via sentence transformers (SemScore), and an LLM-based judge (LLM-Eval) using Mixtral.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (cleanliness cross-entropy), all-mpnet-base SentenceTransformer (SemScore), Mixtral 8x-7B-Instruct (LLM-Eval judge)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 used as a proxy language model for cross-entropy cleanliness scoring; all-mpnet-base (sentence-transformer BERT family) used to embed answers for cosine similarity; Mixtral 8x-7B-Instruct used to judge correctness (Yes/No) between provided and ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Telecommunications QA and equation-containing text</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>mathematical formulas and correctness/semantic equivalence of equations</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Ans-PPL: compute perplexity of model conditioned on ground-truth answer; SemScore: cosine similarity between embeddings of ground-truth and prediction; LLM-Eval: prompt an instruct LLM to compare predicted answer to ground truth and return binary correct/incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Evaluation performed on Tele-Eval (750k QnA pairs) and standard benchmarks (MMLU, HellaSWAG, GSM-8K). Cleanliness assessed on 500 raw vs cleaned arXiv papers via GPT-2 cross-entropy on 1024-token chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use metrics across datasets; compare within-family (Ans-PPL, SemScore) and across families (LLM-Eval); LLM-Eval defined as percent of answers judged correct by Mixtral 8x-7B-Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM-Eval found to be the most robust for telecom domain (average ~25% relative improvement for Tele-LLMs). Ans-PPL and SemScore useful within model families but biased by response format; LLM-Eval more concept-focused but computationally expensive (~50 ms per question on Nvidia A6000 with Mixtral).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Empirical observation: Ans-PPL shows small relative differences for larger models and is not comparable across families; SemScore can reward coherent but incorrect answers; LLM-Eval better captures conceptual correctness including equations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>ROUGE/BLEU are inadequate for equation evaluation; Ans-PPL and SemScore are sensitive to answer format and model family biases; LLM-Eval has high runtime cost and its correctness depends on the judge model's capabilities and biases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No human-judged baseline at Tele-Eval scale; LLM-Eval validated against prior work showing LLM judges can match human preferences, but direct human expert evaluation not reported due to dataset scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9592.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9592.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEFT-vs-FFT-Equations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations of Parameter-Efficient Fine-Tuning (LoRA) for Injecting Equation-Heavy Knowledge versus Full Fine-Tuning (FFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that PEFT (LoRA) saturates and is insufficient to inject rich telecom/equation knowledge into some LLMs, whereas full-parameter fine-tuning across two epochs was required to achieve substantive domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoRA (r=64, alpha=32) applied to Gemma-2B and LLaMA-3-8B vs Full Fine-Tuning (FFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LoRA: low-rank adapters added to weights (rank r=64, alpha=32, dropout 0.1) used as PEFT; FFT: updating all model parameters with AdamW, cosine LR schedule, mixed precision and large token batch sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Telecommunications domain knowledge injection (including equations/formulaic content)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>empirical mathematical/technical relationships encoded in LLM parameters</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compare LoRA PEFT to full fine-tuning by training Gemma-2B and LLaMA-3-8B on Tele-Data and monitor training loss, gradient norms, and downstream Tele-Eval performance.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>20% of Tele-Data used in initial experiments (with 10% of that as validation) for comparative PEFT vs FFT experiments; sequence lengths aligned to model pretraining (8192 tokens used in some experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Track training/validation loss curves, gradient norms, and downstream Tele-Eval performance metrics (LLM-Eval, SemScore, Ans-PPL).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LoRA could inject some telecom knowledge in smaller models but saturated quickly; for LLaMA-3-8B LoRA gradient norms remained extremely low and training loss barely changed, preventing substantive adaptation. FFT across two epochs was required to obtain strong telecom performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Fig.4 demonstrates LoRA saturation for Gemma-2B and near-zero gradient updates for LLaMA-3-8B; decision to use FFT for building Tele-LLMs series.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>PEFT methods may fail when the base model already encodes competing distributions or requires broader parameter shifts to encode domain-specific mathematical structures; FFT is compute-expensive (Tele-LLMs training consumed ~5,000 GPU hours on 8 A6000 GPUs).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LoRA (PEFT) used as baseline PEFT technique; FFT outperformed LoRA for domain adaptation in telecom both in training dynamics and downstream Tele-Eval results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TeleQnA: A benchmark dataset to assess large language models telecommunications knowledge <em>(Rating: 2)</em></li>
                <li>Adapting foundation models for information synthesis of wireless communication specifications <em>(Rating: 2)</em></li>
                <li>Understanding Telecom Language Through Large Language Models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Textbooks are all you need <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9592",
    "paper_id": "paper-272524329",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [
        {
            "name_short": "Eq-Std (Tele-Data cleaning)",
            "name_full": "Equation Standardization and Cleaning Pipeline (Tele-Data)",
            "brief_description": "A preprocessing pipeline that standardizes heterogeneous equation representations across arXiv (LaTeX), 3GPP .doc/.docx (XML), Wikipedia and web text into unified LaTeX inline equations and removes non-inline content to enable LLM training on mathematical content.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "docx2tex + demacro + regex pipeline (non-LLM tools)",
            "model_description": "A deterministic preprocessing stack: convert .doc/.docx to LaTeX with docx2tex; flatten multi-file LaTeX with latexpand; replace custom macros using de-macro tools; remove figures/tables and noninformative LaTeX commands via regexes to produce inline LaTeX equations.",
            "scientific_domain": "Telecommunications (scholarly documents, standards, wiki, web pages)",
            "law_type": "mathematical formula / equation representations (preprocessing for downstream extraction)",
            "method_description": "Automated document conversion and cleaning (docx2tex, latexpand, demacro, regex-based removal of non-inline content) to produce uniform LaTeX equations across heterogeneous source types before LLM continual pretraining.",
            "input_corpus_description": "Tele-Data: curated corpus including arXiv (pre-filtered from ~610k papers), 3GPP standards (~2.8k docs converted from .doc/.docx), Wikipedia (19.5k telecom-relevant articles after keyword+LLM filter), and Common Crawl-derived web pages (~740k links); cleaning removes figures/tables and standardizes equations into LaTeX.",
            "evaluation_method": "Cross-entropy comparison using GPT-2 on 1024-token chunks (compare raw vs cleaned distributions, focus on 90th percentile tokens to assess 'cleanliness'), qualitative checks and downstream model training performance.",
            "results_summary": "Cleaning produced lower mean and SD cross-entropy in the top 10% hardest tokens vs raw arXiv; equations standardized into LaTeX enabling models to output runnable LaTeX in later stages (e.g., examples where Tele-LLMs produced full LaTeX equations).",
            "notable_examples": "Conversion of 3GPP .doc files (XML equations) to LaTeX; Appendix example showing Gemma-2B-Tele producing LaTeX-formatted stopping condition for Wirtinger gradient descent.",
            "limitations_challenges": "Conversion and macro-demacro rules are heuristics and may miss edge-case custom macros; removing figures/tables discards contextual diagrams that can be important for interpreting equations; standardization does not by itself guarantee semantic correctness of extracted equations.",
            "baseline_comparison": "Compared to raw ingestion of mixed formats, standardized LaTeX improved downstream cross-entropy proxy for cleanliness and facilitated equation-aware model fine-tuning (no external baseline specific to equation-standardization experiments reported).",
            "uuid": "e9592.0",
            "source_info": {
                "paper_title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-LaTeX-Gen",
            "name_full": "LLM-based Generation and Reproduction of LaTeX Equations and Mathematical Answers",
            "brief_description": "Using domain-adapted LLMs (Tele-LLMs) to produce explicit mathematical relationships and runnable LaTeX equations from telecom literature, demonstrating the ability to reproduce or synthesize equations in answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2B-Tele, LLaMA-3-8B-Tele, Mixtral 8x-7B-Instruct (used for generation/filtering), Phi-1.5-Tele (instruction-tuned variant)",
            "model_description": "Series of adapted LLMs (1B–8B) produced by full-parameter continual pretraining on Tele-Data; Mixtral 8x-7B-Instruct used for data filtering and QnA generation. Example sizes: Gemma-2B (~2B params), LLaMA-3-8B (~8B params), Mixtral 8x-7B (~7B instruct-tuned).",
            "scientific_domain": "Telecommunications (equation-heavy scholarly and standards texts)",
            "law_type": "mathematical formulas and algorithmic stopping conditions (runnable LaTeX expressions, e.g., gradient conditions)",
            "method_description": "Continual pretraining (full fine-tuning, FFT) on cleaned Tele-Data containing standardized LaTeX; LLMs were prompted to generate question-answer pairs and to produce answers in natural language or LaTeX where appropriate. Examples and filtering ensured questions were non-local.",
            "input_corpus_description": "Tele-Data (cleaned, LaTeX-standardized) used for continual pretraining across 2 epochs (full Tele-Data plus 5% SlimPajama general data to reduce forgetting). Tele-Eval (750k LLM-generated open-ended QnA pairs linked to source IDs) used for evaluation.",
            "evaluation_method": "Qualitative examples (Appendix D) showing LLMs outputting runnable LaTeX equations; quantitative improvements measured on GSM-8K (math benchmark) and Tele-Eval metrics (LLM-Eval, SemScore, Ans-PPL).",
            "results_summary": "Adapted Tele-LLMs produced explicit LaTeX equations in answers (Appendix example: Wirtinger gradient descent stopping condition), and demonstrated improved mathematical reasoning — e.g., LLaMA-3-8B-Tele achieved ~10% higher accuracy on GSM-8K compared to its base counterpart; overall Tele-Eval LLM-Eval average relative improvement ~25%.",
            "notable_examples": "Appendix Example 3: Gemma-2B-Tele answered stopping condition for Wirtinger gradient descent with full LaTeX equations (partial derivatives and second-derivative inequality).",
            "limitations_challenges": "Models can be biased by formatting preferences; open-ended generation risks hallucinating incorrect equations; producing syntactically correct LaTeX does not guarantee mathematical correctness; evaluation of equation correctness is nontrivial.",
            "baseline_comparison": "Compared to base models (Gemma-2B, LLaMA variants) and instruct-tuned counterparts, Tele-LLMs outperform on telecom QnA and in some math benchmarks; instruct-tuned base models sometimes outperform base Tele models unless instruction fine-tuning is applied to Tele models (Tele-Alpaca).",
            "uuid": "e9592.1",
            "source_info": {
                "paper_title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Eq-Eval Metrics",
            "name_full": "Evaluation Framework for Equation-Containing Answers (Ans-PPL / SemScore / LLM-Eval)",
            "brief_description": "A three-pronged evaluation approach to judge open-ended telecom QnA that often include equations: answer perplexity conditioned on ground truth (Ans-PPL), semantic similarity via sentence transformers (SemScore), and an LLM-based judge (LLM-Eval) using Mixtral.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (cleanliness cross-entropy), all-mpnet-base SentenceTransformer (SemScore), Mixtral 8x-7B-Instruct (LLM-Eval judge)",
            "model_description": "GPT-2 used as a proxy language model for cross-entropy cleanliness scoring; all-mpnet-base (sentence-transformer BERT family) used to embed answers for cosine similarity; Mixtral 8x-7B-Instruct used to judge correctness (Yes/No) between provided and ground-truth answers.",
            "scientific_domain": "Telecommunications QA and equation-containing text",
            "law_type": "mathematical formulas and correctness/semantic equivalence of equations",
            "method_description": "Ans-PPL: compute perplexity of model conditioned on ground-truth answer; SemScore: cosine similarity between embeddings of ground-truth and prediction; LLM-Eval: prompt an instruct LLM to compare predicted answer to ground truth and return binary correct/incorrect.",
            "input_corpus_description": "Evaluation performed on Tele-Eval (750k QnA pairs) and standard benchmarks (MMLU, HellaSWAG, GSM-8K). Cleanliness assessed on 500 raw vs cleaned arXiv papers via GPT-2 cross-entropy on 1024-token chunks.",
            "evaluation_method": "Use metrics across datasets; compare within-family (Ans-PPL, SemScore) and across families (LLM-Eval); LLM-Eval defined as percent of answers judged correct by Mixtral 8x-7B-Instruct.",
            "results_summary": "LLM-Eval found to be the most robust for telecom domain (average ~25% relative improvement for Tele-LLMs). Ans-PPL and SemScore useful within model families but biased by response format; LLM-Eval more concept-focused but computationally expensive (~50 ms per question on Nvidia A6000 with Mixtral).",
            "notable_examples": "Empirical observation: Ans-PPL shows small relative differences for larger models and is not comparable across families; SemScore can reward coherent but incorrect answers; LLM-Eval better captures conceptual correctness including equations.",
            "limitations_challenges": "ROUGE/BLEU are inadequate for equation evaluation; Ans-PPL and SemScore are sensitive to answer format and model family biases; LLM-Eval has high runtime cost and its correctness depends on the judge model's capabilities and biases.",
            "baseline_comparison": "No human-judged baseline at Tele-Eval scale; LLM-Eval validated against prior work showing LLM judges can match human preferences, but direct human expert evaluation not reported due to dataset scale.",
            "uuid": "e9592.2",
            "source_info": {
                "paper_title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PEFT-vs-FFT-Equations",
            "name_full": "Limitations of Parameter-Efficient Fine-Tuning (LoRA) for Injecting Equation-Heavy Knowledge versus Full Fine-Tuning (FFT)",
            "brief_description": "Empirical finding that PEFT (LoRA) saturates and is insufficient to inject rich telecom/equation knowledge into some LLMs, whereas full-parameter fine-tuning across two epochs was required to achieve substantive domain adaptation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LoRA (r=64, alpha=32) applied to Gemma-2B and LLaMA-3-8B vs Full Fine-Tuning (FFT)",
            "model_description": "LoRA: low-rank adapters added to weights (rank r=64, alpha=32, dropout 0.1) used as PEFT; FFT: updating all model parameters with AdamW, cosine LR schedule, mixed precision and large token batch sizes.",
            "scientific_domain": "Telecommunications domain knowledge injection (including equations/formulaic content)",
            "law_type": "empirical mathematical/technical relationships encoded in LLM parameters",
            "method_description": "Compare LoRA PEFT to full fine-tuning by training Gemma-2B and LLaMA-3-8B on Tele-Data and monitor training loss, gradient norms, and downstream Tele-Eval performance.",
            "input_corpus_description": "20% of Tele-Data used in initial experiments (with 10% of that as validation) for comparative PEFT vs FFT experiments; sequence lengths aligned to model pretraining (8192 tokens used in some experiments).",
            "evaluation_method": "Track training/validation loss curves, gradient norms, and downstream Tele-Eval performance metrics (LLM-Eval, SemScore, Ans-PPL).",
            "results_summary": "LoRA could inject some telecom knowledge in smaller models but saturated quickly; for LLaMA-3-8B LoRA gradient norms remained extremely low and training loss barely changed, preventing substantive adaptation. FFT across two epochs was required to obtain strong telecom performance.",
            "notable_examples": "Fig.4 demonstrates LoRA saturation for Gemma-2B and near-zero gradient updates for LLaMA-3-8B; decision to use FFT for building Tele-LLMs series.",
            "limitations_challenges": "PEFT methods may fail when the base model already encodes competing distributions or requires broader parameter shifts to encode domain-specific mathematical structures; FFT is compute-expensive (Tele-LLMs training consumed ~5,000 GPU hours on 8 A6000 GPUs).",
            "baseline_comparison": "LoRA (PEFT) used as baseline PEFT technique; FFT outperformed LoRA for domain adaptation in telecom both in training dynamics and downstream Tele-Eval results.",
            "uuid": "e9592.3",
            "source_info": {
                "paper_title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TeleQnA: A benchmark dataset to assess large language models telecommunications knowledge",
            "rating": 2,
            "sanitized_title": "teleqna_a_benchmark_dataset_to_assess_large_language_models_telecommunications_knowledge"
        },
        {
            "paper_title": "Adapting foundation models for information synthesis of wireless communication specifications",
            "rating": 2,
            "sanitized_title": "adapting_foundation_models_for_information_synthesis_of_wireless_communication_specifications"
        },
        {
            "paper_title": "Understanding Telecom Language Through Large Language Models",
            "rating": 2,
            "sanitized_title": "understanding_telecom_language_through_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Textbooks are all you need",
            "rating": 1,
            "sanitized_title": "textbooks_are_all_you_need"
        }
    ],
    "cost": 0.014388999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications
5 May 2025</p>
<p>Ali Maatouk 
Yale University
06511New HavenCTUSA</p>
<p>Kenny Chirino Ampudia 
Yale University
06511New HavenCTUSA</p>
<p>Amazon Web Services
10001New YorkNYUSA</p>
<p>Leandros Tassiulas 
Yale University
06511New HavenCTUSA</p>
<p>Pretrained LLMs Tele-Data Training
Tinyllama-1.1 Phi-1.5</p>
<p>Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications
5 May 20258A8CE4DEDE515B5149C95107A6904F40arXiv:2409.05314v3[cs.IT]Filteringprocessingcleaning 2 Tele-Eval Generation QnA generation and filtering Filter Generate Chunks Models Citation Link Prediction Citation Recommendation Title Generation Abstract Completion Citation Sentence Generation
The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance.However, despite their rapid proliferation, the applications of LLMs in telecommunications remain limited, often relying on general-purpose models that lack domain-specific specialization.This lack of specialization results in underperformance, particularly when dealing with telecommunications-specific technical terminology and their associated mathematical representations.This paper addresses this gap by first creating and disseminating Tele-Data, a comprehensive dataset of telecommunications material curated from relevant sources, and Tele-Eval, a large-scale questionand-answer dataset tailored to the domain.Through extensive experiments, we explore the most effective training techniques for adapting LLMs to the telecommunications domain, ranging from examining the division of expertise across various telecommunications aspects to employing parameter-efficient techniques.We also investigate how models of different sizes behave during adaptation and analyze the impact of their training data on this behavior.Leveraging these findings, we develop and opensource Tele-LLMs 1 , the first series of language models ranging from 1B to 8B parameters, specifically tailored for telecommunications.Our evaluations demonstrate that these models outperform their general-purpose counterparts on Tele-Eval and telecommunications-related literature tasks while retaining their previously acquired capabilities, thus avoiding the catastrophic forgetting phenomenon.</p>
<p>I. INTRODUCTION</p>
<p>Large language models (LLMs) have recently marked a major breakthrough in natural language processing.Since 2022, these deep learning systems have proliferated rapidly, with numerous models released by tech giants, research institutions, and open-source communities [1], [2], [3].Trained on vast text corpora, LLMs have demonstrated unprecedented capabilities in understanding context, generating human-like text, and performing reasoning tasks across diverse domains [4].These abilities have sparked interest from researchers and industry professionals to adopt and explore their potential applications across a wide variety of fields.</p>
<p>Researchers in the telecommunications and networking domain are no exception, aiming to leverage LLMs' potential across various tasks within the field.These applications include chatbots for engineers [5], network document analysis [6], network modeling and development [7], and wireless systems design [8], [9].The list of tasks being explored for potential LLM support continues to grow [10], with new research emerging rapidly to assess the limits and utility of these models in the telecommunications context.</p>
<p>Thus far, applications of LLMs in the telecommunications domain have primarily involved prompting [8], [7], in-context learning [5], and task-specific fine-tuning [11], [6], utilizing either proprietary LLMs like OpenAI's GPT [1] or local opensource generic LLMs like Meta's LLaMA series [3].However, proprietary LLMs raise concerns as they may require sharing prompts and related data with the LLM owners.Additionally, they offer limited maneuverability and adaptability since users do not have access to the model weights.Furthermore, despite their adaptability, general-purpose open-source LLMs lack specialization in telecommunications, as they encompass knowledge from diverse domains such as medicine, history, and law.This lack of specialization hinders their performance; in fact, it is well established that LLM applications in specific domains, from prompting to task-specific fine-tuning, perform better when using domain-specific LLMs rather than general ones [12].</p>
<p>Given the above, in a multitude of domains, efforts have been made to create specialized open-source LLMs to push the performance limits of LLMs applications in those fields, such as medicine [13], law [14], and finance [15].However, to date, there are no domain-specific open-source models in the telecommunications literature.This stems from the particularities of the telecommunications domain, which includes heterogeneous material such as standards and scholarly work, a heavy reliance on complex equations, varying formatting in datasets, and the generally closed-source nature of work done by companies and institutions thus far with LLMs [5], [16], [17].Given the anticipated role of LLMs in telecommunications, this represents a significant gap in the field.Proposed Work.This work aims to address the aforementioned gap by introducing the first open-source series of LLMs specialized for the telecommunications field.Our contribution extends beyond open-sourcing the models; we open-source every step of the framework required for this specialization.We also provide, through extensive experiments, key insights into the process of adapting LLMs to the telecommunications domain.This includes the techniques to be used and the different behaviors exhibited by various LLMs during adaptation, based on their sizes and overall tendencies.</p>
<p>First, we curate a comprehensive set of telecommunications material, referred to as Tele-Data, by leveraging an LLMbased filtering of arXiv papers, standards, Wikipedia articles, and web content to identify relevant sources.We report the precision and recall of our filtering method based on humanlabeled data and highlight its high recall rate.The collected material is then subjected to extensive cleaning using regular expressions and LLM-based filtering techniques tailored to the particularities of the telecommunications domain.Specifically, our process addresses formatting differences in standards documents and unifies the equation materials across all types of documents into LaTeX formatting.To quantify the cleanliness of the resulting material, we propose a cross-entropy-based approach and demonstrate that the material is significantly cleaner after this process.Second, leveraging Tele-Data, we use an LLM-based framework to create an evaluation dataset consisting of 750k question-and-answer (QnA) pairs, referred to as Tele-Eval.This dataset represents the first large-scale open-ended QnA dataset in the telecommunications field.It also includes references to the specific material from which each question was generated, allowing users to select questions from the material they are interested in and facilitating retrieval-augmented generation frameworks [18].To ensure the relevance and quality of the questions generated by the LLM, we apply strict regular expressions and LLM-based filtering to exclude questions of local interest, such as simulation results or locally-defined content.</p>
<p>Third, we examine the viability of using parameter-efficient fine-tuning (PEFT) techniques to inject telecommunications knowledge into these LLMs.Our findings show that, instead, full fine-tuning (FFT) is required.Based on this, we conduct experiments to determine the number of training epochs needed to maximize model performance and to identify when overfitting occurs.We also explore how models of different sizes behave during adaptation and how training dynamics vary depending on the original model's data.Furthermore, we assess whether splitting the adaptation into multiple specialized models, each focused on different aspects of telecommunications, is more effective than a single, combined model.Our results demonstrate that the latter approach is superior due to the transfer learning that occurs across these aspects.</p>
<p>Finally, building on these findings, our work culminates in adapting and open-sourcing a series of telecommunicationsspecialized LLMs ranging from 1B to 8B parameters.Our series is based on TinyLlama-1.1B,Phi-1.5B,Gemma-2B, Gemma-2-2B, LLaMA-3.2-1B,LLaMA-3.2-3B, and LLaMA-3-8B, and includes both base models and their instructiontuned versions for chatbot applications.To evaluate our telecom-adapted models, we first compare them against their original counterparts using Tele-Eval, employing both quantitative analysis-based on a set of proposed metrics-and qualitative assessment.Our results show an average relative improvement of 25% on Tele-Eval, thus demonstrating specialization for telecom knowledge while preserving the models' original capabilities and avoiding catastrophic forgetting.Next, to further evaluate our telecom-adapted models, we curate downstream tasks related to telecom literature, such as citation prediction and recommendation.Our results show that the telecom-specialized models consistently outperform their original counterparts on these tasks.Moreover, when fine-tuned appropriately, they can rival larger general-purpose models like GPT-4o on these telecom literature applications.The overall adaptation pipeline is provided in Fig. 1.</p>
<p>II. DOMAIN ADAPTATION</p>
<p>A. Background</p>
<p>LLMs are trained using a next-token prediction objective on corpora that encompass a wide variety of domains, such as medicine, history, and more.In this context, a token refers to a subword that represents the most basic unit in the text.This training process allows the LLM to become accustomed to the distribution of tokens in these datasets and develop an understanding of the statistical relationships between them.Consequently, the LLM develops a well-rounded understanding of the multiple domains it has been trained on.</p>
<p>As demonstrated in [19], if one intends to use an LLM solely for a specific domain, specializing the LLM in that domain is advantageous for any subsequent in-context learning or task-specific fine-tuning.This is because the probability distribution of tokens in the domain of interest can differ significantly from those in other domains internalized by the LLM.By adapting to this distribution, the LLM's knowledge becomes more targeted to the domain, leading to transfer learning for subsequent usage stages.</p>
<p>The primary approach for domain adaptation is continual pretraining.This method involves further training the LLM on domain-specific corpora, allowing it to adapt its parameters to the field of interest [12].Continual pretraining has become the standard for LLM domain adaptation, as evidenced by its application in various fields such as medicine [13], law [14], and finance [15].In the following, we will explore the details of continual pretraining, with a particular emphasis on its application in the telecommunications domain.</p>
<p>Remark 1: Adapting a pretrained LLM to a specific domain is typically preferred over training an LLM from scratch on domain-specific data.This approach leverages capabilities learned from other domains (e.g., English language proficiency, coding) and transfers them to the new domain, reducing the need for extensive data and time to learn basic skills.</p>
<p>B. Continual Pretraining</p>
<p>Given the scale of the training datasets of current LLMs (e.g., 15T tokens for LLama-3 [3]), it is fair to assume that the language models available in the literature have been trained on all available public-source data.Therefore, these models have likely already encountered most of the publicly available telecommunications-related data.Nevertheless, the goal of continual pretraining is to adapt the LLM's knowledge to this specific domain by re-exposing the model to these data.Specifically, consider an LLM trained on a corpus of tokens D drawn from a distribution µ.This LLM is trained over the entire corpus D to minimize the cross-entropy loss function for tokens x = (x 1 , . . ., x T ) drawn from this corpus:
min W ∈R n E x∼µ [L CE (x; W )] = −E x∼µ T t=1 log P (x t | x 1:t−1 ; W ) (1)
where T is the context length, x 1:t−1 is equal to (x 1 , . . ., x t−1 ), W is the LLM's parameters, and n is the total number of parameters.On the other hand, let us consider a set of telecom-related data D Tele-Data .The samples of tokens x = (x 1 , . . ., x T ) drawn from D Tele-Data originate from a different distribution, denoted by σ.Continual pretraining involves initializing the LLM to its existing parameters, denoted by W 0 , and further reducing the cross-entropy loss, this time using samples of tokens x drawn from D Tele-Data :
min W ∈R n −E x∼σ T t=1 log P (x t | x 1:t−1 ; W , W 0 ) . (2)
By doing so, the model parameters shift closer to the new distribution, allowing the LLM to be better calibrated to telecommunications-specific data.</p>
<p>C. Catastrophic Forgetting</p>
<p>Ideally, one would hope that such domain adaptation comes at no penalty.However, generally, a penalty is incurred.By shifting the focus from the general token distribution µ of the training dataset D to a new distribution σ, the model risks forgetting previously acquired knowledge.This risk depends on the size of the model, the size of the domain-specific dataset, and how different σ is from µ [20].Addressing this issue is crucial; otherwise, one may end up with a telecomknowledge proficient LLM but lose reasoning capabilities, coding abilities, and general English language understanding in the process.Later in this paper, specifically in Section VII, we will detail how we addressed this issue in our continual pretraining framework.With this in mind, the next step in our framework involves curating a comprehensive telecommunications corpus, which we will refer to as Tele-Data.</p>
<p>Items</p>
<p>III. TELE-DATA</p>
<p>Our collection of telecommunications material comprises four main sources: (1) scientific papers from arXiv, (2) 3GPP standards, (3) Wikipedia articles related to telecommunications, and (4) telecommunications-related websites extracted from Common Crawl dumps.This diverse range of sources ensures broad coverage of telecommunications knowledge and is essential for enabling the transfer of expertise from one aspect of telecommunications covered in one source to another, as will be shown later in Section VIII-A.The detailed composition of our dataset is reported in Table I, with LLama-3 tokenizer used to provide token counts as an example.</p>
<p>A. Arxiv</p>
<p>Curation.One of the largest sources of open-access research on telecommunications consists of preprints submitted by authors to the arXiv repository.As of March 2024, the combined arXiv snapshot for both computer science and electrical engineering categories contains approximately 610k papers.However, given the overlap between these categories and the inclusion of topics beyond telecommunications , targeted filtering is necessary to identify the relevant material.To achieve this, we use a language model-based filtering approach.Specifically, we leverage the Mixtral 8x-7B-Instruct2 model, providing it with the abstract of each paper to determine if it is related to the telecommunications and networking domain.The model is prompted 3 to provide a Yes or No answer regarding whether the paper is relevant.We then utilize the logits of the 'Yes' and 'No' tokens to classify whether or not a paper is related to telecommunications.</p>
<p>To assess the quality of our filtering process, we randomly sampled 500 arXiv papers and manually annotated them to determine their relevance to the telecommunications and networking domain.To ensure a balance between relevant and irrelevant examples, we drew an equal number of samples from both the computer science and electrical engineering categories given that the latter is more likely to contain telecommunications material, while the former is more likely to include unrelated papers.We then evaluated both the precision and recall of the filtering process.As shown in Table II, the process demonstrates high recall and moderate precision.High recall is particularly important, as it indicates that our filtering process captures virtually all relevant telecommunications material.The moderate precision can be attributed to the inclusion of papers from related domains.For instance, some papers investigating cloud computing topics were included.Since these  additional domains are not orthogonal to telecommunications, they can provide transferable knowledge that may enhance the language model's adaptation to the telecommunications domain.All in all, this demonstrates the effectiveness of our filtering process.</p>
<p>Cleaning.Following curation, we implement a comprehensive cleaning process for the papers, which includes: (1) removing comments, (2) flattening LaTeX sources, (3) substituting userdefined macros with standard LaTeX commands, (4) eliminating LaTeX native commands, and (5) standardizing citation formats and removing markup changes.We also remove figures and tables to focus on inline text and equations.Details on this procedure are provided in Appendix A. To assess the effectiveness of our cleaning process, we randomly selected 500 arXiv papers and derived a cross-entropy-based approach to evaluate cleanliness as detailed below.Let X = (x 1 , x 2 , . . ., x N ) and Y = (y 1 , y 2 , . . ., y M ) denote the nonoverlapping chunks of T tokens from the raw and cleaned datasets, respectively, where N and M are the total number of chunks.Next, let us define the cross-entropy loss for any token index t ∈ {1, . . ., T } in any chunk of either the raw or cleaned data using GPT-2 [4] as follows
L raw CE (x i t |x i 1:t−1 ) = − log P (x i t |x i 1:t−1 ; W GPT-2 ), i = 1, . . . , N, L clean CE (y i t |y i 1:t−1 ) = − log P (y i t |y i 1:t−1 ; W GPT-2 ), i = 1, . . . , M.
(3) GPT-2 was chosen because it was primarily trained on web content, excluding arXiv scientific papers.Therefore, the cross-entropy here serves as a proxy for how closely the text resembles generic online content rather than the more complex arXiv papers, thus providing an indication of the dataset's cleanliness.With this in mind, we set T to 1024, in line with GPT-2's training data [4], and define the total set of crossentropy losses for the raw and cleaned datasets as
R = N i=1 {L CE (x i t |x i 1:t−1 ) : t ∈ [1, 1024]}, C = M i=1 {L CE (y i t |y i 1:t−1 ) : t ∈ [1, 1024]},(4)
respectively.We then focus on the hardest 10% of the tokens to evaluate the differences between the two sets R and C. To that end, let τ 1 = Q 0.9 (R) and τ 2 = Q 0.9 (C) represent their 90th percentiles.By comparing the distributions of
R high = {L CE ∈ R : L CE ≥ τ 1 } and C high = {L CE ∈ C : L CE ≥ τ 2 } in
Figures 2 and 3, we observe that both the mean cross-entropy loss and its standard deviation within this percentile are lower in the cleaned dataset compared to the raw, uncleaned papers.This demonstrates the improved cleanliness of our dataset.</p>
<p>B. Standards</p>
<p>Curation.Standards play a pivotal role in telecommunications as they ensure interoperability among technologies from various vendors.These standards are established and maintained by recognized bodies such as 3GPP, IEEE, and ITU.Due to their open-source nature, we focus on incorporating 3GPP documents into our dataset.To do so, using the 3GPP FTP portal 4 , we downloaded the latest specifications for each standard of each series, resulting in a dataset of approximately 2.8k documents.</p>
<p>Cleaning.Following the curation process, we clean and process the standards files through several steps.We begin by removing non-essential sections such as related works and appendices, and eliminating figures and tables to focus on inline text and equations, similar to our approach with arXiv papers.A key caveat is that equations in .docfiles are formatted in XML, unlike the LaTeX format used in arXiv papers.To address this, we first convert all .docfiles to .docxformat and then utilize docx2tex 5 to transform the standards into LaTeX format.This standardization improves the training process by ensuring consistency of equations across all document types.Finally, we apply the same cleaning pipeline used for arXiv papers to the converted standards LaTex files to ensure a uniform level of cleanliness and coherence across our entire dataset.</p>
<p>C. Wikipedia</p>
<p>Another source of telecommunications material is the Wikipedia corpus, specifically articles related to the telecommunications domain and its associated technical content.To curate this dataset, we utilize the English subset of the Wikipedia dataset 6 , which contains 6.4 million samples.Given the size of this dataset, applying a pure LLM-based classification would be computationally expensive.Instead, we employ a two-step process:</p>
<p>1) Keyword Filtering: We define a set of 100 telecom-related keywords, including terms such as telecommunications, base station, Wi-Fi, and 5G.Articles containing any of these keywords are flagged for the next step.This process significantly reduces the number of articles from 6.4M to approximately 70k. 2) LLM-based Content Evaluation: In this second step, we apply an LLM-based filtering process to the flagged articles.Particularly, the first 10,000 characters of each article are provided to the Mixtral 8x-7B-Instruct model, and the LLM is prompted to provide a Yes or No answer regarding the article's relevance and the presence of technical content related to telecommunications.The reason behind this is to exclude articles that discuss non-technical aspects, such as a telecom operator's history.</p>
<p>Through this two-step process, we curate a dataset of 19.5k technically relevant telecommunications articles from Wikipedia.Similarly, we evaluated our LLM-based filtering by sampling 500 flagged articles and manually annotating them.We then assessed the precision and recall of our filtering process.The results in Table II showcase the high recall of the process, showcasing its ability to identify relevant technical content in telecommunications.The moderate precision, on the other hand, stems from the challenge of determining the appropriate level of technicality required for an article, resulting in false positives within the dataset.</p>
<p>D. Websites</p>
<p>The last source of telecommunications material we consider is the Common Crawl dataset, which comprises web archives from across the internet.To avoid issues with duplicates, non-English content, and potential profanity found in raw dumps, we utilized the refined web dataset [21].This curated version of Common Crawl contains approximately 1 billion rows across 2.8 terabytes of data.To further eliminate duplicates, we filtered out Wikipedia articles from the dataset.Next, to extract telecommunications-related content from this refined dataset, we employed the same two-step process used for Wikipedia articles.Additionally, we incorporated content from well-known telecommunications blogs, such as ShareTechNote, to enhance the dataset's relevance.The resulting collection consists of content from 740k website links, providing a comprehensive representation of telecommunications information available on the web.</p>
<p>To evaluate the quality of the LLM-based filtering process, we sampled and manually annotated 500 websites containing telecommunications keywords.We then assessed the precision and recall of our filtering process.We observed perfect recall, as the incorporated websites included patents, technical blogs, and forum discussions about telecommunications.However, precision was lower due to the difficulty in discerning the technical content.As a result, websites with product technical specifications, marketing sites for VPN services containing technical details, and product reviews were also included, leading to false positives in the dataset.</p>
<p>E. Dataset Format</p>
<p>Tele-Data is structured as a JSONL (JSON Lines) file, where each line represents a JSON object with five distinct fields:</p>
<p>• ID: A unique identifier for each data entry, combining the data category and a number.For example, 'wiki_132' refers to the 132th item of the Wikipedia data points.• Category: A string indicating the source of the data: wiki, standard, arxiv, or web.• Content: A string containing the main text of the material.</p>
<p>• Metadata: A JSON object containing various information relevant to each specific element, with the structure varying depending on the category.For example, for standards, the JSON object includes the 3GPP series number, release, and standard file name, while for arXiv papers, it contains the arXiv ID, title, and abstract.Examples of the dataset can be found in Appendix B.</p>
<p>IV. EVALUATION DATASET</p>
<p>After preparing Tele-Data, the next step involves creating an evaluation dataset to test the resulting domain-adapted models' telecommunications knowledge.Currently, only one such dataset exists: TeleQnA [22].TeleQnA is a multiplechoice question (MCQ) dataset drawn from standards and research papers, generated with human involvement.Although an MCQ dataset simplifies evaluation in terms of accuracy, it remains limiting because LLMs are not robust MCQ selectors due to their inherent 'selection bias,' a bias prevalent in virtually all LLMs [23], [24].</p>
<p>Given the above, to examine the telecommunications proficiency of LLMs, we take another approach by creating a dataset of open-ended telecommunications questions.Openended questions assess a model's ability to elaborate on its own to answer questions about specific telecommunications concepts.Given the large number of concepts to be covered and the specialized nature of telecommunications knowledge, large-scale open-ended questions are needed, making a purely human-based approach infeasible.</p>
<p>To overcome this challenge, we adopt an LLM-based approach to create our evaluation dataset, which we refer to as Tele-Eval.Specifically, we gather material from papers, standards, and wiki articles within the collected Tele-Data.We focus on these categories due to their relative cleanliness compared to the Common Crawl data.We then segment the material into 20,000-character chunks.Using Mixtral 8x7B-Instruct, we prompt the LLM to generate five questions for each segment in a three-shot manner, providing examples similar to those in Trivia QA [25], but tailored to the telecommunications domain.The generated questions then undergo extensive regular expression-based filtering.Particularly, we implement filters to remove questions containing expressions such as 'in this context,' 'in this paper,' 'as highlighted in,' and references to equations, figures, tables, and other local elements, along with 100 other similar filters.Our aim is to achieve the lowest possible retention rate to ensure the highest quality in the automatically generated dataset.</p>
<p>The next step in our process involves feeding the QnA pairs to another instance of the Mixtral LLM.This instance is prompted to determine if the provided QnA pair can be answered without access to the source material (i.e., eliminate local context dependency).We retain only those QnA pairs that pass both the regex-based filtering and this LLM-based filtering.As a result, a retention rate of 1% was achieved, and the dataset consists of 750k QnA pairs.It is important to note that given this scale, even if some questions are not perfectly answerable by an LLM, the dataset still serves its purpose as an effective evaluator of telecommunications knowledge.</p>
<p>Remark 2: One important aspect of our dataset is that each generated QnA pair includes the ID of the specific telecommunications material upon which it was based.This allows users to select questions from the material they are interested in and facilitating retrieval-augmented generation frameworks [18].For example, if someone is interested in a specific topic (e.g., source and channel coding), they can identify the relevant content in the telecommunications dataset, obtain their IDs, and then use the portion of Tele-Eval linked to those IDs.We provide below a couple of examples of the dataset to illustrate the style and format of the data:</p>
<p>Statement: Under what circumstances should the UE insert a public GRUU value in the Contact header field?Answer: The UE should insert the public GRUU value in the Contact header field if a public GRUU value has been saved associated with the public user identity from the P-Called-Party-ID header field, and the UE does not indicate privacy of the P-Asserted-Identity.</p>
<p>ID: standard_1309</p>
<p>Statement: What is the difference between unassisted capacity and entanglement assisted capacity in the context of quantum channels?Answer: Unassisted capacity refers to the maximum rate of transmitting classical or quantum information through a quantum channel without the use of entanglement, while entanglement assisted capacity refers to the maximum rate with the assistance of entanglement.ID: arxiv_36721</p>
<p>V. EVALUATION METRICS</p>
<p>One of the key challenges after continual pretraining is effectively evaluating the resulting domain-adapted model.As seen in the previous section, Tele-Eval consists of openended question-answer pairs.Evaluating open-ended responses is more complex compared to MCQ datasets.For example, traditional evaluation metrics such as ROUGE [26] and BLEU [27], commonly used for summarization and translation tasks, measure lexical overlap between the model's output and the reference answers.However, they fail to capture the semantic similarity and correctness of responses when the model uses alternative wording or lexicon to convey the same meaning as the reference answers.This challenge is especially pronounced when evaluating equations generated by the models, as is often required in the telecommunications domain, since these metrics struggle to capture such nuances.To address this, we adopted three evaluation metrics: Answer perplexity, SemScore [28], and LLM-Eval [29].In Section VII, we will showcase the pros and cons of each evaluation metric and conclude that LLM-Eval is the most robust comparative tool for the telecommunications domain.Below, we provide details on each of these metrics.</p>
<p>A. Answer Perplexity</p>
<p>We define answer perplexity as the perplexity of the model with respect to the ground truth answer, conditioned on the question.Specifically, let us consider N samples of Tele-Eval, where x i = (x i 1 , . . ., x i T ) represents a concatenation of both the question and the ground truth answer of the i-th pair.Assume that at index k i , the ground truth answer begins.With that in mind, we define the answer perplexity as:
Ans-PPL(x) = exp − 1 N N i=1 Ti j=ki 1 T i − k i + 1 log P (x i j | x i 1:j−1 ; W ) ,(5)
where T i is the number of tokens in the i-th pair, and W represents the model weights.This metric can be seen as how surprised the model is by the answer, given the question.</p>
<p>Intuitively, a model that is well-versed in telecommunications should be less surprised by the answer (hence, have a lower perplexity) compared to a non-specialized model.</p>
<p>B. SemScore</p>
<p>Another metric we use to evaluate the correctness of a model's output relative to the ground truth answer is semantic similarity.This is achieved by leveraging sentence transformer models [30], such as the all-mpnet-base model 7 from the Sentence Transformers family.These encoder-only BERT models are trained using contrastive loss to encode pairs of sequences into a high-dimensional space, such that the cosine similarity between embeddings of similar pairs is high (closer to 1), while it is lower for dissimilar ones (closer to -1).With this in mind, we define the SemScore [28] as SemScore(x, y) = cos(embed(x), embed(y)), (6) where embed(•) refers to the BERT embedding model, and x and y refer to the ground-truth answer and the model's output, respectively.Thus, this metric allows us to judge how closely the model's output aligns with the correct answer.</p>
<p>C. LLM-Eval</p>
<p>The last metric in our framework involves using an LLM as a judge to assess the correctness of a model's output compared to the ground truth answer [29].Given the extensive knowledge LLMs have acquired through their training, we can leverage them as assistants to evaluate the quality and accuracy of model outputs.Previous studies have demonstrated that LLM-based judges can effectively match both controlled and crowdsourced human preferences in various evaluation tasks [29].With this approach, when evaluating any model outputs for questions in Tele-Eval, we prompt Mixtral 8x-7B-Instruct to compare the model's output to the ground truth and provide a Yes or No answer regarding its correctness.We define the LLM-Eval metric as the average score of these boolean responses, such that
LLM-Eval = 100 × 1 N N i=1 1 LLM judges y i as correct ,(7)
where y i is the model's answer to question i of Tele-Eval, N is the total number of evaluated outputs, and 1{•} is the indicator function.</p>
<p>VI. INITIAL EXPERIMENTS</p>
<p>With both the training and evaluation datasets prepared, the next step is to train our models.However, before proceeding, several key questions regarding training techniques and parameters need to be addressed to lay the foundation for the training procedure.While general guidelines on training parameters, such as learning rate and batch size, can be drawn from previous research [13], [14], [15], the characteristics of telecommunications knowledge-such as the prevalence of equations-introduce specific challenges.Key questions that need to be answered include whether full parameter finetuning is necessary or if parameter-efficient tuning methods are sufficient, as well as determining the number of training epochs needed to effectively adapt the LLM to the telecommunications domain.We address these questions in this section, while noting that our results can serve as a guide for adapting other models to the telecommunications domain beyond those discussed in our paper.</p>
<p>A. Training Settings</p>
<p>For these experiments, we considered two models: Gemma-2B and LLaMA-3-8B.This choice was motivated by our intention to explore the behavior of model training across different parameter sizes.Throughout these studies, we use 20% of the Tele-Data dataset as the training data, with 10% of this subset serving as the validation set.To solve eq. ( 2), we set the batch size to 4M tokens (with a sequence length of 8192 tokens) and use the AdamW optimizer with a weight decay of 0.1, keeping the default Hugging Face β parameters unchanged.The maximum gradient norm is set to 1.The learning rate is decreased according to a cosine learning rate schedule, down to 10% of the maximum learning rate, and a linear warmup of 10% of an epoch is applied [31].The maximum learning rate is set to 1e-5 for LLaMA and 2e-5 for Gemma.These values are aligned with the pretraining stages of these LLMs [3], [32].To improve efficiency, we employ mixed-precision training alongside sample packing and block attention techniques to avoid cross-contamination [33].</p>
<p>B. PEFT vs. FFT</p>
<p>PEFT techniques aim to reduce computational costs and memory requirements during domain adaptation by training only a minimal number of parameters, in contrast to FFT methods that update all the model's parameters.Among these techniques, LoRa [34] stands out as a prominent PEFT method.It adds small, trainable low-rank matrices to the existing weights of the pretrained model.To investigate whether PEFT methods like LoRa are sufficient for adapting language models to the telecommunications domain, we conducted a comparative study between LoRa and FFT using both Gemma-2B and LLaMA-3-8B models.For our LoRa implementations, we fixed the LoRa rank to r=64, set the Lora alpha to 32, and used a LoRa dropout rate of 0.1.</p>
<p>The results, reported in Fig. 4, demonstrate that for smaller models, LoRa can initially inject telecom knowledge but quickly saturates due to its limited capacity.In contrast, for the LLaMA-3-8B model, which is more knowledgeable about telecommunications, the gradient norm of the LoRa method remained extremely low.This hindered parameter updates, causing the training loss to barely change and limiting LoRa's ability to inject additional knowledge.These findings indicate that, despite its computational efficiency, LoRa may not be sufficient to adapt the model to the telecommunications field.Consequently, in the next sections, we will rely on full parameter fine-tuning for all our training.</p>
<p>C. One or Multiple Epochs?</p>
<p>A crucial question when adapting an LLM to a target domain, in our case being telecommunications, is determining the most effective duration for model training.Is a single epoch enough to adapt the model, or do multiple exposures to the data with shuffling at each epoch yield better results?To address this, we conducted experiments using two different model scales: Gemma-2B and LLama-3-8B.As we will demonstrate, this distinction arises from the distinct behaviors of these models due to their different sizes.The results are reported in Fig. 5 and discussed below.Gemma-2B.As shown in Fig. 5, the training loss of Gemma-2B demonstrates consistent behavior, decreasing and then plateauing after approximately two epochs.A similar pattern is observed in the validation loss, which also stabilizes after two epochs.Additionally, the gradient norm approaches 0, indicating that the training is reaching convergence.It is worth noting that for smaller models, after three epochs of training, there are no signs of overfitting, as both validation and training losses plateau at similar levels.This suggests that the model has developed a comprehensive understanding of the telecom data, and further training results in only minor adjustments to this understanding without significantly adding to or diminishing the existing telecom knowledge.LLama-3-8B.The first observation one can make by investigating the bottom plots of Fig. 5 is that the training loss starts with a cross-entropy of around 1.42.This indicates that the model is already quite knowledgeable when it comes to telecommunications.This is not surprising for two reasons: first, this model was trained on 15T tokens, hence it has likely encountered a wide variety of telecom-related data; second, the large number of parameters in the model allows it to retain such information.Regardless, continual pretraining is helpful to further solidify this knowledge.</p>
<p>By further examining the training loss, an interesting phenomenon can be witnessed: at the end of each epoch, the training loss experiences a noticeable drop.This suggests that the model is memorizing patterns in the training dataset and becoming more confident in its next token prediction as it encounters these patterns after each epoch.These patterns have some generalizable components, as evidenced by the drop in validation loss up to the second epoch.However, beyond two epochs, this memorization becomes problematic as the learned patterns become more specific to the training data and less transferable to the validation data, thus leading to overfitting.We hypothesize that two epochs represent a sweet spot: the first epoch serves to warm up the model and absorb telecom knowledge, while the second epoch, aided by the cosine decay, shapes this knowledge with a very low learning rate, allowing the model to make final adjustments in the loss landscape.Based on the above, we use two epochs to create the Tele-LLMs series in the next section.</p>
<p>Remark 3: We have observed similar training loss behavior across other models (e.g., Mistral-7B).As the number of parameters increases, the model becomes more expressive and better able to capture intricate relationships in the training data, leading to this behavior.We have not reported these results due to their similarity to those shown in the bottom plots of Fig. 5.</p>
<p>VII. TELE-LLMS</p>
<p>Having completed the initial experiments, we proceed with the full-scale training of the LLMs to create the telecomadapted series of models, starting with the following models: TinyLlama-1.1B,Gemma-2B, Gemma-2-2B, LLaMA-3.2-1B,LLaMA-3.2-3B, and LLaMA-3-8B.Our selection of these base models was guided by a combination of factors, including their performance on the Hugging Face Open LLM leaderboard and a consideration of their licenses.We also accounted for the diversity of the releasing companies to mitigate potential risks associated with changes in these licenses.</p>
<p>Tele-Eval</p>
<p>General Knowledge Ans-PPL SemScore LLM-Eval MMLU HellaSWAG GSM8K  Training Settings.We maintain the training settings described in Section VI-A, where models smaller than 8B parameters follow the same approach as Gemma-2B while aligning the sequence length to each model's pretraining settings.For the training dataset the entire series, we utilize the entire Tele-Data dataset for two epochs, augmented with 5% of regular general-purpose data taken from SlimPajama 8 .This augmentation is designed to mitigate the catastrophic forgetting phenomenon by reintroducing general-purpose data to the model, helping it retain key skills acquired during the initial pretraining stage.The training of these models was conducted on a cluster of 8 NVIDIA A6000 GPUs, consuming a total of 5,000 GPU hours.Loss Trends.In Fig. 6, we report the training loss across three models.Note that all sub-8B parameter models exhibited similar trends to those of Gemma-2B and Tinyllama-1.1Bshown in Fig. 6.These models, in particular, demonstrate steady training loss behavior, while LLaMA-3-8B displays the sharp loss reduction characteristic of larger models at each epoch depicted in the previous section.Evaluation Settings.We evaluate the LLMs on Tele-Eval using the three metrics discussed in Section V, generating 8 https://huggingface.co/datasets/cerebras/SlimPajama-627B model answers through greedy decoding of 100 new tokens with the prompt provided in Appendix C. To assess the impact of our training on other LLM capabilities, we evaluate our models on three additional benchmarks: MMLU [35] for general world knowledge, HellaSWAG [36] for commonsense understanding, and GSM-8K [37] for mathematical reasoning abilities.All evaluations, including Tele-Eval and the additional benchmarks, are conducted in a zero-shot setting.</p>
<p>Quantitative Evaluation.As shown in Table III, the adapted telecom LLMs outperform their counterparts across all metrics on Tele-Eval.For general knowledge tasks, these models exhibit stable performance compared to the original models, with slight variations-either improvements or degradations-across most tasks.Notably, there are improvements on GSM-8K, with LLama-3-8B-Tele achieving a 10% higher accuracy.This enhancement is attributed to the equations found in Tele-Data, which enable the adapted model to become stronger in mathematical reasoning.</p>
<p>Comparing the LLMs' performance on Tele-Eval across the three metrics reveals several insights: First, Ans-PPL shows minimal relative differences for larger models.This occurs because larger models, with their higher proficiency, are less surprised by the terms used in the ground truth answers.Consequently, improvements in telecommunications knowl-edge become less apparent and more diluted in this metric.Another observation is the lack of comparability across different models using the Ans-PPL metric, as evidenced by the performance comparison between Tinyllama-1.1B-Teleand Gemma-2B-Tele.The tendency of a model to follow certain formats when answering questions biases this metric, resulting in either better or worse performance.This makes Ans-PPL more useful for comparing models within the same family rather than across different families, due to the heterogeneous nature of their tendencies.</p>
<p>SemScore exhibits patterns similar to Ans-PPL; an LLM producing coherent responses with appropriate technical terminology may achieve good scores even when providing incorrect answers.With this in mind, and given the heterogeneous response styles across LLM families, both SemScore and Ans-PPL are more effective at highlighting significant differences within the same LLM family.For detecting subtle disparities and comparing across different LLM families, LLM-Eval proves more robust.It disregards linguistic variations and instead focuses on the underlying concepts being assessed.Using this metric, we observe an average relative improvement of 25% for the telecom-adapted LLMs.</p>
<p>Although LLM-Eval provides the most accurate evaluation, its drawback is the runtime.Given the O(n 2 ) scaling of the attention mechanism, where n is the input prompt length for the evaluation, our evaluation with Mixtral-8x-7B took approximately 50 milliseconds per question on a single Nvidia A6000 GPU.Note that given the size of Tele-Eval, the standard deviation of our results is negligible.</p>
<p>Remark 4: It is worth noting that Tele-Eval, with its focus on highly granular telecommunications concepts, presents a significant challenge for LLMs, as evidenced by their performance on the LLM-Eval metric.This inherent difficulty is one of Tele-Eval's advantages; if the questions were easily answerable, the dataset would fail to effectively distinguish the advantages gained through adapting models to the telecom domain.Qualitative Evaluations.Going beyond the quantitative comparison provided in Table III, we offer a set of examples in Appendix D that illustrate how the behavior of these models change, using Gemma-2B and Gemma-2B-Tele as a case study.These examples highlight the model's enhanced ability to provide detailed explanations, confidently answer telecommunications-related questions, and even delve into telecommunications concepts when they are mentioned in the prompt.These qualitative observations further emphasize the successful adaptation of these models to the telecommunications domain.</p>
<p>VIII. FURTHER EXPLORATION</p>
<p>In the final phase of our work, we investigate various adaptation strategies and specific dynamics that emerge when tailoring LLMs to the telecommunications domain.We particularly focus on two key aspects: the potential for expertise division in the adaptation process, and the unique adaptation dynamics that arise based on each model's characteristics and training data.We also adapt our models to follow instructions, creating chatbot-like models that users can interact  with.We show that these adapted models demonstrate greater proficiency in telecommunications compared to their generalpurpose instruct counterparts.</p>
<p>A. Division of Expertise</p>
<p>When adapting an LLM to the telecommunications domain, a key question arises: should we adapt a single LLM to cover the entire domain, or should we adapt multiple LLMs, each specialized in a specific aspect of telecommunications?For example, one LLM could focus on scholarly material while another concentrates on standards.To explore this, we compare two versions of the Gemma-2B model: one trained exclusively on the standards portion of Tele-Data, referred to as Gemma-2B-Standards, and Gemma-2B-Tele.The training parameters align with those previously reported for Gemma-2B-Tele.Evaluation is based on both the entirety of Tele-Eval and questions related to standards using the LLM-eval metric, along with general knowledge evaluation datasets to assess the retention of the model's capabilities.The results are reported in Table IV.</p>
<p>As shown, Gemma-2B-Tele outperforms Gemma-2B-Standards on the standards-related questions while also demonstrating stronger general knowledge capabilities.Additionally, Gemma-2B-Standards performs worse than the base model Gemma-2B on the overall Tele-Eval dataset.The reason behind this is that the Gemma-2B-Standards model, having been trained solely on standards, becomes more attuned to that specific type of content.Standards tend to be highly technical with a unique token distribution that differs from the broader telecommunications knowledge found in scholarly and Wikipedia articles.This narrow focus makes the model less capable of handling scholarly material compared to the base model, leading to underperformance on Tele-Eval.At the same time, this narrow focus also negatively impacts the LLM's general abilities.All in all, this highlights that the most effective strategy is to adapt the LLM to the entire telecommunications dataset, thereby benefiting from the transfer learning that occurs across these diverse materials rather than narrowly focusing on a single aspect.</p>
<p>B. Pretraining Data Impact</p>
<p>Perhaps the most impactful element that shapes an LLM's behavior is its pretraining data.This data is generally not publicly available, so one can only infer its type through interactions with the model.Understanding this behavior is crucial because when adapting an LLM to the telecom domain, specific trends emerge based on this behavior.</p>
<p>Instructions Following</p>
<p>Prompt: A poem about telecommunications.</p>
<p>Phi-1.5: Answer: Telecommunications is a field that connects people and places, Through wires and signals, it's a vital task.It's like a bridge that spans the distance, Bringing us closer, no matter the distance.</p>
<p>Gemma-2B:</p>
<p>The poem is about the telephone.It is about the telephone that is in the room.Fig. 7: Generated responses from different models for a telecommunications-themed prompt.To illustrate this, let us consider two models: Gemma-2B and Gemma-2B-it 9 .The latter is an instruct version of the former that has been post-trained on instruction-following data, which includes tasks like question answering and summarization.As Table V shows, instruct models outperform their base counterparts on Tele-Eval.This is because the additional instruction-focused training allows the LLM to better handle questions, follow instructions, and leverage its internalized knowledge to effectively answer queries.This advantage is significant enough that Gemma-2B-it is able to outperform our telecommunications-adapted LLM, Gemma-2B-Tele.However, since Gemma-2B-Tele has been enriched with telecommunications knowledge, we can surpass the performance of Gemma-2B-it by applying a similar post-training instruction-following adaptation using a dataset like Alpaca 10 and utilizing the same training settings previously detailed, thus creating Gemma-2B-Tele-Alpaca.</p>
<p>Tele-Eval</p>
<p>The significance of the above observation is that some base models have been trained on instruct-like data during their pretraining stage.An example of this is Microsoft Phi-1.5.In their paper [2], the authors specify that Phi-1.5 can be prompted as an instruct model because the pretraining data included a large portion of question-answer formats.This leads to noticeable behavioral differences between Gemma-2B and Phi-1.5, as illustrated in Fig. 7.One can see that Phi-1.5 behaves more like a chatbot, following the instructions of the prompt, while Gemma-2B fills in sentences without treating the input prompt as an instruction to follow.</p>
<p>Given the above, when applying the training recipe described in our paper to Phi-1.5, the performance on Tele-Eval declines for Phi-1.5-Tele,similar to what occurred for Gemma- 2B-it.However, by post-training Phi-1.5-Tele on Alpaca using the same training settings, the performance significantly improves beyond that of the base model.This is because, by doing so, we align the model back to its original behavior.With this in mind, it is crucial to be mindful of the model's behavior before proceeding with the adaptation process, in order to anticipate its dynamics accordingly.</p>
<p>Given the above observations and the special behavior of the Phi-1.5 model, we only release the Alpaca fine-tuned version of Phi-1.5 as part of our Tele-LLM series.It is worth noting that its performance on general knowledge datasets is 0.3965 on MMLU, 0.4680 on HellaSWAG, and 0.1107 on GSM8K, compared to 0.4072, 0.4798, and 0.0576, respectively, for the original Phi-1.5.This demonstrates that our model performs better in telecom while still retaining its general knowledge and even gaining in mathematical performance.</p>
<p>C. Instructions Fine-tuning</p>
<p>Although base models are essential, as they contain the raw LLM's knowledge and are best suited for fine-tuning for specific telecommunications applications, it is common for users to interact with these models as chatbots.Therefore, in this section, we proceed to fine-tune our telecom-adapted models to follow instructions through instructions fine-tuning.Training Settings.We maintain the training settings from Section VII with minor adjustments.Specifically, we decrease the batch size to 128k tokens, set the context length to 2048, and limit the number of epochs to 1.For this stage, we use two datasets: Alpaca and Open-Instruct 11 .The combined datasets provide approximately 200k samples for training.Results.In Table VI, we compare the Tele-Instruct versions of our models to their general instruct counterparts.Our instruction-tuned series outperforms the general instruct models, thereby showcasing their superior knowledge in telecom compared to their general-purpose counterparts.</p>
<p>D. Benefits of Specialization</p>
<p>A central question we explore is how well domain specialization improves performance on downstream tasks beyond question answering.To investigate this, we focus on several literature-centric tasks: citation prediction, citation recommendation, title generation, abstract completion, and introductionto-abstract conversion.Leveraging the Tele-Data dataset, we curate fine-tuning samples tailored to each of these tasks.</p>
<p>We then fine-tune base models from the Llama family and compare their performance to the specialized Tele-LLMs, using a dedicated hold-out set excluded from training.Details on the fine-tuning process-including instruction datasets and hyperparameters-are provided in Appendix E.</p>
<p>For tasks involving text generation (e.g., citation sentence generation or abstract-to-title conversion), we adopted BERTScore-incorporating both precision and F1 measuresas our primary evaluation metric [38].As shown in Table VII, fine-tuned models (referred to by the suffix Lit) has the potential of outperforming GPT-4o on literature tasks (e.g., as is the case for LLama-3-8B) where domain-specific knowledge and graph structure are critical, while GPT-4o maintained an advantage in English language formulation.Notably, these fine-tuned models achieve competitive performance despite being significantly smaller than GPT-4o, demonstrating the efficiency benefits of targeted fine-tuning.Furthermore, the fine-tuned telecom models (referred to by the suffix Tele-Lit) consistently outperformed their generic counterparts across all tasks.This demonstrates how domain specialization fundamentally improves the model's representations, aligning them more effectively with the telecom domain and enhancing performance on downstream tasks after task-specific finetuning.These results highlight the value of our developed Tele-LLMs series in enhancing performance on telecom-specific downstream tasks.</p>
<p>IX. CONCLUSIONS</p>
<p>In this paper, we addressed the challenge of adapting LLMs for specialized use in telecommunications.In our endeavor, we created and released Tele-Data and Tele-Eval, a comprehensive telecommunications training and evaluation datasets.Through extensive experimentation, we identified the most suitable training strategies for adapting LLMs to this domain.The culmination of our work is Tele-LLMs, a series of opensource models ranging from 1B to 8B parameters, specifically designed for the telecommunications domain.These models outperform their general-purpose counterparts on Tele-Eval while retaining their broader capabilities.Beyond this culmination, our work also investigated various adaptation strategies, such as the division of expertise, and explored the dynamics that arise during the adaptation process depending on the models involved.As a future direction, our work will aim to leverage Tele-LLMs and augment them with multimodal capabilities to understand and reason about wireless measurements and signals.</p>
<p>APPENDICES APPENDIX A MANUSCRIPTS CLEANING PROCEDURE</p>
<p>Given the importance of ensuring that the arXiv papers are clean for training, we employ a rigorous cleaning process on these sources.Our process begins with the removal of all comments written in the LaTeX files.For this, we leverage Google's arXiv LaTeX Cleaner 12 .</p>
<p>Next, because LaTeX sources can include multiple LaTeX files, we start by unifying the LaTeX commands used for importing these files.Particularly, we ensure that imports use the \input{•} command.By creating a directed graph that maps these relationships, we identify the main LaTeX file of the paper.We then use the Latexpand Perl script 13 to flatten the document, ensuring the main file contains all material.</p>
<p>In the subsequent step, we address the author's custom commands by 'de-macoring' them.To do so, we unify all custom commands-from \def{•} and \DeclareMathOperator{•} to \newcommand{•}-before using the Python library demacro (https://ctan.org/pkg/de-macro) to replace these macros with their native LaTeX equivalents.Afterwards, we target the removal of figures and tables, as our focus is on the in-line text and equations.We also compile a list of over a hundred LaTeX commands and environments that are not informative, which we remove using regular expression matching.This ensures that the final LaTeX files contain only the text and equations without any LaTeX residue.</p>
<p>Additionally, given the many forms that citations can take, along with the wide variety of text within their brackets, we unify all citation, label, and reference commands.This standardization helps the LLM to avoid dealing with heterogeneity during training.Lastly, we remove all preambles of the LaTeX files and ensure that the title of the manuscript is retained.This provides a unified format for these sources, thus facilitating the LLM training in the next stage.The following prompts were utilized throughout our framework to develop and evaluate various areas.In summary, and in order, the prompts were used for the following tasks:</p>
<p>1) Prompt 1: Used to find arXiv papers related to the telecommunications and networking domains.2) Prompt 2: Used to find websites and Wikipedia pages related to the telecommunications and networking domains.</p>
<p>Example 1</p>
<p>Question: What is the objective of user association in wireless communications?Ground Truth: The objective of user association is to manage the assignment of users to base stations or cells, taking into account factors such as quality of service (QoS) requirements, energy efficiency, and network resources.Gemma-2B: The objective of user association is to reduce the number of access points in the network.Gemma-2B-Tele: The objective of user association is to select the best radio resource for a user.Observation: This example demonstrates how the model becomes more knowledgeable about specific telecommunications concepts and is able to provide more accurate answers compared to the original base model.</p>
<p>Example 2</p>
<p>Question: What does MDS stand for in the context of array codes?Ground Truth: MDS stands for Maximum Distance Separable.Gemma-2B: MDS stands for Minimum Distance Decoding.Gemma-2B-Tele: MDS stands for Maximum Distance Separable.Observation: This example highlights the model's proficiency with telecommunications-related acronyms compared to the original model.</p>
<p>Example 3</p>
<p>Question: What is the stopping condition used in the implementation of Wirtinger gradient descent for minimizing the objective function?Ground Truth: The stopping condition used in the implementation of Wirtinger gradient descent for minimizing the objective function is when the iterates converge to a critical point.Gemma-2B: The stopping condition is the condition that the gradient of the objective function is less than a certain threshold.The gradient of the objective function is calculated using the Wirtinger gradient descent algorithm.The algorithm is a variant of the gradient descent algorithm that is used to minimize the objective function.Gemma-2B-Tele: The stopping condition used in the implementation of Wirtinger gradient descent for minimizing the objective function is the following:
∂f (x) ∂x = 0 ∂ 2 f (x)
∂x 2 &lt; 0 where f (x) is the objective function.Observation: This example demonstrates that the trained model can create entire answers in runnable LaTeX equations instead of relying solely on natural language.</p>
<p>Example 4</p>
<p>Prompt: Radio Gemma-2B: active decay is first order in the decaying isotope.For example, strontium-90 contained in fallout from nuclear explosions decays to yttrium-90 and a beta particle.Gemma-2B-Tele: -frequency identification, also known as wireless tag or transponder, is technology intended for automatic identification without physical contact or line-of-sight between tags and the readers (transceivers).RFIDs may use wireless communication protocols, such as Bluetooth, and employ modulation and encoding techniques such as frequency-shift or amplitude modulation.Observation: This example demonstrates how the LLM's knowledge becomes tailored to the telecommunications domain, as evidenced by the word 'radio' being associated with radiofrequency rather than radioactivity.</p>
<p>APPENDIX E LITERATURE TASKS PREPARATION</p>
<p>We create instruction datasets for literature tasks using our collected Tele-Data.The content of each paper generates instructions for title generation, abstract completion, and introduction-to-abstract tasks.Using citation information from Tele-Data, each edge creates a positive citation link prediction instruction.For balance, we replace the target paper in each edge with a random paper to create negative citation link prediction instructions, maintaining a 1:1 ratio.For every citation sentence associated with an edge, we develop an instruction that uses the titles and abstracts of both connected nodes to generate the citing sentence.For citation recommendation tasks, we randomly sample 10 negative nodes per edge to create a candidate set, requiring the model to identify the correct positive candidate.</p>
<p>During the fine-tuning process, we adopt the Q-LoRa approach [39], initializing the model in 8-bit precision and configuring the LoRa rank to 8. The LoRa layers are applied specifically to the Q, K, V, and O matrices.To optimize training efficiency, we use a batch size of 8 along with a gradient accumulation step of 2. The learning rate is set to 0.0002, and optimization is performed using AdamW [40].Furthermore, we assign a LoRa scaling hyperparameter lora_alpha of 32 and incorporate a dropout rate of 0.05 during fine-tuning.</p>
<p>Fig. 1 :
1
Fig. 1: Overall pipeline of the LLM adaptation to telecommunications.</p>
<p>Fig. 2 : 3 :
23
Raw cross-entropy loss.Fig. Cleaned cross-entropy loss.</p>
<p>Fig. 4 :
4
Fig.4: Training metrics for Gemma-2B (top) and Llama-3-8B (bottom) models using LoRa.</p>
<p>Fig. 5 :
5
Fig. 5: Training metrics for Gemma-2B (top) and Llama-3-8B (bottom) models across three epochs.</p>
<p>Fig. 6 :
6
Fig. 6: Training loss of the Tele-LLMs series.</p>
<p>3 ) 3 : 4 ) 5 )
3345
Prompt Leveraged to generate the QnAs that form the initial Tele-eval dataset.Prompt 4: Leveraged to filter out locally relevant QnAs, resulting in the filtered Tele-eval dataset.Prompt 5: Used to instruct the base model to complete the answer based on the provided question.6) Prompt 6: Utilized as a prompt for LLM-Eval.Prompt 1: arXiv filtering Given the following scientific paper abstract: {Abstract}, Answer by Yes or No if this paper is related to the telecommunications and networking domains .</p>
<p>Prompt 2 :Prompt 3 :
23
Wiki &amp; websites filtering Given the following website content: { Website}, Answer by Yes or No if this content contains technical content about the telecommunications and networking domains.Evaluation dataset generation Generate 5 questions and short answers based on the following passage: {Passage }.The questions should follow this format: Question 1: What frequency band does Bluetooth use?Answer 1: 2.4 GHz Question 2: Which pairs of wires are used in 10/100Base-T?Answer 2: Pair 2 and pair 3 Question 3: What is a Heterogeneous Network?Answer 3: A network consisting of multiple cells with different characteristics Prompt 4: Questions filtering Answer by Yes or No if I can find the answer to this question online without access to the material from which the question came: {question} Prompt 5: Answering format The following is a question about telecommunications and networking.Question: {{statement}} Answer: Prompt 6: LLM-Eval format Task: Evaluate the correctness of a provided answer to a telecommunications and networking question.Question: {question} Ground Truth Answer: {ground_truth} Provided Answer: {prediction} Instructions: 1. Compare the Provided Answer to the Ground Truth Answer.2. Determine if the Provided Answer is correct based on the Ground Truth Answer.3. Respond with only Yes or No.Is the Provided Answer correct?APPENDIX D QUALITATIVE EXAMPLES Below, we present examples that illustrate the qualitative differences between the base Gemma-2B model and Gemma-2B-Tele.These examples include telecommunications-related questions from Tele-Eval (Question), and simple string prompts (Prompt).</p>
<p>TABLE I :
I
Tele-Data division across categories.</p>
<p>TABLE II :
II
Datasets filtering statistics.
Precision Recall F1-ScoreArxiv0.6660.9560.785Wikipedia0.6320.8970.741Webpages0.45510.625</p>
<p>TABLE III :
III
LLMs' performance across Tele-Eval and other general knowledge benchmarks.</p>
<p>TABLE IV :
IV
Performance comparison between Gemma-2B, Gemma-2B-Tele, and Gemma-2B-Standards.</p>
<p>TABLE V :
V
LLM-Eval performance on Tele-Eval.
Original Tele Tele-AlpacaGemma-2B13.5917.0725.31Gemma-2B-it19.8418.0524.83Phi-1.514.8713.0618.84</p>
<p>9https://huggingface.co/google/gemma-2b-it 10 https://huggingface.co/datasets/tatsu-lab/alpaca
Tele-EvalBase Instruct Tele Tele-InstructTinyllama-1.1B 8.2615.4211.3717.40Gemma-2B13.5919.8417.0727.78Gemma-2-2B15.1022.3017.3425.46LLama-3.2-1B11.8514.5012.9816.20LLama-3.2-3B24.6825.2026.7128.56LLama-3-8B24.6030.6529.6034.51TABLE VI: LLM-Eval performance on Tele-Eval.
The Hugging Face links to both the datasets and Tele-LLMs can be found at https://github.com/Ali-maatouk/Tele-LLMs This work was conducted during Kenny's time at Yale University.
https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
All the prompts utilized in our framework are reported in Appendix C
https://www.3gpp.org/ftp/
https://github.com/transpect/docx2tex
https://huggingface.co/datasets/wikimedia/wikipedia
https://sbert.net/docs/sentence_transformer/pretrained_models.html
https://huggingface.co/datasets/VMware/open-instruct
https://github.com/google-research/arxiv-latex-cleaner
We tested fatex (https://ctan.org/pkg/fatex) and fap (https://github.com/ fchauvel/fap), but achieved the best results with latexpand ( https://ctan.org/ pkg/latexpand).</p>
<p>Gpt-4 technical report. Openai, 2024</p>
<p>Textbooks are all you need. S G , 2023</p>
<p>The llama 3 herd of models. A D , 2024</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019160025533</p>
<p>Adapting foundation models for information synthesis of wireless communication specifications. M Kotaru, arXiv:2308.04033August 2023arXiv preprint</p>
<p>Understanding Telecom Language Through Large Language Models. L Bariah, H Zou, Q Zhao, B Mouhouche, F Bader, M Debbah, arXiv:2306.079332023arXiv preprint</p>
<p>Large language models for telecom: Forthcoming impact on the industry. A Maatouk, N Piovesan, F Ayed, A D Domenico, M Debbah, IEEE Communications Magazine. 2024</p>
<p>The Power of Large Language Models for Wireless Communication System Development: A Case Study on FPGA Platforms. Y Du, S C Liew, K Chen, Y Shao, arXiv:2307.073192023arXiv preprint</p>
<p>Large multi-modal models (lmms) as universal foundation models for ai-native wireless systems. S Xu, C K Thomas, O Hashash, N Muralidhar, W Saad, N Ramakrishnan, 2024</p>
<p>Large Language Models for Telecom: The Next Big Thing. L Bariah, Q Zhao, H Zou, Y Tian, F Bader, M Debbah, arXiv:2306.102492023arXiv preprint</p>
<p>Generative ai as a service in 6g edge-cloud: Generation task offloading by in-context learning. H Zhou, C Hu, D Yuan, Y Yuan, D Wu, X Liu, Z Han, C Zhang, 2024</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. S Gururangan, A Marasović, S Swayamdipta, K Lo, I Beltagy, D Downey, N A Smith, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>Biomistral: A collection of open-source pretrained large language models for medical domains. Y Labrak, A Bazoge, E Morin, P.-A Gourraud, M Rouvier, R Dufour, 2024</p>
<p>Saullm-7b: A pioneering large language model for law. P Colombo, T P Pires, M Boudiaf, D Culver, R Melo, C Corro, A F T Martins, F Esposito, V L Raposo, S Morgado, M Desa, 2024</p>
<p>Efficient continual pre-training for building domain specific large language models. Y Xie, K Aggarwal, A Ahmad, 2023</p>
<p>Telecomgpt: A framework to build telecomspecfic large language models. H Zou, Q Zhao, Y Tian, L Bariah, F Bader, T Lestable, M Debbah, 2024</p>
<p>Bidirectional Encoder Representations from Transformers (BERT) for Question Answering in the Telecom Domain: Adapting a BERT-like language model to the telecom domain using the ELECTRA pre-training approach. H Holm, 2021</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Yih, T Rocktäschel, S Riedel, D Kiela, 2021</p>
<p>Investigating continual pretraining in large language models: Insights and implications. N K Çagatay Yıldız, P Ravichandran, M Punia, B Bethge, Ermis, 2024</p>
<p>An empirical study of catastrophic forgetting in large language models during continual fine-tuning. Y Luo, Z Yang, F Meng, Y Li, J Zhou, Y Zhang, 2024</p>
<p>G Penedo, Q Malartic, R Hesslow, A Cojocaru, H Cappelli, B Alobeidli, E Pannier, J Almazrouei, Launay, arXiv:2306.01116The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. 2023arXiv preprint</p>
<p>Teleqna: A benchmark dataset to assess large language models telecommunications knowledge. A Maatouk, F Ayed, N Piovesan, A D Domenico, M Debbah, Z.-Q Luo, 2023</p>
<p>Large language models are not robust multiple choice selectors. C Zheng, H Zhou, F Meng, J Zhou, M Huang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Multiple choice questions and large languages models: A case study with fictional medical data. M Griot, J Vanderdonckt, D Yuksel, C Hemptinne, 2024</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D Weld, L Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJul. 20171</p>
<p>ROUGE: A package for automatic evaluation of summaries. C.-Y Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJul. 2004</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ser. ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ser. ACL '02USAAssociation for Computational Linguistics2002</p>
<p>Semscore: Automated evaluation of instruction-tuned llms based on semantic textual similarity. A Aynetdinov, A Akbik, 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, 2023</p>
<p>Making monolingual sentence embeddings multilingual using knowledge distillation. N Reimers, I Gurevych, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2020</p>
<p>Continual pre-training of large language models: How to re-warm your model?. K Gupta, B Thérien, A Ibrahim, M L Richter, Q G Anthony, E Belilovsky, I Rish, T Lesort, Workshop on Efficient Systems for Foundation Models @ ICML2023. 2023</p>
<p>Gemma: Open models based on gemini research and technology. G Team, 2024</p>
<p>Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. M M Krell, M Kosec, S P Perez, A Fitzgibbon, 2022</p>
<p>LoRA: Low-rank adaptation of large language models. E J Hu, P Shen, Z Wallis, Y Allen-Zhu, S Li, L Wang, W Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2021</p>
<p>HellaSwag: Can a machine really finish your sentence?. R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJul. 2019</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>Bertscore: Evaluating text generation with bert. T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, 2020</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, 2023</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, 2019</p>            </div>
        </div>

    </div>
</body>
</html>
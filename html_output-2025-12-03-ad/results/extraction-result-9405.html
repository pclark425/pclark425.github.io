<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9405 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9405</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9405</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-863171ed35ca0035074f73bb202b153cc346f2f3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/863171ed35ca0035074f73bb202b153cc346f2f3" target="_blank">PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Knowledge and Data Engineering</p>
                <p><strong>Paper TL;DR:</strong> The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction, and in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.</p>
                <p><strong>Paper Abstract:</strong> This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios. We evaluate different SOTA numerical-based forecasting methods and language generation models. The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction. Additionally, in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9405.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9405.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptCast</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-based Time Series Forecasting (PromptCast)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm that converts numerical time-series inputs and outputs into natural-language prompts so that off-the-shelf language generation models can be fine-tuned (or used zero-shot) to perform forecasting; the paper proposes PromptCast and releases the PISA dataset and benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>language transformer (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numerical time series expressed as textual sequences (prompted lists of numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>weather temperature, electricity consumption, human mobility (POI visitor counts); (general time-series domains)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>proposed extension to anomaly detection (no experiments performed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Stated as a possible extension: transform numerical sequences or tabular/time-series data into text prompts and apply language models (fine-tuning or zero-shot prompting) to downstream tasks such as anomaly detection. No concrete prompting templates, scoring rules, or anomaly-detection algorithms are provided for this task in the paper — it is described conceptually as an application area for the PromptCast paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only a proposal/mention in Limitations/Future Work; the paper contains no experimental evaluation, metrics, or method recipes for anomaly detection. The authors note template bias and the need for automatic prompting and more work to design prompts suitable for multivariate or more complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Authors argue prompt-based framing brings auxiliary/contextual information and numerical tokens into a single token sequence, which may allow language models' sequence modeling and attention mechanisms to jointly learn relations useful for non-language downstream tasks (including anomaly detection). They highlight 'code-less' benefits, prompt-agnostic model reuse, and stronger zero-shot generalization observed for forecasting as motivating reasons why the approach could transfer to anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9405.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9405.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Negative-value decoding failure (Missing Rate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing-rate decoding failures for negative/edge numeric values in PromptCast outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An observed failure mode during PromptCast forecasting where some language models failed to generate decodable numerical outputs (reported as a nonzero Missing Rate), with failures linked to negative-valued ground truth on the City Temperature (CT) subset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProphetNet, Electra, BERT (as reported examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based language models (sequence-to-sequence / encoder-decoder variants for generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>ProphetNet ~1.6 GB (pretrained large, per Table VI), Electra ~135.0 MB, BERT ~440.5 MB (paper reports pretrained model sizes in MB/GB)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numerical time series expressed as textual context and question (prompt); single-step forecasting outputs as short textual answers (single numeric tokens or short strings)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>City Temperature subset (daily average temperatures, includes negative values)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>generation/decoding failure for negative numerical outputs (missing/undecodable output)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tuned language models were used to generate an output prompt containing the predicted numerical value; during evaluation the pipeline extracts numeric values by simple string parsing. Missing Rate counts instances where parsing fails because the model output omitted or malformed the numeric token (e.g., generated 'The temperature will be -' or 'The temperature will be .' with nothing following the negative sign).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Not an anomaly-detection baseline — this is an observed failure in the forecasting task. Numerical forecasting baselines (Transformers, Informer, Autoformer, FEDformer, LSTM, TCN, naive methods) were evaluated for forecasting but not as cures for this decoding failure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Missing Rate (percentage of test instances where numeric value could not be decoded), RMSE, MAE (for forecasting overall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On CT subset the paper reports small but nonzero Missing Rates for three language models: ProphetNet 0.412% ± 0.045, Electra 0.319% ± 0.068, BERT 0.244% ± 0.151; other evaluated language models had Missing Rate = 0. (Missing Rate is defined as (n_test - n_decoded)/n_test × 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Most language models produced zero Missing Rate; only a few showed small Missing Rates concentrated on negative-valued targets. This failure mode is specific to generation/decoding and is not reported for numerical forecasting baselines (which do not generate textual outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failure cases are concentrated on negative values: models sometimes omitted tokens after the '-' sign or otherwise produced undecodable outputs. The paper flags this as a limitation and suggests the dataset can be used to study and address negative-number generation and decoding robustness. No remedial methods (e.g., constrained decoding, numeric normalization strategies, special tokens for negative numbers) were tested or reported.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>The paper demonstrates that using simple string parsing to recover numeric predictions from generated text can reveal small but important failure modes (Missing Rate) tied to tokenization/generation of particular numeric forms (notably negatives). This is an actionable risk for applying language-model-based approaches to tasks involving edge-value numeric outputs (which would include anomaly detection on lists or tabular data where negative/rare numeric forms matter).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Lift: Language-interfaced fine-tuning for non-language machine learning tasks <em>(Rating: 2)</em></li>
                <li>A generalist agent <em>(Rating: 2)</em></li>
                <li>Translating human mobility forecasting through natural language generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9405",
    "paper_id": "paper-863171ed35ca0035074f73bb202b153cc346f2f3",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "PromptCast",
            "name_full": "Prompt-based Time Series Forecasting (PromptCast)",
            "brief_description": "A paradigm that converts numerical time-series inputs and outputs into natural-language prompts so that off-the-shelf language generation models can be fine-tuned (or used zero-shot) to perform forecasting; the paper proposes PromptCast and releases the PISA dataset and benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_type": "language transformer (general)",
            "model_size": null,
            "data_type": "numerical time series expressed as textual sequences (prompted lists of numbers)",
            "data_domain": "weather temperature, electricity consumption, human mobility (POI visitor counts); (general time-series domains)",
            "anomaly_type": "proposed extension to anomaly detection (no experiments performed in paper)",
            "method_description": "Stated as a possible extension: transform numerical sequences or tabular/time-series data into text prompts and apply language models (fine-tuning or zero-shot prompting) to downstream tasks such as anomaly detection. No concrete prompting templates, scoring rules, or anomaly-detection algorithms are provided for this task in the paper — it is described conceptually as an application area for the PromptCast paradigm.",
            "baseline_methods": null,
            "performance_metrics": null,
            "performance_results": null,
            "comparison_to_baseline": null,
            "limitations_or_failure_cases": "Only a proposal/mention in Limitations/Future Work; the paper contains no experimental evaluation, metrics, or method recipes for anomaly detection. The authors note template bias and the need for automatic prompting and more work to design prompts suitable for multivariate or more complex tasks.",
            "unique_insights": "Authors argue prompt-based framing brings auxiliary/contextual information and numerical tokens into a single token sequence, which may allow language models' sequence modeling and attention mechanisms to jointly learn relations useful for non-language downstream tasks (including anomaly detection). They highlight 'code-less' benefits, prompt-agnostic model reuse, and stronger zero-shot generalization observed for forecasting as motivating reasons why the approach could transfer to anomaly detection.",
            "uuid": "e9405.0",
            "source_info": {
                "paper_title": "PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Negative-value decoding failure (Missing Rate)",
            "name_full": "Missing-rate decoding failures for negative/edge numeric values in PromptCast outputs",
            "brief_description": "An observed failure mode during PromptCast forecasting where some language models failed to generate decodable numerical outputs (reported as a nonzero Missing Rate), with failures linked to negative-valued ground truth on the City Temperature (CT) subset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ProphetNet, Electra, BERT (as reported examples)",
            "model_type": "transformer-based language models (sequence-to-sequence / encoder-decoder variants for generation)",
            "model_size": "ProphetNet ~1.6 GB (pretrained large, per Table VI), Electra ~135.0 MB, BERT ~440.5 MB (paper reports pretrained model sizes in MB/GB)",
            "data_type": "numerical time series expressed as textual context and question (prompt); single-step forecasting outputs as short textual answers (single numeric tokens or short strings)",
            "data_domain": "City Temperature subset (daily average temperatures, includes negative values)",
            "anomaly_type": "generation/decoding failure for negative numerical outputs (missing/undecodable output)",
            "method_description": "Fine-tuned language models were used to generate an output prompt containing the predicted numerical value; during evaluation the pipeline extracts numeric values by simple string parsing. Missing Rate counts instances where parsing fails because the model output omitted or malformed the numeric token (e.g., generated 'The temperature will be -' or 'The temperature will be .' with nothing following the negative sign).",
            "baseline_methods": "Not an anomaly-detection baseline — this is an observed failure in the forecasting task. Numerical forecasting baselines (Transformers, Informer, Autoformer, FEDformer, LSTM, TCN, naive methods) were evaluated for forecasting but not as cures for this decoding failure.",
            "performance_metrics": "Missing Rate (percentage of test instances where numeric value could not be decoded), RMSE, MAE (for forecasting overall)",
            "performance_results": "On CT subset the paper reports small but nonzero Missing Rates for three language models: ProphetNet 0.412% ± 0.045, Electra 0.319% ± 0.068, BERT 0.244% ± 0.151; other evaluated language models had Missing Rate = 0. (Missing Rate is defined as (n_test - n_decoded)/n_test × 100%).",
            "comparison_to_baseline": "Most language models produced zero Missing Rate; only a few showed small Missing Rates concentrated on negative-valued targets. This failure mode is specific to generation/decoding and is not reported for numerical forecasting baselines (which do not generate textual outputs).",
            "limitations_or_failure_cases": "Failure cases are concentrated on negative values: models sometimes omitted tokens after the '-' sign or otherwise produced undecodable outputs. The paper flags this as a limitation and suggests the dataset can be used to study and address negative-number generation and decoding robustness. No remedial methods (e.g., constrained decoding, numeric normalization strategies, special tokens for negative numbers) were tested or reported.",
            "unique_insights": "The paper demonstrates that using simple string parsing to recover numeric predictions from generated text can reveal small but important failure modes (Missing Rate) tied to tokenization/generation of particular numeric forms (notably negatives). This is an actionable risk for applying language-model-based approaches to tasks involving edge-value numeric outputs (which would include anomaly detection on lists or tabular data where negative/rare numeric forms matter).",
            "uuid": "e9405.1",
            "source_info": {
                "paper_title": "PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Lift: Language-interfaced fine-tuning for non-language machine learning tasks",
            "rating": 2
        },
        {
            "paper_title": "A generalist agent",
            "rating": 2
        },
        {
            "paper_title": "Translating human mobility forecasting through natural language generation",
            "rating": 1
        }
    ],
    "cost": 0.0132985,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting</h1>
<p>Hao Xue, Flora D. Salim<br>School of Computer Science and Engineering, University of New South Wales, Sydney, Australia</p>
<h4>Abstract</h4>
<p>This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve timeseries forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-tosentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios. We evaluate different SOTA numerical-based forecasting methods and language generation models. The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction. Additionally, in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.</p>
<p>Index Terms-time series forecasting, natural language generation, dataset and benchmark</p>
<h2>I. INTRODUCTION</h2>
<p>Time series forecasting is a research-intensive field, especially with the increasing of applying various deep learning frameworks for prediction such as models based on LSTM [1], Temporal Convolutional Network (TCN) [2], and Transformer [3]. More recently, we are witnessing a fast growth of large-scale pre-trained models in the Natural Language Processing (NLP) field. These models, also known as foundation models [4], are often pre-trained with an extremely large amount of data and have demonstrated good performance across various downstream tasks. For example, BERT [5] can be adapted for multiple NLP tasks, CLIP [6] and GLIP [7] are good at CV tasks. However, we also notice that this evolution seems mostly limited to the NLP and CV fields. Hence, we are particularly interested in exploring the research question of whether we can take advantage of large-scale pre-trained foundation models and adapt these models for predicting time series. To investigate this question, in this paper, we formally introduce a novel task: prompt-based time series forecasting (PromptCast). The existing forecasting methods</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>including the state-of-the-art Transformer-based forecasting models [8], [9], [10], [11], [12], [13] can be simplified as a numerical forecasting paradigm as shown in Figure 1 (a). Numerical forecasting methods always take numerical values as input and generate numerical values as the prediction for the next time step. Instead, the input and output of the proposed prompt-based forecasting (Figure 1 (b)) are natural language sentences. This paradigm change enables the utilization of language generation models for forecasting.</p>
<p>This new forecasting paradigm is beneficial in multiple aspects. PromptCast presents a novel "code less" solution for time series forecasting, which could provide a new perspective rather than purely focusing on designing more and more complicated deep learning forecasting models (e.g., Transformerbased Informer, Autoformer, and FEDformer). It also becomes a relatively easy-accessible and user-friendly method for nonresearcher users, compared to existing forecasting models that require many tedious parameter searching and training processes, especially under a new forecasting scenario. As pointed out in a recent research [14], the benefits of using a single neural model across different tasks are significant. The PromptCast task explores the potential of using language foundation models for the forecasting task which could make it possible to broaden the language models beyond the realm of typical text-based tasks. In addition, it could inspire new research directions and new applications to better serve society. For example, as illustrated in Figure 1 (c), a chatbot with forecasting ability is one of the prospective future applications driven by the research of PromptCast task. Currently, although AI-powered intelligent assistants or chatbots like Siri and Alexa can answer queries about general topics, they still fail to answer specific time series forecasting questions. With the help of PromptCast related research, they would be able to yield predictions based on the given contexts.</p>
<p>To the best of our knowledge, this is the first effort in approaching general time-series forecasting from a languagebased perspective without any modifications on the model architectures, resulting also in the first large-scale dataset, PISA (Prompt based tIme Series forecAsting), tailored for the task of prompt-based time series forecasting. It covers three real-world forecasting scenarios: weather temperature forecasting, energy consumption forecasting, and customer flow forecasting. We believe that the release of this dataset will not only support the research of the PromptCast task but also have a great potential to stimulate related research in the time series analysis domain. We further develop a benchmark in which we report the forecasting performance of</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Conceptual illustrations of: (a) the typical framework of existing numerical-based forecasting method; (b) the framework of the proposed PromptCast; (c) a potential forecasting chatbot application based on PromptCast.</p>
<p>Multiple methods including both numerical-based forecasting methods and language generation models. To evaluate the generalization ability of PromptCast, this benchmark also explores various forecasting settings such as train-from-scratch, zero-shot prediction, multi-step forecasting, and multivariate forecasting. In summary, our contributions are three-fold: (1) We propose a novel prompt-based forecasting paradigm, which differs from the existing forecasting methods. This is the first time the general time series forecasting problem is addressed in a natural language generation manner. (2) We release a large-scale dataset (PISA) with 311,932 data instances in total for the newly introduced task. The dataset covers diverse time series forecasting scenarios. (3) We develop a benchmark (our data and codes for the benchmark are available at https://github.com/HaoUNSW/PISA) on the proposed PISA datasets. It evaluates the state-of-the-art numerical-based forecasting methods and popular language generation models.</p>
<h2>II. PROMPT-BASED TIME SERIES FORECASTING</h2>
<p>The prompt-based time series forecasting task is developed from the general time series task. Here we first describe the general numerical-based forecasting and then formulate the proposed PromptCast task. Let $$U = {U_1, U_2, \cdots, U_M}$$ denotes a set of M objects-of-interest. Depending on different specific forecasting scenarios, the objects-of-interest could stand for different objects. For example, the objects could be places-of-interest (POI) such as bars and parks in human mobility forecasting [15] or cities in weather forecasting. Under the general numerical time series forecasting task setting, the input is a historical record of interested numerical data points collected on t_obs continuous time steps (e.g., daily data): $$x_{t_1:t_{\text{obs}}} = [x_{t_1}^{m}, x_{t_2}^{m}, \cdots, x_{t_{\text{obs}}} \mid x_{t^m}]$$, where $$x_{t}^{m}$$ represents the value of object-of-interest U_m observed on time step t. The forecasting target (output) is the numerical data value $$x_{t_{\text{obs+1}}}^{m}, x_{t_{\text{obs+2}}}^{m}, \cdots, x_{t_{\text{obs+n}}}^{m}$$ of the next n time steps t_obs+1, t_obs+2, ..., t_obs+n (n is the prediction horizon).</p>
<p>The overarching goal of the PromptCast task is to leverage language foundation models to forecast time series in a sentence-to-sentence fashion. To achieve this goal, based on the above formulated problem of numerical time series forecasting, the numerical values need to be transferred and described as natural language sentences. This data-to-text transformation is referred to as a prompting process in this work (the details of prompting are presented in the next section). Specifically, the input numerical sequence $$x_{t_1:t_{\text{obs}}}^{m}$$ is turned into input prompts, and the forecasting target values are transformed as the output prompt. Consequently, time series forecasting can be addressed through a natural language generation paradigm, and language foundation models can be adopted as the core forecasting models in PromptCast task. For simplification, we primarily use the fundamental univariate time series single-step forecasting as the default setting to illustrate the concept of our novel PromptCast paradigm in the main paper. More discussion and experimental results of PromptCast in multi-step forecasting and multivariate forecasting are reported in Sec. IV-H and Sec. IV-I.</p>
<h2>III. DATASET DESIGN AND DESCRIPTION</h2>
<p>In this section, we demonstrate the design and construction of the proposed PISA dataset. The overall designing guideline is: (1) to preprocess original data given in the numerical format (raw data) for the forecasting task setting (Sec. III-A) and (2) to transform the numerical data to natural language input/output formats with prompts (Sec. III-B). We also describe the features and statistics (Sec. III-C).</p>
<h3>A. Data Sources and Processing</h3>
<p>To create a diverse dataset, we consider three real-world forecasting scenarios (3 sub-sets of our PISA dataset) from various domains: weather forecasting, energy consumption forecasting, and human mobility forecasting. The data sources for these scenarios are: <strong>City Temperature (CT)</strong>: This data source provides the daily average temperature (in Fahrenheit degrees) of multiple cities globally. 110 international cities are randomly selected to form the dataset. <strong>Electricity Consumption Load (ECL)</strong>: The original data includes the electricity consumption values (in Kwh) of 321 users. We eliminated users with incomplete records and randomly selected 50 users with complete data for the entire collection.</p>
<p><sup>1</sup>https://academic.udayton.edu/kissock/http/Weather/default.htm</p>
<p><sup>2</sup>https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014</p>
<p>TABLE I: TEMPLATES FOR TRANSFORMING PISA-NUMERICAL TO PISA-PROMPT.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th>Template</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>CT</td>
<td>Input Prompt (Source)</td>
<td>Context</td>
<td>From {t_{1}} to {t_{obs}}, the average temperature of region {U_{ns}} was {x_{t_{1}-t_{obs}}^{ns}} degree on each day.</td>
<td>From August 16, 2019, Friday to August 30, 2019, Friday, the average temperature of region 110 was 78, 81, 83, 84, 84, 82, 83, 78, 77, 77, 74, 77, 78, 73, 76 degree on each day.</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Question</td>
<td>What is the temperature going to be on {t_{obs+1}}?</td>
<td>What is the temperature going to be on August 31, 2019, Saturday?</td>
</tr>
<tr>
<td></td>
<td>Output Prompt (Target)</td>
<td>Answer</td>
<td>The temperature will be {x_{tobs+1}^{ns}} degree.</td>
<td>The temperature will be 78 degree.</td>
</tr>
<tr>
<td>ECL</td>
<td>Input Prompt (Source)</td>
<td>Context</td>
<td>From {t_{1}} to {t_{obs}}, client {U_{ns}} consumed {x_{t_{1}-t_{obs}}^{ns}} kWh of electricity on each day.</td>
<td>From May 16, 2014, Friday to May 30, 2014, Friday, client 50 consumed 8975, 9158, 8786, 8205, 7693, 7419, 7595, 7596, 7936, 7646, 7808, 7736, 7913, 8074, 8329 kWh of electricity on each day.</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Question</td>
<td>What is the consumption going to be on {t_{obs+1}}?</td>
<td>What is the consumption going to be on May 31, 2014, Saturday?</td>
</tr>
<tr>
<td></td>
<td>Output Prompt (Target)</td>
<td>Answer</td>
<td>This client will consume {x_{tobs+1}^{ns}} kWh of electricity.</td>
<td>This client will consume 8337 kWh of electricity.</td>
</tr>
<tr>
<td>SG</td>
<td>Input Prompt (Source)</td>
<td>Context</td>
<td>From {t_{1}} to {t_{obs}}, there were {x_{t_{1}-t_{obs}}^{ns}} people visiting POI {U_{ns}} on each day.</td>
<td>From May 23, 2021, Sunday to June 06, 2021, Sunday, there were 13, 17, 13, 20, 16, 16, 17, 17, 19, 20, 12, 12, 14, 12, 13 people visiting POI 324 on each day.</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Question</td>
<td>How many people will visit POI {U_{ns}} on {t_{obs+1}}?</td>
<td>How many people will visit POI 324 on June 07, 2021, Monday?</td>
</tr>
<tr>
<td></td>
<td>Output Prompt (Target)</td>
<td>Answer</td>
<td>There will be {x_{tobs+1}^{ns}} visitors.</td>
<td>There will be 15 visitors.</td>
</tr>
</tbody>
</table>
<p>period. Additionally, the hourly usage values for each chosen user were consolidated into daily usage data. SafeGraph Human Mobility Data (SG): This real-world human mobility data from SafeGraph Weekly Patterns ${ }^{3}$ contains the daily raw counts of visitors to POIs. We expanded the data collection from 5 months in [15] to almost 15 months and then randomly selected 324 POIs with full records. The exact data collection periods are reported in Table II. Following the standard protocol [11], [15], each sub-set is divided into train/val/test at the ratio of 7:1:2 by the chronological order (Table II). The numerical sequence of each object-of-interest in each sub-set is then split into multiple instances (for training/validation/test) by applying sliding windows. The window size equals $t_{\text {obs }}+1$ (including $t_{\text {obs }}$ time steps as input historical data and 1 step as the forecasting target) and the step size of the sliding window is 1 day. Specifically, following previous work [15], the observation length of the input sequence is set as $15\left(t_{\text {obs }}=15\right)$. To distinguish the numerical data used for numerical methods and the language-based dataset processed for language models, the numerical sequences processed by the above sliding window are referred to as PISA-numerical whereas the other is named as PISA-prompt (see next subsection). Ethical Considerations. The only possible sensitive information is the identifier of the object-of-interest (e.g., the POI id in SG). To remove this, we randomly assigned object-of-interest index $U_{n s}$ starting from 1 to $M$. Given that the original data source for each subset is aggregate statistics with no personally identifiable information, the generated PISA dataset does not contain any private information that can be deciphered.</p>
<h2>B. Template-Based Prompting</h2>
<p>The core of the proposed PromptCast task is shaping the time series forecasting in a language generation manner. To serve this purpose, a key step in building a dataset for PromptCast is to describe and transform the numerical sequential</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>data (i.e., PISA-numerical) to natural language sentences. As demonstrated in [15], using template-based description is an effective and efficient approach to achieve the data-to-text transformation. In this work, we explicitly introduce three templates for the three sub-sets and Table I lists the templates and the corresponding examples.</p>
<p>In a nutshell, the template consists of two main parts: input prompt and output prompt. The input prompt covers the description of the historical observation and the indicators of the prediction target time step (i.e., $t_{\text {obs }+1}$ ). The output prompt handles the desired prediction value ( $x_{t_{\text {obs }+1}^{s s}}^{s s}$ ) which is used as the ground truth label for training or evaluation. This input/output prompt setting is similar to the source/target sentence in machine translation. For researchers who are more familiar with the open question-answering setting, our PISA dataset can also be interpreted as a question-answering task setting. The input prompt can be broken into the context part and the question part. The context provides the historical information for forecasting and the question part can be seen as the input query about the future. Naturally, the output prompt is the ground truth answer that responds to the question. Based on the templates and the processed numerical sequences, PISA-prompt is then generated. Note that the instances in PISA-numerical map the instances in PISA-prompt. For example, the first instance in PISA-prompt is transferred from the first instance in PISA-numerical. This is to ensure that they can be used to compare the performance of numerical forecasting methods and the language-based forecasting methods in the benchmark. Our PISA-prompt provides the input prompts and the corresponding output prompts in separate files (e.g., val_x_prompt.txt and val_y_prompt.txt).</p>
<h2>C. Statistics Overview</h2>
<p>To highlight the diversity of PISA, we analyze its key statistics as given in Table II. PISA dataset comprises a total of 311,932 instances from three different forecasting application domains. Each sub-set has its distinct statistical characteristics. The last row of Table II lists the value range distributions</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The distribution plots of three sub-sets: (a) the City Temperature (CT) sub-set includes negative values; (b) the Electricity Consumption Load (ECL) sub-set covers large numbers spanning a wide range; and (c) the sub-set of SafeGraph Human Mobility Data (SG) involves relatively small values ranging from 0 to 400.</p>
<p>TABLE II: PISA DATASET OVERVIEW AND KEY STATISTICS.</p>
<table>
<thead>
<tr>
<th></th>
<th>CT</th>
<th>ECL</th>
<th>SG</th>
</tr>
</thead>
<tbody>
<tr>
<td>Objects-of-interest</td>
<td>110 cities</td>
<td>50 Users</td>
<td>324 POIs</td>
</tr>
<tr>
<td>Collection Period</td>
<td>From 2017/01/01</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2020/04/30</td>
<td>From 2012/01/01</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2014/12/31</td>
<td>From 2020/06/15</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2021/09/05</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Training Set</td>
<td>From 2017/01/01</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2019/04/30</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>850 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>91850 instances</td>
<td>From 2012/01/01</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2017/01/31</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>762 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>37350 instances</td>
<td>From 2020/06/15</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2021/04/23</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>313 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>96552 instances</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Validation Set</td>
<td>From 2019/05/01</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2019/08/31</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>123 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>11880 instances</td>
<td>From 2014/02/01</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2014/05/31</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>120 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>5250 instances</td>
<td>From 2021/04/24</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2021/06/07</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>45 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>9720 instances</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Test Set</td>
<td>From 2019/09/01</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2020/04/30</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>243 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>25080 instances</td>
<td>From 2014/06/01</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2014/12/31</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>214 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>9950 instances</td>
<td>From 2021/06/08</td>
<td></td>
<td></td>
</tr>
<tr>
<td>To 2021/09/05</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>90 days</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>24300 instances</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Value Range</td>
<td>[-44, 104]</td>
<td>[2799, 24906]</td>
<td>[3, 383]</td>
</tr>
<tr>
<td>Average Value</td>
<td>58.070</td>
<td>11479.120</td>
<td>29.355</td>
</tr>
</tbody>
</table>
<p>and the average value of each sub-set. Furthermore, Figure 2 shows the distribution plots of three sub-sets in our PISA dataset. It can be observed that the distributions of the three sub-sets are different. As shown in the figure and according to the value ranges reported in Table II, these three selected data sources ensure the data diversity of our PISA datasets. Specifically, the proposed PISA covers a range of negative numerical values (the CT sub-set), large values (the ECL sub-set), and small/regular values (the SG sub-set). This diversity of data in our dataset ensures the representativeness of our PISA dataset and provides a comprehensive benchmark for the performance of different models under different scenarios.</p>
<h2>IV. BENCHMARK</h2>
<p>In this section, we present our benchmarking study and analysis for the proposed PromptCast task. By conducting experiments on the established PISA dataset, we aim to address the following two main research questions: <em>RQ1</em>: Can we use language generation models to predict time series? Compared to the conventional numerical-based time-series forecasting methods, what is the performance of our PromptCast? <em>RQ2</em>: Can forecasting time series with prompts as well as using language generation models achieve better generalization ability?</p>
<h3>A. Evaluation Metrics</h3>
<p>While the proposed PromptCast task is a language generation task that aims to generate the target output prompts, we are particularly interested in evaluating the time series forecasting performance. To this end, the first step of the evaluation protocol is to extract the predicted numerical values from the generated sentences. Given that the output prompts follow the same template for each sub-set (<em>e.g.</em>, "There will be ..." in the SG sub-set), the numerical value can be easily extracted by simple string parsing. However, in practice, due to the uncertainty of the inference process, it is not guaranteed that the numerical value can be extracted from the generated output for every testing instance. To reflect this in the evaluation, we explicitly introduce the Missing Rate as one evaluation metric. It is defined as (n_{test} − n_{decoded})/n_{test} × 100% where n_{test} and n_{decoded} are the total number of instances in the test set and the number of generated instances that can successfully decode the predicted value, respectively. A lower Missing Rate indicates better performance.</p>
<p>After extracting the predicted numerical values, the evaluation of the PromptCast task will be similar to the evaluation of traditional numerical-based forecasting methods. Therefore, we also use two widely-used metrics, the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE), to evaluate the prediction performance in our benchmark. For each deep learning method, we report the average performance and the standard deviation over five runs with different random seeds. The Missing Rate is not considered when evaluating numerical-based methods as it is not applicable.</p>
<h3>B. Baselines</h3>
<p>In order to provide a comprehensive benchmark for the proposed PromptCast task, we test the performance of 10 popular natural language generation models on our PISA dataset (<em>i.e.</em>, PISA-prompt). These language models are T5 [16], Bart [17], BERT [5], RoBERTa [18], Electra [19], Bigbird [20], ProphetNet [21], LED [22], Blenderbot [23], and Pegasus [24]. Furthermore, for the comparison purpose (RQ1) and providing strong baselines for forecasting with prompts</p>
<p>TABLE III
RESULTS OF NUMERICAL-BASED FORECASTING METHODS ON PISA-NUMERICAL.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Temporal <br> Embedding</th>
<th style="text-align: center;">CT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SG</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">MAE</td>
</tr>
<tr>
<td style="text-align: center;">CY</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">6.710</td>
<td style="text-align: center;">4.991</td>
<td style="text-align: center;">680.142</td>
<td style="text-align: center;">381.247</td>
<td style="text-align: center;">10.945</td>
<td style="text-align: center;">7.691</td>
</tr>
<tr>
<td style="text-align: center;">HA</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">8.089</td>
<td style="text-align: center;">6.321</td>
<td style="text-align: center;">694.658</td>
<td style="text-align: center;">455.288</td>
<td style="text-align: center;">9.198</td>
<td style="text-align: center;">6.221</td>
</tr>
<tr>
<td style="text-align: center;">CLW</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">10.352</td>
<td style="text-align: center;">7.950</td>
<td style="text-align: center;">835.590</td>
<td style="text-align: center;">553.485</td>
<td style="text-align: center;">10.387</td>
<td style="text-align: center;">7.381</td>
</tr>
<tr>
<td style="text-align: center;">AutoARIMA</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">6.904</td>
<td style="text-align: center;">5.234</td>
<td style="text-align: center;">644.253</td>
<td style="text-align: center;">387.608</td>
<td style="text-align: center;">9.290</td>
<td style="text-align: center;">6.383</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$6.511 \pm 0.053$</td>
<td style="text-align: center;">$4.956 \pm 0.056$</td>
<td style="text-align: center;">598.962 $\pm 2.027$</td>
<td style="text-align: center;">367.798 $\pm 2.088$</td>
<td style="text-align: center;">$8.994 \pm 0.032$</td>
<td style="text-align: center;">$6.107 \pm 0.011$</td>
</tr>
<tr>
<td style="text-align: center;">TCN</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$6.397 \pm 0.089$</td>
<td style="text-align: center;">$4.876 \pm 0.072$</td>
<td style="text-align: center;">589.785 $\pm 6.280$</td>
<td style="text-align: center;">368.682 $\pm 6.077$</td>
<td style="text-align: center;">$8.389 \pm 0.029$</td>
<td style="text-align: center;">$5.927 \pm 0.039$</td>
</tr>
<tr>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">$6.790 \pm 0.072$</td>
<td style="text-align: center;">$5.238 \pm 0.058$</td>
<td style="text-align: center;">$612.102 \pm 25.081$</td>
<td style="text-align: center;">$400.182 \pm 24.956$</td>
<td style="text-align: center;">$8.230 \pm 0.029$</td>
<td style="text-align: center;">$5.851 \pm 0.023$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">$6.603 \pm 0.177$</td>
<td style="text-align: center;">$4.989 \pm 0.137$</td>
<td style="text-align: center;">$557.813 \pm 22.754$</td>
<td style="text-align: center;">$357.253 \pm 6.875$</td>
<td style="text-align: center;">$8.274 \pm 0.035$</td>
<td style="text-align: center;">$5.856 \pm 0.036$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">$6.873 \pm 0.143$</td>
<td style="text-align: center;">$5.294 \pm 0.108$</td>
<td style="text-align: center;">$567.307 \pm 10.261$</td>
<td style="text-align: center;">$394.226 \pm 8.900$</td>
<td style="text-align: center;">$8.408 \pm 0.274$</td>
<td style="text-align: center;">$5.940 \pm 0.103$</td>
</tr>
<tr>
<td style="text-align: center;">Informer</td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">$6.778 \pm 0.085$</td>
<td style="text-align: center;">$5.195 \pm 0.075$</td>
<td style="text-align: center;">$597.011 \pm 15.373$</td>
<td style="text-align: center;">$383.704 \pm 21.694$</td>
<td style="text-align: center;">$8.167 \pm 0.049$</td>
<td style="text-align: center;">$5.832 \pm 0.032$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">$6.457 \pm 0.268$</td>
<td style="text-align: center;">$4.922 \pm 0.209$</td>
<td style="text-align: center;">$536.921 \pm 33.375$</td>
<td style="text-align: center;">$349.331 \pm 11.916$</td>
<td style="text-align: center;">$8.151 \pm 0.068$</td>
<td style="text-align: center;">$5.868 \pm 0.049$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">$6.844 \pm 0.106$</td>
<td style="text-align: center;">$5.307 \pm 0.083$</td>
<td style="text-align: center;">$561.661 \pm 19.709$</td>
<td style="text-align: center;">$394.813 \pm 13.871$</td>
<td style="text-align: center;">$8.403 \pm 0.281$</td>
<td style="text-align: center;">$5.914 \pm 0.133$</td>
</tr>
<tr>
<td style="text-align: center;">Autoformer</td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">$6.681 \pm 0.094$</td>
<td style="text-align: center;">$5.040 \pm 0.081$</td>
<td style="text-align: center;">$608.499 \pm 9.051$</td>
<td style="text-align: center;">$384.782 \pm 9.361$</td>
<td style="text-align: center;">$8.180 \pm 0.020$</td>
<td style="text-align: center;">$5.831 \pm 0.017$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">$6.438 \pm 0.064$</td>
<td style="text-align: center;">$4.909 \pm 0.064$</td>
<td style="text-align: center;">$588.466 \pm 9.446$</td>
<td style="text-align: center;">$375.703 \pm 8.107$</td>
<td style="text-align: center;">$8.239 \pm 0.053$</td>
<td style="text-align: center;">$5.898 \pm 0.025$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">$6.812 \pm 0.091$</td>
<td style="text-align: center;">$5.200 \pm 0.072$</td>
<td style="text-align: center;">$593.071 \pm 3.476$</td>
<td style="text-align: center;">$393.695 \pm 2.385$</td>
<td style="text-align: center;">$8.392 \pm 0.220$</td>
<td style="text-align: center;">$6.044 \pm 0.158$</td>
</tr>
<tr>
<td style="text-align: center;">FEDformer</td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">$6.567 \pm 0.158$</td>
<td style="text-align: center;">$5.015 \pm 0.130$</td>
<td style="text-align: center;">$633.060 \pm 7.646$</td>
<td style="text-align: center;">$401.925 \pm 7.186$</td>
<td style="text-align: center;">$8.314 \pm 0.081$</td>
<td style="text-align: center;">$5.941 \pm 0.055$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">$6.358 \pm 0.050$</td>
<td style="text-align: center;">$4.841 \pm 0.029$</td>
<td style="text-align: center;">$596.240 \pm 13.169$</td>
<td style="text-align: center;">$403.764 \pm 12.324$</td>
<td style="text-align: center;">$8.214 \pm 0.013$</td>
<td style="text-align: center;">$5.913 \pm 0.024$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">$6.650 \pm 0.049$</td>
<td style="text-align: center;">$5.108 \pm 0.036$</td>
<td style="text-align: center;">$539.039 \pm 2.878$</td>
<td style="text-align: center;">$387.422 \pm 1.611$</td>
<td style="text-align: center;">$8.374 \pm 0.051$</td>
<td style="text-align: center;">$6.049 \pm 0.049$</td>
</tr>
</tbody>
</table>
<p>methods, we also include the performance of conventional numerical paradigm forecasting methods on the PISA-numerical. We consider 3 naive forecasting methods: Copy Yesterday (CY), Historical Average (HA), and Copy Last Week (CLW). We also consider 3 basic numerical forecasting methods: AutoARIMA, LSTM, and temporal convolutional network (TCN). Transformer-based forecasting methods including the vanilla Transformer [3], the state-of-the-art Informer [9], Autoformer [11], and FEDformer [12] are also included.</p>
<h2>C. Implementation Details</h2>
<p>For the evaluated numerical forecasting methods, the implementations are based on the official FEDformer ${ }^{4}$ repository which also includes the implementation of Transformer, Informer, and Autoformer. We follow the default hyperparameter settings except for the pred_len parameter (used in Informer, FEDformer, and Autoformer). For numerical methods, the data normalization process is also included when we processed the numerical data. In our experiments, the pred_len parameter is set to 7 (around half of the observation length, which is 15 in PISA). As for the hyperparameter searching, the factor $c$ (used in Informer and Autoformer) is the one that could affect the forecasting performance of numerical forecasting methods [11]). The default factor given by the official implementation is 3 and this default factor has also been used for different datasets according to the official implementations. This justifies that it is a reasonable choice when we evaluate Informer and Autoformer performance on our PISA dataset.</p>
<p>For the language models in the benchmark, their implementations can be grouped into two categories. The first category follows the EncoderDecoderModel ${ }^{5}$ framework. Three language models are implemented under this category: BERT, RoBERTa, Electra. The rest 7 language models are accomplished through the second category, which is the Condi-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tionalGeneration in HuggingFace (e.g., BartForConditionalGeneration class ${ }^{6}$ ). The fine-tuning process in PromptCast is based on the standard Trainer provided by HuggingFace. Specifically, the sequence-to-sequence trainer is applied as our downstream time series forecasting task in PromptCast is a sequence-to-sequence task. Additionally, no modifications are introduced to the loss function during fine-tuning. For decoding the generation from the language models, the standard tokenizers provided by HuggingFace are used to detokenize the direct output tokens to yield sentences and there are no extra regularization steps on the yielded sentences to acquire numerical predicted outputs. We simply decode the numerical values through string parsing.</p>
<p>We would like to emphasize that the language models and numerical forecasting methods are treated and processed equally and fairly in our benchmark. There is also no specific hyperparameter tuning for ourPromptCast. For both language models and numerical models, we all use the default settings provided/recommended by the official implementations. Thus, the comparison is fair and convincing. This hyperparameter searching-free characteristic could also reflect another benefit of PromptCast in real-world applications, that is, no need to conduct complicated and time-consuming hyperparameter tuning processes. The forecasting models can then be deployed more quickly for new forecasting scenarios. All the above scripts are provided in our GitHub ${ }^{7}$. The experiments were performed with PyTorch on a Linux server equipped with Nvidia V100 GPUs. In our experiments, only 1 GPU is enabled per run for each method.</p>
<h2>D. Experimental Performance</h2>
<p>1) Numerical-Based Methods: This part focuses on evaluating the typical numerical-based forecasting methods with</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>TABLE IV
RESULTS (RMSE AND MAE) OF OUR PROMPTCAST USING DIFFERENT LANGUAGE MODELS ON PISA-PROMPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE <br> mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">MAE <br> std</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE <br> mean</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE <br> std</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE <br> mean</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE <br> mean</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">6.499</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">4.830</td>
<td style="text-align: center;">0.038</td>
<td style="text-align: center;">527.425</td>
<td style="text-align: center;">10.280</td>
<td style="text-align: center;">353.450</td>
<td style="text-align: center;">2.696</td>
<td style="text-align: center;">8.450</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">5.879</td>
<td style="text-align: center;">0.020</td>
</tr>
<tr>
<td style="text-align: center;">Bart</td>
<td style="text-align: center;">6.432</td>
<td style="text-align: center;">0.040</td>
<td style="text-align: center;">4.759</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">527.350</td>
<td style="text-align: center;">10.608</td>
<td style="text-align: center;">355.390</td>
<td style="text-align: center;">2.751</td>
<td style="text-align: center;">8.279</td>
<td style="text-align: center;">0.053</td>
<td style="text-align: center;">5.785</td>
<td style="text-align: center;">0.023</td>
</tr>
<tr>
<td style="text-align: center;">Blenderbot</td>
<td style="text-align: center;">6.667</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">4.828</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">541.713</td>
<td style="text-align: center;">10.838</td>
<td style="text-align: center;">355.846</td>
<td style="text-align: center;">4.154</td>
<td style="text-align: center;">8.429</td>
<td style="text-align: center;">0.080</td>
<td style="text-align: center;">5.798</td>
<td style="text-align: center;">0.022</td>
</tr>
<tr>
<td style="text-align: center;">LED</td>
<td style="text-align: center;">6.376</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">4.730</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">540.924</td>
<td style="text-align: center;">16.542</td>
<td style="text-align: center;">367.276</td>
<td style="text-align: center;">6.742</td>
<td style="text-align: center;">8.277</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">5.787</td>
<td style="text-align: center;">0.036</td>
</tr>
<tr>
<td style="text-align: center;">Pegasus</td>
<td style="text-align: center;">6.379</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">4.727</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">537.186</td>
<td style="text-align: center;">11.296</td>
<td style="text-align: center;">361.135</td>
<td style="text-align: center;">4.728</td>
<td style="text-align: center;">8.289</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">5.817</td>
<td style="text-align: center;">0.013</td>
</tr>
<tr>
<td style="text-align: center;">ProphetNet</td>
<td style="text-align: center;">6.375</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">4.740</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">584.814</td>
<td style="text-align: center;">4.124</td>
<td style="text-align: center;">356.632</td>
<td style="text-align: center;">2.712</td>
<td style="text-align: center;">8.466</td>
<td style="text-align: center;">0.135</td>
<td style="text-align: center;">5.847</td>
<td style="text-align: center;">0.071</td>
</tr>
<tr>
<td style="text-align: center;">Bigbird</td>
<td style="text-align: center;">6.351</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">4.707</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">519.665</td>
<td style="text-align: center;">3.440</td>
<td style="text-align: center;">350.699</td>
<td style="text-align: center;">1.953</td>
<td style="text-align: center;">8.326</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">5.841</td>
<td style="text-align: center;">0.031</td>
</tr>
<tr>
<td style="text-align: center;">Electra</td>
<td style="text-align: center;">6.397</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">4.740</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">576.506</td>
<td style="text-align: center;">3.789</td>
<td style="text-align: center;">352.187</td>
<td style="text-align: center;">3.413</td>
<td style="text-align: center;">8.311</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">5.820</td>
<td style="text-align: center;">0.046</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">6.388</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">4.758</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">577.076</td>
<td style="text-align: center;">3.608</td>
<td style="text-align: center;">354.653</td>
<td style="text-align: center;">2.169</td>
<td style="text-align: center;">8.395</td>
<td style="text-align: center;">0.040</td>
<td style="text-align: center;">5.823</td>
<td style="text-align: center;">0.030</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">6.450</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">4.786</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">659.874</td>
<td style="text-align: center;">23.218</td>
<td style="text-align: center;">448.902</td>
<td style="text-align: center;">19.320</td>
<td style="text-align: center;">8.260</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">5.785</td>
<td style="text-align: center;">0.009</td>
</tr>
<tr>
<td style="text-align: center;">Numerical Best</td>
<td style="text-align: center;">6.358</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">4.841</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">536.921</td>
<td style="text-align: center;">33.375</td>
<td style="text-align: center;">349.331</td>
<td style="text-align: center;">11.916</td>
<td style="text-align: center;">8.151</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">5.831</td>
<td style="text-align: center;">0.017</td>
</tr>
</tbody>
</table>
<p>TABLE V
THE MISSING RATE PERFORMANCE OF LANGUAGE MODELS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Missing Rate <br> on CT (\%)</th>
<th style="text-align: center;">ProphetNet</th>
<th style="text-align: center;">Electra</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$0.412 \pm 0.045$</td>
<td style="text-align: center;">$0.319 \pm 0.068$</td>
<td style="text-align: center;">$0.244 \pm 0.151$</td>
</tr>
</tbody>
</table>
<p>TABLE VI
DETAILS OF HUGGINGFACE PRE-TRAINED MODELS</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">HuggingFace Key</th>
<th style="text-align: center;">Pretrained Model Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">t5-base</td>
<td style="text-align: center;">891.7 MB</td>
</tr>
<tr>
<td style="text-align: center;">Bart</td>
<td style="text-align: center;">facebook/bart-base</td>
<td style="text-align: center;">557.8 MB</td>
</tr>
<tr>
<td style="text-align: center;">Blenderbot</td>
<td style="text-align: center;">facebook/blenderbot_small-90M</td>
<td style="text-align: center;">350.4 MB</td>
</tr>
<tr>
<td style="text-align: center;">LED</td>
<td style="text-align: center;">allenai/led-base-16384</td>
<td style="text-align: center;">647.7 MB</td>
</tr>
<tr>
<td style="text-align: center;">Pegasus</td>
<td style="text-align: center;">google/pegasus-xsum</td>
<td style="text-align: center;">2.3 GB</td>
</tr>
<tr>
<td style="text-align: center;">ProphetNet</td>
<td style="text-align: center;">microsoft/prophetnet-large-uncased</td>
<td style="text-align: center;">1.6 GB</td>
</tr>
<tr>
<td style="text-align: center;">Bigbird</td>
<td style="text-align: center;">google/bigbird-pegasus-large-arxiv</td>
<td style="text-align: center;">2.3 GB</td>
</tr>
<tr>
<td style="text-align: center;">Electra</td>
<td style="text-align: center;">google/electra-base-generator</td>
<td style="text-align: center;">135.0 MB</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">bert-base-uncased</td>
<td style="text-align: center;">440.5 MB</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">roberta-base</td>
<td style="text-align: center;">501.2 MB</td>
</tr>
</tbody>
</table>
<p>TABLE V
THE MISSING RATE PERFORMANCE OF LANGUAGE MODELS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Missing Rate <br> on CT (\%)</th>
<th style="text-align: center;">ProphetNet</th>
<th style="text-align: center;">Electra</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$0.412 \pm 0.045$</td>
<td style="text-align: center;">$0.319 \pm 0.068$</td>
<td style="text-align: center;">$0.244 \pm 0.151$</td>
</tr>
</tbody>
</table>
<p>our PISA dataset. Normally, the position embeddings used in the Transformer architecture (and its variants) only contain the limited position information (e.g., the first time step in each input sequence). This kind of position information remains the same for all different input data instances. However, for time series data, temporal information (e.g., day-of-week and month-of-year) is an important cue for predicting future data and reflects the global position relations. For example, the first time step of instance A could correspond to Monday whereas the first time step (same position) of instance B could be Friday. Thus, appending the temporal embeddings to the basic position embedding becomes popular in Transformerbased time series forecasting methods. This is equivalent to providing temporal context in the input prompt (i.e., From $\left{t_{1}\right}$ to $\left{t_{\text {obs }}\right}$ in Table I) in PromptCast. Based on the implementations of Informer ${ }^{8}$ and Autoformer ${ }^{9}$, we fully investigate and benchmark three different embedding approaches, namely, timeF, fixed, and learned. For the three different temporal embeddings used in numerical methods benchmarking, we also follow the Autoformer implementations ${ }^{10}$. Basically, the time $F$ embedding is accomplished via nn.Linear() function and the fixed and learned are based on nn.Embedding() function. The fixed embedding has fixed non-trainable parameters (similar to the original sin/cos position embedding weight calculation in the vanilla Transformer) for the nn.Embedding() layer, whereas the parameters for the learned embedding are trainable. In this way, the three different embedding methods can be used for numerical forecasting methods to capture the temporal information in the input data and this offers a comprehensive evaluation of the temporal embeddings for the forecasting task.</p>
<p>Table III presents the performance of different methods with different temporal embedding policies. In general, FEDformer,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Informer and Autoformer achieve the best performance across different sub-sets. In most cases, these advanced time series forecasting frameworks outperform the vanilla Transformer, naive methods, and non-Transformer methods. Naive methods demonstrate worse forecasting performance compared to other methods, which is as expected. When comparing different embeddings, the fixed embedding demonstrates overall good performance. This embedding leads to good predictions on 5 out of 6 metrics and the time $F$ is the best performer of the remaining metric (MAE on SG). The learned embedding has the worst performance on CT and SG, but it beats the time $F$ on the ECL sub-set. These results suggest that the fixed embedding is a favorable approach for incorporating temporal cues.
2) Pre-trained Language Models: For language models investigated in the benchmark, the ready-to-use pre-trained weights provided by HuggingFace [25] are used for initialization. The configuration details are listed in Table VI and with the model key given in the table, the corresponding pretrained model can be accessed and downloaded from HuggingFace. It is worth noting that the pre-trained weights are trained with general English-language corpora datasets such as BookCorpus [26], CC-News [18], and OpenWebText [27]. These common language datasets are about general articles and do not include specific time series sequences orientated data. Although three data sources in PISA are open available, only the original numerical data (in the csv format) can be acquired online instead of the text format with our prompts.</p>
<p>TABLE VII
RESULTS OF NUMERICAL FORECASTING METHODS AND PROMPTCAST UNDER DIFFERENT SETTINGS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Temporal <br> Embedding</th>
<th style="text-align: center;">CT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
</tr>
<tr>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">75.465</td>
<td style="text-align: center;">1.330</td>
<td style="text-align: center;">73.238</td>
<td style="text-align: center;">1.473</td>
<td style="text-align: center;">11866.762</td>
<td style="text-align: center;">40.561</td>
<td style="text-align: center;">11288.860</td>
<td style="text-align: center;">41.504</td>
<td style="text-align: center;">29.010</td>
<td style="text-align: center;">2.554</td>
<td style="text-align: center;">18.903</td>
<td style="text-align: center;">1.087</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">67.964</td>
<td style="text-align: center;">12.021</td>
<td style="text-align: center;">65.991</td>
<td style="text-align: center;">12.311</td>
<td style="text-align: center;">5780.931</td>
<td style="text-align: center;">1432.223</td>
<td style="text-align: center;">5055.838</td>
<td style="text-align: center;">1836.453</td>
<td style="text-align: center;">52.461</td>
<td style="text-align: center;">17.611</td>
<td style="text-align: center;">47.150</td>
<td style="text-align: center;">21.680</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">48.691</td>
<td style="text-align: center;">14.586</td>
<td style="text-align: center;">40.968</td>
<td style="text-align: center;">17.008</td>
<td style="text-align: center;">7938.621</td>
<td style="text-align: center;">550.239</td>
<td style="text-align: center;">6982.758</td>
<td style="text-align: center;">647.932</td>
<td style="text-align: center;">28.238</td>
<td style="text-align: center;">1.348</td>
<td style="text-align: center;">18.719</td>
<td style="text-align: center;">1.743</td>
</tr>
<tr>
<td style="text-align: center;">Informer</td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">67.783</td>
<td style="text-align: center;">15.014</td>
<td style="text-align: center;">64.901</td>
<td style="text-align: center;">16.422</td>
<td style="text-align: center;">11887.368</td>
<td style="text-align: center;">30.596</td>
<td style="text-align: center;">11306.690</td>
<td style="text-align: center;">32.765</td>
<td style="text-align: center;">34.927</td>
<td style="text-align: center;">3.421</td>
<td style="text-align: center;">25.205</td>
<td style="text-align: center;">3.983</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">69.109</td>
<td style="text-align: center;">8.656</td>
<td style="text-align: center;">67.065</td>
<td style="text-align: center;">9.090</td>
<td style="text-align: center;">11180.022</td>
<td style="text-align: center;">296.532</td>
<td style="text-align: center;">10649.465</td>
<td style="text-align: center;">259.677</td>
<td style="text-align: center;">26.761</td>
<td style="text-align: center;">2.290</td>
<td style="text-align: center;">15.930</td>
<td style="text-align: center;">1.857</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">45.517</td>
<td style="text-align: center;">17.482</td>
<td style="text-align: center;">38.000</td>
<td style="text-align: center;">17.228</td>
<td style="text-align: center;">11509.084</td>
<td style="text-align: center;">113.513</td>
<td style="text-align: center;">10923.215</td>
<td style="text-align: center;">114.072</td>
<td style="text-align: center;">27.417</td>
<td style="text-align: center;">2.241</td>
<td style="text-align: center;">17.310</td>
<td style="text-align: center;">1.471</td>
</tr>
<tr>
<td style="text-align: center;">Autoformer</td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">52.814</td>
<td style="text-align: center;">5.002</td>
<td style="text-align: center;">39.577</td>
<td style="text-align: center;">5.842</td>
<td style="text-align: center;">694.693</td>
<td style="text-align: center;">2.715</td>
<td style="text-align: center;">455.658</td>
<td style="text-align: center;">2.188</td>
<td style="text-align: center;">38.710</td>
<td style="text-align: center;">11.207</td>
<td style="text-align: center;">30.857</td>
<td style="text-align: center;">9.751</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">47.691</td>
<td style="text-align: center;">5.329</td>
<td style="text-align: center;">34.531</td>
<td style="text-align: center;">2.996</td>
<td style="text-align: center;">674.641</td>
<td style="text-align: center;">1.845</td>
<td style="text-align: center;">440.564</td>
<td style="text-align: center;">1.678</td>
<td style="text-align: center;">36.801</td>
<td style="text-align: center;">3.523</td>
<td style="text-align: center;">28.637</td>
<td style="text-align: center;">1.927</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">83.349</td>
<td style="text-align: center;">9.332</td>
<td style="text-align: center;">59.951</td>
<td style="text-align: center;">7.855</td>
<td style="text-align: center;">693.810</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">454.691</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">56.787</td>
<td style="text-align: center;">3.050</td>
<td style="text-align: center;">40.890</td>
<td style="text-align: center;">2.004</td>
</tr>
<tr>
<td style="text-align: center;">FEDformer</td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">63.851</td>
<td style="text-align: center;">4.729</td>
<td style="text-align: center;">46.117</td>
<td style="text-align: center;">4.608</td>
<td style="text-align: center;">693.017</td>
<td style="text-align: center;">2.127</td>
<td style="text-align: center;">454.284</td>
<td style="text-align: center;">1.983</td>
<td style="text-align: center;">50.252</td>
<td style="text-align: center;">8.780</td>
<td style="text-align: center;">40.091</td>
<td style="text-align: center;">8.115</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">77.699</td>
<td style="text-align: center;">3.711</td>
<td style="text-align: center;">54.176</td>
<td style="text-align: center;">4.005</td>
<td style="text-align: center;">655.196</td>
<td style="text-align: center;">3.142</td>
<td style="text-align: center;">424.823</td>
<td style="text-align: center;">2.603</td>
<td style="text-align: center;">64.622</td>
<td style="text-align: center;">5.056</td>
<td style="text-align: center;">45.391</td>
<td style="text-align: center;">2.996</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">239.426</td>
<td style="text-align: center;">24.961</td>
<td style="text-align: center;">146.535</td>
<td style="text-align: center;">21.858</td>
<td style="text-align: center;">694.019</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">454.866</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">108.169</td>
<td style="text-align: center;">8.851</td>
<td style="text-align: center;">85.243</td>
<td style="text-align: center;">6.055</td>
</tr>
<tr>
<td style="text-align: center;">Train-from-Scratch</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Bart)</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">6.886</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">5.130</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">546.881</td>
<td style="text-align: center;">8.904</td>
<td style="text-align: center;">371.102</td>
<td style="text-align: center;">4.369</td>
<td style="text-align: center;">8.923</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">6.000</td>
<td style="text-align: center;">0.174</td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Pegasus)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$6.791^{*}$</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">5.043</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">632.351</td>
<td style="text-align: center;">14.745</td>
<td style="text-align: center;">411.915</td>
<td style="text-align: center;">6.316</td>
<td style="text-align: center;">9.484</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">6.180</td>
<td style="text-align: center;">0.078</td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Bigbird)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.643</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">4.964</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">639.889</td>
<td style="text-align: center;">8.340</td>
<td style="text-align: center;">416.529</td>
<td style="text-align: center;">4.021</td>
<td style="text-align: center;">10.529</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">6.365</td>
<td style="text-align: center;">0.031</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Bart)</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">7.379</td>
<td style="text-align: center;">0.086</td>
<td style="text-align: center;">5.501</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">660.082</td>
<td style="text-align: center;">16.205</td>
<td style="text-align: center;">493.035</td>
<td style="text-align: center;">18.166</td>
<td style="text-align: center;">8.592</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">5.961</td>
<td style="text-align: center;">0.038</td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Pegasus)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.918</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">5.178</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">643.483</td>
<td style="text-align: center;">16.536</td>
<td style="text-align: center;">446.876</td>
<td style="text-align: center;">5.822</td>
<td style="text-align: center;">9.293</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">6.116</td>
<td style="text-align: center;">0.041</td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Bigbird)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7.070</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">5.248</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">665.191</td>
<td style="text-align: center;">55.176</td>
<td style="text-align: center;">417.634</td>
<td style="text-align: center;">4.815</td>
<td style="text-align: center;">9.439</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">6.289</td>
<td style="text-align: center;">0.027</td>
</tr>
</tbody>
</table>
<ul>
<li>Pegasus Missing Rate (\%) on CT: $2.482 \pm 3.754$</li>
</ul>
<p>This also guarantees that the data from these three datasets are not used in pre-training and prevents the potential data leakage. In the experiments, each language model is fine-tuned with the training set of each sub-set in PISA.</p>
<p>The prediction results (RMSE and MAE) of using different language generation models on PISA dataset are listed in Table IV. According to the table, the top performers (shown in bold) include Bigbird, Bart, and RoBERTa. Bigbird achieves the best performance on 4 out of 6 metrics. When we jointly consider Table III and Table IV, it can be seen that using language models performs reasonably well on the CT and ECL sub-sets. For ECL, although the MAE of using language models is slightly worse than the best performer in Table III, the RMSE has a relatively large improvement. Compared with numerical methods, using language models can also yield comparable results on SG. This benchmark answers RQ1 and indicates that prompt-based forecasting with language models is a promising direction for time series forecasting research.</p>
<p>Table V reports the Missing Rate metric. It is clearly noticeable that only three methods (ProphetNet, Electra, and BERT) have a tiny amount (less than $0.5 \%$ ) of missing cases and all cases appear on the CT sub-set. For other methods that are not listed in Table V, the missing rates are all 0 . We further investigate the output sentences that cannot be decoded and find out that the failure cases are related and potentially caused by negative values. For example, the failure generated sentences are like the temperature will be . . . where the models fail to generate the tokens after "-". Our PISA dataset is valuable in supporting the research directions to address this limitation in the future. For more qualitative examples, we provide all generated examples of language
models in our GitHub repository ${ }^{11}$ including all the three test sets with various settings such as with pre-trained weights, train-from-scratch, zero-shot, prompt ablations, and multi-step forecasting.
3) Performance of ChatGPT: To showcase the time series forecasting capabilities of recent GPT models, we employed the OpenAI API to assess the performance of GPT-3.5 ("gpt-3.5-turbo", accessed in August 2023) using identical PISA sub-sets. Given the prohibitively high expenses associated with using GPT-4 and fine-tuning GPT-3.5 services offered by the OpenAI API, this section of our experiment focused on GPT3.5 without fine-tuning. During the evaluation process, we employed the input prompts from the testing sets in PISA as queries and collected the responses from the API. The resulting performance of GPT-3.5 is summarized in Table VIII. When compared to the results reported in the main paper for numerical forecasting methods and our PromptCast, GPT-3.5 exhibits higher RMSE and MAE values across all three subsets. From the table, we can also clearly notice that the GPT3.5 has large Missing Rates, especially on the SG sub-set. We observed numerous outputs containing text such as "it is difficult to predict the consumption" for the ECL sub-set and "we cannot determine the number of people" for the SG sub-set. In most cases that GPT-3.5 can make predictions, the predicted values are simply the averaging of the input values of the observation period. The complete set of outputs for the three sub-sets can be found in our GitHub repository for further detailed inspection.
4) Computation Cost: To further enrich the manuscript, we have compared the number of parameters, the inference execution time (per instance, in seconds), and the deployment cost</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>TABLE VIII
RESULTS OF GPT-3.5 THROUGH THE OPENAI API.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">RMSE</th>
<th style="text-align: center;">MAE</th>
<th style="text-align: center;">MissingRate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">10.349</td>
<td style="text-align: center;">6.424</td>
<td style="text-align: center;">0.889\%</td>
</tr>
<tr>
<td style="text-align: center;">ECL</td>
<td style="text-align: center;">9413.488</td>
<td style="text-align: center;">1321.210</td>
<td style="text-align: center;">9.879\%</td>
</tr>
<tr>
<td style="text-align: center;">SG</td>
<td style="text-align: center;">21.142</td>
<td style="text-align: center;">6.827</td>
<td style="text-align: center;">82.819\%</td>
</tr>
</tbody>
</table>
<p>TABLE IX
THE COMPARISON OF DEPLOYMENT COST IN USD AND INFERENCE SPEED</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"># params</th>
<th style="text-align: center;">CT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SG</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">time seconds</td>
<td style="text-align: center;">cost $\times 10^{-5}$</td>
<td style="text-align: center;">time seconds</td>
<td style="text-align: center;">cost $\times 10^{-5}$</td>
<td style="text-align: center;">time seconds</td>
<td style="text-align: center;">cost $\times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">10.6 M</td>
<td style="text-align: center;">0.058</td>
<td style="text-align: center;">4.902</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">5.303</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">4.704</td>
</tr>
<tr>
<td style="text-align: center;">Informer</td>
<td style="text-align: center;">11.4 M</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">10.385</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">8.668</td>
<td style="text-align: center;">0.096</td>
<td style="text-align: center;">8.134</td>
</tr>
<tr>
<td style="text-align: center;">Autoformer</td>
<td style="text-align: center;">10.6 M</td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">8.077</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">5.976</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">5.391</td>
</tr>
<tr>
<td style="text-align: center;">FEDformer</td>
<td style="text-align: center;">11.0 M</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">4.113</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">3.524</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">7.401</td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Bart)</td>
<td style="text-align: center;">139.4 M</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">3.757</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">3.033</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">7.468</td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Bigbird)</td>
<td style="text-align: center;">576.9 M</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">5.489</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">4.393</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">11.046</td>
</tr>
<tr>
<td style="text-align: center;">PromptCast (Pegasus)</td>
<td style="text-align: center;">568.7 M</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">5.415</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">4.337</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">10.785</td>
</tr>
<tr>
<td style="text-align: center;">GPT3.5</td>
<td style="text-align: center;">175 B</td>
<td style="text-align: center;">5.811</td>
<td style="text-align: center;">54.047</td>
<td style="text-align: center;">5.872</td>
<td style="text-align: center;">59.311</td>
<td style="text-align: center;">3.514</td>
<td style="text-align: center;">33.334</td>
</tr>
</tbody>
</table>
<p>(per instance, measured in $\times 10^{-5}$ USD) of the Transformerbased numerical forecasting methods, our PromptCast with different language models, and the GPT-3.5 (the model for ChatGPT). We assessed GPT-3.5's costing performance utilizing the OpenAI API (accessed in August 2023) with the "gpt-3.5-turbo" model. For the remaining methods, we estimated deployment costs based on the utilization of Nvidia V100 GPUs on AWS (i.e., p3.2xlarge instance with one Nvidia V100 is 3.06 USD per hour ${ }^{12}$ ). The costs of these models are associated with the computation time. For GPT-3.5, it is costed based on the number of input and output tokens, with run time including API request and response durations.</p>
<p>Table IX provides a comprehensive comparison of various models on the three sub-sets in PISA. As demonstrated in the table, noticeably, GPT-3.5 (OpenAI API) stands out as an exceptionally large and resource-intensive model, with significantly higher computational requirements and associated costs compared to other models, making it less cost-effective. For our proposed PromptCast, although the language models featuring more parameters, there is no substantial increase in computation times and associated costs when contrasted with Transformer-based numerical forecasting models. This computation cost comparison further shows PromptCast is not only effective on the forecasting accuracy but also costeffective.</p>
<h2>E. Training From Scratch</h2>
<p>Another question we aim to explore is whether the language models in PromptCast can still be used for time series forecasting without the use of pre-trained weights. Hence, we disable the pre-trained weights and train language models from scratch with the training set of PISA dataset. We select three language models that exhibit excellent forecasting performance on all three subsets (according to Table IV) for examination in this part of the experiment: Bart, Pegasus, and Bigbird. The results are reported in Table VII. Comparing the results of using pre-training weights (Table IV), we note that there is</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>a performance decrease for each method when pre-trained weights are not loaded, suggesting that utilizing pre-trained weights could result in further performance improvement. However, even training from scratch, the language generation models (especially Bart) still can yield comparable prediction results compared to numerical-based methods (Table III). This demonstrates that our proposed PromptCast is robust and not entirely reliant on pre-trained weights. While training a model from scratch does yield acceptable predictions, leveraging the pre-trained weights of language models can significantly enhance forecasting performance. From the comparison, we can also notice that the influence of without using pretrained weights has less impact on Bart, whereas the performance decrease in the case of Pegasus and Bigbird is more substantial. This disparity can be attributed to the marked difference in model size between Bart, Pegasus, and Bigbird, as evidenced in Table VI. Bart's significantly smaller model size indicates a lower number of parameters that require training. Consequently, under the train-from-scratch setting (i.e., without using pre-trained weights), training models with a larger number of parameters becomes relatively challenging, especially when considering the same volume of training data.</p>
<h2>F. Zero-shot Performance</h2>
<p>To further explore the language models for prompt-based forecasting, we conduct an experiment under the zero-shot setting (see Table VII). Specifically, we fine-tune each method on two sub-sets and test the fine-tuned model on the test set of the left sub-set (e.g., fine-tune with the training sets of CT and ECL, test on the test set of SG). For the comparison purpose, we also evaluate the Transformer-based numerical methods under the same zero-shot setting and the results are given in Table VII (the upper part). Similar to the results given in Table III (the normal training setting), the fixed embedding has better performance ( 5 out of 6 metrics) than the other two embeddings under this challenging zero-shot setting. Since the date range of the three sub-sets are dissimilar (Table II), learnable learned and timeF temporal embeddings would lead to inferior performance when transferred to unseen scenarios as the learned embeddings may not be appropriate for the new scenarios.</p>
<p>Except for the Autoformer and FEDformer on ECL, the numerical-based methods fail to produce satisfactory predictions under the zero-shot setting. Given the distinct characteristics of the three subsets, such poor performance is as expected for numerical methods. However, for the language models, although the performance is lower than the standard setting and the train-from-scratch setting, prompt-based forecasting can still generate reasonable predictions. This demonstrates a strong generalization ability when time series forecasting is addressed with prompts (RQ2). This strong zero-shot ability could bring advantages in real-world forecasting applications such as rapid deployment for new forecasting scenarios and cold-start forecasting for scenarios without any historical data. In the future, prompt-based forecasting could also enable the exploration of more complex forecasting scenarios such as predicting energy consumption based on weather temperature or forecasting customer traffic with temperature trends.</p>
<p>TABLE X
RESULTS OF OUR PROMPTCAST WITH DIFFERENT PROMPT TYPES.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">CT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
</tr>
<tr>
<td style="text-align: center;">Basic</td>
<td style="text-align: center;">PromptCast (Bart)</td>
<td style="text-align: center;">6.512</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">4.790</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">603.571</td>
<td style="text-align: center;">2.461</td>
<td style="text-align: center;">380.583</td>
<td style="text-align: center;">1.695</td>
<td style="text-align: center;">8.677</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">5.912</td>
<td style="text-align: center;">0.013</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PromptCast (Pegasus)</td>
<td style="text-align: center;">6.496</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">4.767</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">595.942</td>
<td style="text-align: center;">2.246</td>
<td style="text-align: center;">366.677</td>
<td style="text-align: center;">1.373</td>
<td style="text-align: center;">8.530</td>
<td style="text-align: center;">0.032</td>
<td style="text-align: center;">5.879</td>
<td style="text-align: center;">0.010</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PromptCast (Bigbird)</td>
<td style="text-align: center;">6.478</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">4.752</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">609.315</td>
<td style="text-align: center;">3.071</td>
<td style="text-align: center;">378.100</td>
<td style="text-align: center;">2.112</td>
<td style="text-align: center;">8.576</td>
<td style="text-align: center;">0.035</td>
<td style="text-align: center;">5.922</td>
<td style="text-align: center;">0.009</td>
</tr>
<tr>
<td style="text-align: center;">Minimum</td>
<td style="text-align: center;">PromptCast (Bart)</td>
<td style="text-align: center;">6.564</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">4.830</td>
<td style="text-align: center;">0.083</td>
<td style="text-align: center;">612.487</td>
<td style="text-align: center;">3.660</td>
<td style="text-align: center;">382.931</td>
<td style="text-align: center;">2.509</td>
<td style="text-align: center;">8.686</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">5.937</td>
<td style="text-align: center;">0.021</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PromptCast (Pegasus)</td>
<td style="text-align: center;">6.560</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">4.818</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">597.459</td>
<td style="text-align: center;">3.763</td>
<td style="text-align: center;">368.174</td>
<td style="text-align: center;">1.178</td>
<td style="text-align: center;">8.632</td>
<td style="text-align: center;">0.046</td>
<td style="text-align: center;">5.915</td>
<td style="text-align: center;">0.016</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PromptCast (Bigbird)</td>
<td style="text-align: center;">6.496</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">4.766</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">618.907</td>
<td style="text-align: center;">4.422</td>
<td style="text-align: center;">384.327</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">8.774</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">5.950</td>
<td style="text-align: center;">0.015</td>
</tr>
</tbody>
</table>
<p>TABLE XI
BASIC PROMPTING TEMPLATES.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Input</td>
<td style="text-align: left;">The average temperature of was $\left{x_{T_{1} \text {; } \mathrm{f}_{\text {obs }}}^{m}\right}$ degree on each day. <br> What is the temperature going to be on tomorrow?</td>
</tr>
<tr>
<td style="text-align: left;">Output</td>
<td style="text-align: left;">The temperature will be $\left{x_{T_{\text {obs }+1}}^{m}\right}$ degree.</td>
</tr>
<tr>
<td style="text-align: left;">Input</td>
<td style="text-align: left;">The client consumed $\left{x_{T_{1} \text {; } \mathrm{f}_{\text {obs }}}^{m}\right}$ kWh of electricity on each day. <br> What is the consumption going to be on tomorrow?</td>
</tr>
<tr>
<td style="text-align: left;">Output</td>
<td style="text-align: left;">This client will consume $\left{x_{T_{\text {obs }+1}}^{m}\right}$ kWh of electricity.</td>
</tr>
<tr>
<td style="text-align: left;">Input</td>
<td style="text-align: left;">There were $\left{x_{T_{1} \text {; } \mathrm{f}_{\text {obs }}}^{m}\right}$ people visiting the POI on each day. <br> How many people will visit the POI on tomorrow?</td>
</tr>
<tr>
<td style="text-align: left;">Output</td>
<td style="text-align: left;">There will be $\left{x_{T_{\text {obs }+1}}^{m}\right}$ visitors.</td>
</tr>
</tbody>
</table>
<h2>G. Prompts Ablation Study</h2>
<p>To further investigate the prompts in the PromptCast task, we conduct an ablation study on our prompting templates and two simplified prompts are developed: (1) Basic Prompt: as shown in Table XI, we remove auxiliary information (e.g., date information) and only keep the core information (i.e., the historical observation values $x_{T_{1} ; t_{\text {obs }}}^{m}$ ) in the input prompts. The output prompts remain the same. (2) Minimum Prompt: This is the simplest and the most straightforward version of prompts. We use commas to convert the sequential numerical values $x_{T_{1} ; t_{\text {obs }}}^{m}$ into comma-delimited strings as the input prompts (e.g., "78, 81, 83, 84, 84, 82, 83, 78, 77, 77, 74, 77, $78,73,76$ "). For the output prompts, the prediction targets $x_{T_{\text {obs }+1}}^{m}$ are directly used as single-word strings (e.g., ' ' 78 ' ').</p>
<p>The prediction results of using these two types of prompts on our PISA dataset are presented in Table X. We can notice that both two simplified prompts lead to worse performance compared to the default template reported in the main paper (Table I). While the minimum prompts can produce acceptable outcomes, the basic prompts have been found to have superior performance compared to the minimum prompts with the same language model. These comparison results demonstrate that: (1) including proper contexts (even if these contexts introduce no extra data, e.g., the basic prompt) in the prompt is beneficial; and (2) the auxiliary information is a significant component in the prompt for better prediction performance.</p>
<h2>H. Multi-step Forecasting</h2>
<p>The proposed language foundation models-based PromptCast forecasting paradigm is also suitable for multi-step forecasting scenarios. For the multi-step forecasting (i.e., larger
prediction horizon $n$ ), the question part of the input prompt template can be updated to "how many people will visit POI in the next Y days?" and the output prompt is also needed to be revised to reflect the multi-step future data values such as "For day $X$ to day $Y$, there will be $A, B, C, \ldots$ visitors." Here is an example of predicting the next 7 -time steps $(n=7)$ with the SG sub-set: Input Prompt "From April 24, 2021, Saturday to May 08, 2021, Saturday, there were 6, 8, 6, $15,12,4,13,7,8,12,16,9,11,18,10$ people visiting POI 1 on each day. How many people will visit POI 1 in the next 7 days?" and Output Prompt "From May 09, 2021, Sunday to May 15, 2021, Saturday, there will be 11, 11, 8, 13, 10, 18, 19 visitors." Building on these modified prompts, we further examine the performance of our PromptCast with three language models (Bart, Bigbird, Pegasus) when using varying observation lengths (input lengths) and prediction horizons (output lengths). Specifically, we test the models using observation length settings of 7 days and 15 days, and prediction horizons of 1 day, 4 days, and 7 days, resulting in a total of six length configuration combinations. To handle the multitude of experiments involving different length setting combinations, we exclusively present the results for Transformer-based numerical forecasting methods using the "fixed" temporal embedding. The performance of these three language models on these multi-step forecasting settings are listed in Table XII.</p>
<p>From the table, we can see that PromptCast can be easily adapted to multi-step forecasting while using the exact same forecasting model architectures (i.e., the language foundation models) for different settings. Generally, compared to the single-step forecasting performance, increasing the prediction horizon leads to larger missing rates. Compared to Transformer-based methods, language models exhibit less favorable predictions in the context of "observing 7 days to predict 7 days" and "observing 15 days to predict 7 days," which are notably challenging scenarios. Bart, in particular, displays suboptimal performance, as evidenced by substantial missing rates across various subsets. However, both Bigbird and Pegasus manage to deliver reasonably accurate forecasts with extremely low missing rates in these difficult settings. Especially, the performance of Pegasus on these two settings are quite close to the state-of-the-art Transformer-based methods. Additionally, we notice that using a 15-day observation period often leads to better performance when predicting the same future length. This is likely because a 15-day period</p>
<p>TABLE XII
RESULTS OF APPLYING DIFFERENT LENGTH SETTINGS.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Obs</td>
<td>Pred</td>
<td></td>
<td>RMSE</td>
<td>CT <br> MAE</td>
<td>MissingRate</td>
<td>RMSE</td>
<td>$\begin{gathered} \text { AUX } \ \text { MAE } \end{gathered}$</td>
<td>MissingRate</td>
<td>RMSE</td>
<td>$\begin{gathered} \text { AU } \ \text { MAE } \end{gathered}$</td>
<td>MissingRate</td>
</tr>
<tr>
<td>7 Days</td>
<td>1 Day</td>
<td>Transformer</td>
<td>6.717 $\pm 0.094$</td>
<td>5.144 $\pm 0.066$</td>
<td>n/a</td>
<td>545.909 $\pm 18.380$</td>
<td>358.854 $\pm 3.179$</td>
<td>n/a</td>
<td>8.270 $\pm 0.021$</td>
<td>5.906 $\pm 0.009$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Informer</td>
<td>6.779 $\pm 0.082$</td>
<td>5.171 $\pm 0.073$</td>
<td>n/a</td>
<td>540.269 $\pm 31.190$</td>
<td>358.886 $\pm 14.493$</td>
<td>n/a</td>
<td>8.211 $\pm 0.122$</td>
<td>5.884 $\pm 0.039$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Autoformer</td>
<td>7.152 $\pm 0.099$</td>
<td>5.472 $\pm 0.067$</td>
<td>n/a</td>
<td>669.884 $\pm 18.113$</td>
<td>467.813 $\pm 19.735$</td>
<td>n/a</td>
<td>9.594 $\pm 0.466$</td>
<td>7.687 $\pm 0.412$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>FEDformer</td>
<td>6.378 $\pm 0.019$</td>
<td>4.825 $\pm 0.018$</td>
<td>n/a</td>
<td>669.981 $\pm 15.411$</td>
<td>442.723 $\pm 16.427$</td>
<td>n/a</td>
<td>8.677 $\pm 0.059$</td>
<td>6.228 $\pm 0.051$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Burt)</td>
<td>6.762 $\pm 0.090$</td>
<td>5.017 $\pm 0.083$</td>
<td>0</td>
<td>527.772 $\pm 7.282$</td>
<td>360.066 $\pm 3.688$</td>
<td>0</td>
<td>8.427 $\pm 0.016$</td>
<td>5.909 $\pm 0.023$</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Bighbub)</td>
<td>6.425 $\pm 0.024$</td>
<td>4.716 $\pm 0.013$</td>
<td>0</td>
<td>530.929 $\pm 4.792$</td>
<td>365.122 $\pm 2.184$</td>
<td>0</td>
<td>8.526 $\pm 0.052$</td>
<td>5.962 $\pm 0.026$</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Pegasus)</td>
<td>6.820 $\pm 0.044$</td>
<td>5.088 $\pm 0.031$</td>
<td>0</td>
<td>527.735 $\pm 5.921$</td>
<td>360.702 $\pm 2.352$</td>
<td>0</td>
<td>8.406 $\pm 0.063$</td>
<td>5.957 $\pm 0.028$</td>
<td>0</td>
</tr>
<tr>
<td>7 Days</td>
<td>4 Days</td>
<td>Transformer</td>
<td>9.084 $\pm 0.171$</td>
<td>7.043 $\pm 0.132$</td>
<td>n/a</td>
<td>722.580 $\pm 19.818$</td>
<td>472.678 $\pm 14.923$</td>
<td>n/a</td>
<td>8.666 $\pm 0.029$</td>
<td>6.117 $\pm 0.024$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Informer</td>
<td>9.105 $\pm 0.078$</td>
<td>7.059 $\pm 0.057$</td>
<td>n/a</td>
<td>684.407 $\pm 30.136$</td>
<td>459.958 $\pm 11.455$</td>
<td>n/a</td>
<td>8.483 $\pm 0.019$</td>
<td>6.021 $\pm 0.016$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Autoformer</td>
<td>9.385 $\pm 0.545$</td>
<td>7.404 $\pm 0.479$</td>
<td>n/a</td>
<td>804.546 $\pm 22.951$</td>
<td>563.633 $\pm 19.278$</td>
<td>n/a</td>
<td>10.487 $\pm 0.733$</td>
<td>7.711 $\pm 0.654$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>FEDformer</td>
<td>8.326 $\pm 0.039$</td>
<td>6.461 $\pm 0.049$</td>
<td>n/a</td>
<td>701.663 $\pm 3.688$</td>
<td>474.881 $\pm 5.411$</td>
<td>n/a</td>
<td>8.908 $\pm 0.022$</td>
<td>6.244 $\pm 0.039$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Burt)</td>
<td>10.087 $\pm 0.064$</td>
<td>7.478 $\pm 0.058$</td>
<td>1.622\% $\pm 0.721 \%$</td>
<td>1075.721 $\pm 218.227$</td>
<td>542.006 $\pm 16.379$</td>
<td>6.791\% $\pm 2.949 \%$</td>
<td>10.156 $\pm 0.164$</td>
<td>6.982 $\pm 0.101$</td>
<td>2.655\% $\pm 1.285 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Bighbub)</td>
<td>9.213 $\pm 0.117$</td>
<td>6.880 $\pm 0.073$</td>
<td>0</td>
<td>759.708 $\pm 28.990$</td>
<td>493.466 $\pm 9.704$</td>
<td>0.047\% $\pm 0.015 \%$</td>
<td>9.619 $\pm 0.824$</td>
<td>6.654 $\pm 0.493$</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Pegasus)</td>
<td>9.422 $\pm 0.080$</td>
<td>7.003 $\pm 0.047$</td>
<td>0</td>
<td>741.911 $\pm 9.613$</td>
<td>482.426 $\pm 4.749$</td>
<td>0.120\% $\pm 0.116 \%$</td>
<td>8.813 $\pm 0.039$</td>
<td>6.142 $\pm 0.005$</td>
<td>0</td>
</tr>
<tr>
<td>7 Days</td>
<td>7 Days</td>
<td>Transformer</td>
<td>9.577 $\pm 0.476$</td>
<td>7.448 $\pm 0.364$</td>
<td>n/a</td>
<td>760.256 $\pm 24.305$</td>
<td>510.032 $\pm 17.605$</td>
<td>n/a</td>
<td>8.903 $\pm 0.058$</td>
<td>6.235 $\pm 0.036$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Informer</td>
<td>9.538 $\pm 0.320$</td>
<td>7.423 $\pm 0.211$</td>
<td>n/a</td>
<td>731.768 $\pm 34.708$</td>
<td>398.438 $\pm 18.072$</td>
<td>n/a</td>
<td>8.606 $\pm 0.026$</td>
<td>6.080 $\pm 0.030$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Autoformer</td>
<td>10.020 $\pm 0.279$</td>
<td>7.852 $\pm 0.263$</td>
<td>n/a</td>
<td>825.750 $\pm 12.678$</td>
<td>580.922 $\pm 15.157$</td>
<td>n/a</td>
<td>9.039 $\pm 0.203$</td>
<td>6.405 $\pm 0.168$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>FEDformer</td>
<td>8.990 $\pm 0.032$</td>
<td>6.997 $\pm 0.025$</td>
<td>n/a</td>
<td>739.487 $\pm 1.591$</td>
<td>516.934 $\pm 2.579$</td>
<td>n/a</td>
<td>8.959 $\pm 0.037$</td>
<td>6.287 $\pm 0.047$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Burt)</td>
<td>11.681 $\pm 0.185$</td>
<td>8.620 $\pm 0.058$</td>
<td>22.745\%1.926\%</td>
<td>1332.287 $\pm 266.239$</td>
<td>612.469 $\pm 32.104$</td>
<td>29.310\% $\pm 20.472 \%$</td>
<td>12.453 $\pm 0.004$</td>
<td>8.375 $\pm 0.169$</td>
<td>32.716\% $\pm 5.301 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Bighbub)</td>
<td>10.200 $\pm 0.120$</td>
<td>7.601 $\pm 0.083$</td>
<td>0.036\% $\pm 0.018 \%$</td>
<td>946.462 $\pm 57.607$</td>
<td>551.675 $\pm 6.601$</td>
<td>0.537\% $\pm 0.314 \%$</td>
<td>10.851 $\pm 0.612$</td>
<td>7.433 $\pm 0.410$</td>
<td>0.034\% $\pm 0.026 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Pegasus)</td>
<td>10.144 $\pm 0.188$</td>
<td>7.526 $\pm 0.131$</td>
<td>0.022\% $\pm 0.014 \%$</td>
<td>851.768 $\pm 8.422$</td>
<td>545.536 $\pm 2.627$</td>
<td>3.669\% $\pm 1.086 \%$</td>
<td>8.926 $\pm 0.017$</td>
<td>6.247 $\pm 0.011$</td>
<td>0.013\% $\pm 0.005 \%$</td>
</tr>
<tr>
<td>15 Days</td>
<td>1 Day</td>
<td>Transformer</td>
<td>6.603 $\pm 0.177$</td>
<td>4.989 $\pm 0.137$</td>
<td>n/a</td>
<td>557.813 $\pm 22.754$</td>
<td>357.253 $\pm 4.875$</td>
<td>n/a</td>
<td>8.274 $\pm 0.035$</td>
<td>5.856 $\pm 0.036$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Informer</td>
<td>6.457 $\pm 0.268$</td>
<td>4.922 $\pm 0.209$</td>
<td>n/a</td>
<td>536.921 $\pm 33.375$</td>
<td>349.531 $\pm 11.916$</td>
<td>n/a</td>
<td>8.151 $\pm 0.068$</td>
<td>5.868 $\pm 0.049$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Autoformer</td>
<td>6.438 $\pm 0.064$</td>
<td>4.909 $\pm 0.064$</td>
<td>n/a</td>
<td>588.466 $\pm 9.446$</td>
<td>375.703 $\pm 8.107$</td>
<td>n/a</td>
<td>8.239 $\pm 0.053$</td>
<td>5.898 $\pm 0.025$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>FEDformer</td>
<td>6.338 $\pm 0.050$</td>
<td>4.841 $\pm 0.029$</td>
<td>n/a</td>
<td>596.240 $\pm 13.169$</td>
<td>403.764 $\pm 12.324$</td>
<td>n/a</td>
<td>8.214 $\pm 0.013$</td>
<td>5.913 $\pm 0.024$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Burt)</td>
<td>6.432 $\pm 0.046$</td>
<td>4.759 $\pm 0.027$</td>
<td>0</td>
<td>527.350 $\pm 10.608$</td>
<td>355.390 $\pm 2.751$</td>
<td>0</td>
<td>8.279 $\pm 0.053$</td>
<td>5.785 $\pm 0.021$</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Bighbub)</td>
<td>6.351 $\pm 0.016$</td>
<td>4.707 $\pm 0.019$</td>
<td>0</td>
<td>519.665 $\pm 3.440$</td>
<td>350.609 $\pm 1.955$</td>
<td>0</td>
<td>8.326 $\pm 0.048$</td>
<td>5.841 $\pm 0.031$</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Pegasus)</td>
<td>6.379 $\pm 0.023$</td>
<td>4.727 $\pm 0.014$</td>
<td>0</td>
<td>537.188 $\pm 11.296$</td>
<td>361.135 $\pm 4.728$</td>
<td>0</td>
<td>8.289 $\pm 0.016$</td>
<td>5.817 $\pm 0.013$</td>
<td>0</td>
</tr>
<tr>
<td>15 Days</td>
<td>4 Days</td>
<td>Transformer</td>
<td>9.325 $\pm 0.180$</td>
<td>7.214 $\pm 0.154$</td>
<td>n/a</td>
<td>671.953 $\pm 10.473$</td>
<td>445.729 $\pm 5.224$</td>
<td>n/a</td>
<td>8.946 $\pm 0.182$</td>
<td>6.211 $\pm 0.193$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Informer</td>
<td>8.951 $\pm 0.091$</td>
<td>6.941 $\pm 0.064$</td>
<td>n/a</td>
<td>657.442 $\pm 10.941$</td>
<td>447.143 $\pm 3.776$</td>
<td>n/a</td>
<td>8.522 $\pm 0.117$</td>
<td>5.989 $\pm 0.093$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Autoformer</td>
<td>8.937 $\pm 0.163$</td>
<td>6.951 $\pm 0.143$</td>
<td>n/a</td>
<td>745.216 $\pm 11.103$</td>
<td>522.237 $\pm 8.988$</td>
<td>n/a</td>
<td>8.714 $\pm 0.131$</td>
<td>6.119 $\pm 0.086$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>FEDformer</td>
<td>8.251 $\pm 0.082$</td>
<td>6.431 $\pm 0.038$</td>
<td>n/a</td>
<td>594.728 $\pm 4.838$</td>
<td>478.788 $\pm 3.258$</td>
<td>n/a</td>
<td>8.617 $\pm 0.038$</td>
<td>6.114 $\pm 0.051$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Burt)</td>
<td>10.135 $\pm 0.042$</td>
<td>7.631 $\pm 0.033$</td>
<td>1.799\% $\pm 1.651 \%$</td>
<td>855.906 $\pm 60.640$</td>
<td>528.256 $\pm 15.276$</td>
<td>25.701\% $\pm 3.393 \%$</td>
<td>9.476 $\pm 0.079$</td>
<td>6.559 $\pm 0.029$</td>
<td>1.212\% $\pm 0.525 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Bighbub)</td>
<td>9.432 $\pm 0.156$</td>
<td>7.075 $\pm 0.096$</td>
<td>0</td>
<td>700.280 $\pm 14.048$</td>
<td>481.145 $\pm 7.026$</td>
<td>0.041\% $\pm 0.032 \%$</td>
<td>9.392 $\pm 0.571$</td>
<td>6.436 $\pm 0.313$</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Pegasus)</td>
<td>9.582 $\pm 0.118$</td>
<td>7.166 $\pm 0.085$</td>
<td>0</td>
<td>769.008 $\pm 17.279$</td>
<td>476.013 $\pm 3.494$</td>
<td>0.096\% $\pm 0.112 \%$</td>
<td>8.743 $\pm 0.062$</td>
<td>6.082 $\pm 0.026$</td>
<td>0</td>
</tr>
<tr>
<td>15 Days</td>
<td>7 Days</td>
<td>Transformer</td>
<td>9.732 $\pm 0.114$</td>
<td>7.542 $\pm 0.092$</td>
<td>n/a</td>
<td>721.983 $\pm 6.746$</td>
<td>491.780 $\pm 12.208$</td>
<td>n/a</td>
<td>9.004 $\pm 0.166$</td>
<td>6.198 $\pm 0.082$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Informer</td>
<td>9.615 $\pm 0.136$</td>
<td>7.476 $\pm 0.099$</td>
<td>n/a</td>
<td>709.744 $\pm 2.346$</td>
<td>487.892 $\pm 3.851$</td>
<td>n/a</td>
<td>8.528 $\pm 0.131$</td>
<td>5.959 $\pm 0.078$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Autoformer</td>
<td>9.812 $\pm 0.138$</td>
<td>7.667 $\pm 0.077$</td>
<td>n/a</td>
<td>831.116 $\pm 41.423$</td>
<td>595.765 $\pm 45.440$</td>
<td>n/a</td>
<td>8.726 $\pm 0.174$</td>
<td>6.148 $\pm 0.175$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>FEDformer</td>
<td>8.746 $\pm 0.083$</td>
<td>6.807 $\pm 0.058$</td>
<td>n/a</td>
<td>732.936 $\pm 13.858$</td>
<td>508.407 $\pm 15.869$</td>
<td>n/a</td>
<td>8.677 $\pm 0.079$</td>
<td>6.158 $\pm 0.104$</td>
<td>n/a</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Burt)</td>
<td>12.033 $\pm 0.603$</td>
<td>8.773 $\pm 0.109$</td>
<td>27.107\% $\pm 1.159 \%$</td>
<td>1190.564 $\pm 111.114$</td>
<td>611.117 $\pm 55.916$</td>
<td>50.259\% $\pm 28.139 \%$</td>
<td>11.048 $\pm 0.325$</td>
<td>7.653 $\pm 0.129$</td>
<td>38.328\% $\pm 5.362 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Bighbub)</td>
<td>10.329 $\pm 0.162$</td>
<td>7.745 $\pm 0.120$</td>
<td>0.023\% $\pm 0.031 \%$</td>
<td>1063.650 $\pm 222.601$</td>
<td>541.538 $\pm 7.397$</td>
<td>0.329\% $\pm 0.190 \%$</td>
<td>9.731 $\pm 0.427$</td>
<td>6.658 $\pm 0.280$</td>
<td>0.807\% $\pm 0.011 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PromptCast (Pegasus)</td>
<td>10.522 $\pm 0.187$</td>
<td>7.899 $\pm 0.142$</td>
<td>0.179\% $\pm 0.265 \%$</td>
<td>835.386 $\pm 23.031$</td>
<td>525.537 $\pm 6.067$</td>
<td>1.422\% $\pm 0.465 \%$</td>
<td>8.802 $\pm 0.031$</td>
<td>6.139 $\pm 0.016$</td>
<td>0.011\% $\pm 0.006 \%$</td>
</tr>
</tbody>
</table>
<p>encompasses weekly patterns in the observation, leading to more accurate forecasting.</p>
<p>To provide further insight into the performance of the multistep forecasting, additional examples for the next 7 days setting are available in our repository ${ }^{13}$. Our exploration of PromptCast on different lengths illustrates that the model is robust to dynamic observation/prediction lengths, and presents a promising research direction for time series forecasting.</p>
<h2>I. Multivariate Time Series Forecasting</h2>
<p>As the first attempt to leverage language models for time series forecasting, we take the basic forecasting setting to demonstrate the concept of PromptCast. However, we would like to emphasize that the proposed new paradigm is flexible and suitable for other forecasting settings such as the multivariate time series forecasting setting. We only need to update the prompt templates accordingly when PromptCast is adapted to multivariate time series forecasting setting. Under the new setting, although input/output prompts might be updated, the core prediction model (i.e., language foundation models) in PromptCastcan remain the exact same (prompt-agnostic) and same pre-trained weights can also be used. For example, in the experiments under Section IV-G, different prompts are applied. But we can also directly use the same models (without any changes to the model structures) to yield predictions. For conventional numerical forecasting methods, however, necessary</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>updates on the deep learning model (e.g., update the encoder part to fit multivariate features) must be introduced when the forecasting setting is changed. Additionally, processes such as extra hyperparameter search are often required when the model structure is modified. This could also reflect the "code less" benefits and robustness of the proposed PromptCast paradigm.</p>
<p>To investigate the capability of the proposed PromptCast on multivariate forecasting, we further conduct a pilot study of multivariate PromptCast on Beijing Air Quality Data ${ }^{14}$ in addition to the three sub-sets in the main PISA dataset. The data collection period of this Air Quality (AQ) set is from 2013/03/01 to 2017/02/28. Similar to the main PISA, we split this period into training set (2013/03/01-2015/12/18), validation set (2015/12/19-2016/05/12), and test set (2016/05/132017/02/28). After filtering missing values, we focus on three types of air pollutants: PM2.5, PM10, and SO2, which means the feature dimension is 3 .</p>
<p>To apply PromptCast for multivariate forecasting, we update the prompt template to include multiple features as Table XIV. In total, we design and examine three different prompts. For Prompt A and B, the input prompt consists of the description of all 3 features and the main difference of these two prompts is in the output prompt part. Output Prompt A jointly describes the three features, whereas the three feature values are given separately in three short sentences in Output Prompt B. Prompt C can be seen as an extended version of the prompt template</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup> <sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: 13 https://github.com/HaoUNSW/PISA/Dataset/PISA_Plus_Multistep_ examples/README.md</p>
<p>TABLE XIII
EXAMPLES OF TWO TYPES OF PROMPTS UNDER THE MULTIVARIATE TIME SERIES SETTING.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Prompt A\&amp;B</th>
<th style="text-align: center;">From May 13, 2016, Friday to May 27, 2016, Friday, the PM2.5 was 32, 23, 20, 53, 82, 113, 133, $94,64,83,20,4,18,48,57$; PM10 was $32,40,57,87,90,113,133,94,64,83,20,20,18,48$, 103; and SO2 was $2,3,4,11,29,27,26,33,13,14,6,2,2,6,16$ on each day. What are the pollutant values going to be on May 28, 2016, Saturday?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Output Prompt A</td>
<td style="text-align: center;">The pollutant values will be 3, 23, 2.</td>
</tr>
<tr>
<td style="text-align: center;">Output Prompt B</td>
<td style="text-align: center;">The PM2.5 will be 3. The PM10 will be 23. The SO2 will be 2.</td>
</tr>
<tr>
<td style="text-align: center;">Input Prompt C</td>
<td style="text-align: center;">From May 13, 2016, Friday to May 27, 2016, Friday, the PM2.5 was 32, 23, 20, 53, 82, 113, 133, $94,64,83,20,4,18,48,57$ on each day. What is the PM2.5 value going to be on May 28, 2016, Saturday? <br> From May 13, 2016, Friday to May 27, 2016, Friday, the PM10 was 32, 40, 57, 87, 90, 113, 133, $94,64,83,20,20,18,48,103$ on each day. What is the PM10 value going to be on May 28, 2016, Saturday? <br> From May 13, 2016, Friday to May 27, 2016, Friday, the SO2 was 2, 3, 4, 11, 29, 27, 26, 33, 13, $14,6,2,2,6,16$ on each day. What is the SO2 value going to be on May 28, 2016, Saturday?</td>
</tr>
<tr>
<td style="text-align: center;">Output Prompt C</td>
<td style="text-align: center;">The PM2.5 will be 3. <br> The PM10 will be 23. <br> The SO2 will be 2.</td>
</tr>
</tbody>
</table>
<p>TABLE XIV
RESULTS OF LANGUAGE MODELS ON MULTIVARIATE TIME SERIES FORECASTING.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RMSE</th>
<th style="text-align: center;">MAE</th>
<th style="text-align: center;">MissingRate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CopyYesterday</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73.058</td>
<td style="text-align: center;">43.274</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">Histro</td>
<td style="text-align: center;">average</td>
<td style="text-align: center;">63.289</td>
<td style="text-align: center;">40.772</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">CopyJ</td>
<td style="text-align: center;">week</td>
<td style="text-align: center;">87.250</td>
<td style="text-align: center;">52.796</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">52.859 $\pm 1.127$</td>
<td style="text-align: center;">39.341 $\pm 1.300$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">55.262 $\pm 0.850$</td>
<td style="text-align: center;">41.147 $\pm 1.142$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">54.839 $\pm 1.156$</td>
<td style="text-align: center;">40.385 $\pm 1.785$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">Informer</td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">53.770 $\pm 1.295$</td>
<td style="text-align: center;">39.952 $\pm 1.018$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">54.881 $\pm 0.630$</td>
<td style="text-align: center;">40.903 $\pm 0.650$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">54.801 $\pm 0.839$</td>
<td style="text-align: center;">41.009 $\pm 0.890$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">Autoformer</td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">56.693 $\pm 0.602$</td>
<td style="text-align: center;">42.770 $\pm 0.459$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">58.917 $\pm 2.084$</td>
<td style="text-align: center;">45.011 $\pm 1.740$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">62.864 $\pm 2.090$</td>
<td style="text-align: center;">48.464 $\pm 1.738$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">FEDformer</td>
<td style="text-align: center;">fixed</td>
<td style="text-align: center;">55.849 $\pm 1.291$</td>
<td style="text-align: center;">42.470 $\pm 1.195$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">55.905 $\pm 0.542$</td>
<td style="text-align: center;">52.402 $\pm 0.588$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">timeF</td>
<td style="text-align: center;">63.427 $\pm 4.349$</td>
<td style="text-align: center;">49.619 $\pm 4.254$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">Prompt A</td>
<td style="text-align: center;">Bart</td>
<td style="text-align: center;">79.218 $\pm 4.948$</td>
<td style="text-align: center;">46.553 $\pm 2.838$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BigBird</td>
<td style="text-align: center;">75.783 $\pm 1.988$</td>
<td style="text-align: center;">45.590 $\pm 2.105$</td>
<td style="text-align: center;">$2.681 \% \pm 3.471 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pegasus</td>
<td style="text-align: center;">82.859 $\pm 0.588$</td>
<td style="text-align: center;">48.168 $\pm 0.444$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Prompt B</td>
<td style="text-align: center;">Bart</td>
<td style="text-align: center;">75.268 $\pm 2.150$</td>
<td style="text-align: center;">41.869 $\pm 0.915$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BigBird</td>
<td style="text-align: center;">81.510 $\pm 2.524$</td>
<td style="text-align: center;">48.913 $\pm 0.639$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pegasus</td>
<td style="text-align: center;">75.706 $\pm 1.736$</td>
<td style="text-align: center;">45.206 $\pm 1.156$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Prompt C</td>
<td style="text-align: center;">Bart</td>
<td style="text-align: center;">68.282 $\pm 2.345$</td>
<td style="text-align: center;">40.690 $\pm 1.131$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BigBird</td>
<td style="text-align: center;">67.729 $\pm 1.880$</td>
<td style="text-align: center;">41.547 $\pm 1.071$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pegasus</td>
<td style="text-align: center;">67.834 $\pm 0.859$</td>
<td style="text-align: center;">39.893 $\pm 0.691$</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>(e.g., Table I) for the univariate forecasting. It decomposes the multivariate features (i.e., three pollutants in this AQ dataset) into several univariate features.</p>
<p>The results of PromptCast with language foundation models using different prompts are reported in Table XIV. Additionally, we list the performance of other baselines. From the table, we can observe that: (1) All three language models can generate plausible predicted future descriptions as evidenced by almost all zero missing rates. (2) The design of prompts could definitely affect the prediction performance. Specifically, Prompt C leads to better performance than Prompt A and B on this multivariate forecasting dataset. (3) PromptCast
has a better MAE performance (e.g., Transformer-fixed VS. Prompt C-Pegasus) whereas the performance on RMSE is worse than the Transformer-based baselines. We think the potential reason of why PromptCast is not the best performer in this comparison is because that the current prompts cannot fully characterize the relation of different features. This also explains why Prompt C outperforms the other two prompts.
Open Question and Future Work. The results show that the model architectures used for univariate time series can also be applied to multivariate time series, this is an indication that the PromptCast task is a promising direction for multivariate time series forecasting. However, further research is needed to fully understand the behavior of multivariate time series forecasting with PromptCast and to develop more robust models that can handle this type of data. The above performance analysis points out an open question for PromptCast, that is, how to design prompts for the more challenging multivariate time series. One potential working direction is to learn the internal correlations between different temporal features and then develop prompts to represent the learned correlations. In the future, we will focus on this direction and also explore techniques such as learnable prompts for the multivariate PromptCast. We hope the proposed PISA dataset and the corresponding benchmark could encourage other researchers as well to investigate this interesting PromptCast topic.</p>
<h2>V. CASE STUDIES</h2>
<p>Why Language Model Works for Forecasting. Through the above experiments, we have shown that the proposed PromptCast task represents a new paradigm for time-series prediction. In this section, we aim to further investigate the reasons behind why language models can be used for forecasting purposes. First, the model architecture of language models aligns seamlessly with the nature of time series data, as both time series forecasting and language generation are inherently sequence-to-sequence processes. This alignment provides a solid foundation for employing language models in time series forecasting. When viewed from the sequence-to-sequence perspective, time series prediction can be seen</p>
<p>as similar to the problem of predicting the next sentence in NLP, which has been effectively tackled using pre-trained foundation models. Second, using auxiliary information such as time-of-day, day-of-week [9] and semantic information [28] has been shown to be beneficial for improving forecasting performance. However, incorporating such semantic contexts in conventional numerical-based forecasting methods often requires additional efforts to explicitly design special layers or modules in a heuristic way for the auxiliary inputs. Even though some approaches (such as the temporal embedding methods discussed above) have been proposed for numericalbased methods to incorporate auxiliary information, it remains challenging for a forecasting model to seamlessly model the contexts and the main temporal information. In contrast, with PromptCast, the prompting process effectively integrates contextual information and time series data, transforming the cross-modality relationship between semantic information and numerical values into an in-modality synergy. Both auxiliary and core temporal inputs are considered together as tokens after prompting. The intra-relation of numerical value tokens at different time steps and the inter-relation between numerical value tokens and contextual information tokens (e.g., date information) could be better learned simultaneously by language foundation models (e.g., through the self-attentions in Transformers). Modeling these relations would then result in good forecasting performance.</p>
<p>To validate the above hypothesis, we conduct two case studies and visualize the attentions (between the input sentence and the output sentence) learned by the language models (using the Bart model as an example in these case studies).</p>
<ul>
<li>Case Study 1</li>
<li>Input: From September 06, 2019, Friday to September 20, 2019, Friday, the average temperature of region 1 was $65,66,70,71,71,77,78,65,70$, $76,74,70,64,61,64$ degree on each day. What is the temperature going to be on September 21, 2019, Saturday?</li>
<li>Predicted: The temperature will be 68 degree.</li>
<li>Ground Truth: The temperature will be 71 degree.</li>
<li>Case Study 2</li>
<li>Input: From June 08, 2021, Tuesday to June 22, 2021, Tuesday, there were 13, 16, 9, 11, 21, 9, 13, $15,8,24,9,10,11,10,14$ people visiting POI 1 on each day. How many people will visit POI 1 on June 23, 2021, Wednesday?</li>
<li>Predicted: There will be 11 visitors.</li>
<li>Ground Truth: There will be 12 visitors.</li>
</ul>
<p>Specifically, in Figure 3, we show the attention weights between the numerical predicted values in the generated outputs and all the tokens in the input prompts. For each heatmap plot, with hotter regions representing larger attention values, the horizontal axis represents the input prompt (in token format) and the vertical axis represents different attention heads.</p>
<p>For Case 1 (Figure 3 (a)), we can clearly observe that head 6 pays more attention to the numerical sequential data in the input prompt while the other heads have higher attention to other semantic auxiliary information such as September
and Friday. Moreover, head 7 shows attentions to both the input numerical historical observations (e.g., 76, 74, and 64) and semantic tokens simultaneously. For Case 2, the attention visualization of two layers are displayed in Figure 3 (b) and Figure 3 (c). It can be noticed that the learned attentions between the predicted value and the numerical historical values in the input prompt are mainly in layer 5 (Figure 3 (c)) of the Bart model. Compared to layer 5, layer 4 (Figure 3 (b)) focuses on not only the numerical tokens but also contextual tokens. More specifically, head 7 demonstrates large attention weights to semantic tokens like there were and How many people will visit whereas head 10 highlights the auxiliary date token 22 as well as the punctuation token 7. These case studies demonstrate that the attention learned by language models is able to take into account both the numerical value tokens (historical values) and auxiliary information tokens to make predictions under the PromptCast paradigm. It further justifies the rationale of leveraging language models for forecasting time series.</p>
<h2>VI. DISCUSSION AND CONCLUSION</h2>
<p>In this paper, we introduce a new task, PromptCast, which utilizes language models to predict time series in a language generation manner. As this is the first work on the PromptCast task, and no existing datasets are suitable, we construct the first dataset, PISA, to investigate prompt-based forecasting. This large-scale dataset contains three real-world time series forecasting scenarios. To further advance the research of PromptCast, we also establish a benchmark on the released dataset and provide a set of strong baselines that include both state-of-the-art numerical forecasting methods and language generation models. The experimental results demonstrate that using language models in the PromptCast setting results in good forecasting performance and generalization ability.</p>
<p>Broader Impact. We believe that the findings of this study will provide forward-thinking concepts and fresh insights for researchers working in the field of time series forecasting. We also think that the proposed PromptCast task, as well as the PISA dataset, could open new related research directions and provide visionary ideas about downstream applications that could be enabled by this work. Some potential directions for future research are discussed below. (1) Automatic Prompting: In this paper, the transformation of numerical data to text is achieved through the use of templates. Although template-based prompting is efficient, it can be difficult to produce diverse prompts and fixed templates may introduce biases (biases towards certain templates). To address these limitations, a potential research direction is the development of automatic time series prompting or time series captioning (similar to image captioning [29]), which utilizes generative models to describe time series data. (2) Explainable PromptCast: Another area that has yet to be fully investigated is the question of why models designed for language modeling tasks are able to predict time series. Recent research has begun to explore the use of language models for non-language downstream tasks [30]. Although we have initially discussed our thoughts on why language models can be effective for</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Visualizations of attentions in the proposed PromptCast.</p>
<p>time series forecasting under the PromptCast task through case studies, further studies on the interpretability and explainability of PromptCast models would be an interesting and valuable research direction to better understand the underlying mechanisms. This can help us to design better models and understand the limitations of current models. By understanding the interpretability, we can also have a better understanding of how to apply these models to real-world problems. (3) <em>PromptCast QA and Chatbots</em>: The research of PromptCast task would promote the development of time series forecasting question-answering tasks and building chatbot applications with forecasting capabilities. Note that the PromptCast QA task differs from recent tasks such as TimeQA [31], which is proposed to answer general time-related questions based on Wikipedia text, and ForecastQA [32], which is also based on text articles. The core of the PromptCast QA task is the question-answering ability for forecasting based on given sequential numerical value contexts. This task can be applied to digital assistants such as Siri.</p>
<p><strong>Limitations and Future Work.</strong> As the first attempt to create a dataset for the novel PromptCast task, in this dataset release, we have mainly focused on the univariate time series forecasting setting. As discussed in Section IV-I, one potential direction for our future work is to extend the dataset to include multivariate time series, which would support the research of more complex time series forecasting scenarios. Another future work is to develop an auto-regressive method to further enhance the PromptCast performance on multi-step forecasting setting. For example, the language models could be fine-tuned to predict the value at the next time step. The generated sentence will then be recurrently appended to the input prompts to predict longer horizons. Additionally, more challenging heterogeneous datasets that incorporate multiple data sources will also be considered for the PromptCast task in the future. The prompt-based paradigm can also be extended to other tasks such as time series classification and anomaly detection. This would enable the proposed paradigm to be applied to a wider range of real-world scenarios.</p>
<h3>REFERENCES</h3>
<ul>
<li>[1] S. Hochreiter and J. Schmidhuber, "Long short-term memory," <em>Neural computation</em>, vol. 9, no. 8, pp. 1735–1780, 1997.</li>
<li>[2] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, "Temporal convolutional networks for action segmentation and detection," in <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2017, pp. 156–165.</li>
<li>[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," <em>Advances in neural information processing systems</em>, vol. 30, 2017.</li>
<li>[4] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill <em>et al.</em>, "On the opportunities and risks of foundation models," <em>arXiv preprint arXiv:2108.07258</em>, 2021.</li>
</ul>
<p>[5] J. Devlin, M. Chang, K. Lee, and K. Toutanova, "BERT: pre-training of deep bidirectional transformers for language understanding," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 2019, pp. 4171-4186.
[6] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," in ICML, ser. Proceedings of Machine Learning Research, vol. 139, 2021, pp. 8748-8763.
[7] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang et al., "Grounded language-image pretraining," arXiv preprint arXiv:2112.03857, 2021.
[8] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan, "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting," Advances in neural information processing systems, vol. 32, 2019.
[9] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang, "Informer: Beyond efficient transformer for long sequence time-series forecasting," in Proceedings of AAAI, 2021.
[10] S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and S. Dustdar, "Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting," in International Conference on Learning Representations, 2021.
[11] J. Xu, J. Wang, M. Long et al., "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting," Advances in Neural Information Processing Systems, vol. 34, 2021.
[12] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, and R. Jin, "Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting," in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, ser. Proceedings of Machine Learning Research, vol. 162. PMLR, 2022, pp. 27 268-27 286.
[13] A. Drouin, E. Marcotte, and N. Chapados, "Tactis: Transformerattentional copulas for time series," in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, ser. Proceedings of Machine Learning Research, vol. 162. PMLR, 2022, pp. 5447-5493.
[14] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg et al., "A generalist agent," arXiv preprint arXiv:2205.06175, 2022.
[15] H. Xue, F. D. Salim, Y. Ren, and C. L. Clarke, "Translating human mobility forecasting through natural language generation," in Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, 2022, pp. 1224-1233.
[16] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," J. Mach. Learn. Res., vol. 21, pp. 140:1-140:67, 2020.
[17] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "BART: denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. Association for Computational Linguistics, 2020, pp. 7871-7880.
[18] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.
[19] K. Clark, M. Luong, Q. V. Le, and C. D. Manning, "ELECTRA: pre-training text encoders as discriminators rather than generators," in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
[20] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang et al., "Big bird: Transformers for longer sequences," Advances in Neural Information Processing Systems, vol. 33, pp. 17 283-17 297, 2020.
[21] W. Qi, Y. Yan, Y. Gong, D. Liu, N. Duan, J. Chen, R. Zhang, and M. Zhou, "Prophetnet: Predicting future n-gram for sequence-tosequence pre-training," in Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, ser. Findings of ACL, vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 2401-2410.
[22] I. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The longdocument transformer," arXiv preprint arXiv:2004.05150, 2020.
[23] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y. Liu, J. Xu, M. Ott, E. M. Smith, Y. Boureau, and J. Weston, "Recipes for building
an open-domain chatbot," in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021. Association for Computational Linguistics, 2021, pp. 300-325.
[24] J. Zhang, Y. Zhao, M. Saleh, and P. Liu, "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization," in International Conference on Machine Learning. PMLR, 2020, pp. 11 328-11 339.
[25] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Loof, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, "Transformers: State-of-the-art natural language processing," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Oct. 2020, pp. 38-45.
[26] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler, "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books," in Proceedings of the IEEE international conference on computer vision, 2015, pp. 19-27.
[27] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019.
[28] H. Xue, F. Salim, Y. Ren, and N. Oliver, "Mobtcast: Leveraging auxiliary trajectory forecasting for human mobility prediction," Advances in Neural Information Processing Systems, vol. 34, pp. 30 380-30 391, 2021.
[29] S. Herdade, A. Kappeler, K. Boakye, and J. Soares, "Image captioning: Transforming objects into words," Advances in Neural Information Processing Systems, vol. 32, 2019.
[30] T. Dinh, Y. Zeng, R. Zhang, Z. Lin, S. Rajput, M. Gira, J.-y. Sohn, D. Papailiopoulos, and K. Lee, "Lift: Language-interfaced fine-tuning for non-language machine learning tasks," arXiv preprint arXiv:2208.06565, 2022.
[31] W. Chen, X. Wang, W. Y. Wang, and W. Y. Wang, "A dataset for answering time-sensitive questions," in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, vol. 1, 2021.
[32] W. Jin, R. Khanna, S. Kim, D. Lee, F. Morstatter, A. Galstyan, and X. Ren, "Forecastqa: A question answering challenge for event forecasting with temporal text data," in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 2021, pp. 4636-4650.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Hao Xue is currently a Lecturer at the School of Computer Science and Engineering at UNSW Sydney. He earned his PhD from The University of Western Australia in 2020. After completing his doctorate, he worked as a Research Fellow at the School of Computing Technologies at RMIT University and UNSW Sydney. He was awarded the DAAD Alnet Fellowship in 2022. His research interests include spatiotemporal data modelling, time series forecasting, language generation based forecasting, and data-efficient time series representation learning.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Professor Flora Salim is the CISCO Chair of Digital Transport, School of Computer Science and Engineering, UNSW Sydney.Her research, on behaviour modelling, AI and machine learning on time-series and spatio-temporal sensor data, has been funded by the ARC, Humboldt Foundation, Bayer Foundation, Microsoft Research, Qatar National Research Fund, and many local and international industry partners. She won the Women in AI Awards 2022 ANZ Defence and Intelligence category.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 4. The plots of cost and RMSE accuracy on three sub-sets.</p>
<h2>APPENDIX</h2>
<p>This Appendix document is organized as follows:</p>
<ul>
<li>Section A shows scatter plots representing the relationship between the cost and accuracy of each evaluated forecasting method.</li>
<li>Section B introduces the hosting and licensing of our PISA dataset.</li>
<li>Section C presents the Datasheet for Datasets for our PISA dataset.</li>
</ul>
<h2>A. Additional Cost Analysis</h2>
<p>Three plots corresponding to the three sub-sets within PISA are presented in Figure 4. These plots visually describe the cost and accuracy (e.g., RMSE) of different forecasting methods. As shown in the figure, our PromptCast presents a good forecasting accuracy while maintaining cost-effectiveness.</p>
<h2>B. PISA Hosting and Licensing</h2>
<h2>Source Data.</h2>
<ul>
<li>CT: The source data for the CT sub-set can be accessed through the Average Daily Temperature Archive ${ }^{15}$. Based on its description, this source data is available for research and non-commercial purposes only.</li>
<li>ECL: The original data is available at UCI Machine Learning Repository ${ }^{16}$. We used a processed version of the original data provided by Informer repository which is licensed under Apache License $2.0^{17}$.</li>
<li>SG: We access the raw SafeGraph Weekly Patterns data through SafeGraph Data for Academics ${ }^{18}$. "SafeGraph ${ }^{19}$, a data company that aggregates anonymized location data from numerous applications in order to provide insights about physical places, via the SafeGraph Community. To enhance privacy, SafeGraph excludes census block group information if fewer than five devices visited an establishment in a month from a given census block group." According to the policy, it is against SafeGraph's terms of service to directly re-share raw SafeGraph data.</li>
</ul>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>However, it is acceptable under SafeGraph's terms of service to share aggregated and derived data, and to include data in chart and visual forms. We strongly recommend users to register SafeGraph Data for Academics before accessing our PISA dataset.
PISA. The PISA dataset and codes for models reported in the benchmark are available at https://github.com/HaoUNSW/ PISA. In this repository, we also provide some generated examples of language models for the PromptCast task. It's worth noting that only the validation sets are provided as PISA examples in the above-mentioned repository during the submission period. After the acceptance decision notification, the full PISA dataset (including train/val/testing sets) will be uploaded to the same repository and made publicly available. The full dataset will also be available through HuggingFace Dataset ${ }^{20}$, which will make it easier to use our dataset with HuggingFace models.</p>
<p>The PISA dataset will be distributed under CC BY-NC-SA $4.0^{21}$.</p>
<h2>C. PISA Datasheet</h2>
<h2>Motivation</h2>
<p>For what purpose was the dataset created? (Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.)</p>
<p>The PISA dataset is created to support the research of novel PromptCast task proposed in this paper.</p>
<p>Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?</p>
<p>The authors (institution: School of Computer Science and Engineering, University of New South Wales, Sydney, Australia) of this paper created this PISA dataset.</p>
<p>Who funded the creation of the dataset? (If there is an associated grant, please provide the name of the grant or and the grant name and number.)</p>
<p>The creator of the dataset was supported by Australian Research Council (ARC) Discovery Project DP190101485.</p>
<p>Any other comments?
None.</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>Composition</h2>
<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? (Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.)</p>
<p>For the CT sub-set, the instances represent the daily temperature of a city. For the ECL sub-set, the instances represent the daily electricity consumption of a user. For the SG sub-set, the instances represent the daily visitor counts of a POI.</p>
<p>How many instances are there in total (of each type, if appropriate)?</p>
<p>The proposed PISA dataset includes 311,932 instances in total. The details of the partitions are presented in Table II.</p>
<p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? (If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).)</p>
<p>For the CT sub-set, 110 international cities are randomly selected to form the dataset. For the ECL sub-set, We filtered users with missing values and randomly selected 50 users (from 321 users) with full records of the entire data collection period. For the SG sub-set, we randomly selected 324 POIs with full records. We list the data value range of each sub-set in Table II and show the data distributions in Figure 2.</p>
<p>What data does each instance consist of? ("Raw" data (e.g., unprocessed text or images)or features? In either case, please provide a description.)</p>
<p>Each instance consists of the input prompt and the output prompt. The input prompt is the input of a model and the output prompt is the desired output of the model. Our PISA provides the input prompts and the corresponding output prompts in separate text files (e.g., val_x_prompt.txt and val_y_prompt.txt).</p>
<p>Is there a label or target associated with each instance? If so, please provide a description.</p>
<p>Yes. As described in the above response, the output prompt is considered as the label.</p>
<p>Is any information missing from individual instances? (If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.)</p>
<p>There is no missing data (beyond what was intentionally omitted, e.g., the POI geo-location of SG sub-set).</p>
<p>Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? ( If so, please describe how these relationships are made explicit.)</p>
<p>Since the sliding window approach is applied (see Section III-A), the observation data of different instances might belong to the same object-of-interest. However, aach instance should be considered and treated as an independent instance with no relationship to other instances in PISA.</p>
<p>Are there recommended data splits (e.g., training, development / validation, testing)? (If so, please provide a description of these splits, explaining the rationale behind them.)</p>
<p>Yes. Each sub-set in our PISA is divided into train/val/test at the ratio of 7:1:2 by the chronological order. Please refer to Section III-A and Table II for more details about the data splits.</p>
<p>Are there any errors, sources of noise, or redundancies in the dataset? (If so, please provide a description.)</p>
<p>No.
Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? (If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.)</p>
<p>The PISA dataset is self-contained.
Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? (If so, please provide a description.)</p>
<p>No.
Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? (If so, please describe why.)</p>
<p>No.
Does the dataset relate to people? (If not, you may skip the remaining questions in this section.)</p>
<p>No.
Does the dataset identify any subpopulations (e.g., by age, gender)? (If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.)</p>
<p>N/A.
Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? (If so, please describe how.)</p>
<p>N/A.
Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? (If so, please provide a description.)</p>
<p>N/A.
Any other comments?
None.</p>
<h2>Collection Process</h2>
<p>How was the data associated with each instance acquired? (Was the data directly observable (e.g., raw text, movie</p>
<p>ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.)</p>
<p>The data associated with each instance is acquired and derived from three data sources: CT, ECL, and SG. We have examined and verified the data. These data sources have also been used in the literature forecasting tasks.</p>
<p>What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? (How were these mechanisms or procedures validated?)</p>
<p>N/A. Our PISA dataset is based on existing data sources. We did not collect the data by ourselves.</p>
<p>If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?</p>
<p>For the CT sub-set, 110 international cities are randomly selected to form the dataset. For the ECL sub-set, We filtered users with missing values and randomly selected 50 users (from 321 users) with full records of the entire data collection period. For the SG sub-set, we randomly selected 324 POIs with full records.</p>
<p>Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?</p>
<p>All collection and annotation was done by the first author.
Over what timeframe was the data collected? (Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.)</p>
<p>The data collection period of the data sources are reported in Table II. The collection period of CT is 2017/01/01 2020/04/30. The collection period of ECL is 2012/01/01 2014/12/31. The collection period of SG is 2020/06/15 2021/09/05.</p>
<p>Were any ethical review processes conducted (e.g., by an institutional review board)? (If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.)</p>
<p>N/A. The original data sources are publicly available and have been widely used in the literature.</p>
<p>Does the dataset relate to people? (If not, you may skip the remaining questions in this section.)</p>
<p>No.
Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?
N/A.
Were the individuals in question notified about the data collection? (If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.)</p>
<p>N/A.</p>
<p>Did the individuals in question consent to the collection and use of their data? (If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.)</p>
<p>N/A.
If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? (If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).)</p>
<p>N/A.
Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? (If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.)</p>
<p>N/A.
Any other comments?
None.</p>
<h2>Preprocessing/cleaning/labeling</h2>
<p>Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? (If so, please provide a description. If not, you may skip the remainder of the questions in this section.)</p>
<p>Yes. The details of the preprocessing/cleaning/labeling of the raw data sources are described in Section III-A. The prompting process of our PISA dataset is provided in Section III-B.</p>
<p>Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? (If so, please provide a link or other access point to the "raw" data.)</p>
<p>For the CT and ECL sub-sets, the raw data sources are publicly available. For the SG sub-set, due to SafeGraph policy, the raw data will not be provided. Please refer to Section B for more details.</p>
<p>Is the software used to preprocess/clean/label the instances available? (If so, please provide a link or other access point.)</p>
<p>Yes. In the GitHub repository (https://github.com/ HaoUNSW/PISA), we provide the codes of transferring the source numerical data to prompts.</p>
<p>Any other comments?
None.</p>
<h2>Uses</h2>
<p>Has the dataset been used for any tasks already? (If so, please provide a description.)</p>
<p>No. This PISA dataset is designed and introduced for the novel PromptCast task proposed in the paper.</p>
<p>Is there a repository that links to any or all papers or systems that use the dataset? (If so, please provide a link or other access point.)</p>
<p>Considering that the proposed dataset PISA is associated with the novel task PromptCast formulated in this paper, there</p>
<p>is no repository for the time being. In the future, we do have a plan to create such a repository to summarize the papers related to this dataset or this task.</p>
<p>What (other) tasks could the dataset be used for?
The PISA dataset could also be used for the conventional numerical-based time series forecasting task.</p>
<p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? (For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?)</p>
<p>No.
Are there tasks for which the dataset should not be used? (If so, please provide a description.)</p>
<p>No. We encourage other researchers to try our dataset on other related tasks.</p>
<p>Any other comments?
None.</p>
<h2>Distribution</h2>
<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.</p>
<p>Yes. During the reviewing period, the dataset (i.e., the validation sets as examples) is available https://github.com/ HaoUNSW/PISA. The full dataset will be publicly available on the same GitHub page for download by all interested third parties for research purpose after the acceptance decision notification.</p>
<p>How will the dataset will be distributed (e.g., tarball on website, API, GitHub) Does the dataset have a digital object identifier (DOI)?</p>
<p>The dataset will be distributed through the GitHub page and HuggingFace dataset page. Please refer to Section B for more details.</p>
<p>When will the dataset be distributed?
The full dataset will be made publicly available after the acceptance decision notification.</p>
<p>Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.</p>
<p>Our PISA dataset will be distributed under the CreativeCommons Attribution-NonCommercial-ShareAlike license (CC-BY-NC-SA). The terms of this license may be found at: https://creativecommons.org/licenses/by-ncsa/2.0/ legalcode.</p>
<p>Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.</p>
<p>No. There are no third party restrictions on the dataset.</p>
<p>Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.</p>
<p>No export controls or other regulatory restrictions apply to the dataset. However, we strongly recommend users to register SafeGraph Data for Academics before considering our PISA dataset. Please refer to Section B for more details.</p>
<p>Any other comments?
None.</p>
<h2>Maintenance</h2>
<p>Who will be supporting/hosting/maintaining the dataset?
The authors of this paper will support/host/maintain the proposed PISA dataset.</p>
<p>How can the owner/curator/manager of the dataset be contacted (e.g., email address)?</p>
<p>The owner/curator/manager of the dataset can be contacted via: hao.xue1 @unsw.edu.au.</p>
<p>Is there an erratum? If so, please provide a link or other access point.</p>
<p>No erratum for the current version of PISA dataset.
Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?</p>
<p>The dataset will be updated in the future by the authors of this paper. The updates will focus on adding new forecasting scenarios, introducing more prompt templates, or expanding PISA to multivarite time series setting. The updated will be communicated to users through the GitHub page and the HuggingFace page.</p>
<p>If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.
$\mathrm{N} / \mathrm{A}$.
Will older versions of the dataset continue to be supported/ hosted/ maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users.</p>
<p>Yes, all the versions of the dataset will be supported/ hosted/ maintained by the authors of this paper. The versioning information will be communicated to users through the GitHub page and the HuggingFace dataset page.</p>
<p>If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.</p>
<p>If others want to extend/augment/build on/contribute to the dataset, the authors of this paper should be first contacted. Other researchers are encouraged to pull requests on the GitHub page. We will validate and verify the contributed data before merge the contributed data.</p>
<p>Any other comments?
None.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{15}$ https://academic.udayton.edu/kissock/http/Weather/default.htm
${ }^{16}$ UCI Machine Learning Repository
${ }^{17}$ https://github.com/zhouhaoyi/Informer2020/blob/main/LICENSE
${ }^{18}$ https://www.safegraph.com/academics
${ }^{19}$ https://www.safegraph.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{20}$ https://huggingface.co/datasets
${ }^{21}$ https://creativecommons.org/licenses/by-nc-sa/4.0/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
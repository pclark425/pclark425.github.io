<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8879 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8879</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8879</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-7cb303190a8910df8a29aa142cffe285fc88c431</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7cb303190a8910df8a29aa142cffe285fc88c431" target="_blank">Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Beam Enumeration is proposed to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted, showing that improvements to explainability and sample efficiency for molecular design can be made synergistic.</p>
                <p><strong>Paper Abstract:</strong> Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration shows that improvements to explainability and sample efficiency for molecular design can be made synergistic.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8879.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8879.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Beam Enumeration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam Enumeration (probabilistic subsequence enumeration for language-based molecular generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-agnostic algorithm that exhaustively enumerates high-probability token sub-sequences from an autoregressive SMILES language model to extract frequent molecular substructures and then self-condition generation by filtering sampled molecules to those containing these substructures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Beam Enumeration (method applied to language-based SMILES models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Method applied to autoregressive language-based molecular generative models (SMILES-based autoregressive RNN/LSTM in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not a model; operates on a pre-trained language-based generative model (the paper uses priors trained on ChEMBL).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular design for drug discovery (property optimization and docking-guided inhibitor design in case studies).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Enumerates top-k token probabilities sequentially for N beam steps to produce an exhaustive set of partial SMILES subsequences; extracts valid substructures (Scaffold or Structure), maintains a pool of most frequent substructures, and performs self-conditioned RL generation by filtering sampled batches to molecules containing those substructures (only such molecules are evaluated by the oracle and used to update the policy).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty relative to training set is not reported explicitly (no Tanimoto or %outside-training-set metric given); qualitative statements and quantitative outputs show many unique generated molecules and scaffolds above reward thresholds (e.g., substantially increased counts of unique high-reward molecules and unique high-reward scaffolds up to 19x in some analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced via reward-guided RL objectives (e.g., docking score minimization, QED maximization, MW constraints, tPSA/ring-count objectives) and by restricting oracle evaluation to molecules containing extracted substructures (self-conditioning); substructures are extracted from the model's own conditional token probabilities after it begins improving on the objective.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Generative Yield (count of unique generated molecules above a reward threshold), Oracle Burden (number of oracle calls required to obtain N unique molecules above threshold), standard property objectives (AutoDock Vina docking score, QED, MW constraints, tPSA, ring counts), diversity metrics (IntDiv1), synthetic accessibility (SA score), counts of unique Bemis–Murcko scaffolds, and statistical tests (Welch's t-test across replicates).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Beam Enumeration extracts meaningful substructures (Scaffold or Structure) from partial SMILES trajectories and, when used to self-condition generation, substantially improves sample efficiency versus baseline RL: increases Generative Yield and decreases Oracle Burden across illustrative and three docking case studies (DRD2, MK2, AChE). Reported gains include >4x improvement in Yield >0.8 for Augmented Memory across case studies and up to a 29-fold increase (MK2) in finding high-reward molecules under limited oracle budgets; Beam Enumeration enabled finding many high-reward docking candidates with only ~2,000 Vina calls in some tasks. Improvements come with a modest reduction in diversity and require hyperparameter tuning (Beam k, Beam steps N, Pool size, Patience, Structure Minimum Size, token-sampling method).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Applied on top of existing language-based RL algorithms (Augmented Memory and REINVENT) and consistently improved sample efficiency compared to those base methods; the paper reports Augmented Memory+Beam substantially outperforms REINVENT baseline and baseline Augmented Memory in Yield and Oracle Burden across tasks. Beam Enumeration differs from standard beam search by exhaustively enumerating high-probability subsequences (not only top complete sequences) to extract substructures for self-conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Enumerating subsequences grows exponentially with beam steps (k^N), constraining feasible beam depth (practical upper tested N≈18) and requiring careful hyperparameter choices; many partial subsequences do not map to valid molecular fragments, so signal is sparse and relies on sufficient enumeration; Structure extraction can be too permissive (small functional groups) unless a minimum size is enforced; self-conditioning biases generation (reduced diversity) which may be undesirable in some settings; 'sample' token-sampling leads to higher variance and can cause many 'filter rounds' where no sampled molecule contains the extracted substructures (increasing runtime); correctness of extracted substructures depends on the oracle quality (explainability explains oracle behavior, not necessarily true target biology); sparse-reward environments remain difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8879.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8879.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Augmented Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmented Memory (experience replay accelerated SMILES-based RL for molecular design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-based, SMILES autoregressive generative algorithm that builds on REINVENT by using experience replay to accelerate de novo molecular design and improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Augmented Memory</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>SMILES-based autoregressive recurrent neural network (RNN, LSTM cells) with reinforcement learning and experience replay (language-based molecular generative model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained prior model trained on ChEMBL (the pre-trained Prior is cited and used as the initial model in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular design for drug discovery, tested on multiple optimization tasks including docking-guided inhibitor design (DRD2, MK2, AChE) and illustrative property optimization (tPSA, MW, ring count).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>On-policy reinforcement learning (REINVENT-style objective) optimizing an Augmented Likelihood combining prior log-probability and scaled reward; uses experience replay to learn from high-reward samples and update the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Explicit novelty vs training set not reported; empirical results show Augmented Memory produces more unique high-reward molecules and scaffolds than REINVENT under fixed oracle budgets (e.g., much higher Generative Yield and lower Oracle Burden).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Task specificity is achieved by defining scalar reward functions that combine oracle outputs (e.g., docking score), property filters (QED, MW), and RL optimization of the augmented likelihood; also compatible with Beam Enumeration self-conditioning to bias generation toward substructures inferred to be high-reward.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Generative Yield, Oracle Burden, reward trajectories, docking scores (AutoDock Vina), QED, MW constraints, SA score, diversity (IntDiv1), and counts of unique scaffolds; statistical comparison across replicates.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Augmented Memory is more sample efficient than REINVENT in the tested tasks; when augmented with Beam Enumeration, sample efficiency improved further (greater Yield and lower Oracle Burden). For example, Augmented Memory+Beam produced >4x more molecules above 0.8 reward in all docking cases and up to 29x improvement on MK2 in one measure, and found many high-reward molecules within 2,000 Vina calls in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to REINVENT (SMILES-based RL) and found to be significantly more sample-efficient; paper notes Augmented Memory achieved state-of-the-art on the Practical Molecular Optimization benchmark prior to adding Beam Enumeration. Beam Enumeration further improved Augmented Memory performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Performance depends on the quality of the Prior and oracle; without additional methods like Beam Enumeration, sample efficiency may still be insufficient for very expensive high-fidelity oracles; baseline Augmented Memory could fail to find many >0.8-reward molecules in some replicates for challenging tasks under tight oracle budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8879.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8879.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REINVENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REINVENT (Reinforcement learning for de novo molecular design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMILES-based LSTM autoregressive molecular generative model that uses reinforcement learning to bias generation toward molecules with high reward for user-defined objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular de-novo design through deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REINVENT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>SMILES-based autoregressive recurrent neural network (LSTM) optimized with reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained prior model trained on ChEMBL (used as a prior in experiments when relevant).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular design for drug discovery; used as a baseline model in docking-guided optimization case studies (DRD2, MK2, AChE).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>On-policy reinforcement learning applied to SMILES generation, optimizing a reward function (e.g., docking score, QED, MW constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not explicitly quantified in this paper; shown empirically to produce fewer unique high-reward molecules and higher Oracle Burden compared to Augmented Memory (and Augmented Memory+Beam) under the same oracle budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Generates molecules tailored to objectives via reward engineering (docking, druglikeness, MW constraints); Beam Enumeration can be applied on top of REINVENT to improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Generative Yield, Oracle Burden, docking scores, QED, MW constraints, diversity (IntDiv1), SA score; comparisons across algorithms under fixed oracle budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>REINVENT is less sample efficient than Augmented Memory in the tasks presented; adding Beam Enumeration on top of REINVENT improved performance but gains were more pronounced when applied to Augmented Memory. In docking case studies REINVENT baseline produced significantly fewer high-reward molecules and required more oracle calls.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as a baseline for comparison; Augmented Memory (and Augmented Memory+Beam) outperform REINVENT in Yield and Oracle Burden under the same experimental budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Lower sample efficiency compared to more recent algorithms (Augmented Memory); performance sensitive to prior and reward shaping; benefits from augmentation (e.g., Beam Enumeration) to reduce oracle usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design. <em>(Rating: 2)</em></li>
                <li>Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization <em>(Rating: 2)</em></li>
                <li>Molecular de-novo design through deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Beam search for automated design and scoring of novel ror ligands with machine intelligence. <em>(Rating: 1)</em></li>
                <li>Leveraging molecular structure and bioactivity with chemical language models for de novo drug design. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8879",
    "paper_id": "paper-7cb303190a8910df8a29aa142cffe285fc88c431",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "Beam Enumeration",
            "name_full": "Beam Enumeration (probabilistic subsequence enumeration for language-based molecular generative models)",
            "brief_description": "A task-agnostic algorithm that exhaustively enumerates high-probability token sub-sequences from an autoregressive SMILES language model to extract frequent molecular substructures and then self-condition generation by filtering sampled molecules to those containing these substructures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Beam Enumeration (method applied to language-based SMILES models)",
            "model_type": "Method applied to autoregressive language-based molecular generative models (SMILES-based autoregressive RNN/LSTM in this work)",
            "model_size": null,
            "training_data": "Not a model; operates on a pre-trained language-based generative model (the paper uses priors trained on ChEMBL).",
            "application_domain": "De novo molecular design for drug discovery (property optimization and docking-guided inhibitor design in case studies).",
            "generation_method": "Enumerates top-k token probabilities sequentially for N beam steps to produce an exhaustive set of partial SMILES subsequences; extracts valid substructures (Scaffold or Structure), maintains a pool of most frequent substructures, and performs self-conditioned RL generation by filtering sampled batches to molecules containing those substructures (only such molecules are evaluated by the oracle and used to update the policy).",
            "novelty_of_chemicals": "Novelty relative to training set is not reported explicitly (no Tanimoto or %outside-training-set metric given); qualitative statements and quantitative outputs show many unique generated molecules and scaffolds above reward thresholds (e.g., substantially increased counts of unique high-reward molecules and unique high-reward scaffolds up to 19x in some analyses).",
            "application_specificity": "Specificity enforced via reward-guided RL objectives (e.g., docking score minimization, QED maximization, MW constraints, tPSA/ring-count objectives) and by restricting oracle evaluation to molecules containing extracted substructures (self-conditioning); substructures are extracted from the model's own conditional token probabilities after it begins improving on the objective.",
            "evaluation_metrics": "Generative Yield (count of unique generated molecules above a reward threshold), Oracle Burden (number of oracle calls required to obtain N unique molecules above threshold), standard property objectives (AutoDock Vina docking score, QED, MW constraints, tPSA, ring counts), diversity metrics (IntDiv1), synthetic accessibility (SA score), counts of unique Bemis–Murcko scaffolds, and statistical tests (Welch's t-test across replicates).",
            "results_summary": "Beam Enumeration extracts meaningful substructures (Scaffold or Structure) from partial SMILES trajectories and, when used to self-condition generation, substantially improves sample efficiency versus baseline RL: increases Generative Yield and decreases Oracle Burden across illustrative and three docking case studies (DRD2, MK2, AChE). Reported gains include &gt;4x improvement in Yield &gt;0.8 for Augmented Memory across case studies and up to a 29-fold increase (MK2) in finding high-reward molecules under limited oracle budgets; Beam Enumeration enabled finding many high-reward docking candidates with only ~2,000 Vina calls in some tasks. Improvements come with a modest reduction in diversity and require hyperparameter tuning (Beam k, Beam steps N, Pool size, Patience, Structure Minimum Size, token-sampling method).",
            "comparison_to_other_methods": "Applied on top of existing language-based RL algorithms (Augmented Memory and REINVENT) and consistently improved sample efficiency compared to those base methods; the paper reports Augmented Memory+Beam substantially outperforms REINVENT baseline and baseline Augmented Memory in Yield and Oracle Burden across tasks. Beam Enumeration differs from standard beam search by exhaustively enumerating high-probability subsequences (not only top complete sequences) to extract substructures for self-conditioning.",
            "limitations_and_challenges": "Enumerating subsequences grows exponentially with beam steps (k^N), constraining feasible beam depth (practical upper tested N≈18) and requiring careful hyperparameter choices; many partial subsequences do not map to valid molecular fragments, so signal is sparse and relies on sufficient enumeration; Structure extraction can be too permissive (small functional groups) unless a minimum size is enforced; self-conditioning biases generation (reduced diversity) which may be undesirable in some settings; 'sample' token-sampling leads to higher variance and can cause many 'filter rounds' where no sampled molecule contains the extracted substructures (increasing runtime); correctness of extracted substructures depends on the oracle quality (explainability explains oracle behavior, not necessarily true target biology); sparse-reward environments remain difficult.",
            "uuid": "e8879.0",
            "source_info": {
                "paper_title": "Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Augmented Memory",
            "name_full": "Augmented Memory (experience replay accelerated SMILES-based RL for molecular design)",
            "brief_description": "An RL-based, SMILES autoregressive generative algorithm that builds on REINVENT by using experience replay to accelerate de novo molecular design and improve sample efficiency.",
            "citation_title": "Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design.",
            "mention_or_use": "use",
            "model_name": "Augmented Memory",
            "model_type": "SMILES-based autoregressive recurrent neural network (RNN, LSTM cells) with reinforcement learning and experience replay (language-based molecular generative model).",
            "model_size": null,
            "training_data": "Pre-trained prior model trained on ChEMBL (the pre-trained Prior is cited and used as the initial model in experiments).",
            "application_domain": "De novo molecular design for drug discovery, tested on multiple optimization tasks including docking-guided inhibitor design (DRD2, MK2, AChE) and illustrative property optimization (tPSA, MW, ring count).",
            "generation_method": "On-policy reinforcement learning (REINVENT-style objective) optimizing an Augmented Likelihood combining prior log-probability and scaled reward; uses experience replay to learn from high-reward samples and update the generator.",
            "novelty_of_chemicals": "Explicit novelty vs training set not reported; empirical results show Augmented Memory produces more unique high-reward molecules and scaffolds than REINVENT under fixed oracle budgets (e.g., much higher Generative Yield and lower Oracle Burden).",
            "application_specificity": "Task specificity is achieved by defining scalar reward functions that combine oracle outputs (e.g., docking score), property filters (QED, MW), and RL optimization of the augmented likelihood; also compatible with Beam Enumeration self-conditioning to bias generation toward substructures inferred to be high-reward.",
            "evaluation_metrics": "Generative Yield, Oracle Burden, reward trajectories, docking scores (AutoDock Vina), QED, MW constraints, SA score, diversity (IntDiv1), and counts of unique scaffolds; statistical comparison across replicates.",
            "results_summary": "Augmented Memory is more sample efficient than REINVENT in the tested tasks; when augmented with Beam Enumeration, sample efficiency improved further (greater Yield and lower Oracle Burden). For example, Augmented Memory+Beam produced &gt;4x more molecules above 0.8 reward in all docking cases and up to 29x improvement on MK2 in one measure, and found many high-reward molecules within 2,000 Vina calls in some tasks.",
            "comparison_to_other_methods": "Compared directly to REINVENT (SMILES-based RL) and found to be significantly more sample-efficient; paper notes Augmented Memory achieved state-of-the-art on the Practical Molecular Optimization benchmark prior to adding Beam Enumeration. Beam Enumeration further improved Augmented Memory performance.",
            "limitations_and_challenges": "Performance depends on the quality of the Prior and oracle; without additional methods like Beam Enumeration, sample efficiency may still be insufficient for very expensive high-fidelity oracles; baseline Augmented Memory could fail to find many &gt;0.8-reward molecules in some replicates for challenging tasks under tight oracle budgets.",
            "uuid": "e8879.1",
            "source_info": {
                "paper_title": "Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "REINVENT",
            "name_full": "REINVENT (Reinforcement learning for de novo molecular design)",
            "brief_description": "A SMILES-based LSTM autoregressive molecular generative model that uses reinforcement learning to bias generation toward molecules with high reward for user-defined objectives.",
            "citation_title": "Molecular de-novo design through deep reinforcement learning.",
            "mention_or_use": "use",
            "model_name": "REINVENT",
            "model_type": "SMILES-based autoregressive recurrent neural network (LSTM) optimized with reinforcement learning.",
            "model_size": null,
            "training_data": "Pre-trained prior model trained on ChEMBL (used as a prior in experiments when relevant).",
            "application_domain": "De novo molecular design for drug discovery; used as a baseline model in docking-guided optimization case studies (DRD2, MK2, AChE).",
            "generation_method": "On-policy reinforcement learning applied to SMILES generation, optimizing a reward function (e.g., docking score, QED, MW constraints).",
            "novelty_of_chemicals": "Not explicitly quantified in this paper; shown empirically to produce fewer unique high-reward molecules and higher Oracle Burden compared to Augmented Memory (and Augmented Memory+Beam) under the same oracle budgets.",
            "application_specificity": "Generates molecules tailored to objectives via reward engineering (docking, druglikeness, MW constraints); Beam Enumeration can be applied on top of REINVENT to improve sample efficiency.",
            "evaluation_metrics": "Generative Yield, Oracle Burden, docking scores, QED, MW constraints, diversity (IntDiv1), SA score; comparisons across algorithms under fixed oracle budgets.",
            "results_summary": "REINVENT is less sample efficient than Augmented Memory in the tasks presented; adding Beam Enumeration on top of REINVENT improved performance but gains were more pronounced when applied to Augmented Memory. In docking case studies REINVENT baseline produced significantly fewer high-reward molecules and required more oracle calls.",
            "comparison_to_other_methods": "Used as a baseline for comparison; Augmented Memory (and Augmented Memory+Beam) outperform REINVENT in Yield and Oracle Burden under the same experimental budgets.",
            "limitations_and_challenges": "Lower sample efficiency compared to more recent algorithms (Augmented Memory); performance sensitive to prior and reward shaping; benefits from augmentation (e.g., Beam Enumeration) to reduce oracle usage.",
            "uuid": "e8879.2",
            "source_info": {
                "paper_title": "Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design.",
            "rating": 2
        },
        {
            "paper_title": "Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization",
            "rating": 2
        },
        {
            "paper_title": "Molecular de-novo design through deep reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Beam search for automated design and scoring of novel ror ligands with machine intelligence.",
            "rating": 1
        },
        {
            "paper_title": "Leveraging molecular structure and bioactivity with chemical language models for de novo drug design.",
            "rating": 1
        }
    ],
    "cost": 0.012886249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-Conditioned Molecular Design</h1>
<p>Jeff Guo \&amp; Philippe Schwaller<br>Laboratory of Artificial Chemical Intelligence (LIAC), Institut des Sciences et Ingénierie Chimiques<br>National Centre of Competence in Research (NCCR) Catalysis<br>École Polytechnique Fédérale de Lausanne (EPFL)<br>Lausanne, Switzerland<br>{jeff.guo,philippe.schwaller}@epfl.ch</p>
<h4>Abstract</h4>
<p>Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration shows that improvements to explainability and sample efficiency for molecular design can be made synergistic. The code is available at https://github.com/schwallergroup/augmented_memory.</p>
<h2>1 INTRODUCTION</h2>
<p>Molecular discovery requires identifying candidate molecules possessing desired properties amidst an enormous chemical space (Sanchez-Lengeling \&amp; Aspuru-Guzik (2018). Generative molecular design has become a popular paradigm in drug discovery, offering the potential to navigate chemical space more efficiently with promise for accelerated discovery. Very recently, efforts have come to fruition and a large number of works have reported experimental validation of generated inhibitors, notably for both distribution learning (Merk et al. (2018); Moret et al. (2021); Grisoni et al. (2021); Yu et al. (2021); Eguida et al. (2022); Li et al. (2022); Tan et al. (2021); Jang et al. (2022); Chen et al. (2022); Hua et al. (2022); Song et al. (2023); Moret et al. (2023); Ballarotto et al. (2023) and goaldirected generation (Korshunova et al. (2022); Yoshimori et al. (2021); Zhavoronkov et al. (2019); Ren et al. (2023); Li et al. (2023); Salas-Estrada et al. (2023) approaches. Perhaps now more than ever, existing challenges in explainability and sample efficiency offer an avenue to propel generative molecular design towards outcomes that are not yet possible. Specifically, if one can elucidate why certain substructures or molecules satisfy a target objective, the model's knowledge can be made actionable, for example, in an interplay with domain experts. Moreover, sample efficiency concerns with how many experiments, i.e., oracle calls, are required for a model to optimize the target objective. This is a pressing problem as the most informative high-fidelity oracles are computationally expensive, e.g., molecular dynamics (MD) for binding energy prediction (Wang et al. (2015); Moore et al. (2023). If a generative model can directly optimize these expensive oracles, the capabilities of generative design can be vastly advanced.</p>
<p>In this work, we propose Beam Enumeration to exhaustively enumerate the most probable token sub-sequences in language-based molecular generative models and show that valid molecular substructures can be extracted from these partial trajectories. We demonstrate that the extracted substructures are informative when coupled with reinforcement learning (RL) and show that this information can be made actionable to self-condition the model's generation by only evaluating sampled molecules containing these substructures with the oracle. The results show significantly enhanced sample efficiency with an expected small trade-off in diversity. Beam Enumeration jointly addresses explainability and sample efficiency. Our contribution is as follows:</p>
<ol>
<li>We propose Beam Enumeration as a task-agnostic method to exhaustively enumerate subsequences and show that molecular substructures can be extracted.</li>
<li>We demonstrate that during the course of RL, extracted substructures are on track to yield high rewards and can be used for self-conditioned molecular generation. We extract structural insights from these substructures, thereby providing a source of explainability.</li>
<li>We perform exhaustive hyperparameter investigations (2,224 experiments and 144 with molecular docking) and provide insights on the predictable behavior of Beam Enumeration and recommend default hyperparameters for out-of-the-box applications.</li>
<li>We introduce a new metric: Oracle Burden, which measures how many oracle calls are required to generate N unique molecules above a reward threshold as one is often interested in identifying a small set of excellent candidate molecules amongst many good ones.</li>
<li>We combine Beam Enumeration with the recently reported Augmented Memory (Guo \&amp; Schwaller (2023) optimization algorithm and show that the sample efficiency becomes sufficient (up to a 29 -fold increase) to find high reward molecules satisfying a docking objective with only 2,000 oracle calls in three drug discovery case studies.</li>
</ol>
<h1>2 Related Work</h1>
<p>Sample Efficiency in Molecular Design. Tailored molecular generation is vital for practical applications as every use case requires optimizing for a bespoke property profile. Over the past several years, so-called goal-directed generation has been achieved using a variety of architectures, including Simplified molecular-input line-entry system (SMILES) (Weininger (1988)-based recurrent neural networks (RNNs) (Olivecrona et al. (2017); Popova et al. (2018); Segler et al. (2018); Goel et al. (2021), generative adversarial networks (GANs) (Goodfellow et al. (2014); Sanchez-Lengeling et al. (2017); Guimaraes et al. (2018), variational autoencoders (VAEs) (Kingma \&amp; Welling (2022); Gómez-Bombarelli et al. (2018); Zhavoronkov et al. (2019), graph-based models (You et al. (2019); Jin et al. (2020); Mercado et al. (2021c); Atance et al. (2022), GFlowNets (Bengio et al. (2021a), and genetic algorithms (Fu et al. (2022a). However, while all methods can be successful in optimizing for various properties, the oracle budget, i.e., how many oracle calls (computational calculations) were required to do so, is rarely reported. To address this, Gao et al. (Gao et al. (2022) proposed the Practical Molecular Optimization (PMO) benchmark, which assesses 25 models across 23 tasks and enforces a budget of 10,000 oracle calls. Recently, Guo et al. proposed Augmented Memory (Guo \&amp; Schwaller (2023), which uses a language-based molecular generative model and achieves the new state-of-the-art on the PMO benchmark. In this work, Beam Enumeration is proposed as an addition to language-based molecular generative models, and we show that coupling it with Augmented Memory drastically improves the sample efficiency.
Explainability for Molecules. Explainable AI (XAI) (Gohel et al. (2021) to interpret and explain model predictions is a vital component for decision-making. Existing methods include Gradientweighted Class Activation Mapping (Grad-CAM) (Selvaraju et al. (2017), which uses gradient-based heat maps for convolutional layers and Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al. (2016), which uses a locally interpretable model. Other methods include permutation importance (Altmann et al. (2010), which measures the performance change when shuffling feature values, and SHAP values (Shapley (1953), which measure the average contribution of each feature to the model's prediction. For molecules, the Molecular Model Agnostic Counterfactual Explanations (MMACE) (Wellawatte et al. (2022) method was proposed to search for the most similar counterfactual (model predicts the opposite label) molecule. Recently, the pBRICS (Vangala et al. (2023) algorithm was proposed to decompose a molecule into functional groups and then applying</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Beam Enumeration overview. a. The proposed method proceeds via 4 steps: 1. generate batch of molecules. 2. filter molecules based on pool to enforce substructure presence, discarding the rest. 3. compute reward 4. update the model. After updating the model, if the reward has improved for consecutive epochs, execute Beam Enumeration. b. Beam Enumeration sequentially enumerates the top $k$ tokens by probability for $N$ beam steps, resulting in an exhaustive set of token sub-sequences. c. All valid substructures (either by the Structure or Scaffold criterion) are extracted from the sub-sequences. The most frequent substructures are used for self-conditioned generation.</p>
<p>Grad-CAM to explain matched molecular pairs (MMPs), i.e., pairs of molecules differing by a single chemical group. In the context of generative models, previous works have explicitly addressed explainability (Guo et al. (2022); Fu et al. (2022b) and jointly with sample efficiency (Fu et al. (2022b). In this work, we aim to make explainability actionable during a generative design experiment.</p>
<p>To achieve this, we introduce Beam Enumeration, which extracts molecular substructures directly from the model's token sampling probabilities and derives explainability from a generative probabilistic perspective that is modulated by reward feedback. Our approach is based on the fact that during a successful optimization trajectory, it must become increasingly likely to generate desirable molecules. It is thus reasonable to hypothesize that the most probable substructures are on track to receiving high reward. We verify this statement in the Results section and show that Beam Enumeration can also jointly address explainability and sample efficiency.</p>
<h1>3 Proposed Method: Beam Enumeration</h1>
<p>In this section, each component of Beam Enumeration (Fig. 1) is described: the base molecular generative model, the Beam Enumeration algorithm, and how Beam Enumeration harnesses the model's built-in explainability which can be used to improve sample efficiency through self-conditioned generation (further details on Beam Enumeration are presented in Appendix A).</p>
<p>Autoregressive Language-based Molecular Generative Model. The starting point of Beam Enumeration is any autoregressive language-based molecular generative model. The specific model used in this work is Augmented Memory (Guo \&amp; Schwaller (2023) which recently achieved the new state-of-the-art performance on the PMO (Gao et al. (2022) benchmark for sample efficiency, outperforming modern graph neural network-based approaches (Fu et al. (2020); Xie et al. (2021) and GFlowNets (Bengio et al. (2021b). Augmented Memory builds on REINVENT (Olivecrona et al. (2017); Blaschke et al. (2020) which is a SMILES-based (Weininger (1988) RNN using long-shortterm memory (LSTM) cells (Hochreiter \&amp; Schmidhuber (1997). The optimization process is cast as an on-policy RL problem. We define the state space, $S_{t}$, as all intermediate token sequences and the action space, $A_{t}\left(s_{t}\right)$, as the token sampling probabilities (conditioned on a given state). $A_{t}\left(s_{t}\right)$ is given by the policy, $\pi_{\theta}$, which is parameterized by the RNN. The objective is to iteratively update the policy such that token sampling, $A_{t}\left(s_{t}\right)$, yields trajectories (SMILES) with increasing reward.</p>
<p>Formally, sampling a SMILES, $x$, is given by the product of conditional state probabilities (Equation 1), and the token sampling is Markovian:</p>
<p>$$
P(x)=\prod_{t=1}^{T} P\left(s_{t} \mid s_{t-1}, s_{t-2}, \ldots, s_{1}\right)
$$</p>
<p>The Augmented Likelihood is defined, (Equation 2) where the Prior is the pre-trained model and $S$ is the objective function which yields a reward given a SMILES, $x$.</p>
<p>$$
\log \pi_{\theta_{\text {Augmented }}}=\log \pi_{\theta_{\text {Prior }}}+\sigma S(x)
$$</p>
<p>The policy is directly optimized by minimizing the squared difference between the Augmented Likelihood and the Agent Likelihood given a sampled batch, $B$, of SMILES constructed following the actions, $a \in A^{*}$ (Equation 3):</p>
<p>$$
L(\theta)=\frac{1}{|B|} \cdot\left[\sum_{a \in A^{*}}\left(\log \pi_{\theta_{\text {Augmented }}}-\log \pi_{\theta_{\text {Agent }}}\right)\right]^{2}
$$</p>
<p>Minimizing $L(\theta)$ is equivalent to maximizing the expected reward as shown previously (Guo \&amp; Schwaller (2023); Fialková et al. (2022).</p>
<p>Beam Enumeration. Beam Enumeration is proposed based on two facts: firstly, on a successful optimization trajectory, the model's weights must change such that it becomes increasingly likely to generate high reward molecules. Secondly, generation involves sampling from conditional probability distributions. It is therefore reasonable to assume that the highest probability trajectories are more likely to yield high reward. Correspondingly, Beam Enumeration (Fig. 1) enumerates the top $k$ tokens (by probability) sequentially for $N$ beam steps (as it is infeasible to sample the full trajectories due to combinatorial explosion), resulting in an exhaustive set of token sub-sequences. We show that meaningful molecular substructures can be extracted from these sub-sequences, which we harness and demonstrate how it can be made actionable. We note the closest work to ours is the application of Beam Search (Graves (2012); Boulanger-Lewandowski et al. (2013) for molecular design (Moret et al. (2021) to find the highest probability trajectories. Our work differs as the objective is not to find a small set of the most probable sequences. Rather, we exhaustively enumerate the highest probability sub-sequences to extract molecular substructures for self-conditioned generation. We further detail the differences to Beam Search in Appendix G.</p>
<p>Probabilistic Explainability. Here, we describe how probabilistic explainability can be extracted from the exhaustive set of token sub-sequences. Usually, token sequences are only translated into SMILES once the sequence is complete, i.e., the "end" token has been sampled. We hypothesized that molecular substructures can be extracted from a given sub-sequence by iteratively considering every (sub)-sub-sequence (Fig. 1). For example, given the sub-sequence "ABCD", the set of (sub)-sub-sequences are: "A", "AB", "ABC", and "ABCD". In practice, we only consider (sub)-subsequences with at least three characters ("ABC" and "ABCD") since each character loosely maps to one atom and three is approximately the minimum for meaningful functional groups, e.g., "C=O", a carbonyl. It is expected that not every sub-sequence possesses (sub)-sub-sequences mapping to valid molecular substructures. Still, we show that a sufficient signal can be extracted (Appendix C). Finally, we implement two types of substructures: Scaffold, which extracts the Bemis-Murcko (Bemis \&amp; Murcko (1996) scaffold and Structure, which extracts any valid substructure. In the Results section, we discuss the predictable difference in behavior.</p>
<p>Self-conditioned Generation. The sub-sequences were enumerated by taking the most probable $k$ tokens, and the model's weights should be updated such that high reward molecules are increasingly likely to be generated. Correspondingly, it is reasonable to posit that the most frequent molecular substructures are on track to becoming high reward full molecules and that the substructures themselves possess properties aligned with the target objective. Beam Enumeration saves a Pool of these substructures and filters future sampled batches of molecules to contain them, discarding those that do not. Effectively, the generative process is self-conditioned as the model will only be updated by generated molecules containing the extracted substructures (Fig. 1).</p>
<h1>4 METRICS</h1>
<p>Sample Efficiency Metrics. We define two metrics to assess sample efficiency: Generative Yield (referred to as Yield from now on) and Oracle Burden. Yield (Equation 4) is defined as the number of unique generated molecules above a reward threshold, where $g \in G$ are the molecules in the generated set, $\mathbb{I}$ is the indicator function which returns 1 if the reward, $R(g)$, is above a threshold, $T$. Yield is a useful metric for drug discovery as it is ubiquitous to triage the generated set (thus, a higher Yield is desirable) and prioritize molecules, e.g., based on synthetic feasibility, for experimental validation or more expensive computational oracles.</p>
<p>$$
\text { Generative Yield }=\sum_{g=1}^{G} \mathbb{I}[R(g)&gt;T]
$$</p>
<p>Oracle Burden (Equation 5) is defined as the number of oracle calls (c) required to generate $N$ unique molecules above a reward threshold. This is a direct measure of sample efficiency as high reward molecules satisfy the target objective, and the metric becomes increasingly important with expensive high-fidelity oracles. In this work, all Oracle Burden metrics are computed by not allowing more than 10 molecules to possess the same Bemis-Murcko (Bemis \&amp; Murcko (1996) scaffold, thus also explicitly considering diversity in the generated set.</p>
<p>$$
\text { Oracle Burden }=c \mid \sum_{g=1}^{G} \mathbb{I}[R(g)&gt;T]=N
$$</p>
<h2>5 ReSults and DisCussion</h2>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustrative experiment with the following multi-parameter optimization objective: maximize tPSA, molecular weight $&lt;350$ Da, number of rings $\geq 2$. a. Augmented Memory (Guo \&amp; Schwaller (2023) reward trajectory with annotated top-4 (excluding benzene) most frequent molecular substructure scaffolds at varying epochs using Beam Enumeration. b. Examples of molecules with high reward.</p>
<p>We first design an illustrative experiment to demonstrate the feasibility of Beam Enumeration to extract meaningful substructures and, in turn, enable self-conditioned generation. Next, three drug discovery case studies to design inhibitors against dopamine type 2 receptor (DRD2) (Wang et al. (2018), MK2 kinase (Argiriadi et al. (2010), and acetylcholinesterase (AChE) (Kryger et al. (1999) were performed to demonstrate real-world application. The key result we convey is that Beam Enumeration can be added directly to existing algorithms, and it both provides structural insights into why certain molecules receive high rewards and improves sample efficiency to not only generate more high reward molecules, but also faster, given a fixed oracle budget.</p>
<p>Table 1: Illustrative experiment: Beam Enumeration improves the sample efficiency of Augmented Memory. All experiments were run for 100 replicates with an oracle budget of 5,000 calls, and reported values are the mean and standard deviation. Scaffold and Structure indicate the type of substructure, and the number after is the Structure Minimum Size. Parentheses after Oracle Burden denote the cut-off number of molecules. Parentheses after values represent the number of unsuccessful replicates (for achieving the metric).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Augmented Memory</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Beam Scaffold 15</td>
<td style="text-align: center;">Beam Structure 15</td>
<td style="text-align: center;">Beam Scaffold</td>
<td style="text-align: center;">Beam Structure</td>
<td style="text-align: center;">Baseline</td>
</tr>
<tr>
<td style="text-align: left;">Generative Yield ${ }_{&gt;0.7}(\uparrow)$</td>
<td style="text-align: center;">$\mathbf{1 7 5 7} \pm \mathbf{3 0 5}$</td>
<td style="text-align: center;">$1669 \pm 389$</td>
<td style="text-align: center;">$1117 \pm 278$</td>
<td style="text-align: center;">$864 \pm 202$</td>
<td style="text-align: center;">$496 \pm 108$</td>
</tr>
<tr>
<td style="text-align: left;">Generative Yield ${ }_{&gt;0.8}(\uparrow)$</td>
<td style="text-align: center;">$\mathbf{8 1 9} \pm \mathbf{2 9 1}$</td>
<td style="text-align: center;">$700 \pm 389$</td>
<td style="text-align: center;">$425 \pm 256$</td>
<td style="text-align: center;">$199 \pm 122$</td>
<td style="text-align: center;">$85 \pm 56$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle Burden ${ }_{&gt;0.7}(1)(\downarrow)$</td>
<td style="text-align: center;">$\mathbf{5 7 7} \pm \mathbf{3 1 0}$</td>
<td style="text-align: center;">$616 \pm 230$</td>
<td style="text-align: center;">$1037 \pm 414$</td>
<td style="text-align: center;">$897 \pm 347$</td>
<td style="text-align: center;">$1085 \pm 483$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle Burden ${ }_{&gt;0.7}(10)(\downarrow)$</td>
<td style="text-align: center;">$947 \pm 350$</td>
<td style="text-align: center;">$\mathbf{9 2 6} \pm \mathbf{3 3 2}$</td>
<td style="text-align: center;">$1881 \pm 259$</td>
<td style="text-align: center;">$1745 \pm 292$</td>
<td style="text-align: center;">$2392 \pm 216$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle Burden ${ }_{&gt;0.7}(100)(\downarrow)$</td>
<td style="text-align: center;">$\mathbf{1 5 3 0} \pm \mathbf{4 6 8}$</td>
<td style="text-align: center;">$1547 \pm 513$</td>
<td style="text-align: center;">$2736 \pm 335$</td>
<td style="text-align: center;">$2713 \pm 402$</td>
<td style="text-align: center;">$3672 \pm 197$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle Burden ${ }_{&gt;0.8}(1)(\downarrow)$</td>
<td style="text-align: center;">$\mathbf{1 3 1 1} \pm \mathbf{6 2 8}$</td>
<td style="text-align: center;">$1401 \pm 695$</td>
<td style="text-align: center;">$2423 \pm 487$</td>
<td style="text-align: center;">$2295 \pm 482$</td>
<td style="text-align: center;">$3164 \pm 492$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle Burden ${ }_{&gt;0.8}(10)(\downarrow)$</td>
<td style="text-align: center;">$\mathbf{1 7 9 4} \pm \mathbf{6 1 7}$ (1)</td>
<td style="text-align: center;">$2009 \pm 804$ (1)</td>
<td style="text-align: center;">$3124 \pm 497$</td>
<td style="text-align: center;">$3241 \pm 492$</td>
<td style="text-align: center;">$4146 \pm 326$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle Burden ${ }_{&gt;0.8}(100)(\downarrow)$</td>
<td style="text-align: center;">$\mathbf{2 7 0 4} \pm \mathbf{6 8 9}$ (1)</td>
<td style="text-align: center;">$2943 \pm 811$ (6)</td>
<td style="text-align: center;">$3973 \pm 592$ (6)</td>
<td style="text-align: center;">$4415 \pm 437$ (20)</td>
<td style="text-align: center;">$4827 \pm 170$ (69)</td>
</tr>
</tbody>
</table>
<h1>5.1 ILLUSTRATIVE EXPERIMENT</h1>
<p>Extracted Substructures are Meaningful. The illustrative experiment aims to optimize the following multi-parameter optimization (MPO) objective: maximize topological polar surface area (tPSA), molecular weight (MW) $&lt;350$ Da, and number of rings $\geq 2$. This specific MPO was chosen because it is plausible to predict what structural features would be necessary to optimize the objective: rings saturated with heteroatoms. Due to constraining the molecular weight, the model cannot just learn to generate large molecules that would, on average, possess a higher tPSA. Augmented Memory (Guo \&amp; Schwaller (2023) was used to optimize the MPO objective. The reward trajectory tends towards 1, indicating the model gradually learns to satisfy the target objective, as desired (Fig. 2) Next, we investigate the top $k$ and $N$ beam steps parameters for Beam Enumeration and show that while the majority of sub-sequences do not possess valid substructures, a meaningful signal can still be extracted (Appendix C). We hypothesize that the optimal parameters are using a low top $k$ as we are interested in the most probable sub-sequences and large $N$ beam steps, which would enable extracting larger (and potentially more meaningful) substructures. Fig. 2 shows the top four substructures from Beam Enumeration at varying epochs. As hypothesized, the substructures are informative when considering the MPO objective: the most frequent substructures gradually become rings saturated with heteroatoms, which possess a high tPSA.</p>
<p>Self-conditioned Generation Improves Sample Efficiency. Thus far, the results only show that Beam Enumeration can extract meaningful molecular substructures. To enable self-conditioned generation, a criterion is required to decide when to execute Beam Enumeration. We consider when extracted substructures would be meaningful and propose to execute Beam Enumeration when the reward improves for Patience number of successive epochs (to mitigate sampling stochasticity). We combine Beam Enumeration with Augmented Memory (Guo \&amp; Schwaller (2023) and perform an exhaustive hyperparameter grid search (with replicates) using Yield and Oracle Burden as the performance metrics (Appendix A). The results elucidate the behavior of Beam Enumeration with three key observations: firstly, Structure extraction is much more permissive compared to Scaffold and often leads to small functional groups, e.g., carbonyl, being the most frequent substructures which diminish the sample efficiency benefits (Appendix C). Secondly, enforcing larger substructures to be extracted (Structure Minimum Size) improves performance across all hyperparameter combinations. This reinforces that extracted substructures are meaningful as larger substructures heavily bias molecular generation during self-conditioning. If they were not meaningful, sample efficiency would not improve (and would likely be detrimental). Thirdly, Structure extraction while enforcing a higher Structure Minimum Size prevents small functional group extraction which significantly enhances performance.
We perform five experiments ( $\mathrm{N}=100$ replicates each) based on the optimal hyperparameters identified from the grid search: Augmented Memory (Guo \&amp; Schwaller (2023) (baseline) and Augmented Memory with Beam Enumeration (Scaffold and Structure with and without Structure Minimum Size $=15)$. Table 1 shows that Beam Enumeration drastically improves the Yield and Oracle Burden compared to the baseline at both the $&gt;0.7$ and $&gt;0.8$ reward thresholds, particularly when Structure</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Three drug discovery case studies showing the top generated molecule (triplicate experiments) using Augmented Memory (Guo \&amp; Schwaller (2023) with Beam Enumeration Structure Minimum Structure Size $=15$ and the reference ligand. Extracted substructures from Beam Enumeration are highlighted. The multi-parameter optimization objective is: Minimize Vina score, maximize QED, and molecular weight $&lt;500$ Da. The values, with the Synthetic Accessibility (SA) score (Ertl \&amp; Schuffenhauer (2009) are annotated. a. Dopamine type 2 receptor (Wang et al. (2018). b. MK2 kinase (Argiriadi et al. (2010). c. Acetylcholinesterase (Kryger et al. (1999).</p>
<p>Minimum Size $=15$ is enforced. We highlight that the improved sample efficiency is especially significant as baseline Augmented Memory could not find 100 molecules $&gt;0.8$ reward in 69/100 replicates. The results are further compared using Welch's t-test, and all p-values are significant at the $95 \%$ confidence level (Appendix C).</p>
<h1>5.2 Drug Discovery Case Studies</h1>
<p>The positive results from the illustrative experiment suggest that Beam Enumeration can be applied to real-world drug discovery case studies to design inhibitors against DRD2 which is implicated in neurodegenerative diseases (Wang et al. (2018), MK2 kinase which is involved in pro-inflammatory responses (Argiriadi et al. (2010), and AChE which is a target of interest against Alzheimer's disease (Kryger et al. (1999). Following Guo et al. (Guo et al. (2021b); Guo \&amp; Schwaller (2023), we formulate the following MPO objective: minimize the AutoDock Vina (Trott \&amp; Olson (2010) docking score as a proxy for binding affinity, maximize the Quantitative Estimate of Druglikeness (QED) (Bickerton et al. (2012) score, and constrain MW $&lt;500$ Da. The QED and MW objectives prevent the generative model from exploiting the weaknesses of docking algorithms to give inflated docking scores to large, lipophilic molecules, which can be promiscuous binders (Arnott \&amp; Planey (2012). Moreover, an oracle budget of 5,000 Vina calls was enforced which is almost half the budget of the original Augmented Memory (Guo \&amp; Schwaller (2023) work $(9,600)$.</p>
<p>Since the observations made from the illustrative experiment hyperparameter grid search may not be generalizable to docking tasks, we perform an additional hyperparameter grid search (with replicates). The results (Appendix D) show that the optimal hyperparameters from the illustrative experiment are also the optimal hyperparameters across all three drug discovery case studies. We designate these the default hyperparameters and demonstrate the applicability of Beam Enumeration to both Augmented Memory (Guo \&amp; Schwaller (2023) and REINVENT (Olivecrona et al. (2017); Blaschke et al. (2020) which is the second most (behind Augmented Memory) sample efficient model in the PMO (Gao et al. (2022) benchmark.
Qualitative Inspection: Why Were These Molecules Generated? We first show that Augmented Memory with Beam Enumeration generates molecules that satisfy the MPO objective (Fig. 3). We emphasize that results were not cherry-picked and the three generated examples shown are the top 1 (by reward) across triplicate experiments. All molecules possess better Vina scores and higher QED than the reference molecules, as desired. Fig. 3 shows the highlighted substructures extracted using Structure extraction with Structure Minimum Size $=15$ with three key observations: firstly, "uncommon" molecular substructures may be extracted such as the bridged cycle against DRD2. The exact substructure extracted was an amide bond with a long carbon chain which implicitly enforces the bridged cycle, and the Vina pose shows that it fits in the binding cavity with no clashes, despite being a bulky group. Secondly, bicylic or double-ring systems are often extracted (for all case studies), forming central scaffolds of the full molecule. Thirdly, scaffolds with branch points, i.e., a central ring with single carbon bond extensions, are often extracted (for all case studies). These substructures are particularly interesting as they heavily bias what can be generated in the remaining portion of the full molecule. An exemplary example of this is in the first generated molecule against MK2, where the branch points are effectively a part of two other ring systems (Fig. 3). Beam Enumeration can provide insights into the tolerability and suitability of certain substructures in the context of the full molecules (see Appendix D for more examples of extracted substructures). Finally, we posit that the extreme bias of Structure extraction is the reason why it can be more performant than Scaffold. Overall, the extracted substructures are meaningful and act both as a source of generative explainability and can self-direct the generative model into specific regions of chemical space with high reward.
Quantitative Analysis: Sample Efficiency. Next, we reinforce results from previous work showing that Augmented Memory (Guo \&amp; Schwaller (2023) is significantly more sample efficient than REINVENT (Olivecrona et al. (2017); Blaschke et al. (2020) (Table 2). Notably, the Yield of Augmented Memory is much greater than REINVENT at both the $&gt;0.7$ and $&gt;0.8$ reward thresholds, indicating that more high reward molecules are generated. Moreover, Augmented Memory has a lower Oracle Burden than REINVENT in all cases, except for Oracle Burden $&gt;0.8$ (1) for DRD2 and AChE where there is essentially no difference. The reason for this is because molecules with $&gt;0.8$ reward were already generated at epoch 1, indicating the pre-trained model (trained on ChEMBL (Gaulton et al. (2012a)) is a good Prior for these case studies. By contrast, the MK2 case study is considerably more challenging as extremely few $&gt;0.8$ reward molecules are generated under a 5,000 oracle calls budget. Augmented Memory significantly outperforms REINVENT as the latter could not find 10 molecules with reward $&gt;0.8$ (Table 2).
Subsequently, we demonstrate that Beam Enumeration can be applied out-of-the-box on top of Augmented Memory and REINVENT. Firstly, the addition of Beam Enumeration improves the sample efficiency of both base algorithms, as evidenced by the Yield and Oracle Burden metrics in Table 2 with a small trade-off in diversity (Appendix D). However, the benefits are more pronounced in Augmented Memory as observed by the Yield $&gt;0.8$ improving by $&gt;4 \mathrm{x}$ in all cases (MK2 improves by 29x) and the Oracle Burden $&gt;0.8$ (10 and 100) over halved in most cases. Notably, for MK2 Oracle Burden $&gt;0.8$ (100), baseline Augmented Memory could not accomplish the task while Beam Enumeration is successful in almost under 2,000 oracle calls (Table 2). These findings are in agreement with the original Augmented Memory (Guo \&amp; Schwaller (2023) work in that the algorithm is much more data efficient and capitalizes on learning from high reward molecules via experience replay (Lin (1992). Beam Enumeration decreases the diversity of the generated set (as measured by IntDiv1Polykovskiy et al. (2020b), but finds considerably more unique scaffolds above the 0.8 reward threshold (up to 19x) (Appendix D). With many unique scaffolds built around (often) central substructures, the generated set could conceivably provide insights into structure-activity relationships. These results demonstrate that the combined algorithm achieves both exploration and exploitation. Overall, the results show that Beam Enumeration is task-agnostic and can be applied on top of ex-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Table 2: Drug discovery case studies: effect of Beam Enumeration on sample efficiency. All experiments were run in triplicate with an oracle budget of 5,000 calls and reported values are the mean and standard deviation. Scaffold and Structure indicate the type of substructure (Structure Minimum Size $=15$ ) extracted. The Generative Yield and Oracle Burden are reported at varying reward thresholds. Parentheses after Oracle Burden denote the cut-off number of molecules. Best performance is bolded with the exception of Oracle Burden (1) (DRD2/AChE) which have essentially identical performance due to the pre-trained model. * and ** denote one and two replicates were unsuccessful, respectively.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">Target</td>
<td style="text-align: center;">Augmented Memory</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">REINVENT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beam</td>
<td style="text-align: center;">Beam</td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Beam</td>
<td style="text-align: center;">Beam</td>
<td style="text-align: center;">Baseline</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Structure 15</td>
<td style="text-align: center;">Scaffold 15</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Structure 15</td>
<td style="text-align: center;">Scaffold 15</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Generative Yield ${ }_{&gt;0.7}(7)$</td>
<td style="text-align: center;">DRD2</td>
<td style="text-align: center;">3474 $\pm 158$</td>
<td style="text-align: center;">$3412 \pm 95$</td>
<td style="text-align: center;">$2513 \pm 442$</td>
<td style="text-align: center;">$2392 \pm 699$</td>
<td style="text-align: center;">$2686 \pm 235$</td>
<td style="text-align: center;">$1879 \pm 16$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MK2</td>
<td style="text-align: center;">3127 $\pm 138$</td>
<td style="text-align: center;">$2584 \pm 443$</td>
<td style="text-align: center;">$1446 \pm 173$</td>
<td style="text-align: center;">$1822 \pm 444$</td>
<td style="text-align: center;">$1553 \pm 391$</td>
<td style="text-align: center;">$879 \pm 10$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AChE</td>
<td style="text-align: center;">3824 $\pm 162$</td>
<td style="text-align: center;">3902 $\pm 189$</td>
<td style="text-align: center;">$3288 \pm 85$</td>
<td style="text-align: center;">$2511 \pm 369$</td>
<td style="text-align: center;">$2684 \pm 242$</td>
<td style="text-align: center;">$2437 \pm 53$</td>
</tr>
<tr>
<td style="text-align: center;">Generative Yield ${ }_{&gt;0.8}(7)$</td>
<td style="text-align: center;">DRD2</td>
<td style="text-align: center;">1780 $\pm 439$</td>
<td style="text-align: center;">$1607 \pm 379$</td>
<td style="text-align: center;">$363 \pm 195$</td>
<td style="text-align: center;">$417 \pm 275$</td>
<td style="text-align: center;">$687 \pm 366$</td>
<td style="text-align: center;">$102 \pm 6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MK2</td>
<td style="text-align: center;">987 $\pm 211$</td>
<td style="text-align: center;">$523 \pm 438$</td>
<td style="text-align: center;">$34 \pm 13$</td>
<td style="text-align: center;">$179 \pm 241$</td>
<td style="text-align: center;">$19 \pm 7$</td>
<td style="text-align: center;">$2 \pm 0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AChE</td>
<td style="text-align: center;">2059 $\pm 327$</td>
<td style="text-align: center;">2124 $\pm 326$</td>
<td style="text-align: center;">$556 \pm 47$</td>
<td style="text-align: center;">$323 \pm 58$</td>
<td style="text-align: center;">$310 \pm 207$</td>
<td style="text-align: center;">$147 \pm 11$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle Burden ${ }_{&gt;0.8}(1)(↓)$</td>
<td style="text-align: center;">DRD2</td>
<td style="text-align: center;">$126 \pm 90$</td>
<td style="text-align: center;">$83 \pm 29$</td>
<td style="text-align: center;">$187 \pm 51$</td>
<td style="text-align: center;">$63 \pm 0$</td>
<td style="text-align: center;">$127 \pm 52$</td>
<td style="text-align: center;">$168 \pm 149$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MK2</td>
<td style="text-align: center;">736 $\pm 166$</td>
<td style="text-align: center;">$1221 \pm 564$</td>
<td style="text-align: center;">$1360 \pm 543$</td>
<td style="text-align: center;">$1110 \pm 268$</td>
<td style="text-align: center;">$808 \pm 524$</td>
<td style="text-align: center;">$1724 \pm 802$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AChE</td>
<td style="text-align: center;">$105 \pm 29$</td>
<td style="text-align: center;">$63 \pm 0$</td>
<td style="text-align: center;">$62 \pm 0$</td>
<td style="text-align: center;">$62 \pm 0$</td>
<td style="text-align: center;">$84 \pm 29$</td>
<td style="text-align: center;">$83 \pm 29$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle Burden ${ }_{&gt;0.8}(10)(↓)$</td>
<td style="text-align: center;">DRD2</td>
<td style="text-align: center;">$582 \pm 83$</td>
<td style="text-align: center;">571 $\pm 104$</td>
<td style="text-align: center;">$711 \pm 120$</td>
<td style="text-align: center;">$1099 \pm 930$</td>
<td style="text-align: center;">$604 \pm 71$</td>
<td style="text-align: center;">$883 \pm 105$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MK2</td>
<td style="text-align: center;">1122 $\pm 154$</td>
<td style="text-align: center;">$2426 \pm 1525$</td>
<td style="text-align: center;">$3833 \pm 394$</td>
<td style="text-align: center;">$1778 \pm 0^{<em> </em>}$</td>
<td style="text-align: center;">$3891 \pm 631$</td>
<td style="text-align: center;">Failed</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AChE</td>
<td style="text-align: center;">$462 \pm 25$</td>
<td style="text-align: center;">$418 \pm 27$</td>
<td style="text-align: center;">380 $\pm 0$</td>
<td style="text-align: center;">$441 \pm 132$</td>
<td style="text-align: center;">$421 \pm 120$</td>
<td style="text-align: center;">$481 \pm 108$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle Burden ${ }_{&gt;0.8}(100)(↓)$</td>
<td style="text-align: center;">DRD2</td>
<td style="text-align: center;">1120 $\pm 194$</td>
<td style="text-align: center;">$1056 \pm 146$</td>
<td style="text-align: center;">$2558 \pm 30^{*}$</td>
<td style="text-align: center;">$1928 \pm 117^{*}$</td>
<td style="text-align: center;">$2109 \pm 1090$</td>
<td style="text-align: center;">$4595 \pm 0^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MK2</td>
<td style="text-align: center;">2189 $\pm 181$</td>
<td style="text-align: center;">$2676 \pm 403^{*}$</td>
<td style="text-align: center;">Failed</td>
<td style="text-align: center;">$3208 \pm 0^{<em> </em>}$</td>
<td style="text-align: center;">Failed</td>
<td style="text-align: center;">Failed</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AChE</td>
<td style="text-align: center;">$1110 \pm 265$</td>
<td style="text-align: center;">884 $\pm 162$</td>
<td style="text-align: center;">$2021 \pm 89$</td>
<td style="text-align: center;">$3073 \pm 427$</td>
<td style="text-align: center;">$3596 \pm 678$</td>
<td style="text-align: center;">$3931 \pm 286$</td>
</tr>
</tbody>
</table>
<h1>6 CONCLUSION</h1>
<p>In this work, we propose Beam Enumeration to exhaustively enumerate sub-sequences from language-based molecular generative models and show that substructures can be extracted, providing a source of generative explainability. Next, we show that the extracted molecular substructures can be used to self-condition the generative model to only perform oracle evaluation for molecules possessing these substructures (discarding the rest). We show that Beam Enumeration can be coupled with existing RL-based algorithms including Augmented Memory (Guo \&amp; Schwaller (2023) and REINVENT (Olivecrona et al. (2017); Blaschke et al. (2020) to improve their sample efficiency. Moreover, enforcing the extraction of larger substructures improves performance across all hyperparameter combinations. We believe this is a particularly interesting observation as it demonstrates the model's remarkable robustness and tolerability to extreme bias. Subsequently, in three drug discovery case studies to design molecules that dock well, the addition of Beam Enumeration to Augmented Memory and REINVENT substantially improves sample efficiency as assessed by the Yield (number of unique molecules generated above a reward threshold) and Oracle Burden (number of oracle calls required for the model to generate $N$ unique molecules above a reward threshold) with a small trade-off in diversity (which is expected). The extracted substructures themselves provide valuable structural insights, often enforcing the generation of specific cyclic systems and scaffolds with branch points which impose an overall molecular geometry that complements the protein binding cavity. Beam Enumeration shows that improvements to explainability and sample efficiency for molecular design can be made synergistic. The improvements in the latter will enable more expensive high-fidelity oracles to be explicitly optimized. We note, however, that sparse reward environments (Korshunova et al. (2022) remain a difficult optimization task. Finally, Beam Enumeration is a task-agnostic method and can be combined with recent work integrating active learning with molecular generation to further improve sample efficiency (Dodds et al. (2024); Kyro et al. (2023). If the benefits can be synergistic, we may approach sufficient sample efficiency to directly optimize expensive state-of-the-art (in predictive accuracy) physics-based oracles such as MD simulations (Wang et al. (2015); Moore et al. (2023). Excitingly, this would in turn enhance explainability as high-fidelity oracles are inherently more informative.</p>
<h1>7 REPRODUCIbility STATEMENT</h1>
<p>The code is provided in the GitHub link in the Abstract and also provided here: https: //github.com/schwallergroup/augmented_memory. In the repository, there are prepared configuration files that can be directly run to reproduce all experiments in this work.</p>
<h2>REFERENCES</h2>
<p>André Altmann, Laura Toloşi, Oliver Sander, and Thomas Lengauer. Permutation importance: a corrected feature importance measure. Bioinformatics, 26(10):1340-1347, 2010.</p>
<p>Maria A Argiriadi, Anna M Ericsson, Christopher M Harris, David L Banach, David W Borhani, David J Calderwood, Megan D Demers, Jennifer DiMauro, Richard W Dixon, Jennifer Hardman, et al. 2, 4-diaminopyrimidine mk2 inhibitors. part i: observation of an unexpected inhibitor binding mode. Bioorganic \&amp; medicinal chemistry letters, 20(1):330-333, 2010.</p>
<p>John A Arnott and Sonia Lobo Planey. The influence of lipophilicity in drug discovery and design. Expert opinion on drug discovery, 7(10):863-875, 2012.</p>
<p>Sara Romeo Atance, Juan Viguera Diez, Ola Engkvist, Simon Olsson, and Rocío Mercado. De novo drug design using reinforcement learning with graph-based deep generative models. Journal of Chemical Information and Modeling, 62(20):4863-4872, 2022.</p>
<p>Marco Ballarotto, Sabine Willems, Tanja Stiller, Felix Nawa, Julian A Marschner, Francesca Grisoni, and Daniel Merk. De novo design of nurr1 agonists via fragment-augmented generative deep learning in low-data regime. Journal of Medicinal Chemistry, 2023.</p>
<p>Guy W Bemis and Mark A Murcko. The properties of known drugs. 1. molecular frameworks. Journal of medicinal chemistry, 39(15):2887-2893, 1996.</p>
<p>Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34:27381-27394, 2021a.</p>
<p>Yoshua Bengio, Tristan Deleu, Edward J. Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. GFlowNet foundations. CoRR, abs/2111.09266, 2021b.
G. Richard Bickerton, Gaia V. Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L. Hopkins. Quantifying the chemical beauty of drugs. Nature Chem, 4(2):90-98, February 2012. ISSN 17554349. doi: 10.1038/nchem.1243. URL https://www.nature.com/articles/nchem. 1243. Number: 2 Publisher: Nature Publishing Group.</p>
<p>Thomas Blaschke, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. REINVENT 2.0: An AI Tool for De Novo Drug Design. J. Chem. Inf. Model., 60(12):5918-5922, December 2020. ISSN 15499596. doi: 10.1021/acs.jcim.0c00915. URL https://doi.org/10.1021/acs.jcim. 0c00915. Publisher: American Chemical Society.</p>
<p>Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. High-dimensional sequence transduction. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 3178-3182. IEEE, 2013.</p>
<p>Nannan Chen, Lijuan Yang, Na Ding, Guiwen Li, Jiajing Cai, Xiaoli An, Zhijie Wang, Jie Qin, and Yuzhen Niu. Recurrent neural network (rnn) model accelerates the development of antibacterial metronidazole derivatives. RSC advances, 12(35):22893-22901, 2022.</p>
<p>Michael Dodds, Jeff Guo, Thomas Löhr, Alessandro Tibo, Ola Engkvist, and Jon Paul Janet. Sample efficient reinforcement learning with active learning for molecular design. Chem. Sci., 2024. ISSN 2041-6539. doi: 10.1039/d3sc04653b.</p>
<p>Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. Openmm 7: Rapid development of high performance algorithms for molecular dynamics. PLoS computational biology, 13(7):e1005659, 2017.</p>
<p>Merveille Eguida, Christel Schmitt-Valencia, Marcel Hibert, Pascal Villa, and Didier Rognan. Target-focused library design by pocket-applied computer vision and fragment deep generative linking. Journal of Medicinal Chemistry, 65(20):13771-13783, 2022.</p>
<p>Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1:1-11, 2009.</p>
<p>Vendy Fialková, Jiaxi Zhao, Kostas Papadopoulos, Ola Engkvist, Esben Jannik Bjerrum, Thierry Kogej, and Atanas Patronov. LibINVENT: Reaction-based Generative Scaffold Decoration for in Silico Library Design. J. Chem. Inf. Model., 62(9):2046-2063, May 2022. ISSN 1549-9596. doi: 10.1021/acs.jcim.1c00469. URL https://doi.org/10.1021/acs.jcim.1c00469. Publisher: American Chemical Society.</p>
<p>Tianfan Fu, Cao Xiao, Xinhao Li, Lucas M Glass, and Jimeng Sun. MIMOSA: Multi-constraint molecule sampling for molecule optimization. AAAI, 2020.</p>
<p>Tianfan Fu, Wenhao Gao, Connor Coley, and Jimeng Sun. Reinforced genetic algorithm for structure-based drug design. Advances in Neural Information Processing Systems, 35:1232512338, 2022a.</p>
<p>Tianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun. Differentiable scaffolding tree for molecular optimization. International Conference on Learning Representations, 2022b.</p>
<p>Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor W. Coley. Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization, October 2022. URL http://arxiv.org/abs/ 2206.12411. arXiv:2206.12411 [cs, q-bio].</p>
<p>Anna Gaulton, Louisa J. Bellis, A. Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, and John P. Overington. ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic Acids Res, 40 (Database issue):D1100-D1107, January 2012a. ISSN 0305-1048. doi: 10.1093/nar/gkr777. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3245175/.</p>
<p>Anna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, et al. Chembl: a large-scale bioactivity database for drug discovery. Nucleic acids research, 40(D1):D1100-D1107, 2012b.</p>
<p>Manan Goel, Shampa Raghunathan, Siddhartha Laghuvarapu, and U Deva Priyakumar. Molegular: molecule generation using reinforcement learning with alternating rewards. Journal of Chemical Information and Modeling, 61(12):5815-5826, 2021.</p>
<p>Prashant Gohel, Priyanka Singh, and Manoranjan Mohanty. Explainable ai: current status and future directions. arXiv preprint arXiv:2107.07045, 2021.</p>
<p>Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268-276, 2018.</p>
<p>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks, June 2014. URL http: //arxiv.org/abs/1406.2661. arXiv:1406.2661 [cs, stat].</p>
<p>Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711, 2012.</p>
<p>Francesca Grisoni, Berend JH Huisman, Alexander L Button, Michael Moret, Kenneth Atz, Daniel Merk, and Gisbert Schneider. Combining generative artificial intelligence and on-chip synthesis for de novo drug design. Science Advances, 7(24):eabg3338, 2021.</p>
<p>Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha Farias, and Alán Aspuru-Guzik. Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models, February 2018. URL http://arxiv.org/abs/1705. 10843. arXiv:1705.10843 [cs, stat].</p>
<p>Jeff Guo and Philippe Schwaller. Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design. arXiv preprint arXiv:2305.16160, 2023.</p>
<p>Jeff Guo, Jon Paul Janet, Matthias R Bauer, Eva Nittinger, Kathryn A Giblin, Kostas Papadopoulos, Alexey Voronov, Atanas Patronov, Ola Engkvist, and Christian Margreitter. Dockstream: a docking wrapper to enhance de novo molecular design. Journal of cheminformatics, 13(1):1-21, 2021a.</p>
<p>Jeff Guo, Jon Paul Janet, Matthias R. Bauer, Eva Nittinger, Kathryn A. Giblin, Kostas Papadopoulos, Alexey Voronov, Atanas Patronov, Ola Engkvist, and Christian Margreitter. DockStream: a docking wrapper to enhance de novo molecular design. Journal of Cheminformatics, 13(1): 89, November 2021b. ISSN 1758-2946. doi: 10.1186/s13321-021-00563-7. URL https: //doi.org/10.1186/s13321-021-00563-7.</p>
<p>Minghao Guo, Veronika Thost, Beichen Li, Payel Das, Jie Chen, and Wojciech Matusik. Dataefficient graph grammar learning for molecular generation. arXiv preprint arXiv:2203.08031, 2022.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$.</p>
<p>Yi Hua, Xiaobao Fang, Guomeng Xing, Yuan Xu, Li Liang, Chenglong Deng, Xiaowen Dai, Haichun Liu, Tao Lu, Yanmin Zhang, et al. Effective reaction-based de novo strategy for kinase targets: a case study on mertk inhibitors. Journal of Chemical Information and Modeling, 62(7):1654-1668, 2022.</p>
<p>Seong Hun Jang, Dakshinamurthy Sivakumar, Sathish Kumar Mudedla, Jaehan Choi, Sungmin Lee, Minjun Jeon, Suneel Kumar Bvs, Jinha Hwang, Minsung Kang, Eun Gyeong Shin, et al. Pcwa1001, ai-assisted de novo design approach to design a selective inhibitor for flt-3 (d835y) in acute myeloid leukemia. Frontiers in Molecular Biosciences, 9:1072028, 2022.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pp. 2323-2332. PMLR, 2018.</p>
<p>Wengong Jin, Dr Regina Barzilay, and Tommi Jaakkola. Multi-Objective Molecule Generation using Interpretable Substructures. In Proceedings of the 37th International Conference on Machine Learning, pp. 4849-4859. PMLR, November 2020. URL https://proceedings.mlr. press/v119/jin20b.html. ISSN: 2640-3498.</p>
<p>Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes, December 2022. URL http://arxiv.org/abs/1312.6114. arXiv:1312.6114 [cs, stat].</p>
<p>Maria Korshunova, Niles Huang, Stephen Capuzzi, Dmytro S. Radchenko, Olena Savych, Yuriy S. Moroz, Carrow I. Wells, Timothy M. Willson, Alexander Tropsha, and Olexandr Isayev. Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds. Commun Chem, 5(1):1-11, October 2022. ISSN 23993669. doi: 10.1038/s42004-022-00733-0. URL https://www.nature.com/articles/ s42004-022-00733-0. Number: 1 Publisher: Nature Publishing Group.</p>
<p>Gitay Kryger, Israel Silman, and Joel L Sussman. Structure of acetylcholinesterase complexed with e2020 (aricept®): implications for the design of new anti-alzheimer drugs. Structure, 7(3):297307, 1999 .</p>
<p>Gregory W Kyro, Anton Morgunov, Rafael I Brent, and Victor S Batista. Chemspaceal: An efficient active learning methodology applied to protein-specific molecular generation. arXiv preprint arXiv:2309.05853, 2023.</p>
<p>Yangguang Li, Yingtao Liu, Jianping Wu, Xiaosong Liu, Lin Wang, Ju Wang, Jiaojiao Yu, Hongyun Qi, Luoheng Qin, Xiao Ding, et al. Discovery of potent, selective, and orally bioavailable smallmolecule inhibitors of cdk8 for the treatment of cancer. Journal of Medicinal Chemistry, 2023.</p>
<p>Yueshan Li, Liting Zhang, Yifei Wang, Jun Zou, Ruicheng Yang, Xinling Luo, Chengyong Wu, Wei Yang, Chenyu Tian, Haixing Xu, et al. Generative deep learning enables the discovery of a potent and selective ripk1 inhibitor. Nature Communications, 13(1):6891, 2022.</p>
<p>Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8:293-321, 1992.</p>
<p>Rocío Mercado, Esben J Bjerrum, and Ola Engkvist. Exploring graph traversal algorithms in graphbased molecular generation. Journal of Chemical Information and Modeling, 62(9):2093-2100, 2021a.</p>
<p>Rocío Mercado, Tobias Rastemo, Edvard Lindelöf, Günter Klambauer, Ola Engkvist, Hongming Chen, and Esben Jannik Bjerrum. Graph networks for molecular design. Machine Learning: Science and Technology, 2(2):025023, 2021b.</p>
<p>Rocío Mercado, Tobias Rastemo, Edvard Lindelöf, Günter Klambauer, Ola Engkvist, Hongming Chen, and Esben Jannik Bjerrum. Graph networks for molecular design. Mach. Learn.: Sci. Technol., 2(2):025023, March 2021c. ISSN 2632-2153. doi: 10.1088/2632-2153/abcf91. URL https://dx.doi.org/10.1088/2632-2153/abcf91. Publisher: IOP Publishing.</p>
<p>Daniel Merk, Francesca Grisoni, Lukas Friedrich, and Gisbert Schneider. Tuning artificial intelligence on the de novo design of natural-product-inspired retinoid x receptor modulators. Communications Chemistry, 1(1):68, 2018.</p>
<p>J Harry Moore, Christian Margreitter, Jon Paul Janet, Ola Engkvist, Bert L de Groot, and Vytautas Gapsys. Automated relative binding free energy calculations from smiles to $\delta \delta \mathrm{g}$. Communications Chemistry, 6(1):82, 2023.</p>
<p>Michael Moret, Moritz Helmstädter, Francesca Grisoni, Gisbert Schneider, and Daniel Merk. Beam search for automated design and scoring of novel ror ligands with machine intelligence. Angewandte Chemie International Edition, 60(35):19477-19482, 2021.</p>
<p>Michael Moret, Irene Pachon Angona, Leandro Cotos, Shen Yan, Kenneth Atz, Cyrill Brunner, Martin Baumgartner, Francesca Grisoni, and Gisbert Schneider. Leveraging molecular structure and bioactivity with chemical language models for de novo drug design. Nature Communications, 14(1):114, 2023.</p>
<p>Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of Cheminformatics, 9(1):48, September 2017. ISSN 1758-2946. doi: 10.1186/s13321-017-0235-x. URL https://doi.org/10. 1186/s13321-017-0235-x.</p>
<p>Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson, Hongming Chen, Sergey Nikolenko, Alán AspuruGuzik, and Alex Zhavoronkov. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. Frontiers in Pharmacology, 11, 2020a. ISSN 1663-9812. URL https://www.frontiersin.org/articles/10.3389/fphar.2020.565644.</p>
<p>Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models. Frontiers in pharmacology, 11:565644, 2020b.</p>
<p>Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. Science Advances, 4(7):eaap7885, July 2018. doi: 10.1126/sciadv.aap7885. URL https://www.science.org/doi/10.1126/sciadv.aap7885. Publisher: American Association for the Advancement of Science.</p>
<p>Anthony K Rappé, Carla J Casewit, KS Colwell, William A Goddard III, and W Mason Skiff. Uff, a full periodic table force field for molecular mechanics and molecular dynamics simulations. Journal of the American chemical society, 114(25):10024-10035, 1992.</p>
<p>Feng Ren, Xiao Ding, Min Zheng, Mikhail Korzinkin, Xin Cai, Wei Zhu, Alexey Mantsyzov, Alex Aliper, Vladimir Aladinskiy, Zhongying Cao, Shanshan Kong, Xi Long, Bonnie Hei Man Liu, Yingtao Liu, Vladimir Naumov, Anastasia Shneyderman, Ivan V. Ozerov, Ju Wang, Frank W. Pun, Daniil A. Polykovskiy, Chong Sun, Michael Levitt, Alán Aspuru-Guzik, and Alex Zhavoronkov. AlphaFold accelerates artificial intelligence powered drug discovery: efficient discovery of a novel CDK20 small molecule inhibitor. Chemical Science, 14(6):14431452, 2023. doi: 10.1039/D2SC05709C. URL https://pubs.rsc.org/en/content/ articlelanding/2023/sc/d2sc05709c. Publisher: Royal Society of Chemistry.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144, 2016.</p>
<p>Leslie Salas-Estrada, Davide Provasi, Xing Qiu, H Umit Kaniskan, Xi-Ping Huang, Jeffrey DiBerto, Joao Marcelo Lamim Ribeiro, Jian Jin, Bryan L Roth, and Marta Filizola. De novo design of $\kappa$-opioid receptor antagonists using a generative deep learning framework. bioRxiv, pp. 2023-04, 2023.</p>
<p>Benjamin Sanchez-Lengeling and Alán Aspuru-Guzik. Inverse molecular design using machine learning: Generative models for matter engineering. Science, 361(6400):360-365, July 2018. doi: 10.1126/science.aat2663. URL https://www.science.org/doi/10.1126/ science.aat2663. Publisher: American Association for the Advancement of Science.</p>
<p>Benjamin Sanchez-Lengeling, Carlos Outeiral, Gabriel L Guimaraes, and Alan Aspuru-Guzik. Optimizing distributions over molecular space. an objective-reinforced generative adversarial network for inverse-design chemistry (organic). ChemRxiv, 2017.</p>
<p>Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1): $120-131,2018$.</p>
<p>Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626, 2017.</p>
<p>Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10): 1095-1100, 1953.</p>
<p>Shukai Song, Haotian Tang, Ting Ran, Feng Fang, Linjiang Tong, Hongming Chen, Hua Xie, and Xiaoyun Lu. Application of deep generative model for design of pyrrolo [2, 3-d] pyrimidine derivatives as new selective tank binding kinase 1 (tbk1) inhibitors. European Journal of Medicinal Chemistry, 247:115034, 2023.</p>
<p>Xiaoqin Tan, Chunpu Li, Ruirui Yang, Sen Zhao, Fei Li, Xutong Li, Lifan Chen, Xiaozhe Wan, Xiaohong Liu, Tianbiao Yang, et al. Discovery of pyrazolo [3, 4-d] pyridazinone derivatives as selective ddr1 inhibitors via deep learning based design, synthesis, and biological evaluation. Journal of Medicinal Chemistry, 65(1):103-119, 2021.</p>
<p>Oleg Trott and Arthur J Olson. Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455-461, 2010.</p>
<p>Sarveswara Rao Vangala, Sowmya Ramaswamy Krishnan, Navneet Bung, Rajgopal Srinivasan, and Arijit Roy. pbrics: A novel fragmentation method for explainable property prediction of drug-like small molecules. Journal of Chemical Information and Modeling, 2023.</p>
<p>Lingle Wang, Yujie Wu, Yuqing Deng, Byungchan Kim, Levi Pierce, Goran Krilov, Dmitry Lupyan, Shaughnessy Robinson, Markus K Dahlgren, Jeremy Greenwood, et al. Accurate and reliable prediction of relative ligand binding potency in prospective drug discovery by way of a modern free-energy calculation protocol and force field. Journal of the American Chemical Society, 137 (7):2695-2703, 2015.</p>
<p>Sheng Wang, Tao Che, Anat Levit, Brian K Shoichet, Daniel Wacker, and Bryan L Roth. Structure of the d2 dopamine receptor bound to the atypical antipsychotic drug risperidone. Nature, 555 (7695):269-273, 2018.</p>
<p>David Weininger. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):31-36, February 1988. ISSN 00952338. doi: 10.1021/ci00057a005. URL https://doi.org/10.1021/ci00057a005. Publisher: American Chemical Society.</p>
<p>Geemi P Wellawatte, Aditi Seshadri, and Andrew D White. Model agnostic generation of counterfactual explanations for molecules. Chemical science, 13(13):3697-3705, 2022.</p>
<p>Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. MARS: Markov molecular sampling for multi-objective drug discovery. In ICLR, 2021.</p>
<p>Atsushi Yoshimori, Yasunobu Asawa, Enzo Kawasaki, Tomohiko Tasaka, Seiji Matsuda, Toru Sekikawa, Satoshi Tanabe, Masahiro Neya, Hideaki Natsugari, and Chisato Kanai. Design and synthesis of ddr1 inhibitors with a desired pharmacophore using deep generative models. ChemMedChem, 16(6):955-958, 2021.</p>
<p>Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation, February 2019. URL http: //arxiv.org/abs/1806.02473. arXiv:1806.02473 [cs, stat].</p>
<p>Yang Yu, Tingyang Xu, Jiawen Li, Yaping Qiu, Yu Rong, Zhen Gong, Xuemin Cheng, Liming Dong, Wei Liu, Jin Li, et al. A novel scalarized scaffold hopping algorithm with graph-based variational autoencoder for discovery of jak1 inhibitors. ACS omega, 6(35):22945-22954, 2021.</p>
<p>Alex Zhavoronkov, Yan A. Ivanenkov, Alex Aliper, Mark S. Veselov, Vladimir A. Aladinskiy, Anastasiya V. Aladinskaya, Victor A. Terentiev, Daniil A. Polykovskiy, Maksim D. Kuznetsov, Arip Asadulaev, Yury Volkov, Artem Zholus, Rim R. Shayakhmetov, Alexander Zhebrak, Lidiya I. Minaeva, Bogdan A. Zagribelnyy, Lennart H. Lee, Richard Soll, David Madge, Li Xing, Tao Guo, and Alán Aspuru-Guzik. Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Nat Biotechnol, 37(9):1038-1040, September 2019. ISSN 15461696. doi: 10.1038/s41587-019-0224-x. URL https://www.nature.com/articles/ s41587-019-0224-x. Number: 9 Publisher: Nature Publishing Group.</p>
<h1>A APPENDIX</h1>
<p>The Appendix contains further experiments, ablation studies, experiment hyperparameters, and algorithmic details.</p>
<h2>A Beam Enumeration</h2>
<p>This section contains full details on Beam Enumeration including hyperparameters, design decisions, and pseudo-code.</p>
<h2>A. 1 Algorithm Overview</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure A4: Beam Enumeration overview. a. The proposed method proceeds via 4 steps: 1. generate batch of molecules. 2. filter molecules based on pool to enforce substructure presence, discarding the rest. 3. compute reward 4. update the model. After updating the model, if the reward has improved for consecutive epochs, execute Beam Enumeration. b. Beam Enumeration sequentially enumerates the top $k$ tokens by probability for $N$ beam steps, resulting in an exhaustive set of token sub-sequences. c. All valid substructures (either by the Structure or Scaffold criterion) are extracted from the sub-sequences. The most frequent substructures are used for self-conditioned generation. This overview figure is the same as in the main text.</p>
<p>Beam Enumeration (Fig. A4) is an algorithm that extracts molecular substructures from a generative model's weights for self-conditioned generation. The problem set-up is any molecular design task to optimize for a target property profile, e.g., high predicted solubility and binding affinity. When molecular generative models are coupled with an optimization algorithm, it should be increasingly likely to generate desirable molecules, i.e., molecules that possess the target property profile.</p>
<p>Beam Enumeration is proposed based on two facts:</p>
<ol>
<li>On a successful optimization trajectory, the model's weights must change such that desirable molecules are more likely to be generated, on average.</li>
<li>The act of generating molecules in an autoregressive manner involves sequentially sampling from conditional probability distributions.</li>
</ol>
<p>In this work, Beam Enumeration is applied to a language-based autoregressive generative model operating on the simplified molecular-input line-entry system (SMILES) (Weininger (1988) rep-</p>
<p>resentation. The optimization algorithm is Augmented Memory (Guo \&amp; Schwaller (2023) which builds on REINVENT (Olivecrona et al. (2017); Blaschke et al. (2020) and casts the optimization process as an on-policy reinforcement learning (RL) problem. Following RL terminology, sampling from the generative model involves sampling trajectories, which in this case, are SMILES, and the desirability of the corresponding molecule is given by the reward.</p>
<p>The underlying hypothesis of Beam Enumeration is that during the RL optimization process, partial trajectories provide a source of signal that can be exploited. Usually, full trajectories are sampled which map to a complete SMILES sequence that can be translated to a molecule. Our assumption is that partial trajectories (partial SMILES sequence) can be mapped to molecular substructures (a part of the full molecule). This statement is not guaranteed as SMILES and molecules are discrete and small perturbations often leads to invalid SMILES. We prove this assumption in Section C by showing that although the vast number of partial trajectories do not map to valid SMILES, the raw number is sufficient to extract a meaningful signal. Correspondingly, Beam Enumeration leverages partial trajectories on the assumption that molecular substructures are on track to becoming full molecules that would receive high reward.</p>
<h1>A. 2 Enumerating Partial Trajectories</h1>
<p>In order to extract molecular substructures, a set of partial trajectories must be sampled from the generative model. Recalling the fact that on a successful optimization trajectory, it must become increasingly likely to generate desirable molecules, partial trajectories are sampled by enumerating the top $k$ tokens, based on the conditional probability. Therefore, the process of enumerating partial trajectories involves sequentially extending each token sequence by their next top $k$ probable tokens, resulting in the total number of partial trajectories as $2^{N}$ where $N$ is the number of beam steps, i.e., how many tokens in the partial trajectory. We note that taking the top $k$ most probable tokens does not guarantee that the partial trajectories are indeed the most probable, as paying a probability penalty early can lead to higher probabilities later. However, our assumption is that on average, this leads to a set of partial trajectories that are at the very least, amongst the most probable. Moreover, there is a practical limit to how many partial trajectories are sampled due to exponential growth which makes scaling quickly computationally prohibitive. In the later section, we discuss this thoroughly. Finally, from here, partial trajectories will be referred to as token sub-sequences.</p>
<h2>A. 3 Extracting Molecular Substructures</h2>
<p>Given a set of token sub-sequences, the goal is to extract out the most frequent molecular substructures. This is done by taking each sequence, considering every (sub)-sub-sequence, and counting the number of valid substructures (Fig. A). For example, given the sub-sequence "ABCD", the set of (sub)-sub-sequences are: "A", "AB", "ABC", and "ABCD". In practice, we only consider (sub)-subsequences with at least three characters ("ABC" and "ABCD") since each character loosely maps to one atom and three is approximately the minimum for meaningful functional groups, e.g., " $\mathrm{C}=\mathrm{O}$ ", a carbonyl. The set of most frequent substructures is assumed to be on track to receive a high reward.</p>
<h2>A. 4 Defining Molecular Substructures: Scaffold vs. Structure</h2>
<p>As shown in Fig. A, molecular substructures can be defined on the Scaffold or Structure level. The former extracts the Bemis-Murcko (Bemis \&amp; Murcko (1996) scaffold while the latter extracts any valid structure. The any valid structure is an important distinction as our experiments find that extracting by Structure leads to the most frequent molecular substructures being small functional groups that do not have corresponding scaffolds. By contrast, extracting the scaffold always leads to ring structures. Moreover, extracting specifically the Bemis-Murcko scaffold is important as heavy atoms, e.g., nitrogen, are important for biological activity. Consequently, extracted substructures are also enforced to contain at least one heavy atom as we find that benzene, perhaps unsurprisingly, is commonly the most frequent substructure. See Section B for more details on the differing behavior of 'Scaffold' vs. 'Structure'.</p>
<h1>A. 5 Self-Conditioned Generation</h1>
<p>Self-conditioned generation is achieved by filtering sampled batches of molecules from the generative model to only keep the ones that possess at least one of the most frequent substructures. The effect is that the generative process is self-biased to focus on a narrower chemical space which we show can drastically improve sample efficiency at the expense of some diversity, which is acceptable when expensive high-fidelity oracles are used: we want to identify a small set of excellent candidate molecules under minimal oracle calls.</p>
<h2>A. 6 Probabilistic Explainability</h2>
<p>The set of most frequent molecular substructures should be meaningful as otherwise, the model's weights would not have been updated such that these substructures have become increasingly likely to be generated. We verify this statement in the illustrative experiment in the main text and in Section C. In the drug discovery case studies (Appendix D), the extracted substructures are more subtle in why they satisfy the target objective but certainly must possess meaning, however subtle, as otherwise, they would not receive a high reward. In the main text, we show that extracted substructures form core scaffolds and structural motifs in the generated molecules that complement the protein binding cavity. Finally, we emphasize that the correctness and usefulness of this explainability deeply depends on the oracle(s) being optimized for. The extracted substructures do not explain why the generated molecules satisfy the target objective. Rather, they explain why the generated molecules satisfy the oracle. The assumption in a generative design task is that optimizing the oracle is a good proxy for the target objective, e.g., generating molecules that dock well increases the likelihood of the molecules being true binders. This observation directly provides additional commentary on why sample efficiency is so important: the ability to directly optimize expensive high-fidelity oracles would inherently enhance the correctness of the extracted substructures.</p>
<h2>B Beam Enumeration: Findings from Hyperparameter Screening</h2>
<p>In this section, we introduce all seven hyperparameters of Beam Enumeration and then present results on an exhaustive hyperparameter search which elucidates the behavior and interactions of all the hyperparameters. In the end, we present our analyses and provide hyperparameter recommendations for Beam Enumeration which can serve as default values to promote out-of-the-box application.</p>
<h2>B. 1 Beam Enumeration Hyperparameters</h2>
<p>Beam k. This hyperparameter denotes how many tokens to enumerate at each step. Given that our hypothesis is that the most probable sub-sequences yield meaningful substructures, we fix Beam $k$ to 2. A larger value would also decrease the number of Beam Steps possible as the total number of sub-sequences is $k^{N}$ and the exponential growth quickly leads to computational infeasibility.</p>
<p>Beam Steps N. This hyperparameter denotes how many token enumeration steps to execute and is the final token length of the enumerate sub-sequences. This parameter leads to exponential growth in the number of sequences which can quickly become computationally prohibitive. An important implication of this hyperparameter is that larger Beam Steps means that larger substructures can be extracted. In our experiments, we find that enforcing size in the extracted substructures can drastically improve sample efficiency with decreased diversity as the trade-off. We thoroughly discuss this in a later sub-section. Finally, in our experiments, the upper-limit investigated is 18 Beam Steps.</p>
<p>Substructure Type. This hyperparameter has two possible values: Scaffold or Structure. Scaffold extracts Bemis-Murcko (Bemis \&amp; Murcko (1996) scaffolds while Structure extracts any valid substructure.</p>
<p>Structure Structure Minimum Size. This hyperparameter enforces the partial SMILES to contain at least a certain number of characters. In effect, this enforces extracted molecular substructures to be larger than a Structure Minimum Size. From the illustrative experiment in the main text and Section C, Structure extraction often leads to small functional groups being the most frequent in the sub-sequences. By enforcing a minimum structure size, Structure extraction leads to partial</p>
<p>structures which may carry more meaning. We find that this hyperparameter greatly impacts sample efficiency and we present all our findings in a later sub-section.</p>
<p>Pool Size. This hyperparameter controls how many molecular substructures to keep track of. These pooled substructures are what is used to perform self-conditioning. The hypothesis is that the most frequent ones carry the most meaning and thus, a very large pool size may not be desired.</p>
<p>Patience. This hyperparameter controls how many successive reward improvements are required before Beam Enumeration executes and molecular substructures are extracted. Recalling the first fact in which Beam Enumeration was proposed on: On a successful optimization trajectory, the model's weights must change such that desirable molecules are more likely to be generated, on average. Patience is effectively an answer to "when would extracted substructures be meaningful?" Too low a patience and stochasticity can lead to negative effects while too high a patience diminishes the benefits of Beam Enumeration on sample efficiency.</p>
<p>Token Sampling Method. This hyperparameter has two possible values: "topk" or "sample" and denotes how tokens sub-sequences are enumerated. "topk" takes the top k most probable tokens at each Beam Step while "sample" samples from the distribution just like during batch generation. Our results show interesting observations surrounding this hyperparameter as "sample" can work just as well and sometimes even better than taking the "topk". These results were unexpected as the underlying hypothesis is that the most probable sub-sequences lead to the most useful substructures being extracted. However, our findings are not in contradiction as sampling the conditional probability distributions would still lead to sampling the top k tokens, on average. Moreover, after extensive experiments, we find that "sample" leads to more variance in performance across replicates which is in agreement with the assumption that sampling the distributions can lead to more improbable structures. We thoroughly discuss our findings in a later sub-section where we provide hyperparameter recommendations and analyses to the effects of tuning each hyperparameter.</p>
<h1>B. 2 HYPERPARAMETERS: GRID SEARCH</h1>
<p>We performed two exhaustive hyperparameter grid searches on the illustrative experiment which has the following multi-parameter optimization (MPO) objective: maximize topological polar surface area (tPSA), molecular weight $&lt;350$ Da, number of rings $\geq 2$ with an oracle budget of 5,000 . The first grid search investigated the following hyperparameter combinations:</p>
<ul>
<li>Beam $\mathrm{K}=2$</li>
<li>Beam Steps $=[15,16,17,18]$</li>
<li>Substructure Type $=[$ Scaffold, Structure]</li>
<li>Pool Size $=[3,4,5]$</li>
<li>Patience $=[3,4,5]$</li>
<li>Token Sampling Method $=$ ['topk', 'sample']</li>
</ul>
<p>All hyperparameter combinations (144) were tried and run for 10 replicates each for statistical reproducibility, total of $\mathbf{1 , 4 4 0}$ experiments. Next, an additional grid search was performed with the following hyperparameter combinations:</p>
<ul>
<li>Beam $\mathrm{K}=2$</li>
<li>Beam Steps $=[17,18]$</li>
<li>Substructure Type $=[$ Scaffold, Structure]</li>
<li>Structure Structure Minimum Size $=[10,15]$</li>
<li>Pool Size $=[4,5]$</li>
<li>Patience $=[4,5]$</li>
<li>Token Sampling Method $=$ ['topk', 'sample']</li>
</ul>
<p>We take the general trends from the first grid search and narrow down the most optimal hyperparameters to further investigate Substructure Type and structure Structure Minimum Size. As from</p>
<p>before, all hyperparameter combinations (64) were tried and run for 10 replicates each for statistical reproducibility, total of 640 experiments.</p>
<p>The following heatmaps performance by the Generative Yield and Oracle Burden (10) metrics at the $&gt;0.8$ reward threshold and under a 5,000 oracle budget. The Generative Yield measures how many unique molecules above 0.8 reward were generated. The Oracle Burden (10) measures how few oracle calls were required to generate 10 molecules above 0.8 reward.</p>
<h1>B. 3 ANALYSIS OF GRID SEARCH ReSULTS</h1>
<p>In this section, we summarize our analysis on the grid search experiments. Unless stated, each bullet point means the observation was observed for both Generative Yield and Oracle Burden (10). For example the point: Scaffold $&gt;$ Structure means Scaffold is generally more performant than Structure across all hyperparameters on both the Generative Yield and Oracle Burden (10).</p>
<ul>
<li>For Scaffold, higher Pool, higher Patience, and higher Beam Steps improves performance</li>
<li>For Structure, lower pool and lower patience improves performance</li>
<li>Scaffold $&gt;$ Structure</li>
<li>Scaffold and Structure become more performant with increasing Structure Minimum Size</li>
<li>Scaffold and Structure with Structure Minimum Size: "sample" sampling can be better than "topk" sampling but with more variance</li>
</ul>
<p>Based on the above analysis, we propose the optimal hyperparameters for the illustrative experiment as:</p>
<ul>
<li>Scaffold</li>
<li>"topk" sampling ("sample" sampling can be more performant but exhibits higher variance)</li>
<li>Patience $=5$</li>
<li>Pool Size $=4$</li>
<li>Beam Steps $=18$</li>
</ul>
<p>Finally, we provide more commentary on interesting observations from the grid search results. Structure without Structure Minimum Size enforcing often leads to small functional groups being the most frequent molecular substructures extracted with Beam Enumeration. Enforcing Structure Minimum Size puts it almost on par with Scaffold, suggesting (perhaps not surprisingly) that larger substructures can carry more meaningful information. Moreover, when using "sample" sampling, the generative model undergoes more "filter rounds". Specifically, at each epoch, the sampled batch is filtered to contain the extracted substructures. When using "sample" sampling, the model is more prone to some epochs containing no molecules with the substructures. In practice, this is inconsequential as sampling is computationally inexpensive and a next batch of molecules can easily be sampled. However, specifically in the Structure with "sample" sampling and Structure Minimum Size $=15$ experiment, "filter round" can be quite extensive, taking up to 100,000 epochs (maximum observed) for an oracle budget of 5,000 (adding about an hour to the wall time which is minor when the oracle is expensive). This means that many epochs contained molecules without the extracted substructures. There are two observations here: firstly, "sample" sampling can lead to more improbable substructures which are hence less likely to be sampled and secondly, Structure with Structure Minimum Size enforcement leads to extreme biasing (which improves sample efficiency). We believe the remarkable tolerability of the generative model sampling to such bias is an interesting observation. By contrast, Scaffold with Structure Minimum Size enforcement is not as prone to "filter rounds" because Scaffold "truncates" the substructure to its central shape (scaffold). For example, toluene (benzene with a methyl group) has a Bemis-Murcko (Bemis \&amp; Murcko (1996) scaffold of just benzene. The consequence is that Structure leads to more extreme biasing (it is more likely for a molecule to contain benzene than specifically toluene) which is in agreement with the general observation that the diversity of the generated set decreases when using Structure. Overall, both Scaffold and Structure with Structure Minimum Size enforcing exhibits the best performance and "sample" sampling can be more performant than "topk" sampling but exhibits notably higher variance.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure B5: illustrative experiment Generative Yield $&gt;0.8$. The IntDiv1 (Polykovskiy et al. (2020b) is annotated.</p>            </div>
        </div>

    </div>
</body>
</html>
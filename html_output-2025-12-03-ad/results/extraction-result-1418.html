<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1418 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1418</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1418</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-c433c7f0a1c5fe3449274c7234ff5a8fc4f3c3ff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c433c7f0a1c5fe3449274c7234ff5a8fc4f3c3ff" target="_blank">STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> STORM is introduced, an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders and achieves a new record among state-of-the-art methods that do not employ lookahead search techniques.</p>
                <p><strong>Paper Abstract:</strong> Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial. In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of $126.7\%$ on the Atari $100$k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with $1.85$ hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only $4.3$ hours, showcasing improved efficiency compared to previous methodologies.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1418.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1418.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Transformer-based wORld Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based latent world model that uses a categorical VAE encoder to produce discrete stochastic latents and a GPT-like masked Transformer sequence model for dynamics prediction and imagined rollouts; optimized for sample-efficient RL on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model: images encoded by a categorical VAE (stochastic distribution of 32 categories × 32 classes), sample z_t via straight-through gradient trick; z_t is concatenated with action a_t into a single token e_t; sequence model f_phi is a GPT-like masked Transformer (default K=2 layers, D=512, 8 heads) producing hidden states h_t that predict next latent distribution, reward, and continuation flag; agent state is [z_t, h_t]; imagination samples from predicted prior distribution and uses KV-cache to accelerate inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic discrete latent + transformer sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k (Atari games, pixel-input RL)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction L2 loss for images (||o_hat - o||_2), symlog two-hot loss for reward prediction, binary cross-entropy for continuation flag, and KL divergences for dynamics and representation terms (KL between encoder posterior and sequence model predicted distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numerical reconstruction / KL values reported; fidelity is evaluated indirectly via downstream task scores (mean human-normalized score 126.7% on Atari 100k). Loss terms include reconstruction L2, symlog reward loss, continuation CE, and two KL terms weighted by beta1=0.5 and beta2=0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural model; attention mechanism helps preserve history of moving objects which is argued to make velocity/direction information more accessible; discrete categorical latents and attention offer some avenues for visualization but no dedicated interpretability attribution was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Implicit attention behaviour (attention mechanism can be inspected); discrete latent representation (categorical VAE) could be visualized, but no explicit latent visualization or attribution studies were reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training reported as 4.3 hours on a single NVIDIA GeForce RTX 3090 for the 100k-sample Atari runs (equivalent V100 hours reported as 9.3 in the paper's comparison table); world-model uses small Transformer (2 layers, D=512) and image encoder/decoder CNNs (encoder outputs 32x32 categorical grid). Imagination batch size B2=1024, imagination horizon L=16. No exact parameter count provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reported faster than prior RNN-based world models due to parallelizable Transformer blocks; outperforms DreamerV3 in wall-clock and sample efficiency (DreamerV3 reported 12 V100 hours vs STORM 4.3 hours on RTX3090 in original setup) and achieves higher mean human-normalized score (126.7% vs DreamerV3 112%).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Achieves mean human-normalized score of 126.7% and median 58.4% on Atari 100k (average over 26 games, evaluated over 5 seeds). Shows particular strength on games with large or multiple reward-related objects (e.g., Amidar, Ms. Pacman, Chopper Command, Gopher) and weaknesses on games with a single small moving object (Pong, Breakout).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Design choices (stochastic categorical latent + Transformer) reduce accumulated autoregressive prediction errors and improve robustness for many Atari games; higher-fidelity reconstruction is not guaranteed to map directly to best policy — STORM prioritizes stochastic latent modeling to prevent policy exploiting world-model errors; performs well on tasks requiring long-range context due to attention, but struggles on tasks where autoencoder sampling noise dominates small important features.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Stochastic latents reduce overconfident deterministic rollouts (improves robustness) but sampled randomness can corrupt attention when small objects are critical; increasing Transformer depth/larger sequence model did not improve performance under 100k-sample regime and can cause the encoder to be overly influenced by sequence model (non-stationarity); joint end-to-end training introduces non-stationarity because the world model must predict its own internal outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Categorical VAE encoder (32 categories × 32 classes) with straight-through gradient; single-token per timestep combining z_t and a_t; GPT-like masked Transformer with 2 layers, D=512, 8 heads; dynamics and representation KL losses with stop-gradient on opposite terms; agent state concatenates z_t and h_t; imagination samples from predicted prior; KV-cache used for inference acceleration; reconstruction loss applied directly to encoder output (not to sequence model).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against SimPLe (LSTM + binary VAE), TWM (Transformer-XL with multiple tokens), IRIS (VQ-VAE + spatial-temporal Transformer), and DreamerV3 (GRU + categorical VAE); STORM reports higher mean human-normalized score than all listed baselines (126.7% vs DreamerV3 112%, IRIS 105%, TWM 96%, SimPLe 33%) and lower wall-clock training time in the authors' reported setup; claims Transformers give faster parallel training than RNNs and better capture long-range dependencies, while single stochastic latent token is more efficient than multi-token spatial latents for their setting.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends a relatively small Transformer (2 layers) for the low-data Atari 100k regime, applying reconstruction loss on encoder outputs (not sequence outputs), using stochastic discrete latent representation to mitigate model bias, including h_t in agent state when contextual frames are necessary, and sampling from prior during imagination; warns that larger Transformers did not help under 100k samples and that joint end-to-end training can introduce non-stationarity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1418.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1418.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3 (Mastering diverse domains through world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL world model that uses a categorical VAE for image representation and a GRU-based recurrent sequence model to imagine trajectories for policy learning; presented as a strong baseline for diverse domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering diverse domains through world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model: images encoded by a categorical VAE into discrete latents; sequence modeling is performed by a GRU recurrent neural network which produces hidden states used for next-latent, reward and continuation prediction; actor-critic policy trained on imagined trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic discrete latent + RNN sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and diverse control domains (benchmarking across domains as reported in reference)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (L2), symlog two-hot reward loss, continuation prediction cross-entropy, and KL-based representation/dynamics terms (as described in related literature and referenced by the STORM paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in STORM as a baseline human-mean normalized score of 112% on Atari 100k (from Table 2); the STORM paper does not report DreamerV3 internal reconstruction/KL numeric values.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural architecture (VAE + GRU); no interpretability analyses reported in STORM's discussion of DreamerV3. Recurrent hidden states are used, which can be opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in the STORM paper for DreamerV3 beyond standard diagnostic evaluation via rollout quality and downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>DreamerV3 reported as 12 V100 hours in the comparison table quoted by STORM (exact device setups differ across papers).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>STORM claims improved training efficiency relative to DreamerV3 due to Transformer parallelism and KV-cache inference; DreamerV3 uses GRU which is less parallelizable and thus slower in wall-clock time in the authors' comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Human-mean normalized score reported in STORM's comparison table: 112% on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Effective across diverse domains per its original work; in STORM's comparison DreamerV3 performs well but is outperformed by STORM in aggregate Atari 100k performance; GRU hidden states can help when historical information influences reconstruction/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RNN-based sequence models (GRU) can model temporal dynamics effectively but limit parallel training speed; DreamerV3 uses recurrent hidden states which provide historical conditioning but may slow training compared to STORM's Transformer approach.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Categorical VAE encoder, GRU sequence model, hidden states included in agent state (latent + hidden), symlog two-hot loss for reward regression classification, actor-critic on imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared by STORM as a strong RNN-based baseline; STORM claims better wall-clock efficiency and higher mean performance for Atari 100k by replacing GRU with a Transformer and using a stochastic categorical latent with single-token per timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>STORM suggests that for 100k-sample Atari tasks, smaller models (e.g., small Transformer) suffice and that design choices like stochastic latents reduce model exploitation; by implication, DreamerV3's GRU + categorical VAE design is strong but may be outperformed by parallel Transformer-based designs under the authors' settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1418.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1418.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated Policy Learning (SimPLe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early image-based model-based RL approach using a VAE-LSTM world model trained to reconstruct image observations and predict dynamics for policy learning in Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model based reinforcement learning for atari</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>World model built with a binary-VAE image encoder and an LSTM recurrent sequence model to predict future latent states and reconstruct images for imagined rollouts and policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic binary latent + RNN sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (original SimPLe experiments focused on Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Image reconstruction (decoder), next-state prediction error on latent/image space, and downstream policy performance (sample-efficiency metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in STORM's comparison table as human-mean normalized score 33% on Atari 100k; STORM also reports SimPLe had lengthy training (originally 20 days on P100, extrapolated to V100 hours in their table).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box VAE+LSTM; no interpretability methods discussed in STORM's mention beyond the general VAE latent structure.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specified in STORM for SimPLe beyond use of VAE latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Original SimPLe reported as 20 days on NVIDIA P100 (extrapolated in STORM's comparison table to 240 V100 hours).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>SimPLe is slower and less sample-efficient than later methods (Dreamer variants, STORM) per STORM's comparison and requires much larger wall-clock training time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Human-mean normalized score 33% on Atari 100k (as quoted in STORM's comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrated feasibility of learning policies from simulated data in image-based environments but gave lower final performance under limited-sample benchmarks relative to later world models.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>LSTM-based recurrence models capture temporal structure but are less parallelizable (slower training) and earlier VAE choices (binary-VAE) offered different tradeoffs in latent expressivity versus stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Binary-VAE latent, LSTM recurrent dynamics model, end-to-end training for image reconstruction and dynamics prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Considered an earlier baseline; STORM claims substantial improvements in both performance and training speed over SimPLe by using categorical VAE + Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>STORM suggests that replacing RNN recurrence with Transformer parallelism and using categorical stochastic latents improves efficiency and robustness under low-sample regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1418.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1418.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based World Model (TWM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-XL based world model that treats observation, action, and reward as equal input tokens for sequence modeling, evaluated under the Atari 100k regime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer-based world models are happy with 100k interactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TWM (Transformer-XL based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequence model uses Transformer-XL; input tokenization treats observation latents, actions, and rewards as separate tokens per timestep; dynamics are modeled autoregressively over these tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete/continuous latent + Transformer-XL sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k (Atari games under low-sample regime)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream policy performance (human-normalized score) and sequence modeling/prediction accuracy as implied by reported results; STORM does not report internal numeric fidelity metrics for TWM.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in STORM's comparison table as human-mean normalized score 96% on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box Transformer-XL; using multiple token types (obs/action/reward) increases token count which may complicate attention attribution; no specific interpretability analyses reported in STORM.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specified in STORM's discussion beyond noting the different tokenization that may affect attention.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported by TWM authors as A100 10 hours; STORM quotes original TWM runs giving ~12.5 hours on RTX 3090 in their comparison table (extrapolated values in STORM).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TWM's multi-token design increases token count and slows training relative to STORM's single-token-per-timestep approach; STORM claims a simpler tokenization (combine z_t and a_t) is more efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Human-mean normalized score 96% on Atari 100k (as quoted in STORM's comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Transformer-XL can model long contexts, but treating reward and action as equal tokens may dilute attention and hurt performance per STORM's analysis; higher token counts also raise compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Tokenization scheme tradeoff: more tokens (obs/action/reward) can encode more detail but increase attention cost and may degrade performance if attention mixes heterogeneous modality signals.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Transformer-XL backbone, multi-token per timestep (latent, action, reward) input representation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to STORM which uses single-token per timestep and a vanilla GPT-like Transformer, TWM performed worse in STORM's Atari 100k comparisons and was noted to be slower due to higher token counts.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>STORM suggests simpler single-token-per-timestep and smaller transformer layers are preferable under the 100k-sample Atari setting for efficiency and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1418.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1418.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IRIS (Transformers are sample-efficient world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A world model that uses VQ-VAE to map images into spatial 4×4 latent tokens and a spatial-temporal Transformer to capture within- and across-image information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers are sample-efficient world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image encoder: VQ-VAE converting images into a 4×4 grid of discrete tokens; sequence model: spatial-temporal Transformer attends over many tokens within and across frames to predict dynamics and reconstruct images.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (vector-quantized spatial token latents + spatial-temporal transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games / visual RL (sample-efficient settings)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream task performance (human-normalized score) and quality of multi-token reconstructions; STORM does not provide IRIS internal numeric fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in STORM's comparison table as human-mean normalized score 105% on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Spatial-token representation yields a more explicit spatial decomposition (4×4 tokens) that is more amenable to inspection than monolithic latent vectors; however, attention over many tokens increases complexity and visualization overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Spatial tokenization via VQ-VAE enables inspecting discrete spatial latents; attention maps over tokens could be examined but no specific method reported in STORM.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Attention on large number of tokens in the spatial-temporal Transformer can significantly slow down training (noted by STORM); IRIS original runs were computationally heavy (A100 7 days for two runs quoted in STORM's table, extrapolated).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient in wall-clock compared to STORM due to heavy attention over many tokens; may have richer spatial fidelity at higher compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Human-mean normalized score 105% on Atari 100k (as quoted in STORM's comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Explicit spatial tokens can better capture object positions and local dynamics, potentially improving fidelity for visually complex tasks, but increased token attention cost slows training and may not provide net gains under strict low-sample budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher spatial fidelity via many tokens vs increased computational cost and slower training; STORM opts for single discrete latent token to trade some spatial granularity for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VQ-VAE encoder producing 4×4 token grid, spatial-temporal Transformer for attention across tokens and time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>IRIS attains strong sample-efficiency but STORM argues its single-token categorical VAE + vanilla Transformer is computationally cheaper and competitive or superior under 100k-sample constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1418.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1418.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TransDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TransDreamer (Transformer-based replacement of Dreamer GRU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that replaces the GRU sequence model in Dreamer with a Transformer architecture to build a Transformer-based world model for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transdreamer: Reinforcement learning with transformer world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TransDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variant of Dreamer where the GRU recurrent module is replaced by a Transformer for sequence modeling; aims to leverage attention for better long-range temporal modeling in imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (categorical/discrete latent + transformer sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>RL domains similar to Dreamer (Atari, control tasks) as per original TransDreamer work</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream task performance and sequence modeling quality; STORM notes lack of published evidence of TransDreamer performance under low-sample widely-accepted benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>STORM states there is a lack of evidence demonstrating TransDreamer's performance in widely accepted environments or under limited sample conditions; no numeric fidelity provided in the STORM paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box Transformer-based model; no interpretability analyses discussed in STORM's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in STORM for TransDreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified in STORM; replacing GRU with Transformer could increase parallelization but may increase per-step attention cost depending on configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>STORM positions its own Transformer design as an efficient Transformer-based world model and notes that prior Transformer attempts including TransDreamer did not convincingly surpass GRU baselines under low-sample benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>No reliable task performance numbers provided by STORM for TransDreamer in the Atari 100k benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Potential benefits of Transformer's long-range modeling are noted, but STORM highlights that evidence of practical gains in low-sample RL is limited for TransDreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Replacing recurrence with attention may improve context modeling but can increase token processing cost and requires careful architecture choices to avoid slowing training or overfitting in low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Transformer sequence model substituted for GRU within Dreamer-like architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>STORM claims more careful use of Transformers (single-token, small depth, KV-cache) yields better efficiency and performance than some prior direct substitutions like TransDreamer under the 100k-sample setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1418.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1418.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero/EfficientZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero / EfficientZero (model + MCTS planners)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model-based RL approaches that combine learned models with Monte Carlo Tree Search (MCTS) lookahead planning to improve policies at test time, often achieving strong scores on Atari benchmarks at the cost of high computational demand.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero / EfficientZero (learned model + MCTS planning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learned dynamics and value/policy prediction networks are used as a forward model inside Monte Carlo Tree Search to perform lookahead planning and improve action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid: learned model with planning (neural simulator + tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and board games (Chess, Go) — high-performance benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Policy performance (game scores) via planning; not described in detail in STORM beyond their use as lookahead baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not numerically compared in STORM's main results (authors exclude lookahead search methods from direct comparison but acknowledge they achieve promising performance on Atari 100k).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Planning-based methods are partially interpretable in terms of search trajectories but learned neural dynamics remain black-box; STORM notes high computational demand of MCTS lookahead.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Tree search trajectories can be inspected; no specific interpretability methods discussed in STORM.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Characterized as high computational demand by STORM; not included in direct wall-clock comparison since authors focus on world-model-only pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>STORM does not directly compare to these lookahead methods due to their high compute; notes that lookahead can be combined with STORM but would increase computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Known from referenced work to achieve strong Atari performance (MuZero is a high-performing baseline) but STORM does not provide direct numeric comparisons on 100k run.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Lookahead planning improves action selection by leveraging model rollouts but at high runtime cost; STORM opts to focus on improving the world model itself without planning in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Planning with MCTS often improves policy but requires substantially more computation (wall-clock and inference-time) compared to pure imagined-rollout policy training used by STORM and other latent world-model approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Combine learned model of environment with search/planning (MCTS) to evaluate action outcomes more precisely at decision time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>STORM explicitly excludes direct comparison to MuZero/EfficientZero in tables because those methods use lookahead search; authors state that such techniques can be combined with STORM in future work but carry high computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1418.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1418.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Categorical-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Categorical Variational Autoencoder (Categorical-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrete stochastic latent image encoder used by STORM and DreamerV3 to produce categorical latent distributions (here 32×32) that inject controlled stochasticity into the world model and reduce overconfident deterministic rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Categorical-VAE (discrete latent VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image encoder producing a stochastic categorical distribution over discrete latent codes (in STORM: 32 categories each with 32 classes forming a 32×32 output grid); decoder reconstructs images from sampled discrete latents using straight-through gradient estimator for backprop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent representation model (discrete stochastic encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Image-based RL (Atari 100k), used as the observation encoder/decoder in latent world models</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction L2 loss on decoded images; downstream policy performance (task score) used as proxy for useful fidelity; KL-based losses to align sequence predictions with encoder posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No standalone numeric reconstruction metrics reported for categorical-VAE in STORM; contributes to overall STORM task performance (mean human-normalized 126.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete latent codes provide some interpretability potential (discrete clusters) compared to continuous embeddings; STORM notes categorical latents inject stochasticity that can improve robustness but does not provide explicit interpretability analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not reported in STORM beyond noting discrete token structure; straight-through estimator used to propagate gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Encoder/decoder are CNNs with shapes described in Appendix B; discrete latents can reduce dimensionality and speed subsequent sequence modeling relative to pixel-based dynamics, but exact parameter counts not given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Categorical discrete latents reduce sequence-model token size compared to spatial multi-token schemes (e.g., IRIS VQ-VAE 4×4 tokens) and are positioned as an efficient compromise in STORM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used as part of STORM which achieves 126.7% mean human-normalized score; categorical-VAE is credited with improving robustness and reducing accumulated autoregressive errors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Stochastic discrete latents supply controlled noise that prevents the agent from exploiting deterministic model errors and improves generalization of imagined rollouts; however, sampling noise can harm attention when small objects are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Discrete stochastic latents trade spatial granularity for robustness/efficiency: they reduce overconfident rollouts but may lose fine-grained small-object detail compared with richer spatial tokenizations.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>32×32 categorical grid latent; straight-through gradient trick for sampling; paired with Transformer sequence model; representation and dynamics KL losses to stabilize learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against VQ-VAE (IRIS) and binary-VAE (SimPLe): categorical-VAE provides stochasticity and compact representation, claimed by STORM to be an efficient choice for Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Model based reinforcement learning for atari <em>(Rating: 2)</em></li>
                <li>Transformer-based world models are happy with 100k interactions <em>(Rating: 2)</em></li>
                <li>Transformers are sample-efficient world models <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Transdreamer: Reinforcement learning with transformer world models <em>(Rating: 2)</em></li>
                <li>Mastering Atari, Go, chess and shogi by planning with a learned model <em>(Rating: 1)</em></li>
                <li>Mastering Atari with discrete world models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1418",
    "paper_id": "paper-c433c7f0a1c5fe3449274c7234ff5a8fc4f3c3ff",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "STORM",
            "name_full": "Stochastic Transformer-based wORld Model",
            "brief_description": "A Transformer-based latent world model that uses a categorical VAE encoder to produce discrete stochastic latents and a GPT-like masked Transformer sequence model for dynamics prediction and imagined rollouts; optimized for sample-efficient RL on Atari 100k.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "STORM",
            "model_description": "Latent world model: images encoded by a categorical VAE (stochastic distribution of 32 categories × 32 classes), sample z_t via straight-through gradient trick; z_t is concatenated with action a_t into a single token e_t; sequence model f_phi is a GPT-like masked Transformer (default K=2 layers, D=512, 8 heads) producing hidden states h_t that predict next latent distribution, reward, and continuation flag; agent state is [z_t, h_t]; imagination samples from predicted prior distribution and uses KV-cache to accelerate inference.",
            "model_type": "latent world model (stochastic discrete latent + transformer sequence model)",
            "task_domain": "Atari 100k (Atari games, pixel-input RL)",
            "fidelity_metric": "Reconstruction L2 loss for images (||o_hat - o||_2), symlog two-hot loss for reward prediction, binary cross-entropy for continuation flag, and KL divergences for dynamics and representation terms (KL between encoder posterior and sequence model predicted distribution).",
            "fidelity_performance": "No explicit numerical reconstruction / KL values reported; fidelity is evaluated indirectly via downstream task scores (mean human-normalized score 126.7% on Atari 100k). Loss terms include reconstruction L2, symlog reward loss, continuation CE, and two KL terms weighted by beta1=0.5 and beta2=0.1.",
            "interpretability_assessment": "Primarily a black-box neural model; attention mechanism helps preserve history of moving objects which is argued to make velocity/direction information more accessible; discrete categorical latents and attention offer some avenues for visualization but no dedicated interpretability attribution was reported.",
            "interpretability_method": "Implicit attention behaviour (attention mechanism can be inspected); discrete latent representation (categorical VAE) could be visualized, but no explicit latent visualization or attribution studies were reported in the paper.",
            "computational_cost": "Training reported as 4.3 hours on a single NVIDIA GeForce RTX 3090 for the 100k-sample Atari runs (equivalent V100 hours reported as 9.3 in the paper's comparison table); world-model uses small Transformer (2 layers, D=512) and image encoder/decoder CNNs (encoder outputs 32x32 categorical grid). Imagination batch size B2=1024, imagination horizon L=16. No exact parameter count provided.",
            "efficiency_comparison": "Reported faster than prior RNN-based world models due to parallelizable Transformer blocks; outperforms DreamerV3 in wall-clock and sample efficiency (DreamerV3 reported 12 V100 hours vs STORM 4.3 hours on RTX3090 in original setup) and achieves higher mean human-normalized score (126.7% vs DreamerV3 112%).",
            "task_performance": "Achieves mean human-normalized score of 126.7% and median 58.4% on Atari 100k (average over 26 games, evaluated over 5 seeds). Shows particular strength on games with large or multiple reward-related objects (e.g., Amidar, Ms. Pacman, Chopper Command, Gopher) and weaknesses on games with a single small moving object (Pong, Breakout).",
            "task_utility_analysis": "Design choices (stochastic categorical latent + Transformer) reduce accumulated autoregressive prediction errors and improve robustness for many Atari games; higher-fidelity reconstruction is not guaranteed to map directly to best policy — STORM prioritizes stochastic latent modeling to prevent policy exploiting world-model errors; performs well on tasks requiring long-range context due to attention, but struggles on tasks where autoencoder sampling noise dominates small important features.",
            "tradeoffs_observed": "Stochastic latents reduce overconfident deterministic rollouts (improves robustness) but sampled randomness can corrupt attention when small objects are critical; increasing Transformer depth/larger sequence model did not improve performance under 100k-sample regime and can cause the encoder to be overly influenced by sequence model (non-stationarity); joint end-to-end training introduces non-stationarity because the world model must predict its own internal outputs.",
            "design_choices": "Categorical VAE encoder (32 categories × 32 classes) with straight-through gradient; single-token per timestep combining z_t and a_t; GPT-like masked Transformer with 2 layers, D=512, 8 heads; dynamics and representation KL losses with stop-gradient on opposite terms; agent state concatenates z_t and h_t; imagination samples from predicted prior; KV-cache used for inference acceleration; reconstruction loss applied directly to encoder output (not to sequence model).",
            "comparison_to_alternatives": "Compared against SimPLe (LSTM + binary VAE), TWM (Transformer-XL with multiple tokens), IRIS (VQ-VAE + spatial-temporal Transformer), and DreamerV3 (GRU + categorical VAE); STORM reports higher mean human-normalized score than all listed baselines (126.7% vs DreamerV3 112%, IRIS 105%, TWM 96%, SimPLe 33%) and lower wall-clock training time in the authors' reported setup; claims Transformers give faster parallel training than RNNs and better capture long-range dependencies, while single stochastic latent token is more efficient than multi-token spatial latents for their setting.",
            "optimal_configuration": "Paper recommends a relatively small Transformer (2 layers) for the low-data Atari 100k regime, applying reconstruction loss on encoder outputs (not sequence outputs), using stochastic discrete latent representation to mitigate model bias, including h_t in agent state when contextual frames are necessary, and sampling from prior during imagination; warns that larger Transformers did not help under 100k samples and that joint end-to-end training can introduce non-stationarity.",
            "uuid": "e1418.0",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DreamerV3",
            "name_full": "DreamerV3 (Mastering diverse domains through world models)",
            "brief_description": "A model-based RL world model that uses a categorical VAE for image representation and a GRU-based recurrent sequence model to imagine trajectories for policy learning; presented as a strong baseline for diverse domains.",
            "citation_title": "Mastering diverse domains through world models",
            "mention_or_use": "mention",
            "model_name": "DreamerV3",
            "model_description": "Latent world model: images encoded by a categorical VAE into discrete latents; sequence modeling is performed by a GRU recurrent neural network which produces hidden states used for next-latent, reward and continuation prediction; actor-critic policy trained on imagined trajectories.",
            "model_type": "latent world model (stochastic discrete latent + RNN sequence model)",
            "task_domain": "Atari games and diverse control domains (benchmarking across domains as reported in reference)",
            "fidelity_metric": "Reconstruction loss (L2), symlog two-hot reward loss, continuation prediction cross-entropy, and KL-based representation/dynamics terms (as described in related literature and referenced by the STORM paper).",
            "fidelity_performance": "Reported in STORM as a baseline human-mean normalized score of 112% on Atari 100k (from Table 2); the STORM paper does not report DreamerV3 internal reconstruction/KL numeric values.",
            "interpretability_assessment": "Black-box neural architecture (VAE + GRU); no interpretability analyses reported in STORM's discussion of DreamerV3. Recurrent hidden states are used, which can be opaque.",
            "interpretability_method": "None reported in the STORM paper for DreamerV3 beyond standard diagnostic evaluation via rollout quality and downstream task performance.",
            "computational_cost": "DreamerV3 reported as 12 V100 hours in the comparison table quoted by STORM (exact device setups differ across papers).",
            "efficiency_comparison": "STORM claims improved training efficiency relative to DreamerV3 due to Transformer parallelism and KV-cache inference; DreamerV3 uses GRU which is less parallelizable and thus slower in wall-clock time in the authors' comparisons.",
            "task_performance": "Human-mean normalized score reported in STORM's comparison table: 112% on Atari 100k.",
            "task_utility_analysis": "Effective across diverse domains per its original work; in STORM's comparison DreamerV3 performs well but is outperformed by STORM in aggregate Atari 100k performance; GRU hidden states can help when historical information influences reconstruction/prediction.",
            "tradeoffs_observed": "RNN-based sequence models (GRU) can model temporal dynamics effectively but limit parallel training speed; DreamerV3 uses recurrent hidden states which provide historical conditioning but may slow training compared to STORM's Transformer approach.",
            "design_choices": "Categorical VAE encoder, GRU sequence model, hidden states included in agent state (latent + hidden), symlog two-hot loss for reward regression classification, actor-critic on imagined rollouts.",
            "comparison_to_alternatives": "Compared by STORM as a strong RNN-based baseline; STORM claims better wall-clock efficiency and higher mean performance for Atari 100k by replacing GRU with a Transformer and using a stochastic categorical latent with single-token per timestep.",
            "optimal_configuration": "STORM suggests that for 100k-sample Atari tasks, smaller models (e.g., small Transformer) suffice and that design choices like stochastic latents reduce model exploitation; by implication, DreamerV3's GRU + categorical VAE design is strong but may be outperformed by parallel Transformer-based designs under the authors' settings.",
            "uuid": "e1418.1",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SimPLe",
            "name_full": "Simulated Policy Learning (SimPLe)",
            "brief_description": "An early image-based model-based RL approach using a VAE-LSTM world model trained to reconstruct image observations and predict dynamics for policy learning in Atari.",
            "citation_title": "Model based reinforcement learning for atari",
            "mention_or_use": "mention",
            "model_name": "SimPLe",
            "model_description": "World model built with a binary-VAE image encoder and an LSTM recurrent sequence model to predict future latent states and reconstruct images for imagined rollouts and policy training.",
            "model_type": "latent world model (stochastic binary latent + RNN sequence model)",
            "task_domain": "Atari games (original SimPLe experiments focused on Atari)",
            "fidelity_metric": "Image reconstruction (decoder), next-state prediction error on latent/image space, and downstream policy performance (sample-efficiency metrics).",
            "fidelity_performance": "Reported in STORM's comparison table as human-mean normalized score 33% on Atari 100k; STORM also reports SimPLe had lengthy training (originally 20 days on P100, extrapolated to V100 hours in their table).",
            "interpretability_assessment": "Black-box VAE+LSTM; no interpretability methods discussed in STORM's mention beyond the general VAE latent structure.",
            "interpretability_method": "None specified in STORM for SimPLe beyond use of VAE latent space.",
            "computational_cost": "Original SimPLe reported as 20 days on NVIDIA P100 (extrapolated in STORM's comparison table to 240 V100 hours).",
            "efficiency_comparison": "SimPLe is slower and less sample-efficient than later methods (Dreamer variants, STORM) per STORM's comparison and requires much larger wall-clock training time.",
            "task_performance": "Human-mean normalized score 33% on Atari 100k (as quoted in STORM's comparison table).",
            "task_utility_analysis": "Demonstrated feasibility of learning policies from simulated data in image-based environments but gave lower final performance under limited-sample benchmarks relative to later world models.",
            "tradeoffs_observed": "LSTM-based recurrence models capture temporal structure but are less parallelizable (slower training) and earlier VAE choices (binary-VAE) offered different tradeoffs in latent expressivity versus stochasticity.",
            "design_choices": "Binary-VAE latent, LSTM recurrent dynamics model, end-to-end training for image reconstruction and dynamics prediction.",
            "comparison_to_alternatives": "Considered an earlier baseline; STORM claims substantial improvements in both performance and training speed over SimPLe by using categorical VAE + Transformer.",
            "optimal_configuration": "STORM suggests that replacing RNN recurrence with Transformer parallelism and using categorical stochastic latents improves efficiency and robustness under low-sample regimes.",
            "uuid": "e1418.2",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "TWM",
            "name_full": "Transformer-based World Model (TWM)",
            "brief_description": "A Transformer-XL based world model that treats observation, action, and reward as equal input tokens for sequence modeling, evaluated under the Atari 100k regime.",
            "citation_title": "Transformer-based world models are happy with 100k interactions",
            "mention_or_use": "mention",
            "model_name": "TWM (Transformer-XL based)",
            "model_description": "Sequence model uses Transformer-XL; input tokenization treats observation latents, actions, and rewards as separate tokens per timestep; dynamics are modeled autoregressively over these tokens.",
            "model_type": "latent world model (discrete/continuous latent + Transformer-XL sequence model)",
            "task_domain": "Atari 100k (Atari games under low-sample regime)",
            "fidelity_metric": "Downstream policy performance (human-normalized score) and sequence modeling/prediction accuracy as implied by reported results; STORM does not report internal numeric fidelity metrics for TWM.",
            "fidelity_performance": "Reported in STORM's comparison table as human-mean normalized score 96% on Atari 100k.",
            "interpretability_assessment": "Black-box Transformer-XL; using multiple token types (obs/action/reward) increases token count which may complicate attention attribution; no specific interpretability analyses reported in STORM.",
            "interpretability_method": "None specified in STORM's discussion beyond noting the different tokenization that may affect attention.",
            "computational_cost": "Reported by TWM authors as A100 10 hours; STORM quotes original TWM runs giving ~12.5 hours on RTX 3090 in their comparison table (extrapolated values in STORM).",
            "efficiency_comparison": "TWM's multi-token design increases token count and slows training relative to STORM's single-token-per-timestep approach; STORM claims a simpler tokenization (combine z_t and a_t) is more efficient.",
            "task_performance": "Human-mean normalized score 96% on Atari 100k (as quoted in STORM's comparison table).",
            "task_utility_analysis": "Transformer-XL can model long contexts, but treating reward and action as equal tokens may dilute attention and hurt performance per STORM's analysis; higher token counts also raise compute cost.",
            "tradeoffs_observed": "Tokenization scheme tradeoff: more tokens (obs/action/reward) can encode more detail but increase attention cost and may degrade performance if attention mixes heterogeneous modality signals.",
            "design_choices": "Transformer-XL backbone, multi-token per timestep (latent, action, reward) input representation.",
            "comparison_to_alternatives": "Compared to STORM which uses single-token per timestep and a vanilla GPT-like Transformer, TWM performed worse in STORM's Atari 100k comparisons and was noted to be slower due to higher token counts.",
            "optimal_configuration": "STORM suggests simpler single-token-per-timestep and smaller transformer layers are preferable under the 100k-sample Atari setting for efficiency and performance.",
            "uuid": "e1418.3",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "IRIS",
            "name_full": "IRIS (Transformers are sample-efficient world models)",
            "brief_description": "A world model that uses VQ-VAE to map images into spatial 4×4 latent tokens and a spatial-temporal Transformer to capture within- and across-image information.",
            "citation_title": "Transformers are sample-efficient world models",
            "mention_or_use": "mention",
            "model_name": "IRIS",
            "model_description": "Image encoder: VQ-VAE converting images into a 4×4 grid of discrete tokens; sequence model: spatial-temporal Transformer attends over many tokens within and across frames to predict dynamics and reconstruct images.",
            "model_type": "latent world model (vector-quantized spatial token latents + spatial-temporal transformer)",
            "task_domain": "Atari games / visual RL (sample-efficient settings)",
            "fidelity_metric": "Downstream task performance (human-normalized score) and quality of multi-token reconstructions; STORM does not provide IRIS internal numeric fidelity metrics.",
            "fidelity_performance": "Reported in STORM's comparison table as human-mean normalized score 105% on Atari 100k.",
            "interpretability_assessment": "Spatial-token representation yields a more explicit spatial decomposition (4×4 tokens) that is more amenable to inspection than monolithic latent vectors; however, attention over many tokens increases complexity and visualization overhead.",
            "interpretability_method": "Spatial tokenization via VQ-VAE enables inspecting discrete spatial latents; attention maps over tokens could be examined but no specific method reported in STORM.",
            "computational_cost": "Attention on large number of tokens in the spatial-temporal Transformer can significantly slow down training (noted by STORM); IRIS original runs were computationally heavy (A100 7 days for two runs quoted in STORM's table, extrapolated).",
            "efficiency_comparison": "Less efficient in wall-clock compared to STORM due to heavy attention over many tokens; may have richer spatial fidelity at higher compute cost.",
            "task_performance": "Human-mean normalized score 105% on Atari 100k (as quoted in STORM's comparison table).",
            "task_utility_analysis": "Explicit spatial tokens can better capture object positions and local dynamics, potentially improving fidelity for visually complex tasks, but increased token attention cost slows training and may not provide net gains under strict low-sample budgets.",
            "tradeoffs_observed": "Higher spatial fidelity via many tokens vs increased computational cost and slower training; STORM opts for single discrete latent token to trade some spatial granularity for efficiency.",
            "design_choices": "VQ-VAE encoder producing 4×4 token grid, spatial-temporal Transformer for attention across tokens and time.",
            "comparison_to_alternatives": "IRIS attains strong sample-efficiency but STORM argues its single-token categorical VAE + vanilla Transformer is computationally cheaper and competitive or superior under 100k-sample constraints.",
            "uuid": "e1418.4",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "TransDreamer",
            "name_full": "TransDreamer (Transformer-based replacement of Dreamer GRU)",
            "brief_description": "An approach that replaces the GRU sequence model in Dreamer with a Transformer architecture to build a Transformer-based world model for RL.",
            "citation_title": "Transdreamer: Reinforcement learning with transformer world models",
            "mention_or_use": "mention",
            "model_name": "TransDreamer",
            "model_description": "Variant of Dreamer where the GRU recurrent module is replaced by a Transformer for sequence modeling; aims to leverage attention for better long-range temporal modeling in imagined rollouts.",
            "model_type": "latent world model (categorical/discrete latent + transformer sequence model)",
            "task_domain": "RL domains similar to Dreamer (Atari, control tasks) as per original TransDreamer work",
            "fidelity_metric": "Downstream task performance and sequence modeling quality; STORM notes lack of published evidence of TransDreamer performance under low-sample widely-accepted benchmarks.",
            "fidelity_performance": "STORM states there is a lack of evidence demonstrating TransDreamer's performance in widely accepted environments or under limited sample conditions; no numeric fidelity provided in the STORM paper.",
            "interpretability_assessment": "Black-box Transformer-based model; no interpretability analyses discussed in STORM's mention.",
            "interpretability_method": "None reported in STORM for TransDreamer.",
            "computational_cost": "Not quantified in STORM; replacing GRU with Transformer could increase parallelization but may increase per-step attention cost depending on configuration.",
            "efficiency_comparison": "STORM positions its own Transformer design as an efficient Transformer-based world model and notes that prior Transformer attempts including TransDreamer did not convincingly surpass GRU baselines under low-sample benchmarks.",
            "task_performance": "No reliable task performance numbers provided by STORM for TransDreamer in the Atari 100k benchmark.",
            "task_utility_analysis": "Potential benefits of Transformer's long-range modeling are noted, but STORM highlights that evidence of practical gains in low-sample RL is limited for TransDreamer.",
            "tradeoffs_observed": "Replacing recurrence with attention may improve context modeling but can increase token processing cost and requires careful architecture choices to avoid slowing training or overfitting in low-data regimes.",
            "design_choices": "Transformer sequence model substituted for GRU within Dreamer-like architecture.",
            "comparison_to_alternatives": "STORM claims more careful use of Transformers (single-token, small depth, KV-cache) yields better efficiency and performance than some prior direct substitutions like TransDreamer under the 100k-sample setting.",
            "uuid": "e1418.5",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MuZero/EfficientZero",
            "name_full": "MuZero / EfficientZero (model + MCTS planners)",
            "brief_description": "Model-based RL approaches that combine learned models with Monte Carlo Tree Search (MCTS) lookahead planning to improve policies at test time, often achieving strong scores on Atari benchmarks at the cost of high computational demand.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MuZero / EfficientZero (learned model + MCTS planning)",
            "model_description": "Learned dynamics and value/policy prediction networks are used as a forward model inside Monte Carlo Tree Search to perform lookahead planning and improve action selection.",
            "model_type": "hybrid: learned model with planning (neural simulator + tree search)",
            "task_domain": "Atari games and board games (Chess, Go) — high-performance benchmarks",
            "fidelity_metric": "Policy performance (game scores) via planning; not described in detail in STORM beyond their use as lookahead baselines.",
            "fidelity_performance": "Not numerically compared in STORM's main results (authors exclude lookahead search methods from direct comparison but acknowledge they achieve promising performance on Atari 100k).",
            "interpretability_assessment": "Planning-based methods are partially interpretable in terms of search trajectories but learned neural dynamics remain black-box; STORM notes high computational demand of MCTS lookahead.",
            "interpretability_method": "Tree search trajectories can be inspected; no specific interpretability methods discussed in STORM.",
            "computational_cost": "Characterized as high computational demand by STORM; not included in direct wall-clock comparison since authors focus on world-model-only pipelines.",
            "efficiency_comparison": "STORM does not directly compare to these lookahead methods due to their high compute; notes that lookahead can be combined with STORM but would increase computational cost.",
            "task_performance": "Known from referenced work to achieve strong Atari performance (MuZero is a high-performing baseline) but STORM does not provide direct numeric comparisons on 100k run.",
            "task_utility_analysis": "Lookahead planning improves action selection by leveraging model rollouts but at high runtime cost; STORM opts to focus on improving the world model itself without planning in this work.",
            "tradeoffs_observed": "Planning with MCTS often improves policy but requires substantially more computation (wall-clock and inference-time) compared to pure imagined-rollout policy training used by STORM and other latent world-model approaches.",
            "design_choices": "Combine learned model of environment with search/planning (MCTS) to evaluate action outcomes more precisely at decision time.",
            "comparison_to_alternatives": "STORM explicitly excludes direct comparison to MuZero/EfficientZero in tables because those methods use lookahead search; authors state that such techniques can be combined with STORM in future work but carry high computational cost.",
            "uuid": "e1418.6",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Categorical-VAE",
            "name_full": "Categorical Variational Autoencoder (Categorical-VAE)",
            "brief_description": "A discrete stochastic latent image encoder used by STORM and DreamerV3 to produce categorical latent distributions (here 32×32) that inject controlled stochasticity into the world model and reduce overconfident deterministic rollouts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Categorical-VAE (discrete latent VAE)",
            "model_description": "Image encoder producing a stochastic categorical distribution over discrete latent codes (in STORM: 32 categories each with 32 classes forming a 32×32 output grid); decoder reconstructs images from sampled discrete latents using straight-through gradient estimator for backprop.",
            "model_type": "latent representation model (discrete stochastic encoder-decoder)",
            "task_domain": "Image-based RL (Atari 100k), used as the observation encoder/decoder in latent world models",
            "fidelity_metric": "Reconstruction L2 loss on decoded images; downstream policy performance (task score) used as proxy for useful fidelity; KL-based losses to align sequence predictions with encoder posteriors.",
            "fidelity_performance": "No standalone numeric reconstruction metrics reported for categorical-VAE in STORM; contributes to overall STORM task performance (mean human-normalized 126.7%).",
            "interpretability_assessment": "Discrete latent codes provide some interpretability potential (discrete clusters) compared to continuous embeddings; STORM notes categorical latents inject stochasticity that can improve robustness but does not provide explicit interpretability analyses.",
            "interpretability_method": "Not reported in STORM beyond noting discrete token structure; straight-through estimator used to propagate gradients.",
            "computational_cost": "Encoder/decoder are CNNs with shapes described in Appendix B; discrete latents can reduce dimensionality and speed subsequent sequence modeling relative to pixel-based dynamics, but exact parameter counts not given.",
            "efficiency_comparison": "Categorical discrete latents reduce sequence-model token size compared to spatial multi-token schemes (e.g., IRIS VQ-VAE 4×4 tokens) and are positioned as an efficient compromise in STORM.",
            "task_performance": "Used as part of STORM which achieves 126.7% mean human-normalized score; categorical-VAE is credited with improving robustness and reducing accumulated autoregressive errors.",
            "task_utility_analysis": "Stochastic discrete latents supply controlled noise that prevents the agent from exploiting deterministic model errors and improves generalization of imagined rollouts; however, sampling noise can harm attention when small objects are critical.",
            "tradeoffs_observed": "Discrete stochastic latents trade spatial granularity for robustness/efficiency: they reduce overconfident rollouts but may lose fine-grained small-object detail compared with richer spatial tokenizations.",
            "design_choices": "32×32 categorical grid latent; straight-through gradient trick for sampling; paired with Transformer sequence model; representation and dynamics KL losses to stabilize learning.",
            "comparison_to_alternatives": "Compared against VQ-VAE (IRIS) and binary-VAE (SimPLe): categorical-VAE provides stochasticity and compact representation, claimed by STORM to be an efficient choice for Atari 100k.",
            "uuid": "e1418.7",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2
        },
        {
            "paper_title": "Model based reinforcement learning for atari",
            "rating": 2
        },
        {
            "paper_title": "Transformer-based world models are happy with 100k interactions",
            "rating": 2
        },
        {
            "paper_title": "Transformers are sample-efficient world models",
            "rating": 2
        },
        {
            "paper_title": "World models",
            "rating": 2
        },
        {
            "paper_title": "Transdreamer: Reinforcement learning with transformer world models",
            "rating": 2
        },
        {
            "paper_title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
            "rating": 1
        },
        {
            "paper_title": "Mastering Atari with discrete world models",
            "rating": 1
        }
    ],
    "cost": 0.020233249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning</h1>
<p>Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan<br>National Key Lab of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology<br>Beijing Institute of Technology Chongqing Innovation Center<br>zhangwp.bit@gmail.com, {gangwang, sunjian, ytyuan}@bit.edu.cn</p>
<p>Gao Huang<br>Department of Automation, BNRist, Tsinghua University<br>gaohuang@tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial. In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of $126.7 \%$ on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with 1.85 hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only 4.3 hours, showcasing improved efficiency compared to previous methodologies. We release our code at https://github.com/weipu-zhang/STORM.</p>
<h2>1 Introduction</h2>
<p>Deep reinforcement learning (DRL) has exhibited remarkable success across diverse domains. However, its widespread application in real-world environments is hindered by the substantial number of interactions with the environment required for achieving such success. This limitation becomes particularly challenging when dealing with broader real-world settings in e.g., unmanned and manufacturing systems [1, 2] that lack adjustable speed simulation tools. Consequently, improving the sample efficiency has emerged as a key challenge for DRL algorithms.
Popular DRL methods, including Rainbow [3] and PPO [4], suffer from low sample efficiency due to two primary reasons. Firstly, the estimation of the value function proves to be a challenging task. This</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>involves approximating the value function using a deep neural network (DNN) and updating it with $n$-step bootstrapped temporal difference, which naturally requires numerous iterations to converge [5]. Secondly, in scenarios where rewards are sparse, many samples exhibit similarity in terms of value functions, providing limited useful information for training and generalization of the DNN [6, 7]. This further exacerbates the challenge of improving the sample efficiency of DRL algorithms.</p>
<p>To address these challenges, model-based DRL algorithms have emerged as a promising approach that tackles both issues simultaneously while demonstrating significant performance gains in sampleefficient settings. These algorithms start by constructing a parameterized simulation world model of the real environment through self-supervised learning. Self-supervised learning can be implemented in various ways, such as reconstructing the original input state using a decoder [8-10], predicting actions between frames [7], or employing contrastive learning to capture the internal consistency of input states [6, 7]. These approaches provide more supervision information than conventional modelfree RL losses, enhancing the feature extraction capabilities of DNNs. Subsequently, the agent's policy is improved by leveraging the experiences generated using the world model, eliminating sampling constraints and enabling faster updates to the value function compared to model-free algorithms.</p>
<p>However, the process of imagining with a world model involves an autoregressive process that can accumulate prediction errors over time. In situations where discrepancies arise between the imagined trajectory and the real trajectory, the agent may inadvertently pursue virtual goals, resulting in subpar performance in the real environment. To mitigate this issue, introducing random noise into the world model has been proven beneficial [9-11, 14]. Variational autoencoders, capable of automatically learning low-dimensional latent representations of highdimensional data while incorporating reasonable random noise into the latent space, offer an ideal choice for image encoding.</p>
<p>Numerous endeavors have been undertaken to construct an efficient world model. For instance, SimPLe [11] leverages LSTM [15], while DreamerV3 [10] employs GRU [16] as the sequence model. LSTM and GRU, both variants of recurrent neural networks (RNNs), excel at sequence modeling tasks. However, the recurrent nature of RNNs impedes parallelized computing, resulting in slower training speeds [17]. In contrast, the Transformer architecture [17] has lately demonstrated superior performance over RNNs in various sequence modeling and generation tasks. It overcomes the challenge of forgetting long-term dependencies and is designed for efficient parallel computing. While several attempts have been made to incorporate Transformers into the world model [12, 13, 18], these works do not fully harness the capabilities of this architecture. Furthermore, these approaches require even longer training times and fail to surpass the performance of the GRUbased DreamerV3.</p>
<p>In this paper, we introduce the Stochastic Transformer-based wORld Model (STORM), a highly effective and efficient structure for model-based RL. STORM employs a categorical variational autoencoder (VAE) as the image encoder, enhancing the agent robustness and reducing accumulated autoregressive prediction errors. Subsequently, we incorporate the Transformer as the sequence</p>
<p>model, improving modeling and generation quality while accelerating training. STORM achieves a remarkable mean human normalized score of $126.7 \%$ on the challenging Atari 100k benchmark, establishing a new record for methods without resorting to lookahead search. Furthermore, training an agent with 1.85 hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only 4.3 hours, demonstrating superior efficiency compared to previous methodologies. The comparison of our approach with the state-of-the-art methods is depicted in Figure 1.</p>
<h1>2 Related work</h1>
<p>Model-based DRL algorithms aim to construct a simulation model of the environment and utilize simulated experiences to improve the policy. While traditional model-based RL techniques like Dyna-Q have shown success in tabular cases [5], modeling complex environments such as video games and visual control tasks presents significant challenges. Recent advances in computing and DNNs have enabled model-based methods to learn the dynamics of the environments and start to outperform model-free methods on these tasks.</p>
<p>The foundation of VAE-LSTM-based world models was introduced by Ha and Schmidhuber [14] for image-based environments, demonstrating the feasibility of learning a good policy solely from generated data. SimPLe [11] applied this methodology to Atari games, resulting in substantial sample efficiency improvements compared to Rainbow [3], albeit with relatively lower performance under limited samples. The Dreamer series [8-10] also adopt this framework and showcase notable capabilities in Atari games, DeepMind Control, Minecraft, and other domains, using GRU [16] as the core sequential model. However, as discussed earlier, RNN structures suffer from slow training [17].</p>
<p>Recent approaches such as IRIS [13], TWM [12], and TransDreamer [18] incorporate the Transformer architecture into their world models. IRIS [13] employs VQ-VAE [19] as the encoder to map images into $4 \times 4$ latent tokens and uses a spatial-temporal Transformer [20] to capture information within and across images. However, the attention operations on a large number of tokens in the spatial-temporal structure can result in a significant training slowdown. TWM [12] adopts Transformer-XL [21] as its core architecture and organizes the sequence model in a structure similar to Decision Transformer [22], treating the observation, action, and reward as equivalent input tokens for the Transformer. Performing self-attention across different types of data may have a negative impact on the performance, and the increased number of tokens considerably slows down training. TransDreamer [18] directly replaces the GRU structure of Dreamer with Transformer. However, there is a lack of evidence demonstrating their performance in widely accepted environments or under limited sample conditions.</p>
<p>Other model-based RL methods such as MuZero [23], EfficientZero [7], and SpeedyZero [24] incorporate Monte Carlo tree search (MCTS) to enhance policy and achieve promising performance on the Atari 100k benchmark. Lookahead search techniques like MCTS can be employed to enhance other model-based RL algorithms, but they come with high computational demands. Additionally, certain model-free methods [6, 25-27] incorporate a self-supervised loss as an auxiliary term alongside the standard RL loss, demonstrating their effectiveness in sample-efficient settings. Additionally, recent studies [28, 29] delve deeper into model-free RL, demonstrating strong performance and high data efficiency, rivaling that of model-based methods on several benchmarks. However, since the primary objective of this paper is to enhance the world model, we do not delve further into these methods.</p>
<p>We highlight the distinctions between STORM and recent approaches in the world model as follows:</p>
<p>Table 1: Comparison between STORM and recent approaches. "Tokens" refers to the input tokens introduced to the sequence model during a single timestep. "Historical information" indicates whether the VAE reconstruction process incorporates historical data, such as the hidden states of an RNN.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Attributes</th>
<th style="text-align: center;">SimPLe [11]</th>
<th style="text-align: center;">TWM [12]</th>
<th style="text-align: center;">IRIS [13]</th>
<th style="text-align: center;">DreamerV3 [10]</th>
<th style="text-align: center;">STORM (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Sequence model</td>
<td style="text-align: center;">LSTM [15]</td>
<td style="text-align: center;">Transformer-XL [21]</td>
<td style="text-align: center;">Transformer [17]</td>
<td style="text-align: center;">GRU [16]</td>
<td style="text-align: center;">Transformer</td>
</tr>
<tr>
<td style="text-align: center;">Tokens</td>
<td style="text-align: center;">Latent</td>
<td style="text-align: center;">Latent, action, reward</td>
<td style="text-align: center;">Latent( $4 \times 4)$</td>
<td style="text-align: center;">Latent</td>
<td style="text-align: center;">Latent</td>
</tr>
<tr>
<td style="text-align: center;">Latent representation</td>
<td style="text-align: center;">Binary-VAE</td>
<td style="text-align: center;">Categorical-VAE</td>
<td style="text-align: center;">VQ-VAE</td>
<td style="text-align: center;">Categorical-VAE</td>
<td style="text-align: center;">Categorical-VAE</td>
</tr>
<tr>
<td style="text-align: center;">Historical information</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;">Agent state</td>
<td style="text-align: center;">Reconstructed image</td>
<td style="text-align: center;">Latent</td>
<td style="text-align: center;">Reconstructed image</td>
<td style="text-align: center;">Latent, hidden</td>
<td style="text-align: center;">Latent, hidden</td>
</tr>
<tr>
<td style="text-align: center;">Agent training</td>
<td style="text-align: center;">PPO [4]</td>
<td style="text-align: center;">As DreamerV2 [9]</td>
<td style="text-align: center;">As DreamerV2 [9]</td>
<td style="text-align: center;">DreamerV3</td>
<td style="text-align: center;">As DreamerV3</td>
</tr>
</tbody>
</table>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Structure and imagination process of STORM. The symbols used in the figure are explained in Sections 3.1 and 3.2. The Transformer blocks depict the sequence model $f_{\phi}$ in Equation (2). The Agent block, represented by a neural network, corresponds to $\pi_{\theta}\left(a_{t} \mid s_{t}\right)$ in Equation (6).</p>
<ul>
<li>SimPLe [11] and Dreamer [10] rely on RNN-based models, whereas STORM employs a GPT-like Transformer [30] as the sequence model.</li>
<li>In contrast to IRIS [13] that employs multiple tokens, STORM utilizes a single stochastic latent variable to represent an image.</li>
<li>STORM follows a vanilla Transformer [17] structure, while TWM [12] adopts a TransformerXL [21] structure.</li>
<li>In the sequence model of STORM, an observation and an action are fused into a single token, whereas TWM [12] treats observation, action, and reward as three separate tokens of equal importance.</li>
<li>Unlike Dreamer [10] and TransDreamer [18], which incorporate hidden states, STORM reconstructs the original image without utilizing this information.</li>
</ul>
<h1>3 Method</h1>
<h3>3.1 World model learning</h3>
<p>Our approach adheres to the established framework of model-based RL algorithms, which focus on enhancing the agent's policy by imagination [5, 9-11, 13]. We iterate through the following steps until reaching the prescribed number of real environment interactions.</p>
<p>S1) Gather real environment data by executing the current policy for several steps and append them to the replay buffer.
S2) Update the world model using trajectories sampled from the replay buffer.
S3) Improve the policy using imagined experiences generated by the world model, with the starting points for the imagination process sampled from the replay buffer.</p>
<p>At each time $t$, a data point comprises an observation $o_{t}$, an action $a_{t}$, a reward $r_{t}$, and a continuation flag $c_{t}$ (a Boolean variable indicating whether the current episode is ongoing). The replay buffer maintains a first-in-first-out queue structure, enabling the sampling of consecutive trajectories from the buffer.</p>
<p>Section 3.1 provides a detailed description of the architecture and training losses employed by STORM. On the other hand, Section 3.2 elaborates on the imagination process and the training methodology employed by the agent. It provides an thorough explanation of how the agent leverages the world model to simulate experiences and improve its policy.</p>
<p>Model structure The complete structure of our world model is illustrated in Figure 2. In our experiments, we focus on Atari games [31], which generate image observations $o_{t}$ of the environment. Modeling the dynamics of the environment directly on raw images is computationally expensive and prone to errors [7-11, 13, 23]. To address this, we leverage a VAE [32] formulated in Equation (1) to convert $o_{t}$ into latent stochastic categorical distributions $\mathcal{Z}<em t="t">{t}$. Consistent with prior work [9, 10, 12], we set $\mathcal{Z}</em>}$ as a stochastic distribution comprising 32 categories, each with 32 classes. The encoder ( $q_{\phi}$ ) and decoder ( $p_{\phi}$ ) structures are implemented as convolutional neural networks (CNNs) [33]. Subsequently, we sample a latent variable $z_{t}$ from $\mathcal{Z<em t="t">{t}$ to represent the original observation $o</em>$. Since sampling from a distribution lacks gradients for backward propagation, we apply the straight-through gradients trick [9,34] to preserve them.</p>
<p>$$
\begin{array}{ll}
\text { Image encoder: } &amp; z_{t} \sim q_{\phi}\left(z_{t} \mid o_{t}\right)=\mathcal{Z}<em t="t">{t} \
\text { Image decoder: } &amp; \hat{o}</em>\right)
\end{array}
$$}=p_{\phi}\left(z_{t</p>
<p>Before entering the sequence model, we combine the latent sample $z_{t}$ and the action $a_{t}$ into a single token $e_{t}$ using multi-layer perceptrons (MLPs) and concatenation. This operation, denoted as $m_{\phi}$, prepares the inputs for the sequence model. The sequence model $f_{\phi}$ takes the sequence of $e_{t}$ as input and produces hidden states $h_{t}$. We adopt a GPT-like Transformer structure [30] for the sequence model, where the self-attention blocks are masked with a subsequent mask allowing $e_{t}$ to attend to the sequence $e_{1}, e_{2}, \ldots, e_{t}$. By utilizing MLPs $g_{\phi}^{E}, g_{\phi}^{R}$, and $g_{\phi}^{C}$, we rely on $h_{t}$ to predict the current reward $\hat{r}<em t="t">{t}$, the continuation flag $\hat{c}</em>$. The formulation of this part of the world model is as follows}$, and the next distribution $\hat{\mathcal{Z}}_{t+1</p>
<p>$$
\begin{array}{ll}
\text { Action mixer: } &amp; e_{t}=m_{\phi}\left(z_{t}, a_{t}\right) \
\text { Sequence model: } &amp; h_{1: T}=f_{\phi}\left(e_{1: T}\right) \
\text { Dynamics predictor: } &amp; \hat{\mathcal{Z}}<em _phi="\phi">{t+1}=g</em>}^{D}\left(\hat{z<em t="t">{t+1} \mid h</em>\right) \
\text { Reward predictor: } &amp; \hat{r}<em _phi="\phi">{t}=g</em>\right) \
\text { Continuation predictor: } &amp; \hat{c}}^{R}\left(h_{t<em _phi="\phi">{t}=g</em>\right)
\end{array}
$$}^{C}\left(h_{t</p>
<p>Loss functions The world model is trained in a self-supervised manner, optimizing it end-to-end. The total loss function is calculated as in Equation (3) below, with fixed hyperparameters $\beta_{1}=0.5$ and $\beta_{2}=0.1$. In the equation, $B$ denotes the batch size, and $T$ denotes the batch length</p>
<p>$$
\mathcal{L}(\phi)=\frac{1}{B T} \sum_{n=1}^{B} \sum_{t=1}^{T}\left[\mathcal{L}<em t="t">{t}^{\mathrm{rec}}(\phi)+\mathcal{L}</em>}^{\mathrm{rew}}(\phi)+\mathcal{L<em 1="1">{t}^{\mathrm{con}}(\phi)+\beta</em>} \mathcal{L<em 2="2">{t}^{\mathrm{dyn}}(\phi)+\beta</em>(\phi)\right]
$$} \mathcal{L}_{t}^{\mathrm{rep}</p>
<p>The individual components of the loss function are defined as follows: $\mathcal{L}<em t="t">{t}^{\text {rec }}(\phi)$ represents the reconstruction loss of the original image, $\mathcal{L}</em>(\phi)$ represents the prediction loss of the continuation flag.}^{\text {rew }}(\phi)$ represents the prediction loss of the reward, and $\mathcal{L}_{t}^{\text {con }</p>
<p>$$
\begin{aligned}
\mathcal{L}<em t="t">{t}^{\text {rec }}(\phi) &amp; =\left|\hat{o}</em>\right|}-o_{t<em t="t">{2} \
\mathcal{L}</em>}^{\text {rew }}(\phi) &amp; =\mathcal{L}^{\text {sym }}\left(\hat{r<em t="t">{t}, r</em>\right) \
\mathcal{L}<em t="t">{t}^{\text {con }}(\phi) &amp; =c</em>} \log \hat{c<em t="t">{t}+\left(1-c</em>\right)
\end{aligned}
$$}\right) \log \left(1-\hat{c}_{t</p>
<p>Additionally, $\mathcal{L}^{\text {sym }}$ in Equation (4b) denotes the symlog two-hot loss, as described in [10]. This loss function transforms the regression problem into a classification problem, ensuring consistent loss scaling across different environments.</p>
<p>The losses $\mathcal{L}<em t="t">{t}^{\text {dyn }}(\phi)$ and $\mathcal{L}</em>}^{\text {rep }}(\phi)$ are expressed as Kullback-Leibler (KL) divergences but differ in their gradient backward and weighting. The dynamics loss $\mathcal{L<em t="t">{t}^{\text {dyn }}(\phi)$ guides the sequence model in predicting the next distribution, while the representation loss $\mathcal{L}</em>(\phi)$ allows the output of the encoder to be weakly influenced by the sequence model's prediction. This ensures that the learning of distributional dynamics is not excessively challenging.}^{\text {rep }</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _phi="\phi">{t}^{\text {dyn }}(\phi) &amp; =\max \left(1, \operatorname{KL}\left[\operatorname{sg}\left(q</em>}\left(z_{t+1} \mid o_{t+1}\right)\right) | g_{\phi}^{D}\left(\hat{z<em t="t">{t+1} \mid h</em>\right)\right]\right) \
\mathcal{L}<em _phi="\phi">{t}^{\text {rep }}(\phi) &amp; =\max \left(1, \operatorname{KL}\left[q</em>}\left(z_{t+1} \mid o_{t+1}\right) | \operatorname{sg}\left(g_{\phi}^{D}\left(\hat{z<em t="t">{t+1} \mid h</em>\right)\right)\right]\right)
\end{aligned}
$$</p>
<p>where $\operatorname{sg}(\cdot)$ denotes the operation of stop-gradients.</p>
<h1>3.2 Agent learning</h1>
<p>The agent's learning is solely based on the imagination process facilitated by the world model, as illustrated in Figure 2. To initiate the imagination process, a brief contextual trajectory is randomly selected from the replay buffer, and the initial posterior distribution $\mathcal{Z}<em t="t">{t}$ is computed. During inference, rather than sampling directly from the posterior distribution $\mathcal{Z}</em>}$, we sample $z_{t}$ from the prior distribution $\hat{\mathcal{Z}<em t="t">{t}$. To accelerate the inference, we employ the KV cache technique [35] within the Transformer structure.
The agent's state is formed by concatenating $z</em>$, as shown below:}$ and $h_{t</p>
<p>$$
\begin{array}{ll}
\text { State: } &amp; s_{t}=\left[z_{t}, h_{t}\right] \
\text { Critic: } &amp; V_{\psi}\left(s_{t}\right) \approx \mathbb{E}<em _theta="\theta">{\pi</em>\right] \
\text { Actor: } &amp; a_{t} \sim \pi_{\theta}\left(a_{t} \mid s_{t}\right)
\end{array}
$$}, p_{\phi}}\left[\sum_{k=0}^{\infty} \gamma^{k} r_{t+k</p>
<p>We adopt the actor learning settings from DreamerV3 [10]. The complete loss of the actor-critic algorithm is described by Equation (7), where $\hat{r}<em t="t">{t}$ corresponds to the reward predicted by the world model, and $\hat{c}</em>$ represents the predicted continuation flag:</p>
<p>$$
\begin{aligned}
\mathcal{L}(\theta) &amp; =\frac{1}{B L} \sum_{n=1}^{B} \sum_{t=1}^{L}\left[-\operatorname{sg}\left(\frac{G_{t}^{\lambda}-V_{\psi}\left(s_{t}\right)}{\max (1, S)}\right) \ln \pi_{\theta}\left(a_{t} \mid s_{t}\right)-\eta H\left(\pi_{\theta}\left(a_{t} \mid s_{t}\right)\right)\right] \
\mathcal{L}(\psi) &amp; =\frac{1}{B L} \sum_{n=1}^{B} \sum_{t=1}^{L}\left[\left(V_{\psi}\left(s_{t}\right)-\operatorname{sg}\left(G_{t}^{\lambda}\right)\right)^{2}+\left(V_{\psi}\left(s_{t}\right)-\operatorname{sg}\left(V_{\psi \text { EMA }}\left(s_{t}\right)\right)\right)^{2}\right]
\end{aligned}
$$</p>
<p>where $H(\cdot)$ denotes the entropy of the policy distribution, while constants $\eta$ and $L$ represent the coefficient for entropy loss and the imagination horizon, respectively. The $\lambda$-return $G_{t}^{\lambda}[5,10]$ is recursively defined as follows</p>
<p>$$
\begin{aligned}
&amp; G_{t}^{\lambda} \doteq r_{t}+\gamma c_{t}\left[(1-\lambda) V_{\psi}\left(s_{t+1}\right)+\lambda G_{t+1}^{\lambda}\right] \
&amp; G_{L}^{\lambda} \doteq V_{\psi}\left(s_{L}\right)
\end{aligned}
$$</p>
<p>The normalization ratio $S$ utilized in the actor loss (7a) is defined in Equation (9), which is computed as the range between the 95 th and 5 th percentiles of the $\lambda$-return $G_{t}^{\lambda}$ across the batch [10]</p>
<p>$$
S=\operatorname{percentile}\left(G_{t}^{\lambda}, 95\right)-\operatorname{percentile}\left(G_{t}^{\lambda}, 5\right)
$$</p>
<p>To regularize the value function, we maintain the exponential moving average (EMA) of $\psi$. The EMA is defined in Equation (10), where $\psi_{t}$ represents the current critic parameters, $\sigma$ is the decay rate, and $\psi_{t+1}^{\mathrm{EMA}}$ denotes the updated critic parameters. This regularization technique aids in stabilizing training and preventing overfitting</p>
<p>$$
\psi_{t+1}^{\mathrm{EMA}}=\sigma \psi_{t}^{\mathrm{EMA}}+(1-\sigma) \psi_{t}
$$</p>
<h2>4 Experiments</h2>
<p>We evaluated the performance of STORM on the widely-used benchmark for sample-efficient RL, Atari 100k [31]. For detailed information about the benchmark, evaluation methodology, and the baselines used for comparison, please refer to Section 4.1. The comprehensive results for the Atari 100k games are presented in Section 4.2.</p>
<h3>4.1 Benchmark and baselines</h3>
<p>Atari 100k consists of 26 different video games with discrete action dimensions of up to 18. The 100 k sample constraint corresponds to 400 k actual game frames, taking into account frame skipping ( 4 frames skipped) and repeated actions within those frames. This constraint corresponds to</p>
<p>approximately 1.85 hours of real-time gameplay. The agent's human normalized score $\tau=\frac{A-R}{R-R}$ is calculated based on the score $A$ achieved by the agent, the score $R$ obtained by a random policy, and the average score $H$ achieved by a human player in a specific environment. To determine the human player's performance $H$, a player is allowed to become familiar with the game under the same sample constraint.</p>
<p>To demonstrate the efficiency of our proposed world model structure, we compare it with model-based DRL algorithms that share a similar training pipeline, as discussed in Section 2. However, similarly to [10, 12, 13], we do not directly compare our results with lookahead search methods like MuZero [23] and EfficientZero [7], as our primary goal is to refine the world model itself. Nonetheless, lookahead search techniques can be combined with our method in the future to further enhance the agent's performance.</p>
<h1>4.2 Results on Atari 100k</h1>
<p>Detailed results for each environment can be found in Table 2, and the corresponding performance curve is presented in Appendix A due to space limitations. In our experiments, we trained STORM using 5 different seeds and saved checkpoints every 2,500 sample steps. We assessed the agent's performance by conducting 20 evaluation episodes for each checkpoint and computed the average score. The result reported in Table 2 is the average of the scores attained using the final checkpoints.</p>
<p>STORM demonstrates superior performance compared to previous methods in environments where the key objects related to rewards are large or multiple, such as Amidar, MsPacman, Chopper Command, and Gopher. This advantage can be attributed to the attention mechanism, which explicitly preserves the history of these moving objects, allowing for an easy inference of their speed and direction information, unlike RNN-based methods. However, STORM faces challenges when handling a single small moving object, as observed in Pong and Breakout, due to the nature of autoencoders. Moreover, performing attention operations under such circumstances can potentially harm performance, as the randomness introduced by sampling may excessively influence the attention weights.</p>
<p>Table 2: Game scores and overall human-normalized scores on the 26 games in the Atari 100k benchmark. Following the conventions of [9], scores that are the highest or within $5 \%$ of the highest score are highlighted in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: right;">Random</th>
<th style="text-align: right;">Human</th>
<th style="text-align: right;">SimPLe [11]</th>
<th style="text-align: right;">TWM [12]</th>
<th style="text-align: right;">IRIS [13]</th>
<th style="text-align: right;">DreamerV3 [10]</th>
<th style="text-align: right;">STORM (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alien</td>
<td style="text-align: right;">228</td>
<td style="text-align: right;">7128</td>
<td style="text-align: right;">617</td>
<td style="text-align: right;">675</td>
<td style="text-align: right;">420</td>
<td style="text-align: right;">$\mathbf{9 5 9}$</td>
<td style="text-align: right;">$\mathbf{9 8 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Amidar</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1720</td>
<td style="text-align: right;">74</td>
<td style="text-align: right;">122</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">139</td>
<td style="text-align: right;">$\mathbf{2 0 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Assault</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">742</td>
<td style="text-align: right;">527</td>
<td style="text-align: right;">683</td>
<td style="text-align: right;">$\mathbf{1 5 2 4}$</td>
<td style="text-align: right;">706</td>
<td style="text-align: right;">801</td>
</tr>
<tr>
<td style="text-align: left;">Asterix</td>
<td style="text-align: right;">210</td>
<td style="text-align: right;">8503</td>
<td style="text-align: right;">$\mathbf{1 1 2 8}$</td>
<td style="text-align: right;">$\mathbf{1 1 1 6}$</td>
<td style="text-align: right;">854</td>
<td style="text-align: right;">932</td>
<td style="text-align: right;">1028</td>
</tr>
<tr>
<td style="text-align: left;">Bank Heist</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">753</td>
<td style="text-align: right;">34</td>
<td style="text-align: right;">467</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">$\mathbf{6 4 9}$</td>
<td style="text-align: right;">$\mathbf{6 4 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Battle Zone</td>
<td style="text-align: right;">2360</td>
<td style="text-align: right;">37188</td>
<td style="text-align: right;">4031</td>
<td style="text-align: right;">5068</td>
<td style="text-align: right;">$\mathbf{1 3 0 7 4}$</td>
<td style="text-align: right;">12250</td>
<td style="text-align: right;">$\mathbf{1 3 5 4 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Boxing</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">$\mathbf{7 8}$</td>
<td style="text-align: right;">70</td>
<td style="text-align: right;">$\mathbf{7 8}$</td>
<td style="text-align: right;">$\mathbf{8 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Breakout</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">$\mathbf{8 4}$</td>
<td style="text-align: right;">31</td>
<td style="text-align: right;">16</td>
</tr>
<tr>
<td style="text-align: left;">Chopper Command</td>
<td style="text-align: right;">811</td>
<td style="text-align: right;">7388</td>
<td style="text-align: right;">979</td>
<td style="text-align: right;">1697</td>
<td style="text-align: right;">1565</td>
<td style="text-align: right;">420</td>
<td style="text-align: right;">$\mathbf{1 8 8 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Crazy Climber</td>
<td style="text-align: right;">10780</td>
<td style="text-align: right;">35829</td>
<td style="text-align: right;">62584</td>
<td style="text-align: right;">71820</td>
<td style="text-align: right;">59234</td>
<td style="text-align: right;">$\mathbf{9 7 1 9 0}$</td>
<td style="text-align: right;">66776</td>
</tr>
<tr>
<td style="text-align: left;">Demon Attack</td>
<td style="text-align: right;">152</td>
<td style="text-align: right;">1971</td>
<td style="text-align: right;">208</td>
<td style="text-align: right;">350</td>
<td style="text-align: right;">$\mathbf{2 0 3 4}$</td>
<td style="text-align: right;">303</td>
<td style="text-align: right;">165</td>
</tr>
<tr>
<td style="text-align: left;">Freeway</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">$\mathbf{3 1}$</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">$\mathbf{3 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Freeway w/o traj</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">$\mathbf{3 1}$</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Frostbite</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">4335</td>
<td style="text-align: right;">237</td>
<td style="text-align: right;">$\mathbf{1 4 7 6}$</td>
<td style="text-align: right;">259</td>
<td style="text-align: right;">909</td>
<td style="text-align: right;">1316</td>
</tr>
<tr>
<td style="text-align: left;">Gopher</td>
<td style="text-align: right;">258</td>
<td style="text-align: right;">2413</td>
<td style="text-align: right;">597</td>
<td style="text-align: right;">1675</td>
<td style="text-align: right;">2236</td>
<td style="text-align: right;">3730</td>
<td style="text-align: right;">$\mathbf{8 2 4 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Hero</td>
<td style="text-align: right;">1027</td>
<td style="text-align: right;">30826</td>
<td style="text-align: right;">2657</td>
<td style="text-align: right;">7254</td>
<td style="text-align: right;">7037</td>
<td style="text-align: right;">$\mathbf{1 1 1 6 1}$</td>
<td style="text-align: right;">$\mathbf{1 1 0 4 4}$</td>
</tr>
<tr>
<td style="text-align: left;">James Bond</td>
<td style="text-align: right;">29</td>
<td style="text-align: right;">303</td>
<td style="text-align: right;">101</td>
<td style="text-align: right;">362</td>
<td style="text-align: right;">463</td>
<td style="text-align: right;">445</td>
<td style="text-align: right;">$\mathbf{5 0 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Kangaroo</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">3035</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">1240</td>
<td style="text-align: right;">838</td>
<td style="text-align: right;">$\mathbf{4 0 9 8}$</td>
<td style="text-align: right;">$\mathbf{4 2 0 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Krull</td>
<td style="text-align: right;">1598</td>
<td style="text-align: right;">2666</td>
<td style="text-align: right;">2204</td>
<td style="text-align: right;">6349</td>
<td style="text-align: right;">6616</td>
<td style="text-align: right;">7782</td>
<td style="text-align: right;">$\mathbf{8 4 1 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Kung Fu Master</td>
<td style="text-align: right;">256</td>
<td style="text-align: right;">22736</td>
<td style="text-align: right;">14862</td>
<td style="text-align: right;">24555</td>
<td style="text-align: right;">21760</td>
<td style="text-align: right;">21420</td>
<td style="text-align: right;">$\mathbf{2 6 1 8 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Ms Pacman</td>
<td style="text-align: right;">307</td>
<td style="text-align: right;">6952</td>
<td style="text-align: right;">1480</td>
<td style="text-align: right;">1588</td>
<td style="text-align: right;">999</td>
<td style="text-align: right;">1327</td>
<td style="text-align: right;">$\mathbf{2 6 7 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Pong</td>
<td style="text-align: right;">-21</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">$\mathbf{1 9}$</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">$\mathbf{1 8}$</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Private Eye</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">69571</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">87</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">882</td>
<td style="text-align: right;">$\mathbf{7 7 8 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Qbert</td>
<td style="text-align: right;">164</td>
<td style="text-align: right;">13455</td>
<td style="text-align: right;">1289</td>
<td style="text-align: right;">3331</td>
<td style="text-align: right;">746</td>
<td style="text-align: right;">3405</td>
<td style="text-align: right;">$\mathbf{4 5 2 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Road Runner</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">7845</td>
<td style="text-align: right;">5641</td>
<td style="text-align: right;">9109</td>
<td style="text-align: right;">9615</td>
<td style="text-align: right;">15565</td>
<td style="text-align: right;">$\mathbf{1 7 5 6 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Seaquest</td>
<td style="text-align: right;">68</td>
<td style="text-align: right;">42055</td>
<td style="text-align: right;">683</td>
<td style="text-align: right;">$\mathbf{7 7 4}$</td>
<td style="text-align: right;">661</td>
<td style="text-align: right;">618</td>
<td style="text-align: right;">525</td>
</tr>
<tr>
<td style="text-align: left;">Up N Down</td>
<td style="text-align: right;">533</td>
<td style="text-align: right;">11693</td>
<td style="text-align: right;">3350</td>
<td style="text-align: right;">$\mathbf{1 5 9 8 2}$</td>
<td style="text-align: right;">3546</td>
<td style="text-align: right;">7667</td>
<td style="text-align: right;">7985</td>
</tr>
<tr>
<td style="text-align: left;">Human Mean</td>
<td style="text-align: right;">$0 \%$</td>
<td style="text-align: right;">$100 \%$</td>
<td style="text-align: right;">$33 \%$</td>
<td style="text-align: right;">$96 \%$</td>
<td style="text-align: right;">$105 \%$</td>
<td style="text-align: right;">$112 \%$</td>
<td style="text-align: right;">$\mathbf{1 2 6 . 7 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Human Median</td>
<td style="text-align: right;">$0 \%$</td>
<td style="text-align: right;">$100 \%$</td>
<td style="text-align: right;">$13 \%$</td>
<td style="text-align: right;">$51 \%$</td>
<td style="text-align: right;">$29 \%$</td>
<td style="text-align: right;">$49 \%$</td>
<td style="text-align: right;">$\mathbf{5 8 . 4 \%}$</td>
</tr>
</tbody>
</table>
<h1>5 Ablation studies</h1>
<p>In our experiments, we have observed that the design and configuration choices of the world model and the agent can have significant impacts on the final results. To further investigate this, we conduct ablation studies on the design and configuration of the world model in Section 5.1, as well as on the agent's design in Section 5.2. Additionally, we propose a novel approach to enhancing the exploration efficiency through the imagination capability of the world model using a single demonstration trajectory, which is explained in Section 5.3.</p>
<h3>5.1 World model design and configuration</h3>
<p>The RNN-based world models utilized in SimPLe [11] and Dreamer [9, 10] can be formulated clearly using variational inference over time. However, the non-recursive Transformer-based world model does not align with this practice and requires manual design. Figure 3a shows alternative structures and their respective outcomes. In the "Decoder at rear" configuration, we employ $z_{t} \sim \overline{\mathcal{Z}}<em t="t">{t}$ instead of $z</em>} \sim \overline{\mathcal{Z}<em t="t">{t}$ for reconstructing the original observation and calculating the loss. The results indicate that the reconstruction loss should be applied directly to the output of the encoder rather than relying on the sequence model. In the "Predictor at front" setup, we utilize $z</em>$. These findings indicate that, while this operation has minimal impact on the final performance for tasks where the reward can be accurately predicted from a single frame (in e.g., Pong), it leads to a performance drop on tasks that require several contextual frames to predict the reward accurately (in e.g., Ms. Pacman).
By default, we configure our Transformer with 2 layers, which is significantly smaller than the 10 layers used in IRIS [13] and TWM [12]. Figure 3b presents the varied outcomes obtained by increasing the number of Transformer layers. The results reveal that increasing the layer count does not have a positive impact on the final performance. However, in the case of the game Pong, even when the sample limit is increased from 100 k to 400 k , the agent still achieves the maximum reward in this environment regardless of whether a 4-layer or 6-layer Transformer is employed. This scaling discrepancy, which differs from the success observed in other fields [36-38], may be attributed to three reasons. Firstly, due to the minor difference between adjacent frames and the presence of residual connections in the Transformer structure [17], predicting the next frame may not require a complex model. Secondly, training a large model naturally requires a substantial amount of data, yet the Atari 100k games neither provide images from diverse domains nor offer sufficient samples for training a larger model. Thirdly, the world model is trained end-to-end, and the representation loss $\mathcal{L}^{\text {rep }}$ in Equation (5b) directly influences the image encoder. The encoder may be overly influenced when tracking the output of a large sequence model.
}$ as input for $g_{o}^{R}(\cdot)$ and $g_{o}^{C}(\cdot)$ in Equation (2), instead of $h_{t<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Ablation studies on the design and configuration of the STORM's world model.</p>
<h3>5.2 Selection of the agent's state</h3>
<p>The choice of the agent's state $s_{t}$ offers several viable options: $\hat{o}<em t="t">{t}$ [13], $h</em>$ in environments that evolve with the agent's policy,}, z_{t}$ (as in TWM [12]), or the combination $\left[h_{t}, z_{t}\right]$ (as demonstrated by Dreamer, [9, 10]). In the case of STORM, we employ $s_{t}=\left[h_{t}, z_{t}\right]$, as in Equation (6). Ablation studies investigating the selection of the agent's state are presented in Figure 4. The results indicate that, in environments where a good policy requires contextual information, such as in Ms. Pacman, the inclusion of $h_{t}$ leads to improved performance. However, in other environments like Pong and Kung Fu Master, this inclusion does not yield a significant difference. When solely utilizing $h_{t</p>
<p>like Pong, the agent may exhibit behaviors similar to catastrophic forgetting [39] due to the nonstationary and inaccurate nature of the world model. Consequently, the introduction of randomness through certain distributions like $\mathcal{Z}_{t}$ proves to be beneficial.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Ablation studies on the selection of the agent's state.</p>
<h1>5.3 Impact of the demonstration trajectory</h1>
<p>The inclusion of a demonstration trajectory is a straightforward implementation step when using a world model, and it is often feasible in real-world settings. Figure 5 showcases the impact of incorporating a single demonstration trajectory in the replay buffer. Details about the provided trajectory can be found in Appendix D. In environments with sparse rewards, adding a trajectory can improve the robustness, as observed in Pong, or the performance, as seen in Freeway. However, in environments with dense rewards like Ms Pacman, including a trajectory may hinder the policy improvement of the agent.
Freeway serves as a prototypical environment characterized by challenging exploration but a simple policy. To receive a reward, the agent must take the "up" action approximately 70 times in a row, but it quickly improves its policy once the first reward is obtained. Achieving the first reward is extremely challenging if the policy is initially set as a uniform distribution of actions. In the case of TWM [12], the entropy normalization technique is employed across all environments, while IRIS [13] specifically reduces the temperature of the Boltzmann exploration strategy for Freeway. These tricks are critical in obtaining the first reward in this environment. It is worth noting that even for most humans, playing exploration-intensive games without prior knowledge is challenging. Typically, human players require instructions about the game's objectives or watch demonstrations from teaching-level or expert players to gain initial exploration directions. Inspired by this observation, we aim to directly incorporate a demonstration trajectory to train the world model and establish starting points for imagination. By leveraging a sufficiently robust world model, the utilization of limited offline information holds the potential to surpass specially designed curiosity-driven exploration strategies in the future.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Ablations studies on adding a demonstration trajectory to the replay buffer.
As an integral part of our methodology, we integrate a single trajectory from Freeway into our extensive results. Furthermore, to ensure fair comparisons in future research, we provide the results without incorporating Freeway's trajectory in Table 2 and Figure 6. It is important to highlight that even without this trajectory, our approach consistently outperforms previous methods, attaining a mean human normalized score of $122.3 \%$.</p>
<h1>6 Conclusions and limitations</h1>
<p>In this work, we introduce STORM, an efficient world model architecture for model-based RL, surpassing previous methods in terms of both performance and training efficiency. STORM harnesses the powerful sequence modeling and generation capabilities of the Transformer structure while fully exploiting its parallelizable training advantages. The improved efficiency of STORM broadens its applicability across a wider range of tasks while reducing computational costs.
Nevertheless, it is important to acknowledge certain limitations. Firstly, both the world model of STORM and the compared baselines are trained in an end-to-end fashion, where the image encoder and sequence model undergo joint optimization. As a result, the world model must predict its own internal output, introducing additional non-stationarity into the optimization process and potentially impeding the scalability of the world model. Secondly, the starting points for imagination are uniformly sampled from the replay buffer, while the agent is optimized using an on-policy actor-critic algorithm. Although acting in the world model is performed on-policy, the corresponding on-policy distribution $\mu(s)$ for these starting points is not explicitly considered, despite its significance in the policy gradient formulation: $\nabla J(\theta) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi_{\theta}}(s, a) \nabla \pi_{\theta}(a \mid s)$ [5].</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>We would like to thank anonymous reviewers for their constructive comments. The work was supported partially by the National Key R\&amp;D Program of China under Grant 2021YFB1714800, partially by the National Natural Science Foundation of China under Grants 62173034, 61925303, 62088101, and partially by the Chongqing Natural Science Foundation under Grant 2021ZX4100027.</p>
<h2>References</h2>
<p>[1] Jie Chen, Jian Sun, and Gang Wang. From unmanned systems to autonomous intelligent systems. Engineering, 12:16-19, 2022.
[2] Boxi Weng, Jian Sun, Gao Huang, Fang Deng, Gang Wang, and Jie Chen. Competitive meta-learning. IEEE/CAA Journal of Automatica Sinica, 10(9):1902-1904, 2023.
[3] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[5] Richard S Sutton, Andrew G Barto, et al. Introduction to Reinforcement Learning, volume 135. MIT Press Cambridge, 1998.
[6] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pages 5639-5650. PMLR, 2020.
[7] Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering Atari games with limited data. Advances in Neural Information Processing Systems, 34:25476-25488, 2021.
[8] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=S1l0TC4tDS.
[9] Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0oabwyZb0u.
[10] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.</p>
<p>[11] Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for atari. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=S1xCPJHtDB.
[12] Jan Robine, Marc Höftmann, Tobias Uelwer, and Stefan Harmeling. Transformer-based world models are happy with 100k interactions. In The International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=TdBaDGCpjly.
[13] Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample-efficient world models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=vhFu1AcbOxb.
[14] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
[15] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9 (8):1735-1780, 1997.
[16] Kyunghyun Cho, Bart Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.
[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.
[18] Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with transformer world models. arXiv preprint arXiv:2202.09481, 2022.
[19] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in Neural Information Processing Systems, 30, 2017.
[20] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836-6846, 2021.
[21] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, 2019.
[22] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in Neural Information Processing Systems, 34:15084-15097, 2021.
[23] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering Atari, Go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.
[24] Yixuan Mei, Jiaxuan Gao, Weirui Ye, Shaohuai Liu, Yang Gao, and Yi Wu. Speedyzero: Mastering atari with limited data and time. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Mg5CLXZgvLJ.
[25] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=uCQfPZwRaUu.
[26] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=GY6-6sTvGaf.</p>
<p>[27] Hado P Van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement learning? Advances in Neural Information Processing Systems, 32, 2019.
[28] Pierluca D’Oro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc G Bellemare, and Aaron Courville. Sample-efficient reinforcement learning by breaking the replay ratio barrier. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=OpC-9aBBVJe.
[29] Max Schwarzer, Johan Samir Obando Ceron, Aaron Courville, Marc G Bellemare, Rishabh Agarwal, and Pablo Samuel Castro. Bigger, better, faster: Human-level atari with human-level efficiency. In International Conference on Machine Learning, pages 30365-30380. PMLR, 2023.
[30] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
[31] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.
[32] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.
[33] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551, 1989.
[34] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
[35] Carol Chen. Transformer inference arithmetic. kipp.ly, 2022. URL https://kipp.ly/blog/ transformer-inference-arithmetic/.
[36] OpenAI. ChatGPT. https://openai.com/chat-gpt/, 2021. Accessed: April 19, 2023.
[37] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $770-778,2016$.
[39] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109-165. Elsevier, 1989.
[40] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages $448-456$. pmlr, 2015.
[41] Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional networks. In 2010 IEEE Computer Society Conference on computer vision and pattern recognition, pages 2528-2535. IEEE, 2010.
[42] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[43] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.</p>
<p>[44] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[45] Florin Gogianu, Tudor Berariu, Lucian Bușoniu, and Elena Burceanu. Atari agents, 2022. URL https://github.com/floringogianu/atari-agents.</p>
<h1>A Atari 100k curves</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Performance comparison on the Atari 100k benchmark. Our method is represented in blue, while DreamerV3 [10] is in orange. The solid line represents the average result over 5 seeds, and the filled area indicates the range between the maximum and minimum results across these 5 seeds.</p>
<h1>B Details of model structure</h1>
<p>Table 3: Structure of the image encoder. The size of the submodules is omitted and can be derived from the shape of the tensors. ReLU refers to the rectified linear units used for activation, while Linear represents a fully-connected layer. Flatten and Reshape operations are employed to alter the indexing method of the tensor while preserving the data and their original order. Conv denotes a CNN layer [33], characterized by kernel $=4$, stride $=2$, and padding $=1$. BN denotes the batch normalization layer[40].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Submodule</th>
<th style="text-align: center;">Output tensor shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input image $\left(o_{t}\right)$</td>
<td style="text-align: center;">$3 \times 64 \times 64$</td>
</tr>
<tr>
<td style="text-align: center;">Conv1 + BN1 + ReLU</td>
<td style="text-align: center;">$32 \times 32 \times 32$</td>
</tr>
<tr>
<td style="text-align: center;">Conv2 + BN2 + ReLU</td>
<td style="text-align: center;">$64 \times 16 \times 16$</td>
</tr>
<tr>
<td style="text-align: center;">Conv3 + BN3 + ReLU</td>
<td style="text-align: center;">$128 \times 8 \times 8$</td>
</tr>
<tr>
<td style="text-align: center;">Conv4 + BN4 + ReLU</td>
<td style="text-align: center;">$256 \times 4 \times 4$</td>
</tr>
<tr>
<td style="text-align: center;">Flatten</td>
<td style="text-align: center;">4096</td>
</tr>
<tr>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Reshape (produce $\left.\mathcal{Z}_{t}\right)$</td>
<td style="text-align: center;">$32 \times 32$</td>
</tr>
</tbody>
</table>
<p>Table 4: Structure of the image decoder. DeConv denotes a transpose CNN layer [41], characterized by kernel $=4$, stride $=2$, and padding $=1$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Submodule</th>
<th style="text-align: center;">Output tensor shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random sample $\left(z_{t}\right)$</td>
<td style="text-align: center;">$32 \times 32$</td>
</tr>
<tr>
<td style="text-align: center;">Flatten</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Linear + BN0 + ReLU</td>
<td style="text-align: center;">4096</td>
</tr>
<tr>
<td style="text-align: center;">Reshape</td>
<td style="text-align: center;">$256 \times 4 \times 4$</td>
</tr>
<tr>
<td style="text-align: center;">DeConv1 + BN1 + ReLU</td>
<td style="text-align: center;">$128 \times 8 \times 8$</td>
</tr>
<tr>
<td style="text-align: center;">DeConv2 + BN2 + ReLU</td>
<td style="text-align: center;">$64 \times 16 \times 16$</td>
</tr>
<tr>
<td style="text-align: center;">DeConv3 + BN3 + ReLU</td>
<td style="text-align: center;">$32 \times 32 \times 32$</td>
</tr>
<tr>
<td style="text-align: center;">DeConv4 (produce $\hat{o}_{t}$ )</td>
<td style="text-align: center;">$3 \times 64 \times 64$</td>
</tr>
</tbody>
</table>
<p>Table 5: Action mixer $e_{t}=m_{\phi}\left(z_{t}, a_{t}\right)$. Concatenate denotes combining the last dimension of two tensors and merging them into one new tensor. The variable $A$ represents the action dimension, which ranges from 3 to 18 across different games. $D$ denotes the feature dimension of the Transformer. LN is an abbreviation for layer normalization [42].</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Submodule</th>
<th style="text-align: center;">Output tensor shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random sample $\left(z_{t}\right)$, Action $\left(a_{t}\right)$</td>
<td style="text-align: center;">$32 \times 32, A$</td>
</tr>
<tr>
<td style="text-align: left;">Reshape and concatenate</td>
<td style="text-align: center;">$1024+A$</td>
</tr>
<tr>
<td style="text-align: left;">Linear1 + LN1 + ReLU</td>
<td style="text-align: center;">$D$</td>
</tr>
<tr>
<td style="text-align: left;">Linear2 + LN2 (output $e_{t}$ )</td>
<td style="text-align: center;">$D$</td>
</tr>
</tbody>
</table>
<p>Table 6: Positional encoding module. $w_{1: T}$ is a learnable parameter matrix with shape $T \times D$, and $T$ refers to the sequence length.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Submodule</th>
<th style="text-align: center;">Output tensor shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input $\left(e_{1: T}\right)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Add $\left(e_{1: T}+w_{1: T}\right)$</td>
<td style="text-align: center;">$T \times D$</td>
</tr>
<tr>
<td style="text-align: center;">LN</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Transformer block. Dropout mechanism [43] can prevent overfitting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Submodule</th>
<th style="text-align: center;">Module alias</th>
<th style="text-align: center;">Output tensor shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input features (label as $x_{1}$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$T \times D$</td>
</tr>
<tr>
<td style="text-align: center;">Multi-head self attention</td>
<td style="text-align: center;">MHSA</td>
<td style="text-align: center;">$T \times D$</td>
</tr>
<tr>
<td style="text-align: center;">Linear1 + Dropout( $p$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Residual (add $x_{1}$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LN1 (label as $x_{2}$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Linear2 + ReLU</td>
<td style="text-align: center;">FFN</td>
<td style="text-align: center;">$T \times 2 D$</td>
</tr>
<tr>
<td style="text-align: center;">Linear3 + Dropout( $p$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$T \times D$</td>
</tr>
<tr>
<td style="text-align: center;">Residual (add $x_{2}$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$T \times D$</td>
</tr>
<tr>
<td style="text-align: center;">LN2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$T \times D$</td>
</tr>
</tbody>
</table>
<p>Table 8: Transformer based sequence model $h_{1: T}=f_{\phi}\left(e_{1: T}\right)$. Positional encoding is explained in Table 6 and Transformer block is explained in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Submodule</th>
<th style="text-align: center;">Output tensor shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input $\left(e_{1: T}\right)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Positional encoding</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Transformer blocks $\times K$</td>
<td style="text-align: center;">$T \times D$</td>
</tr>
<tr>
<td style="text-align: center;">Output $\left(h_{1: T}\right)$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 9: Pure MLP structures. A 1-layer MLP corresponds to a fully-connected layer. 255 is the size of the bucket of symlog two-hot loss [10].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Module name</th>
<th style="text-align: center;">Symbol</th>
<th style="text-align: center;">MLP layers</th>
<th style="text-align: center;">Input/ MLP hidden/ Output dimension</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dynamics head</td>
<td style="text-align: center;">$g_{\phi}^{D}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">D/ -/ 1024</td>
</tr>
<tr>
<td style="text-align: center;">Reward predictor</td>
<td style="text-align: center;">$g_{\phi}^{R}$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">D/ D/ 255</td>
</tr>
<tr>
<td style="text-align: center;">Continuation predictor</td>
<td style="text-align: center;">$g_{\phi}^{C}$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">D/ D/ 1</td>
</tr>
<tr>
<td style="text-align: center;">Policy network</td>
<td style="text-align: center;">$\pi_{\theta}\left(a_{t} \mid s_{t}\right)$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">D/ D/ A</td>
</tr>
<tr>
<td style="text-align: center;">Critic network</td>
<td style="text-align: center;">$V_{\psi}\left(s_{t}\right)$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">D/ D/ 255</td>
</tr>
</tbody>
</table>
<h1>C Hyperparameters</h1>
<p>Table 10: Hyerparameters. Note that the environment will provide a "done" signal when losing a life, but will continue running until the actual reset occurs. This life information configuration aligns with the setup used in IRIS [13]. Regarding data sampling, each time we sample $B_{1}$ trajectories of length $T$ for world model training, and sample $B_{2}$ trajectories of length $C$ for starting the imagination process.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Symbol</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Transformer layers</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Transformer feature dimension</td>
<td style="text-align: center;">$D$</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">Transformer heads</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Dropout probability</td>
<td style="text-align: center;">$p$</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">World model training batch size</td>
<td style="text-align: center;">$B_{1}$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">World model training batch length</td>
<td style="text-align: center;">$T$</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">Imagination batch size</td>
<td style="text-align: center;">$B_{2}$</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Imagination context length</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Imagination horizon</td>
<td style="text-align: center;">$L$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Update world model every env step</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Update agent every env step</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Environment context length</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Gamma</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">0.985</td>
</tr>
<tr>
<td style="text-align: center;">Lambda</td>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: center;">Entropy coefficiency</td>
<td style="text-align: center;">$\eta$</td>
<td style="text-align: center;">$3 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">Critic EMA decay</td>
<td style="text-align: center;">$\sigma$</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Adam [44]</td>
</tr>
<tr>
<td style="text-align: center;">World model learning rate</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$1.0 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">World model gradient clipping</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">Actor-critic learning rate</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$3.0 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">Actor-critic gradient clipping</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Gray scale input</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">False</td>
</tr>
<tr>
<td style="text-align: center;">Frame stacking</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">False</td>
</tr>
<tr>
<td style="text-align: center;">Frame skipping</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4 (max over last 2 frames)</td>
</tr>
<tr>
<td style="text-align: center;">Use of life information</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">True</td>
</tr>
</tbody>
</table>
<h1>D Demonstration trajectory information</h1>
<p>Table 11: To account for frame skipping, the frame count is multiplied by 4 . These trajectories were gathered using pre-trained DQN agents [45].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Game</th>
<th style="text-align: center;">Episode return</th>
<th style="text-align: center;">Frames</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Ms Pacman</td>
<td style="text-align: center;">5860</td>
<td style="text-align: center;">$1612 \times 4$</td>
</tr>
<tr>
<td style="text-align: center;">Pong</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$2079 \times 4$</td>
</tr>
<tr>
<td style="text-align: center;">Freeway</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">$2048 \times 4$</td>
</tr>
</tbody>
</table>
<h1>E Computational cost details and comparison</h1>
<p>Table 12: Computational comparison. In the V100 column, an item marked with a star indicates extrapolation based on other graphics cards, while items without a star are tested using actual devices. The extrapolation method employed aligns with the setup used in DreamerV3 [10], where it assumes the P100 is twice as slow and the A100 is twice as fast.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Original computing resource</th>
<th style="text-align: center;">V100 hours</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SimPLe [11]</td>
<td style="text-align: center;">NVIDIA P100, 20 days</td>
<td style="text-align: center;">$240^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">TWM [12]</td>
<td style="text-align: center;">NVIDIA A100, 10 hours <br> NVIDIA GeForce RTX 3090, 12.5 hours</td>
<td style="text-align: center;">$20^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">IRIS [13]</td>
<td style="text-align: center;">NVIDIA A100, 7 days for two runs</td>
<td style="text-align: center;">$168^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">DreamerV3 [10]</td>
<td style="text-align: center;">NVIDIA V100, 12 hours</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">STORM</td>
<td style="text-align: center;">NVIDIA GeForce RTX 3090, 4.3 hours</td>
<td style="text-align: center;">9.3</td>
</tr>
</tbody>
</table>
<h1>F Atari video predictions</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Multi-step predictions on several environments in Atari games. The world model utilizes 8 observations and actions as contextual input, enabling the imagination of future events spanning 56 frames in an auto-regressive manner.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
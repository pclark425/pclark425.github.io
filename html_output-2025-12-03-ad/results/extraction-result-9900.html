<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9900 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9900</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9900</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-dccd3899dd16beec8adcc97b65f6e24e7927d19b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dccd3899dd16beec8adcc97b65f6e24e7927d19b" target="_blank">ProcessBench: Identifying Process Errors in Mathematical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning, and draws two main observations: existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH, and the best open-source model has demonstrated the critique capability competitive with the proprietary model GPT-4o.</p>
                <p><strong>Paper Abstract:</strong> As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9900.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9900.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROCESSBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROCESSBENCH: Identifying Process Errors in Mathematical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-annotated benchmark of 3,400 step-by-step model-generated solutions (from diverse LLMs) for measuring the ability to identify the earliest erroneous step in mathematical reasoning; includes four subsets (GSM8K, MATH, OlympiadBench, Omni-MATH) spanning increasing difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (Qwen2/Qwen2.5 series, Llama-3 series, QwQ-32B-Preview, GPT-4o, o1-mini, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Diverse set of open-source and proprietary LLMs used as solution generators and as critiquing models; sizes range from ~1.5B to 72B and include specialized reasoning variants (e.g., Qwen2.5-Math, QwQ-32B) and proprietary GPT/O1 models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematical reasoning / formal problem solving (mathematics)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Task: given a math problem and a step-wise solution, models must return the index of the earliest erroneous step (or -1 if no error). Ground truth created by multi-expert human annotation; evaluation runs both (a) process reward models that give per-step correctness/scores and (b) prompted general LLMs used as critic models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-sample: correctness index. Aggregate metrics: accuracy on samples that are annotated as containing process errors ('error' accuracy), accuracy on samples annotated as correct ('correct' accuracy), and their harmonic mean (reported as 'F1' in the paper). PRM scalar outputs are binarized via a threshold (threshold chosen on GSM8K or per-subset for upper bound).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>PROCESSBENCH — 3,400 test cases across GSM8K (grade-school), MATH, OlympiadBench, Omni-MATH (competition/Olympiad level); solutions generated by 12 distinct LLMs and reformatted into coherent step granularity; each solution annotated by >=3 doctoral-level mathematicians (up to 5) with consensus procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key aggregated results (F1, averaged across four subsets): best open-source critic QwQ-32B-Preview average F1 = 71.5; proprietary GPT-4o average F1 = 61.9; o1-mini average F1 = 87.9 (best). Best PRM (fine-tuned by authors: Qwen2.5-Math-7B-PRM800K) average F1 = 56.5; other open-source PRMs generally underperform critics. Performance declines consistently with problem difficulty (GSM8K -> MATH -> OlympiadBench -> Omni-MATH).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human annotation is difficult for Olympiad-level problems (higher annotator disagreement and ~30% discard rate); labels may still contain inaccuracies for the hardest problems; dataset excludes some extremely difficult solutions that annotators could not reach consensus on, possibly biasing distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Ground truth is human expert annotation (doctoral-level mathematicians using reference solutions). PROCESSBENCH focuses on earliest-error identification rather than final-answer correctness used in more traditional evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use earliest-error index as the task, report accuracy on erroneous and correct samples separately and their harmonic mean (F1) to avoid bias toward over/under-criticism; generate solution diversity from multiple LLM families; apply rigorous multi-expert annotation with consensus and discard samples lacking consensus; standardize step granularity (automatic reformatting + verify final answer unchanged).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProcessBench: Identifying Process Errors in Mathematical Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9900.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9900.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process Reward Models (PRMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models trained to assess the correctness of intermediate reasoning steps, producing per-step binary correctness predictions or scalar scores to supervise the reasoning process of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Let's verify step by step.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>family: Math-Shepherd, RLHFlow PRMs (Mistral/Deepseek), Skywork PRMs, Qwen2.5-Math-7B-PRM800K (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Open-source PRMs of various sizes (1.5B–8B) and training regimes; some estimate empirical probability that a given step leads to correct final answer (on-policy estimation), while others are fine-tuned on human-labeled PRM800K to predict per-step correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematical reasoning / internal process supervision</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>PRMs output per-step correctness labels or scalar scores; earliest erroneous step extracted from the first predicted incorrect step. For scalar scores, a threshold (selected on GSM8K or per-subset) converts scores to binary correctness before extracting earliest error.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same as PROCESSBENCH: accuracy on erroneous samples, accuracy on correct samples, harmonic mean (F1). For PRMs producing scores: threshold tuning to maximize F1 on GSM8K (or per-subset in upper-bound analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on PROCESSBENCH (GSM8K, MATH, OlympiadBench, Omni-MATH). Training references include PRM800K dataset and synthetic/on-policy generation approaches (e.g., Math-Shepherd methodology).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Existing PRMs (Math-Shepherd, RLHFlow, Skywork variants) often underperform prompt-driven critic LLMs. The authors' Qwen2.5-Math-7B-PRM800K (fine-tuned on human-annotated PRM800K) outperforms other open-source PRMs: average F1 = 56.5, with subset F1s: GSM8K 68.2, MATH 62.6, OlympiadBench 50.7, Omni-MATH 44.3. Other PRMs have much lower performance on harder subsets (e.g., many PRMs drop below ~25 F1 on Olympiad/Omni-MATH).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Two major issues: (1) 'On-policy' label estimation ties process labels to the generator model, hurting generalization across generators; (2) current LLMs can reach correct final answers despite process errors, invalidating empirical label estimation methods. PRMs trained on synthetic or on-policy data struggled to generalize to more difficult problems and diverse generators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>PRMs are automated approximations of human per-step judgments; when trained on human-labeled PRM800K they improve but still lag behind large prompted critic models, especially on hardest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer human-annotated training data (e.g., PRM800K) for PRM fine-tuning to improve generalization; evaluate PRMs directly on per-step error-identification metrics (not only Best-of-N final-answer improvements); tune scalar-to-binary thresholds on held-out subsets but be aware of limited cross-domain transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProcessBench: Identifying Process Errors in Mathematical Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9900.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9900.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic Models (prompted LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-driven Critic Models (general LLMs repurposed as critics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose LLMs are prompted to critique solutions paragraph-by-paragraph and return the index of the earliest erroneous step; inference uses prompt template and optionally majority voting over multiple samplings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Criticbench: Benchmarking llms for critique-correct reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>QwQ-32B-Preview, Qwen2.5 variants (7B/14B/32B/72B), Llama-3 series, GPT-4o, o1-mini, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Open-source and proprietary LLMs spanning a range of sizes and specializations (math/codec/reasoning-tuned), used in zero-shot or few-shot prompting to produce step-wise critiques; some models evaluated under majority voting (8 samplings) or greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematical reasoning critique / verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt template (Figure 7) asks model to inspect paragraphs and return earliest error index in a boxed format; evaluation modes include majority voting over 8 stochastic samplings (for open-source models) and greedy decoding (reported separately); models evaluated by extracting returned index and comparing to human-annotated ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same metrics: erroneous-sample accuracy, correct-sample accuracy, harmonic mean (F1). Additionally, sampling/decoding settings (p, t, k) and voting affect performance and are reported in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on PROCESSBENCH; prompt template and decoding settings (majority voting with k=20 for some generation; majority voting over 8 samples for open-source critics) documented.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Critic models generally outperform many PRMs. Top open-source critic QwQ-32B-Preview achieved average F1 = 71.5 (subset F1s: GSM8K 88.0, MATH 78.7, OlympiadBench 57.8, Omni-MATH 61.3). Proprietary GPT-4o average F1 = 61.9 (GSM8K 79.2, MATH 63.6, Olympiad 51.4, Omni 53.5). Leading reasoning-specialized o1-mini outperforms all with average F1 = 87.9.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance drops as problem difficulty increases; prompt and decoding choices (greedy vs sampling+majority) materially affect results; critic models can still lag on very hard Olympiad-level problems relative to the best specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Critic LLMs can approach or exceed some human-like critique abilities (in automated earliest-error identification) but ground-truth remains human annotations; critics provide more detailed explanations than PRMs but must be evaluated against human-labeled indices.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use explicit paragraph-index prompt template, consider majority voting over multiple samples for stability, report both error/correct accuracies and their F1 to balance false positives and negatives, and include greedy-decoding baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProcessBench: Identifying Process Errors in Mathematical Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9900.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9900.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-Math-7B-PRM800K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-Math-7B-PRM800K (fine-tuned PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors' PRM trained by fine-tuning Qwen2.5-Math-7B-Instruct on the human-annotated PRM800K dataset with a binary reward head predicting per-step correctness at paragraph break positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Qwen2.5-Math-7B-PRM800K</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Qwen2.5-Math-7B Instruct model modified with a reward modeling head for binary classification at paragraph breakpoints; fine-tuned on PRM800K with contaminated-sample removal; trained on eight A100 80GB GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematical reasoning / process supervision</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Model outputs binary correctness logits per paragraph; earliest erroneous step extracted from first negative prediction; evaluated on PROCESSBENCH with same metrics (error/correct accuracies and F1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy on erroneous samples, accuracy on correct samples, harmonic mean (F1).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Trained on PRM800K (human-annotated) with PRM800K training samples overlapping PROCESSBENCH removed; evaluated on PROCESSBENCH's four subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>This PRM achieved the strongest PRM results reported: average F1 = 56.5 with subset F1s: GSM8K 68.2, MATH 62.6, OlympiadBench 50.7, Omni-MATH 44.3 — outperforming other open-source PRMs but still below top critic models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although improved via human-labeled PRM800K, it still underperforms the best prompted critic models on average and declines on hardest subsets; training requires high-quality human-labeled PRM data and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Fine-tuning on human labels improves PRM alignment with expert judgments versus synthetic/on-policy label methods, but lacks the flexible reasoning chain generation of critic LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use fully human-annotated PRM data for PRM fine-tuning to improve generalization; remove overlapping test problems to avoid contamination; evaluate PRMs directly on per-step error detection metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProcessBench: Identifying Process Errors in Mathematical Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9900.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9900.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Annotation Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert Multi-annotator Protocol for Earliest-Error Labeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Annotation pipeline using doctoral-level mathematicians, required training and proficiency tests, reference solutions provided, 3 annotators per sample minimum with escalation up to 5 to reach 3-agreement; samples without 3-agreement within 5 annotators are discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>human experts (doctoral-level mathematicians)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Human annotators (doctoral-level) who passed a proficiency exam and tutorial; used as ground truth labelers rather than LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematical reasoning annotation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Each model-generated, reformatted solution assigned to 3 experts; if no 3-way agreement, add annotators until 3 agree or discard after 5 annotators. Annotators identify earliest erroneous paragraph index per the provided task definition and can consult reference solutions/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Agreement statistics tracked: % samples achieving 3/3, 3/4, 3/5 agreement; discard rate reported (~30% overall).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used to create PROCESSBENCH labels across 3,400 test cases from four subsets; reference solutions from original datasets provided to annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Annotation yielded variable agreement by subset and by whether final answer was correct: e.g., % 3/3 agreement ranges from ~47.8% to 95.9% depending on subset and correctness; overall ~30% of candidate solutions discarded due to lack of consensus or other issues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High difficulty of Olympiad-level problems led to more annotator uncertainty and higher numbers of annotators needed or discards, which may bias the final dataset; annotators sometimes hesitant when uncertain of correct solution approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Protocol uses multiple experts and consensus escalation to approximate reliable human ground truth, more rigorous than single-annotator labeling used in some prior synthetic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Provide reference solutions to annotators for difficult instances, require proficiency testing, use multi-annotator consensus with escalation up to a cap, and discard samples lacking reliable consensus to preserve label quality (but track and report discard bias).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProcessBench: Identifying Process Errors in Mathematical Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9900.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9900.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Error-index Accuracy & Harmonic Mean (F1) Metrics for Process Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric suite that separately measures a model's ability to detect erroneous-process samples and to correctly accept correct-process samples, summarized by their harmonic mean (F1) to balance false positives and negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>applies to all evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>N/A (metric set used to evaluate LLMs/PRMs/critics)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology for LLM reasoning/process verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute (a) accuracy on samples annotated as containing an earliest error (i.e., model correctly returns the earliest-error index), (b) accuracy on samples annotated as having no process error, and (c) harmonic mean of the two accuracies reported as 'F1' in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-subset and per-model: error-sample accuracy, correct-sample accuracy, and F1; thresholds for score-to-label conversion are tuned on GSM8K (or per-subset for upper-bound analyses). Reporting includes breakdowns per subset and per model and uses average across subsets as a summary.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on PROCESSBENCH subsets; used to compare PRMs and critic models and to tune scalar thresholds for PRMs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Metric reveals trade-offs: some models are overly critical (high error accuracy but low correct accuracy), others under-critical (opposite); F1 advised as primary comparison metric to balance these behaviors. Example: QwQ-32B-Preview F1s per subset: GSM8K 88.0, MATH 78.7, OlympiadBench 57.8, Omni-MATH 61.3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Harmonic mean (F1) assumes equal weight of error and correct sample accuracies; sampling design (balanced error/correct final-answer sampling) is required to make F1 meaningful. Threshold selection for PRM scalar outputs can inflate per-subset scores if tuned on the same subset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Compared to final-answer-only evaluation (traditional), these metrics provide step-level diagnostic power; human evaluation remains the ground truth for per-step correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report separate error/correct accuracies and their harmonic mean; tune PRM thresholds on held-out data (avoid per-test-subset tuning unless reporting as upper bound); ensure balanced sampling of correct/incorrect final-answer cases to avoid skewed metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProcessBench: Identifying Process Errors in Mathematical Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let's verify step by step. <em>(Rating: 2)</em></li>
                <li>Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. <em>(Rating: 2)</em></li>
                <li>Criticbench: Benchmarking llms for critique-correct reasoning. <em>(Rating: 2)</em></li>
                <li>Is your model really a good math reasoner? evaluating mathematical reasoning with checklist. <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9900",
    "paper_id": "paper-dccd3899dd16beec8adcc97b65f6e24e7927d19b",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "PROCESSBENCH",
            "name_full": "PROCESSBENCH: Identifying Process Errors in Mathematical Reasoning",
            "brief_description": "A human-annotated benchmark of 3,400 step-by-step model-generated solutions (from diverse LLMs) for measuring the ability to identify the earliest erroneous step in mathematical reasoning; includes four subsets (GSM8K, MATH, OlympiadBench, Omni-MATH) spanning increasing difficulty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple (Qwen2/Qwen2.5 series, Llama-3 series, QwQ-32B-Preview, GPT-4o, o1-mini, etc.)",
            "llm_description": "Diverse set of open-source and proprietary LLMs used as solution generators and as critiquing models; sizes range from ~1.5B to 72B and include specialized reasoning variants (e.g., Qwen2.5-Math, QwQ-32B) and proprietary GPT/O1 models.",
            "scientific_domain": "Mathematical reasoning / formal problem solving (mathematics)",
            "evaluation_method": "Task: given a math problem and a step-wise solution, models must return the index of the earliest erroneous step (or -1 if no error). Ground truth created by multi-expert human annotation; evaluation runs both (a) process reward models that give per-step correctness/scores and (b) prompted general LLMs used as critic models.",
            "evaluation_criteria": "Per-sample: correctness index. Aggregate metrics: accuracy on samples that are annotated as containing process errors ('error' accuracy), accuracy on samples annotated as correct ('correct' accuracy), and their harmonic mean (reported as 'F1' in the paper). PRM scalar outputs are binarized via a threshold (threshold chosen on GSM8K or per-subset for upper bound).",
            "benchmark_or_dataset": "PROCESSBENCH — 3,400 test cases across GSM8K (grade-school), MATH, OlympiadBench, Omni-MATH (competition/Olympiad level); solutions generated by 12 distinct LLMs and reformatted into coherent step granularity; each solution annotated by &gt;=3 doctoral-level mathematicians (up to 5) with consensus procedure.",
            "results_summary": "Key aggregated results (F1, averaged across four subsets): best open-source critic QwQ-32B-Preview average F1 = 71.5; proprietary GPT-4o average F1 = 61.9; o1-mini average F1 = 87.9 (best). Best PRM (fine-tuned by authors: Qwen2.5-Math-7B-PRM800K) average F1 = 56.5; other open-source PRMs generally underperform critics. Performance declines consistently with problem difficulty (GSM8K -&gt; MATH -&gt; OlympiadBench -&gt; Omni-MATH).",
            "limitations_or_challenges": "Human annotation is difficult for Olympiad-level problems (higher annotator disagreement and ~30% discard rate); labels may still contain inaccuracies for the hardest problems; dataset excludes some extremely difficult solutions that annotators could not reach consensus on, possibly biasing distribution.",
            "comparison_to_human_or_traditional": "Ground truth is human expert annotation (doctoral-level mathematicians using reference solutions). PROCESSBENCH focuses on earliest-error identification rather than final-answer correctness used in more traditional evaluation.",
            "recommendations_or_best_practices": "Use earliest-error index as the task, report accuracy on erroneous and correct samples separately and their harmonic mean (F1) to avoid bias toward over/under-criticism; generate solution diversity from multiple LLM families; apply rigorous multi-expert annotation with consensus and discard samples lacking consensus; standardize step granularity (automatic reformatting + verify final answer unchanged).",
            "uuid": "e9900.0",
            "source_info": {
                "paper_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "PRM",
            "name_full": "Process Reward Models (PRMs)",
            "brief_description": "Models trained to assess the correctness of intermediate reasoning steps, producing per-step binary correctness predictions or scalar scores to supervise the reasoning process of LLMs.",
            "citation_title": "Let's verify step by step.",
            "mention_or_use": "use",
            "llm_name": "family: Math-Shepherd, RLHFlow PRMs (Mistral/Deepseek), Skywork PRMs, Qwen2.5-Math-7B-PRM800K (fine-tuned)",
            "llm_description": "Open-source PRMs of various sizes (1.5B–8B) and training regimes; some estimate empirical probability that a given step leads to correct final answer (on-policy estimation), while others are fine-tuned on human-labeled PRM800K to predict per-step correctness.",
            "scientific_domain": "Mathematical reasoning / internal process supervision",
            "evaluation_method": "PRMs output per-step correctness labels or scalar scores; earliest erroneous step extracted from the first predicted incorrect step. For scalar scores, a threshold (selected on GSM8K or per-subset) converts scores to binary correctness before extracting earliest error.",
            "evaluation_criteria": "Same as PROCESSBENCH: accuracy on erroneous samples, accuracy on correct samples, harmonic mean (F1). For PRMs producing scores: threshold tuning to maximize F1 on GSM8K (or per-subset in upper-bound analysis).",
            "benchmark_or_dataset": "Evaluated on PROCESSBENCH (GSM8K, MATH, OlympiadBench, Omni-MATH). Training references include PRM800K dataset and synthetic/on-policy generation approaches (e.g., Math-Shepherd methodology).",
            "results_summary": "Existing PRMs (Math-Shepherd, RLHFlow, Skywork variants) often underperform prompt-driven critic LLMs. The authors' Qwen2.5-Math-7B-PRM800K (fine-tuned on human-annotated PRM800K) outperforms other open-source PRMs: average F1 = 56.5, with subset F1s: GSM8K 68.2, MATH 62.6, OlympiadBench 50.7, Omni-MATH 44.3. Other PRMs have much lower performance on harder subsets (e.g., many PRMs drop below ~25 F1 on Olympiad/Omni-MATH).",
            "limitations_or_challenges": "Two major issues: (1) 'On-policy' label estimation ties process labels to the generator model, hurting generalization across generators; (2) current LLMs can reach correct final answers despite process errors, invalidating empirical label estimation methods. PRMs trained on synthetic or on-policy data struggled to generalize to more difficult problems and diverse generators.",
            "comparison_to_human_or_traditional": "PRMs are automated approximations of human per-step judgments; when trained on human-labeled PRM800K they improve but still lag behind large prompted critic models, especially on hardest problems.",
            "recommendations_or_best_practices": "Prefer human-annotated training data (e.g., PRM800K) for PRM fine-tuning to improve generalization; evaluate PRMs directly on per-step error-identification metrics (not only Best-of-N final-answer improvements); tune scalar-to-binary thresholds on held-out subsets but be aware of limited cross-domain transfer.",
            "uuid": "e9900.1",
            "source_info": {
                "paper_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Critic Models (prompted LLMs)",
            "name_full": "Prompt-driven Critic Models (general LLMs repurposed as critics)",
            "brief_description": "General-purpose LLMs are prompted to critique solutions paragraph-by-paragraph and return the index of the earliest erroneous step; inference uses prompt template and optionally majority voting over multiple samplings.",
            "citation_title": "Criticbench: Benchmarking llms for critique-correct reasoning.",
            "mention_or_use": "use",
            "llm_name": "QwQ-32B-Preview, Qwen2.5 variants (7B/14B/32B/72B), Llama-3 series, GPT-4o, o1-mini, etc.",
            "llm_description": "Open-source and proprietary LLMs spanning a range of sizes and specializations (math/codec/reasoning-tuned), used in zero-shot or few-shot prompting to produce step-wise critiques; some models evaluated under majority voting (8 samplings) or greedy decoding.",
            "scientific_domain": "Mathematical reasoning critique / verification",
            "evaluation_method": "Prompt template (Figure 7) asks model to inspect paragraphs and return earliest error index in a boxed format; evaluation modes include majority voting over 8 stochastic samplings (for open-source models) and greedy decoding (reported separately); models evaluated by extracting returned index and comparing to human-annotated ground truth.",
            "evaluation_criteria": "Same metrics: erroneous-sample accuracy, correct-sample accuracy, harmonic mean (F1). Additionally, sampling/decoding settings (p, t, k) and voting affect performance and are reported in appendices.",
            "benchmark_or_dataset": "Evaluated on PROCESSBENCH; prompt template and decoding settings (majority voting with k=20 for some generation; majority voting over 8 samples for open-source critics) documented.",
            "results_summary": "Critic models generally outperform many PRMs. Top open-source critic QwQ-32B-Preview achieved average F1 = 71.5 (subset F1s: GSM8K 88.0, MATH 78.7, OlympiadBench 57.8, Omni-MATH 61.3). Proprietary GPT-4o average F1 = 61.9 (GSM8K 79.2, MATH 63.6, Olympiad 51.4, Omni 53.5). Leading reasoning-specialized o1-mini outperforms all with average F1 = 87.9.",
            "limitations_or_challenges": "Performance drops as problem difficulty increases; prompt and decoding choices (greedy vs sampling+majority) materially affect results; critic models can still lag on very hard Olympiad-level problems relative to the best specialized models.",
            "comparison_to_human_or_traditional": "Critic LLMs can approach or exceed some human-like critique abilities (in automated earliest-error identification) but ground-truth remains human annotations; critics provide more detailed explanations than PRMs but must be evaluated against human-labeled indices.",
            "recommendations_or_best_practices": "Use explicit paragraph-index prompt template, consider majority voting over multiple samples for stability, report both error/correct accuracies and their F1 to balance false positives and negatives, and include greedy-decoding baselines.",
            "uuid": "e9900.2",
            "source_info": {
                "paper_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Qwen2.5-Math-7B-PRM800K",
            "name_full": "Qwen2.5-Math-7B-PRM800K (fine-tuned PRM)",
            "brief_description": "The authors' PRM trained by fine-tuning Qwen2.5-Math-7B-Instruct on the human-annotated PRM800K dataset with a binary reward head predicting per-step correctness at paragraph break positions.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Qwen2.5-Math-7B-PRM800K",
            "llm_description": "Qwen2.5-Math-7B Instruct model modified with a reward modeling head for binary classification at paragraph breakpoints; fine-tuned on PRM800K with contaminated-sample removal; trained on eight A100 80GB GPUs.",
            "scientific_domain": "Mathematical reasoning / process supervision",
            "evaluation_method": "Model outputs binary correctness logits per paragraph; earliest erroneous step extracted from first negative prediction; evaluated on PROCESSBENCH with same metrics (error/correct accuracies and F1).",
            "evaluation_criteria": "Accuracy on erroneous samples, accuracy on correct samples, harmonic mean (F1).",
            "benchmark_or_dataset": "Trained on PRM800K (human-annotated) with PRM800K training samples overlapping PROCESSBENCH removed; evaluated on PROCESSBENCH's four subsets.",
            "results_summary": "This PRM achieved the strongest PRM results reported: average F1 = 56.5 with subset F1s: GSM8K 68.2, MATH 62.6, OlympiadBench 50.7, Omni-MATH 44.3 — outperforming other open-source PRMs but still below top critic models.",
            "limitations_or_challenges": "Although improved via human-labeled PRM800K, it still underperforms the best prompted critic models on average and declines on hardest subsets; training requires high-quality human-labeled PRM data and compute.",
            "comparison_to_human_or_traditional": "Fine-tuning on human labels improves PRM alignment with expert judgments versus synthetic/on-policy label methods, but lacks the flexible reasoning chain generation of critic LLMs.",
            "recommendations_or_best_practices": "Use fully human-annotated PRM data for PRM fine-tuning to improve generalization; remove overlapping test problems to avoid contamination; evaluate PRMs directly on per-step error detection metrics.",
            "uuid": "e9900.3",
            "source_info": {
                "paper_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Annotation Protocol",
            "name_full": "Expert Multi-annotator Protocol for Earliest-Error Labeling",
            "brief_description": "Annotation pipeline using doctoral-level mathematicians, required training and proficiency tests, reference solutions provided, 3 annotators per sample minimum with escalation up to 5 to reach 3-agreement; samples without 3-agreement within 5 annotators are discarded.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "human experts (doctoral-level mathematicians)",
            "llm_description": "Human annotators (doctoral-level) who passed a proficiency exam and tutorial; used as ground truth labelers rather than LLMs.",
            "scientific_domain": "Mathematical reasoning annotation",
            "evaluation_method": "Each model-generated, reformatted solution assigned to 3 experts; if no 3-way agreement, add annotators until 3 agree or discard after 5 annotators. Annotators identify earliest erroneous paragraph index per the provided task definition and can consult reference solutions/answers.",
            "evaluation_criteria": "Agreement statistics tracked: % samples achieving 3/3, 3/4, 3/5 agreement; discard rate reported (~30% overall).",
            "benchmark_or_dataset": "Used to create PROCESSBENCH labels across 3,400 test cases from four subsets; reference solutions from original datasets provided to annotators.",
            "results_summary": "Annotation yielded variable agreement by subset and by whether final answer was correct: e.g., % 3/3 agreement ranges from ~47.8% to 95.9% depending on subset and correctness; overall ~30% of candidate solutions discarded due to lack of consensus or other issues.",
            "limitations_or_challenges": "High difficulty of Olympiad-level problems led to more annotator uncertainty and higher numbers of annotators needed or discards, which may bias the final dataset; annotators sometimes hesitant when uncertain of correct solution approach.",
            "comparison_to_human_or_traditional": "Protocol uses multiple experts and consensus escalation to approximate reliable human ground truth, more rigorous than single-annotator labeling used in some prior synthetic datasets.",
            "recommendations_or_best_practices": "Provide reference solutions to annotators for difficult instances, require proficiency testing, use multi-annotator consensus with escalation up to a cap, and discard samples lacking reliable consensus to preserve label quality (but track and report discard bias).",
            "uuid": "e9900.4",
            "source_info": {
                "paper_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Evaluation Metrics",
            "name_full": "Error-index Accuracy & Harmonic Mean (F1) Metrics for Process Evaluation",
            "brief_description": "A metric suite that separately measures a model's ability to detect erroneous-process samples and to correctly accept correct-process samples, summarized by their harmonic mean (F1) to balance false positives and negatives.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "applies to all evaluated LLMs",
            "llm_description": "N/A (metric set used to evaluate LLMs/PRMs/critics)",
            "scientific_domain": "Evaluation methodology for LLM reasoning/process verification",
            "evaluation_method": "Compute (a) accuracy on samples annotated as containing an earliest error (i.e., model correctly returns the earliest-error index), (b) accuracy on samples annotated as having no process error, and (c) harmonic mean of the two accuracies reported as 'F1' in the paper.",
            "evaluation_criteria": "Per-subset and per-model: error-sample accuracy, correct-sample accuracy, and F1; thresholds for score-to-label conversion are tuned on GSM8K (or per-subset for upper-bound analyses). Reporting includes breakdowns per subset and per model and uses average across subsets as a summary.",
            "benchmark_or_dataset": "Applied on PROCESSBENCH subsets; used to compare PRMs and critic models and to tune scalar thresholds for PRMs.",
            "results_summary": "Metric reveals trade-offs: some models are overly critical (high error accuracy but low correct accuracy), others under-critical (opposite); F1 advised as primary comparison metric to balance these behaviors. Example: QwQ-32B-Preview F1s per subset: GSM8K 88.0, MATH 78.7, OlympiadBench 57.8, Omni-MATH 61.3.",
            "limitations_or_challenges": "Harmonic mean (F1) assumes equal weight of error and correct sample accuracies; sampling design (balanced error/correct final-answer sampling) is required to make F1 meaningful. Threshold selection for PRM scalar outputs can inflate per-subset scores if tuned on the same subset.",
            "comparison_to_human_or_traditional": "Compared to final-answer-only evaluation (traditional), these metrics provide step-level diagnostic power; human evaluation remains the ground truth for per-step correctness.",
            "recommendations_or_best_practices": "Report separate error/correct accuracies and their harmonic mean; tune PRM thresholds on held-out data (avoid per-test-subset tuning unless reporting as upper bound); ensure balanced sampling of correct/incorrect final-answer cases to avoid skewed metrics.",
            "uuid": "e9900.5",
            "source_info": {
                "paper_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let's verify step by step.",
            "rating": 2
        },
        {
            "paper_title": "Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations.",
            "rating": 2
        },
        {
            "paper_title": "Criticbench: Benchmarking llms for critique-correct reasoning.",
            "rating": 2
        },
        {
            "paper_title": "Is your model really a good math reasoner? evaluating mathematical reasoning with checklist.",
            "rating": 1
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 1
        }
    ],
    "cost": 0.017875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PROCESSBENCH: Identifying Process Errors in Mathematical Reasoning</h1>
<p>Chujie Zheng<em> Zhenru Zhang Beichen Zhang Runji Lin Keming Lu<br>Bowen Yu</em> Dayiheng Liu<em> Jingren Zhou Junyang Lin</em><br>Qwen Team, Alibaba Inc.<br>https://huggingface.co/datasets/Qwen/ProcessBench<br>0 https://github.com/QwenLM/ProcessBench</p>
<h4>Abstract</h4>
<p>As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcesSBENCH for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcesSBENCH, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcesSBENCH can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of evaluation results on ProcesSBench (see Table 3 for details).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>In recent years, language models have made remarkable progress in complex reasoning tasks, such as mathematics and programming (Hurst et al., 2024; OpenAI, 2024; Yang et al., 2024a;b; Dubey et al., 2024; Wake et al., 2024), yet they still make mistakes when solving challenging problems. To achieve scalable oversight (Amodei et al., 2016; Bowman et al., 2022; Cao et al., 2024), i.e., effectively supervising AI systems that get close to or go beyond broadly human-level performance, particularly in complex tasks that are difficult for general humans, we expect language models can identify errors in their reasoning process in an automated way. However, existing benchmarks related to assessing language models' reasoning process may be hard to satisfy the growing evaluation demand for the error identification ability. Either their covered problems have become less challenging for recent language models (Zhou et al., 2024; Lightman et al., 2023), or they merely label the correctness of final answers but lack annotations for specific erroneous steps (Lin et al., 2024).</p>
<div class="codehilite"><pre><span></span><code><span class="s">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;math-22&quot;</span><span class="p">,</span>
<span class="s">&quot;generator&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Qwen2-7B-Instruct&quot;</span><span class="p">,</span>
<span class="s">&quot;problem&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Find the integer 8n,$ 80 \\le n \\le 180,$ such that $\\cos n^\\circ = \\cos</span>
<span class="s">568^\\circ.$&quot;</span><span class="p">,</span>
<span class="s">&quot;steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s">&quot;To find the integer \\(n\\), \\(0 \\leq n \\leq 180\\), such that \\(\\cos n^\\circ = \\cos</span>
<span class="s">568^\\circ\\), we can use the periodic properties of the cosine function. First, recall that</span>
<span class="s">    the cosine function has a period of \\(360^\\circ\\). This means that: \\[ \\cos \\theta =</span>
<span class="s">\\cos (\\theta + 360k) \\] for any integer \\(k\\).&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;Given \\(\\cos 568^\\circ\\), we need to reduce this angle to an equivalent angle within the</span>
<span class="s">    first cycle of \\(0\\) to \\(360^\\circ\\). We do this by subtracting multiples of</span>
<span class="s">    \\(360^\\circ\\) until we get an angle within this range: \\[ 568^\\circ - 360^\\circ =</span>
<span class="s">    208^\\circ \\] So, \\(\\cos 568^\\circ = \\cos 208^\\circ\\).&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;However, we want to find \\(n\\) such that \\(0 \\leq n \\leq 180\\). Since cosine is also</span>
<span class="s">    symmetric about \\(180^\\circ\\), we know that: \\[ \\cos \\theta = \\cos (180^\\circ -</span>
<span class="s">    \\theta) \\] Therefore, \\(\\cos 208^\\circ = \\cos (180^\\circ - 208^\\circ))\,, which</span>
<span class="s">    simplifies to: \\[ \\cos 208^\\circ = \\cos (-28^\\circ) \\]&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;The cosine function is also even, meaning it is symmetric about the y-axis: \\[ \\cos (-</span>
<span class="s">\\theta) = \\cos \\theta \\] Thus, \\[ \\cos (-28^\\circ) = \\cos 28^\\circ \\]&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">    </span><span class="s">&quot;So, \\(n = 28^\\circ\\). Hence, the integer \\(n\\), \\(0 \\leq n \\leq 180\\), such that</span>
<span class="s">    \\(\\cos n^\\circ = \\cos 568^\\circ\\) is \\(n = 28\\). The answer is \\(\\boxed(28)\\).&quot;</span>
<span class="p">],</span>
<span class="s">&quot;final_answer_correct&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="s">&quot;label&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span>
</code></pre></div>

<p>Figure 2: Data example of ProcesSBench. The label 2 denotes that the earliest error occurs in the 2nd step (indexed from 0). For test cases with no errors, the labels are -1 .</p>
<p>In this paper, we introduce ProcesSBench for measuring the ability to identify erroneous steps in mathematical reasoning. Figure 2 presents a data example. We prioritize several principles when designing this benchmark:</p>
<ul>
<li>Problem difficulty and solution diversity. ProcesSBench primarily covers competition- and Olympiad-level math problems and utilizes various open-source language models to generate solutions. This ensures both the difficulty of math problems and the diversity of solution styles, enabling robust evaluation.</li>
<li>Scale and accuracy. ProcesSBench consists of 3,400 test cases, with all solutions annotated with error locations by multiple human experts. The large scale and expert annotation ensure the data quality and the reliability of evaluation.</li>
<li>Simplicity. ProcesSBench requires models to identify the earliest erroneous step occurring in the solution, if any exists. This straightforward evaluation protocol enables easy adaptation for various types of models, such as process reward models (PRMs) and critic models.</li>
</ul>
<p>We conduct extensive evaluation on ProcesSBench, involving two types of models: process reward models (PRMs) and critic models. For PRMs, we include multiple open-source PRMs (Wang et al., 2024; Skywork, 2024; Xiong et al., 2024b) to assess the correctness of each reasoning step in the solution. For critic models, we prompt general language models like Qwen (Yang et al., 2024a; Qwen, 2024a; Hui et al., 2024) and GPT-4o (Hurst et al., 2024) to critique each solution step by step. We show that, despite</p>
<p>recent growing interest, existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset, which raises questions about the generalization abilities and scalability of the current data synthesis methodologies used to build PRMs. In contrast, general language models manifest non-trivial critique capabilities that can not only identify erroneous steps but also provide detailed explanations. The best open-source model, QwQ-32B-Preview (Qwen, 2024b), has performed competitively with the proprietary GPT-4o model, while it still lags behind the reasoning-specialized o1-mini (OpenAI, 2024). We hope ProcesSBench can catalyze future research in automated reasoning process assessment, establishing crucial foundations for scalable oversight of language models.</p>
<h1>2 Related Work</h1>
<p>There exist several benchmarks or datasets related to assessing language models' reasoning process. CriticBench (Lin et al., 2024) evaluates language models' abilities to critique solutions and correct mistakes in various reasoning tasks. MathCheck (Zhou et al., 2024) synthesizes solutions containing erroneous steps using the GSM8K dataset (Cobbe et al., 2021), in which language models are tasked with judging the correctness of final answers or reasoning steps. PRM800K (Lightman et al., 2023) builds on the MATH problems (Hendrycks et al., 2021) and annotates the correctness and soundness of reasoning steps in model-generated solutions. It also has sparked a blooming of research interest in building process reward models (PRMs) (Wang et al., 2024; Xiong et al., 2024b;a).</p>
<p>Table 1: Comparison between ProcesSBench and other benchmarks or datasets related to reasoning process assessment (Lin et al., 2024; Zhou et al., 2024; Lightman et al., 2023). ${ }^{1}$ : Solution diversity denotes the diversity of language models used for solution generation, corresponding to the "# Solution Generators" column. ${ }^{2}$ : For PRM800K, we only count the 90 complete solutions in its phase 1 test set, as the complete solutions in its phase 2 test set are all terminated at the earliest erroneous steps.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Problem <br> Diffculty</th>
<th style="text-align: center;"># Solution <br> Generators</th>
<th style="text-align: center;">Solution <br> Diversity ${ }^{\dagger}$</th>
<th style="text-align: center;">Step <br> Annotation?</th>
<th style="text-align: center;">Annotator</th>
<th style="text-align: center;">Test Case Size <br> (Identifying <br> Process Errors)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CriticBench</td>
<td style="text-align: center;">$\star \star$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$\star \star \star$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MathCheck-GSM</td>
<td style="text-align: center;">$\star$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\star$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Synthetic</td>
<td style="text-align: center;">516</td>
</tr>
<tr>
<td style="text-align: left;">PRM800K</td>
<td style="text-align: center;">$\star \star$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\star$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">$90^{\ddagger}$</td>
</tr>
<tr>
<td style="text-align: left;">ProcesSBench</td>
<td style="text-align: center;">$\star \star \star$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$\star \star \star$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">3,400</td>
</tr>
</tbody>
</table>
<p>ProcesSBench is distinguished from prior benchmarks or datasets in three key aspects, as highlighted in Table 1. First, ProcesSBench primarily covers more challenging math problems with competition- or Olympiad-level difficulty, which better fit the rapidly growing capabilities of modern language models. Second, rather than relying on synthetic data, ProcesSBench leverages diverse model-generated natural solutions and employs expert annotation to label erroneous steps, which ensures both real-world applicability and label accuracy. Third, the large scale of ProcesSBench ( 3,400 test cases in total) enables more comprehensive and robust evaluation.</p>
<p>There has also been extensive research on language models' scalable oversight (Amodei et al., 2016; Bowman et al., 2022; Cao et al., 2024) and studies on whether language models can identify the errors in their own outputs. Lightman et al. (2023); Wang et al. (2024); Luo et al. (2024) propose to train specialized reward models to supervise language models' reasoning process (i.e., process reward models or PRMs). Huang et al. (2023); Kamoi et al. (2024) argue that general language models struggle to identify and correct their reasoning errors without external feedback. Saunders et al. (2022); McAleese et al. (2024) show that language models can be trained to write informative critiques for both assisting human evaluation and enabling self-refinement, which favorably scales with increased model capabilities (or model sizes). We believe the improved capabilities of error identification will build strong foundations for language models' scalable oversight.</p>
<h2>3 Benchmark Construction</h2>
<h3>3.1 Task Definition</h3>
<p>As shown in Figure 2, given a math problem and a step-by-step solution, ProcesSBench requires models to either identify the earliest-occurring error, or conclude that all steps are correct. Formally,</p>
<p>given a math problem $P$ and its step-by-step solution $S=\left{s_{0}, \ldots, s_{n-1}\right}$, the task is to output an index $i \in{-1,0, \ldots, n-1}$, where $i=-1$ indicates that all steps are correct, and $i \geq 0$ indicates that the earliest error occurs at step $s_{i}$.
Typically but non-inclusively, we consider a step as erroneous if it contains any of the following: (1) Mathematical errors: incorrect calculations, algebraic manipulations, or formula applications. (2) Logical errors: invalid deductions, unwarranted assumptions, or flawed reasoning steps. (3) Conceptual errors: misunderstanding or misapplication of mathematical or problem concepts. (4) Completeness errors: missing crucial conditions, constraints, or necessary justifications that affect the solution's validity. Beyond these types of errors, we encourage human annotators to determine the correctness of reasoning steps based on their own expertise. We do not require human annotators to explicitly annotate error types due to the intractability of intentional categorization.
Note that for steps after the first error, the meaning of their correctness may become ambiguous or debatable: derivations based on incorrect premises can make sense, but still remain on a globally incorrect reasoning path (Lightman et al., 2023). For instance, if step $k$ contains an error in calculating $x=2$, when it should be $x=3$, subsequent steps may follow valid algebraic rules but operate on an incorrect value of $x$, making their individual correctness hard to determine. This is why ProcesSBench focuses on identifying the earliest-occurring error in the reasoning process.</p>
<h1>3.2 Data Collection</h1>
<p>Problem Curation We collect math problems from the test sets of four public and widely used datasets in mathematical reasoning tasks: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), OlympiadBench (He et al., 2024), and Omni-MATH (Gao et al., 2024). Except for GSM8K, which consists of grade school math problems, the other three datasets all contain problems with competition- or Olympiad-level difficulty.</p>
<p>Solution Generation We generate solutions using the widely used Qwen (Yang et al., 2024a; Qwen, 2024a; Yang et al., 2024b) and LLaMA (Dubey et al., 2024) series open-source models, resulting in twelve distinct solution generators in total. This includes a wide range of model families, sizes, and downstream task performance, leading to the high diversity of solution styles. Table 4 in Appendix B presents the breakdown of language models used for ProcesSBench's solution generation.</p>
<p>Solution Reformatting In mathematical reasoning tasks, double line breaks (i.e., " $\backslash \mathrm{n} \backslash \mathrm{n}$ ") are commonly used to segment solution steps (or paragraphs). However, we observed inconsistent step granularity due to varying solution styles and generation randomness. Some generated solutions frequently used double line breaks, resulting in numerous short, logically incomplete steps, while others used them sparingly, leading to lengthy paragraphs that combine multiple logical components. Such inconsistency in step granularity (and potential improper step segmentation) would impede the standardization of human annotation criteria.
To address this issue, we adopt a solution reformatting method to standardize the step granularity, through which the segmented paragraphs can better correspond to logically complete and progressive reasoning steps. Specifically, we first replace all the line breaks with white space, and then ask Qwen2.5-72BInstruct to insert double line breaks (i.e., segment paragraphs) while preserving the solution content. Since we found that Qwen2.5-72B-Instruct sometimes alters the solution content ( $&lt;0.5 \%$ ), we remove those solutions whose final answers change after reformatting (although the content alteration may not influence benchmark construction). Consequently, the reformatting method effectively unifies the step granularity. Figure 6 in Appendix A presents an example of solution reformatting.</p>
<p>Expert Annotation To ensure a balance between erroneous and correct solutions, we first use Qwen2.5-72B-Instruct to verify the correctness of final answers in the model-generated solutions against the reference answers. We then respectively sample solutions with correct or incorrect final answers for subsequent annotation in a balanced way to avoid excessive concentration on solutions from either the weakest or strongest models.
We recruit human experts with doctoral-level mathematical expertise for annotation, and all of them are required to pass the mandatory proficiency examination and annotation tutorial. The annotators are designated with the same task in $\S 3.1$, i.e., identifying the earliest-occurring error in each solution. However, we notice that the competition- or Olympiad-level math problems can still be challenging even for doctoral students majoring in mathematics. According to the feedback from the annotators, although they were not required to solve problems from scratch but rather to identify erroneous steps in presented solutions, they would still become quite hesitant in their annotations when uncertain about the correct solution approach, which affected both the annotation speed and quality. To ease the annotation difficulty,</p>
<p>we provide annotators with the reference solutions and answers from the original datasets, while we still explicitly instructed them to inspect and verify the presented model-generated solutions step by step.
Each solution is initially assigned to three different experts. When the initial three annotators cannot reach consensus, we increase the number of annotators until three of them agree on the same result. If an agreement cannot be achieved within five annotators (e.g., annotation distribution of $(2,2,1)$ or $(2,1,1,1)$ ), we discard this solution. This leads to an overall $\sim 30 \%$ discard rate throughout the entire annotation process. We also discard the solutions where the final answers are incorrect (according to the reference answers) but the human annotation results are correct. Although such cases are fairly rare $(&lt;1 \%)$, they are mostly concentrated in the OlympiadBench and Omni-MATH problems (i.e., Olympiad-level ones). The agreement statistics in Table 2 further evidence that the more challenging problems usually need more annotators to achieve the annotation agreement, particularly for those samples with incorrect final answers. These results suggest the inherent challenge of our human annotation task.</p>
<h1>3.3 Statistics</h1>
<p>Table 2: Statistics of ProcesSBench. " $\%$ Process errors" denotes the proportion of samples with erroneous reasoning steps (i.e., annotated as erroneous) among all the samples with correct final answers. " $\% \geq n$ steps" denotes the proportion of samples whose solutions have $\geq n$ steps (split by double line breaks). " $\% 3 / n$ agreement" denotes the proportion of samples where the three-annotator agreement is achieved within $n$ annotators, so $(\% 3 / 3)+(\% 3 / 4)+(\% 3 / 5)=100 \%$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OlympiadBench</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Omni-MATH</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
</tr>
<tr>
<td style="text-align: center;"># Samples</td>
<td style="text-align: center;">207</td>
<td style="text-align: center;">193</td>
<td style="text-align: center;">594</td>
<td style="text-align: center;">406</td>
<td style="text-align: center;">661</td>
<td style="text-align: center;">339</td>
<td style="text-align: center;">759</td>
<td style="text-align: center;">241</td>
</tr>
<tr>
<td style="text-align: center;">\% Process errors (correct final answers)</td>
<td style="text-align: center;">$\frac{200-193}{200}=3.5 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\frac{500-406}{500}=18.8 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\frac{500-339}{500}=32.2 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\frac{500-241}{500}=51.8 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Steps</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">7.4</td>
</tr>
<tr>
<td style="text-align: center;">$\% \geq 5$ steps</td>
<td style="text-align: center;">$61.8 \%$</td>
<td style="text-align: center;">$57.5 \%$</td>
<td style="text-align: center;">$73.6 \%$</td>
<td style="text-align: center;">$70.4 \%$</td>
<td style="text-align: center;">$92.6 \%$</td>
<td style="text-align: center;">$92.3 \%$</td>
<td style="text-align: center;">$92.5 \%$</td>
<td style="text-align: center;">$81.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\% \geq 10$ steps</td>
<td style="text-align: center;">$3.4 \%$</td>
<td style="text-align: center;">$1.6 \%$</td>
<td style="text-align: center;">$17.8 \%$</td>
<td style="text-align: center;">$8.9 \%$</td>
<td style="text-align: center;">$33.9 \%$</td>
<td style="text-align: center;">$27.1 \%$</td>
<td style="text-align: center;">$29.2 \%$</td>
<td style="text-align: center;">$21.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\% \geq 15$ steps</td>
<td style="text-align: center;">$0.5 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$3.4 \%$</td>
<td style="text-align: center;">$2.0 \%$</td>
<td style="text-align: center;">$9.1 \%$</td>
<td style="text-align: center;">$8.8 \%$</td>
<td style="text-align: center;">$7.5 \%$</td>
<td style="text-align: center;">$4.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">\% 3/3 agreement</td>
<td style="text-align: center;">$66.7 \%$</td>
<td style="text-align: center;">$95.9 \%$</td>
<td style="text-align: center;">$59.4 \%$</td>
<td style="text-align: center;">$91.9 \%$</td>
<td style="text-align: center;">$52.8 \%$</td>
<td style="text-align: center;">$85.0 \%$</td>
<td style="text-align: center;">$47.8 \%$</td>
<td style="text-align: center;">$80.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">\% 3/4 agreement</td>
<td style="text-align: center;">$21.3 \%$</td>
<td style="text-align: center;">$3.6 \%$</td>
<td style="text-align: center;">$24.4 \%$</td>
<td style="text-align: center;">$4.7 \%$</td>
<td style="text-align: center;">$24.1 \%$</td>
<td style="text-align: center;">$9.1 \%$</td>
<td style="text-align: center;">$25.6 \%$</td>
<td style="text-align: center;">$13.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;">\% 3/5 agreement</td>
<td style="text-align: center;">$12.1 \%$</td>
<td style="text-align: center;">$0.5 \%$</td>
<td style="text-align: center;">$16.2 \%$</td>
<td style="text-align: center;">$3.4 \%$</td>
<td style="text-align: center;">$23.1 \%$</td>
<td style="text-align: center;">$5.9 \%$</td>
<td style="text-align: center;">$26.6 \%$</td>
<td style="text-align: center;">$6.2 \%$</td>
</tr>
</tbody>
</table>
<p>The resulting ProcesSBench has four subsets, consisting of 3,400 test cases in total. The detailed statistics are shown in Table 2 and Table 4 (in Appendix B), and we also plot in Figure 3 the distribution of error positions in erroneous samples. In general, the more challenging the problems, the more solution steps the models generate, and incorrect solutions usually contain more steps than correct ones. However, across all four subsets, a large proportion of errors occur in the earlier steps, such as steps 0-3 in GSM8K and MATH, and steps 1-5 in OlympiadBench and Omni-MATH.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Distribution of error positions (indexed from 0 ; truncated to 16 for better visualization), corresponding to the label field as shown in Figure 2.</p>
<p>It is noteworthy that while we have intentionally controlled an equal number of solutions with incorrect and correct final answers ( 200 each for GSM8K and 500 each for other subsets), the annotation</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Process error ratios per models and subsets, computed as the proportions of samples annotated as <em>erroneous</em> among all the samples with <em>correct final answers</em> (same as in Table 2). The models used for solution generation slightly vary across different subsets, see Table 4 in Appendix B. We observe that no particular models have notably higher process error rates, while the process error rates are consistently higher on more difficult problems for all the models.</p>
<p>Results reveal quite different numbers. Specifically, in the more challenging subsets like OlympiadBench and Omni-MATH, a larger proportion of solutions with correct final answers still contain erroneous steps. For instance, in OlympiadBench, $\frac{500-339}{500} = 32.2\%$ of solutions with correct final answers are found to contain process errors, while in Omni-MATH this proportion is even higher ($\frac{500-241}{500} = 51.8\%$). In contrast, these proportions in GSM8K and MATH are $\frac{200-193}{500} = 3.5\%$ and $\frac{500-406}{500} = 18.8\%$, respectively. In Figure 4, for each model used for solution generation, we plot the ratio of samples with <em>erroneous reasoning steps</em> (i.e., annotated as erroneous) among all the samples with <em>correct final answers</em>. We observe that the process error rates are consistently higher on more difficult problems. To our knowledge, our work is the first to present evidence that on more challenging math problems, current language models are more prone to making process errors even when reaching correct final answers. This also suggests the underlying limitation of rule-based RL in mathematical reasoning (i.e., rewarding merely according to the correctness of final answers) and further highlights the significance of identifying errors in the reasoning process.</p>
<h2>4 Evaluation</h2>
<h3>4.1 Setup</h3>
<p>For each subset of PROCESSBENCH, we calculate the accuracies on erroneous and correct samples, respectively, and additionally compute their harmonic mean as the F1 score. We primarily refer to F1 scores to compare model performance, as it balances model behaviors between being overly critical and being incapable of identifying errors.</p>
<p>We consider two types of models in the evaluation on PROCESSBENCH: <em>process reward models (PRMs)</em> and <em>critic models</em>.</p>
<p><strong>Process Reward Models (PRMs)</strong> As a recently focal topic, PRMs are proposed to assess and supervise the intermediate steps in language models' reasoning process (Lightman et al., 2023), thus naturally falling in the scope of our research. In practice, PRMs are typically trained using the process labels for intermediate reasoning steps, outputting either the correctness prediction or a scalar score for each reasoning step during inference. Previous research usually evaluates PRMs based on their improvement in the Best-of-N (BoN) performance of another language model that generates solutions. However, this lacks a finer-grained inspection on their process assessment abilities, and the evaluation reliability can be heavily affected by the underlying solution generation model.</p>
<p>Our evaluation includes several open-source PRMs: (1) Math-Shepherd (Wang et al., 2024), which obtains the process label for each step via estimating the empirical probability of this step leading to the correct final answer. (2) Two LLaMA-3.1-based PRMs from Xiong et al. (2024b), which roughly follow the training methodology of Math-Shepherd but differ in the solution generation models and optimization objectives. (3) Two Qwen2.5-Math-based PRMs recently released by Skywork (2024). (4) We also train a PRM by fine-tuning Qwen2.5-Math-7B-Instruct on the PRM800K dataset, namely Qwen2.5-Math-7B-PRM800K. See Appendix C for its training details.</p>
<p>For the (1)(2)(4) PRMs, we extract the earliest erroneous step from their correctness predictions for reasoning steps. For the (3) PRMs, which produce scalar scores for each reasoning step, we first transform these scores into binary correctness predictions (using a threshold above which steps are considered as correct), and then extract the earliest erroneous step as we do for (1)(2)(4). The transformation threshold is determined as the one giving the highest F1 score on the GSM8K subset.</p>
<p>Critic Models Critic models aim to provide feedback and critique to model-generated texts, noninclusively including verification, reflection, and correction or refinement. They have demonstrated promising utility in achieving scalable oversight (Saunders et al., 2022; McAleese et al., 2024).
Training critic models for specific domains typically requires significant and specialized effort, which is out of the scope of our work. Instead, we are more interested in the critique capabilities of general language models. The task definition (§ 3.1) of ProcesSBench enables us to apply simple prompt engineering to repurpose general language models as critic models. We show in Figure 7 in Appendix E the prompt template we implement for our evaluation. Specifically, models are prompted to return the index of the paragraph where the earliest error occurs as the final answer, similar to the conventional evaluation protocol for mathematical reasoning tasks (Cobbe et al., 2021; Hendrycks et al., 2021; Yang et al., 2024b).
Our evaluation includes the widely-used Qwen2 (Yang et al., 2024a), Qwen2.5 (Qwen, 2024a), Qwen2.5Math (Yang et al., 2024b), Qwen2.5-Coder (Hui et al., 2024), and LLaMA-3 (Dubey et al., 2024) series open-source models, as well as the recently released QwQ-32B-Preview reasoning model (Qwen, 2024b). We also evaluate the proprietary GPT-4o (Hurst et al., 2024) and o1-mini (OpenAI, 2024) models. We report the performance of open-source models under majority voting over eight samplings, while we also report their performance under greedy decoding in Table 9 in Appendix F. For the proprietary model GPT-4o, we report the results under greedy decoding, while for o1-mini, we report the results under single sampling as its API does not support customized decoding parameters.</p>
<p>Table 3: Evaluation results on ProcesSBench. We report the F1 score of the respective accuracies on erroneous and correct samples. See Table 5 and Table 7 for breakdown of evaluation results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">Olympiad- <br> Bench</th>
<th style="text-align: center;">Omni- <br> MATH</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Open-source Process Reward Models (PRMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Math-Shepherd-PRM-7B</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">31.5</td>
</tr>
<tr>
<td style="text-align: left;">RLHFlow-PRM-Mistral-8B</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">28.4</td>
</tr>
<tr>
<td style="text-align: left;">RLHFlow-PRM-Deepseek-8B</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">26.6</td>
</tr>
<tr>
<td style="text-align: left;">Skywork-PRM-1.5B</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: left;">Skywork-PRM-7B</td>
<td style="text-align: center;">$\mathbf{7 0 . 8}$</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">42.1</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B-PRM800K (our trained)</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">$\mathbf{6 2 . 6}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 7}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 3}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Open-source language models, prompted as Critic Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Meta-Llama-3-8B-Instruct</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1-8B-Instruct</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">5.1</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1-70B-Instruct</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.3-70B-Instruct</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B-Instruct</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">19.9</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">45.5</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Coder-7B-Instruct</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">6.7</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Coder-14B-Instruct</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Coder-32B-Instruct</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">56.1</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">13.6</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-72B-Instruct</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">49.8</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-7B-Instruct</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-14B-Instruct</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-32B-Instruct</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">49.3</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-72B-Instruct</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">61.2</td>
</tr>
<tr>
<td style="text-align: left;">QwQ-32B-Preview</td>
<td style="text-align: center;">$\mathbf{8 8 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Proprietary language models, prompted as Critic Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o-0806</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">61.9</td>
</tr>
<tr>
<td style="text-align: left;">o1-mini</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">87.9</td>
</tr>
</tbody>
</table>
<div class="codehilite"><pre><span></span><code><span class="nt">Alright</span><span class="o">,</span><span class="w"> </span><span class="nt">I</span><span class="s1">&#39;ve got this math problem to solve and a solution provided, but I need to review it</span>
<span class="s1">paragraph by paragraph to find any errors. The problem is to find the integer \(n\) , where \(0\)</span>
<span class="s1">\leq n \leq 180\, such that \( \cos n^\circ = \cos 568^\circ\). Let&#39;</span><span class="nt">s</span><span class="w"> </span><span class="nt">dive</span><span class="w"> </span><span class="nt">into</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">solution</span><span class="w"> </span><span class="nt">step</span>
<span class="nt">by</span><span class="w"> </span><span class="nt">step</span><span class="o">.</span>
<span class="o">**</span><span class="nt">Paragraph</span><span class="w"> </span><span class="nt">0</span><span class="o">:**</span>
<span class="nt">The</span><span class="w"> </span><span class="nt">solution</span><span class="w"> </span><span class="nt">starts</span><span class="w"> </span><span class="nt">by</span><span class="w"> </span><span class="nt">recalling</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">cosine</span><span class="w"> </span><span class="nt">function</span><span class="w"> </span><span class="nt">has</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">period</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">360</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">which</span>
<span class="nt">means</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nt">360k</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">any</span><span class="w"> </span><span class="nt">integer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="err">\</span><span class="o">).</span><span class="w"> </span><span class="nt">That</span><span class="s1">&#39;s correct. Cosine repeats</span>
<span class="s1">every \(360^\circ\), so adding or subtracting multiples of \(360^\circ\) doesn&#39;</span><span class="nt">t</span><span class="w"> </span><span class="nt">change</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">value</span>
<span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">cosine</span><span class="o">.</span>
<span class="o">**</span><span class="nt">Paragraph</span><span class="w"> </span><span class="nt">1</span><span class="o">:**</span>
<span class="nt">Given</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="nt">568</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">solution</span><span class="w"> </span><span class="nt">reduces</span><span class="w"> </span><span class="nt">this</span><span class="w"> </span><span class="nt">angle</span><span class="w"> </span><span class="nt">by</span><span class="w"> </span><span class="nt">subtracting</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">360</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">get</span>
<span class="err">\</span><span class="o">(</span><span class="nt">208</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">since</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">568</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="nt">360</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">208</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">).</span><span class="w"> </span><span class="nt">So</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="nt">568</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span>
<span class="nt">208</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">).</span><span class="w"> </span><span class="nt">That</span><span class="w"> </span><span class="nt">seems</span><span class="w"> </span><span class="nt">right</span><span class="o">.</span><span class="w"> </span><span class="nt">208</span><span class="w"> </span><span class="nt">degrees</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">within</span><span class="w"> </span><span class="nt">one</span><span class="w"> </span><span class="nt">cycle</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">0</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">360</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">).</span>
<span class="o">**</span><span class="nt">Paragraph</span><span class="w"> </span><span class="nt">2</span><span class="o">:**</span>
<span class="nt">Now</span><span class="o">,</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">solution</span><span class="w"> </span><span class="nt">mentions</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">cosine</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">symmetric</span><span class="w"> </span><span class="nt">about</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">180</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">states</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span>
<span class="err">\</span><span class="nt">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="o">(</span><span class="nt">180</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">)</span><span class="err">\</span><span class="o">).</span><span class="w"> </span><span class="nt">Wait</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">minute</span><span class="o">,</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">correct</span><span class="o">?</span><span class="w"> </span><span class="nt">Let</span><span class="w"> </span><span class="nt">me</span><span class="w"> </span><span class="nt">think</span><span class="w"> </span><span class="nt">about</span><span class="w"> </span><span class="nt">this</span><span class="o">.</span>
<span class="nt">Actually</span><span class="o">,</span><span class="w"> </span><span class="nt">cosine</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">symmetric</span><span class="w"> </span><span class="nt">about</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">y-axis</span><span class="o">,</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">identity</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="o">(</span><span class="nt">-</span><span class="err">\</span><span class="nt">theta</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="nt">reflects</span><span class="w"> </span><span class="nt">that</span><span class="o">.</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">identity</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="o">(</span><span class="nt">360</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">shows</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">symmetry</span><span class="w"> </span><span class="nt">about</span>
<span class="err">\</span><span class="o">(</span><span class="nt">360</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">but</span><span class="w"> </span><span class="nt">symmetry</span><span class="w"> </span><span class="nt">about</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">180</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">isn</span><span class="s1">&#39;t directly related to cosine&#39;</span><span class="nt">s</span><span class="w"> </span><span class="nt">even</span><span class="w"> </span><span class="nt">property</span><span class="o">.</span>
<span class="nt">Hmm</span><span class="o">,</span><span class="w"> </span><span class="nt">maybe</span><span class="w"> </span><span class="nt">I</span><span class="w"> </span><span class="nt">need</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">recall</span><span class="w"> </span><span class="nt">some</span><span class="w"> </span><span class="nt">trigonometric</span><span class="w"> </span><span class="nt">identities</span><span class="w"> </span><span class="nt">here</span><span class="o">.</span><span class="w"> </span><span class="nt">Let</span><span class="s1">&#39;s see, the cosine of an angle</span>
<span class="s1">in standard position is equal to the cosine of its reference angle, but with a sign depending on</span>
<span class="s1">the quadrant. 208 degrees is in the third quadrant, where cosine is negative.</span>
<span class="s1">Alternatively, there&#39;</span><span class="nt">s</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">identity</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="o">(</span><span class="nt">180</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">-</span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">but</span><span class="w"> </span><span class="nt">that</span><span class="s1">&#39;s not</span>
<span class="s1">directly applicable here.</span>
<span class="s1">Let me double-check the symmetry properties. Cosine is an even function, meaning \( \cos(-\theta)</span>
<span class="s1">- \cos \theta), which shows symmetry about the y-axis. The identity \( \cos (360^\circ - \theta)</span>
<span class="s1">\cos \theta\) shows symmetry every full cycle.</span>
<span class="s1">But the statement \( \cos \theta = \cos (180^\circ - \theta)\) isn&#39;</span><span class="nt">t</span><span class="w"> </span><span class="nt">generally</span><span class="w"> </span><span class="nt">true</span><span class="o">.</span><span class="w"> </span><span class="nt">Let</span><span class="s1">&#39;s test it</span>
<span class="s1">with a specific angle. For example, \( \theta = 30^\circ): \( \cos 30^\circ = \sqrt{3}/2\), and</span>
<span class="s1">\( \cos (180^\circ - 30^\circ) = \cos 150^\circ = -\sqrt{3}/2\). These are not equal; in fact,</span>
<span class="s1">they&#39;</span><span class="nt">re</span><span class="w"> </span><span class="nt">negatives</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">other</span><span class="o">.</span>
<span class="nt">So</span><span class="o">,</span><span class="w"> </span><span class="nt">it</span><span class="w"> </span><span class="nt">seems</span><span class="w"> </span><span class="nt">there</span><span class="err">&#39;</span><span class="nt">s</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">error</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">this</span><span class="w"> </span><span class="nt">paragraph</span><span class="o">.</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">correct</span><span class="w"> </span><span class="nt">identity</span><span class="w"> </span><span class="nt">involving</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">180</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">is</span>
<span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="o">(</span><span class="nt">180</span><span class="o">^</span><span class="err">\</span><span class="nt">circ</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">-</span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="nt">cos</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="o">).</span>
<span class="nt">Therefore</span><span class="o">,</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">earliest</span><span class="w"> </span><span class="nt">error</span><span class="w"> </span><span class="nt">occurs</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">paragraph</span><span class="w"> </span><span class="nt">2</span><span class="o">.</span>
<span class="o">**</span><span class="nt">Final</span><span class="w"> </span><span class="nt">Answer</span><span class="o">**</span>
</code></pre></div>

<p>Figure 5: Critique generated by QwQ-32B-Preview for the test case in Figure 2.</p>
<h1>4.2 Results</h1>
<p>We present the evaluation results in Table 3. Our observations are summarized as follows:
Generalization Across Difficulty From GSM8K and MATH to OlympiadBench and Omni-MATH, with the increased difficulty of math problems, we observe a consistent performance decline for all the models, which suggests the common challenge of both PRMs and critic models in generalization abilities.</p>
<p>Comparison Between PRMs and Critic Models We find that existing PRMs typically underperform the top prompt-driven critic models even on the simpler GSM8K and MATH subsets, suggesting that these PRMs struggle to indicate the correctness of the intermediate steps in mathematical reasoning. Moreover, when moving toward the more challenging OlympiadBench and Omni-MATH subsets, PRMs suffer from a more notable performance decline than critic models. This raises our concerns about the generalization abilities and scalability of the current data synthesis methodologies used to build PRMs. More specifically, current methodologies, as exemplified by Math-Shepherd (Wang et al., 2024), measure the correctness of an intermediate step by estimating the empirical probability of this step leading to the correct final answer. This kind of approach has two intuitive major issues: (1) The process labels heavily depend on the language model used to generate solutions (i.e., highly "on-policy"), which</p>
<p>would naturally fail to indicate the correctness of reasoning steps generated by other models. (2) As demonstrated in $\S 3.3$, current language models are prone to making process errors even when reaching correct final answers. This could substantially invalidate the estimated process labels, particularly on the more challenging math problems. In contrast, Qwen2.5-Math-7B-PRM800K, which is straightforwardly fine-tuned on the fully human-annotated PRM800K training set, exhibits significantly stronger performance and generalization ability than other PRMs.</p>
<p>Comparison Among Critic Models Compared to PRMs, critic models can benefit from separate reasoning processes when critiquing solutions, as they can "think" more before indicating the correctness of each solution step, which leads to their better performance in this error identification task. Within the same model family, the error identification performance favorably scales with increased model sizes. Notably, the recently released reasoning model QwQ-32B-Preview performs best among the open-source models and is highly competitive with GPT-4o. It is noteworthy that QwQ-32B-Preview achieves more balanced accuracies on erroneous and correct samples (see Table 5 and 7 in Appendix F). We show in Figure 5 an example of critique generated by QwQ-32B-Preview to the test case in Figure 2, which not only identifies the erroneous step but also provides the detailed thinking process and explanation. Nevertheless, QwQ-32B-Preview still lags behind ol-mini, suggesting that although the gap in problem-solving performance is getting closer between open-source and proprietary models, there still exists another large gap in their critique capabilities.</p>
<h1>5 Conclusion</h1>
<p>We introduce the ProCESSBENCH benchmark for measuring the ability to identify erroneous steps in mathematical reasoning, characterized by its high problem difficulty and solution diversity, large scale, rigorous human annotation, and simple evaluation protocol. Through extensive evaluation with existing process reward models (PRMs) and prompt-driven critic models, we draw two main observations: (1) Existing PRMs typically underperform critic models in identifying erroneous reasoning steps, and struggle more to generalize to challenging math problems. (2) Open-source language models, as exemplified by QwQ-32B-Preview, have demonstrated critique capabilities competitive with the proprietary model GPT-4o, yet still lag behind the reasoning-specialized ol-mini model. We envision ProCESSBENCH as a cornerstone testbed for advancing automated reasoning process assessment, a critical step toward achieving scalable oversight of language models.</p>
<p>Limitations Despite our best efforts throughout the entire benchmark construction process (§ 3.2), ProCESSBENCH may still contain inaccurate labels of error locations, particularly for the more challenging Olympiad-level math problems. Additionally, the solutions discarded in human annotation (§ 3.2 may involve the particularly challenging problems, which could bias the problem distribution in ProCESSBENCH, although such samples may have exceeded the capabilities of the human annotators in our annotation task.</p>
<h2>References</h2>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.</p>
<p>Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilė Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022.</p>
<p>Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, et al. Towards scalable automated alignment of llms: A survey. arXiv preprint arXiv:2406.01252, 2024.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.</p>
<p>Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: A universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024.</p>
<p>Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.</p>
<p>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186, 2024.
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.</p>
<p>Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can LLMs actually correct their own mistakes? a critical survey of self-correction of LLMs. Transactions of the Association for Computational Linguistics, 12:1417-1440, 2024. doi: 10.1162/tacl.a.00713. URL https://aclanthology. org/2024.tacl-1.78.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626, 2023.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809, 2024.
Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024.
Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024.
OpenAI. Openai o1-mini: Advancing cost-efficient reasoning, 2024. URL https://openai.com/index/ openai-o1-mini-advancing-cost-efficient-reasoning/.
Team Qwen. Qwen2.5: A party of foundation models, September 2024a. URL https://qwenlm.github. io/blog/qwen2.5/.
Team Qwen. Qwq: Reflect deeply on the boundaries of the unknown, November 2024b. URL https: //qwenlm.github.io/blog/qwq-32b-preview/.
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.
o1 Team Skywork. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork.
Alan Wake, Albert Wang, Bei Chen, CX Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Ethan Dai, et al. Yi-lightning technical report. arXiv preprint arXiv:2412.01253, 2024.
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426-9439, August 2024. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/2024. acl-long. 510 .
Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, et al. Building math agents with multi-turn iterative preference learning. arXiv preprint arXiv:2409.02392, 2024a.</p>
<p>Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024b.</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a.</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b.</p>
<p>Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F Wong, Xiaowei Huang, Qiufeng Wang, and Kaizhu Huang. Is your model really a good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733, 2024.</p>
<h1>A Example of Solution Reformatting</h1>
<div class="codehilite"><pre><span></span><code><span class="nv">The</span><span class="w"> </span><span class="nv">given</span><span class="w"> </span><span class="nv">equation</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">ellipse</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">standard</span><span class="w"> </span><span class="k">for</span><span class="nv">m</span>
\<span class="p">[</span>
\<span class="nv">frac</span><span class="p">{(</span><span class="nv">x</span><span class="o">-</span><span class="nv">h</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="p">}{</span><span class="nv">a</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span><span class="o">+</span>\<span class="nv">frac</span><span class="p">{(</span><span class="nv">y</span><span class="o">-</span><span class="nv">k</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="p">}{</span><span class="nv">b</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">]</span>
<span class="nv">where</span><span class="w"> </span><span class="p">$(</span><span class="nv">h</span><span class="p">,</span><span class="w"> </span><span class="nv">k</span><span class="p">)$</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">center</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ellipse</span><span class="p">,</span><span class="w"> </span><span class="p">$</span><span class="nv">a</span><span class="p">$</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">semi</span><span class="o">-</span>
<span class="nv">major</span><span class="w"> </span><span class="nv">axis</span><span class="w"> </span><span class="nv">length</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="p">$</span><span class="nv">b</span><span class="p">$</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">semi</span><span class="o">-</span><span class="nv">minor</span><span class="w"> </span><span class="nv">axis</span><span class="w"> </span><span class="nv">length</span><span class="o">.</span>
<span class="nv">In</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">case</span><span class="p">,</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">center</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ellipse</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="p">$(</span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)$,</span><span class="w"> </span><span class="p">$</span><span class="nv">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span><span class="p">$</span>
<span class="ow">and</span><span class="w"> </span><span class="p">$</span><span class="nv">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="p">$</span><span class="o">.</span>
<span class="nv">The</span><span class="w"> </span><span class="nv">foci</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">ellipse</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">located</span><span class="w"> </span><span class="nv">along</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">major</span><span class="w"> </span><span class="nv">axis</span><span class="p">,</span><span class="w"> </span><span class="nv">which</span>
<span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">line</span><span class="w"> </span><span class="nv">segment</span><span class="w"> </span><span class="nv">connecting</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">vertices</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ellipse</span><span class="o">.</span>
<span class="nv">The</span><span class="w"> </span><span class="nv">distance</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">center</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ellipse</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">focus</span><span class="w"> </span><span class="nv">is</span>
<span class="nv">given</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="p">$</span>\<span class="nv">sqrt</span><span class="p">{</span>\<span class="nv">sqrt</span><span class="p">{</span><span class="nv">a</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">b</span><span class="o">^</span><span class="mi">2</span><span class="p">}}$</span><span class="o">.</span>
<span class="nv">First</span><span class="p">,</span><span class="w"> </span><span class="nv">we</span><span class="w"> </span><span class="nv">calculate</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">distance</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">center</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">focus</span><span class="o">:</span>
\<span class="p">[</span>
\<span class="nv">sqrt</span><span class="p">{</span><span class="nv">a</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">b</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span><span class="o">=</span>\<span class="nv">sqrt</span><span class="p">{</span><span class="mi">5</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">3</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span><span class="o">=</span>\<span class="nv">sqrt</span><span class="p">{</span><span class="mi">25</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">9</span><span class="p">}</span><span class="o">=</span>\<span class="nv">sqrt</span><span class="p">{</span><span class="mi">16</span><span class="p">}</span>
<span class="p">]</span>
<span class="nv">Since</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">major</span><span class="w"> </span><span class="nv">axis</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ellipse</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nf">horizontal</span><span class="w"> </span><span class="p">(</span><span class="nv">because</span><span class="w"> </span><span class="p">$</span><span class="nv">a</span>
<span class="nv">b</span><span class="p">),</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">foci</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">located</span><span class="w"> </span><span class="nv">at</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">distance</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="nv">units</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">the</span>
<span class="nv">left</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nv">right</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">center</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ellipse</span><span class="o">.</span>
<span class="nv">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">foci</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">at</span><span class="w"> </span><span class="p">$(</span><span class="mi">6</span>\<span class="nv">pm</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)$</span><span class="w"> </span><span class="ow">and</span>
<span class="p">$(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)$</span><span class="o">.</span>
<span class="nv">The</span><span class="w"> </span><span class="nv">one</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">larger</span><span class="w"> </span><span class="p">$</span><span class="nv">a</span><span class="o">-</span><span class="nv">coordinate</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="p">$(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)$</span><span class="o">.</span>
<span class="nv">The</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">is</span><span class="o">:</span><span class="w"> </span><span class="p">$</span>\<span class="nv">sqrt</span><span class="p">{</span>\<span class="nv">operatorname</span><span class="p">{</span><span class="nv">bosed</span><span class="p">}(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">)}$</span><span class="o">.</span>
</code></pre></div>

<p>The given equation is of an ellipse in the standard form $\backslash[\sqrt{\sqrt{(x-h)^2}(a^2)}+\backslash \sqrt{\sqrt{(y-6)^2}(b^2)}=1, \backslash]$ where $\backslash(\mathrm{h}$, $\mathrm{k}(\backslash)$ is the center of the ellipse, $\backslash(\mathrm{a} \backslash)$ is the semi-major axis length, and $\backslash(\mathrm{b} \backslash)$ is the semi-minor axis length. In this case, the center of the ellipse is $\backslash(16$, $3) \backslash$, $\backslash(a=5 \backslash$, and $\backslash(b=3 \backslash)$.</p>
<p>The foci of an ellipse are located along the major axis, which is the line segment connecting the two vertices of the ellipse. The distance from the center of the ellipse to each focus is given by $\backslash(\sqrt{\sqrt{a^2 - b^2}} \backslash)$. First, we calculate the distance from the center to each focus: $\backslash[\sqrt{\sqrt{a^2 - b^2}} =\sqrt{\sqrt{5^2}-}$ $3^2)=\sqrt{\sqrt{25}-9}=\sqrt{\sqrt{16}}=4 . \backslash]$ Since the major axis of the ellipse is horizontal (because $\backslash(a \geq b \backslash$ ), the foci are located at a distance of 4 units to the left and right of the center of the ellipse. Therefore, the two foci are at $\backslash((6 \pm 4,3)=(2,3))$ and $\backslash((10,3))$.</p>
<p>The one with the larger $\backslash(\mathrm{a} \backslash)$-coordinate is $\backslash((10,3))$. The answer is: $\backslash(\sqrt{\operatorname{bosed}(10,3)}) \backslash$.</p>
<p>Figure 6: Example of solution reformatting. The left is the original solution (generated by Qwen2-7BInstruct) and the right is the reformatted one. The problem, coming from the MATH test set, is "The ellipse $\frac{(x-h)^{2}}{25}+\frac{(y-3)^{2}}{9}=1$ has two foci. Find the one with the larger $x$-coordinate. Enter your answer as an ordered pair, like $(2,1)$."</p>
<h2>B Breakdown Statistics of ProcessBench</h2>
<p>Table 4: Breakdown statistics of ProCesSBench. ${ }^{\dagger}$ : We encountered a code bug when using Llama-3.170B-Instruct and Qwen2.5-72B-Instruct to generate solutions for the MATH problems, thus their counts are all zero in the MATH subset of ProcesSBench. $\ddagger$ : For the more challenging OlympiadBench and Omni-MATH problems, we exclude models with lower accuracies from subsequent annotation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Generator</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MATH ${ }^{\ddagger}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OlympiadBench ${ }^{\ddagger}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Omni-MATH ${ }^{\ddagger}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-8B-Instruct</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B-Instruct</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-70B-Instruct</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">19</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-1.5B-Instruct</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-72B-Instruct</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-1.5B-Instruct</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-7B-Instruct</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">29</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-72B-Instruct</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">38</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-Instruct</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">29</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">207</td>
<td style="text-align: center;">193</td>
<td style="text-align: center;">594</td>
<td style="text-align: center;">406</td>
<td style="text-align: center;">661</td>
<td style="text-align: center;">339</td>
<td style="text-align: center;">759</td>
<td style="text-align: center;">241</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">400</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>C Training Details of Qwen2.5-Math-7B-PRM800K</h1>
<p>Qwen2.5-Math-7B-PRM800K is obtained by fine-tuning Qwen2.5-Math-7B-Instruct on the PRM800K training set. We replace the original language modeling head with a new reward modeling head that outputs binary classification logits. The classification loss is computed at the second line break positions in all the " $\backslash \mathrm{n} \backslash \mathrm{n}$ ". We treat the original 1 and 0 labels in PRM800K as our positive labels, while -1 as negative ones. To eliminate test data contamination, we also remove the PRM800K training samples that have the same problems in PROCESSBENCH. The training was run on eight A100 80GB GPUs.</p>
<h2>D Inference Details</h2>
<p>For solution generation in $\S 3.2$, all the models are set with $p=0.9, t=0.7$. For majority voting evaluation in $\S 4$, we set $p=0.8, t=0.7, k=20$ for Qwen2.5-Math-7/72B-Instruct to ensure their normal generation, while all the other models are set with only $p=0.9$. All the inference in the evaluation was run with vLLM (Kwon et al., 2023) on eight A100 80GB GPUs.</p>
<h2>E Prompt Template for Critic Model Evaluation</h2>
<div class="codehilite"><pre><span></span><code><span class="n">The</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">math</span><span class="w"> </span><span class="n">problem</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">solution</span><span class="w"> </span><span class="p">(</span><span class="n">split</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">paragraphs</span><span class="p">,</span><span class="w"> </span><span class="n">enclosed</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">tags</span><span class="w"> </span><span class="n">and</span>
<span class="n">indexed</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="o">:</span>
<span class="p">[</span><span class="n">Math</span><span class="w"> </span><span class="n">Problem</span><span class="p">]</span>
<span class="p">...(</span><span class="n">math</span><span class="w"> </span><span class="n">problem</span><span class="p">)...</span>
<span class="p">[</span><span class="n">Solution</span><span class="p">]</span>
<span class="o">&lt;</span><span class="n">paragraph_0</span><span class="o">&gt;</span>
<span class="p">...(</span><span class="n">paragraph</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">solution</span><span class="p">)...</span>
<span class="o">&lt;/</span><span class="n">paragraph_0</span><span class="o">&gt;</span>
<span class="p">...</span>
<span class="o">&lt;</span><span class="n">paragraph_n</span><span class="mi">-1</span><span class="o">&gt;</span>
<span class="w">    </span><span class="p">(</span><span class="n">paragraph</span><span class="w"> </span><span class="n">n</span><span class="mi">-1</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">solution</span><span class="p">)...</span>
<span class="o">&lt;/</span><span class="n">paragraph_n</span><span class="mi">-1</span><span class="o">&gt;</span>
<span class="n">Your</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">review</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">critique</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">solution</span><span class="w"> </span><span class="n">paragraph</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">paragraph</span><span class="p">.</span><span class="w"> </span><span class="n">Once</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">identify</span><span class="w"> </span><span class="n">an</span>
<span class="n">error</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">paragraph</span><span class="p">,</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">paragraph</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">earliest</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">occurs</span><span class="p">.</span><span class="w"> </span><span class="n">Otherwise</span><span class="p">,</span>
<span class="k">return</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">-1</span><span class="w"> </span><span class="p">(</span><span class="n">which</span><span class="w"> </span><span class="n">typically</span><span class="w"> </span><span class="n">denotes</span><span class="w"> </span><span class="s">&quot;not found&quot;</span><span class="p">).</span>
<span class="n">Please</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">e</span><span class="p">.,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="n">boxed</span><span class="p">().</span>
</code></pre></div>

<p>Figure 7: Prompt template for critic model evaluation. The blue texts indicate the input math problem and the solution (split into paragraphs). The red texts describe the required output content and format.</p>
<h1>F Supplementary Evaluation Results</h1>
<p>Table 5: Breakdown of evaluation results on the GSM8K and MATH subsets of ProcesSBench. The open-source language models (middle block) are evaluated via majority voting over eight samplings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Open-source Process Reward Models (PRMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Math-Shepherd-PRM-7B</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">29.5</td>
</tr>
<tr>
<td style="text-align: center;">RLHFlow-PRM-Mistral-8B</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">33.4</td>
</tr>
<tr>
<td style="text-align: center;">RLHFlow-PRM-Deepseek-8B</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">33.8</td>
</tr>
<tr>
<td style="text-align: center;">Skywork-PRM-1.5B</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">48.0</td>
</tr>
<tr>
<td style="text-align: center;">Skywork-PRM-7B</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">53.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-PRM800K (our trained)</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">62.6</td>
</tr>
<tr>
<td style="text-align: center;">Open-source language models, prompted as Critic Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-8B-Instruct</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">22.8</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B-Instruct</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">5.1</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-70B-Instruct</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">48.2</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.3-70B-Instruct</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">59.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-Instruct</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">25.7</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">52.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-7B-Instruct</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">6.5</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-14B-Instruct</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">39.9</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-32B-Instruct</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">19.0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-72B-Instruct</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">49.2</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-7B-Instruct</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">36.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-14B-Instruct</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">53.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-32B-Instruct</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">53.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-72B-Instruct</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">61.8</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">78.7</td>
</tr>
<tr>
<td style="text-align: center;">Proprietary language models, prompted as Critic Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-40-0806</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">63.6</td>
</tr>
<tr>
<td style="text-align: center;">o1-mini</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">88.9</td>
</tr>
</tbody>
</table>
<p>Table 6: For the two PRMs from Skywork (2024), we additionally adjust the threshold (§ 4.1) as the one leading to the highest F1 score on each subset (i.e., each subset adopts a respective optimal threshold), which can be viewed as the two PRMs' upper bound performance on ProcesSBench. This table presents the results on the GSM8K and MATH subsets, which are marginally higher than those in Table 5 that all adopt the threshold selected on the GSM8K subset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">Skywork-PRM-1.5B (respective thresholds)</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: left;">Skywork-PRM-7B (respective thresholds)</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">54.4</td>
</tr>
</tbody>
</table>
<p>Table 7: Breakdown of evaluation results on the OlympiadBench and Omni-MATH subsets of ProcesSBench. The open-source language models (middle block) are evaluated via majority voting over eight samplings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">OlympiadBench</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Omni-MATH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Open-source Process Reward Models (PRMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Math-Shepherd-PRM-7B</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">23.8</td>
</tr>
<tr>
<td style="text-align: center;">RLHFlow-PRM-Mistral-8B</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">15.8</td>
</tr>
<tr>
<td style="text-align: center;">RLHFlow-PRM-Deepseek-8B</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">16.9</td>
</tr>
<tr>
<td style="text-align: center;">Skywork-PRM-1.5B</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">19.2</td>
</tr>
<tr>
<td style="text-align: center;">Skywork-PRM-7B</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">21.0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-PRM800K (our trained)</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: center;">Open-source language models, prompted as Critic Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-8B-Instruct</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">12.6</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B-Instruct</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.6</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-70B-Instruct</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">41.0</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.3-70B-Instruct</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">43.0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-Instruct</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">12.7</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">31.7</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-7B-Instruct</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-14B-Instruct</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">27.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-32B-Instruct</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">46.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">12.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-72B-Instruct</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">40.2</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-7B-Instruct</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-14B-Instruct</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">41.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-32B-Instruct</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-72B-Instruct</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">61.3</td>
</tr>
<tr>
<td style="text-align: center;">Proprietary language models, prompted as Critic Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o-0806</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;">o1-mini</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">82.4</td>
</tr>
</tbody>
</table>
<p>Table 8: For the two PRMs from Skywork (2024), we additionally adjust the threshold (§ 4.1) as the one leading to the highest F1 score on each subset (i.e., each subset adopts a respective optimal threshold), which can be viewed as the two PRMs' upper bound performance on ProcesSBench. This table presents the results on the OlympiadBench and Omni-MATH subsets, which are slightly higher than those in Table 7 that all adopt the threshold selected on the GSM8K subset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">OlympiadBench</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Omni-MATH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">Skywork-PRM-1.5B (respective thresholds)</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">22.6</td>
</tr>
<tr>
<td style="text-align: left;">Skywork-PRM-7B (respective thresholds)</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">23.1</td>
</tr>
</tbody>
</table>
<p>Table 9: Breakdown of evaluation results of the open-source language models (prompted as critic models) using greedy decoding.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-8B-Instruct</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">8.9</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B-Instruct</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">11.8</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-70B-Instruct</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.3-70B-Instruct</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">54.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-Instruct</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">23.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">50.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-7B-Instruct</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-14B-Instruct</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">26.2</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-32B-Instruct</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">54.8</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">8.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-72B-Instruct</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">46.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-7B-Instruct</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">34.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-14B-Instruct</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">55.2</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-32B-Instruct</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">49.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-72B-Instruct</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">60.7</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">56.3</td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">OlympiadBench</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Omni-MATH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">error</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-8B-Instruct</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">6.7</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">29.6</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B-Instruct</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">12.6</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-70B-Instruct</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.3-70B-Instruct</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">41.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-Instruct</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">8.9</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">30.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-7B-Instruct</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-14B-Instruct</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">11.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Coder-32B-Instruct</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">4.8</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-72B-Instruct</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">37.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-7B-Instruct</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-14B-Instruct</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">39.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-32B-Instruct</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">35.7</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-72B-Instruct</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">34.6</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding authors.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2067 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2067</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2067</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-277856996</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.12976v1.pdf" target="_blank">Sparks of Science: Hypothesis Generation Using Structured Paper Data</a></p>
                <p><strong>Paper Abstract:</strong> Generating novel and creative scientific hypotheses is a cornerstone in achieving Artificial General Intelligence. Large language and reasoning models have the potential to aid in the systematic creation, selection, and validation of scientifically informed hypotheses. However, current foundation models often struggle to produce scientific ideas that are both novel and feasible. One reason is the lack of a dedicated dataset that frames Scientific Hypothesis Generation (SHG) as a Natural Language Generation (NLG) task. In this paper, we introduce HypoGen, the first dataset of approximately 5500 structured problem-hypothesis pairs extracted from top-tier computer science conferences structured with a Bit-Flip-Spark schema, where the Bit is the conventional assumption, the Spark is the key insight or conceptual leap, and the Flip is the resulting counterproposal. HypoGen uniquely integrates an explicit Chain-of-Reasoning component that reflects the intellectual process from Bit to Flip. We demonstrate that framing hypothesis generation as conditional language modelling, with the model fine-tuned on Bit-Flip-Spark and the Chain-of-Reasoning (and where, at inference, we only provide the Bit), leads to improvements in the overall quality of the hypotheses. Our evaluation employs automated metrics and LLM judge rankings for overall quality assessment. We show that by fine-tuning on our HypoGen dataset we improve the novelty, feasibility, and overall quality of the generated hypotheses. The HypoGen dataset is publicly available at huggingface.co/datasets/UniverseTBD/hypogen-dr1.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2067.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2067.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3.1-8B-FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1 8B (fine-tuned on HypoGen)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter LLaMA 3.1 language model variant fine-tuned on the HypoGen Bit-Flip-Spark+Chain-of-Reasoning dataset to generate structured scientific hypotheses conditioned on a problem statement (Bit). Fine-tuning increases alignment with expert proposals while shifting the novelty/feasibility trade-off toward more feasible outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLaMA 3.1 8B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (causal transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>hypothesis generation / computer science</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>scientific hypotheses (Spark + Chain-of-Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (reduced novelty after fine-tuning relative to one-shot; human outputs are more novel)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>conditional causal language modelling fine-tuned on structured Bit-Flip-Spark+Chain-of-Reasoning pairs (prompted with Bit only at inference to generate Spark and Chain-of-Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>automated proxy metrics (perplexity, IAScore, Idea Distinctness Index) and LLM-based pairwise judgements (Claude 3.7 Sonnet and o3-mini), plus a small human blind evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Perplexity=32.41 (after fine-tuning on HypoGen); IAScore=0.6746; Idea Distinctness Index=0.6297. Fine-tuned variant wins overall quality vs one-shot variants ~86-92% in LLM-judge pairwise comparisons (reported range across experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>LLM-judge pairwise assessments report high feasibility win rates for fine-tuned models (feasibility win rate reported 74-86% over one-shot variants); human vs model comparisons still favor humans overall (human win rate ~80-90%), while fine-tuned model feasibility is comparable to humans in some measures (reported 62-64% for model vs 36-38% for human in a particular comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Reported trade-off: fine-tuning increases feasibility/alignment (IAScore) but reduces novelty (Idea Distinctness Index declines), i.e., validation-favored outputs are less novel.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper directly compares generation (novelty/diversity metrics, perplexity) to validation (LLM-judge and human judgements) and reports an asymmetry: models generate ideas that are less preferred overall than human ideas despite higher feasibility after fine-tuning; generation often outpaces reliable validation for highly novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>none reported for this model's generative confidence (no calibrated probability or uncertainty bands reported); paper notes broader literature finding LLMs can present high-confidence hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported; authors caution about LLMs presenting inaccurate outputs with high confidence but provide no calibration metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not explicitly quantified; evaluation includes an independent test set of 50 recent hypotheses (2024-2025) but no explicit OOD metrics are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses perplexity (fluency), IAScore (alignment to authors' proposed future research ideas via IdeaMatcher), and Idea Distinctness Index (semantic diversity) as proxy validation metrics; LLM pairwise judges used for novelty/feasibility/overall quality.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Small-scale human evaluation performed on a subset (20 hypothesis pairs) and authors state further extensive human evaluation is planned; frequency increases recommended for higher novelty outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (computer science experimental/theoretical domain)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Fine-tuning on structured Chain-of-Reasoning data, using multiple LLM judges to cross-check decisions, small human blind evaluations, and plans to build reward models aligned to human expertise to reduce judge bias.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Human-generated hypotheses win overall in LLM-judge comparisons (~80-90% win rate), and LLM judges / automatic proxies can be biased; the novelty/feasibility trade-off indicates models may generate ideas whose validation is less reliable as novelty increases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Fine-tuning improves IAScore substantially (e.g., from 0.2781 to 0.6746 for standard LLaMA), and feasibility of fine-tuned models approaches or matches human feasibility in reported comparisons (model feasibility reported 62-64% vs human 36-38% in a stated comparison), suggesting the gap can be reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported as a ratio; reported compute used for fine-tuning: 4 NVIDIA H100 GPUs (80 GB each) with 4-bit quantization and LoRA; no quantitative comparison of validation vs generation cost provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2067.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2067.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R1-distilled-LLaMA-3.1-8B-FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R1-distilled LLaMA 3.1 8B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled variant of LLaMA 3.1 (knowledge transferred from a larger 671B DeepSeek-R1 model) fine-tuned on HypoGen; shows different trade-offs in diversity and alignment compared to the standard LLaMA fine-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>R1-distilled LLaMA 3.1 8B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (distilled causal transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>hypothesis generation / computer science</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>scientific hypotheses (Spark + Chain-of-Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel but shows reduced semantic diversity after fine-tuning (distinctiveness declines)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>conditional causal language modelling with distillation-derived pretraining then LoRA fine-tuning on HypoGen structured pairs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>same automated proxies (perplexity, IAScore, Idea Distinctness Index) and LLM judge pairwise comparisons; small human evaluation subset</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Perplexity before fine-tuning 19.85, after fine-tuning 34.98; IAScore improved from 0.6049 to 0.6729; Idea Distinctness Index decreased from 0.7146 to 0.6288 indicating reduced semantic diversity after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Fine-tuned R1-distilled model demonstrates improved alignment and feasibility in LLM and small human evaluations (large improvements reported in the human evaluation for novelty and feasibility for R1-distilled comparisons), but humans still preferred their own outputs overall in majority of comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Similar trade-off to standard LLaMA: fine-tuning improves alignment/feasibility (IAScore up) while reducing idea distinctiveness (novelty metric down).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper reports that fine-tuned R1-distilled models show dramatic improvements in judged novelty and feasibility (per human eval) but still participate in the overall human-vs-model gap; explicit asymmetry noted between generation creativity and robust validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>none reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not explicitly reported beyond use of a held-out set of 50 recent hypotheses; no numeric OOD metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Perplexity, IAScore (IdeaMatcher), and Idea Distinctness Index used as validation proxies; LLM judges and small-scale human evaluation used as higher-fidelity validators.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Small-scale human evaluation used; authors recommend more extensive human evaluation in future, especially to validate novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (computer science)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Same as standard LLaMA: fine-tuning on Chain-of-Reasoning data, cross-checking with multiple LLM judges, and human-in-the-loop evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Despite measured improvements, humans still win overall in pairwise comparisons, indicating validation limits persist; reductions in idea distinctiveness suggest fine-tuning may move models toward 'safe' validated outputs at the expense of novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Human evaluations showed dramatic improvements for fine-tuned R1-distilled outputs on novelty and feasibility (reported strong preferences in reported small-scale human eval), indicating the gap can be narrowed substantially with appropriate fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported; fine-tuning used same compute setup (4 H100 GPUs, LoRA, 4-bit quantization) but no explicit cost ratio between generation and validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2067.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2067.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1 model (used for structured extraction and baseline one-shot generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed proprietary OpenAI model used in this study to extract Bit/Flip/Spark components from abstracts and to produce human-structured hypotheses for the evaluation set (used as a human proxy in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAI o1</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (proprietary, generative)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>data extraction / hypothesis structuring / one-shot hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured hypothesis components (Bit, Flip, Spark, Chain-of-Reasoning); one-shot generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>varies (used as reference/human proxy; presumed to produce high-quality structured extractions and one-shot hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompted extraction and one-shot generation from paper abstracts and full text to create structured Bit-Flip-Spark and Chain-of-Reasoning outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Outputs from o1 were treated as 'human' hypothesis baselines for pairwise LLM-judge comparison and were included in automated metric calculations (e.g., perplexity) but not validated with external ground-truth experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>o1-generated structured hypotheses were used as 'gold' or human baselines in evaluation; specific numeric generation metrics for o1 (perplexity, IAScore) are not itemized separately in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Treated as human baselines; LLM judges and small human evaluation favored true human-authored entries over model outputs generally, but o1 outputs served as the reference in multiple comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not separately quantified in the paper for o1 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used primarily as a source of structured dataset and as a one-shot baseline; the paper emphasizes dependence on model-based extraction and notes the need for human verification of such outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>o1 outputs were evaluated via the same proxies (perplexity, IAScore, Idea Distinctness) and LLM judges when compared in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>o1 was used to generate dataset entries and evaluation baselines; authors used small human checks and note the need for broader human validation for extracted content.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (CS papers extraction and summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>The authors used retry mechanisms (up to three attempts) and parallel processing to improve extraction quality and included human authors' checks for an evaluation subset; still advocate for more human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Reliance on an LLM (o1) for structured data extraction introduces potential for copying/hallucination and highlights the difficulty of automated validation absent human checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>o1 was effective enough to construct the dataset and baselines, enabling measurable improvements in downstream fine-tuned models, indicating utility despite validation concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2067.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2067.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.7-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 3.7 Sonnet-Thinking (LLM Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large Anthropic model used as the primary automated evaluator (LLM judge) to perform pairwise comparisons of generated hypotheses on novelty, feasibility, and overall quality with an extended token budget for deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude 3.7 Sonnet-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (evaluator/judge)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation/validation of generated hypotheses (general scientific reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>pairwise judgement labels (win/tie) for novelty, feasibility, overall quality</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>n/a (serves as evaluator rather than generator)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>pairwise comparative judgement on (Spark + Chain-of-Reasoning) proposals, randomized ordering, 8,000 token budget to encourage deliberation; used to compute win rates across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Provided consistent, scalable judgements that aligned with observed human evaluation trends; reported outcomes include fine-tuned models winning overall vs one-shot 86-92% and fine-tuned feasibility win rates 74-86%; humans still win overall 82-90% in judge's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Judge outputs reveal and quantify the novelty vs feasibility trade-off across models: non-fine-tuned models often rated higher for novelty while fine-tuned models rated higher for feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used to compare generator outputs; authors note LLM-as-judge modules can be biased and less verifiable than human judges, so they rerun with another judge and perform limited human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>no explicit uncertainty calibration reported for judge outputs; only deliberation token budget used to encourage thorough reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported; authors caution about judge bias but do not provide calibration metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Produces pairwise novelty/feasibility/overall labels used as validation signals; complements automated proxies like IAScore and perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Paper supplements LLM-judge results with a small-scale human blind evaluation (20 pairs) and recommends broader human evaluation in future.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Cross-evaluating with multiple LLM judges (o3-mini) and conducting human blind evaluations to check judge consistency and biases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors explicitly state that reliance on LLM judges is a limitation and may bias validation in non-trivial ways, supporting the idea that automated validation can be insufficient for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Judge results are broadly consistent with human evaluation trends in this study, suggesting LLM judges can be useful proxies under constrained conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2067.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2067.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o3-mini (secondary LLM Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller OpenAI model used as a secondary LLM judge to re-run the experimental analysis and measure agreement with the primary judge, confirming the novelty-feasibility trade-off and main win/loss patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAI o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (evaluator/judge)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation/validation of generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>pairwise judgement labels (novelty, feasibility, overall)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>pairwise comparative evaluation for consistency check with primary judge</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported to show consistent behavior with Claude judge across most experiments: agreement on novelty-feasibility trade-off and confirming human win for overall quality in most comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Confirmed the trade-off patterns seen with the primary judge, but no calibration or error rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used only to validate consistency of automated judgement; highlights that different LLM judges broadly agree but not necessarily that automated validation is fully reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Used as an LLM judge proxy complementing the primary judge and automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used alongside a small human blind evaluation subset; more human validation recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Cross-judge comparison to detect judge-specific biases and reinforce conclusions where judges agree.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Agreement between different LLM judges on the novelty-feasibility trade-off supports the observation that generation and validation capabilities differ and that automated judges may share biases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Cross-judge agreement suggests automated judgement can be reproducible, providing some counter-evidence to the idea that validation is wholly unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2067.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2067.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IdeaMatcher (GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IdeaMatcher model (GPT-based idea containment classifier, per Kumar et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classifier used to compute IAScore by measuring alignment between generated ideas and author-proposed future research ideas; prior work (Kumar et al., 2024) reported using GPT with 91.8% accuracy for this role, and HypoGen uses the same IdeaMatcher concept as an automated alignment metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IdeaMatcher (GPT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model used as a classification/comparison module</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>automated idea alignment / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>alignment scores between generated hypotheses and author proposals (used to compute IAScore)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>n/a (evaluation tool)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>computes pairwise containment/similarity between author-proposed future research ideas and generated hypotheses to produce IAScore; prior reported accuracy 91.8 in Kumar et al. (2024) when GPT was used as IdeaMatcher.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>IAScore used to quantify alignment; HypoGen reports IAScore improvements after fine-tuning (e.g., 0.27810.6746 for LLaMA), indicating better alignment to author ideas as judged by the IdeaMatcher proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Higher IAScore corresponds to greater alignment (less novelty) in practice; paper reports inverse relationship between IAScore improvement and Idea Distinctness decline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>IdeaMatcher IAScore is a proxy linking generated content to existing literature-based author proposals; it helps detect when generated ideas reflect known directions versus being novel.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>not reported in HypoGen study; prior work reports classifier accuracy but uncertainty intervals not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Serves directly as a proxy metric (IAScore) for alignment; complements diversity and fluency proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used as an automated proxy across the dataset; supplemented by human evaluation in a subset to check alignment validity.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Using IdeaMatcher IAScore in concert with diversity metrics and human checks to detect copying and measure novelty vs known proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper notes the difficulty of automated validation and potential for models to copy training data; reliance on a proxy IdeaMatcher highlights limits of automated checks for true novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>IAScore improvements after fine-tuning indicate automated alignment proxies can meaningfully track model changes and improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models unlock novel scientific research ideas? <em>(Rating: 2)</em></li>
                <li>Hypothesis generation with large language models <em>(Rating: 2)</em></li>
                <li>Improving scientific hypothesis generation with knowledge grounded large language models <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 1)</em></li>
                <li>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2067",
    "paper_id": "paper-277856996",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "LLaMA-3.1-8B-FT",
            "name_full": "LLaMA 3.1 8B (fine-tuned on HypoGen)",
            "brief_description": "An 8-billion-parameter LLaMA 3.1 language model variant fine-tuned on the HypoGen Bit-Flip-Spark+Chain-of-Reasoning dataset to generate structured scientific hypotheses conditioned on a problem statement (Bit). Fine-tuning increases alignment with expert proposals while shifting the novelty/feasibility trade-off toward more feasible outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLaMA 3.1 8B (fine-tuned)",
            "system_type": "large language model (causal transformer)",
            "scientific_domain": "hypothesis generation / computer science",
            "output_type": "scientific hypotheses (Spark + Chain-of-Reasoning)",
            "novelty_level": "moderately novel (reduced novelty after fine-tuning relative to one-shot; human outputs are more novel)",
            "generation_method": "conditional causal language modelling fine-tuned on structured Bit-Flip-Spark+Chain-of-Reasoning pairs (prompted with Bit only at inference to generate Spark and Chain-of-Reasoning)",
            "validation_method": "automated proxy metrics (perplexity, IAScore, Idea Distinctness Index) and LLM-based pairwise judgements (Claude 3.7 Sonnet and o3-mini), plus a small human blind evaluation",
            "generation_performance": "Perplexity=32.41 (after fine-tuning on HypoGen); IAScore=0.6746; Idea Distinctness Index=0.6297. Fine-tuned variant wins overall quality vs one-shot variants ~86-92% in LLM-judge pairwise comparisons (reported range across experiments).",
            "validation_performance": "LLM-judge pairwise assessments report high feasibility win rates for fine-tuned models (feasibility win rate reported 74-86% over one-shot variants); human vs model comparisons still favor humans overall (human win rate ~80-90%), while fine-tuned model feasibility is comparable to humans in some measures (reported 62-64% for model vs 36-38% for human in a particular comparison).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Reported trade-off: fine-tuning increases feasibility/alignment (IAScore) but reduces novelty (Idea Distinctness Index declines), i.e., validation-favored outputs are less novel.",
            "generation_validation_comparison": "Paper directly compares generation (novelty/diversity metrics, perplexity) to validation (LLM-judge and human judgements) and reports an asymmetry: models generate ideas that are less preferred overall than human ideas despite higher feasibility after fine-tuning; generation often outpaces reliable validation for highly novel outputs.",
            "uncertainty_quantification": "none reported for this model's generative confidence (no calibrated probability or uncertainty bands reported); paper notes broader literature finding LLMs can present high-confidence hallucinations.",
            "calibration_quality": "not reported; authors caution about LLMs presenting inaccurate outputs with high confidence but provide no calibration metrics.",
            "out_of_distribution_performance": "not explicitly quantified; evaluation includes an independent test set of 50 recent hypotheses (2024-2025) but no explicit OOD metrics are reported.",
            "validation_proxy_metrics": "Uses perplexity (fluency), IAScore (alignment to authors' proposed future research ideas via IdeaMatcher), and Idea Distinctness Index (semantic diversity) as proxy validation metrics; LLM pairwise judges used for novelty/feasibility/overall quality.",
            "human_validation_required": true,
            "human_validation_frequency": "Small-scale human evaluation performed on a subset (20 hypothesis pairs) and authors state further extensive human evaluation is planned; frequency increases recommended for higher novelty outputs.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal (computer science experimental/theoretical domain)",
            "gap_mitigation_strategies": "Fine-tuning on structured Chain-of-Reasoning data, using multiple LLM judges to cross-check decisions, small human blind evaluations, and plans to build reward models aligned to human expertise to reduce judge bias.",
            "evidence_supporting_gap": "Human-generated hypotheses win overall in LLM-judge comparisons (~80-90% win rate), and LLM judges / automatic proxies can be biased; the novelty/feasibility trade-off indicates models may generate ideas whose validation is less reliable as novelty increases.",
            "evidence_contradicting_gap": "Fine-tuning improves IAScore substantially (e.g., from 0.2781 to 0.6746 for standard LLaMA), and feasibility of fine-tuned models approaches or matches human feasibility in reported comparisons (model feasibility reported 62-64% vs human 36-38% in a stated comparison), suggesting the gap can be reduced.",
            "computational_cost_ratio": "Not reported as a ratio; reported compute used for fine-tuning: 4 NVIDIA H100 GPUs (80 GB each) with 4-bit quantization and LoRA; no quantitative comparison of validation vs generation cost provided.",
            "uuid": "e2067.0"
        },
        {
            "name_short": "R1-distilled-LLaMA-3.1-8B-FT",
            "name_full": "R1-distilled LLaMA 3.1 8B (fine-tuned)",
            "brief_description": "A distilled variant of LLaMA 3.1 (knowledge transferred from a larger 671B DeepSeek-R1 model) fine-tuned on HypoGen; shows different trade-offs in diversity and alignment compared to the standard LLaMA fine-tuned model.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "R1-distilled LLaMA 3.1 8B (fine-tuned)",
            "system_type": "large language model (distilled causal transformer)",
            "scientific_domain": "hypothesis generation / computer science",
            "output_type": "scientific hypotheses (Spark + Chain-of-Reasoning)",
            "novelty_level": "moderately novel but shows reduced semantic diversity after fine-tuning (distinctiveness declines)",
            "generation_method": "conditional causal language modelling with distillation-derived pretraining then LoRA fine-tuning on HypoGen structured pairs",
            "validation_method": "same automated proxies (perplexity, IAScore, Idea Distinctness Index) and LLM judge pairwise comparisons; small human evaluation subset",
            "generation_performance": "Perplexity before fine-tuning 19.85, after fine-tuning 34.98; IAScore improved from 0.6049 to 0.6729; Idea Distinctness Index decreased from 0.7146 to 0.6288 indicating reduced semantic diversity after fine-tuning.",
            "validation_performance": "Fine-tuned R1-distilled model demonstrates improved alignment and feasibility in LLM and small human evaluations (large improvements reported in the human evaluation for novelty and feasibility for R1-distilled comparisons), but humans still preferred their own outputs overall in majority of comparisons.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Similar trade-off to standard LLaMA: fine-tuning improves alignment/feasibility (IAScore up) while reducing idea distinctiveness (novelty metric down).",
            "generation_validation_comparison": "Paper reports that fine-tuned R1-distilled models show dramatic improvements in judged novelty and feasibility (per human eval) but still participate in the overall human-vs-model gap; explicit asymmetry noted between generation creativity and robust validation.",
            "uncertainty_quantification": "none reported",
            "calibration_quality": "not reported",
            "out_of_distribution_performance": "not explicitly reported beyond use of a held-out set of 50 recent hypotheses; no numeric OOD metrics provided.",
            "validation_proxy_metrics": "Perplexity, IAScore (IdeaMatcher), and Idea Distinctness Index used as validation proxies; LLM judges and small-scale human evaluation used as higher-fidelity validators.",
            "human_validation_required": true,
            "human_validation_frequency": "Small-scale human evaluation used; authors recommend more extensive human evaluation in future, especially to validate novelty.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal (computer science)",
            "gap_mitigation_strategies": "Same as standard LLaMA: fine-tuning on Chain-of-Reasoning data, cross-checking with multiple LLM judges, and human-in-the-loop evaluation.",
            "evidence_supporting_gap": "Despite measured improvements, humans still win overall in pairwise comparisons, indicating validation limits persist; reductions in idea distinctiveness suggest fine-tuning may move models toward 'safe' validated outputs at the expense of novelty.",
            "evidence_contradicting_gap": "Human evaluations showed dramatic improvements for fine-tuned R1-distilled outputs on novelty and feasibility (reported strong preferences in reported small-scale human eval), indicating the gap can be narrowed substantially with appropriate fine-tuning.",
            "computational_cost_ratio": "Not reported; fine-tuning used same compute setup (4 H100 GPUs, LoRA, 4-bit quantization) but no explicit cost ratio between generation and validation.",
            "uuid": "e2067.1"
        },
        {
            "name_short": "o1 (OpenAI)",
            "name_full": "OpenAI o1 model (used for structured extraction and baseline one-shot generation)",
            "brief_description": "A closed proprietary OpenAI model used in this study to extract Bit/Flip/Spark components from abstracts and to produce human-structured hypotheses for the evaluation set (used as a human proxy in experiments).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "OpenAI o1",
            "system_type": "large language model (proprietary, generative)",
            "scientific_domain": "data extraction / hypothesis structuring / one-shot hypothesis generation",
            "output_type": "structured hypothesis components (Bit, Flip, Spark, Chain-of-Reasoning); one-shot generated hypotheses",
            "novelty_level": "varies (used as reference/human proxy; presumed to produce high-quality structured extractions and one-shot hypotheses)",
            "generation_method": "prompted extraction and one-shot generation from paper abstracts and full text to create structured Bit-Flip-Spark and Chain-of-Reasoning outputs",
            "validation_method": "Outputs from o1 were treated as 'human' hypothesis baselines for pairwise LLM-judge comparison and were included in automated metric calculations (e.g., perplexity) but not validated with external ground-truth experiments.",
            "generation_performance": "o1-generated structured hypotheses were used as 'gold' or human baselines in evaluation; specific numeric generation metrics for o1 (perplexity, IAScore) are not itemized separately in the paper.",
            "validation_performance": "Treated as human baselines; LLM judges and small human evaluation favored true human-authored entries over model outputs generally, but o1 outputs served as the reference in multiple comparisons.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not separately quantified in the paper for o1 outputs.",
            "generation_validation_comparison": "Used primarily as a source of structured dataset and as a one-shot baseline; the paper emphasizes dependence on model-based extraction and notes the need for human verification of such outputs.",
            "uncertainty_quantification": "not reported",
            "calibration_quality": "not reported",
            "out_of_distribution_performance": "not reported",
            "validation_proxy_metrics": "o1 outputs were evaluated via the same proxies (perplexity, IAScore, Idea Distinctness) and LLM judges when compared in experiments.",
            "human_validation_required": true,
            "human_validation_frequency": "o1 was used to generate dataset entries and evaluation baselines; authors used small human checks and note the need for broader human validation for extracted content.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal (CS papers extraction and summarization)",
            "gap_mitigation_strategies": "The authors used retry mechanisms (up to three attempts) and parallel processing to improve extraction quality and included human authors' checks for an evaluation subset; still advocate for more human validation.",
            "evidence_supporting_gap": "Reliance on an LLM (o1) for structured data extraction introduces potential for copying/hallucination and highlights the difficulty of automated validation absent human checks.",
            "evidence_contradicting_gap": "o1 was effective enough to construct the dataset and baselines, enabling measurable improvements in downstream fine-tuned models, indicating utility despite validation concerns.",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2067.2"
        },
        {
            "name_short": "Claude-3.7-Sonnet",
            "name_full": "Anthropic Claude 3.7 Sonnet-Thinking (LLM Judge)",
            "brief_description": "A large Anthropic model used as the primary automated evaluator (LLM judge) to perform pairwise comparisons of generated hypotheses on novelty, feasibility, and overall quality with an extended token budget for deliberation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Claude 3.7 Sonnet-Thinking",
            "system_type": "large language model (evaluator/judge)",
            "scientific_domain": "evaluation/validation of generated hypotheses (general scientific reasoning)",
            "output_type": "pairwise judgement labels (win/tie) for novelty, feasibility, overall quality",
            "novelty_level": "n/a (serves as evaluator rather than generator)",
            "generation_method": "n/a",
            "validation_method": "pairwise comparative judgement on (Spark + Chain-of-Reasoning) proposals, randomized ordering, 8,000 token budget to encourage deliberation; used to compute win rates across experiments.",
            "generation_performance": "n/a",
            "validation_performance": "Provided consistent, scalable judgements that aligned with observed human evaluation trends; reported outcomes include fine-tuned models winning overall vs one-shot 86-92% and fine-tuned feasibility win rates 74-86%; humans still win overall 82-90% in judge's comparisons.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Judge outputs reveal and quantify the novelty vs feasibility trade-off across models: non-fine-tuned models often rated higher for novelty while fine-tuned models rated higher for feasibility.",
            "generation_validation_comparison": "Used to compare generator outputs; authors note LLM-as-judge modules can be biased and less verifiable than human judges, so they rerun with another judge and perform limited human evaluation.",
            "uncertainty_quantification": "no explicit uncertainty calibration reported for judge outputs; only deliberation token budget used to encourage thorough reasoning.",
            "calibration_quality": "not reported; authors caution about judge bias but do not provide calibration metrics.",
            "out_of_distribution_performance": "not reported",
            "validation_proxy_metrics": "Produces pairwise novelty/feasibility/overall labels used as validation signals; complements automated proxies like IAScore and perplexity.",
            "human_validation_required": true,
            "human_validation_frequency": "Paper supplements LLM-judge results with a small-scale human blind evaluation (20 pairs) and recommends broader human evaluation in future.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal",
            "gap_mitigation_strategies": "Cross-evaluating with multiple LLM judges (o3-mini) and conducting human blind evaluations to check judge consistency and biases.",
            "evidence_supporting_gap": "Authors explicitly state that reliance on LLM judges is a limitation and may bias validation in non-trivial ways, supporting the idea that automated validation can be insufficient for novel outputs.",
            "evidence_contradicting_gap": "Judge results are broadly consistent with human evaluation trends in this study, suggesting LLM judges can be useful proxies under constrained conditions.",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2067.3"
        },
        {
            "name_short": "o3-mini",
            "name_full": "OpenAI o3-mini (secondary LLM Judge)",
            "brief_description": "A smaller OpenAI model used as a secondary LLM judge to re-run the experimental analysis and measure agreement with the primary judge, confirming the novelty-feasibility trade-off and main win/loss patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "OpenAI o3-mini",
            "system_type": "large language model (evaluator/judge)",
            "scientific_domain": "evaluation/validation of generated hypotheses",
            "output_type": "pairwise judgement labels (novelty, feasibility, overall)",
            "novelty_level": "n/a",
            "generation_method": "n/a",
            "validation_method": "pairwise comparative evaluation for consistency check with primary judge",
            "generation_performance": "n/a",
            "validation_performance": "Reported to show consistent behavior with Claude judge across most experiments: agreement on novelty-feasibility trade-off and confirming human win for overall quality in most comparisons.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Confirmed the trade-off patterns seen with the primary judge, but no calibration or error rates provided.",
            "generation_validation_comparison": "Used only to validate consistency of automated judgement; highlights that different LLM judges broadly agree but not necessarily that automated validation is fully reliable.",
            "uncertainty_quantification": "not reported",
            "calibration_quality": "not reported",
            "out_of_distribution_performance": "not reported",
            "validation_proxy_metrics": "Used as an LLM judge proxy complementing the primary judge and automated metrics.",
            "human_validation_required": true,
            "human_validation_frequency": "Used alongside a small human blind evaluation subset; more human validation recommended.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal",
            "gap_mitigation_strategies": "Cross-judge comparison to detect judge-specific biases and reinforce conclusions where judges agree.",
            "evidence_supporting_gap": "Agreement between different LLM judges on the novelty-feasibility trade-off supports the observation that generation and validation capabilities differ and that automated judges may share biases.",
            "evidence_contradicting_gap": "Cross-judge agreement suggests automated judgement can be reproducible, providing some counter-evidence to the idea that validation is wholly unreliable.",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2067.4"
        },
        {
            "name_short": "IdeaMatcher (GPT)",
            "name_full": "IdeaMatcher model (GPT-based idea containment classifier, per Kumar et al. 2024)",
            "brief_description": "A classifier used to compute IAScore by measuring alignment between generated ideas and author-proposed future research ideas; prior work (Kumar et al., 2024) reported using GPT with 91.8% accuracy for this role, and HypoGen uses the same IdeaMatcher concept as an automated alignment metric.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "IdeaMatcher (GPT-based)",
            "system_type": "large language model used as a classification/comparison module",
            "scientific_domain": "automated idea alignment / evaluation",
            "output_type": "alignment scores between generated hypotheses and author proposals (used to compute IAScore)",
            "novelty_level": "n/a (evaluation tool)",
            "generation_method": "n/a",
            "validation_method": "computes pairwise containment/similarity between author-proposed future research ideas and generated hypotheses to produce IAScore; prior reported accuracy 91.8 in Kumar et al. (2024) when GPT was used as IdeaMatcher.",
            "generation_performance": "n/a",
            "validation_performance": "IAScore used to quantify alignment; HypoGen reports IAScore improvements after fine-tuning (e.g., 0.27810.6746 for LLaMA), indicating better alignment to author ideas as judged by the IdeaMatcher proxy.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Higher IAScore corresponds to greater alignment (less novelty) in practice; paper reports inverse relationship between IAScore improvement and Idea Distinctness decline.",
            "generation_validation_comparison": "IdeaMatcher IAScore is a proxy linking generated content to existing literature-based author proposals; it helps detect when generated ideas reflect known directions versus being novel.",
            "uncertainty_quantification": "not reported in HypoGen study; prior work reports classifier accuracy but uncertainty intervals not provided here.",
            "calibration_quality": "not reported",
            "out_of_distribution_performance": "not reported",
            "validation_proxy_metrics": "Serves directly as a proxy metric (IAScore) for alignment; complements diversity and fluency proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "Used as an automated proxy across the dataset; supplemented by human evaluation in a subset to check alignment validity.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal",
            "gap_mitigation_strategies": "Using IdeaMatcher IAScore in concert with diversity metrics and human checks to detect copying and measure novelty vs known proposals.",
            "evidence_supporting_gap": "Paper notes the difficulty of automated validation and potential for models to copy training data; reliance on a proxy IdeaMatcher highlights limits of automated checks for true novelty.",
            "evidence_contradicting_gap": "IAScore improvements after fine-tuning indicate automated alignment proxies can meaningfully track model changes and improvements.",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2067.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models unlock novel scientific research ideas?",
            "rating": 2
        },
        {
            "paper_title": "Hypothesis generation with large language models",
            "rating": 2
        },
        {
            "paper_title": "Improving scientific hypothesis generation with knowledge grounded large language models",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "rating": 1
        },
        {
            "paper_title": "Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents",
            "rating": 1
        }
    ],
    "cost": 0.01702675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sparks of Science: Hypothesis Generation Using Structured Paper Data
17 Apr 2025</p>
<p>Charles O'neill cponeill00@gmail.com 
Tirthankar Ghosal ghosalt@ornl.gov 
Roberta Rileanu r.raileanu@ucl.ac.uk 
Mike Walmsley m.walmsley@utoronto.ca 
Thang Bui thang.bui@anu.edu.au 
Kevin Schawinski schawinski@gmail.com 
Ioana Ciuc iciuca@stanford.edu 
Hypogen Dataset </p>
<p>University of Oxford</p>
<p>Oak Ridge National Laboratory</p>
<p>University College London</p>
<p>University of Toronto</p>
<p>Australian National University</p>
<p>Stanford University</p>
<p>Research Papers</p>
<p>Sparks of Science: Hypothesis Generation Using Structured Paper Data
17 Apr 2025D189B4D36A41240A8FF1ECB0A4C2A548arXiv:2504.12976v1[cs.CL]
Generating novel and creative scientific hypotheses is a cornerstone in achieving Artificial General Intelligence.Large language and reasoning models have the potential to aid in the systematic creation, selection, and validation of scientifically informed hypotheses.However, current foundation models often struggle to produce scientific ideas that are both novel and feasible.One reason is the lack of a dedicated dataset that frames Scientific Hypothesis Generation (SHG) as a Natural Language Generation (NLG) task.In this paper, we introduce HypoGen, the first dataset of approximately 5500 structured problem-hypothesis pairs extracted from top-tier computer science conferences structured with a Bit-Flip-Spark schema, where the Bit is the conventional assumption, the Spark is the key insight or conceptual leap, and the Flip is the resulting counterproposal.HypoGen uniquely integrates an explicit Chain-of-Reasoning component that reflects the intellectual process from Bit to Flip.We demonstrate that framing hypothesis generation as conditional language modelling, with the model fine-tuned on Bit-Flip-Spark and the Chain-of-Reasoning (and where, at inference, we only provide the Bit), leads to improvements in the overall quality of the hypotheses.Our evaluation employs automated metrics and LLM judge rankings for overall quality assessment.We show that by fine-tuning on our HypoGen dataset we improve the novelty, feasibility, and overall quality of the generated hypotheses.The HypoGen dataset is publicly available at huggingface.co/datasets/UniverseTBD/hypogen-dr1.</p>
<p>Introduction</p>
<p>Hypothesis generation is the first step of the scientific process and its de facto foundation.Creative and innovative ideas have long enabled scientists to model and predict the behaviour of complex systems, from neuroscience to astrophysics.Recently, the impressive capabilities of large language models have prompted researchers to explore their potential to advance the generation of scientific ideas (Ziems et al., 2023;Birhane et al., 2023;Xie et al., 2023;Noever &amp; McKee, 2023;Si et al., 2024;Kumar et al., 2024;Xiong et al., 2024b;Zhou et al., 2024b;Cohrs et al., 2025).Not only do these models excel in understanding and generating human language (e.g., Devlin et al., 2018;Brown et al., 2020;Team et al., 2023;Grattafiori et al., 2024), but they also demonstrate a remarkable ability to make nuanced deductions and establish relationships across varied contexts (Elkins &amp; Chun, 2020), rendering them an ideal basis for the generation of semantic hypotheses.Recent work has evaluated LLMs on the entire scientific discovery process, from hypothesis generation to running experiments, analyzing the results, and even writing a paper (Lu et al., 2024a;Chan et al., 2024;Chen et al., 2024;Gottweis et al., 2025;Nathani et al., 2025;Schmidgall et al., 2025;Schmidgall &amp; Moor, 2025).However, most works highlight limitations of current models when applied to open research problems, particularly with respect to generating novel, creative, diverse, feasible, actionable, interesting, and useful ideas or hypotheses (Nathani et al., 2025).</p>
<p>LLMs face significant challenges when applied to scientific ideation.These models are prone to hallucinations, often producing non-factual content due to their token likelihood maximization objective (Manakul et al., 2023;McKenna et al., 2023;Li et al., 2023;Zhang, 2023;Tonmoy et al., 2024;Lu et al., 2024a).Recent benchmarks highlight that such inaccuracies can be difficult to detect, as LLMs often present them with high confidence (Qi et al., 2023;Zhou et al., 2024a).Additionally, probability-maximizing decoding strategies (e.g., greedy or high-beam search) can lead to text that lacks lexical diversity, a problem that persists even in models with hundreds of billions of parameters (Holtzman et al., 2019;Li et al., 2022;Meister et al., 2022;Su et al., 2022;Zhou et al., 2024a).</p>
<p>The design of a validation scheme to rigorously test these machine-generated hypotheses poses additional challenges (Alaa et al., 2021;Si et al., 2024;Luo et al., 2025).To be effective, scientific hypotheses not only require creative insight drawn from a broad understanding of the domain at hand, but also must be rooted in the existing literature to ensure their novelty and relevance (Simonton, 2004;Runco &amp; Jaeger, 2012;Doboli et al., 2014;Strm, 2018;Wang et al., 2023).In addition, it is difficult to determine in an automated fashion to what extent a certain idea already exists in the literature, which is particularly problematic due to the tendency of LLMs to copy subsets of their training data in generation (McCoy et al., 2021;Liu &amp; Hulden, 2021).Given that validation is integral to the scientific method, the closed-box nature of LLMs requires a careful and nuanced approach to ensure that the results are replicable and robust.</p>
<p>To address these challenges, we introduce HypoGen, a dataset comprising approximately 5500 structured problem-hypothesis pairs extracted from top-tier computer science conferences.This dataset represents a significant step forward in framing scientific hypothesis generation as a conditional language modeling problem.By conditioning hypotheses on a clear formulation of the problem (the Bit), our approach provides a robust foundation for developing and evaluating LLMs in the context of scientific discovery.Importantly, HypoGen incorporates a detailed Chain-of-Reasoning narrative that mirrors the iterative and reflective process used by human scientists to transition from conventional wisdom to innovative counterproposals, thus improving both the quality and the trustworthiness of the generated hypotheses.</p>
<p>Our key contributions include the development of the HypoGen dataset and the novel framing of scientific hypothesis generation as a conditional language modeling problem enriched with an explicit reasoning chain.We present baseline performance measures of an LLaMA-based model on a hypothesis generation task after being fine-tuned on the HypoGen dataset.We employ a straightforward evaluation framework that assesses hypotheses along the dimensions of novelty and feasibility, incorporating automated metrics and LLM judgements.By capturing the full chain of reasoning, our approach provides valuable insights into the thought processes underlying scientific discovery.</p>
<p>Related Work</p>
<p>Several approaches factor the decision process into sub-stages.In the proposal stage, reasoning and sometimes retrieval are used to generate candidate actions or hypotheses (Chen et al., 2021;Wang et al., 2022).The evaluation stage then scores these candidates (for example, perplexity (Ahn et al., 2022) or learned reward functions (Yao et al., 2020)), identifying which candidates are the most promising.Techniques such as ToT (Yao et al., 2023) and RAP (Hao et al., 2023) use tree search paradigms to propose and evaluate multiple solution paths in a structured manner.Reflexive approaches such as Shinn et al. (2023) and Lindes &amp; Peter (2023) explicitly incorporate iterative self-correction of hypothesized actions.The work of Zhou et al. (2024a) with HypoGeniC expands this process with iterative reinforcement learning with human feedback.These advances stress the need for benchmarks that realistically reflect the capacity of LLMs to generate, validate, and refine scientific hypotheses (e.g., Kumar et al., 2024;Majumder et al., 2024;Luo et al., 2025).For example, the "Knowledge Grounded Chain of Ideas" or KG-CoI system (Xiong et al., 2024a) removes specific links from a biomedical knowledge graph and asks LLMs to propose plausible missing relations.Because these links are derived from previously held information, LLM-generated hypotheses can be validated against known ground truths.Such tasks resemble real-world discovery scenarios, where a laboratory of AI agents can interact with human experts, document interactions, and call tools to achieve a particular task, for example, to design a novel protein binder (e.g., Swanson et al., 2024).Other innovative evaluation environments, such as Discovery World (Jansen et al., 2024) or AI Scientist (Lu et al., 2024b), provide virtual environments where an AI agent can propose hypotheses and conduct simulated experiments, opening the possibility of end-to-end science.</p>
<p>However, there remains a lack of standardized "frontier" benchmarks designed to evaluate hypothesis generation capabilities, especially in the context of agentic AI systems, which rely on highly interconnected modules that require complex reasoning (Shao et al., 2024).To this end, we introduce HypoGen, a benchmark dataset specifically designed to address current deficits in the evaluation of the generation of scientific hypotheses.In contrast to existing benchmarks, HypoGen explicitly emphasizes Chain-of-Reasoning: each hypothesis includes a transparent abductive logic trail that mirrors the thought process of a human expert.Our method uses a structured Bit-Flip-Spark + Chain-of-Reasoning format to capture the conceptual progression from an initial problem statement (Bit), to a key insight (Spark), and finally to a refined idea (Flip).By incorporating a detailed reasoning chain, HypoGen helps mitigate the risk of hallucination (Tonmoy et al., 2024), while simultaneously providing researchers with a reproducible step-by-step notebook of how a new idea was generated.</p>
<p>Methodology and the Bit-Flip-Spark+Chain-of-Reasoning Format</p>
<p>Figure 1 illustrates the overall pipeline of HypoGen1 , which is designed to extract structured information from scientific papers using the Bit-Flip-Spark+Chain-of-Reasoning format.The Stanford Bit-Flip schema2 serves as a concise and structured hypothesis formulation technique designed to encapsulate the core intellectual contribution of a research paper.The Bit identifies the prevailing belief or assumption in the research domain that the paper aims to challenge.The Flip articulates the novel approach or counterargument that the paper introduces to advance the field.In addition, we introduce a novel concept, Spark, which contains the "essence of an idea", formalized as a conceptual leap.We provide an example extracted from Bahdanau et al. (2015) and the full prompt to obtain this representation in Appendix A.</p>
<p>Example HypoGen Hypothesis</p>
<p>Bit: Conventional neural machine translation encodes the entire source sentence into a single fixed-length vector.This approach imposes a bottleneck, because crucial contextual information can be lost in the compression process.Spark: Soft alignment for flexible translation.Flip: An idea was introduced that enables the model to automatically search for relevant parts of the source sentence at each decoding step.This removes the need for a single fixed-length representation by allowing a soft-alignment mechanism that dynamically focuses on important source information.Chain-of-Reasoning: I started by considering that traditionally neural machine translation relies on a single fixed-length vector to encode an entire source sentence before generating the target sequence, and that this rigid bottleneck can lose important contextual information, limiting translation quality and flexibility.I questioned whether a more dynamic way to capture contextual cues was needed, especially for longer sequences.Instead, I introduced a learnable (soft) search mechanism that dynamically aligns each target word with the most relevant parts of the source sentence, shifting away from a fixed-length encoding to attend to context more flexibly and generate higher-quality translations.I trained a bidirectional encoderdecoder on a large bilingual corpus, used a thirty-thousand-word vocabulary, and optimized it via minibatch gradient methods while testing different hidden-layer sizes.How did we confirm that the new approach was identifying correct alignments?I visualized alignment weights and observed that the decoder selectively focused on relevant source words, substantially improving clarity and accuracy.A follow-up question examined how to handle unknown or rare terms, prompting an exploration of further lexical coverage strategies.I recognized a turning point when it became evident that soft attention preserved essential details for both short and long inputs.I validated performance on held-out test sets, noting that the model equaled or surpassed phrase-based benchmarks and maintained robustness on lengthy sentences.This integrated reasoning closed the gap between the original limitation and the dynamic alignment concept, paving the way for more context-aware neural translation.</p>
<p>The objective is to distill the complex ideas within a paper into a simplified yet rigorous representation, allowing for clear communication of both the problem being tackled (Flip) and the proposed solution (Bit).This approach is grounded in the understanding that a wellarticulated hypothesis is the cornerstone of impactful research.Although this structured representation of hypotheses is subjective and is merely one of many options, we found that it worked well for the generation of a solution (i.e.Flip) conditioned on a problem (i.e., the Bit).Finally, the Chain-of-Reasoning presents a detailed narrative that captures the scientist's ideation process that connects the Bit to the Flip.</p>
<p>Preprocessing and Dataset Construction</p>
<p>We compile our dataset from papers accepted at the two top-tier computer science conferences, NeurIPS 2023 (3218 papers) and ICLR 2024 (2260 papers), resulting in 5478 distinct samples.We then used OpenAI's o1 model for the structured extraction step.For each paper, we first extract the Bit, Flip, and Spark components from the abstract.We prompted o1 to identify the conventional assumption, the innovative approach, and a concise 4-6-word summary of the core insight.We then used a robust parallel processing approach with a retry mechanism with up to three attempts per extraction to ensure high-quality output.</p>
<p>For papers with available full text, we extract the Chain-of-Reasoning component using a separate prompt that guides the model to recreate the intellectual progression from Bit to Flip.This step removes the abstract section from the full text to prevent redundancy.It then processes the paper to generate a first-person narrative detailing the scientist's ideation process.We store the output in JSON format and include metadata such as the paper ID, title, authors, venue, year, and citation information.We construct an independent test set of 50 hypotheses from the authors' recent submissions and relevant work between 2024 and 2025.</p>
<p>Fine-tuning and Inference Pipeline</p>
<p>Our baseline models include Meta LLaMA 3.1 8B and R1-distilled LLaMA 3.1 8B.These models are trained on extensive corpora with a context window of 128,000 tokens and employ byte-pair encoding for tokenization (Sennrich et al., 2015;Kudo &amp; Richardson, 2018), incorporating a vocabulary of 128,000 tokens.The R1-distilled LLaMA 3.1 8B is a specialized model with knowledge transferred from the larger DeepSeek-R1 model with 671B parameters.This substantial pre-training provides robust language understanding capabilities essential for scientific hypothesis generation.</p>
<p>We leverage our curated dataset of structured problem-hypothesis pairs for fine-tuning, employing the causal language modeling objective.The process utilizes four NVIDIA H100 GPUs, each with 80GB of VRAM.We implement 4-bit quantization and deploy LoRA (Hu et al., 2021) with hyperparameters:  = 16 and a dropout rate of 0.1.The models are loaded with 4-bit precision base loading, using appropriate compute precision (bf16 where supported otherwise fp16).We use the AdamW 8-bit optimizer (Loshchilov &amp; Hutter, 2017) with a weight decay of 0.01, a batch size of 32, and a learning rate of 2  10 4 .The training follows a linear scheduler with 5 warmup steps and proceeds for approximately 60 total steps, with logging at each step.During inference, only the Bit is provided to the model.The model then generates the corresponding Spark along with a detailed Chain-of-Reasoning.We use the ollama LLM framework for the LLaMA one-shot inference.</p>
<p>Evaluation</p>
<p>The task of evaluating generative models tailored for the generation of scientific hypotheses is challenging, given the inherently subjective nature of scientific research.In this paper, we focus on a dual evaluation framework that primarily incorporates traditional automated metrics and LLM-based judges.</p>
<p>Our evaluation strategy relies on a test set of 50 hypotheses extracted from the recent literature from primarily 2024 and 2025.It combines automated metrics with an LLM Judge module that assesses novelty, feasibility, and overall quality from pairwise comparisons.We further test the robustness of our approach with a second LLM judge.For a subset of our evaluation set, we also use human evaluation to assess whether fine-tuning LLaMA-base models on our HypoGen dataset improves the quality of the hypotheses.</p>
<p>Automated Evaluation Metrics</p>
<p>Perplexity is used as a preliminary metric to assess the fluency and coherence of the hypotheses generated (Chen et al., 1998).It is defined as the exponentiated average negative log-likelihood of a given token sequence X = (x 0 , x 1 , . . .x t ).</p>
<p>Mathematically, this is expressed as:
PPL(X) = exp  1 t t  i log p  (x i | x &lt;i ) (1)
Here, log p  (x i |x &lt;i ) denotes the log-likelihood of the i-th token conditioned on its preceding tokens according to the model.The metric serves as an indicator of the predictive performance of the model, with lower values suggesting better generalization.</p>
<p>IAScore quantifies alignment between LLM-generated hypotheses and expert-proposed research ideas.For each paper j, the IAScore computes the average alignment between author-proposed future research ideas (AP-FRI j ) and each generated idea I ij using an IdeaMatcher (IM) model (Kumar et al., 2024):
AvgScore j = 1 N j N j  i=1 IM(AP-FRI j , I ij )(2)
The domain-wide IAScore for model M is then calculated by averaging across all P papers:
IAScore domain,M = 1 P P  j=1
AvgScore j</p>
<p>(3) Kumar et al. (2024) employed GPT as the IdeaMatcher due to its superior performance (91.8% accuracy) compared to Natural Language Inference using RoBERTa MNLI and BERTScore in determining if a generated idea is contained within the author's proposals.Higher IAScore values indicate greater alignment between LLM-generated ideas and author perspectives across the domain.</p>
<p>Idea Distinctiveness Index evaluates the semantic diversity between the hypotheses generated using embedding-based similarity rather than textual differences at the surface level.For a set of ideas I, each idea id i is embedded into vector v i using a pre-trained BERT model (Kumar et al., 2024).The distinctness between ideas id i and id j is defined as
D ij = 1  sim(v i , v j )
, where sim is cosine similarity.The overall distinctiveness for a set of n ideas is:
D I = 1 n(n  1) n  i=1 n  j=1 j =i D ij (4)
To assess the performance of a model within a domain, we can calculate the Idea Distinctness Index D I p M for all ideas generated by model M for each paper p, then average across all m papers:
D domain,M = 1 m m  p=1 D I p M(5)
Higher D domain,M values signify greater idea diversity, indicating the model's ability to generate semantically varied hypotheses within the domain.</p>
<p>LLM Evaluation</p>
<p>To evaluate the quality of the hypotheses in our evaluation set, we employed Anthropic's Claude 3.7 Sonnet-Thinking model as the automated evaluator.We perform a pairwise evaluation on each dataset consisting of 50 problems and proposals of paired solutions generated by two LLMs for each evaluation experiment.We have nine experiments corresponding to LLaMA 3.1-8B-FT (LLaMA-8B-FT for brevity) vs Human, LlaMA 3.1-8B-FT (LLaMA-8B-FT) vs an o1 model with one example (1shot), followed by an R1-distilled-LlaMA-3.1-8B-FT (R1-distilled-LlaMA-FT) vs Human and o1-1shot, LLaMA-8b-FT vs R1-distilled-LLaMA-8b-FT, Human vs o1-1shot, R1-distilled-LlaMA-8b-1shot vs R1-distilled-LLaMA-8b-FT, LLaMA-8B-1shot vs LLaMA-8B-FT and LLaMA-8B-1shot vs R1-distilled-LLaMA-8B-1shot (R1-distilled-LlaMA-1shot).We provide our results in Fig. 2. The Human hypotheses are the o1 structured hypotheses generated from the evaluation set.</p>
<p>For each Bit, the LLM evaluator was asked to evaluate which proposal (Spark + Chain-of-Reasoning) provided the overall better proposal, taking into account novelty and feasibility.We randomize the presentation order of the solutions to mitigate order effects.After each evaluation experiment, we obtain whether proposal A wins in novelty, feasibility, and overall, with an option for a tie.The model's "thinking" is further enabled with an 8,000 token budget to encourage thorough deliberation.</p>
<p>The LLM-based evaluation provides consistency and scalability; however, it comes at the cost of robustness and verifiability.To account for some of these challenges, we rerun our experimental analysis with the OpenAI o3-mini model as a judge to see the degree of agreement.In addition, we conducted a blind human evaluation with 20 hypothesis pairs evaluated by one of the authors.We provide our complete prompts in Appendix A.</p>
<p>Results</p>
<p>Results from Automated Metrics Table 1 shows that human-generated hypotheses have much higher perplexity values than their LLM counterparts.In particular, LLaMA base models exhibit values between 16.70 and 34.98 compared to human ones (89.31).This could point to the semantic creativity present in human-generated ideas.Although perplexity remains lower overall, fine-tuning increases the perplexity score of the LLaMA models, indicating increased "unpredictability" as it stands to ideation.</p>
<p>Secondly, fine-tuning improves idea alignment with the target domain, as shown by the significant improvement in IAScore for the standard LLaMA model (0.2781  0.6746).This result could mean that the structured Bit-Flip-Spark+Chain-of-Reasoning training enables models to generate hypotheses that better align with expert-level scientific thinking.The fact that we do not see this effect to the same extent in the distilled LLaMA model may hint at the effectiveness of knowledge transfer.</p>
<p>The inverse relationship between IAScore improvements and Idea Distinctness Index reductions, which are particularly notable in the R1 Distilled LLaMA with a reduction from 0.7146  0.6288, indicates a possible trade-off in hypothesis generation: as models better align with expert scientific thinking patterns, they may produce less semantically diverse outputs.</p>
<p>Pairwise Comparison using LLM Judges</p>
<p>As shown in the upper panel of Fig. 2, finetuning consistently improves overall hypothesis quality relative to one-shot variants of the same architecture (86-92% preference for fine-tuned versions), despite the reduction in novelty scores.This indicates that fine-tuning on HypoGen steers models toward generating more practical hypotheses.</p>
<p>The LLM evaluation results in 2 reveal a consistent trade-off between novelty and feasibility in the different experiments.Models that excel in creativity metrics seem to underperform in feasibility and vice versa.Human-generated hypotheses win overall in quality assessments compared to LLM-generated alternatives, with human ideas preferred in 80-90% of the comparisons.However, fine-tuned models demonstrate comparable feasibility scores relative to the human set (A=62-64% vs. B=36-38%).Rerunning our analysis with o3-mini as the LLM judge shows consistent behaviour across most experiment: agreement on the key novelty-feasibility trade-off in fine-tuned versus one-shot models and confirming the win of human hypotheses for overall quality.We show our results in Fig. 3 in Appendix B.</p>
<p>Human Evaluation Results</p>
<p>The results of the small-scale human evaluation trace the observed patterns with the Claude 3.7 Sonnet Thinking model.For the R1-distilled LLaMA comparison, the human evaluator preferred fine-tuned model outputs for novelty (95% vs. 5%) and feasibility (70% vs. 30%), with an overall preference for fine-tuned outputs (70% preference, 25% tie, 5% base model).The standard LLaMA-8B comparison revealed more competitive performance, with the fine-tuned model maintaining modest advantages in novelty (47.6% vs 42.9%, 9. 5% tie) and feasibility (52.4% vs 42.9%, 4. 8% tie), resulting in a narrower overall preference (42.9% fine-tuned, 33.3% one shot, 23.8% tie).The human evaluation provides further evidence that fine-tuning on structured Bit-Flip-Spark+Chainof-Reasoning data improves hypothesis quality, with particularly dramatic improvements observed in the R1-distilled architecture.However, further human evaluation is needed.</p>
<p>Discussion and Future Work</p>
<p>We introduced the HypoGen dataset for the generation of scientific hypothesis that extends the conventional Bit-Flip-Spark format by incorporating a detailed Chain-of-Reasoning component.We showed that fine-tuning on HypoGen enables the LLaMA 3.1-8B and R1-distilled-LLaMA 3.1-8B models to improve their hypotheses.This demonstrates the effectiveness of fine-tuning in the intermediate steps of an idea, which provides more transparency and interpretability.We release HypoGen under an MIT license to encourage the development of AI agents capable of supporting human experts in the ideation process.</p>
<p>The primary limitation of HypoGen is that it uses LLMs to evaluate the hypotheses generated.</p>
<p>Although LLM-as-a-judge modules can perform robustly under certain conditions (Lu et al., 2024a), they may be biased by their training regime in highly non-trivial ways.To mitigate these unexpected effects, we plan to perform an extensive human evaluation to determine the degree to which human and LLM align on a particular judgement.These findings will guide the construction of more robust reward models that align closely with human expertise, further strengthening HypoGens applicability in real-world scientific discovery.</p>
<p>Looking to the future, we want to examine how our approach with HypoGen generalizes to other scientific domains.Our evaluation focused on computer science, and it remains an open question how well the fine-tuning on one domain generalizes to another.We also plan to expand our dataset to fields such as astrophysics, biology, and materials science, where hypothesis generation could accelerate scientific discoveries in fundamentally different fields.This work aims to enable interdisciplinary AI teammates that collaborate with human experts on challenging scientific tasks (Swanson et al., 2024), with the overarching goal of democratising science.</p>
<p>-Explain the ** method ** or ** technique ** that enables this change .</p>
<p>-Include ** enough detail ** so the Flip is understandable on its own .-<strong> Bit </strong>: at least two sentences , with sufficient detail about the conventional approach and its limitation .-<strong> Flip </strong>: at least two sentences , describing the new approach or perspective with enough detail to understand the main technique .-<strong> Spark </strong>: a concise 4 -6 word summary of the core idea .Follow these rules : -Do not cite the paper itself or its authors .</p>
<p>-Instead of saying " We /I introduced an idea ", just say " An idea was introduced ...".</p>
<p>Return ONLY the JSON object in ** this exact format ** ( no extra text ): \{{ " Bit ": " Technical limitation or conventional approach , in at least two sentences ", " Flip ": " Innovative approach or solution , in at least two sentences ", " Spark ": "4 -6 word summary " \}} """ A.2 Chain-of-Reasoning Prompt NOTEBOOK_PROMPT = """ You are a highly advanced computer scientist with extraordinary ability in scientific hypothesis generation .</p>
<p>You are given : -A pre -identified " Bit "( the conventional assumption or limitation ) -A pre -identified " Flip "( the innovative approach or solution ) -The full text of the paper .</p>
<p>Please provide the Scientist ' s Ideation Notebook to obtain the intellectual process that went from Bit to Flip .In other words , how did the Bit go to the Flip ?The goal is to model the intellectual process of a scientist in a comprehensive cycle of analysis , summarizing , exploration , reassessment , reflection , backtracing , and iteration to develop a well -considered thinking process as they understand how to go from the Bit to the Flip .</p>
<p>This scientist_notebook should be detailed enough to write the paper and must include a mix of interrogative and reflective output .It needs to include questions that probe the process alongside reflective answers that elaborate on methodological details , as well as experimental observations and results that emerged during hypothesis testing .It should also include a few additional important questions regarding experimental design , data analysis , and validation , and contain a reflection that highlights a breakthrough insight akin to a Eureka moment , without stating that you experienced one .It needs to be written in first person singular and follow these rules : Rules : -Use scientific language .</p>
<p>-Ensure that the scientist_notebook includes explicit questions that probe your reasoning process , clearly interwoven with your reflective responses .</p>
<p>A.4 Model Generations</p>
<p>A.5 Original Bit</p>
<p>Conventional information retrieval methods rely on large labeled datasets for finetuning, which can be expensive or impractical to obtain.Moreover, even when such datasets are available, their effectiveness rapidly decreases in new domains due to distributional shifts.</p>
<p>A.5.1 LLaMA-8B-1shot</p>
<p>I started by considering the limitations of conventional information retrieval methods.They heavily rely on large labeled datasets, which can be expensive or impractical to obtain in some cases.Moreover, even when such datasets are available, their effectiveness rapidly decreases in new domains due to distributional shifts.This led me to wonder: how might I address these challenges without breaking the bank or requiring extensive data annotation?I explored whether utilizing self-supervised learning techniques could help bridge this gap.By generating pseudo-labels from unlabeled data and using them for fine-tuning, I asked: could we leverage weakly supervised or even unsupervised methods to adapt conventional retrieval models more efficiently?Another question that arose was: might we tap into the strengths of pre-trained language models by leveraging their rich contextual representations?This led me to ponder whether augmenting these representations with domain-specific knowledge graphs could enhance the model's robustness and generalizability across diverse domains.As I delved deeper, I began to consider the potential benefits of meta-learning.By training a model on multiple tasks or datasets simultaneously, it might learn more transferable and flexible representations that can adapt quickly to novel environments.This sparked another question: how would we evaluate the performance of such models in real-world settings?Upon further reflection, I realized that addressing these challenges may involve a multidisciplinary approach.By integrating insights from natural language processing (NLP), graph neural networks (GNNs), and meta-learning, we could potentially develop more adaptive and domainagnostic retrieval methods.</p>
<p>A.5.2 LLaMA-8B-FT</p>
<p>I began by asking whether large labeled datasets were necessary to fine-tune a retrieval model, and I found that in many cases, they are not.I tested a method that learns to rank from unlabeled data and confirmed that it outperformed standard fine-tuning approaches on several benchmarks.I then questioned how to ensure that the model generalizes well in new domains, and I introduced a domain-specific pretraining stage to adapt the model to different distributions.I asked how to measure the impact of this pre-training, so I performed extensive experiments that showed consistent improvements across diverse domains.I also examined how to validate that the model's rankings were robust, and I measured the correlation between the predicted rankings and true relevance scores.I found that the pre-training stage effectively bridged the gap between different distributions, and I wondered if further hyperparameter tuning would yield even better performance.I tested different data augmentation strategies and found that they further enhanced the model's adaptability.I reflected on whether this approach could scale to larger datasets, and I confirmed that it maintained its effectiveness even with millions of unlabeled examples.Finally, I concluded that domain-specific pre-training on unlabeled data can greatly reduce reliance on labeled data and enable more efficient retrieval in new domains.</p>
<p>B o3-mini Evaluation</p>
<p>Figure 1 :
1
Figure 1: The HypoGen process begins with input paper abstracts, from which the structured Bit (the problem), Flip (the solution) and Spark (key insight) are extracted by OpenAI's o1 model.The Chain of Reasoning is extracted by the o1 model from the main body of the paper.These outputs are used to fine-tune a LLaMA-based model, which then generates hypotheses from the provided Bit.A judge module (Claude 3.7 Sonnet) assesses the overall quality based on novelty and feasibility.</p>
<p>Figure 2 :
2
Figure2: Comparative analysis of the quality of generated hypotheses across nine experiments as evaluated by an LLM Judge Claude 3.7 Sonnet.Upper: Win rates comparing non-fine-tuned versus fine-tuned LLaMA 3.1-8B (LlaMA-8B-FT) and R1-distilled-LlaMA-3.1-8B (R1-distilled-8B-FT) models on novelty and feasibility, showing the consistent trade-off in which fine-tuned models excel at feasibility (74-86% win rate).Non-fine-tuned variants show greater novelty (54-86% win rate).Lower: Pairwise win rate heatmap (read on the horizontal) between human experts, fine-tuned models (LLaMA-8B-FT, R1-FT), and one-shot models (O1-1shot, LLaMA-8B-1shot, R1-1shot) across novelty, feasibility, and overall quality dimensions.Human hypotheses are the overall winners (82-90% win rate), with fine-tuned models achieving comparable feasibility scores (62-64% vs Human).The fine-tuned models perform better than their one-shot counterparts in overall quality (86-92% win rate).</p>
<ol>
<li>** Spark ( Core Summary ) <strong>: -A concise </strong>4 -6 word ** phrase capturing the core idea .Now , consider this research abstract : { abstract } Your task : Identify the Bit , Flip , and Spark from the abstract in a ** detailed ** manner :</li>
</ol>
<p>Figure 3 :
3
Figure 3: Comparative analysis of the quality of generated hypotheses across nine experiments as evaluated by an LLM Judge o3-mini.</p>
<p>Table 1 :
1
Automated evaluation metrics comparing different model outputs.IAScore measures idea alignment with source material, while the Idea Distinctness Index quantifies the uniqueness of generated hypotheses.
ModelPerplexity IAScore Idea Distinctness IndexGold Outputs89.31--Before FinetuningLLaMA 3.1 8B16.700.27810.6669R1 Distilled LlaMA 3.1 8B19.850.60490.7146After FinetuningLLaMA 3.1 8B -FT32.410.67460.6297R1 Distilled LlaMA 3.1 8B -FT34.980.67290.6288</p>
<p>-Use only evidence from the paper text , don ' t quote it but rephrase it in a more concise form . -Be very specific and clear about methodological details .Integrate technical and methodological details in a reflective style that explains and justifies each inquiry .-You can use parts of the provided Bit or Flip , but do not incorporate their text verbatim .-Do not use generic phrases such as The Bit suggests ... a l w a y s use the actual content of the Bit .-Only citations referenced in the paper are allowed .Do not make up citations .-Keep the notebook concise and with great logical flow , with a maximum of ten relatively short sentences , optimally .Do not use overlong sentences .-When specific methods or models are mentioned , incorporate further context provided in the paper text to strengthen your analysis .-Include discussion of experimental results and additional probing questions related to experimental design , data analysis , and validation .-Do not mention you experienced an " Eureka !" moment , but provide a question or reflection that clearly highlights a breakthrough insight akin to a turning point .-The output needs to be in continuous flow , for example , no bullet points or numbered lists .-Have good grammar , syntax and punctuation .
Bit : { bit }Flip : { flip }Paper text :{ paper_text }Return ONLY the JSON below ( no other text ):{{" notebook ": scientist_notebook}}"""\ appendix\ section { Prompts Used in Experiments }\ subsection { Abstract -Level Analysis Prompt }\ begin { lstlisting }[ breaklines = true , basicstyle =\ small \ ttfamily ]ABSTRACT_PROMPT = """..."""A.3 Evaluation Promptprompt = f """I need you to evaluate two different proposed solutions to a problem . I 'll provide the problem statement and two options (A and B) , each witha " Spark " ( the core idea ) and a " Chain of Reasoning " ( detailedexplanation ).PROBLEM :{ problem }OPTION A:Spark : { option_a [' spark ']}Chain of Reasoning : { option_a [' chain ']}
Our code implementation is publicly available at github.com/UniverseTBD/hypogen-cs.
https://web.stanford.edu/class/cs197c/slides/02-literature-search.pdf
. ** Flip ( Innovation ) **: -Provide at least ** two ** sentences describing the ** novel approach ** or perspective .
AcknowledgmentsThe authors are deeply grateful to Dr. Charles F. McMillan, whose encouragement to pursue bold ideas inspired this work, and we dedicate this study to him.We thank Microsoft Research and the Microsoft Accelerating Foundation Models Research program for their continuous support and for providing the OpenAI credits used to generate the HypoGen outputs.We also thank the Oak Ridge Leadership Computing Facility for access to highperformance computing resources that supported this research.A Appendix: Prompts used in the analysisA.1 Abstract-Level Bit-Flip-Spark Prompt ABSTRACT_PROMPT = """ You are a highly advanced research assistant , specialized in reading scientific papers for hypothesis generation and identifying innovative ideas .Before you begin , let ' s revisit the Bit -Flip concept with an example ( BERT in NLP ):Bit : Traditional NLP models ( RNNs , LSTMs ) process text sequentially , limiting their ability to understand long -range dependencies and fully capture bidirectional context .Flip : Instead , consider entire sentences at once , allowing context from both directions .This helps capture nuanced relationships among words .Spark : Bidirectional context for NLP .Bit -Flip Defined :A Bit -Flip inverts a commonly held assumption , questioning existing constraints or reapplying techniques to new domains / scales .The " Bit " is the prevailing belief , and the " Flip " is the counterargument .Then provide a brief explanation of your reasoning (2 -3 sentences )."""A.5.3 R1-distilled-LLaMA-1shotI recognized the challenge of costly labeled datasets in information retrieval.I hypothesized that pre-trained models could offer a foundation for new domains without extensive fine-tuning.I explored transfer learning strategies to adapt these models to specific tasks, testing domain adaptation techniques for better generalization.I also considered self-supervised learning to leverage unlabeled data and enhance robustness across distributions.Through empirical validation, I demonstrated that these approaches significantly reduced dependency on large datasets while maintaining performance.A.5.4 R1-distilled-LLaMA-FTI first asked how to train retrieval models without relying on large labeled datasets, and I tested a few approaches that only use unlabeled text.I hypothesized that self-supervised methods could offer a more efficient training path, so I designed a pipeline that first pre-trained a retrieval model on unlabeled text and then used a small labeled subset to refine it.I observed that this approach improved retrieval performance over methods that solely used labeled data, especially in domains with limited labeled samples.To validate this conclusion, I conducted multiple experiments on diverse domains, measuring retrieval accuracy and efficiency.I noticed that the pre-training phase was crucial for capturing general patterns, while the refinement step fine-tuned the model to better match specific labels.I then asked if this approach would generalize across different datasets and found consistent improvements across multiple benchmark sets.I tested the model on a domain with a large labeled set and observed that it performed as well as the fine-tuned model, indicating that the self-supervised pre-training was sufficient.I also questioned whether the refinement step was truly necessary and found that omitting it degraded retrieval performance.In analyzing the data, I realized that the small labeled subset was vital for aligning the model with the target tasks, confirming the importance of both pre-training and refinement.I then considered whether the approach would remain robust if the labeled subset was too small, and the results indicated that it still performed adequately.Ultimately, I concluded that combining self-supervised pre-training with a small labeled refinement step is an effective and efficient method for training retrieval models in diverse domains.and efficient method for training retrieval models in diverse domains.
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models. Ahmed M Alaa, Boris Van Breugel, Evgeny S Saveliev, Mihaela Van Der Schaar, International Conference on Machine Learning. 2021</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2015</p>
<p>Science in the age of large language models. Abeba Birhane, Atoosa Kasirzadeh, David Leslie, Sandra Wachter, Nature Reviews Physics. 52023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.07095arXiv:2107.033742024. 2021arXiv preprintet al. Evaluating large language models trained on code</p>
<p>Evaluation metrics for language models. Douglas Stanley F Chen, Roni Beeferman, Rosenfeld, 1998</p>
<p>Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, arXiv:2410.05080Toward rigorous assessment of language agents for data-driven scientific discovery. 2024arXiv preprint</p>
<p>Vasileios Sitokonstantinou, Gherardo Varando, and Gustau Camps-Valls. Large language models for causal hypothesis generation in science. Kai-Hendrik Cohrs, Emiliano Diaz, Machine Learning: Science and Technology. 61130012025</p>
<p>Bert: Pretraining of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Simona Doboli, Fanshu Zhao, Alex Doboli, arXiv:1406.7582New measures for evaluating creativity in scientific publications. 2014arXiv preprint</p>
<p>Can gpt-3 pass a writers turing test. Katherine Elkins, Jon Chun, Journal of Cultural Analytics. 522020</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, ArXiv, abs/1904.097512019</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Alexander Jansen, Marc-Alexandre Cot'e, Tushar Khot, Erin Bransom, Bhavana Dalvi, Prasad Bodhisattwa, Oyvind Majumder, Peter Tafjord, Clark, ArXiv, abs/2406.067692024270380311</p>
<p>Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, arXiv:1808.062262018arXiv preprint</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, arXiv:2409.061852024arXiv preprint</p>
<p>Halueval: A large-scale hallucination evaluation benchmark for large language models. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jianyun Nie, Ji Rong, Wen , ArXiv, abs/2305.117472023</p>
<p>Contrastive decoding: Open-ended text generation as optimization. Lisa Xiang, Ari Li, Daniel Holtzman, Percy Fried, Jason Liang, Tatsunori Eisner, Luke Hashimoto, Mike Zettlemoyer, Lewis, Annual Meeting of the Association for Computational Linguistics. 2022</p>
<p>Improving knowledge extraction from llms for robotic task learning through agent analysis. R James, Wray Lindes, Peter, arXiv:2306.067702023arXiv preprint</p>
<p>Can a transformer pass the wug test? tuning copying bias in neural morphological inflection models. Ling Liu, Mans Hulden, ArXiv, abs/2104.064832021</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024aarXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N Foerster, Jeff Clune, David Ha, ArXiv, abs/2408.062922024b</p>
<p>Llm4sr: A survey on large language models for scientific research. Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, 2025</p>
<p>Discoverybench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, ArXiv, abs/2407.017252024</p>
<p>Selfcheckgpt: Zeroresource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark John, Francis Gales, ArXiv, abs/2303.088962023</p>
<p>How much do language models copy from their training data? evaluating linguistic novelty in text generation using raven. R Thomas Mccoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz, Transactions of the Association for Computational Linguistics. 112021</p>
<p>Sources of hallucination by large language models on inference tasks. Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, Mark Steedman, ArXiv, abs/2305.145522023</p>
<p>Typical decoding for natural language generation. Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell, ArXiv, abs/2202.006662022246442062</p>
<p>Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, arXiv:2502.14499A new framework and benchmark for advancing ai research agents. 2025arXiv preprint</p>
<p>Numeracy from literacy: Data science as an emergent skill from large language models. David Noever, Forrest Mckee, ArXiv, abs/2301.133822023256416333</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhangren Chen, Bowen Zhou, ArXiv, abs/2311.059652023</p>
<p>The standard definition of creativity. A Mark, Garrett J Runco, Jaeger, Creativity research journal. 2412012</p>
<p>Samuel Schmidgall, Michael Moor, arXiv:2503.18102Agentrxiv: Towards collaborative autonomous research. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, ArXiv, abs/1508.079092015</p>
<p>Collaborative gym: A framework for enabling and evaluating human-agent collaboration. Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, Diyi Yang, ArXiv, abs/2412.157012024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.113662023arXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Creativity in science: Chance, logic, genius, and zeitgeist. Dean Keith, Simonton , 2004Cambridge University Press</p>
<p>Creativity in science-scientific essay. Heidi Angell, Strm , 2018</p>
<p>A contrastive framework for neural text generation. Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, Nigel Collier, ArXiv, abs/2202.064172022246823043</p>
<p>The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, 10.1101/2024.11.11.623004bioRxiv. 2024</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>S M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Aman Vipula Rawte, Amitava Chadha, Das, arXiv:2401.01313A comprehensive survey of hallucination mitigation techniques in large language models. 2024arXiv preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Large language models as master key: Unlocking the secrets of materials science with gpt. Tong Xie, Yuwei Wan, Wei Huang, Yufei Zhou, Yixuan Liu, Qingyuan, Shaozhou Linghu, Chunyu Wang, Clara Kit, W Grazian, Zhang, Bram, Hoex, ArXiv, abs/2304.022132023</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, ArXiv, abs/2411.023822024a</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024barXiv preprint</p>
<p>Keep calm and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, arXiv:2010.029032020arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>User-controlled knowledge fusion in large language models: Balancing creativity and hallucination. Chen Zhang, ArXiv, abs/2307.161392023260334043</p>
<p>Hypothesis generation with large language models. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, ArXiv, abs/2404.043262024a</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024barXiv preprint</p>
<p>Can large language models transform computational social science?. Caleb Ziems, William B Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, ArXiv, abs/2305.035142023</p>            </div>
        </div>

    </div>
</body>
</html>
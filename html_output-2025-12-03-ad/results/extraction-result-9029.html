<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9029 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9029</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9029</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-267412707</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.02547v2.pdf" target="_blank">Integration of cognitive tasks into artificial general intelligence test for large models</a></p>
                <p><strong>Paper Abstract:</strong> Summary During the evolution of large models, performance evaluation is necessary for assessing their capabilities. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of cognitive science-inspired artificial general intelligence (AGI) tests, including crystallized, fluid, social, and embodied intelligence. The AGI tests consist of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community. We propose increasing the complexity of AGI testing tasks commensurate with advancements in large models and emphasizing the necessity for the interpretation of test results to avoid false negatives and false positives. We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9029.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9029.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (ToM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Theory of Mind evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on Theory of Mind style tasks (false-belief and emotion understanding), reported to show an ability to infer others' mental states with patterns similar to human inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer language model developed by OpenAI (referred to in the paper as an example of modern LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Theory of Mind tests (false-belief tasks, emotion understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Classic cognitive-psychology-style Theory of Mind paradigms assessing the model's ability to infer others' beliefs, intentions, or emotions (e.g., false-belief tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: 'demonstrates a certain level of Theory of Mind capability' and 'exhibits ToM capabilities similar to human inference patterns' (no numeric score reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not reported in this paper (comparison described qualitatively as 'similar to human inference patterns').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM performance described as similar to human inference patterns (qualitative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Cited prior work; specific experimental protocols, prompts, or scoring metrics are not provided in this perspective paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No numeric metrics or standardized comparison reported here; claim is drawn from cited prior studies (refs cited) and may depend on task framing, prompt engineering, and dataset specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9029.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9029.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Emotional Intelligence / SECEU)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (evaluated with the Situational Evaluation of Complex Emotional Understanding, SECEU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was evaluated in an emotional-understanding battery (SECEU) and reportedly achieved an emotional-intelligence score exceeding the majority of human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer language model developed by OpenAI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A scenario-based emotional understanding battery that elicits complex emotions in varied situations to measure emotional understanding and emotional intelligence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported: GPT-4 achieved an EQ score that 'surpassed those of 89% of human participants' (percentile comparison reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants' distribution used as baseline; exact mean/SD not reported here — only that GPT-4 exceeded 89% of participants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 outperformed most human participants on this emotional-intelligence metric (above the 89th percentile of the human sample in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Summary result is reported from prior work; this perspective does not provide prompt formats, sample sizes, or scoring rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The paper notes that such cross-subject comparisons can be sensitive to task design and that high scores may reflect different underlying mechanisms (e.g., pattern matching) rather than human-like emotional understanding; full methodological details are in the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9029.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9029.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (decision-making / gambling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (evaluated on decision-making/gambling tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports on GPT-3's performance on decision-making tasks are mixed: some cited work finds GPT-3 falls short on gambling tasks, while other cited work indicates GPT-3 can outperform humans on some decision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained autoregressive transformer model (OpenAI GPT-3 family referenced as an early high-capacity LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Decision-making tasks (e.g., gambling tasks such as the Iowa Gambling Task / multi-armed bandit variants / other decision paradigms)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Cognitive tests assessing decision-making under uncertainty, reward-based choice, and risk evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Described as mixed: 'GPT-3's performance falls short of human performance' on gambling tasks (ref. cited), while other cited work reports GPT-3 'outperforms humans in decision-making tasks' in different settings (refs cited). No numeric accuracies/scores provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not specified numerically in this paper; referenced comparisons are qualitative across cited studies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Conflicting: some studies report GPT-3 below human baseline on gambling tasks, others report above-human performance on some decision tasks — depends on task and methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Outcomes depend on the particular decision paradigm and experimental setup in the cited literature; this paper highlights the potential for modality- and task-specific differences and does not provide original experimental protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors emphasize that discrepancies may arise from task alignment, prompt framing, and differences between human cognitive constraints and model memory/strategy; the mixed findings illustrate sensitivity to experimental design and risk of false positives/negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9029.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9029.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM vs PaLM 2 (memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM and PaLM 2 (memory/repetition capacity comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of memory capacity between PaLM and PaLM 2 with respect to item repetition: PaLM 2 is reported to be inferior to PaLM when repetition count is less than three.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM, PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's Pathways Language Model family (PaLM) and an updated variant (PaLM 2) referenced with respect to memory/retrieval comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Memory/repetition memory test (short-term memory / repetition-based memory capacity)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tests that probe the model's ability to remember and reproduce items across brief repetition counts (working memory-like behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: 'The memory capacity of PaLM 2 is inferior to that of PaLM when the repetition count is less than three.' No numeric scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Relative: PaLM > PaLM 2 for low repetition counts, but no human baseline comparison given here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Reported result is cited from prior work (ref. cited). The perspective does not provide protocol details such as prompt format, exact tasks, or sample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No numeric metrics or statistical details given; the direction of the effect is reported but underlying reasons (architecture, fine-tuning, prompting) are not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9029.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9029.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniGPT-4 (Raven examples)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniGPT-4 (evaluated on Raven's-style visual/physics reasoning examples shown in figures)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MiniGPT-4 is illustrated with two Raven-style examples: one physics question where it answered incorrectly but appeared to grasp the concept, and one image reasoning question where it answered correctly but its internal reasoning was incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal LMM (MiniGPT-4 used here as an illustrative multimodal model); the perspective presents brief example evaluations with visual reasoning items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Raven's Progressive Matrices / RAVEN visual-relational reasoning (illustrative physics and visual reasoning items)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Relational and analogical visual reasoning tasks inspired by Raven's Progressive Matrices, assessing abstract relational reasoning with visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative examples: answered a physics Raven item incorrectly (but showed conceptual grasp) and answered an image reasoning item correctly while exhibiting an incorrect reasoning process — no aggregate accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided for these illustrative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Mixed/ambiguous — examples illustrate potential false negatives (apparent failure despite concept grasp) and false positives (correct answer with incorrect reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Illustrative examples presented in figure captions; exact prompt/context, item selection, and scoring are not given in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors use these examples to highlight pitfalls: correct outputs do not always reflect correct internal reasoning (risk of false positives), and misalignment of input modalities can yield false negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9029.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9029.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogEval (Evaluating cognitive maps and planning in LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CogEval is a prior evaluation methodology that probes cognitive maps and planning abilities of large language models, reporting that LLMs often lack comprehension of underlying relational structures in planning problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various, unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generic reference to LLMs evaluated by CogEval (authors of this perspective cite CogEval results summarizing deficits in relational/planning comprehension).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogEval (cognitive maps and planning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A set of cognitive tasks designed to evaluate map-like representations, planning, and relational reasoning in sequential decision problems.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported finding: LLMs 'lack comprehension of the underlying relational structures within planning problems' (qualitative summary from cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not reported in the summary in this perspective article.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs underperform relative to the normative expectation of relational comprehension in planning contexts (qualitative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Summary citation; the perspective does not reproduce CogEval's exact protocols or numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Claims are drawn from CogEval results; specifics (which LLMs, prompt settings) are in the cited paper. Authors use this to motivate broader AGI-style, multimodal, and virtual-community testing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9029.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9029.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogBench (continuous psychological tests for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CogBench is a framework conducting continuous psychological tests on large models via dynamic information flow to assess cognitive levels; referenced as an approach rather than reporting specific scores here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various, unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generic set of LLMs that can be evaluated in CogBench; the perspective cites CogBench as an example of continuous psychological testing pipelines for models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench continuous psychological test suite</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A platform/framework for running dynamic, ongoing psychological-style assessments on LLMs to probe cognitive attributes over time.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Not reported in this perspective; CogBench is referenced as a methodological example rather than to present specific performance numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not applicable in this paper (methodological mention).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Referenced as an existing platform (citation given); no experimental details reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Perspective notes that continuous psychological testing is useful but does not provide dataset-level results; users should inspect CogBench primary source for specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9029.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9029.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo-0613 (CoT example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo-0613 (chain-of-thought / reflection example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The perspective illustrates that gpt-3.5-turbo-0613 fails on a triangle-angle reasoning problem without inspiration/adaptation, but with chain-of-thought (CoT) and reflection it produces a correct response.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A GPT-3.5-series chat model (specific revision 0613 referenced in figure captions) used in illustrative prompting examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Adaptation/Inspiration reasoning example (triangle-angle problem)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A small deductive reasoning problem requiring multi-step inference; used to illustrate the effect of chain-of-thought prompting and reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Without CoT/reflection: incorrect; with CoT/reflection: correct (illustrative example). No formal accuracy aggregate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided for this toy example.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Shows that prompt engineering (CoT/reflection) can change model performance from incorrect to correct on specific reasoning tasks; not a population-level comparison to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors note use of chain-of-thought, reflection, and adaptation/inspiration techniques; example based on gpt-3.5-turbo-0613 as stated in the figure caption.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>This is an illustrative demonstration rather than a controlled experiment; it underscores sensitivity to prompting and the risk of false negatives if appropriate reasoning aids are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9029.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9029.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAVEN visual reasoning (general claim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAVEN (Relational and Analogical Visual rEasoNing dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAVEN is a visual-relational benchmark adapting Raven's Progressive Matrices to images; the perspective notes that on RAVEN, some models have demonstrated reasoning capabilities similar to humans (cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified vision-and-reasoning models (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models evaluated on the RAVEN dataset for visual relational/analogical reasoning; specific model names and sizes are not detailed in the perspective's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>RAVEN (visual-relational Raven-style benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Dataset combining visual perception with abstract relational reasoning (Raven-like matrix reasoning but in image form).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Paper states that in this visual reasoning task 'models have demonstrated reasoning capabilities similar to those of humans' per cited studies; no numeric performance values are given in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided here; comparison is described qualitatively (similarity to humans in reasoning capability reported in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Cited reports claim models' reasoning on RAVEN can be similar to human performance in some measures; details and numeric comparisons are in the primary references.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Summary citation only; this perspective does not reproduce the exact evaluation protocol, model list, or numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors caution that similar output performance does not guarantee human-like internal reasoning and highlight risks of false positives where correct answers arise from spurious shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integration of cognitive tasks into artificial general intelligence test for large models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of mind may have spontaneously emerged in large language models <em>(Rating: 2)</em></li>
                <li>Emotional intelligence of large language models <em>(Rating: 2)</em></li>
                <li>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval <em>(Rating: 2)</em></li>
                <li>Raven: A dataset for relational and analogical visual reasoning <em>(Rating: 2)</em></li>
                <li>Can AI language models replace human participants? <em>(Rating: 1)</em></li>
                <li>Situational Evaluation of Complex Emotional Understanding (SECEU) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9029",
    "paper_id": "paper-267412707",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4 (ToM)",
            "name_full": "GPT-4 (Theory of Mind evaluations)",
            "brief_description": "GPT-4 evaluated on Theory of Mind style tasks (false-belief and emotion understanding), reported to show an ability to infer others' mental states with patterns similar to human inference.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large autoregressive transformer language model developed by OpenAI (referred to in the paper as an example of modern LLMs).",
            "model_size": null,
            "test_battery_name": "Theory of Mind tests (false-belief tasks, emotion understanding)",
            "test_description": "Classic cognitive-psychology-style Theory of Mind paradigms assessing the model's ability to infer others' beliefs, intentions, or emotions (e.g., false-belief tasks).",
            "llm_performance": "Qualitative: 'demonstrates a certain level of Theory of Mind capability' and 'exhibits ToM capabilities similar to human inference patterns' (no numeric score reported in this paper).",
            "human_baseline_performance": "Not reported in this paper (comparison described qualitatively as 'similar to human inference patterns').",
            "performance_comparison": "LLM performance described as similar to human inference patterns (qualitative claim).",
            "experimental_details": "Cited prior work; specific experimental protocols, prompts, or scoring metrics are not provided in this perspective paper.",
            "limitations_or_caveats": "No numeric metrics or standardized comparison reported here; claim is drawn from cited prior studies (refs cited) and may depend on task framing, prompt engineering, and dataset specifics.",
            "uuid": "e9029.0",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 (Emotional Intelligence / SECEU)",
            "name_full": "GPT-4 (evaluated with the Situational Evaluation of Complex Emotional Understanding, SECEU)",
            "brief_description": "GPT-4 was evaluated in an emotional-understanding battery (SECEU) and reportedly achieved an emotional-intelligence score exceeding the majority of human participants.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large autoregressive transformer language model developed by OpenAI.",
            "model_size": null,
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "A scenario-based emotional understanding battery that elicits complex emotions in varied situations to measure emotional understanding and emotional intelligence.",
            "llm_performance": "Reported: GPT-4 achieved an EQ score that 'surpassed those of 89% of human participants' (percentile comparison reported in the paper).",
            "human_baseline_performance": "Human participants' distribution used as baseline; exact mean/SD not reported here — only that GPT-4 exceeded 89% of participants.",
            "performance_comparison": "GPT-4 outperformed most human participants on this emotional-intelligence metric (above the 89th percentile of the human sample in the cited work).",
            "experimental_details": "Summary result is reported from prior work; this perspective does not provide prompt formats, sample sizes, or scoring rubrics.",
            "limitations_or_caveats": "The paper notes that such cross-subject comparisons can be sensitive to task design and that high scores may reflect different underlying mechanisms (e.g., pattern matching) rather than human-like emotional understanding; full methodological details are in the cited study.",
            "uuid": "e9029.1",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-3 (decision-making / gambling)",
            "name_full": "GPT-3 (evaluated on decision-making/gambling tasks)",
            "brief_description": "Reports on GPT-3's performance on decision-making tasks are mixed: some cited work finds GPT-3 falls short on gambling tasks, while other cited work indicates GPT-3 can outperform humans on some decision tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Large pre-trained autoregressive transformer model (OpenAI GPT-3 family referenced as an early high-capacity LLM).",
            "model_size": null,
            "test_battery_name": "Decision-making tasks (e.g., gambling tasks such as the Iowa Gambling Task / multi-armed bandit variants / other decision paradigms)",
            "test_description": "Cognitive tests assessing decision-making under uncertainty, reward-based choice, and risk evaluation.",
            "llm_performance": "Described as mixed: 'GPT-3's performance falls short of human performance' on gambling tasks (ref. cited), while other cited work reports GPT-3 'outperforms humans in decision-making tasks' in different settings (refs cited). No numeric accuracies/scores provided here.",
            "human_baseline_performance": "Not specified numerically in this paper; referenced comparisons are qualitative across cited studies.",
            "performance_comparison": "Conflicting: some studies report GPT-3 below human baseline on gambling tasks, others report above-human performance on some decision tasks — depends on task and methodology.",
            "experimental_details": "Outcomes depend on the particular decision paradigm and experimental setup in the cited literature; this paper highlights the potential for modality- and task-specific differences and does not provide original experimental protocols.",
            "limitations_or_caveats": "Authors emphasize that discrepancies may arise from task alignment, prompt framing, and differences between human cognitive constraints and model memory/strategy; the mixed findings illustrate sensitivity to experimental design and risk of false positives/negatives.",
            "uuid": "e9029.2",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PaLM vs PaLM 2 (memory)",
            "name_full": "PaLM and PaLM 2 (memory/repetition capacity comparison)",
            "brief_description": "Comparison of memory capacity between PaLM and PaLM 2 with respect to item repetition: PaLM 2 is reported to be inferior to PaLM when repetition count is less than three.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PaLM, PaLM 2",
            "model_description": "Google's Pathways Language Model family (PaLM) and an updated variant (PaLM 2) referenced with respect to memory/retrieval comparisons.",
            "model_size": null,
            "test_battery_name": "Memory/repetition memory test (short-term memory / repetition-based memory capacity)",
            "test_description": "Tests that probe the model's ability to remember and reproduce items across brief repetition counts (working memory-like behavior).",
            "llm_performance": "Qualitative: 'The memory capacity of PaLM 2 is inferior to that of PaLM when the repetition count is less than three.' No numeric scores provided.",
            "human_baseline_performance": "Not provided in this paper.",
            "performance_comparison": "Relative: PaLM &gt; PaLM 2 for low repetition counts, but no human baseline comparison given here.",
            "experimental_details": "Reported result is cited from prior work (ref. cited). The perspective does not provide protocol details such as prompt format, exact tasks, or sample sizes.",
            "limitations_or_caveats": "No numeric metrics or statistical details given; the direction of the effect is reported but underlying reasons (architecture, fine-tuning, prompting) are not specified here.",
            "uuid": "e9029.3",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MiniGPT-4 (Raven examples)",
            "name_full": "MiniGPT-4 (evaluated on Raven's-style visual/physics reasoning examples shown in figures)",
            "brief_description": "MiniGPT-4 is illustrated with two Raven-style examples: one physics question where it answered incorrectly but appeared to grasp the concept, and one image reasoning question where it answered correctly but its internal reasoning was incorrect.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MiniGPT-4",
            "model_description": "A multimodal LMM (MiniGPT-4 used here as an illustrative multimodal model); the perspective presents brief example evaluations with visual reasoning items.",
            "model_size": null,
            "test_battery_name": "Raven's Progressive Matrices / RAVEN visual-relational reasoning (illustrative physics and visual reasoning items)",
            "test_description": "Relational and analogical visual reasoning tasks inspired by Raven's Progressive Matrices, assessing abstract relational reasoning with visual input.",
            "llm_performance": "Qualitative examples: answered a physics Raven item incorrectly (but showed conceptual grasp) and answered an image reasoning item correctly while exhibiting an incorrect reasoning process — no aggregate accuracy reported.",
            "human_baseline_performance": "Not provided for these illustrative examples.",
            "performance_comparison": "Mixed/ambiguous — examples illustrate potential false negatives (apparent failure despite concept grasp) and false positives (correct answer with incorrect reasoning).",
            "experimental_details": "Illustrative examples presented in figure captions; exact prompt/context, item selection, and scoring are not given in the main text.",
            "limitations_or_caveats": "Authors use these examples to highlight pitfalls: correct outputs do not always reflect correct internal reasoning (risk of false positives), and misalignment of input modalities can yield false negatives.",
            "uuid": "e9029.4",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CogEval",
            "name_full": "CogEval (Evaluating cognitive maps and planning in LLMs)",
            "brief_description": "CogEval is a prior evaluation methodology that probes cognitive maps and planning abilities of large language models, reporting that LLMs often lack comprehension of underlying relational structures in planning problems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large language models (various, unspecified)",
            "model_description": "Generic reference to LLMs evaluated by CogEval (authors of this perspective cite CogEval results summarizing deficits in relational/planning comprehension).",
            "model_size": null,
            "test_battery_name": "CogEval (cognitive maps and planning tasks)",
            "test_description": "A set of cognitive tasks designed to evaluate map-like representations, planning, and relational reasoning in sequential decision problems.",
            "llm_performance": "Reported finding: LLMs 'lack comprehension of the underlying relational structures within planning problems' (qualitative summary from cited work).",
            "human_baseline_performance": "Not reported in the summary in this perspective article.",
            "performance_comparison": "LLMs underperform relative to the normative expectation of relational comprehension in planning contexts (qualitative claim).",
            "experimental_details": "Summary citation; the perspective does not reproduce CogEval's exact protocols or numeric results.",
            "limitations_or_caveats": "Claims are drawn from CogEval results; specifics (which LLMs, prompt settings) are in the cited paper. Authors use this to motivate broader AGI-style, multimodal, and virtual-community testing.",
            "uuid": "e9029.5",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CogBench",
            "name_full": "CogBench (continuous psychological tests for LLMs)",
            "brief_description": "CogBench is a framework conducting continuous psychological tests on large models via dynamic information flow to assess cognitive levels; referenced as an approach rather than reporting specific scores here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large language models (various, unspecified)",
            "model_description": "Generic set of LLMs that can be evaluated in CogBench; the perspective cites CogBench as an example of continuous psychological testing pipelines for models.",
            "model_size": null,
            "test_battery_name": "CogBench continuous psychological test suite",
            "test_description": "A platform/framework for running dynamic, ongoing psychological-style assessments on LLMs to probe cognitive attributes over time.",
            "llm_performance": "Not reported in this perspective; CogBench is referenced as a methodological example rather than to present specific performance numbers.",
            "human_baseline_performance": "Not provided here.",
            "performance_comparison": "Not applicable in this paper (methodological mention).",
            "experimental_details": "Referenced as an existing platform (citation given); no experimental details reproduced here.",
            "limitations_or_caveats": "Perspective notes that continuous psychological testing is useful but does not provide dataset-level results; users should inspect CogBench primary source for specifics.",
            "uuid": "e9029.6",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "gpt-3.5-turbo-0613 (CoT example)",
            "name_full": "gpt-3.5-turbo-0613 (chain-of-thought / reflection example)",
            "brief_description": "The perspective illustrates that gpt-3.5-turbo-0613 fails on a triangle-angle reasoning problem without inspiration/adaptation, but with chain-of-thought (CoT) and reflection it produces a correct response.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_description": "A GPT-3.5-series chat model (specific revision 0613 referenced in figure captions) used in illustrative prompting examples.",
            "model_size": null,
            "test_battery_name": "Adaptation/Inspiration reasoning example (triangle-angle problem)",
            "test_description": "A small deductive reasoning problem requiring multi-step inference; used to illustrate the effect of chain-of-thought prompting and reflection.",
            "llm_performance": "Without CoT/reflection: incorrect; with CoT/reflection: correct (illustrative example). No formal accuracy aggregate provided.",
            "human_baseline_performance": "Not provided for this toy example.",
            "performance_comparison": "Shows that prompt engineering (CoT/reflection) can change model performance from incorrect to correct on specific reasoning tasks; not a population-level comparison to humans.",
            "experimental_details": "Authors note use of chain-of-thought, reflection, and adaptation/inspiration techniques; example based on gpt-3.5-turbo-0613 as stated in the figure caption.",
            "limitations_or_caveats": "This is an illustrative demonstration rather than a controlled experiment; it underscores sensitivity to prompting and the risk of false negatives if appropriate reasoning aids are not provided.",
            "uuid": "e9029.7",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "RAVEN visual reasoning (general claim)",
            "name_full": "RAVEN (Relational and Analogical Visual rEasoNing dataset)",
            "brief_description": "RAVEN is a visual-relational benchmark adapting Raven's Progressive Matrices to images; the perspective notes that on RAVEN, some models have demonstrated reasoning capabilities similar to humans (cited prior work).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified vision-and-reasoning models (various)",
            "model_description": "Models evaluated on the RAVEN dataset for visual relational/analogical reasoning; specific model names and sizes are not detailed in the perspective's summary.",
            "model_size": null,
            "test_battery_name": "RAVEN (visual-relational Raven-style benchmark)",
            "test_description": "Dataset combining visual perception with abstract relational reasoning (Raven-like matrix reasoning but in image form).",
            "llm_performance": "Paper states that in this visual reasoning task 'models have demonstrated reasoning capabilities similar to those of humans' per cited studies; no numeric performance values are given in this perspective.",
            "human_baseline_performance": "Not provided here; comparison is described qualitatively (similarity to humans in reasoning capability reported in cited work).",
            "performance_comparison": "Cited reports claim models' reasoning on RAVEN can be similar to human performance in some measures; details and numeric comparisons are in the primary references.",
            "experimental_details": "Summary citation only; this perspective does not reproduce the exact evaluation protocol, model list, or numeric results.",
            "limitations_or_caveats": "Authors caution that similar output performance does not guarantee human-like internal reasoning and highlight risks of false positives where correct answers arise from spurious shortcuts.",
            "uuid": "e9029.8",
            "source_info": {
                "paper_title": "Integration of cognitive tasks into artificial general intelligence test for large models",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Emotional intelligence of large language models",
            "rating": 2,
            "sanitized_title": "emotional_intelligence_of_large_language_models"
        },
        {
            "paper_title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
            "rating": 2,
            "sanitized_title": "evaluating_cognitive_maps_and_planning_in_large_language_models_with_cogeval"
        },
        {
            "paper_title": "Raven: A dataset for relational and analogical visual reasoning",
            "rating": 2,
            "sanitized_title": "raven_a_dataset_for_relational_and_analogical_visual_reasoning"
        },
        {
            "paper_title": "Can AI language models replace human participants?",
            "rating": 1,
            "sanitized_title": "can_ai_language_models_replace_human_participants"
        },
        {
            "paper_title": "Situational Evaluation of Complex Emotional Understanding (SECEU)",
            "rating": 2,
            "sanitized_title": "situational_evaluation_of_complex_emotional_understanding_seceu"
        }
    ],
    "cost": 0.01844175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Integration of cognitive tasks into artificial general intelligence test for large models</p>
<p>Youzhi Qu 
Department of Biomedical Engineering
Southern University of Science and Technology
518055ShenzhenChina</p>
<p>Chen Wei 
Department of Biomedical Engineering
Southern University of Science and Technology
518055ShenzhenChina</p>
<p>Penghui Du 
Department of Biomedical Engineering
Southern University of Science and Technology
518055ShenzhenChina</p>
<p>Wenxin Che 
Chi Zhang 
Department of Biomedical Engineering
Southern University of Science and Technology
518055ShenzhenChina</p>
<p>Wanli Ouyang 
Department of Biomedical Engineering
Southern University of Science and Technology
518055ShenzhenChina</p>
<p>Shanghai AI Laboratory
200232ShanghaiChina</p>
<p>Yatao Bian 
Tencent AI lab
518057ShenzhenChina</p>
<p>Feiyang Xu 
iFLYTEK AI Research
518057ShenzhenChina</p>
<p>Bin Hu 
School of Medical Technology
Beijing Institute of Technology
100081BeijingChina</p>
<p>Kai Du 
Institute for Artificial Intelligence
Peking University
100871BeijingChina</p>
<p>Haiyan Wu 
Centre for Cognitive and Brain Sciences and Department of Psychology
University of Macau
999078MacauChina</p>
<p>Jia Liu 
Department of Psychology
Tsinghua University
100084BeijingChina</p>
<p>Quanying Liu 
Department of Biomedical Engineering
Southern University of Science and Technology
518055ShenzhenChina</p>
<p>Integration of cognitive tasks into artificial general intelligence test for large models
9A3F148CA6465D7F01FD84EC1A87E0A6
During the evolution of large models, performance evaluation is necessarily performed to assess their capabilities and ensure safety before practical application.However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models.In this perspective, we advocate for a comprehensive framework of cognitive science-inspired artificial general intelligence (AGI) tests, aimed at fulfilling the testing needs of large models with enhanced capabilities.The cognitive science-inspired AGI tests encompass the full spectrum of intelligence facets, including crystallized intelligence, fluid intelligence, social intelligence, and embodied intelligence.To assess the multidimensional intelligence of large models, the AGI tests consist of a battery of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community.We propose increasing the complexity of AGI testing tasks commensurate with advancements in large models and emphasizing the necessity for the interpretation of test results to avoid false negatives and false positives.We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have made impressive progress in a short time, reaching a high level of proficiency in human language, 1 mathematics, 2,3 physics, 4 biology, 5,6 and clinic, [7][8][9] which illuminates the path towards artificial general intelligence (AGI).AGI refers to an intelligent agent with the same or higher level of intelligence as humans, capable of solving a variety of complex problems across diverse domains. 10,11As the general capabilities of LLMs continue to evolve, their performance in conventional language tasks and datasets is exhibiting a ceiling effect. 12This suggests that these evaluation methods are increasingly inadequate for assessing the diverse abilities of large models.Large models refer to neural networks with an extensive number of parameters, including large vision models such as SAM 13 and large language models like GPT. 14 Due to the exceptionally high costs of training a large model from scratch, it is crucial to evaluate the capabilities of the intermediate models during the evolution of the large model.This approach can help design and adjust training strategies promptly, thereby reducing the expenses of training large models.A united framework of AGI tests, beyond the traditional Turing Test, offers a comprehensive assessment of the model's ability and guides the evolution of large models.</p>
<p>Cognitive science is a discipline focused on the study of cognition and intelligence.Prior to the advent of LLMs, the field of cognitive science has been actively exploring ability assessment techniques.</p>
<p>With several decades of experience in intelligence assessment, cognitive science has cultivated a robust, multidimensional system for human intelligence assessment.This system extends across crystallized, fluid, social, and embodied intelligence.The theories of intelligence and methodologies for intelligence assessment developed in cognitive science offer innovative approaches for evaluating large models, beyond traditional natural language tasks. 15Integrating task paradigms and methods commonly used for cognitive assessment into the evaluation of large models not only improves our comprehension of model intelligence but also guides the direction of evolution, improves training efficiency, and accelerates progress towards the ultimate goal of AGI.</p>
<p>During the evolution of large models, a remarkable amplification of their capabilities ensues, prompting their consciousness toward the world and themselves.LLMs have empowered various domains, including mathematics 2 and medicine. 7,8As LLMs continue to evolve, their integration into human society is becoming increasingly prevalent.If LLMs contain cognitive biases, hallucinations, or deliberately attempt deception, or even worse, are employed for malicious purposes, the potential harm to society could be catastrophic. 16The artificial intelligence (AI) community is fervently exploring and devising methods.Platforms for evaluating LLMs, such as EPP 17 and OpenCompass, 18 are being developed to gain a more comprehensive understanding of LLMs' capabilities.Approaches such as reinforcement learning from human feedback (RLHF) encourage LLMs to generate responses more congruent with human values or preferences. 19However, human annotators might not only lead LLMs to generate sycophantic content that caters to human viewpoints but also overlook implicit biases in LLMs. 20Among these biases, explicit bias is relatively straightforward to identify and rectify, while implicit bias is more elusive.LLMs may express implicit bias through subtly crafted prompts, leading to discriminatory discourse with serious consequences.Cognitive science has developed numerous experimental paradigms and assessment scales to help detect and identify human biases, false memories, and sycophancy issues, such as the Implicit Association Test (IAT), 21 Deese-Roediger-McDermott (DRM), 22,23 and Social Desirability Scales. 24Integrating cognitive assessment systems into the evaluation of large models can provide invaluable insights into understanding them, thereby enhancing the safety of their applications.</p>
<p>In this perspective, we advocate for the cognitive science-inspired AGI tests.Such AGI tests, grounded in cognitive science, can offer a comprehensive assessment of various capabilities and a thorough evaluation of ethical principles, mental states, and personality traits in LLMs. 25 Drawing on theories and empirical studies in cognitive science, intelligence assessments can be formulated from diverse aspects, including abstract thinking, 26,27 complex reasoning, 28 comprehension in environments, 29,30 creative thinking, 31 social cognition, 32,33 and moral awareness. 34The cognitive scienceinspired AGI tests align with the concept of infinite tasks, as they are not restricted to a fixed number of tasks. 35We propose expanding beyond natural language tasks to include broader and more natural tasks within virtual communities for the construction of AGI tests.Conducting AGI tests in virtual communities includes assessments of different dimensions of intelligence, such as causal understanding and embodiment, as well as more complex tests involving multidimensional intelligence.This necessitates that LLMs not only excel in natural language tasks such as language comprehension but also exhibit a deeper understanding of the world.It reflects complex reasoning, creative thinking, social cognition, and moral awareness, among other cognitive capabilities, in LLMs.The cognitive scienceinspired AGI tests will more accurately assess the performance and safety levels of LLMs in scenarios involving interaction with humans.</p>
<p>Evaluating the capabilities of large models</p>
<p>From language tests to cognitive tests</p>
<p>LLMs have demonstrated remarkable proficiency in accomplishing both "pretext tasks" and "downstream tasks".In the pretext task, LLMs such as GPT 1,14 and BERT 36 can learn language representations from large-scale texts without the need for manual annotation through self-supervised learning methods.The foundational language knowledge acquired by LLMs demonstrated zero-shot generalization capabilities.This facilitates their broad applicability across various downstream tasks, 37 such as understanding tasks, [38][39][40] generation tasks, 41,42 and reasoning tasks, 43,44 as shown in Table 1.</p>
<p>Furthermore, LLMs have extended their language ability to encompass cognitive capabilities, such as few-shot learning, 14 in-context learning, 45 problem solving. 46,479][50][51][52] Interestingly, the emergence of "advanced intelligence" is not a result of deliberate training on specific tasks but rather a natural consequence of the pre-training process using extensive amounts of textual data.</p>
<p>Initiating training for LLMs from scratch requires significant time and computational resources.To circumvent potentially expensive and ineffective training, it is conventional to periodically evaluate the capabilities of LLMs during the training process, thereby enabling timely adaptations of training strategies.However, relying solely on language tasks fails to provide a comprehensive evaluation of the capabilities of LLMs.A lower loss in language tasks does not necessarily indicate a higher level of intelligence.There is a need to bridge the gap between language tasks testing and general intelligence evaluation, transitioning from language tests to cognitive tests, and ultimately to AGI test, as shown in  provide valuable insights into comprehending and evaluating the cognitive abilities of LLMs.For instance, a commonly employed approach in the assessment of "working memory" is the n-back task.</p>
<p>This task is like a memory game where participants judge whether a new stimulus matches one from the previous n stimuli. 53Certain cognitive tasks that were traditionally employed to assess human or animal cognition are now being utilized to evaluate the cognitive capabilities of LLMs. 54,55Some pioneering work reports that LLMs have demonstrated human-like performance. 54,56,57For instance, Theory of mind (ToM) has been applied to assess large models, revealing that GPT-4 exhibits ToM capabilities similar to human inference patterns. 48,56,58In research on embodied cognition, GPT-4 has shown perceptual boundaries more similar to humans. 57There have even been suggestions to utilize LLMs as substitutes for human participants in cognitive experiments. 59Although tests based on indicator properties derived from the science of consciousness have not yet shown large models exhibiting consciousness, theories of consciousness in cognitive science provide empirical support for assessing consciousness in artificial intelligence. 60Incorporating cognitive science knowledge into language tasks represents one approach, while evaluating the intelligence levels of large models through cognitive tasks is also feasible.CogEval assesses the cognitive maps and planning abilities of large models based on cognitive tasks, revealing that LLMs lack comprehension of the underlying relational structures within planning problems. 61ditionally, CogBench conducts continuous psychological tests on large models through dynamic information flow to assess their cognitive levels. 62The Situational Evaluation of Complex Emotional Understanding (SECEU) sets up various scenarios to evoke a range of complex emotions, creating emotional understanding tasks applicable to both humans and large models. 63Emotional intelligence is evaluated based on the emotional understanding capacity in each scenario, revealing that the majority of LLMs achieved emotional intelligence scores above the average level, with GPT-4 surpassing 89% of human participants. 63These findings provide empirical evidence that cognitive tasks possess the capacity to assess specific cognitive levels to some extent in LLMs.Sentiment analysis Identify and categorize opinions expressed in a text IMDb, 73 Yelp, 74 GLUE, 70 SST-2 75 Language modeling Predict the next word or character in a sequence</p>
<p>WikiText-103, 76 Penn Treebank, 64 The Pile, 77 LAMBADA 78 Question answering Answer questions based on a given context Natural Questions, 79 TriviaQA, 80 HotpotQA, 81 WikiQA, 82</p>
<p>Reasoning</p>
<p>Knowledge reasoning</p>
<p>Reason over structured knowledge CSQA, 43 StrategyQA, 93 SocialIQA, 94 CConS, 95 SummEdits 96</p>
<p>Symbolic reasoning</p>
<p>Reason over symbols following formal rules Big-bench, 97 PAL, 98 TabFact 99</p>
<p>Mathematical reasoning</p>
<p>Solve mathematical problems based on text description MMLU, 100 GSM8k, 101 SVAMP, 102 MathQA, 103 AQUA-RAT, 104 MathVista, 105 STEM 106</p>
<p>From cognitive tests to AGI tests</p>
<p>Cognitive tasks have proven effective in assessing the cognitive abilities of models. 54Research on cognitive functions of AI models has progressed from focusing solely on small-scale models designed for specific tasks, including recurrent neural networks (RNNs), convolutional neural networks (CNNs), and spiking neural networks (SNNs), to encompassing LLMs such as GPT 1,14 and PaLM, 107 as shown in the Table 2. Typically, specific cognitive tasks target only a single aspect of intelligence, lacking in providing a comprehensive intelligence assessment for LLMs.Human intelligence is both comprehensive and multifaceted, encompassing knowledge accumulation and application, logical reasoning, social interaction, and environmental adaptation.To achieve a more comprehensive understanding and perform a quantitative assessment of model intelligence, broadening the evaluation scope is essential.This can be achieved by incorporating diverse evaluation methods from cognitive science, including intelligence quotient (IQ) tests, emotional quotient (EQ) tests, and assessments of embodied intelligence.IQ tests, designed to quantify and assess human intelligence, typically involve tasks such as problem-solving, logical reasoning, and mathematics. 108Emotional intelligence plays a crucial role in the development of comprehensive intelligence and encompasses the capacities such as self-emotion recognition, understanding, and regulation. 109Embodied intelligence offers a fresh perspective on the understanding of intelligence, emphasizing natural interaction with the environment. 110,111A comprehensive intelligent agent requires not solely a high level of IQ and EQ, but also the ability to perceive and interact with the environment.Therefore, AGI tests should adopt a holistic and multidimensional approach to evaluating the intelligence of LLMs.Decision-making Gambling GPT-3's performance falls short of human performance. 54</p>
<p>GPT-3</p>
<p>Information search Horizon task GPT-3 exhibits the capacity to make rational decisions when provided with option descriptions. 54</p>
<p>GPT-3</p>
<p>Deliberation Two-step task GPT-3 exhibits a preference for intuitive answers. 54T-3, GPT-4</p>
<p>Causal reasoning Causal reasoning task 119 GPT-3 has difficulties with causal reasoning, 54 but GPT-4 demonstrates remarkable capabilities in causal analysis across different domains.GPT-3 outperforms humans in decision-making tasks. 55</p>
<p>GPT-3 Decision-making Lexical decision</p>
<p>The semantic activation patterns of GPT-3 are similar to humans. 121aLM, PaLM 2</p>
<p>Memorization</p>
<p>Memory test</p>
<p>The memory capacity of PaLM 2 is inferior to that of PaLM when the repetition count is less than three. 122</p>
<p>GPT-4</p>
<p>Theory of mind False belief test, emotion understanding GPT-4 demonstrates a certain level of Theory of mind capability, being able to infer the mental states of others. 48,56,58nstructGPT, LLaMA, GPT-3, GPT-4</p>
<p>Self-knowledge</p>
<p>Self-knowledge test Although GPT-4 surpasses GPT-3, InstructGPT, and LLaMA in self-awareness capability, it still falls short of human-level self-knowledge recognition. 123LaMa, GPT-4</p>
<p>Emotion</p>
<p>Emotion recognition, emotion understanding GPT-4 achieved an EQ score that surpassed those of 89% of human participants. 63</p>
<p>A framework for AGI tests from the cognitive perspective</p>
<p>We advocate the construction of novel AGI tests framework, inspired by cognitive science perspectives.To accurately measure an agent's intelligence level, it is crucial to recognize that a comprehensive agent exhibits not just a single type of intelligence, but a more complex and multidimensional form of intelligence.The field of cognitive science has long been dedicated to exploring the complexity and diversity of human intelligence, and developing well-established evaluation methods to measure human intelligence such as crystallized intelligence, fluid intelligence, social intelligence, and embodied intelligence.Building upon these insights, we propose that the cognitive science-inspired AGI tests should assess intelligence levels from these four dimensions.Table 3 outlines the definitions of the four types of intelligence and corresponding tasks in the fields of cognitive science and artificial intelligence.These tests together pave the way to a united framework for AGI tests.</p>
<p>Crystallized intelligence is the foundational ability acquired through the accumulation of extensive knowledge and experience, and it is not easily subject to loss.The characteristics of crystallized intelligence, as suggested by its name, are as stable and fixed as a crystal.LLMs have already shown powerful "crystallized intelligence" in tasks such as language understanding and generation. 1,48Despite its strengths, crystallized intelligence lacks flexibility.Therefore, it is imperative to consider another important dimension of intelligence, namely "fluid intelligence".As its name suggests, fluid intelligence is a type of intelligence that can flow like a liquid, changing its shape to adapt to new environments.In contrast to crystallized intelligence, fluid intelligence is not reliant on empirical knowledge or information.It mainly involves the capacity for adaptability and flexibility in new situations, including advanced cognitive abilities such as creative thinking, problem solving, and logical reasoning. 124But crystallized intelligence and fluid intelligence alone cannot fully reflect capabilities in social interactions.</p>
<p>Considering that models will increasingly interact and collaborate with humans in the future, it is also necessary to incorporate "social intelligence" in the AGI tests.Social intelligence mainly involves the ability to comprehend oneself and others, as well as to handle complex social scenarios.It includes understanding and interpreting the behaviors and intentions of others, as well as adjusting one's behavior in different social settings.Moreover, AGI tests also need to consider the model's "embodied intelligence", which is primarily concerned with the body and its role in cognition, involving the ability to perceive, adapt, and interact with the environment.Since 1999, a series of empirical studies have consistently supported the theory of embodied cognition, revealing the fundamental role of perceptions and action experiences in human cognition. 125,126 propose a novel AGI tests framework that surpasses the traditional Turing Test, offering a tool to thoroughly analyze its performance across various dimensions of intelligence.It allows for a comprehensive measurement of the multidimensional intelligence of an AI agent.This evaluation method addresses the limitations of traditional intelligence tests, which often focus only on specific skills or knowledge areas, such as memory, logical reasoning, or vocabulary comprehension. 109,127,128We advocate that AGI tests should encompass a range of abilities, including crystallized intelligence, fluid intelligence, social intelligence, and embodied intelligence, in order to assess the comprehensive capabilities of models more accurately.By examining the capabilities of models across diverse tasks, we can more objectively and directly assess the performance of models in practical application scenarios, as well as potential risks.The AGI tests play a crucial role in facilitating the evolution and ensuring the safety of large models, especially as they are increasingly deployed in various human work scenarios.Part-of-speech tagging, 64 named entity recognition 40</p>
<p>Comprehension</p>
<p>Nelson-Denny Reading Test, 131 Peabody Individual Achievement Test 132 Text summarization, 41 question answering 79</p>
<p>Fluid</p>
<p>Adaptability to new problems and environments</p>
<p>Reasoning</p>
<p>Raven's Progressive Matrices, 108 deductive reasoning task 133 Knowledge reasoning， 43 mathematical reasoning， 44 automated planning 134 Decision-making Multi-armed bandit, twoalternative forced choice, 135 Iowa gambling task 136 Game AI, 137,138 legal judgment, 139 AIassisted decision making 140 Social Environmental and social understanding Emotion Facial emotion recognition, 141 emotion regulation task 142 Emotion recognition, 143 emotion understanding, 63 sentiment analysis 144 Social perception False belief task, 145 implicit association test 146 Social reasoning, 56 social media analysis, 147 chatbot 1</p>
<p>Embodied</p>
<p>Environmental perception and interaction</p>
<p>Vision and motion Object recognition task, 148 motor task 149 Object detection, 150 robot locomotion 151 Navigation Maze navigation, 152 wayfinding task 153 Simultaneous localization and mapping, 154 autonomous driving 155</p>
<p>Implementation of AGI tests</p>
<p>Virtual community integration testing: interpretation from the perspective of cognitive science</p>
<p>AGI tests should evaluate the model's overall capabilities across various dimensions and scenarios, rather than focusing solely on performance in specific tasks.To achieve enhanced realism, cognitive science has implemented immersive virtual reality (VR) technology in cognitive task experiments. 156The consistency of test results between VR environments and real laboratory settings validates the effectiveness of VR in assessment tests. 157However, these in-lab VR experiments often fall short of authentically evaluating social interactions due to their limited scenarios.Virtual communities provide an ideal environment for AGI tests, with the metaverse being uniquely characterized by enhanced interactivity. 158,159Immersive virtual environments enable models to interact naturally with humans or other models, without the need for either party to know the other's real identity.Conducting AGI tests in an open virtual community meets the criterion of self-driven task generation, where large models can freely explore the environment, autonomously interact with it, and spontaneously execute tasks. 35The metaverse not only offers a more authentic assessment of a model's real-world problem-solving capacity but also facilitates the development and safety of large models in anticipation of their deep future integration with humans.</p>
<p>Within virtual communities, diverse scenarios and interactive agents can be configured to simulate the complexities and diversities of the real world.As shown in Figure 2, virtual communities provide the models with environments that closely resemble reality, facilitating comprehensive evaluations encompassing crystallized intelligence, fluid intelligence, social intelligence, and embodied intelligence, thereby assessing models in more natural scenarios.Specifically, we can assess a model's crystallized interaction performance in various social scenarios, we assess its social intelligence.(D) In interactive scenarios such as firefighting within virtual communities, the model is required to constantly perceive and interact with its environment.By testing the model's understanding of the environment and its ability to manipulate it, we assess its embodied intelligence.</p>
<p>intelligence using daily conversation and question-answering scenarios in a virtual community (see Figure 2A), focusing primarily on its knowledge mastery and the accuracy and reasonableness of responses.In the case of fluid intelligence, we can simulate an emergency situation, like a fire (see Figure 2B), to evaluate the model's problem-solving strategies and response speed when faced with unknown and complex problems.To evaluate social intelligence, intricate social scenarios, such as group discussions (see Figure 2C), can be employed to appraise the model's performance, including its ability to participate in discussions and adapt to different social scenarios.The assessment of embodied intelligence can be performed in scenarios where the model needs to perceive and interact with the environment in real-time, such as firefighting scenarios (see Figure 2D), which focuses on the model's capabilities in environmental perception and manipulation, as well as its response and outcome in specific tasks.These tests, which encompass multidimensional intelligence, are naturally integrated into the setting of a virtual community.</p>
<p>Key considerations in interpreting AGI test results</p>
<p>To accurately assess the capabilities of large models, it is essential that testing methods are aligned with the model's abilities.As shown in Figure 3, careful consideration must be given to how adaptation and inspiration affect model performance, in order to minimize the risk of misjudgments.If the testing method is not well-suited to the model, this may lead to misunderstandings about the model's In AGI tests, a false negative refers to situations when a model underperforms compared to its actual capability level.This situation may arise when the task is not well-aligned with the model, preventing the model from fully understanding the task requirements and thus failing to fully demonstrate its inherent abilities.These inherent abilities of the model do not rely on fine-tuning or the addition of extra weights.To fully leverage the inherent abilities of a model, guidance such as chain of thought (CoT), 46 tree of thoughts (ToT), 47 black box optimization, 160 and self-reflection 161 can be provided to solve previously unsolvable problems.Alternatively, a model's inadequacy in perceiving or processing certain types of input information can lead to poor task performance.For example, a model with advanced reasoning capabilities may perform poorly on cognitive tasks requiring the parsing of complex images, simply because of its insufficient image processing capabilities (see Figure 4A).It is particularly important to design appropriate test tasks and forms that align with perceptual abilities of the model, in order to avoid false negatives in the model evaluation.</p>
<p>In AGI tests, a false positive occurs when a model's evaluation result exceeds its actual capability.</p>
<p>This phenomenon is often encountered when human exams are used to assess LLMs, which may not accurately reflect the model's abilities.This may be due to the model possessing a memory capacity far exceeding that of humans, making it excel in memory-intensive tasks, even though it may not fully understand the essence of the task.In addition, the model may blindly guess the answer correctly on the test, but its reasoning approach is completely wrong, which is also a typical false positive (see Figure 4B).For a more precise evaluation of visual reasoning capabilities, the Relational and Analogical</p>
<p>Visual rEasoNing (RAVEN) dataset, based on Raven's Progressive Matrices, integrates visual information with reasoning. 162In this visual reasoning task, models have demonstrated reasoning capabilities similar to those of humans. 163,164It is worth noting that the relationships and transferability between various capabilities of a model may differ from those of humans.It is difficult to infer LLMs' performance in other related abilities from their performance in a specific task.For instance, LLMs might excel in language understanding tests, but this does not necessarily imply they possess corresponding logical reasoning abilities.Therefore, we acknowledge that accurately testing a singular type of ability is both difficult and incomplete.A more comprehensive and accurate test should assess the overall performance of an intelligent agent.</p>
<p>New insights from AGI tests</p>
<p>The evolution of multidimensional intelligence</p>
<p>The AGI tests for large models provide potential tools for measuring multidimensional intelligence levels, including crystallized intelligence, fluid intelligence, and embodied intelligence.Based on the assessment results, a variety of learning strategies can be employed to enhance the multidimensional intelligence of large models.For instance, the model's capabilities can be specifically enhanced through internal learning, external guidance, and embodied learning (Figure 5A).Specifically, internal learning involves targeted training to enhance specific abilities of the model, through specially constructed datasets, custom loss functions, and alignment adjustments.AGI tests can uncover the shortcomings of the model, providing a clear indication of abilities requiring improvement.Based on these insights, appropriate strategies or methods may be externally introduced to improve related abilities.For example, a potential approach to enhancing a model's social intelligence is training it to develop comprehension and responsiveness in social situations using a task-specific dataset.Alternatively, inspired by the cognitive process of humans in the tasks, large models can leverage the assistance of CoT, 46 ToT, 47 selfreflection, 161 self-improvement, 165 and self-feedback, 166 as shown in Figure 5B.This approach not only enhances the model's abilities such as reasoning complex problems, but also enables the intelligent agent to display more reasonable individual behavior and social interaction. 167Additionally, embodied learning is an effective way to enhance the intelligence of large models.9][170] Figure 5C depicts three stages of improving model capabilities through embodied learning, including perceiving the environment, interacting with the environment, and obtaining feedback.The embodied AI initially detects an unpreferred state that occurs in its surroundings, such as a forest fire.It then employs crystallized and fluid intelligence to strategize extinguishing the fire through recall and reasoning.The embodied AI can extinguish the fire using a fire extinguisher or by calling the fire department, which demands embodied and social intelligence.After the fire is extinguished and the environment transitions from an unpreferred state to a preferred state, the embodied AI receives a reward and further enhances its crystallized, fluid, social, and embodied intelligence through embodied learning.</p>
<p>The safety of large models in human society</p>
<p>The cognitive science-inspired AGI tests provide a comprehensive and in-depth way to quantify and understand the capabilities of large models in various aspects, thus enhancing their safety.</p>
<p>Effectively identifying bias in model outputs is crucial for assessing the safety of the model.In the construction of datasets to measure biases, the performances of large models are evaluated from cognitive science perspectives, considering aspects such as gender polarity, regard, sentiments, and toxicity. 171Cognitive tasks can assist in identifying risky behaviors for large models.For instance, decision-making tasks can be utilized to assess whether large models can correctly understand problems and make decisions by analyzing logical errors. 137Furthermore, the AGI tests framework can quantify multidimensional intelligence, and aids in understanding the safety and reliability of large models.</p>
<p>Similar to a job interview, this framework assists in identifying the most appropriate application fields for large models. 172For instance, enterprises conduct skills assessments and interviews during the recruitment process to determine if a candidate is suitable for a specific position.Similarly, the AGI tests can help us understand and evaluate large models' applicability, safety, and reliability in different fields.</p>
<p>This guidance allows us to deploy large models in the suitable fields more effectively, optimizing user experience and resource usage.For instance, large models with higher fluid intelligence might be more suitable for volatile environments that require rapid adaptability, such as stock market analysis, while models with higher social intelligence are more apt for handling interpersonal issues, such as customer service.</p>
<p>In critical domains like autonomous driving, medicine, and finance, it is important to ensure not only the safety and reliability of large models, but also their understanding and adherence to social norms, 173 legal and moral principles, 174 and ethical guidelines. 175This requirement aligns with the expectations of professionals, as in these fields, decision-making errors can lead to serious consequences and even legal violations.Therefore, by offering a comprehensive and in-depth evaluation of large models' capabilities and mental states, the AGI tests aid in effectively assessing whether large models can be safely and reliably applied in these critical domains, thereby reducing risks linked to an incomplete understanding of the models.</p>
<p>Conclusion</p>
<p>In this perspective, we advocate a cognitive science-inspired model evaluation framework to test the general intelligence of large models.AGI tests should take into account the complexity and diversity of intelligence, encompassing crystallized intelligence, fluid intelligence, social intelligence, and embodied intelligence.We then brought AGI tests to virtual environments and emphasized some key considerations in interpreting AGI test results.We firmly believe that cognitive science-inspired AGI tests will guide the evolution of large models; in turn, AGI tests on large models will shed light on the evolution of intelligence in biological brains.</p>
<p>Figure 1 .
1
Figure 1.Evaluating capabilities of large models, from language tests (left) to cognitive tests (middle) and AGI tests (right).Language tests, including pretext and downstream tasks of LLMs, demonstrate the efficacy of natural language abilities.Cognitive tests, which assess specific cognitive functions such as decision-making, have recently been incorporated into the evaluation of intelligence in LLMs.The AGI tests from a cognitive perspective offer a comprehensive evaluation of the general intelligence for large models, encompassing crystallized intelligence, fluid intelligence, social intelligence, and embodied intelligence.</p>
<p>Figure 1 .
1
Figure 1.Cognitive science-inspired AGI tests allow for the examination of the true level of intelligence in LLMs.Cognitive science and psychology have pioneered numerous classic cognitive tasks, which</p>
<p>Figure 2 .
2
Figure 2. Illustration of virtual communities integration test from the perspective of cognitive science.(A) In the conversational communication scenario, the model involves itself in everyday conversations or responds to specific inquiries.Through the accuracy of question-answer dialogues, we can assess the model's crystallized intelligence and ascertain its understanding and mastery of knowledge.(B) In emergency scenarios such as fire incidents within the virtual community, the model is presented with a series of unforeseen urgent circumstances.By evaluating the model's response strategies and decisionmaking processes, we can assess the model's fluid intelligence and its ability to handle unknown and complex situations.(C) In the group discussion scenario within the virtual community, the model actively participates in collaborative discussions with other members.By evaluating the model's social</p>
<p>Figure 3 .
3
Figure 3. Illustrating the impact of adaptation and inspiration on model task performance.The left panel outlines the principles of adaptation and inspiration.We suggest that adaptation converts primary downstream tasks into pretext tasks, while inspiration decomposes advanced downstream tasks into gradually solving primary downstream tasks.The right panel demonstrates adaptation and inspiration in practice.When determining the third angle of a triangle given two angles, the model produces erroneous responses in the absence of inspiration or adaptation.However, with techniques such as chain of thought (CoT) and reflection, the model accurately responds.These tests are based on gpt-3.5-turbo-0613.</p>
<p>capabilities.Such misjudgments can manifest in two forms: false negatives and false positives.False negatives occur when a model's capability remains undetected because the test fails to adequately stimulate or demonstrate it.False positives indicate an incorrect attribution of a certain capability to the model, often resulting from test bias or influence from unrelated abilities.</p>
<p>Figure 4 .
4
Figure 4. Examples of model capability misjudgments.(A) A false negative example.We evaluated MiniGPT-4 using a physics question from the Raven's IQ test.Although MiniGPT-4 answered incorrectly, it grasped the concept, suggesting that if the test were better aligned with the model, a satisfactory outcome could be achieved.(B) A false positive example.We assessed MiniGPT-4 with a reasoning question involving images from Raven's IQ test.Despite that MiniGPT-4 answered correctly, its reasoning process to get the answer was totally wrong.</p>
<p>Figure 5 .
5
Figure 5. Three approaches for enhancing model performance.(A) Internal learning involves optimizing models in targeted ways to enhance specific capabilities.This is done through approaches like parameter adjustments and custom loss functions.(B) External guidance incorporates the use of supplemental aids like chain of thought, tree of thoughts, and self-reflection during the model's processing, with the aim of enhancing its performance.(C) Embodied learning leverages interactions and feedback within the environment in three stages: perception, interaction, and feedback.This method serves to enhance the model's crystallized, fluid, social, and embodied intelligence.</p>
<p>Table 1 . Downstream tasks for LLM
1CategoryTaskDescriptionDatasetPart-of-speech taggingLabel each word in a sentence with its part of speechPenn Treebank, 64 Ritter, 65 UD 66UnderstandingNamed entity recognitionIdentify named entities in a textCoNLL-2003, 67 WNUT-2017, 68 OntoNotes 69Textual entailmentDetermine if a sentence logically follows from another oneGLUE, 70 MNLI, 71 RTE 72</p>
<p>Table 2 . Cognitive task evaluation in models
2TypeModelCognitive FunctionCognitive TaskResultRNNDecision-makingPerceptual decision-RNN exhibits representations highly similar to themakingbiological brain. 112RNNTimingTime production taskRNN demonstrates effective capturing of flexibletiming in time intervals. 113RNNNavigationPath integration taskRNN exhibits strong ability in path integrationand can effectively model the neural responses ofgrid cells. 114Task-specificRNNReward-basedValue-based taskRNN captures experimental observations frommodelslearningdiverse cognitive and value-based tasks. 115CNNVisionObject recognition taskRNN proves highly predictive of neural responsesin visual cortex. 116CNNAuditoryAuditory taskCNN demonstrates strong fitting to auditoryprocessing-related cortical areas. 117SNNDecision-makingTwo alternative forced choice taskSNN exhibits excellent performance and dynamic properties in the two alternative forced choice task. 118GPT-3</p>
<p>Table 3 . The dimensions of intelligence and the associated tasks
3IntelligenceDescriptionCategoryCognitive tasksArtificial intelligence tasksPeabody Picture VocabularyCrystallizedAccumulation of knowledge andKnowledgeTest, 129 Expressive Vocabulary Test 130experience
AcknowledgmentsThis work was funded in part by the National Key R&amp;D Program of China (2021YFF1200804), National Natural Science Foundation of China (62001205), Shenzhen Science and Technology Innovation Committee (2022410129, KCXFZ20201221173400001), Shenzhen-Hong Kong-Macao Science and Technology Innovation Project (SGDX2020110309280100), Guangdong Provincial Key Laboratory of Advanced Biomaterials (2022B1212010003).Author contributionsCompeting interestsThe authors declare no competing interests.
GPT-4. Openai, 10.48550/arXiv.2303.087742023Preprint at arXiv</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J Ruiz, J S Ellenberg, P Wang, O Fawzi, Nature. 6252023</p>
<p>MathPrompter: Mathematical Reasoning using Large Language Models. S Imani, L Du, H Shrivastava, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsIndustry Track20235</p>
<p>Incorporating physics into data-driven computer vision. A Kadambi, C Melo, De, C.-J Hsieh, M Srivastava, S Soatto, Nat. Mach. Intell. 52023</p>
<p>Linguistically inspired roadmap for building biologically reliable protein language models. M H Vu, R Akbar, P A Robert, B Swiatczak, G K Sandve, V Greiff, D T T Haug, Nat. Mach. Intell. 52023</p>
<p>Large language models generate functional protein sequences across diverse families. A Madani, B Krause, E R Greene, S Subramanian, B P Mohr, J M Holton, J L OlmosJr, C Xiong, Z Z Sun, R Socher, Nat. Biotechnol. 412023</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. 6202023</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nat. Med. 292023</p>
<p>Health system-scale language models are all-purpose prediction engines. L Y Jiang, X C Liu, N P Nejatian, M Nasir-Moin, D Wang, A Abidin, K Eaton, H A Riina, I Laufer, P Punjabi, Nature. 6192023</p>
<p>Artificial general intelligence. B Goertzel, C Pennachin, 2007Springer</p>
<p>Alien Versus Natural-Like Artificial General Intelligences. H Schneider, P Bołtuć, International Conference on Artificial General Intelligence. 2023</p>
<p>Holistic evaluation of language models. P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, 10.48550/arXiv.2211.091102022Preprint at arXiv</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, 10.48550/arXiv.2304.02643Segment anything. Preprint at arXiv. 2023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 2020</p>
<p>Baby steps in evaluating the capacities of large language models. M C Frank, Nat. Rev. Psychol. 22023</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Comput. Surv. 552023</p>
<p>Interpretable meta-score for model performance. A Gosiewska, K Woźnica, P Biecek, Nat. Mach. Intell. 42022</p>
<p>OpenCompass: A Universal Evaluation Platform for Foundation Models. O Contributors, 2023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 2022</p>
<p>Discovering language model behaviors with model-written evaluations. E Perez, S Ringer, K Lukošiūtė, K Nguyen, E Chen, S Heiner, C Pettit, C Olsson, S Kundu, S Kadavath, 10.48550/arXiv.2212.092512022Preprint at arXiv</p>
<p>Measuring individual differences in implicit cognition: the implicit association test. A G Greenwald, D E Mcghee, J L Schwartz, J. Pers. Soc. Psychol. 741998. 1464</p>
<p>Creating false memories: Remembering words not presented in lists. H L Roediger, K B Mcdermott, J. Exp. Psychol.: Learn. Mem. Cogn. 218031995</p>
<p>On the prediction of occurrence of particular verbal intrusions in immediate recall. J Deese, J. Exp. Psychol. 58171959</p>
<p>A new scale of social desirability independent of psychopathology. D P Crowne, D Marlowe, J. Consult. Psychol. 243491960</p>
<p>Promoting interactions between cognitive science and large language models. Y Qu, P Du, W Che, C Wei, C Zhang, W Ouyang, Y Bian, F Xu, B Hu, K Du, 20245100579</p>
<p>Geometry of abstract learned knowledge in the hippocampus. E H Nieh, M Schottdorf, N W Freeman, R J Low, S Lewallen, S A Koay, L Pinto, J L Gauthier, C D Brody, D W Tank, Nature. 5952021</p>
<p>S A Park, D S Miller, H Nili, C Ranganath, E D Boorman, Map making: constructing, combining, and inferring on abstract cognitive maps. 2020107</p>
<p>Precursors of logical reasoning in preverbal human infants. N Cesana-Arlotti, A Martí N, E Té Glá S, L Vorobyova, R Cetnarski, L L Bonatti, Science. 3592018</p>
<p>Navigating cognition: Spatial codes for human thinking. J L Bellmund, P Gä Rdenfors, E I Moser, C F Doeller, Science. 36267662018</p>
<p>Hippocampal spatial representations exhibit a hyperbolic geometry that expands with experience. H Zhang, P D Rich, A K Lee, T O Sharpee, Nat. Neurosci. 262023</p>
<p>A meta-analysis of 25 years of moodcreativity research: Hedonic tone, activation, or regulatory focus?. M Baas, C K De Dreu, B A Nijstad, Psychol. Bull. 1347792008</p>
<p>The neuroscience of social decision-making. J K Rilling, A G Sanfey, Annu. Rev. Psychol. 622011</p>
<p>Neuro-computational mechanisms and individual biases in action-outcome learning under moral conflict. L Fornari, K Ioumpa, A D Nostro, N J Evans, L De Angelis, S P Speer, R Paracampo, S Gallo, M Spezio, C Keysers, Nat. Commun. 142023. 1218</p>
<p>Moral transgressions corrupt neural representations of value. M J Crockett, J Z Siegel, Z Kurth-Nelson, P Dayan, R J Dolan, Nat. Neurosci. 202017</p>
<p>The tong test: Evaluating artificial general intelligence through dynamic embodied physical and social interactions. Y Peng, J Han, Z Zhang, L Fan, T Liu, S Qi, X Feng, Y Ma, Y Wang, S.-C Zhu, 10.1016/j.eng.2023.07.0062023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterHuman Language Technologies2019</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2024</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, Advances in Neural Information Processing Systems. 2000</p>
<p>Teaching machines to read and comprehend. K M Hermann, T Kocisky, E Grefenstette, L Espeholt, W Kay, M Suleyman, P Blunsom, Advances in Neural Information Processing Systems. 2015</p>
<p>A survey of named entity recognition and classification. D Nadeau, S Sekine, Lingvisticae Investigationes. 302007</p>
<p>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. R Nallapati, B Zhou, C Santos, Ç Gu̇lç Ehre, B Xiang, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language Learning2016</p>
<p>A Neural Attention Model for Abstractive Sentence Summarization. A M Rush, S Chopra, J Weston, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. A Saha, V Pahuja, M Khapra, K Sankaranarayanan, S Chandar, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2018</p>
<p>Analysing Mathematical Reasoning Abilities of Neural Models. D Saxton, E Grefenstette, F Hill, P Kohli, International Conference on Learning Representations. 2019</p>
<p>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?. S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, 10.48550/arXiv.2202.128372022Preprint at arXiv</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 2023</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, 10.48550/arXiv.2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. Preprint at arXiv. 2023</p>
<p>Solving Quantitative Reasoning Problems with Language Models. A Lewkowycz, Advances in Neural Information Processing Systems. 2022</p>
<p>CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. E Nijkamp, B Pang, H Hayashi, L Tu, H Wang, Y Zhou, S Savarese, C Xiong, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Dal Lago, Science. 3782022</p>
<p>Evaluating large language models trained on code. M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, 10.48550/arXiv.2107.033742021</p>
<p>Working memory, attention control, and the N-back task: a question of construct validity. M J Kane, A R Conway, T K Miura, G J Colflesh, J. Exp. Psychol.: Learn. Mem. Cogn. 336152007</p>
<p>Using cognitive psychology to understand. M Binz, E Schulz, GPT-3. PNAS. 1202023. e2218523120</p>
<p>Probing the psychology of AI models. R Shiffrin, M Mitchell, PNAS. 1202023. e2300963120</p>
<p>Theory of mind may have spontaneously emerged in large language models. M Kosinski, 10.48550/arXiv.2302.02083arXiv2023Preprint at</p>
<p>Body size as a metric for the affordable world. X Feng, S Xu, Y Li, J Liu, 10.1101/2023.03.20.5333362023Preprint at arXiv</p>
<p>Understanding social reasoning in language models with language models. K Gandhi, J.-P Frä Nken, T Gerstenberg, N D Goodman, 10.48550/arXiv.2306.154482023Preprint at arXiv</p>
<p>Can AI language models replace human participants?. D Dillion, N Tandon, Y Gu, K Gray, Trends Cognit. Sci. 272023</p>
<p>P Butlin, R Long, E Elmoznino, Y Bengio, J Birch, A Constant, G Deane, S M Fleming, C Frith, X Ji, 10.48550/arXiv.2308.08708arXivConsciousness in artificial intelligence: insights from the science of consciousness. 2023Preprint at</p>
<p>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval. I Momennejad, H Hasanbeig, F Vieira, H Sharma, R O Ness, N Jojic, H Palangi, J Larson, 10.48550/arXiv.2309.15129arXiv2023Preprint at</p>
<p>CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models. Y Lv, H Pan, R Fu, M Liu, Z Wang, B Qin, 10.48550/arXiv.2401.08438arXiv2024Preprint at</p>
<p>Emotional intelligence of large language models. X Wang, X Li, Z Yin, Y Wu, J Liu, J. Pac. Rim Psychol. 172023</p>
<p>Building a Large Annotated Corpus of English: The Penn Treebank. M P Marcus, B Santorini, M A Marcinkiewicz, Comput. Linguist. 191993</p>
<p>Named entity recognition in tweets: an experimental study. A Ritter, S Clark, O Etzioni, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language Processing2011</p>
<p>Universal dependencies v1: A multilingual treebank collection. J Nivre, M.-C De Marneffe, F Ginter, Y Goldberg, J Hajic, C D Manning, R Mcdonald, S Petrov, S Pyysalo, N Silveira, Proceedings of the Tenth International Conference on Language Resources and Evaluation. the Tenth International Conference on Language Resources and Evaluation2016</p>
<p>Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. Tjong Kim, Sang , E F De Meulder, F , Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. the Seventh Conference on Natural Language Learning at HLT-NAACL 20032003</p>
<p>Results of the WNUT2017 shared task on novel and emerging entity recognition. L Derczynski, E Nichols, M Van Erp, N Limsopatham, Proceedings of the 3rd Workshop on Noisy User-generated Text. the 3rd Workshop on Noisy User-generated Text2017</p>
<p>Towards robust linguistic analysis using ontonotes. S Pradhan, A Moschitti, N Xue, H T Ng, A Björkelund, O Uryupina, Y Zhang, Z Zhong, Proceedings of the Seventeenth Conference on Computational Natural Language Learning. the Seventeenth Conference on Computational Natural Language Learning2013</p>
<p>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2018</p>
<p>A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. A Williams, N Nangia, S Bowman, Proceedings of the 2018 Conference of the North American Chapter. the 2018 Conference of the North American ChapterHuman Language Technologies2018</p>
<p>The pascal recognising textual entailment challenge. I Dagan, O Glickman, B Magnini, 2005Springer</p>
<p>Learning word vectors for sentiment analysis. A Maas, R E Daly, P T Pham, D Huang, A Y Ng, C Potts, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies2011</p>
<p>Character-level convolutional networks for text classification. X Zhang, J Zhao, Y Lecun, Advances in Neural Information Processing Systems. 2015</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. R Socher, A Perelygin, J Wu, J Chuang, C D Manning, A Y Ng, C Potts, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language Processing2013</p>
<p>S Merity, C Xiong, J Bradbury, R Socher, 10.48550/arXiv.1609.07843Pointer Sentinel Mixture Models. 2017Preprint at arXiv</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, 10.48550/arXiv.2101.000272020Preprint at arXiv</p>
<p>The LAMBADA dataset: Word prediction requiring a broad discourse context. D Paperno, G Kruszewski, A Lazaridou, N Q Pham, R Bernardi, S Pezzelle, M Baroni, G Boleda, R Ferná Ndez, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics2016</p>
<p>Natural questions: a benchmark for question answering research. T Kwiatkowski, J Palomaki, O Redfield, M Collins, A Parikh, C Alberti, D Epstein, I Polosukhin, J Devlin, K Lee, Trans. Assoc. Comput. Linguist. 72019</p>
<p>TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. M Joshi, E Choi, D Weld, L Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics2017</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Wikiqa: A challenge dataset for open-domain question answering. Y Yang, W.-T Yih, C Meek, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>SQuAD: 100,000+ Questions for Machine Comprehension of Text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>Findings of the 2014 workshop on statistical machine translation. O Bojar, C Buck, C Federmann, B Haddow, P Koehn, J Leveling, C Monz, P Pecina, M Post, H Saint-Amand, Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine Translation2014</p>
<p>Wit3: Web inventory of transcribed and translated talks. M Cettolo, C Girardi, M Federico, Proceedings of the Conference of European Association for Machine Translation. the Conference of European Association for Machine Translation2012</p>
<p>Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. S Narayan, S B Cohen, M Lapata, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Personalizing Dialogue Agents: I have a dog, do you have pets too. S Zhang, E Dinan, J Urbanek, A Szlam, D Kiela, J Weston, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics2018</p>
<p>The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. R Lowe, N Pow, I Serban, J Pineau, Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue2015</p>
<p>Measuring Coding Challenge Competence With APPS. D Hendrycks, S Basart, S Kadavath, M Mazeika, A Arora, E Guo, C Burns, S Puranik, H He, D Song, Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Spoc: Search-based pseudocode to code. S Kulal, P Pasupat, K Chandra, M Lee, O Padon, A Aiken, P S Liang, Advances in Neural Information Processing Systems. 2019</p>
<p>Translating embeddings for modeling multi-relational data. A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko, Advances in Neural Information Processing Systems. 2013</p>
<p>Assessing the factual accuracy of generated text. B Goodrich, V Rao, P J Liu, M Saleh, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. M Geva, D Khashabi, E Segal, T Khot, D Roth, J Berant, Trans. Assoc. Comput. Linguist. 92021</p>
<p>M Sap, H Rashkin, D Chen, R Lebras, Y Choi, 10.48550/arXiv.1904.09728Socialiqa: Commonsense reasoning about social interactions. 2019Preprint at arXiv</p>
<p>Probing Physical Reasoning with Counter-Commonsense Context. K Kondo, S Sugawara, A Aizawa, 10.48550/arXiv.2306.022582023Preprint at arXiv</p>
<p>P Laban, W Kryściński, D Agarwal, A R Fabbri, C Xiong, S Joty, C.-S Wu, 10.48550/arXiv.2305.14540arXivLLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. 2023Preprint at</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. A Srivastava, Transactions on Machine Learning Research. 2023</p>
<p>PAL: Program-aided Language Models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, 10.48550/arXiv.2211.104352022Preprint at arXiv</p>
<p>TabFact: A Large-scale Dataset for Table-based Fact Verification. W Chen, H Wang, J Chen, Y Zhang, H Wang, S Li, X Zhou, W Y Wang, International Conference on Learning Representations. 2020</p>
<p>Measuring Massive Multitask Language Understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2020</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, 10.48550/arXiv.2110.14168Training Verifiers to Solve Math Word Problems. Preprint at arXiv. 2021</p>
<p>Are NLP Models really able to Solve Simple Math Word Problems?. A Patel, S Bhattamishra, N Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. A Amini, S Gabriel, S Lin, R Koncel-Kedziorski, Y Choi, H Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterHuman Language Technologies2019</p>
<p>Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. W Ling, D Yogatama, C Dyer, P Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics2017</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. P Lu, H Bansal, T Xia, J Liu, C Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, 10.48550/arXiv.2310.02255arXiv2023Preprint at</p>
<p>Measuring Vision-Language STEM Skills of Neural Models. J Shen, Y Yuan, S Mirzoyan, M Zhang, C Wang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, 10.48550/arXiv.2204.02311arXiv2022Preprint at</p>
<p>The Raven's progressive matrices: change and stability over culture and time. J Raven, Cogn. Psychol. 412000</p>
<p>Emotional intelligence: in search of an elusive construct. M Davies, L Stankov, R D Roberts, J. Pers. Soc. Psychol. 759891998</p>
<p>Catalyzing next-generation artificial intelligence through neuroai. A Zador, S Escola, B Richards, B Ölveczky, Y Bengio, K Boahen, M Botvinick, D Chklovskii, A Churchland, C Clopath, Nat. Commun. 1415972023</p>
<p>Embodied intelligence via learning and evolution. A Gupta, S Savarese, S Ganguli, L Fei-Fei, Nat. Commun. 1257212021</p>
<p>Context-dependent computation by recurrent dynamics in prefrontal cortex. V Mante, D Sussillo, K V Shenoy, W T Newsome, Nature. 5032013</p>
<p>Flexible timing by temporal scaling of cortical responses. J Wang, D Narain, E A Hosseini, M Jazayeri, Nat. Neurosci. 212018</p>
<p>A unified theory for the computational and mechanistic origins of grid cells. B Sorscher, G C Mel, S A Ocko, L M Giocomo, S Ganguli, Neuron. 1112023</p>
<p>Reward-based training of recurrent neural networks for cognitive and value-based tasks. H F Song, G R Yang, X.-J Wang, 20176e21492</p>
<p>Performanceoptimized hierarchical models predict neural responses in higher visual cortex. D Yamins, H Hong, C Cadieu, E Solomon, D Seibert, J Dicarlo, PNAS. 1112014</p>
<p>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. A J Kell, D L Yamins, E N Shook, S V Norman-Haignere, J H Mcdermott, Neuron. 982018</p>
<p>Dynamics of a Recurrent Spiking Neural Network in the Two-Alternative Choice Task. M Pugavko, O Maslennikov, V Nekorkin, Radiophys. Quantum Electron. 642022</p>
<p>Seeing versus doing: two modes of accessing causal knowledge. M R Waldmann, Y Hagmayer, J. Exp. Psychol.: Learn. Mem. Cogn. 312162005</p>
<p>Capabilities of gpt-4 on medical challenge problems. H Nori, N King, S M Mckinney, D Carignan, E Horvitz, 10.48550/arXiv.2303.13375arXiv2023Preprint at</p>
<p>Overlap in meaning is a stronger predictor of semantic activation in GPT-3 than in humans. J Digutsch, M Kosinski, Sci. Rep. 1350352023</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, 10.48550/arXiv.2305.10403Palm 2 technical report. 2023Preprint at arXiv</p>
<p>Z Yin, Q Sun, Q Guo, J Wu, X Qiu, X Huang, 10.48550/arXiv.2305.18153Do Large Language Models Know What They Don't Know? Preprint at arXiv. 2023</p>
<p>Neural mechanisms of general fluid intelligence. J R Gray, C F Chabris, T S Braver, Nat. Neurosci. 62003</p>
<p>R W GibbsJr, Embodiment and cognitive science. Cambridge University Press2005</p>
<p>Grounded cognition. L W Barsalou, Annu. Rev. Psychol. 592008</p>
<p>Immediate and long-term memory and their relation to crystallized and fluid intelligence. D Martinez, Intelligence. 761013822019</p>
<p>Further examination of emotional intelligence as a standard intelligence: A latent variable analysis of fluid intelligence, crystallized intelligence, and emotional intelligence. C Maccann, Pers. Individ. Differ. 492010</p>
<p>Peabody picture vocabulary test fourth edition. L M Dunn, D M Dunn, 10.1037/t15144-0002007APA PsycTests</p>
<p>Expressive vocabulary test second edition (EVT™ 2). K T Williams, J. Am. Acad. Child Adolesc. Psychiatry. 421997</p>
<p>Denny Reading Test: Manual for Scoring and Interpretation. J Brown, V Fishco, G N Hanna, 1993Riverside PublishingForms G &amp; H (Itasca, IL</p>
<p>Peabody Individual Achievement Test-Revised-Normative Update. F C MarkwardtJr, 10.1037/t15144-000APA PsycTests. 1997</p>
<p>Reasoning about a rule. P C Wason, Q. J. Exp. Psychol. 201968</p>
<p>M Ghallab, D Nau, P Traverso, Automated Planning: theory and practice. Elsevier2004</p>
<p>G T Fechner, Elemente der psychophysik (Breitkopf u. Hä rtel). 1860</p>
<p>Deciding advantageously before knowing the advantageous strategy. A Bechara, H Damasio, D Tranel, A R Damasio, Science. 2751997</p>
<p>Evaluating Superhuman Models with Consistency Checks. L Fluri, D Paleka, F Tramè R, 10.48550/arXiv.2306.099832023Preprint at arXiv</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. 5292016</p>
<p>Human decisions and machine predictions. J Kleinberg, H Lakkaraju, J Leskovec, J Ludwig, S Mullainathan, Q. J. Econ. 1332018</p>
<p>How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies. O Vereschak, G Bailly, B Caramiaux, Proc. ACM Hum.-Comput. Interact. 52021</p>
<p>The development of facial emotion recognition: The role of configural information. K Durand, M Gallay, A Seigneuric, F Robichon, J.-Y Baudouin, J. Exp. Child Psychol. 972007</p>
<p>The emerging field of emotion regulation: An integrative review. J J Gross, Rev. Gen. Psychol. 21998</p>
<p>Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review. J Zhang, Z Yin, P Chen, S Nichele, Inf. Fusion. 592020</p>
<p>Sentiment analysis algorithms and applications: A survey. W Medhat, A Hassan, H Korashy, Ain Shams Eng. J. 52014</p>
<p>John thinks that Mary thinks that…" attribution of secondorder beliefs by 5-to 10-year-old children. J Perner, H Wimmer, J. Exp. Child Psychol. 391985</p>
<p>Measuring individual differences in implicit cognition: The implicit association test. A G Greenwald, D E Mcghee, J L K Schwartz, J. Pers. Soc. Psychol. 741998</p>
<p>Machine learning algorithms for social media analysis: A survey. T Balaji, C S R Annavarapu, A Bablani, Comput. Sci. Rev. 401003952021</p>
<p>Object recognition test in mice. M Leger, A Quiedeville, V Bouet, B Haelewyn, M Boulouard, P Schumann-Bard, T Freret, Nat. Protoc. 82013</p>
<p>The acquisition of skilled motor performance: fast and slow experience-driven changes in primary motor cortex. A Karni, G Meyer, C Rey-Hipolito, P Jezzard, M M Adams, R Turner, L G Ungerleider, PNAS. 951998</p>
<p>Object detection in 20 years: A survey. Z Zou, K Chen, Z Shi, Y Guo, J Ye, Proc. IEEE. IEEE2023111</p>
<p>Bigdog, the rough-terrain quadruped robot. M Raibert, K Blankespoor, G Nelson, R Playter, Proceedings of the 17th World Congress the International Federation of Automatic Control. the 17th World Congress the International Federation of Automatic Control2008</p>
<p>Morris water maze: procedures for assessing spatial and related forms of learning and memory. C V Vorhees, M T Williams, Nat. Protoc. 12006</p>
<p>Wayfinding behavior: Cognitive mapping and other spatial processes. R G Golledge, 1999JHU press</p>
<p>A survey of state-of-the-art on visual SLAM. I A Kazerouni, L Fitzgerald, G Dooly, D Toal, Expert Syst. Appl. 2052022. 117734</p>
<p>Motion planning for autonomous driving: The state of the art and future perspectives. S Teng, X Hu, P Deng, B Li, Y Li, Y Ai, D Yang, L Li, Z Xuanyuan, F Zhu, IEEE Trans. Intell. Veh. 82023</p>
<p>Individual variation in the neurophysiological representation of negative emotions in virtual reality is shaped by sociability. R Wang, R Yu, Y Tian, H Wu, NeuroImage. 2631195962022</p>
<p>Assessing the suitability of virtual reality for psychological testing. A C Roberts, Y W Yeap, H S Seah, E Chan, C.-K Soh, G I Christopoulos, Psychol. Assess. 313182019</p>
<p>The promise and pitfalls of the metaverse for science. D Gómez-Zará, P Schiffer, D Wang, Nat. Hum. Behav. 72023</p>
<p>Development of metaverse for intelligent healthcare. G Wang, A Badal, X Jia, J S Maltz, K Mueller, K J Myers, C Niu, M Vannier, P Yan, Z Yu, Nat. Mach. Intell. 42022</p>
<p>Black-box tuning for languagemodel-as-a-service. T Sun, Y Shao, H Qian, X Huang, X Qiu, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine Learning2022</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, 10.48550/arXiv.2303.113662023Preprint at arXiv</p>
<p>Raven: A dataset for relational and analogical visual reasoning. C Zhang, F Gao, B Jia, Y Zhu, S.-C Zhu, Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. the IEEE/CVF conference on Computer Vision and Pattern Recognition2019</p>
<p>Learning perceptual inference by contrasting. C Zhang, B Jia, F Gao, Y Zhu, H Lu, S.-C Zhu, Advances in Neural Information Processing Systems. 2019</p>
<p>Abstract spatial-temporal reasoning via probabilistic abduction and execution. C Zhang, B Jia, S.-C Zhu, Y Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, 10.48550/arXiv.2303.176512023Preprint at arXiv</p>
<p>Refiner: Reasoning feedback on intermediate representations. D Paul, M Ismayilzada, M Peyrard, B Borges, A Bosselut, R West, B Faltings, 10.48550/arXiv.2304.019042023Preprint at arXiv</p>
<p>J S Park, J C O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, 10.48550/arXiv.2304.03442Generative agents: Interactive simulacra of human behavior. 2023Preprint at arXiv</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, 10.48550/arXiv.2303.033782023Preprint at arXiv</p>
<p>Minedojo: Building open-ended embodied agents with internet-scale knowledge. L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, Advances in Neural Information Processing Systems. 2022</p>
<p>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, 10.48550/arXiv.2307.059732023Preprint at arXiv</p>
<p>Bold: Dataset and metrics for measuring biases in open-ended language generation. J Dhamala, T Sun, V Kumar, S Krishna, Y Pruksachatkun, K.-W Chang, R Gupta, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Gpts are gpts: An early look at the labor market impact potential of large language models. T Eloundou, S Manning, P Mishkin, D Rock, 10.48550/arXiv.2303.101302023Preprint at arXiv</p>
<p>Social behavior for autonomous vehicles. W Schwarting, A Pierson, J Alonso-Mora, S Karaman, D Rus, PNAS. 1162019</p>
<p>When to make exceptions: Exploring language models as accounts of human moral judgment. Z Jin, S Levine, F Gonzalez Adauto, O Kamal, M Sap, M Sachan, R Mihalcea, J Tenenbaum, B Schölkopf, Advances in Neural Information Processing Systems. 2022</p>
<p>The global landscape of AI ethics guidelines. A Jobin, M Ienca, E Vayena, Nat. Mach. Intell. 12019</p>            </div>
        </div>

    </div>
</body>
</html>
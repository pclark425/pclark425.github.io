<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1241 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1241</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1241</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-258479970</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.02749v5.pdf" target="_blank">Explainable Reinforcement Learning via a Causal World Model</a></p>
                <p><strong>Paper Abstract:</strong> Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1241.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1241.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal World Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal World Model via Causal Discovery and Attention-based Inference Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interpretable, learned structural causal model (SCM) for factorized MDPs discovered with conditional-independence tests and implemented with attention-based inference networks; it is convertible to an Action Influence Model (AIM) via action-conditioned attention weights to generate causal chains and natural-language explanations while being accurate enough to use in model-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal world model (SCM -> AIM via attention-based inference networks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The model first performs causal discovery (Fast CIT) to obtain a bipartite causal graph from exogenous inputs u = (s, a) to endogenous outputs v = (s', o). For each output v_j an inference network f_j is trained: variable-specific encoders map heterogeneous inputs to vectors; action variables in the parent set are processed by a GRU to produce an action embedding e_j; queries q_j, state contribution vectors c_j^i and an action contribution c_j^a are computed and combined with attention weights α_j^i and α_j^a to form a hidden representation h_j; a distribution decoder D_j maps h_j to a posterior (Normal for real variables, categorical for discrete). The learned attention (influence) weights are thresholded per-action to derive action-conditional parents and thus convert the SCM to an AIM for explanation and causal-chain extraction. The model is trained by maximizing log-likelihood of transitions and used in MBRL with a 5-model bootstrap ensemble for uncertainty-aware rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>interpretable causal world model (structural causal model convertible to AIM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning domains with factorized state/action: LunarLander-Continuous/Discrete, Cartpole, Build-Marine (StarCraft II mini-game); general Factorized MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Maximum log-likelihood / negative log-likelihood on next-state/outcome posterior distributions (Equation 12); AIM recovery accuracy (%) on a ground-truth AIM testbed; downstream policy returns (learning curves) when used in model-based RL; expected KL divergence between true conditional f_j(PA^*) and inferred f_j(PA) (Appendix C)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>AIM-recovery accuracy on the designed testbed: 99.3% for the paper's causal+attention method (reported in the comparison table). In experiments, the causal world model attains predictive accuracy and MBRL task performance 'very close' to dense baselines (MLP, Full model) across tested environments; Lunarlander explanation model trained with 150k transitions (example). No single numeric MBRL return values are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High interpretability: produces a sparse, visualizable causal graph (bipartite u->v), per-output attention-based influence weights, and multi-step causal chains; enables minimally complete explanans (MCE) and contrastive explanations (MCCE) in natural-language templates. Interpretability is tunable via a threshold τ/η to select salient parents for action-conditional explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Causal discovery using conditional independence tests (Fast CIT) to get a sparse causal graph; attention-based influence weights (key-query-contribution architecture) to extract action-specific influences; causal chain analysis to extract paths from initial state/action to reward variables; template-based natural-language explanan (MCE/MCCE).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model parameter scale O(n) (n = max(n_s + n_a, n_s + n_o)); forward-pass time O(n^2 b) for a batch size b; causal discovery cost O(n^3 N log N) where N is number of transition samples (per paper); causal-chain generation time O(n^2 H) for H-step chains. Example training used up to 150k transitions for LunarLander explanation experiments; MBRL used a 5-model bootstrap ensemble (5 models) for rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More sample- and computation-efficient in practice than the naive Direct AIM-learning baseline (which requires splitting data into |A| subsets and learning |A| SCMs). Causal+attention approach recovers AIM structure more accurately than attention-only on a full graph, and attains task performance close to dense baselines while providing interpretability. Exact wall-clock or GPU compute comparisons are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When plugged into a model-based RL loop (k-step rollouts, PPO), the causal world model yields learning curves 'very close' to dense baselines and substantially faster learning than model-free in some environments (Cartpole, LunarLander-Discrete) and higher converged returns in Build-Marine; no absolute numeric return scores are listed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The model is accurate enough to be concurrently used for faithful explanations and to guide policy learning (i.e., high-enough fidelity translates to good task performance). The causal-discovery step reduces spurious correlations and improves the faithfulness of explanations compared to attention-only approaches; ensemble rollouts mitigate epistemic uncertainty for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Explicit trade-off between interpretability and accuracy: sparser causal graphs (fewer parents) are easier to read but reduce predictive accuracy and thus hurt fidelity; conversely, including more parents improves accuracy but lowers interpretability. The paper discusses thresholding (η/τ) to control sparsity. Note: the paper contains an inconsistent verbal statement about the effect of decreasing the threshold vs. the formal theorem in Appendix C; the theorem formalizes how parent sets and expected KL divergences relate to threshold choices.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Assume factorized MDP and independent-transition (outputs v are conditionally independent given inputs u) to obtain a bipartite causal graph; use Fast CIT for structure discovery; variable-specific encoders to handle heterogeneous inputs; GRU to encode action variables; attention-style key-query-contribution computations to obtain influence weights; thresholding attention to build AIM; distribution decoders per-variable (normal/categorical); 5-model bootstrap ensemble for MBRL rollouts; trained with log-likelihood objective and policy updated by PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to dense MLP baseline: MLP can be more accurate if well-trained but is uninterpretable; causal model attains similar task returns with the benefit of explanations. Compared to Full model (same networks but full graph): Full model tends to be more accurate, but attention+causal-graph combination recovers AIM structure far more cleanly and avoids many spurious correlations (attention-only Full had lower AIM recovery accuracy). Compared to Direct AIM learning: Direct approach is theoretically sound but sample-inefficient and computationally heavy; the proposed two-stage (SCM discovery + attention stage) is more practical and accurate in the experiments (table numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends a two-stage approach: discover a unified SCM first to eliminate spurious correlations, then transfer to an AIM via attention. Thresholds (η/τ) are tuning knobs: larger threshold yields laconic (shorter) explanations while smaller threshold includes more parents; the paper used example thresholds η=0.05 (LunarLander) and η=0.15 (Build-Marine). They also recommend a small ensemble (they use 5 models) to mitigate epistemic uncertainty in model-based rollouts. No single 'optimal' universal configuration is given; trade-offs must be tuned per task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1241.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1241.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Influence Model (AIM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal model specialized for RL that associates each action with an action-conditional set of structural equations (an ensemble of SCMs, one per action) to represent how actions causally influence environment variables and rewards, enabling causal-chain explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Action Influence Model (AIM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>AIM defines, for each action a and each endogenous variable v_j, a structural equation f_j^a that specifies Pr(v_j | PA^a(v_j)). Naively this requires learning |A| distinct SCMs (|A| × |V| equations). In this paper the AIM concept is realized by converting a unified SCM into an action-conditional AIM via attention weights: parents with attention α_j^i > τ under a specific action are considered salient parents PA^a(v_j).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>action-conditional causal world model (ensemble of SCMs per action)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning domains for explanation of action consequences; historically used for discrete-action RL explanation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>AIM recovery accuracy (%) when comparing discovered action-conditional parents to ground-truth AIM; quality of causal chains and resultant explanations; predictive log-likelihood under action-conditioned structural equations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the paper's recovery experiment the Direct AIM baseline showed ~97.0% accuracy, the attention-only Full graph ~90.0%, and the proposed causal+attention conversion achieved ~99.3% AIM recovery accuracy on a designed test environment with spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Highly interpretable by construction because each action has an explicit causal graph and causal chains can be traced; however naive Direct AIM learning is impractical for large/infinite action spaces and suffers sample inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Explicit action-conditional causal graphs and causal chains; thresholding of influence weights to create laconic explanations (MCE/MCCE); can be combined with decision trees per Madumal et al. (prior work) for improved explanation format.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Naive Direct AIM learning requires splitting data into |A| subsets and learning |A| SCMs, leading to poor sample efficiency and high computational cost; conversion from SCM via attention (proposed here) avoids the need to learn |A| separate SCMs and supports continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Proposed SCM->AIM conversion via attention is more sample- and computation-efficient than Direct AIM learning; attention-only approaches without causal discovery are less reliable (more spurious links) and thus less effective for explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>AIM is primarily an explanatory model; prior AIM implementations (cited in paper) were low-accuracy and unsuitable for policy learning. The converted AIM in this paper is accurate enough to be used for both explanation and to support MBRL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>AIM provides direct, contrastive and causal-chain explanations that align with human causal reasoning. Direct learning of AIM is often impractical; the conversion approach keeps AIM utility while being tractable for continuous action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>AIM yields high interpretability but naive AIM learning trades interpretability for impractical sample requirements; the paper's approach balances tractability and interpretability at a small cost to raw predictive power compared to dense models.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Instead of learning separate SCMs per action, learn a unified SCM and use attention-conditioned thresholding to produce action-specific parent sets; choose τ to trade off laconic explanations versus completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to unified SCMs: unified SCMs are compact and learnable; compared to Direct AIM: more scalable; compared to attention-only Full models: causal discovery + attention reduces spurious correlations and improves AIM recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Use SCM discovery first to remove spurious correlations, then derive AIM via attention with threshold τ adjusted for explanation conciseness; for laconic explanations use larger τ, for completeness use smaller τ. No single global optimum is prescribed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1241.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1241.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLP baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Layer Perceptron world model (dense baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense neural world model that concatenates all exogenous variables u and uses an MLP to predict endogenous variables; commonly used in model-based RL but uninterpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MLP world model (concatenated-input dense model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Concatenate all input variables u = (s, a) and feed into a multilayer perceptron that outputs parameters of per-variable predictive distributions for s' and outcomes. No explicit causal structure or attention; fully-connected dense mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>dense neural world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL benchmarks (Cartpole, LunarLander, Build-Marine) used as a baseline in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Predictive accuracy on next-state/outcome (log-likelihood / MSE); implicit comparison via downstream policy returns in MBRL experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper states dense models (including this MLP) are typically more accurate if well-trained; exact numeric predictive metrics for the MLP baseline are not listed. Learning curves indicate MLP and Full model are slightly more accurate but causal model achieves similar returns.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low interpretability (black-box): no causal graph or explicit influence weights; unsuitable for producing causal chains or human-understandable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>none mentioned; post-hoc tools (e.g., saliency) are discussed elsewhere in the paper as inadequate for sequential causal explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified in the paper; typically fewer steps for causal discovery omitted, but no complexity numbers provided specifically for MLP baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Denser models may require similar or more data to train, but can achieve higher predictive accuracy; however they do not support faithful explanations. The paper's causal model achieves comparable task performance while being interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used as comparison in MBRL experiments; dense MLP baseline generally attains competitive task returns, and the causal model's performance is close to it.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good predictive fidelity often translates to good policy performance, but lack of interpretability makes these models unsuitable where explanations are required.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High fidelity / low interpretability trade-off; dense MLP tends to be more accurate but gives no causal explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Simple dense MLP architecture with concatenated inputs (standard baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to causal world model: MLP is less interpretable but similar or slightly higher predictive accuracy; however lacks the causal-chain explanation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in detail in the paper; used as a strong baseline to quantify the interpretability-accuracy trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1241.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1241.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full model (Full+Attn)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-graph modular model with attention (dense graph + attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that uses the same inference-network architecture as the causal model but assumes a full causal graph (every input can be parent of every output) and relies purely on attention to select influential inputs; less protected from spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Full model (modular networks with full graph + attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses variable encoders and attention-based inference networks identical to the proposed architecture but uses a full u->v graph (no causal discovery). The attention mechanism alone determines influence weights; no explicit sparsification by causal discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>dense/modular neural world model (full graph)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Same RL benchmarks (LunarLander, Build-Marine, Cartpole) as in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Predictive likelihood; AIM-recovery accuracy when extracting action influence purely from attention on the full graph.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the AIM-recovery experiment, attention-only on the full graph (Full+Attn) achieved about 90.0% accuracy, worse than the paper's causal+attention approach (99.3%). Task returns are similar to other dense baselines; exact numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate interpretability via attention weights, but vulnerable to spurious correlations in non-i.i.d. RL data; causal chains derived from attention-only full graphs often contain unreasonable edges.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Attention weights used to identify salient parents but no causal discovery safeguards.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Same order as causal model forward passes; lacks the additional cost of causal discovery but suffers from lower-quality explanations which may require extra validation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Computationally cheaper than running causal discovery but less effective at producing correct causal explanations; worse AIM recovery accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Comparable to other dense baselines for policy learning; explanation quality worse due to spurious links.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for prediction and policy learning but unreliable for faithful causal explanations in RL due to spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Saves on causal-discovery overhead but trades-off correctness of causal explanations; attention alone is insufficient for robust causal discovery in RL data.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Full input-to-output connectivity with attention-based weighting but without CI-based pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Worse than causal+attention in AIM recovery and explanation faithfulness; similar to MLP in predictive utility but more modular.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not recommended for explanation-critical applications; combining with causal discovery (as in paper) improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1241.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1241.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct AIM learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct approach to learn AIM (per-action causal discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline method that learns an AIM by splitting transition data by action and performing independent causal discovery per action, yielding action-specific SCMs but suffering from poor sample efficiency and high computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Direct AIM learning (per-action causal discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Partition replay buffer into |A| sub-buffers (one per action) and run causal discovery (CITs) separately to obtain |A| action-conditional SCMs and structural equations; theoretically sound but impractical in many RL settings, especially with continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>action-conditional causal world model learned by data partitioning</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used as a baseline in AIM-recovery experiments (artificial testbed with discrete actions)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>AIM-recovery accuracy (%) on the designed testbed; theoretical soundness per conditional independence tests</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the recovery experiment reported in the paper, the Direct approach achieved an AIM recovery accuracy of ~97.0% (worse than the paper's causal+attention 99.3% in that setup).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High interpretability if enough data per action exists because each action has an explicit SCM.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Per-action causal graphs derived from CIT-based discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: requires separate causal-discovery runs and parameter estimation per action; poor sample efficiency because data is split across actions; inapplicable to infinite/continuous action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient than the two-stage SCM->AIM approach in this paper; higher computation and data requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not extensively used for policy learning in the paper due to inefficiency; primarily a baseline for structural-recovery experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good for small discrete-action tasks with ample per-action data, but impractical or ineffective otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Provides explicit per-action explanations at the cost of sample and compute inefficiency; not scalable to continuous/infinite action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Direct splitting by action and per-action causal discovery; no sharing of statistical strength across actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed by the paper's unified-SCM + attention conversion in AIM recovery and practicality; more principled than attention-only full graph but less sample-efficient overall.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Only recommended when the action space is small/discrete and large amounts of per-action data are available; otherwise use SCM discovery + attention conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explainable Reinforcement Learning through a Causal Lens <em>(Rating: 2)</em></li>
                <li>Distal Explanations for Model-free Explainable Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning <em>(Rating: 1)</em></li>
                <li>Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models <em>(Rating: 1)</em></li>
                <li>Learning Invariant Representations for Reinforcement Learning without Reconstruction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1241",
    "paper_id": "paper-258479970",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Causal World Model",
            "name_full": "Causal World Model via Causal Discovery and Attention-based Inference Networks",
            "brief_description": "An interpretable, learned structural causal model (SCM) for factorized MDPs discovered with conditional-independence tests and implemented with attention-based inference networks; it is convertible to an Action Influence Model (AIM) via action-conditioned attention weights to generate causal chains and natural-language explanations while being accurate enough to use in model-based RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Causal world model (SCM -&gt; AIM via attention-based inference networks)",
            "model_description": "The model first performs causal discovery (Fast CIT) to obtain a bipartite causal graph from exogenous inputs u = (s, a) to endogenous outputs v = (s', o). For each output v_j an inference network f_j is trained: variable-specific encoders map heterogeneous inputs to vectors; action variables in the parent set are processed by a GRU to produce an action embedding e_j; queries q_j, state contribution vectors c_j^i and an action contribution c_j^a are computed and combined with attention weights α_j^i and α_j^a to form a hidden representation h_j; a distribution decoder D_j maps h_j to a posterior (Normal for real variables, categorical for discrete). The learned attention (influence) weights are thresholded per-action to derive action-conditional parents and thus convert the SCM to an AIM for explanation and causal-chain extraction. The model is trained by maximizing log-likelihood of transitions and used in MBRL with a 5-model bootstrap ensemble for uncertainty-aware rollouts.",
            "model_type": "interpretable causal world model (structural causal model convertible to AIM)",
            "task_domain": "Reinforcement learning domains with factorized state/action: LunarLander-Continuous/Discrete, Cartpole, Build-Marine (StarCraft II mini-game); general Factorized MDPs",
            "fidelity_metric": "Maximum log-likelihood / negative log-likelihood on next-state/outcome posterior distributions (Equation 12); AIM recovery accuracy (%) on a ground-truth AIM testbed; downstream policy returns (learning curves) when used in model-based RL; expected KL divergence between true conditional f_j(PA^*) and inferred f_j(PA) (Appendix C)",
            "fidelity_performance": "AIM-recovery accuracy on the designed testbed: 99.3% for the paper's causal+attention method (reported in the comparison table). In experiments, the causal world model attains predictive accuracy and MBRL task performance 'very close' to dense baselines (MLP, Full model) across tested environments; Lunarlander explanation model trained with 150k transitions (example). No single numeric MBRL return values are provided in the paper.",
            "interpretability_assessment": "High interpretability: produces a sparse, visualizable causal graph (bipartite u-&gt;v), per-output attention-based influence weights, and multi-step causal chains; enables minimally complete explanans (MCE) and contrastive explanations (MCCE) in natural-language templates. Interpretability is tunable via a threshold τ/η to select salient parents for action-conditional explanations.",
            "interpretability_method": "Causal discovery using conditional independence tests (Fast CIT) to get a sparse causal graph; attention-based influence weights (key-query-contribution architecture) to extract action-specific influences; causal chain analysis to extract paths from initial state/action to reward variables; template-based natural-language explanan (MCE/MCCE).",
            "computational_cost": "Model parameter scale O(n) (n = max(n_s + n_a, n_s + n_o)); forward-pass time O(n^2 b) for a batch size b; causal discovery cost O(n^3 N log N) where N is number of transition samples (per paper); causal-chain generation time O(n^2 H) for H-step chains. Example training used up to 150k transitions for LunarLander explanation experiments; MBRL used a 5-model bootstrap ensemble (5 models) for rollouts.",
            "efficiency_comparison": "More sample- and computation-efficient in practice than the naive Direct AIM-learning baseline (which requires splitting data into |A| subsets and learning |A| SCMs). Causal+attention approach recovers AIM structure more accurately than attention-only on a full graph, and attains task performance close to dense baselines while providing interpretability. Exact wall-clock or GPU compute comparisons are not provided.",
            "task_performance": "When plugged into a model-based RL loop (k-step rollouts, PPO), the causal world model yields learning curves 'very close' to dense baselines and substantially faster learning than model-free in some environments (Cartpole, LunarLander-Discrete) and higher converged returns in Build-Marine; no absolute numeric return scores are listed in the paper.",
            "task_utility_analysis": "The model is accurate enough to be concurrently used for faithful explanations and to guide policy learning (i.e., high-enough fidelity translates to good task performance). The causal-discovery step reduces spurious correlations and improves the faithfulness of explanations compared to attention-only approaches; ensemble rollouts mitigate epistemic uncertainty for planning.",
            "tradeoffs_observed": "Explicit trade-off between interpretability and accuracy: sparser causal graphs (fewer parents) are easier to read but reduce predictive accuracy and thus hurt fidelity; conversely, including more parents improves accuracy but lowers interpretability. The paper discusses thresholding (η/τ) to control sparsity. Note: the paper contains an inconsistent verbal statement about the effect of decreasing the threshold vs. the formal theorem in Appendix C; the theorem formalizes how parent sets and expected KL divergences relate to threshold choices.",
            "design_choices": "Assume factorized MDP and independent-transition (outputs v are conditionally independent given inputs u) to obtain a bipartite causal graph; use Fast CIT for structure discovery; variable-specific encoders to handle heterogeneous inputs; GRU to encode action variables; attention-style key-query-contribution computations to obtain influence weights; thresholding attention to build AIM; distribution decoders per-variable (normal/categorical); 5-model bootstrap ensemble for MBRL rollouts; trained with log-likelihood objective and policy updated by PPO.",
            "comparison_to_alternatives": "Compared to dense MLP baseline: MLP can be more accurate if well-trained but is uninterpretable; causal model attains similar task returns with the benefit of explanations. Compared to Full model (same networks but full graph): Full model tends to be more accurate, but attention+causal-graph combination recovers AIM structure far more cleanly and avoids many spurious correlations (attention-only Full had lower AIM recovery accuracy). Compared to Direct AIM learning: Direct approach is theoretically sound but sample-inefficient and computationally heavy; the proposed two-stage (SCM discovery + attention stage) is more practical and accurate in the experiments (table numbers).",
            "optimal_configuration": "Paper recommends a two-stage approach: discover a unified SCM first to eliminate spurious correlations, then transfer to an AIM via attention. Thresholds (η/τ) are tuning knobs: larger threshold yields laconic (shorter) explanations while smaller threshold includes more parents; the paper used example thresholds η=0.05 (LunarLander) and η=0.15 (Build-Marine). They also recommend a small ensemble (they use 5 models) to mitigate epistemic uncertainty in model-based rollouts. No single 'optimal' universal configuration is given; trade-offs must be tuned per task.",
            "uuid": "e1241.0",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "AIM",
            "name_full": "Action Influence Model (AIM)",
            "brief_description": "A causal model specialized for RL that associates each action with an action-conditional set of structural equations (an ensemble of SCMs, one per action) to represent how actions causally influence environment variables and rewards, enabling causal-chain explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Action Influence Model (AIM)",
            "model_description": "AIM defines, for each action a and each endogenous variable v_j, a structural equation f_j^a that specifies Pr(v_j | PA^a(v_j)). Naively this requires learning |A| distinct SCMs (|A| × |V| equations). In this paper the AIM concept is realized by converting a unified SCM into an action-conditional AIM via attention weights: parents with attention α_j^i &gt; τ under a specific action are considered salient parents PA^a(v_j).",
            "model_type": "action-conditional causal world model (ensemble of SCMs per action)",
            "task_domain": "Reinforcement learning domains for explanation of action consequences; historically used for discrete-action RL explanation tasks",
            "fidelity_metric": "AIM recovery accuracy (%) when comparing discovered action-conditional parents to ground-truth AIM; quality of causal chains and resultant explanations; predictive log-likelihood under action-conditioned structural equations.",
            "fidelity_performance": "In the paper's recovery experiment the Direct AIM baseline showed ~97.0% accuracy, the attention-only Full graph ~90.0%, and the proposed causal+attention conversion achieved ~99.3% AIM recovery accuracy on a designed test environment with spurious correlations.",
            "interpretability_assessment": "Highly interpretable by construction because each action has an explicit causal graph and causal chains can be traced; however naive Direct AIM learning is impractical for large/infinite action spaces and suffers sample inefficiency.",
            "interpretability_method": "Explicit action-conditional causal graphs and causal chains; thresholding of influence weights to create laconic explanations (MCE/MCCE); can be combined with decision trees per Madumal et al. (prior work) for improved explanation format.",
            "computational_cost": "Naive Direct AIM learning requires splitting data into |A| subsets and learning |A| SCMs, leading to poor sample efficiency and high computational cost; conversion from SCM via attention (proposed here) avoids the need to learn |A| separate SCMs and supports continuous actions.",
            "efficiency_comparison": "Proposed SCM-&gt;AIM conversion via attention is more sample- and computation-efficient than Direct AIM learning; attention-only approaches without causal discovery are less reliable (more spurious links) and thus less effective for explanations.",
            "task_performance": "AIM is primarily an explanatory model; prior AIM implementations (cited in paper) were low-accuracy and unsuitable for policy learning. The converted AIM in this paper is accurate enough to be used for both explanation and to support MBRL.",
            "task_utility_analysis": "AIM provides direct, contrastive and causal-chain explanations that align with human causal reasoning. Direct learning of AIM is often impractical; the conversion approach keeps AIM utility while being tractable for continuous action spaces.",
            "tradeoffs_observed": "AIM yields high interpretability but naive AIM learning trades interpretability for impractical sample requirements; the paper's approach balances tractability and interpretability at a small cost to raw predictive power compared to dense models.",
            "design_choices": "Instead of learning separate SCMs per action, learn a unified SCM and use attention-conditioned thresholding to produce action-specific parent sets; choose τ to trade off laconic explanations versus completeness.",
            "comparison_to_alternatives": "Compared to unified SCMs: unified SCMs are compact and learnable; compared to Direct AIM: more scalable; compared to attention-only Full models: causal discovery + attention reduces spurious correlations and improves AIM recovery.",
            "optimal_configuration": "Use SCM discovery first to remove spurious correlations, then derive AIM via attention with threshold τ adjusted for explanation conciseness; for laconic explanations use larger τ, for completeness use smaller τ. No single global optimum is prescribed.",
            "uuid": "e1241.1",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MLP baseline",
            "name_full": "Multi-Layer Perceptron world model (dense baseline)",
            "brief_description": "A dense neural world model that concatenates all exogenous variables u and uses an MLP to predict endogenous variables; commonly used in model-based RL but uninterpretable.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MLP world model (concatenated-input dense model)",
            "model_description": "Concatenate all input variables u = (s, a) and feed into a multilayer perceptron that outputs parameters of per-variable predictive distributions for s' and outcomes. No explicit causal structure or attention; fully-connected dense mapping.",
            "model_type": "dense neural world model",
            "task_domain": "Model-based RL benchmarks (Cartpole, LunarLander, Build-Marine) used as a baseline in the paper",
            "fidelity_metric": "Predictive accuracy on next-state/outcome (log-likelihood / MSE); implicit comparison via downstream policy returns in MBRL experiments",
            "fidelity_performance": "Paper states dense models (including this MLP) are typically more accurate if well-trained; exact numeric predictive metrics for the MLP baseline are not listed. Learning curves indicate MLP and Full model are slightly more accurate but causal model achieves similar returns.",
            "interpretability_assessment": "Low interpretability (black-box): no causal graph or explicit influence weights; unsuitable for producing causal chains or human-understandable explanations.",
            "interpretability_method": "none mentioned; post-hoc tools (e.g., saliency) are discussed elsewhere in the paper as inadequate for sequential causal explanations.",
            "computational_cost": "Not quantified in the paper; typically fewer steps for causal discovery omitted, but no complexity numbers provided specifically for MLP baseline.",
            "efficiency_comparison": "Denser models may require similar or more data to train, but can achieve higher predictive accuracy; however they do not support faithful explanations. The paper's causal model achieves comparable task performance while being interpretable.",
            "task_performance": "Used as comparison in MBRL experiments; dense MLP baseline generally attains competitive task returns, and the causal model's performance is close to it.",
            "task_utility_analysis": "Good predictive fidelity often translates to good policy performance, but lack of interpretability makes these models unsuitable where explanations are required.",
            "tradeoffs_observed": "High fidelity / low interpretability trade-off; dense MLP tends to be more accurate but gives no causal explanations.",
            "design_choices": "Simple dense MLP architecture with concatenated inputs (standard baseline).",
            "comparison_to_alternatives": "Compared to causal world model: MLP is less interpretable but similar or slightly higher predictive accuracy; however lacks the causal-chain explanation capability.",
            "optimal_configuration": "Not discussed in detail in the paper; used as a strong baseline to quantify the interpretability-accuracy trade-off.",
            "uuid": "e1241.2",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Full model (Full+Attn)",
            "name_full": "Full-graph modular model with attention (dense graph + attention)",
            "brief_description": "A model that uses the same inference-network architecture as the causal model but assumes a full causal graph (every input can be parent of every output) and relies purely on attention to select influential inputs; less protected from spurious correlations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Full model (modular networks with full graph + attention)",
            "model_description": "Uses variable encoders and attention-based inference networks identical to the proposed architecture but uses a full u-&gt;v graph (no causal discovery). The attention mechanism alone determines influence weights; no explicit sparsification by causal discovery.",
            "model_type": "dense/modular neural world model (full graph)",
            "task_domain": "Same RL benchmarks (LunarLander, Build-Marine, Cartpole) as in the paper",
            "fidelity_metric": "Predictive likelihood; AIM-recovery accuracy when extracting action influence purely from attention on the full graph.",
            "fidelity_performance": "In the AIM-recovery experiment, attention-only on the full graph (Full+Attn) achieved about 90.0% accuracy, worse than the paper's causal+attention approach (99.3%). Task returns are similar to other dense baselines; exact numbers not provided.",
            "interpretability_assessment": "Moderate interpretability via attention weights, but vulnerable to spurious correlations in non-i.i.d. RL data; causal chains derived from attention-only full graphs often contain unreasonable edges.",
            "interpretability_method": "Attention weights used to identify salient parents but no causal discovery safeguards.",
            "computational_cost": "Same order as causal model forward passes; lacks the additional cost of causal discovery but suffers from lower-quality explanations which may require extra validation.",
            "efficiency_comparison": "Computationally cheaper than running causal discovery but less effective at producing correct causal explanations; worse AIM recovery accuracy.",
            "task_performance": "Comparable to other dense baselines for policy learning; explanation quality worse due to spurious links.",
            "task_utility_analysis": "Useful for prediction and policy learning but unreliable for faithful causal explanations in RL due to spurious correlations.",
            "tradeoffs_observed": "Saves on causal-discovery overhead but trades-off correctness of causal explanations; attention alone is insufficient for robust causal discovery in RL data.",
            "design_choices": "Full input-to-output connectivity with attention-based weighting but without CI-based pruning.",
            "comparison_to_alternatives": "Worse than causal+attention in AIM recovery and explanation faithfulness; similar to MLP in predictive utility but more modular.",
            "optimal_configuration": "Not recommended for explanation-critical applications; combining with causal discovery (as in paper) improves results.",
            "uuid": "e1241.3",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Direct AIM learning",
            "name_full": "Direct approach to learn AIM (per-action causal discovery)",
            "brief_description": "Baseline method that learns an AIM by splitting transition data by action and performing independent causal discovery per action, yielding action-specific SCMs but suffering from poor sample efficiency and high computational cost.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Direct AIM learning (per-action causal discovery)",
            "model_description": "Partition replay buffer into |A| sub-buffers (one per action) and run causal discovery (CITs) separately to obtain |A| action-conditional SCMs and structural equations; theoretically sound but impractical in many RL settings, especially with continuous actions.",
            "model_type": "action-conditional causal world model learned by data partitioning",
            "task_domain": "Used as a baseline in AIM-recovery experiments (artificial testbed with discrete actions)",
            "fidelity_metric": "AIM-recovery accuracy (%) on the designed testbed; theoretical soundness per conditional independence tests",
            "fidelity_performance": "In the recovery experiment reported in the paper, the Direct approach achieved an AIM recovery accuracy of ~97.0% (worse than the paper's causal+attention 99.3% in that setup).",
            "interpretability_assessment": "High interpretability if enough data per action exists because each action has an explicit SCM.",
            "interpretability_method": "Per-action causal graphs derived from CIT-based discovery.",
            "computational_cost": "High: requires separate causal-discovery runs and parameter estimation per action; poor sample efficiency because data is split across actions; inapplicable to infinite/continuous action spaces.",
            "efficiency_comparison": "Less efficient than the two-stage SCM-&gt;AIM approach in this paper; higher computation and data requirements.",
            "task_performance": "Not extensively used for policy learning in the paper due to inefficiency; primarily a baseline for structural-recovery experiments.",
            "task_utility_analysis": "Good for small discrete-action tasks with ample per-action data, but impractical or ineffective otherwise.",
            "tradeoffs_observed": "Provides explicit per-action explanations at the cost of sample and compute inefficiency; not scalable to continuous/infinite action spaces.",
            "design_choices": "Direct splitting by action and per-action causal discovery; no sharing of statistical strength across actions.",
            "comparison_to_alternatives": "Outperformed by the paper's unified-SCM + attention conversion in AIM recovery and practicality; more principled than attention-only full graph but less sample-efficient overall.",
            "optimal_configuration": "Only recommended when the action space is small/discrete and large amounts of per-action data are available; otherwise use SCM discovery + attention conversion.",
            "uuid": "e1241.4",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explainable Reinforcement Learning through a Causal Lens",
            "rating": 2,
            "sanitized_title": "explainable_reinforcement_learning_through_a_causal_lens"
        },
        {
            "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "distal_explanations_for_modelfree_explainable_reinforcement_learning"
        },
        {
            "paper_title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "systematic_evaluation_of_causal_discovery_in_visual_model_based_reinforcement_learning"
        },
        {
            "paper_title": "Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning",
            "rating": 1,
            "sanitized_title": "neural_network_dynamics_for_modelbased_deep_reinforcement_learning_with_modelfree_finetuning"
        },
        {
            "paper_title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
            "rating": 1,
            "sanitized_title": "deep_reinforcement_learning_in_a_handful_of_trials_using_probabilistic_dynamics_models"
        },
        {
            "paper_title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
            "rating": 1,
            "sanitized_title": "learning_invariant_representations_for_reinforcement_learning_without_reconstruction"
        }
    ],
    "cost": 0.02163325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explainable Reinforcement Learning via a Causal World Model</p>
<p>Zhongwei Yu yuzhongwei2021@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Jingqing Ruan ruanjingqing2019@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Dengpeng Xing dengpeng.xing@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Explainable Reinforcement Learning via a Causal World Model
1D4F9B02BEDC1251F8D4B05E8BA4F10F
Generating explanations for reinforcement learning (RL) is challenging as actions may produce longterm effects on the future.In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment.The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards.Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning.As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.</p>
<p>Introduction</p>
<p>Many real-world applications like finance and healthcare require AI systems to be well understood by users due to the demand for safety, security, and legality [Gunning et al., 2019].Aiming to help people better understand and work with AI systems, the field of Explainable AI (XAI) has recently attracted increasing interest from researchers.For example, a number of explanatory tools have been developed to pry into the black box of deep neural networks [Bach et al., 2015;Selvaraju et al., 2020;Wang et al., 2021b].</p>
<p>However, the domain of explainable reinforcement learning (XRL) has been neglected for a long time.Many XRL studies adopt classic tools of XAI such as saliency maps [Nikulin et al., 2019;Joo and Kim, 2019;Shi et al., 2021].These tools are not designed for sequential decision-making and are weak in interpreting the temporal dependencies of RL environments.Therefore, some studies investigate explaining specific components of the decision process, e.g., observations [Koul et al., 2018;Raffin et al., 2019], actions [Fukuchi et al., 2017;Yau et al., 2020], policies [Amir and Amir, 2018;Coppens et al., 2019], and rewards [Juozapaitis et al., 2019].However, these studies rarely combine explanations with the dynamics of environments, which is important for understanding the long-term effects produced by agents' actions.In addition, real-world environments usually contain dynamics unknown to users, making it crucial to interpret these dynamics using explanatory models.Model-based RL (MBRL) uses predictive world models [Nagabandi et al., 2018;Kaiser et al., 2020;Janner et al., 2021] to capture such dynamics.However, these models are usually densely-connected neural networks and cannot be used for the purpose of explanation.</p>
<p>Psychological research suggests that people explain the world through causality [Sloman, 2005].In this paper, we propose a novel framework that uses an interpretable world model to generate explanations.Rather than using a dense and fully-connected model, we perform causal discovery to construct a sparse model that is aware of the causal relationships within the dynamics of environments.In order to explain agents' decisions, the proposed causal model allows us to construct causal chains which present the variables causally influenced by the agent's actions.The proposed model advances the existing work that uses causality for explainable RL [Madumal et al., 2020b;Madumal et al., 2020a], as it does not require a causal structure provided by domain experts, and is applicable to continuous action space.</p>
<p>Apart from interpreting the world, humans also use causality to guide their learning process [Cohen et al., 2020].However, the trade-off between interpretability and performance [Gunning et al., 2019;Longo et al., 2020;Puiutta and Veith, 2020] indicates that explainable models are usually inaccurate and can hardly benefit learning.On the contrary, our model is sufficiently accurate, leading to a performance close to dense models in MBRL.Therefore, we can train the agent and explain its decisions through exactly the same model, making explanations more faithful to the agent's intention.This is significant for overcoming the issue that post-hoc explanations like saliency maps can sometimes fail to faithfully unravel the decision-making process [Atrey et al., 2020].</p>
<p>Our main contributions are as follows: 1) We learn a causal model that captures the environmental dynamics without prior knowledge of the causal structure.2) We design a novel approach to effectively extract the causal influence of actions, allowing us to derive causal chains for explaining the agent's decisions.3) We show that our explanatory model is accurate enough to guide policy learning in MBRL.arXiv:2305.02749v5[cs.LG] 18 Jan 2024 2 Background</p>
<p>Causality in Reinforcement Learning</p>
<p>RL integrated with causality has recently become noticed by RL researchers.For example, Lu [2018], Wang [2021a] and Yang [2022] et al. use causal inference to improve the robustness against confounders or adversarial intervention; Dietterich et al. [2018] improve learning efficiency by removing the variables unrelated to the agent's action; Nair et al. [2019] construct a causal policy model; Seitzer et al. [2021] improve exploration by detecting the causal influence of actions; Wang [2022] and Ding [2022] et al. investigate causal world models as we do.However, they mainly use causality to improve generalization rather than generate explanations.</p>
<p>Only a few studies consider improving explainability using a causal model.Madumal et al. [2020b] propose the Action Influence Model (AIM), a causal model specialized for RL, to generate explanations about the agent's actions.</p>
<p>Another study [Madumal et al., 2020a] further combines the AIM with decision trees to improve the quality of explanations.However, these approaches require finite action space, and the causal structure is given beforehand by human experts.In addition, they only consider a low-accuracy model, which cannot be used for policy learning.Volodin [2021] proposes a method to learn a sparse causal graph of hidden variables abstracted from high-dimensional observations, which improves the understanding of the environment.However, the approach provides no insight into the agent's behavior.</p>
<p>Structural Causal Model</p>
<p>A Structrual Causal Model (SCM) [Pearl et al., 2016], denoted as a tuple (U, V, F), formalizes the causal relationships between multiple variables.U and V contain the variables of the SCM, where U = {u 1 , ..., u p } is the set of exogenous variables and V = {v 1 , ..., v q } is the set of endogenous variables.F = {f 1 , ..., f q } is the set of structural equations, where f j formulates how v j is quantitatively decided by other variables.We call f j a "structural" equation as it defines the subset of variables denoted as P A(v j ) ⊆ U ∪ V ∖ {v j } (i.e., the parent variables) that directly decide the value of v j .</p>
<p>An SCM is usually represented as a directed acyclic graph (DAG) G = (U ∪ V, E) called the causal graph.The node set of G is exactly U ∪ V and the edge set E is given by the structural equations:
(x i , v j ) ∈ E ⇔ x i ∈ P A(v j ), where x i ∈ U ∪ V.
For simplicity, symbols like u i , v j , and x i may denote the names or the values of the variables according to the context.In addition, we consider stochastic structural equations, where f j outputs the posterior distribution of v j conditioned on its direct parents P A(v j ):
Pr(v j |P A(v j )) ∼ f j (P A(v j )).
(1)</p>
<p>Action Influence Model</p>
<p>An AIM, denoted as the tuple (U, V, F, A), is a causal model specialized for RL.Here, U and V follow the same definition in SCM.F is the set of structural equations, and A is the action space of the agent.Different from SCM, each structural equation is related to not only an endogenous variable but also a unique action in A. In other words, there exists a structural equation f j a ∈ F for any action a ∈ A and any endogenous variable v j ∈ V to describe how v j is causally determined under action a.We use P A a (v j ) ⊆ U ∪ V ∖ {v j } to denote the causal parents of v j under action a.Then, the posterior distribution of v j under action a is given by
Pr(v j |P A a (v j ), a) ∼ f j a (P A a (v j )).
(2) As a result, there exist overall |A| × |V| causal equations in the AIM.In fact, we may also reckon the AIM as an ensemble of |A| SCMs, where each SCM accounts for the influence of a unique action in A.</p>
<p>Factorized MDP</p>
<p>We are interested in tasks where the action and state can be factorized into multiple variables, and formalize such a task as a Factorized MDP (FMDP) denoted by the tuple ⟨S, A, O, R, P, T, γ⟩.Here, S, A, and O respectively denote the state space, action space, and outcome space.Each state s ∈ S is factorized into n s state variables such that s = (s 1 , ..., s ns ), where s i is the i-th state variable.Similarly, we have a = (a 1 , ..., a na ) for each action and o = (o 1 , ..., o no ) for each outcome.Figure 1(a) illustrates an example of the factorization for a simple 2-grid environment called the Vacuum world [Russell et al., 2010].Its state variables include the position of the vacuum and whether the places are clean (clean 1 and clean 2 ); it contains only one action variable a that is chosen from Left, Right, and Suck (making the place clean).The action of Left or Right leads to an outcome of failure when blocked by the world boundary.</p>
<p>On each step, the agent observes the current state s and takes an action a, then the state transits and the outcome is produced according to the transition probability P (o ′ , s ′ |s, a), leading to a transition tuple denoted as δ = (s, a, o, s ′ ).Meanwhile, the reward is given by the overall reward function R(δ).Following the reward decomposition [Juozapaitis et al., 2019], we factorize R as the summation of n r reward variables, given by R(δ) = ∑ nr i=1 r i (δ).γ is the discount factor for computing returns.T is the termination condition deciding whether the episode terminates based on the transition δ.How reward variables {r i } nr i=1 and the termination condition T depend on the transition δ is defined by users according to their demands.In those cases where R and T contain components unknown to users, we may put these components into the outcome variables.That is, we use an outcome variable o i to indicate the unknown reward, and define the corresponding reward variable as r i ≡ o i .</p>
<p>The Proposed Framework</p>
<p>There exist two perspectives on the causal model of the dynamics of the environment: 1) In a unified SCM, action variables are merely nodes of the causal structure and are treated evenly as state variables.2) In an AIM, each action specifies a peculiar causal structure, leading to a good understanding of both the environment and the agent's decisions [Madumal et al., 2020b].To make it clearer, Figure 1 illustrates how a unified SCM and an AIM pertain to our setting in the above-mentioned Vacuum world.However, directly learning an AIM is intractable as we must divide data into |A| subsets to respectively learn |A| SCMs.This reduces sample efficiency, produces redundant parameters, and cannot be applied to infinite action space.Therefore, we seek to build a unified SCM that can be converted to an AIM based on specially-designed structural equations.As illustrated in Figure 2(a), the exogenous variables in this SCM are the state variables s and action variables a; the endogenous variables are the next-state variables s ′ and outcome variables o.In the following description, we define u ∶= (s, a) = (s 1 , ..., s ns , a 1 , ..., a na ) as the input (exogenous) variables and v ∶= (s ′ , o) = (s ′ 1 , ..., s ′ ns , o 1 , ..., o no ) as the output (endogenous) variables of our model.In this way, a transition tuple can also be written as δ = (u, v).</p>
<p>The workflow of the proposed framework is illustrated in Figure 2. When the agent interacts with the environment, we store the transition data into a replay buffer D. Using the stored transitions, we perform causal discovery to identify the causal graph of the above-mentioned SCM.Then, we fit the causal equations for the variables of the next state s ′ and the outcome o using Inference Networks.Together with the known reward function R and termination condition T , we construct a causal world model that captures the dynamics of the environment.The attention weights (influence weights) in the inference networks capture the action influence, which allows us to perform causal chain analysis to reveal the variables that are causally influenced by the agent's action.The discovered causal graph interprets the environmental dynamics, and the causal chain analysis provides explanations about the agent's decisions.Moreover, this causal world model can be used by MBRL algorithms to facilitate learning.</p>
<p>Causal Discovery</p>
<p>We assume that output variables v are produced independently conditioned on the input variables u, as independence underlies the intuition of humans to segment the world into components.Under this assumption, it is proven that the causal graph is bipartite, where no lateral edge exists in v, and each edge starts in u and ends in v. Therefore, we only need to determine whether there exists a causal edge for each variable pair (u i , v j ), where 1 ≤ i ≤ n s + n a and 1 ≤ j ≤ n s + n o .Studies have shown that conditional independent tests (CITs) can be used to perform efficient causal discovery [Wang et al., 2022;Ding et al., 2022].In this work, we implement CITs through Fast CIT [Chalupka et al., 2018] and determine each edge using the following rule:
u i ∈ P A(v j ) ⇐⇒ (u i ̸ v j |u −i ),(3)
where u −i denotes all variables in u other than u i .In Appendix A, we provide the theoretical basics of causal discovery and prove a theorem showing that Equation 3 leads to sound causal graphs.</p>
<p>Attention-based Inference Networks</p>
<p>To perform causal inference on the discovered causal graph, we fit the structural equation of each output variable v j using an inference network denoted as f j , which takes the causal parents P A(v j ) as inputs and predicts the posterior distribution Pr(v j |P A(v j )).These inference networks should adapt to the structural changes of the causal graph, as the agent's exploratory behaviors may reveal undiscovered causal relationships and lead to new causal structures.To achieve this, Ding et al. [2022] use Gated Recurrent Unit (GRU) networks that sequentially input all parent variables without discriminating the state and the action.To model the action influence, we design the attention-based inference networks as illustrated in Figure 2(c).</p>
<p>To handle heterogenous input variables (which may be scalars or vectors of different lengths), we first use variable encoders (each uniquely belongs to an input variable) to individually map the input variables to vectors of the same length.These encoders are shared by the inference networks of all output variables.In particular, we use ũ = {s 1 , ..., sn , ã1 , ..., ãm } to denote these encoding vectors of input variables.</p>
<p>Then, for each inference network f j , we compute the contribution vectors of the parent state variables through linear transforms:
c j i = W j s si + b j s , s i ∈ P a(v j ).(4)
The usage of these contribution vectors is equal to the "value vectors" in key-value attention.We use the term "contribution vectors" since the word "value" is ambiguous in the context of RL.Each inference network f j contains a GRU network g j , which receives the action variables in P A(v j ) and outputs the action embedding e j .Then, we feed this action embedding into linear transforms to respectively obtain the query vector q j and the action contribution vector c j a : e j = GRU j ({ã i } ai∈P A(vj ) ) ,</p>
<p>(5)
q j = W j q e j + b j q ,(6)c j a = W j a e j + b j a .(7)
The projection matrices W j s , W j q , W j a and bias vectors b j s , b j q , b j a are all trainable parameters of f j .We use the superscript j to indicate that these parameters belong to f j .Each state variable s i is allocated a key vector k i , which is a trainable parameter learned by gradient descent.We do not use the superscript j for key vectors as they are shared by inference networks of all output variables.The influence weights (i.e., attention weights) of the state variables in P A(v j ) and the action are then computed by
α j i = exp(k T i q j ) 1 + ∑ s i ′ ∈P A(vj ) exp(k T i ′ q j ) ,(8)α j a = 1 1 + ∑ s i ′ ∈P A(vj ) exp(k T i ′ q j ) .(9)
We then compute the hidden representation of the posterior distribution using the weighted sum of the value vectors:
h j = ∑ si∈P A(vj ) α j i ⋅ c j i + α j a ⋅ c j a .(10)
Finally, the distribution decoder D j maps h j to the predicted posterior distribution:
Pr(v j |P A(v j )) ∼ D j (h j ).(11)
We assume the type of this posterior distribution is previously known and D j only outputs the parameters of the distribution.</p>
<p>In our implementation, we use normal distribution (parameterized by the mean and variance) for real-number variables and use categorical distribution (parameterized by the probability of each class) for discrete variables.</p>
<p>The inference networks {f j } ns+no j=1 are trained by maximizing the log-likelihood of the transition data stored in D, written as
L inf er = ns+no ∑ j=1 1 |D| ∑ δ∈D log Pr(v j |P A(v j )). (12)</p>
<p>Causal Chain Analysis</p>
<p>In order to generate explanations, we first describe how our model can be converted to an AIM.Noticing that the key vectors {k i } n i=1 are trainable parameters, the influence weights only depend on the numeric value of action variables.Therefore, the influence weight α j i captures how much the output variable v j depends on state variable s i under the given action a.In order to generate laconic explanations, we define P A a (v j ) ∶= {s i ∈ P A(v j ) | α j i &gt; τ } as the parent set of v j with salient dependencies under the action a, where τ ∈ [0, 1] is a given threshold.In this way, we convert the SCM to an AIM, where the structural equation for v j under action a is written as
f j a (P A a (v j )) = D j ( ∑ si∈P Aa(vj ) α j i ⋅ c j i (s i ) + α j a ⋅ c j a ). (13)
Since we use the AIM for the purpose of explanation, it is tolerable to set a larger threshold, which allows us to ignore parent variables that are not influential enough.Madumal et al. have introduced methods to generate good explanations using an AIM.The key is to build a causal chain containing the variables that (i) are causally affected by the actions, and (ii) causally lead to rewards.A single causal chain starting from state s and action a leads to the explanation for "why the agent took a on s".Contrastive explanations for "why the agent took a instead of b on s" can be obtained by comparing the causal chines produced by the factual action a and the counterfactual action b.Details can be found in Appendix B and the AIM paper [Madumal et al., 2020a].</p>
<p>The rest of this section introduces how to derive a causal chain starting from the state s t at step t and an action a t (can be factual or counterfactual) using our model.First of all, we use our model and the agent's policy to simulate the most-likely trajectory δ t , δ t+1 , ..., δ t+H−1 , where H denotes the number of simulation steps.The symbol δ t+k denotes the transition tuple on step t + k, where the actions a t+k for k ≥ 1 are produced by the agent's policy.For a factual causal chain, this simulation is not necessary if factual data of these future states and actions is available.</p>
<p>Then, we build an extended graph containing the state, outcome, and reward variables of these H steps.The edges of this graph accord to the structure of the AIM derived above.That is, if s i ∈ P A a t+k (v j ) then there exists an edge from s t+k i to v t+k j for all k = 1, ..., H.It is worth mentioning that we treat the first transition δ t differently since a t is exactly the action being explained: If P A(v j ) ∩ a t = ∅, then v t j is not affected by the choice of a t .In this case, no edge will be established from any state s t i to v t j .Afterward, the explainee may specify the target variables (a subset of reward variables) he/she is interested in.Otherwise, all reward variables will be considered.We perform a graph search from the starting state variables s t and highlight all paths from s t to the target rewards.These paths together form the causal chain of action a t starting from s t .Based on this causal chain, the explanation can be presented as a picture or a natural-language description.</p>
<p>Model-based RL</p>
<p>XAI literature has widely discussed the trade-off between interpretability and performance, which is also reflected in our model.A sparser causal graph (discovered using a smaller threshold η) is usually easier to read and produces clearer explanations.However, it also enforces the model to infer posterior distributions using less information from input variables u, leading to inferior accuracy.In Appendix C, we provide a theorem that formally shows that decreasing the threshold η leads to a denser causal graph (i.e., lower interpretability) and also higher predicting accuracy.</p>
<p>In order to show our model is accurate enough to do more than generate post-hoc explanations, we consider applying our world model to MBRL to facilitate policy learning.We use a bootstrap ensemble containing 5 models to alleviate the effect of the epistemic uncertainty [Chua et al., 2018].For each iteration, we first collect real transition data into the model buffer D. Then, we update the world model by causal discovery and fitting structural equations using the data in D. Afterward, we perform k-step model-rollouts [Janner et al., 2021] to generate simulated data for updating the policy.In our implementation, the policy is trained using Proximal Policy Optimization [Schulman et al., 2017].The pseudo-code of the learning procedure is given in Appendix D.</p>
<p>Experiments</p>
<p>We present examples of causal chains in two representative environments: Lunarlander-Continuous for the continuous action space, and Build-Maine for the discrete action space.To verify whether our approach can produce correct causal chains, we design an environment to measure the accuracy of recovering causal dependencies of the ground-truth AIM.</p>
<p>Explanation Results</p>
<p>Lunarlander-Continuous</p>
<p>We factorize the state into 7 variables (x, y, ẋ, ẏ, θ, θ, legs) indicating 1) the horizontal position, 2) the vertical position, 3) the horizontal velocity, 4) the vertical velocity, 5) the angle, 6) the angle velocity, and 7) whether the two legs are in contact with the ground.The action includes 2 continuous variables ranged in (−1, 1), respectively controlling the throttles of the main and the lateral engines.The environment contains 3 outcome variables, including 1) the fuel cost due to firing the engine, 2) whether the lander crashed, and 3) whether the rocket is resting.</p>
<p>In this experiment, we learn a post-hoc model to generate explanations for a previously trained policy.We first use the policy (with noise) to collect 150k samples into the buffer D. Then, we use these samples to discover the causal graph (with the threshold η = 0.05) and train the inference networks.The resulting causal graph is presented in Figure 3(a).The environment contains many kinds of rewards, leading to complicated causal chains if we consider them all.To make our explanation clearer, we present a causal chain in Figure 4 considering only two kinds of rewards: 1) the reduction of the distance to the target location, and 2) the reduction of the angle (i.e., balancing the rocket).This causal chain shows that the agent's action a t first influences the velocities ( θ, ẋ, and ẏ) and thereby reduces (or increases) the angle (θ) and the distance (</p>
<p>√</p>
<p>x 2 + y 2 ).In addition, we observe that no parent of the outcome variable rest is discovered in the causal graph.This means the policy provides insufficient opportunities to reveal its causality.As a result, the variable rest is excluded</p>
<p>Build-Marine</p>
<p>The original observation space provided by the SC2LE interface contains hundreds of variables, which is intractable for causal discovery.In our implementation, we define the state as the tuple containing only 6 variables denoted as (n wk , n mr , n br , n dp , money, time), namely 1) the number of workers, 2) the number of marines, 3) the number of barracks, 4) the number of supply depots, 5) the amount of money, and 6) the game time.We are interested in the macrolevel decision-making and therefore define the action as one discrete variable indicating which unit (workers, marines, barracks, supply depots, or none) to be built.The microlevel control of building these units (e.g., determining where to place the new barracks) is implemented by simple rules.The goal is to produce as many marines as possible within 15 minutes.Therefore, the player is rewarded with 1 for every newly produced marine.In addition, this environment contains no outcome variable.Direct Full+Attn Caus+Attn (ours) AIM accuracy 97.0 % 90.0% 99.3%</p>
<p>Table 1: The accuracy of recovering the causal dependencies of the AIM."Direct" means the direct approach mentioned in Section 5.2; "Full" means using a full graph; "Caus" means using a causal graph; and "Attn" means using attention.</p>
<p>In this experiment, both the policy and the causal model are obtained by model-based learning (see Section 4.4).We present the final causal graph in Figure 3(b) (discovered using the threshold η = 0.15) and an example of the causal chain in Figure 5.The causal chain shows that our attention-based inference networks successfully reason the causal dependencies under different actions, which reflect the following rules of the StarCraftII game: 1) Building new barracks requires at least one supply depot; 2) marines are built from barracks; 3) the number of marines is limited by the number of supply depots; and 4) building more units requires sufficient money in hand.This causal chain explains the reason why the agent builds supply depots: to gain permission to build barracks and provide enough supplies for building marines.Interestingly, we discover no causal relationship between n wk and money ′ .For human players, it is common sense that more workers increase the efficiency of collecting minerals and thus lead to a higher income.Since the causal model is learned using the transition data produced along with policy training, this missing edge indicates that the agent explored inadequately for building more workers, providing insufficient evidence to reveal this causal relation.</p>
<p>Accuracy of Recovering Action Influence</p>
<p>Good explanations are generated from correct causal chains, which require us to accurately recover the AIMs of environments.In Section 4, we have mentioned a Direct approach that learns the AIM by splitting the data buffer D into |A| sub-buffers and performing causal discovery for each action a ∈ A. Though this direct approach is theoretically sound, it suffers from poor sample efficiency and high computational complexity.Noticing that the causal dependencies under dif- ferent actions usually share similar structures, our approach takes 2 stages: 1) In the causal stage, we learn a unified SCM, whose causal graph summarizes causal dependencies for all actions; 2) in the attention stage, we then transfer this SCM into an AIM based on attention weights (influence weights).</p>
<p>To verify whether our approach can accurately recover the AIM, we design a simple environment that contains spurious correlations to confuse neural networks (see Appendix E.2 for details).We compare our approach with two baselines: 1) the Direct approach mentioned above, and 2) a non-causal approach that uses a full causal graph and only relies on attention.The accuracy of recovering the ground-truth causal dependencies of the AIM using non-i.i.d data is shown in Table 1.These results show that: 1) the causal graph discovered in the causal stage precludes most spurious correlations, making our approach more effective than the Direct approach in practice; 2) and attention alone is insufficient to accurately extract the causal influence of actions.</p>
<p>Further, we examined the causal chains derived solely from attention (where full causal graphs are used).In these chains, we found plenty of spurious correlations, which lead to unreasonable explanations (e.g., "the number of supply depots naturally grows with time" in Build-Marine).An example of such a causal chain and the related discussion are provided in Appendix F. This result shows that causal discovery is an indispensable process for producing reasonable explanations.</p>
<p>Performance in Model-Based RL</p>
<p>We evaluate the performance of model-based policy learning using our explanatory model.We compare the learning performance with the Model-Free approach that learns policies without models.In addition, we consider two dense models as baselines: 1) the model that concatenates all exogenous variables u as inputs and infers endogenous variables using a Multi-Layer Perceptron (MLP), and 2) the Full model that adopts the same networks as ours whereas uses a full causal graph.</p>
<p>The MLP model is the most commonly used in MBRL, and the Full model follows the state-of-the-art modular architecture [Ke et al., 2021].These dense models are not suitable for generating explanations.However, they are more accurate (if well-trained) than our explainable model as they are allowed to predict each output variable based on the complete inputs.Existing studies show that causal models generalize better than dense models [Wang et al., 2022;Ding et al., 2022].However, we focus on ordinary learning problems and do not consider using our model for generalization.Therefore, we stress that the goal of this experiment is not to obtain a higher performance than dense baselines.On the contrary, We aim to figure out: 1) whether the proposed model can be of help to the learning process, and 2) how much performance our sparse model sacrifices for improved explainability.</p>
<p>The learning curves are shown in Figure 6.In all environments, the performance of our explanatory model is very close to the dense baselines.Compared to the model-free approach, the model-based approaches significantly learn faster in Cartpole and Lunarlander-Discrete and converge to higher returns in Build-Marine.The returns of all tested approaches are close in Lunarlander-Continuous, whereas the model-based approaches improve the stability of learning.These results show that our model improves explainability at an acceptable cost in performance and well balances the interpretability-accuracy trade-off.Therefore, our model can simultaneously guide policy learning and explain decisions, leading to better consistency between explanations and the agent's cognition of the environment.</p>
<p>Conclusion and Future Work</p>
<p>This paper proposes a framework that learns a causal world model to generate explanations about agents' actions.To achieve this, we perform causal discovery to identify the causal structure in the environment and fit causal equations through attention-based inference networks.These inference networks produce the influence weights that capture the influence of actions, which allow us to perform causal chain analysis in order to generate explanations.The proposed framework does not require the structural knowledge provided by human experts and is applicable to infinite action spaces.Apart from generating explanations, we successfully applied our model to model-based RL, showing that the model can be the bridge between learning and explainability.</p>
<p>A weakness of our approach is that it requires a known factorization of the environment, which limited its application scope.There exists a number of studies aiming to learn the causal feature set from raw observations [Zhang et al., 2020;Volodin, 2021;Zhang et al., 2021].Future work will put representation learning into consideration for better applicability.In addition, we currently consider model-based policy learning as the usage of our model apart from generating explanations.However, this usage does not make full use of the advantage of a causal model.Future work will investigate better usage of our model to further improve learning.</p>
<p>[ Zhang et al., 2021]</p>
<p>A Causal Discovery</p>
<p>In this section, we introduce some basics of causal discovery and then prove the soundness of our approach for causal discovery.First of all, we introduce the concept of Markov Compatibility [Pearl, 2000], which describes whether a directed acyclic graph (DAG) is able to represent the dependencies of a group of random variables.Definition 1 (Markov Compatibility).Assume x = (x 1 , x 2 , ⋯, x n ) is a group (ordered set) of random variables and Pr is a probability function on x.Assume G is a DAG whose nodes are these variables, where the parent set of x i is denoted as P A(x i ).If we have
Pr(x) = n ∏ i=1 Pr(x i |P A(x i )),(14)
then we say that Pr and G are compatible, or that G represents Pr.</p>
<p>The goal of causal discovery is to find some DAG G to represent a given probability Pr.Now, we introduce two important concepts for causal discovery: the d-separation and causal faithfulness.Definition 2 (d-separation).Assume G is a DAG on a set of variables, where x, y, and z are disjoint subsets of variables.We say that x and y is d-separated by on Z (denoted as x G y|z), if every undirected path p from x to y satisfies:
(x G y|z) ⇒ (x Pr y|z), (15)
where Pr means conditional independence under Pr.</p>
<ol>
<li>If (x Pr y|z) holds for every probability function Pr that is compatible with G, we have (x G y|z) [Pearl, 2000].Definition 3 (Causal Faithfulness).Assume G and Pr are respectively a DAG and a probability function on a set of variables.We say that Pr is faithful to G, if
(x Pr y|z) ⇒ (x G y|z),(16)
for all disjoint subsets x, y, and z of variables.</li>
</ol>
<p>Causal faithfulness indicates that all conditional independent relationships are due to the structure of the DAG instead of rare coincidence.In fact, studies have shown that if Pr is compatible with G, the chance of Pr to be not faithful to G is extremely low [Spirtes et al., 2001].In causal discovery, it is usually assumed that the probability Pr is faithful to the DAG G that we are looking for, which makes the structure of G recognizable.</p>
<p>We have not distinguished exogenous and endogenous variables above.The exogenous variables are considered the inputs of a system, and thus their causality does not need to be discussed.In other words, our causal graph only describes the causality of endogenous variables, whereas the causality of exogenous variables is ignored.Unless otherwise specified, the letter v denotes the set of endogenous variables and the letter u denotes the set of endogenous variables in the following discussion.Given a probability function Pr of some variables x = (u, v), the goal of causal discovery now becomes determining only the causal parents of endogenous variables v in a DAG G that represents Pr.</p>
<p>It is worth mentioning that, in the definition given by Pearl [2016], it is assumed that exogenous variables are independent of each other.However, this requirement is released in our definition since the current state and action variables are usually correlated due to the dependencies underlying the agent's policy and the history transitions.Ignoring these correlations leads to spurious edges, which are detrimental to generating reasonable explanations.For example, in Figure 7(a), we show that a backdoor path occurs when the actions are sampled by a policy dependent on the current state; in Figure 7(b) we show that a backdoor path occurs considering the transition history even if the actions are sampled randomly.In both cases, s 1 and s ′ 2 are correlated, making s 1 an "spurious parent" of s ′ 2 in the discovered causal graph.Therefore, we detect causal dependencies using conditional independent tests (CITs), where the condition variables block all the backdoor paths (if there exist any).We assume that state variables transit and outcome variables arise independently, which brings several benefits: 1) The causal graph is bipartite, making our model able to be computed in parallel.2) The causal graph can be uniquely identified using CITs.Wang et al. [2022] adopt a similar approach for causal discovery from the perspective of conditional mutual information.Here, we describe and prove a theorem that is used to discover sound causal graphs in our approach.</p>
<p>Theorem 2 (Causal Discovery for Factorized MDP).Assume u = (s, a) and v = (s ′ , o) are respectively the sets of exogenous and endogenous variables in a Factorized MDP.Assume that Pr is a probability function of these variables δ = (u, v) such that Pr is consistent with the MDP; that is, we have
Pr(δ) = Pr(s)π(a|s)P (v|u)(17)
where Pr(s) may follow arbitrary state distribution, π can be arbitrary policy, and P (v|u) is the transition probability of the MDP.Then, there always exists a DAG G that represents Pr.Further, if we make the following assumptions: 1. (Independent Transition) next-state variables and outcome variables are produced independently, given by
P (v|u) = ns ∏ j=1 P (v j |u) no ∏ k=1 P (o k |u);
2. (Causal Faithfulness) Pr is faithful to G, then the following propositions about G hold:</p>
<p>1.No lateral edge like v i → v j exists among v.In other worlds, we have P A(v j ) ⊆ u for every v j ∈ v. 2. The parent sets of v are uniquely identified by
(u i Pr v j |u −i ) ⇔ u i ∈ P A(v j )
for every i = 1, ⋯n s + n a and every j = 1, ⋯n s + n o .Here u −i denotes u ∖ {u i } 3. The parent sets of v in G is invariant.That is, we may replace Pr with any other probability function Pr * that satisfies Eq. 17 and obtain a new DAG G * that represents Pr * .Assuming Pr * is faithful to G * , the parent sets of v will stays unchanged:
P A(v j ) = P A * (v j ), j = 1, ⋯, n s + n o
Proof.We first prove the existence of G. Since π and P in Eq. 17 are both conditional probability function, we write that Pr(δ) = Pr(s)Pr(a|s)Pr(v|u) where Pr(a|s) = π(a|s) and Pr(v|u) = P (v|u).Using the chain rule of probability functions, we have
Pr(s) =Pr(s 1 )Pr(s 2 |s 1 )⋯Pr(s ns |s 1 , ⋯, s ns−1 ) Pr(a|s) =Pr(a 1 |s)Pr(a 2 |a 1 , s)⋯Pr(a na |a 1 , ⋯, a na−1 , s) Pr(v|u) =Pr(v 1 |u)Pr(v 2 |v 1 , u)⋯ Pr(v ns+no |v 1 , ⋯, v ns+no−1 , u)
We define
P A(s i ) ⊆ (s 1 , ⋯, s i−1 ), i = 1, ⋯, n s
as any subset such that
Pr(s i |s 1 , ⋯, s i−1 ) = Pr(s i |P A(s i )).
Similarly, we may define
P A(a i ) ⊆(s, a 1 , ⋯, a i−1 ), i = 1, ⋯, n a ; P A(s ′ i ) ⊆(u, s ′ 1 , ⋯, s ′ i−1 ), i = 1, ⋯, n s ; P A(o i ) ⊆(u, s ′ , o 1 , ⋯, o i−1 ), i = 1, ⋯, n o .
Together, we will have that
Pr(δ) = ns ∏ i=1 Pr(s i |P A(s i )) na ∏ j=1
Pr(a j |P A(a j ))
n k ∏ k=1 Pr(s ′ k |P A(s ′ k )) no ∏ l=1 Pr(o l |P A(o l )) = ∏ x∈δ Pr(x|P A(x)).
Letting the edges in G be given by the parent sets defined above, it is obvious that G represents P. Now we assume that the independent transition and causal faithfulness hold.</p>
<p>Assume that G contains a lateral edge like v 1 → v 2 for example.According to independent transition, we have
(v 1 Pr v 2 |u). Because v 1 ∈ P A(v 2 ), we have (v 1 ̸ G v 2 |u).
This violates the assumption of causal faithfulness, as we have
(v 1 Pr v 2 |u) / ⇒ (v 1 G v 2 |u)
. Therefore, we prove that no lateral edge among v exists in G.</p>
<p>Because there is no lateral edge among v exists in G, u −i blocks all paths form u i to v j unless u i ∈ P A(v j ).Therefore, it is easy to prove that
(u i ̸ G v j |u −i ) ⇔ u i ∈ P A(v j )
Combining Theorem 1 and Definition 3, we have that
(u i ̸ G v j |u −i ) ⇔ (u i ̸ Pr v j |u −i )
Therefore, the parents of v j are uniquely identified by the rule:
(u i ̸ Pr v j |u −i ) ⇔ u i ∈ P A(v j )
Finally, let us consider another probability function Pr * that satisfies Eq. 17, and assume G is the DAG that Pr * is compatible with and faithful to.For every v j ∈ v it follows that Pr(v j |u) = Pr * (v j |u) = P (v j |u) If u i ∈ P A(v j ) and u i / ∈ P A * (v j ), using the above rule we have (u i ̸ Pr v j |u −i ) and (u i Pr * v j |u −i ).In other words, we have
Pr(v j |u) ≠ Pr(v j |u −i ) Pr * (v j |u) = Pr * (v j |u −i ) This leads to that Pr(v j |u −i ) ≠ Pr * (v j |u −i )
We can also write that
Pr(v j |u −i ) = ∫ ui Pr(v j |u)Pr(u i |u −i ) = ∫ ui Pr * (v j |u)Pr(u i |u −i ) = ∫ ui Pr * (v j |u −i )Pr(u i |u −i ) =Pr * (v j |u −i ) ∫ ui Pr(u i |u −i ) =Pr * (v j |u −i ).
From the above equations, we obtain the paradox that
Pr(v j |u −i ) = Pr * (v j |u −i ).
Using reduction to absurdity, we obtain that u i ∈ P A(v j ) implies u i ∈ P A * (v j ).Similarly, we can prove the opposite direction of this implication.As a result, we have
u i ∈ P A(v j ) ⇔ u i ∈ P A * (v j ),
which shows that P A(v j ) = P A * (v j ).</p>
<p>Proof ends.</p>
<p>B Explanation through Causal Chains</p>
<p>In our paper, we only present the visualization of causal chains.Although this visualization offers a certain extent of interpretability, Madumal et al [2020b] have proposed techniques to better use causal chains to generate high-quality explanations.In this section, we introduce how our causal chains adapt to their techniques.</p>
<p>Consider an H-step trajectory (δ t , δ t+1 , ⋯, δ t+H−1 ), where δ t+k = (s t+k , a t+k , s t+k+1 , o t+k , r t+k ).In this trajectory, we use the bold capital letter C to denote the sub-set of variables in the causal chain.In Madumal's work, an explanation is derived from an "explanan", which contains information about how action leads to rewards.Definition 4 (Explanan).A H-step explanan for an action a t under a factual trajectory (δ t , ⋯, δ t+H−1 ) is a tuple ⟨x r , x h , x i ⟩, where 1. x r contains the reward variables in the causal chain.2. x h = s t ∩ C is the heading variables (the state variables at the beginning) in the causal chain.3. x i ⊂ C ∖ (x h , x i ) contains some intermediate variables between x h and x r .An explanation for "why the agent took action a t at s t " is generated by filling the values in the explanan into a naturallanguage template.If x i contains all intermediate variables, the explanan is called a complete explanan.However, it may contain too much information and difficult to be understood.Therefore, Madumal et al suggest using the minimally complete explanan (MCE), where x i contains only the parents of x r .</p>
<p>Taking the causal chain in the paper's Figure 5 for example, we present the explanations respectively drawn from the complete explanan and the minimally complete explanan below.</p>
<p>Example 1 (Complete Explanation for Build-Marine).The agent build supplies depots because this action causes the following changes:</p>
<ol>
<li>Instantly, the number of supply depots increases from 7 to 11, and money decreases from 505 to 330; 2. After 2 steps, the number of barracks increases from 1 to 3, and the amount of money increases from 330 to 465. 3.After 3 steps, the number of barracks increases from 3 to 6, and the amount of money decreases from 330 to 430. 4.After 4 steps, the number of marines increases from 0 to 6.</li>
</ol>
<p>Which lead to a reward of 6 due to new marines after 4 steps.</p>
<p>Example 2 (Minimally Complete Explanation for Build-Marine).The agent builds supplies depots because this action would eventually cause the number of marines to increase from 0 to 6 after 4 steps, which leads to a reward of 6 due to new marines.</p>
<p>In addition, by comparing two MCEs, we can construct contrastive explanations that answer why that agent did not take another action.Therefore, Madumal et al define the minimally complete contrastive explanation, which contains the difference between the factual MCE and the counterfactual MCE.</p>
<p>Definition 5 (Minimally Complete Contrastive Explanation).Let (δ t , ⋯, δ t+H−1 ) denote the factual trajectory (the trajectory that actually happens) and ( δt , ⋯, δt+H−1 ) denote the counterfactual trajectory, which is produced by replacing a t with another action ãt and using the world model and policy to simulate the following H steps. Assume that x = ⟨x r , x h , x i ⟩ is the MCE for a t under the factual trajectory and that y = ⟨y r , y h , y i ⟩ is the MCE for ãt under the counterfactural trajectory.A minimally complete contrastive explanation (MCCE) is then given by a tuple ⟨x dif f , y dif f , x r ⟩, where 1. x dif f is the subset of variables in x that 1) is not included in y, or 2) owns a value different from that in y.</p>
<ol>
<li>y dif f is the subset of variables in y that 1) is not included in x, or 2) owns a value different from that in x.</li>
</ol>
<p>3.</p>
<p>x r contains the reward variables in the factual causal chain.</p>
<p>C The Trade-off between interpretability and accuracy</p>
<p>Theorem 3. Assume v j is an endogenous variable of the SCM of the Factorized MDP.Let P A 1 (v j ) and P A 2 (v j ) respectively denote its parent sets discovered using the threshold η 1 and η 2 .Assume P A * (v j ) is the ground-truth parent set of v j .If η 1 ≤ η 2 , with the well trained structural equation
f j we have E u [D KL (f j (P A * (v j ))||f j (P A 1 (v j )))] ≥ E u <a href="18">D KL (f j (P A * (v j ))||f j (P A 2 (v j )))</a>
Proof.Obviously, we have P A 1 (v j ) ⊆ P A 2 (v j ).For simplicity, we define
a ∶= P A 1 (v j ) ∩ P A * (v j ), b ∶= (P A 2 (v j ) ∩ P A * (v j )) ∖ P A 1 (v j ), c ∶= P A * (v j ) ∖ P A 2 (v j ), d ∶= P A 1 (v j ) ∖ a, e ∶= P A 2 (v j ) ∖ (a, b, d),
where a, b, c, d, e are non-overlapping subsets of u.More specifically, a, b, c are the true parents of v j , whereas d, e are false parents of v j .Then we have
P A * (v j ) = (a, b, c), P A 1 (v j ) = (a, d), P A 2 (v j ) = (a, b, d, e),
We use ∼ to denote that a probability conforms to a given distribution.Noted that variables in d are not true parents of v j , with well trained f j , the posterior distribution of v j is given by
Pr(v j |a) = Pr(v j |a, d) ∼ f j (a, d) = f j (P A 1 (v j ))(19)
Therefore, we have
E u [D KL (f j (P A * (v j ))||f j (P A 1 (v j )))] =E u [D KL (f j (a, b, c)||f j (a, d))] = ∫ u Pr(u)du ∫ vj Pr(v j |a, b, c) log Pr(v j |a, b, c) Pr(v j |a, d) dv j = ∫ a,b,c Pr(a, b, c)d(a, b, c) ∫ vj Pr(v j |a, b, c) log Pr(v j |a, b, c) Pr(v j |a) dv j = ∫ a,b,c,vj Pr(a, b, c, v j ) log Pr(v j |a, b, c) Pr(v j |a) d(a, b, c, v j ) (20) Similarly, we have E u [D KL (f j (P A * (v j ))||f j (P A 2 (v j )))] =E u [D KL (f jE u [D KL (f j (P A * (v j ))||f j (P A 1 (v j )))] − E u [D KL (f j (P A * (v j ))||f j (P A 2 (v j )))] = ∫ a,b,c,vj Pr(a, b, c, v j ) log Pr(v j |a, b) Pr(v j |a) d(a, b, c, v j ) = ∫ a,b,vj ∫ c (Pr(a, b, c, v j )dc) log Pr(v j |a, b) Pr(v j |a) d(a, b, v j ) = ∫ a,b,vj Pr(a, b, v j ) log Pr(v j |a, b) Pr(v j |a) d(a, b, v j ) = ∫ a,b,vj Pr(a, b)Pr(v j |a, b) log Pr(v j |a, b) Pr(v j |a) d(a, b, v j ) = ∫ a,b Pr(a, b)D KL (f j (a, b)||f j (a))) d(a, b) = E a,b [D KL (f j (a, b)||f j (a)))] ≥ 0(22)
D The Algorithm for Model-Based RL Fit the structural equations by maximizing (12) 6:</p>
<p>for learning round j = 1, ..., n round do 7:</p>
<p>Generate simulated data by performing k-step model rollout using actor π 8:</p>
<p>Update the policy π using PPO algorithm 9:</p>
<p>end for 10: end for E Environments</p>
<p>E.1 Factorization of Public Environments</p>
<p>This section describes how environments are considered Factorized MDPs in our implementation.</p>
<p>Cartpole The state is factorized into 4 variables (x, ẋ, θ, θ) indicating the 1) position of the cart, 2) velocity of the cart, 3) angle of the pole, and 4) angle velocity of the pole.The action only contains one discrete variable indicating the direction (left or right) to push the cart.The goal of the agent is to keep the pole upright as long as possible.Therefore, the agent is rewarded by 1 for each step as long as the state satisfies −2.4 ≤ x ≤ 2.4 and −12 ○ ≤ θ ≤ 12 ○ .The episode terminates if this condition does not hold.In addition, no outcome variable is included in this environment.</p>
<p>Lunarlander-Discrete The state is factorized into 7 variables (x, y, ẋ, ẏ, θ, θ, legs) indicating the (1) horizontal position, (2) vertical position, (3) horizontal velocity, (4) vertical velocity, (5) angle, (6) angle velocity, and (7) whether the two legs are in contact with the ground.The action contains one discrete variable indicating the engine (none, main, left, or right) to be actuated.The environment contains 3 outcome variables, including (1) the fuel cost due to firing the engine, (2) whether the lander crashed, and (3) whether the rocket is resting.The agent is rewarded (or penalized) from multiple sources, including (1) shortening the distance to the destination, (2) reducing the velocity, (3) reducing the angle, (4) increasing the number of landed legs, (5) being resting, and (6) crashing.</p>
<p>Lunarlander-Continuous replaces the action space of Lunarlander-Discrete with a continuous action space.The new action space includes 2 continuous variables ranged in (−1, 1), respectively controlling the throttles of the main and the lateral engines.</p>
<p>Build Marine The state includes 6 variables denoted as (n wk , n mr , n br , n dp , money, time), namely the (1) number of workers, (2) number of marines, (3) number of barracks, (4) number of supply depots, (5) amount of money, and (6) game time.The action includes only one discrete variable indicating which unit (workers, marines, barracks, supply depots, or none) to be built.The player is rewarded with 1 for every newly produced marine.The problem is challenging since marines can only be built from barracks, and barracks can only be built provided there exists at least one supply depot.In addition, the number of workers and marines is limited by the number of supplies provided by supply depots.</p>
<p>E.2 The Environment for Measuring the Accuracy of Recovering Action Influence</p>
<p>To measure the accuracy of recovering the causal dependencies of the AIM, we design an additional environment with a known ground-truth AIM.The environment contains one action variable a and 5 state variables: x 1 , x 2 , x 3 , x 4 , and τ .The action variable a is chosen from 4 options {0, 1, 2, 3}.</p>
<p>The dynamics of these state variables are given by
x ′ 1 = x 1 + N (1, 1)(23)
x ′ 2 = {</p>
<p>x 1 , a = 0 x 2 , otherwise + N (0, 1) (24)
x ′ 3 = x 3 + ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩
x 1 , a = 0 x 2 , a = 1 5, a = 2 10, a = 3
+ N (0, 1)(25)
x ′ 4 = 0.1x 3 + 0.9x 4 + N (0, 0.5) (26)
τ ′ = τ + ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ 10, a = 0 20, a = 1 5, a = 2 5, a = 3(27)
The ground-truth causality of the AIM can be easily derived from the above dynamics.Aiming to confuse neural networks, these dynamics create copious spurious correlations in the data.For example, x 1 and τ present a strong positive correlation, whereas neither of them is the other's causation.</p>
<p>F Importance of Causality: an Example</p>
<p>To show how causal discovery facilitates explanation generation, we investigated the causal chains produced without a discovered causal graph.Figure 8 presents such a causal chain for Build-Marine.It is produced by our model using a full graph, where unreasonable connections are highlighted in bold and red.We found the model attributes a large influence weight (≈ 0.78) to time as the salient parent of n depot under the action of building supply depots.This leads to the incorrect understanding that "if we build supply depots, the number of supply depots turns to 4 because the game time is 140".In fact, the variable time only influences time ′ in the discovered causal graph presented in Figure 3(b), meaning the number of supply depots is not caused by the game time at all.The model is misled by the fact that the agent always builds more supply depots as time goes on, which makes the two variables (time and n ′ depot ) highly related.Similarly, the model wrongly takes time as the salient parent of money under the actions of building barracks since it observes that money usually accrues with time.Therefore, attention alone is insufficient to obtain reasonable causal chains for explanations, as it can be easily misled by spurious correlations (which are rife in the non-i.i.d.data collected in RL).Fortunately, these errors in causal chains are greatly reduced when combining attention with a causal graph, which precludes most spurious correlations and leads to correct causal chains.</p>
<p>G Hyper-parameters of Model-based RL</p>
<p>The main hyper-parameters used in the mentioned environments for model-based RL are presented in Table 2.</p>
<p>H Computational Complexity</p>
<p>Figure 1 :
1
Figure 1: The illustration of causal models in the Vacuum world.(a) illustrates the Vacuum world, where position = 1, clean1 = T rue, and clean2 = F alse.(b) and (c) respectively illustrate the causal graphs of the SCM and the AIM of the Vacuum world.</p>
<p>Figure 2 :
2
Figure 2: The illustration of the proposed framework.(a) shows an example of the causal graph identified by causal discovery.(b) illustrates the structure of the proposed model.(c) shows the inference network that approximates the structural equation of s ′ 3 .(d) illustrates the causal chain analysis, where the causal chain is highlighted in bold and green.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: An example of a 4-step causal chain on Lunarlander-Continuous</p>
<p>Figure 6 :
6
Figure 6: The training curves.Our explainable model (red) is compared to non-explainable dense models (green and blue) to show the performance cost of using a sparse causal graph.The grey curves show the performance without models.</p>
<p>1.</p>
<p>There exists a forward chain a → b → c, a backward chain a ← b ← c, or a fork a ← b → c in p such that b ∈ Z. 2. For every collision structure a → b ← c in p, Z does not contain b or any descendant of b.Theorem 1 (d-separation Criterion).Assume G is a DAG of a set of variables.Assume x, y, and z are disjoint subsets of variables.The following propositions hold: 1. (Global Markov Property [Peters et al., 2017]) Assuming Pr is a probability function such that Pr and G are compatible, then</p>
<p>Figure 7 :
7
Figure 7: two examples of spurious edges.The true causal relations are shown in solid arrows, where the backdoor paths mentioned are highlighted in red.The spurious edges are shown in red dashed lines.</p>
<p>(a, b, c)||f j (a, b, d, e))] = ∫ a,b,c,vj Pr(a, b, c, v j ) log Pr(v j |a, b, c) Pr(v j |a, b) d(a, b, c, v j )</p>
<p>Figure 8 :
8
Figure 8: An example of the causal chain produced by a full causal graph.We highlight the "problematic" edges in bold and red.</p>
<p>Let n = max(n s + n a , n s + n o ) roughly denote the number of variables of the environment.Let N denote the total number of transition samples.Let b denote the batch size.Model The parameter scale of our model is O(n).The time complexity of one forward pass is O(n 2 b).Causal discovery The time complexity of testing each edge through FCIT is O(nN log N ) and the overall time complexity of causal discovery is O(n 3 N log N ).The space complexity for causal discovery is O(nN ) if the tests are sequentially performed.Causal chains The time complexity and space complexity of generating an H-step causal chain are both O(n 2 H).In our experiments, the generation of each causal chain completes almost instantly.Parameter Cartpole LunarLander LunarLander-Continuous Build-Marine total epochs (n epoch ) main parameters used in model-based RL.The form "a → b" denotes the parameter gradually changes from a to b during the training process.</p>
<p>To evaluate the performance of our model in MBRL, we perform experiments in two extra environments: Cartpole and Lunarlander-Discrete.The Build-Marine environ-
next stateoutcomestateaction(a) Lunarlander-Continuousnext statebuild unitsstateaction(b) Build-MarineFigure 3: The discovered causal graphs of two environments.
[Brockman et al., 2016]e of the StartCraftII mini-games in SC2LE[Samvelyan et al., 2019]; the Cartpole and Lunarlander environments are classic control problems provided by OpenAI Gym[Brockman et al., 2016].Our source code is available at https://github.com/EaseOnway/Explainable-Causal-Reinforcement-Learning.</p>
<p>Amy Zhang, Rowan McAllister,  Roberto Calandra, Yarin Gal, andSergey Levine.Learning Invariant Representations for Reinforcement Learning without Reconstruction, April 2021.arXiv:2006.10742[cs, stat].</p>
<p>Model-based RL using causal world models 1: Initialize environment models and the policy π(a|s) 2: for training epoch i = 1, ..., n epoch do
See Algorithm 1.Algorithm 1 3: Interact with the environment using π; Add transitionsinto buffer D4:(for every n graph epochs) Update the causal graphthrough causal discovery5:
AcknowledgmentsThis work was supported in part by National Key R&amp;D Program of China (No.2022ZD0116405) and in part by the National Nature Science Foundation of China under Grant (62073324).
HIGH-LIGHTS: Summarizing Agent Behavior to People. Amir Amir, Dan Amir, Ofra Amir, Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems. the 17th International Conference on Autonomous Agents and Multiagent SystemsStockholm, Sweden2018. 2018</p>
<p>Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning. Atrey, arXiv:1912.057432020. February 2020</p>
<p>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. Bach, PLOS ONE. 107e01301402015. July 2015</p>
<p>. Brockman, arXiv:1606.01540OpenAI Gym. 2016. June 2016</p>
<p>Fast Conditional Independence Test for Vector Variables with Large Sample Sizes. Chalupka, arXiv:1804.027472018. April 2018cs, stat</p>
<p>Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. Chua, arXiv:1805.121142018. November 2018cs, stat</p>
<p>The rational use of causal inference to guide reinforcement learning strengthens with age. npj Science of Learning. Cohen, 2020. December 2020516</p>
<p>Distilling Deep Reinforcement Learning Policies in Soft Decision Trees. Coppens, Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence. the IJCAI 2019 Workshop on Explainable Artificial Intelligence2019. 2019</p>
<p>Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning. Dietterich, arXiv:1806.015842018. June 2018cs, stat</p>
<p>Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. Ding, arXiv:2207.090812022. July 2022cs, stat</p>
<p>Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents. Fukuchi, arXiv:1906.08253Proceedings of the 5th International Conference on Human Agent Interaction. the 5th International Conference on Human Agent InteractionBielefeld GermanyACM2017. October 2017. 2019. 2019. November 20214When to Trust Your Model: Model-Based Policy Optimization. cs, stat</p>
<p>Explainable Reinforcement Learning via Reward Decomposition. Kim ; Ho-Taek Joo, Kyung-Joong Joo, ; Kim, Juozapaitis, arXiv:1903.00374Visualization of Deep Reinforcement Learning using Grad-CAM: How AI Plays Atari Games? In 2019 IEEE Conference on Games (CoG). Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski, 2019. 2019. 2019. 2019. 2020. February 2020Model-Based Reinforcement Learning for Atari. cs, stat</p>
<p>Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning. Ke, arXiv:2107.008482021. July 2021cs, stat</p>
<p>Learning Finite State Representations of Recurrent Policy Networks. Koul, arXiv:1811.125302018. November 2018cs, stat</p>
<p>Explainable Artificial Intelligence: Concepts, Applications, Research Challenges and Visions. Longo, Machine Learning and Knowledge Extraction. 202012279</p>
<p>Deconfounding Reinforcement Learning in Observational Settings. Lu, arXiv:1812.10576Lecture Notes in Computer Science. 2020. 2018. December 2018Springer International PublishingSeries Title. cs, stat</p>
<p>Madumal, arXiv:2001.10284Distal Explanations for Model-free Explainable Reinforcement Learning, September 2020. 2020a</p>
<p>Explainable Reinforcement Learning through a Causal Lens. Madumal, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020b. April 202034</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. Nagabandi, 2018 IEEE International Conference on Robotics and Automation (ICRA). 2018. 2018</p>
<p>Causal Induction from Visual Observations for Goal Directed Tasks. Nair, arXiv:1910.017512019. October 2019cs, stat</p>
<p>Free-Lunch Saliency via Attention in Atari Agents. Nikulin, 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). Seoul, Korea (South; Chichester; West SussexWiley2019. 2016. 2016Causal inference in statistics: a primer</p>
<p>Elements of causal inference: foundations and learning algorithms. Adaptive computation and machine learning series. ; Pearl, Peters, 2000. 2000. 2017. 2017The MIT PressCambridge, U.K.; New York; Cambridge, MassachuesttsCausality: models, reasoning, and inference</p>
<p>Explainable Reinforcement Learning: A Survey. Veith Puiutta, arXiv:2005.062472020. May 2020Erika Puiutta and Eric MSP Veith. cs, stat</p>
<p>Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics. Raffin , arXiv:1901.086512019. June 2019cs, stat</p>
<p>Artificial intelligence: a modern approach. Prentice Hall series in artificial intelligence. Russell , 2010. 2010TabishUpper Saddle River3rd ed edition. Samvelyan et al., 2019] Mikayel Samvelyan</p>
<p>. Christian Rashid, Gregory Schroeder De Witt, Nantas Farquhar, Nardelli, G J Tim, Chia-Man Rudner, Hung, H S Philip, Jakob Torr, Shimon Foerster, Whiteson, arXiv:1902.04043The StarCraft Multi-Agent Challenge. December 2019cs, stat</p>
<p>Proximal Policy Optimization Algorithms. Schulman, arXiv:1707.063472017. August 2017</p>
<p>Causal Influence Detection for Improving Efficiency in Reinforcement Learning. Seitzer, arXiv:2106.034432021. December 2021</p>
<p>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. Selvaraju, arXiv:1610.02391International Journal of Computer Vision. 12822020. February 2020</p>
<p>Self-Supervised Discovering of Interpretable Features for Reinforcement Learning. Shi, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2021. 2021</p>
<p>Causal Models: How People Think about the World and Its Alternatives. Steven Sloman, ; Sloman, Spirtes, 2005. 08 2005. 2001. 2001Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search. The MIT Press</p>
<p>Provably Efficient Causal Reinforcement Learning with Confounded Observational Data. Sergei Volodin, Causeoccam, Wang, arXiv:2104.02297Learning Interpretable Abstract Representations in Reinforcement Learning Environments via Model Sparsity. Xiaoqian Wang, and David I. Inouye. Shapley Explanation Networks2021. 2021a. 2021. 2021b. April 202134École Polytechnique Fédérale de LausannePhD thesisAdvances in Neural Information Processing Systems</p>
<p>Causal Dynamics Learning for Task-Independent State Abstraction. Wang , arXiv:2206.134522022. June 2022</p>
<p>Training a Resilient Q-Network against Observational Interference. Yang , arXiv:2102.096772022. January 2022cs, eess] version: 3</p>
<p>What Did You Think Would Happen? Explaining Agent Behaviour through Intended Outcomes. Yau, Proceedings of the 34th Conference on Neural Information Processing Systems. the 34th Conference on Neural Information Processing SystemsVancouver, Canada2020. 2020</p>
<p>Invariant Causal Prediction for Block MDPs. Zhang, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020. November 2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3127 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3127</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3127</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-ad46feec0cefb78dafa60821598c052cf0b9b7bd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ad46feec0cefb78dafa60821598c052cf0b9b7bd" target="_blank">NT5?! Training T5 to Perform Numerical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that training the T5 multitasking framework with multiple numerical reasoning datasets of increasing difficulty, good performance on DROP can be achieved without manually engineering partitioned functionality between distributed and symbol modules.</p>
                <p><strong>Paper Abstract:</strong> Numerical reasoning over text (NRoT) presents unique challenges that are not well addressed by existing pre-training objectives. We explore five sequential training schedules that adapt a pre-trained T5 model for NRoT. Our final model is adapted from T5, but further pre-trained on three datasets designed to strengthen skills necessary for NRoT and general reading comprehension before being fine-tuned on the Discrete Reasoning over Text (DROP) dataset. The training improves DROP's adjusted F1 performance (a numeracy-focused score) from 45.90 to 70.83. Our model closes in on GenBERT (72.4), a custom BERT-Base model using the same datasets with significantly more parameters. We show that training the T5 multitasking framework with multiple numerical reasoning datasets of increasing difficulty, good performance on DROP can be achieved without manually engineering partitioned functionality between distributed and symbol modules.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3127.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3127.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NT5 (Numeric T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-Small encoder-decoder model sequentially pre-trained on synthetic numeric/textual datasets and SQuAD, then fine-tuned on DROP and a DROP-derived classification task to internalize numerical reasoning without explicit symbolic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Small (NT5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-Small encoder-decoder (approx. 60M parameters) initialized from pre-trained T5 and further pre-trained sequentially on NUM and TXT synthetic datasets (and SQuAD in some schedules), with multitask training using temperature scaling and special dataset-identification tokens; finally fine-tuned on DROP and a DROP-derived classification task.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Number questions from DROP: subtraction, comparison, selection, addition, counting, sorting, date arithmetic, other arithmetic; synthetic NUM tasks covering seven numerical skill types (synthetic single- and multi-step numeric word problems).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Distributed neural internalization of numerical reasoning within the T5 model (no explicit symbolic arithmetic module); model learns to map text to numeric answers via pretraining on synthetic arithmetic examples and multitask finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Substantial performance gains after sequential pretraining: numeric-question F1 increased from 31.83 (baseline T5) to ≈70.3 (best NT5 dev) and overall DROP F1 from ~44.6 to ~70.3; ablation-like comparisons across training schedules show synthetic NUM/TXT and SQuAD stages improve numeric and RC performance respectively; error analysis shows partial digit matches indicating learned surface/representation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Error analysis reveals systematic mistakes (partial digit matches, wrong arithmetic operations instead of extraction) and diminished numeric performance in some multitask setups (e.g., adding SQuAD sometimes slightly reduces numeric accuracy), suggesting the learned mechanism is not a robust algorithmic calculator and may rely on pattern-generalization/memorization rather than stable symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Sequential pre-training on synthetic numeric (NUM) and textual (TXT) datasets; adding SQuAD for reading comprehension; a DROP-derived classification auxiliary task; multitask training vs staged training; validation selection on synthetic dev sets vs DROP dev set.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Sequential pre-training plus fine-tuning dramatically improved numeric and overall performance (numeric F1 from 31.83 to ~70.39 in best schedule); incorporating SQuAD improved extractive RC categories (date/span/spans) by ≈3.06 F1 on average but sometimes slightly degraded numeric performance; full simultaneous multitask training underperformed staged schedules (~6 F1 lower).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best NT5: dev 66.8 EM and 70.3 F1; test 67.0 EM and 70.8 F1 (overall DROP). Baseline T5-Small overall F1 44.64; numeric-only baseline F1 31.83. Decomposed best numeric F1 reported ≈70.39 in RC Experiment 1. Improvements reported as adjusted F1 (numeracy-focused) from 45.90 to 70.83 in abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Partial digit matches (model outputs share some digits with ground truth but are incorrect), performing incorrect arithmetic when extraction would suffice (e.g., trying to compute rather than extract), errors on date arithmetic (39/86 sampled date errors required arithmetic; 9/86 wrong calculations), and failures on reasoning types not covered by pretraining (many span/spans errors).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>NT5 (purely neural) approaches performance of GenBERT (reported ~72.4 F1) but underperforms state-of-the-art hybrid/symbolic approaches (e.g., QDGAT-ALBERT) and human performance on DROP; authors note that hybrid models using explicit symbolic modules still outperform NT5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NT5?! Training T5 to Perform Numerical Reasoning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3127.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3127.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla T5-Small baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The unadapted pre-trained T5-Small model fine-tuned only on DROP (baseline) to evaluate how much sequential numeric pretraining helps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Small (vanilla baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained T5-Small (approx. 60M parameters) without the additional numeric/textual or RC pretraining stages used for NT5; fine-tuned directly on DROP as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>DROP number questions (subtraction, addition, count, comparison, sort, date arithmetic, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Not explicitly analyzed for mechanism in this paper; functions as baseline to show limited numeric competence without targeted numeric pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Performance numbers show limited numeric reasoning: baseline numeric F1 31.83 and overall F1 44.64, implying that default pretrained T5 lacks strong numerical reasoning for DROP-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Baseline fails on many numeric items and benefits substantially from targeted numeric pretraining, indicating the vanilla model does not internalize robust arithmetic behavior for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning only on DROP (no synthetic NUM/TXT or SQuAD pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Serves as control — poor numeric performance compared to NT5; demonstrates large gains are attainable via the interventions used for NT5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline DROP overall F1 44.64; numeric-only F1 31.83 (EM/F1 broken down in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Low numeric F1, inability to handle many arithmetic question types without further pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Baseline T5 underperforms specialized models (GenBERT, NumNet, QDGAT) that incorporate numeric-specific adaptations or symbolic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NT5?! Training T5 to Perform Numerical Reasoning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3127.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3127.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenBERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-Base model adapted with specialized heads for discrete numerical reasoning, trained on similar synthetic datasets to improve numeracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Injecting numerical reasoning skills into language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GenBERT (BERT-Base variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Custom BERT-Base model (~110M parameters) with specialized prediction heads designed to handle discrete/numeric reasoning tasks; trained/fine-tuned on synthetic numeric datasets similar to those used in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>DROP number questions and synthetic numeric tasks (discrete arithmetic over text).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Specialized architectural heads that target discrete numerical reasoning, effectively adding components adapted to numeric operations (engineered modules rather than purely distributed internalization).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Referenced performance: GenBERT achieves ~72.4 F1 (reported in paper) and is cited as comparable/strong; described in related work as inspiration for using synthetic pretraining to inject numeric skills.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>This paper does not provide primary empirical probing or ablation of GenBERT; limited to comparative performance mention.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Model architecture modification (specialized heads) and training on synthetic numeric datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Reported higher F1 (≈72.4) than baseline and comparable to NT5; demonstrates that explicit numeric-oriented heads improve DROP performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in this paper as GenBERT ≈72.4 F1 (no additional breakdown provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not analyzed in this paper (only mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>GenBERT is positioned between vanilla neural and hybrid symbolic approaches — it uses neural architecture modifications to better handle discrete reasoning and outperforms vanilla T5 baseline but is similar in scale to other tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NT5?! Training T5 to Perform Numerical Reasoning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3127.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3127.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QDGAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question Directed Graph Attention Network (QDGAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art model for DROP that combines an ALBERT language encoder with a directed graph attention network and a prediction module for discrete reasoning (a hybrid neural+symbolic approach).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Question directed graph attention network for numerical reasoning over text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QDGAT (with ALBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid framework combining an ALBERT language representation extractor and a directed graph attention network prediction module specialized for discrete/numerical reasoning; models reported are large (RoBERTa/ALBERT-based configurations, cited up to 355M parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>DROP number questions requiring discrete arithmetic, graph-structured reasoning, and composition of numeric operations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Explicit hybrid mechanism using graph attention and a dedicated discrete reasoning/prediction module that models numeric relations and operations symbolically/nearly-symbolically outside of the pure language encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Reported as current state-of-the-art on DROP (cited in paper) and described as using separate language and discrete reasoning modules; outperforming purely neural NT5 in the authors' comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not contradicted in this paper; authors note hybrid approach achieves better performance than their purely neural NT5.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural combination of language encoder (ALBERT) with graph attention prediction module (a symbolic-like discrete reasoning component).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Leads to top performance on DROP (exceeds NT5), indicating benefit of separating representation and discrete arithmetic reasoning into specialized modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not numerically enumerated in detail here, but cited as outperforming NT5 and as the current state-of-the-art in DROP at time of writing.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not analyzed in this paper (only referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>QDGAT exemplifies hybrid neural + symbolic design and serves as a benchmark showing that adding discrete/symbolic reasoning modules improves DROP performance relative to purely neural approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NT5?! Training T5 to Perform Numerical Reasoning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3127.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3127.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NumNet / NumNet+</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reading-comprehension model with explicit numerical reasoning components used for DROP-style tasks; NumNet+ variants use stronger language encoders (e.g., RoBERTa-Large).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NumNet: Machine reading comprehension with numerical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NumNet / NumNet+ (RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that augments a language encoder with modules to perform numerical reasoning over text; reported in variants using RoBERTa-Large (cited model sizes up to 355M parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>DROP-style numeric questions requiring discrete arithmetic operations and reasoning about numbers in passages.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Separation of language representation and explicit numerical reasoning modules (symbolic/discrete components) to perform arithmetic over text.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Referenced as prior work that uses symbolic/discrete modules and achieves strong performance on DROP; cited for using explicit numeric reasoning components.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not directly challenged in this paper; authors present NT5 as an alternative that achieves competitive results without explicit symbolic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural addition of numeric reasoning modules to a neural language encoder (e.g., RoBERTa).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Produces strong DROP performance in prior work (cited as a high-performing baseline compared to purely neural NT5 in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Specific NumNet+ numbers are not provided in this paper beyond listing as a competitive model and parameter counts (RoBERTa-Large, ~355M).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not analyzed here (only referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>NumNet represents an explicit hybrid/symbolic approach that the authors contrast with their purely neural NT5; such hybrid models are noted to outperform NT5 on DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NT5?! Training T5 to Perform Numerical Reasoning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Question directed graph attention network for numerical reasoning over text <em>(Rating: 2)</em></li>
                <li>NumNet: Machine reading comprehension with numerical reasoning <em>(Rating: 2)</em></li>
                <li>Giving BERT a calculator: Finding operations and arguments with reading comprehension <em>(Rating: 1)</em></li>
                <li>Do nlp models know numbers? probing numeracy in embeddings <em>(Rating: 1)</em></li>
                <li>Equate : A benchmark evaluation framework for quantitative reasoning in natural language inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3127",
    "paper_id": "paper-ad46feec0cefb78dafa60821598c052cf0b9b7bd",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "NT5",
            "name_full": "NT5 (Numeric T5)",
            "brief_description": "A T5-Small encoder-decoder model sequentially pre-trained on synthetic numeric/textual datasets and SQuAD, then fine-tuned on DROP and a DROP-derived classification task to internalize numerical reasoning without explicit symbolic modules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-Small (NT5)",
            "model_description": "T5-Small encoder-decoder (approx. 60M parameters) initialized from pre-trained T5 and further pre-trained sequentially on NUM and TXT synthetic datasets (and SQuAD in some schedules), with multitask training using temperature scaling and special dataset-identification tokens; finally fine-tuned on DROP and a DROP-derived classification task.",
            "arithmetic_task_type": "Number questions from DROP: subtraction, comparison, selection, addition, counting, sorting, date arithmetic, other arithmetic; synthetic NUM tasks covering seven numerical skill types (synthetic single- and multi-step numeric word problems).",
            "reported_mechanism": "Distributed neural internalization of numerical reasoning within the T5 model (no explicit symbolic arithmetic module); model learns to map text to numeric answers via pretraining on synthetic arithmetic examples and multitask finetuning.",
            "evidence_for_mechanism": "Substantial performance gains after sequential pretraining: numeric-question F1 increased from 31.83 (baseline T5) to ≈70.3 (best NT5 dev) and overall DROP F1 from ~44.6 to ~70.3; ablation-like comparisons across training schedules show synthetic NUM/TXT and SQuAD stages improve numeric and RC performance respectively; error analysis shows partial digit matches indicating learned surface/representation patterns.",
            "evidence_against_mechanism": "Error analysis reveals systematic mistakes (partial digit matches, wrong arithmetic operations instead of extraction) and diminished numeric performance in some multitask setups (e.g., adding SQuAD sometimes slightly reduces numeric accuracy), suggesting the learned mechanism is not a robust algorithmic calculator and may rely on pattern-generalization/memorization rather than stable symbolic computation.",
            "intervention_type": "Sequential pre-training on synthetic numeric (NUM) and textual (TXT) datasets; adding SQuAD for reading comprehension; a DROP-derived classification auxiliary task; multitask training vs staged training; validation selection on synthetic dev sets vs DROP dev set.",
            "effect_of_intervention": "Sequential pre-training plus fine-tuning dramatically improved numeric and overall performance (numeric F1 from 31.83 to ~70.39 in best schedule); incorporating SQuAD improved extractive RC categories (date/span/spans) by ≈3.06 F1 on average but sometimes slightly degraded numeric performance; full simultaneous multitask training underperformed staged schedules (~6 F1 lower).",
            "performance_metrics": "Best NT5: dev 66.8 EM and 70.3 F1; test 67.0 EM and 70.8 F1 (overall DROP). Baseline T5-Small overall F1 44.64; numeric-only baseline F1 31.83. Decomposed best numeric F1 reported ≈70.39 in RC Experiment 1. Improvements reported as adjusted F1 (numeracy-focused) from 45.90 to 70.83 in abstract.",
            "notable_failure_modes": "Partial digit matches (model outputs share some digits with ground truth but are incorrect), performing incorrect arithmetic when extraction would suffice (e.g., trying to compute rather than extract), errors on date arithmetic (39/86 sampled date errors required arithmetic; 9/86 wrong calculations), and failures on reasoning types not covered by pretraining (many span/spans errors).",
            "comparison_to_humans_or_symbolic": "NT5 (purely neural) approaches performance of GenBERT (reported ~72.4 F1) but underperforms state-of-the-art hybrid/symbolic approaches (e.g., QDGAT-ALBERT) and human performance on DROP; authors note that hybrid models using explicit symbolic modules still outperform NT5.",
            "uuid": "e3127.0",
            "source_info": {
                "paper_title": "NT5?! Training T5 to Perform Numerical Reasoning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "T5-baseline",
            "name_full": "Vanilla T5-Small baseline",
            "brief_description": "The unadapted pre-trained T5-Small model fine-tuned only on DROP (baseline) to evaluate how much sequential numeric pretraining helps.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-Small (vanilla baseline)",
            "model_description": "Pre-trained T5-Small (approx. 60M parameters) without the additional numeric/textual or RC pretraining stages used for NT5; fine-tuned directly on DROP as baseline.",
            "arithmetic_task_type": "DROP number questions (subtraction, addition, count, comparison, sort, date arithmetic, etc.).",
            "reported_mechanism": "Not explicitly analyzed for mechanism in this paper; functions as baseline to show limited numeric competence without targeted numeric pretraining.",
            "evidence_for_mechanism": "Performance numbers show limited numeric reasoning: baseline numeric F1 31.83 and overall F1 44.64, implying that default pretrained T5 lacks strong numerical reasoning for DROP-style tasks.",
            "evidence_against_mechanism": "Baseline fails on many numeric items and benefits substantially from targeted numeric pretraining, indicating the vanilla model does not internalize robust arithmetic behavior for these tasks.",
            "intervention_type": "Fine-tuning only on DROP (no synthetic NUM/TXT or SQuAD pretraining).",
            "effect_of_intervention": "Serves as control — poor numeric performance compared to NT5; demonstrates large gains are attainable via the interventions used for NT5.",
            "performance_metrics": "Baseline DROP overall F1 44.64; numeric-only F1 31.83 (EM/F1 broken down in Table 4).",
            "notable_failure_modes": "Low numeric F1, inability to handle many arithmetic question types without further pretraining.",
            "comparison_to_humans_or_symbolic": "Baseline T5 underperforms specialized models (GenBERT, NumNet, QDGAT) that incorporate numeric-specific adaptations or symbolic modules.",
            "uuid": "e3127.1",
            "source_info": {
                "paper_title": "NT5?! Training T5 to Perform Numerical Reasoning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "GenBERT",
            "name_full": "GenBERT",
            "brief_description": "A BERT-Base model adapted with specialized heads for discrete numerical reasoning, trained on similar synthetic datasets to improve numeracy.",
            "citation_title": "Injecting numerical reasoning skills into language models",
            "mention_or_use": "mention",
            "model_name": "GenBERT (BERT-Base variant)",
            "model_description": "Custom BERT-Base model (~110M parameters) with specialized prediction heads designed to handle discrete/numeric reasoning tasks; trained/fine-tuned on synthetic numeric datasets similar to those used in this work.",
            "arithmetic_task_type": "DROP number questions and synthetic numeric tasks (discrete arithmetic over text).",
            "reported_mechanism": "Specialized architectural heads that target discrete numerical reasoning, effectively adding components adapted to numeric operations (engineered modules rather than purely distributed internalization).",
            "evidence_for_mechanism": "Referenced performance: GenBERT achieves ~72.4 F1 (reported in paper) and is cited as comparable/strong; described in related work as inspiration for using synthetic pretraining to inject numeric skills.",
            "evidence_against_mechanism": "This paper does not provide primary empirical probing or ablation of GenBERT; limited to comparative performance mention.",
            "intervention_type": "Model architecture modification (specialized heads) and training on synthetic numeric datasets.",
            "effect_of_intervention": "Reported higher F1 (≈72.4) than baseline and comparable to NT5; demonstrates that explicit numeric-oriented heads improve DROP performance.",
            "performance_metrics": "Reported in this paper as GenBERT ≈72.4 F1 (no additional breakdown provided here).",
            "notable_failure_modes": "Not analyzed in this paper (only mentioned).",
            "comparison_to_humans_or_symbolic": "GenBERT is positioned between vanilla neural and hybrid symbolic approaches — it uses neural architecture modifications to better handle discrete reasoning and outperforms vanilla T5 baseline but is similar in scale to other tuned models.",
            "uuid": "e3127.2",
            "source_info": {
                "paper_title": "NT5?! Training T5 to Perform Numerical Reasoning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "QDGAT",
            "name_full": "Question Directed Graph Attention Network (QDGAT)",
            "brief_description": "A state-of-the-art model for DROP that combines an ALBERT language encoder with a directed graph attention network and a prediction module for discrete reasoning (a hybrid neural+symbolic approach).",
            "citation_title": "Question directed graph attention network for numerical reasoning over text",
            "mention_or_use": "mention",
            "model_name": "QDGAT (with ALBERT)",
            "model_description": "Hybrid framework combining an ALBERT language representation extractor and a directed graph attention network prediction module specialized for discrete/numerical reasoning; models reported are large (RoBERTa/ALBERT-based configurations, cited up to 355M parameters).",
            "arithmetic_task_type": "DROP number questions requiring discrete arithmetic, graph-structured reasoning, and composition of numeric operations.",
            "reported_mechanism": "Explicit hybrid mechanism using graph attention and a dedicated discrete reasoning/prediction module that models numeric relations and operations symbolically/nearly-symbolically outside of the pure language encoder.",
            "evidence_for_mechanism": "Reported as current state-of-the-art on DROP (cited in paper) and described as using separate language and discrete reasoning modules; outperforming purely neural NT5 in the authors' comparisons.",
            "evidence_against_mechanism": "Not contradicted in this paper; authors note hybrid approach achieves better performance than their purely neural NT5.",
            "intervention_type": "Architectural combination of language encoder (ALBERT) with graph attention prediction module (a symbolic-like discrete reasoning component).",
            "effect_of_intervention": "Leads to top performance on DROP (exceeds NT5), indicating benefit of separating representation and discrete arithmetic reasoning into specialized modules.",
            "performance_metrics": "Not numerically enumerated in detail here, but cited as outperforming NT5 and as the current state-of-the-art in DROP at time of writing.",
            "notable_failure_modes": "Not analyzed in this paper (only referenced).",
            "comparison_to_humans_or_symbolic": "QDGAT exemplifies hybrid neural + symbolic design and serves as a benchmark showing that adding discrete/symbolic reasoning modules improves DROP performance relative to purely neural approaches.",
            "uuid": "e3127.3",
            "source_info": {
                "paper_title": "NT5?! Training T5 to Perform Numerical Reasoning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "NumNet",
            "name_full": "NumNet / NumNet+",
            "brief_description": "A reading-comprehension model with explicit numerical reasoning components used for DROP-style tasks; NumNet+ variants use stronger language encoders (e.g., RoBERTa-Large).",
            "citation_title": "NumNet: Machine reading comprehension with numerical reasoning",
            "mention_or_use": "mention",
            "model_name": "NumNet / NumNet+ (RoBERTa)",
            "model_description": "Framework that augments a language encoder with modules to perform numerical reasoning over text; reported in variants using RoBERTa-Large (cited model sizes up to 355M parameters).",
            "arithmetic_task_type": "DROP-style numeric questions requiring discrete arithmetic operations and reasoning about numbers in passages.",
            "reported_mechanism": "Separation of language representation and explicit numerical reasoning modules (symbolic/discrete components) to perform arithmetic over text.",
            "evidence_for_mechanism": "Referenced as prior work that uses symbolic/discrete modules and achieves strong performance on DROP; cited for using explicit numeric reasoning components.",
            "evidence_against_mechanism": "Not directly challenged in this paper; authors present NT5 as an alternative that achieves competitive results without explicit symbolic modules.",
            "intervention_type": "Architectural addition of numeric reasoning modules to a neural language encoder (e.g., RoBERTa).",
            "effect_of_intervention": "Produces strong DROP performance in prior work (cited as a high-performing baseline compared to purely neural NT5 in this paper).",
            "performance_metrics": "Specific NumNet+ numbers are not provided in this paper beyond listing as a competitive model and parameter counts (RoBERTa-Large, ~355M).",
            "notable_failure_modes": "Not analyzed here (only referenced).",
            "comparison_to_humans_or_symbolic": "NumNet represents an explicit hybrid/symbolic approach that the authors contrast with their purely neural NT5; such hybrid models are noted to outperform NT5 on DROP.",
            "uuid": "e3127.4",
            "source_info": {
                "paper_title": "NT5?! Training T5 to Perform Numerical Reasoning",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Question directed graph attention network for numerical reasoning over text",
            "rating": 2
        },
        {
            "paper_title": "NumNet: Machine reading comprehension with numerical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Giving BERT a calculator: Finding operations and arguments with reading comprehension",
            "rating": 1
        },
        {
            "paper_title": "Do nlp models know numbers? probing numeracy in embeddings",
            "rating": 1
        },
        {
            "paper_title": "Equate : A benchmark evaluation framework for quantitative reasoning in natural language inference",
            "rating": 1
        }
    ],
    "cost": 0.01126575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NT5?! Training T5 to Perform Numerical Reasoning</h1>
<p>Peng-Jian Yang ${ }^{a}$; Ying Ting Chen ${ }^{a}$; Yuechan Chen ${ }^{a}$, Daniel Cer ${ }^{a, b}$<br>{lesterpjy, chentim, sonyachan, dcer}@berkeley.edu<br>${ }^{a}$ University of California Berkeley, CA<br>${ }^{b}$ Google Research Mountain View, CA</p>
<h4>Abstract</h4>
<p>Numerical reasoning over text (NRoT) presents unique challenges that are not well addressed by existing pre-training objectives. We explore five sequential training schedules that adapt a pre-trained T5 model for NRoT. Our final model is adapted from T5, but further pre-trained on three datasets designed to strengthen skills necessary for NRoT and general reading comprehension before being fine-tuned on the Discrete Reasoning over Text (DROP) dataset. The training improves DROP's adjusted F1 performance (a numeracy-focused score) from 45.90 to 70.83. Our model closes in on GenBERT (72.4), a custom BERT-Base model using the same datasets with significantly more parameters. We show that training the T5 multitasking framework with multiple numerical reasoning datasets of increasing difficulty, good performance on DROP can be achieved without manually engineering partitioned functionality between distributed and symbol modules.</p>
<h2>1 Introduction</h2>
<p>Numerical Reasoning over Text (NRoT) is a reading comprehension task that involves producing an answer to numerical question given a short passage as context. Unlike reading comprehension tasks that can be solved by extracting the answer verbatim from the passage, NRoT usually involves using the question to determine the correct mathematical operation(s) while also identifying the correct values from the passage to use.</p>
<p>Research interest in NRoT has grown with the introduction of the Discrete Reasoning Over Paragraphs (DROP) dataset (Dua et al., 2019). The majority of DROP examples are number questions involving arithmetic, which has motivated complex models that combine symbolic and neural processing modules (Andor et al., 2019; Ran et al., 2019;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Chen et al., 2020). The best performing DROP model utilize a symbolic arithmetic module in conjunction with a neural network and other techniques such as ensembling.</p>
<p>We demonstrate in this work that manually engineered partitioning of the functionality between distributed and symbol modules is unnecessary for achieving good performance. Rather, the recently introduced Text-to-Text Transfer Transformer (T5) (Raffel et al., 2020) model is able to internalize NRoT without adaptation. We take full advantage of the multitasking ability of T5 to introduce a sequential training pipeline that is low resource, amiable to experimental cycle, and even achieves good performance using smaller scale models. ${ }^{1}$</p>
<h2>2 T5 for Numerical Reasoning over Text</h2>
<p>We propose five training pipelines for NRoT using T5, each consisting of two stages: pre-training on NRoT and general reading comprehension followed by fine-tuning on DROP and a classification task derived from DROP (Figure 1). Multitask training as described in the T5 paper is used in each stage of training: different datasets are combined with temperature scaling and a special token for identification. Unless specified otherwise, we validate on all the datasets in each respective stage. The first stage begins with a pre-trained T5Small model (Raffel et al., 2020). Each following stage, the model begins training on the best performing model from the previous stage. Our first two configurations (Validation Experiments 1\&amp;2) are designed to test the performance of selecting the best models using different validation data. We experimented with validating on the DROP dev set versus validating on the dev sets of the synthetic</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Summary of the five training pipelines. The validation and RC experiments are pre-trained sequentially on NUM, TXT, and SQuAD before fine-tuning on DROP and DROP classification. All experiments begin with pre-trained T5-Small. RC experiment 2 moves the RC training from pre-training to fine-tuning by combining SQuAD, DROP classification, and DROP with multitasking. Multitask experiment is our attempt to multitask-train on all datasets prior to fine-tuning.</p>
<p>Datasets (NUM/TXT) described in detail in Section 3. The next two experiments (RC Experiments 1&amp;2) attempt to strengthen reading comprehension by multitask training using SQuAD. Finally, we attempt multitasking on all datasets simultaneously (Multitask Experiment). Multitask Experiment is trained with validation on DROP only, instead of validation on the synthetic datasets (NUM/TXT) due to concerns with the model learning parameters for the synthetic datasets closer to fine-tuning and test time. The SQuAD dataset is introduced as an extra step for learning complicated language tasks in RC1 and RC2 for this reason. Since SQuAD is included in the single first step for pre-training in the Multitask experiment, we have deliberately avoided validating its pre-training step on the synthetic datasets. Based on the original T5 paper, we hypothesize that multitasking without stages would be the best way to achieve optimal performance.</p>
<h2>3 Datasets</h2>
<p><strong>DROP</strong> Discrete Reasoning Over Paragraphs (DROP) introduced by AllenNLP in 2019 (Dua et al., 2019), is a crowdsourced, adversarially-created 96k question benchmark. The benchmark consists of four types of questions, which can be answered using the context provided. Approximately 61% of the examples in DROP are number questions that involves arithmetic. The other types are "single-span" (32%), "spans" (6%), and "date" (2%). Note that all four question types in DROP can require NRoT skills, as shown in Table 1.</p>
<p><strong>Synthetic Data</strong> Two synthetic datasets tailored to boost performance on DROP are developed by (Geva et al., 2020). The Numeric dataset (NUM) consists of near 1M synthetically generated questions on seven types of numerical skills. Textual dataset (TXT) builds on NUM, and includes 2M</p>
<table>
<thead>
<tr>
<th>Reasoning</th>
<th>Passage (shorten)</th>
<th>Question</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Count + Sort</td>
<td>Denver would retake the lead. . . yet Carolina answered as kicker John Kasay ties the game with a 39-yard field goal. . . . Carolina closed out the half with Kasay nailing a 44-yard field goal. . . . In the fourth quarter, Carolina sealed the win with Kasay’s 42-yard field goal.</td>
<td>Which kicker kicked the most field goals?</td>
<td>John Kasay</td>
</tr>
<tr>
<td>Subtraction</td>
<td>That year, his Untitled (1981), a painting. . . was sold by Robert Lehrman for 16.3 million, well above its 12 million high estimate.</td>
<td>How many more dollars was the Untitled (1981) painting sold for than the 12 million dollar estimation?</td>
<td>4300000</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of QA pairs found in DROP. The question types and distribution in DROP are subtraction (28.8%), comparison (18.2%), selection (19.4%), addition (11.7%), count (16.5%), sort (11.7%), coreference resolution (3.7%), other arithmetic (3.2%), set of spans (6.0%), other (6.8%). Combinations of reasoning skills are also possible.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Initialization</th>
<th>#Params</th>
</tr>
</thead>
<tbody>
<tr>
<td>NT5</td>
<td>T5-Small</td>
<td>60M</td>
</tr>
<tr>
<td>GenBert</td>
<td>Bert-Base</td>
<td>110M</td>
</tr>
<tr>
<td>NumNet+(RoBERTa)</td>
<td>RoBERTa-Large</td>
<td>355M</td>
</tr>
<tr>
<td>QDGAT(RoBERTa)</td>
<td>RoBERTa-Large</td>
<td>355M</td>
</tr>
</tbody>
</table>
<p>Table 3: Number of parameters used for initialization for respective models.</p>
<p>plus synthetically generated examples.</p>
<p>We introduce an additional synthetic task based on the DROP dataset itself, whereby the model learns to predict the DROP question-type. While not provided at test time, we expect that explicit awareness of the question types will aid the model in knowing what reasoning strategies to use.</p>
<p>SQuAD We investigate using SQuAD v1.1 <em>Rajpurkar et al. (2016)</em> to improve NRoT by strengthening general reading comprehension in question and answering tasks.</p>
<p>Evaluation DROP employs two metrics for evaluation: an adjusted F1, and Exact-Match (EM). EM uses that same criteria as SQuAD. F1 has additional logic that invalidates all matching material within an answer when there is a numeric mismatch. Overall F1 is computed using macro-averaging over individual answers. In the presence of multiple ground truths, both EM and F1 will take a max over all computed scores.</p>
<p>Table 2: Performance summary for our baseline, training experiments, and select benchmarks. NAQANet is the best-performing model proposed in DROP’s original paper. GenBERT is a modified BERT-base model fine-tuned on the same synthetic datasets. Both NumNet and QDGAT are frameworks with separate language and numerical reasoning modules. QDGAT with an ALBERT language module is the current state-of-the-art.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Initialization</th>
<th>#Params</th>
</tr>
</thead>
<tbody>
<tr>
<td>NT5</td>
<td>T5-Small</td>
<td>60M</td>
</tr>
<tr>
<td>GenBert</td>
<td>Bert-Base</td>
<td>110M</td>
</tr>
<tr>
<td>NumNet+(RoBERTa)</td>
<td>RoBERTa-Large</td>
<td>355M</td>
</tr>
<tr>
<td>QDGAT(RoBERTa)</td>
<td>RoBERTa-Large</td>
<td>355M</td>
</tr>
</tbody>
</table>
<p>Table 3: Number of parameters used for initialization for respective models.</p>
<h2>4 Results</h2>
<p>The overall results of our five training experiments are summarized in Table 2, and decomposed in Table 4. Our best model achieves 66.8 EM and 70.3 F1 on the dev set, and a 67.0 EM and 70.8 F1 on test. Although the EM and F1 performance appears to have a degree of variance across the experiments. It is clear based on the overall model performance that RC1 and RC2 experiments are the most successful in internalizing the numerical reasoning required for performing well on DROP. While underperforming QDGAT-ALBERT, the current state-of-the-art that makes use of both neural and symbolic modules, NT5 performs well for a purely neural based method. Notably, our models use T5-Small with significantly fewer parameters than GenBERT. The encoder-decoder T5-small model has 60 million parameters, compared to the 110 million parameters of GenBERT in its encoder alone.</p>
<p>Overall, Table 4 shows that pre-training with DROP, synthetic datasets and SQuAD, and finetuning on DROP and DROP classification sequentially is able to significantly boost the performance on number questions, an increase of F1 from 31.83 to 70.39, while maintaining or improving performance on other types of questions. Additionally, when testing out the baseline, we found T5-Base increase F1 score over T5-Small by 11 points.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Schedule</th>
<th style="text-align: center;">Number</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Span</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Spans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
</tr>
<tr>
<td style="text-align: center;">baseline</td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">31.79</td>
<td style="text-align: center;">31.83</td>
<td style="text-align: center;">43.95</td>
<td style="text-align: center;">53.28</td>
<td style="text-align: center;">62.09</td>
<td style="text-align: center;">67.42</td>
<td style="text-align: center;">26.98</td>
<td style="text-align: center;">55.44</td>
<td style="text-align: center;">41.12</td>
<td style="text-align: center;">44.64</td>
</tr>
<tr>
<td style="text-align: center;">Validation Experiment 1</td>
<td style="text-align: center;">DROP + NUM (validate on DROP)</td>
<td style="text-align: center;">36.97</td>
<td style="text-align: center;">36.99</td>
<td style="text-align: center;">43.95</td>
<td style="text-align: center;">51.63</td>
<td style="text-align: center;">59.45</td>
<td style="text-align: center;">64.67</td>
<td style="text-align: center;">27.69</td>
<td style="text-align: center;">55.79</td>
<td style="text-align: center;">43.52</td>
<td style="text-align: center;">46.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP + TXT (validate on DROP)</td>
<td style="text-align: center;">63.25</td>
<td style="text-align: center;">63.27</td>
<td style="text-align: center;">42.04</td>
<td style="text-align: center;">51.42</td>
<td style="text-align: center;">63.03</td>
<td style="text-align: center;">68.29</td>
<td style="text-align: center;">29.63</td>
<td style="text-align: center;">56.97</td>
<td style="text-align: center;">60.83</td>
<td style="text-align: center;">64.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP + DROP class</td>
<td style="text-align: center;">67.03</td>
<td style="text-align: center;">67.07</td>
<td style="text-align: center;">42.68</td>
<td style="text-align: center;">51.43</td>
<td style="text-align: center;">65.19</td>
<td style="text-align: center;">70.80</td>
<td style="text-align: center;">31.57</td>
<td style="text-align: center;">58.89</td>
<td style="text-align: center;">63.95</td>
<td style="text-align: center;">67.49</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">68.72</td>
<td style="text-align: center;">68.78</td>
<td style="text-align: center;">43.31</td>
<td style="text-align: center;">50.36</td>
<td style="text-align: center;">65.09</td>
<td style="text-align: center;">70.62</td>
<td style="text-align: center;">32.10</td>
<td style="text-align: center;">60.05</td>
<td style="text-align: center;">65.00</td>
<td style="text-align: center;">68.53</td>
</tr>
<tr>
<td style="text-align: center;">Validation Experiment 2</td>
<td style="text-align: center;">DROP + NUM (validate on NUM)</td>
<td style="text-align: center;">41.37</td>
<td style="text-align: center;">41.38</td>
<td style="text-align: center;">40.76</td>
<td style="text-align: center;">48.94</td>
<td style="text-align: center;">61.31</td>
<td style="text-align: center;">66.45</td>
<td style="text-align: center;">29.81</td>
<td style="text-align: center;">58.73</td>
<td style="text-align: center;">46.86</td>
<td style="text-align: center;">50.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP + TXT (validate on TXT)</td>
<td style="text-align: center;">63.16</td>
<td style="text-align: center;">63.18</td>
<td style="text-align: center;">44.59</td>
<td style="text-align: center;">52.76</td>
<td style="text-align: center;">63.23</td>
<td style="text-align: center;">68.56</td>
<td style="text-align: center;">29.81</td>
<td style="text-align: center;">58.85</td>
<td style="text-align: center;">60.90</td>
<td style="text-align: center;">64.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP + DROP class</td>
<td style="text-align: center;">67.73</td>
<td style="text-align: center;">67.75</td>
<td style="text-align: center;">45.86</td>
<td style="text-align: center;">53.80</td>
<td style="text-align: center;">65.16</td>
<td style="text-align: center;">70.61</td>
<td style="text-align: center;">33.69</td>
<td style="text-align: center;">61.18</td>
<td style="text-align: center;">64.54</td>
<td style="text-align: center;">68.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">69.78</td>
<td style="text-align: center;">69.83</td>
<td style="text-align: center;">42.68</td>
<td style="text-align: center;">51.23</td>
<td style="text-align: center;">66.00</td>
<td style="text-align: center;">71.47</td>
<td style="text-align: center;">34.22</td>
<td style="text-align: center;">62.53</td>
<td style="text-align: center;">66.04</td>
<td style="text-align: center;">69.60</td>
</tr>
<tr>
<td style="text-align: center;">RC Experiment 1</td>
<td style="text-align: center;">DROP + SQuAD ${ }^{+}$</td>
<td style="text-align: center;">65.61</td>
<td style="text-align: center;">65.68</td>
<td style="text-align: center;">45.22</td>
<td style="text-align: center;">55.38</td>
<td style="text-align: center;">65.94</td>
<td style="text-align: center;">71.23</td>
<td style="text-align: center;">34.39</td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">63.52</td>
<td style="text-align: center;">67.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP + DROP class</td>
<td style="text-align: center;">68.65</td>
<td style="text-align: center;">68.69</td>
<td style="text-align: center;">45.22</td>
<td style="text-align: center;">53.87</td>
<td style="text-align: center;">66.54</td>
<td style="text-align: center;">72.01</td>
<td style="text-align: center;">36.68</td>
<td style="text-align: center;">63.47</td>
<td style="text-align: center;">65.71</td>
<td style="text-align: center;">69.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">70.34</td>
<td style="text-align: center;">70.39</td>
<td style="text-align: center;">45.22</td>
<td style="text-align: center;">53.85</td>
<td style="text-align: center;">66.75</td>
<td style="text-align: center;">72.35</td>
<td style="text-align: center;">37.74</td>
<td style="text-align: center;">63.43</td>
<td style="text-align: center;">66.87</td>
<td style="text-align: center;">70.31</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { RC } \ &amp; \text { Experiment } 2 \end{aligned}$</td>
<td style="text-align: center;">DROP + DROP class + SQuAD ${ }^{+ * *}$</td>
<td style="text-align: center;">65.90</td>
<td style="text-align: center;">65.94</td>
<td style="text-align: center;">45.22</td>
<td style="text-align: center;">54.31</td>
<td style="text-align: center;">66.48</td>
<td style="text-align: center;">71.73</td>
<td style="text-align: center;">35.80</td>
<td style="text-align: center;">62.55</td>
<td style="text-align: center;">63.95</td>
<td style="text-align: center;">67.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">69.44</td>
<td style="text-align: center;">69.47</td>
<td style="text-align: center;">45.22</td>
<td style="text-align: center;">53.17</td>
<td style="text-align: center;">67.45</td>
<td style="text-align: center;">72.60</td>
<td style="text-align: center;">35.63</td>
<td style="text-align: center;">63.08</td>
<td style="text-align: center;">66.41</td>
<td style="text-align: center;">69.80</td>
</tr>
<tr>
<td style="text-align: center;">Multitask Experiment</td>
<td style="text-align: center;">DROP + TXT + NUM + SQuAD</td>
<td style="text-align: center;">56.84</td>
<td style="text-align: center;">56.86</td>
<td style="text-align: center;">42.68</td>
<td style="text-align: center;">50.44</td>
<td style="text-align: center;">64.58</td>
<td style="text-align: center;">69.72</td>
<td style="text-align: center;">33.51</td>
<td style="text-align: center;">61.78</td>
<td style="text-align: center;">57.62</td>
<td style="text-align: center;">61.04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP + DROP class</td>
<td style="text-align: center;">63.73</td>
<td style="text-align: center;">63.81</td>
<td style="text-align: center;">49.04</td>
<td style="text-align: center;">56.28</td>
<td style="text-align: center;">65.97</td>
<td style="text-align: center;">71.24</td>
<td style="text-align: center;">36.16</td>
<td style="text-align: center;">63.65</td>
<td style="text-align: center;">62.54</td>
<td style="text-align: center;">65.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">64.43</td>
<td style="text-align: center;">64.49</td>
<td style="text-align: center;">45.86</td>
<td style="text-align: center;">52.99</td>
<td style="text-align: center;">66.48</td>
<td style="text-align: center;">71.61</td>
<td style="text-align: center;">36.51</td>
<td style="text-align: center;">63.80</td>
<td style="text-align: center;">63.10</td>
<td style="text-align: center;">66.47</td>
</tr>
</tbody>
</table>
<p>Table 4: The decomposed and overall EM and $\mathrm{F}_{1}$ scores on different answer types in the development set of DROP for each experiment. High scores for each type are in bold. *Notice that the RC experiments begin training using the weights learned in validation experiment 2. $\circ$ RC Experiment 2 fine-tune with SQuAD in addition to DROP and DROP classification.</p>
<h3>4.1 Difference in Validation Dataset</h3>
<p>A surprising finding here is that saving models while validating on the synthetic dev sets outperforms saving models while validating on the DROP dev sets after the first stage. Specifically, this achieves a F1 score (50.32) that is 3.37 points higher (46.95) without sacrificing performance on span/spans questions. We reason that this performance gap is caused by the difference between the loss on development and DROP's evaluation metrics, as detailed in Section 3.</p>
<h3>4.2 Strengthening Reading Comprehension</h3>
<p>Performance on extractive RC tasks is boosted with the addition of SQuAD v1.1 in pre-training. We further test if this performance change persist when multitask training SQuAD v1.1 together with DROP and DROP classification in the finetuning stage. The resulting model sees improvement across all question types at the end of the training on SQuAD v1.1. Crucially, performance on RC tasks (date, span, and spans) sees an average improvement of 3.06 points in F1 over the previous result. However, this came at the expense of minor deteriorated performance on numeric questions.</p>
<h3>4.3 All Datasets Multitasking</h3>
<p>Fine-tuning simultaneously on all datasets underperforms our best model by nearly 6-point on F1.</p>
<h2>5 Error Analysis</h2>
<p>To better understand the achievement and limitations of the best model, we analyzed its errors on the dev set. In 38 of 100 errors sampled from number questions, the model has made at least one partial digit match. Of the total 86 errors on date questions, 39 questions require arithmetic calculations. In 9 of these 86 errors, the model wrongly performs numerical calculations, instead of simply extracting answers. With a sample of 100 span and spans errors, 49 of the questions contain reasoning skills not covered in the pre-training datasets. This is compared to the $43 \%$ shown by Dua et al. (2019) ${ }^{2}$. Many of these errors can be addressed with pre-training datasets that cover more complicated calculations and reasoning skills.</p>
<h2>6 Related Works</h2>
<p>Introduced by (Geva et al., 2020), GenBERT is a BERT-Base model customized with specialized</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>heads for handling discrete reasoning. It is also the main inspiration for our approach. The current state-of-the-art model on DROP, QDGATALBERT, uses a directed graph attention network between a ALBERT based representation extractor and a prediction module for discrete reasoning (Chen et al., 2020). For works analyzing the mathematical reasoning ability of models over text refer to Wallace et al. (2019); Ravichander et al. (2019).</p>
<h2>7 Conclusion</h2>
<p>We introduced a sequential pre-training framework for numeracy with T5. Our method demonstrates strong improvements on NRoT over a baseline vanilla T5 model. Although current state of the art, QDGAT, which makes use of a hybrid of a neural and symbol modules, and human performance on DROP are better performing, our approach touts both simplicity and low resource usage, achieving good performance using only T5-small.</p>
<h2>References</h2>
<p>Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 59475952, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, and Wei Chu. 2020. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6759-6768, Online. Association for Computational Linguistics.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, , and Matt Gardner. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Association for Computational Linguistics.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Association for Computational Linguistics, volume arXiv:2004.04487.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2474-2484, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. Equate : A benchmark evaluation framework for quantitative reasoning in natural language inference. In Proceedings of the 23rd Conference on Computational Natural Language Learning.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do nlp models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Criteria might vary due to human evaluation&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ All source codes and sample models are available at https://github.com/lesterpjy/numeric-t5.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
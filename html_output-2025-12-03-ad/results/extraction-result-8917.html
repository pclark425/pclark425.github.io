<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8917 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8917</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8917</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-2e3dcf5a5d58ac210d0d87e9f918540a8373211a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2e3dcf5a5d58ac210d0d87e9f918540a8373211a" target="_blank">GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</a></p>
                <p><strong>Paper Venue:</strong> Comput. Biol. Medicine</p>
                <p><strong>Paper TL;DR:</strong> GIT-Mol is introduced, a multi-modal large language model that integrates the Graph, Image, and Text information and proposes GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8917.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8917.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIT-Mol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIT-Mol: A Multi-modal Large Language Model for Molecular Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 700M-parameter multi-modal transformer-based model that fuses graph, image and text modalities via a GIT-Former cross-attention module and decodes into text (SMILES/captions) with MolT5; designed for molecule captioning, text-based de novo molecule generation, image-to-SMILES conversion and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GIT-Mol</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Multi-modal transformer-based LLM (GIT-Former modality mixer + MolT5 text decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>700M</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on ~4.8 million chemical compounds from PubChem (images, SMILES, graphs); curated ~320k PubChem molecule-description pairs (filtered) and ChEBI-20 (33,010 pairs) were used for fine-tuning/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General molecular design tasks relevant to drug discovery and cheminformatics: molecule captioning, de novo SMILES generation from text, image-to-SMILES recognition, and molecular property prediction/classification.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-driven any-to-language generation: GIT-Former fuses arbitrary input modalities into fixed-length learnable queries (cross-attention + contrastive pretraining: XTM and XTC); resulting embeddings are fed to MolT5 decoder to autoregressively generate SMILES (text-conditioned generation).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty relative to training set is not explicitly quantified in the paper; novelty is assessed indirectly via similarity metrics (Fingerprint Tanimoto similarities MACCS/RDK/Morgan) and string-based metrics (BLEU, exact match, Levenshtein). The paper emphasizes high chemical validity (92.8%) of generated molecules but does not report percent novel vs. training set.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Generation is conditioned on textual captions, images or graphs via task-specific prompts (e.g. "Given the provided inputs, generate the corresponding SMILES string."); the experiments evaluate general correspondence to ground-truth molecules and chemical validity rather than task-specific objectives like target binding or property optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU (n-gram overlap on SMILES), Exact match, Levenshtein distance, Fingerprint Tanimoto Similarity (MACCS, RDK, Morgan), chemical validity (RDKit validity), NLP metrics for captions (BLEU, ROUGE, METEOR), and downstream classification AUCs for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Text-based de novo generation: BLEU 0.756, Exact match 0.051, Levenshtein 26.315, MACCS FTS 0.738, RDK FTS 0.582, Morgan FTS 0.519, Validity 0.928. Molecule captioning and image-to-SMILES tasks also showed improvements over single-modality baselines; multi-modal setup produced 10–15% gains over single-modality in captioning and yielded a reported ~20.2% absolute improvement in validity versus MolT5-base baseline. For property prediction, multi-modal G+S (graph + SMILES) averaged AUC 74.90, a 5–10% improvement over some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against MolT5-base, SciBERT and SwinOCSR baselines: GIT-Mol achieved similar/sometimes slightly lower BLEU but substantially higher chemical validity than MolT5-base (0.928 vs 0.772 validity), outperformed SwinOCSR on image-to-SMILES metrics, and multi-modal embeddings outperformed single-modality models for property prediction. Ablations show XTC+XTM and prompt tuning contribute significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Training speed and scalability remain challenging; caption/generation evaluation lacks standardized metrics for scientific quality; pretraining with only XTM led to overfitting and low output diversity; novelty vs training set not reported; need for PEFT/LoRA to reduce tuning cost; dataset caption quality required careful curation (many brief/non-informative captions removed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8917.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8917.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (T5-based molecular text model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-inspired text-to-text transformer previously developed for molecule-caption translation and SMILES/text generation; used in this work as the text encoder/decoder and as a primary baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5 (MolT5-base, MolT5-large variants used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5-style text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule captioning and SMILES generation (text↔molecule translation), and as embedding/self-attention layers for downstream property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive text decoding of SMILES from textual prompts (text-to-SMILES generation).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not explicitly reported in paper; evaluated via similarity metrics and validity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioned on captions for generation; not trained here for target-specific optimization in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as for GIT-Mol: BLEU, Exact match, Levenshtein, Fingerprint Tanimoto similarities (MACCS/RDK/Morgan), validity (RDKit).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolT5-base (baseline) in text-based de novo generation: BLEU 0.769, Exact match 0.081, Levenshtein 24.458, MACCS FTS 0.721, RDK FTS 0.588, Morgan FTS 0.529, Validity 0.772. MolT5 gives strong similarity scores but lower validity compared to GIT-Mol.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Serves as the main text-only baseline; GIT-Mol attained comparable similarity metrics but substantially higher validity than MolT5-base, indicating multi-modal fusion improved chemical validity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Lower chemical validity in these experiments relative to GIT-Mol; single-modality text models may miss structural constraints available in graph/image modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8917.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8917.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciBERT (A BERT model pretrained on scientific text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-family encoder pretrained on scientific corpora; used in this work as a scientific-text encoder and as a baseline for generation/captioning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>BERT-style transformer encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Text encoding for molecule-caption tasks and baseline text model in generation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>When used as baseline, SciBERT-based pipeline maps text to molecular outputs (details of decoding depend on downstream decoding module); in this paper used as baseline (not primary autoregressive generator).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Not optimized for chemistry generation beyond serving as a text encoder/baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, Exact match, Levenshtein, Fingerprint similarities, Validity (as reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>In de novo generation baseline, SciBERT row: BLEU 0.459, Exact 0.005, Levenshtein 55.459, MACCS 0.499, RDK 0.344, Morgan 0.254, Validity 0.915 — high validity but low similarity and poor exact-match performance compared to MolT5 and GIT-Mol.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performs worse on similarity and exact-match metrics than MolT5 and GIT-Mol; very different trade-offs (higher validity but low generation fidelity).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not designed as an autoregressive SMILES generator; as a BERT-style encoder it may be ill-suited alone for direct generative SMILES accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8917.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8917.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolReGPT (retrieval-based LLM approach for molecule-caption translation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed retrieval-based paradigm leveraging LLMs (e.g., ChatGPT) for molecule-caption translation without fine-tuning, mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Retrieval-augmented GPT-style approach (LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule-caption translation and molecule retrieval with natural language queries.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-augmented prompting to LLMs (uses external retrieval and LLM generation rather than task-specific fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to map between textual descriptions and molecules (translation/retrieval) rather than to specifically optimize for binding or other properties in the discussed mention.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an approach that leverages LLMs like ChatGPT without fine-tuning for molecule-caption translation; no experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as an alternate paradigm to fine-tuned models; retrieval-based LLM usage contrasts with GIT-Mol's fine-tuned multi-modal fusion approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>No specific limitations provided in this paper beyond implicit lack of fine-tuning; paper notes potential for further advancement in modality alignment and fusion strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8917.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8917.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugGPT / Druggpt (GPT-based ligand design strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-based approach referenced as adapted for designing potential ligands targeting specific proteins (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Druggpt / DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-based generative model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Designing potential ligands for specific protein targets (drug discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>GPT-style generative design (details not reported in this paper; referenced work proposes GPT-based ligand design).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Targeted ligand design for proteins (as stated in citation), implying conditioning on target-related information in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned in related work list as an example of GPT adaptation to molecular science; no experimental details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned among other LLM adaptations for molecular tasks; no direct comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8917.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8917.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT (Generative pre-trained transformer for biomedical text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-family model pre-trained on biomedical corpora and mentioned as an example of LLM adaptation for biomedical/molecular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical text generation/mining; cited as an example of domain-adapted LLMs relevant to molecular science.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive text generation (no molecule-generation specifics provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned in related work as a domain-adapted LLM; no experimental molecule-generation details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8917.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8917.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-family (ChatGPT / GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-family models including ChatGPT and GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose large GPT-family language models cited as influential and sometimes used (via prompting or retrieval) in molecular and biomedical tasks; mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style large autoregressive transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General NLP and being applied experimentally to molecular captioning/translation and retrieval tasks as referenced in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting / retrieval-augmented prompting (no fine-tuning) for molecule-caption translation or editing tasks as discussed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>When used in referenced works, often applied to text-based molecule retrieval/translation; not described in this paper as being fine-tuned for property-driven molecule design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as examples of LLMs being leveraged in molecular contexts (e.g., MolReGPT leverages ChatGPT without fine-tuning); no experimental details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Referenced as off-the-shelf LLMs that can be used with retrieval/prompting versus fine-tuned multi-modal models like GIT-Mol.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Implicit limitations include lack of fine-tuning for domain specifics and potential need for retrieval/augmentation to perform well on molecule-specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective <em>(Rating: 2)</em></li>
                <li>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins <em>(Rating: 2)</em></li>
                <li>Biogpt: generative pre-trained transformer for biomedical text generation and mining <em>(Rating: 1)</em></li>
                <li>SwinOCSR: end-to-end optical chemical structure recognition using a swin transformer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8917",
    "paper_id": "paper-2e3dcf5a5d58ac210d0d87e9f918540a8373211a",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "GIT-Mol",
            "name_full": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science",
            "brief_description": "A 700M-parameter multi-modal transformer-based model that fuses graph, image and text modalities via a GIT-Former cross-attention module and decodes into text (SMILES/captions) with MolT5; designed for molecule captioning, text-based de novo molecule generation, image-to-SMILES conversion and property prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GIT-Mol",
            "model_type": "Multi-modal transformer-based LLM (GIT-Former modality mixer + MolT5 text decoder)",
            "model_size": "700M",
            "training_data": "Pretrained on ~4.8 million chemical compounds from PubChem (images, SMILES, graphs); curated ~320k PubChem molecule-description pairs (filtered) and ChEBI-20 (33,010 pairs) were used for fine-tuning/evaluation.",
            "application_domain": "General molecular design tasks relevant to drug discovery and cheminformatics: molecule captioning, de novo SMILES generation from text, image-to-SMILES recognition, and molecular property prediction/classification.",
            "generation_method": "Prompt-driven any-to-language generation: GIT-Former fuses arbitrary input modalities into fixed-length learnable queries (cross-attention + contrastive pretraining: XTM and XTC); resulting embeddings are fed to MolT5 decoder to autoregressively generate SMILES (text-conditioned generation).",
            "novelty_of_chemicals": "Novelty relative to training set is not explicitly quantified in the paper; novelty is assessed indirectly via similarity metrics (Fingerprint Tanimoto similarities MACCS/RDK/Morgan) and string-based metrics (BLEU, exact match, Levenshtein). The paper emphasizes high chemical validity (92.8%) of generated molecules but does not report percent novel vs. training set.",
            "application_specificity": "Generation is conditioned on textual captions, images or graphs via task-specific prompts (e.g. \"Given the provided inputs, generate the corresponding SMILES string.\"); the experiments evaluate general correspondence to ground-truth molecules and chemical validity rather than task-specific objectives like target binding or property optimization.",
            "evaluation_metrics": "BLEU (n-gram overlap on SMILES), Exact match, Levenshtein distance, Fingerprint Tanimoto Similarity (MACCS, RDK, Morgan), chemical validity (RDKit validity), NLP metrics for captions (BLEU, ROUGE, METEOR), and downstream classification AUCs for property prediction.",
            "results_summary": "Text-based de novo generation: BLEU 0.756, Exact match 0.051, Levenshtein 26.315, MACCS FTS 0.738, RDK FTS 0.582, Morgan FTS 0.519, Validity 0.928. Molecule captioning and image-to-SMILES tasks also showed improvements over single-modality baselines; multi-modal setup produced 10–15% gains over single-modality in captioning and yielded a reported ~20.2% absolute improvement in validity versus MolT5-base baseline. For property prediction, multi-modal G+S (graph + SMILES) averaged AUC 74.90, a 5–10% improvement over some baselines.",
            "comparison_to_other_methods": "Compared against MolT5-base, SciBERT and SwinOCSR baselines: GIT-Mol achieved similar/sometimes slightly lower BLEU but substantially higher chemical validity than MolT5-base (0.928 vs 0.772 validity), outperformed SwinOCSR on image-to-SMILES metrics, and multi-modal embeddings outperformed single-modality models for property prediction. Ablations show XTC+XTM and prompt tuning contribute significantly.",
            "limitations_and_challenges": "Training speed and scalability remain challenging; caption/generation evaluation lacks standardized metrics for scientific quality; pretraining with only XTM led to overfitting and low output diversity; novelty vs training set not reported; need for PEFT/LoRA to reduce tuning cost; dataset caption quality required careful curation (many brief/non-informative captions removed).",
            "uuid": "e8917.0",
            "source_info": {
                "paper_title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5 (T5-based molecular text model)",
            "brief_description": "A T5-inspired text-to-text transformer previously developed for molecule-caption translation and SMILES/text generation; used in this work as the text encoder/decoder and as a primary baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MolT5 (MolT5-base, MolT5-large variants used in experiments)",
            "model_type": "T5-style text-to-text transformer",
            "model_size": null,
            "training_data": null,
            "application_domain": "Molecule captioning and SMILES generation (text↔molecule translation), and as embedding/self-attention layers for downstream property prediction.",
            "generation_method": "Autoregressive text decoding of SMILES from textual prompts (text-to-SMILES generation).",
            "novelty_of_chemicals": "Not explicitly reported in paper; evaluated via similarity metrics and validity.",
            "application_specificity": "Conditioned on captions for generation; not trained here for target-specific optimization in reported experiments.",
            "evaluation_metrics": "Same as for GIT-Mol: BLEU, Exact match, Levenshtein, Fingerprint Tanimoto similarities (MACCS/RDK/Morgan), validity (RDKit).",
            "results_summary": "MolT5-base (baseline) in text-based de novo generation: BLEU 0.769, Exact match 0.081, Levenshtein 24.458, MACCS FTS 0.721, RDK FTS 0.588, Morgan FTS 0.529, Validity 0.772. MolT5 gives strong similarity scores but lower validity compared to GIT-Mol.",
            "comparison_to_other_methods": "Serves as the main text-only baseline; GIT-Mol attained comparable similarity metrics but substantially higher validity than MolT5-base, indicating multi-modal fusion improved chemical validity.",
            "limitations_and_challenges": "Lower chemical validity in these experiments relative to GIT-Mol; single-modality text models may miss structural constraints available in graph/image modalities.",
            "uuid": "e8917.1",
            "source_info": {
                "paper_title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SciBERT",
            "name_full": "SciBERT (A BERT model pretrained on scientific text)",
            "brief_description": "A BERT-family encoder pretrained on scientific corpora; used in this work as a scientific-text encoder and as a baseline for generation/captioning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SciBERT",
            "model_type": "BERT-style transformer encoder",
            "model_size": null,
            "training_data": null,
            "application_domain": "Text encoding for molecule-caption tasks and baseline text model in generation experiments.",
            "generation_method": "When used as baseline, SciBERT-based pipeline maps text to molecular outputs (details of decoding depend on downstream decoding module); in this paper used as baseline (not primary autoregressive generator).",
            "novelty_of_chemicals": "Not reported.",
            "application_specificity": "Not optimized for chemistry generation beyond serving as a text encoder/baseline.",
            "evaluation_metrics": "BLEU, Exact match, Levenshtein, Fingerprint similarities, Validity (as reported in tables).",
            "results_summary": "In de novo generation baseline, SciBERT row: BLEU 0.459, Exact 0.005, Levenshtein 55.459, MACCS 0.499, RDK 0.344, Morgan 0.254, Validity 0.915 — high validity but low similarity and poor exact-match performance compared to MolT5 and GIT-Mol.",
            "comparison_to_other_methods": "Performs worse on similarity and exact-match metrics than MolT5 and GIT-Mol; very different trade-offs (higher validity but low generation fidelity).",
            "limitations_and_challenges": "Not designed as an autoregressive SMILES generator; as a BERT-style encoder it may be ill-suited alone for direct generative SMILES accuracy.",
            "uuid": "e8917.2",
            "source_info": {
                "paper_title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MolReGPT",
            "name_full": "MolReGPT (retrieval-based LLM approach for molecule-caption translation)",
            "brief_description": "A recently proposed retrieval-based paradigm leveraging LLMs (e.g., ChatGPT) for molecule-caption translation without fine-tuning, mentioned in related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolReGPT",
            "model_type": "Retrieval-augmented GPT-style approach (LLM-based)",
            "model_size": null,
            "training_data": null,
            "application_domain": "Molecule-caption translation and molecule retrieval with natural language queries.",
            "generation_method": "Retrieval-augmented prompting to LLMs (uses external retrieval and LLM generation rather than task-specific fine-tuning).",
            "novelty_of_chemicals": null,
            "application_specificity": "Designed to map between textual descriptions and molecules (translation/retrieval) rather than to specifically optimize for binding or other properties in the discussed mention.",
            "evaluation_metrics": null,
            "results_summary": "Mentioned as an approach that leverages LLMs like ChatGPT without fine-tuning for molecule-caption translation; no experimental details provided in this paper.",
            "comparison_to_other_methods": "Mentioned as an alternate paradigm to fine-tuned models; retrieval-based LLM usage contrasts with GIT-Mol's fine-tuned multi-modal fusion approach.",
            "limitations_and_challenges": "No specific limitations provided in this paper beyond implicit lack of fine-tuning; paper notes potential for further advancement in modality alignment and fusion strategies.",
            "uuid": "e8917.3",
            "source_info": {
                "paper_title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "DrugGPT",
            "name_full": "DrugGPT / Druggpt (GPT-based ligand design strategy)",
            "brief_description": "A GPT-based approach referenced as adapted for designing potential ligands targeting specific proteins (cited in related work).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Druggpt / DrugGPT",
            "model_type": "GPT-based generative model",
            "model_size": null,
            "training_data": null,
            "application_domain": "Designing potential ligands for specific protein targets (drug discovery).",
            "generation_method": "GPT-style generative design (details not reported in this paper; referenced work proposes GPT-based ligand design).",
            "novelty_of_chemicals": null,
            "application_specificity": "Targeted ligand design for proteins (as stated in citation), implying conditioning on target-related information in the referenced work.",
            "evaluation_metrics": null,
            "results_summary": "Mentioned in related work list as an example of GPT adaptation to molecular science; no experimental details in this paper.",
            "comparison_to_other_methods": "Positioned among other LLM adaptations for molecular tasks; no direct comparisons in this paper.",
            "limitations_and_challenges": null,
            "uuid": "e8917.4",
            "source_info": {
                "paper_title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT (Generative pre-trained transformer for biomedical text)",
            "brief_description": "A GPT-family model pre-trained on biomedical corpora and mentioned as an example of LLM adaptation for biomedical/molecular tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BioGPT",
            "model_type": "GPT-style transformer",
            "model_size": null,
            "training_data": null,
            "application_domain": "Biomedical text generation/mining; cited as an example of domain-adapted LLMs relevant to molecular science.",
            "generation_method": "Autoregressive text generation (no molecule-generation specifics provided in this paper).",
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": "Mentioned in related work as a domain-adapted LLM; no experimental molecule-generation details provided here.",
            "comparison_to_other_methods": null,
            "limitations_and_challenges": null,
            "uuid": "e8917.5",
            "source_info": {
                "paper_title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-family (ChatGPT / GPT-4)",
            "name_full": "GPT-family models including ChatGPT and GPT-4",
            "brief_description": "General-purpose large GPT-family language models cited as influential and sometimes used (via prompting or retrieval) in molecular and biomedical tasks; mentioned in related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / GPT-4",
            "model_type": "GPT-style large autoregressive transformer LLMs",
            "model_size": null,
            "training_data": null,
            "application_domain": "General NLP and being applied experimentally to molecular captioning/translation and retrieval tasks as referenced in the literature.",
            "generation_method": "Prompting / retrieval-augmented prompting (no fine-tuning) for molecule-caption translation or editing tasks as discussed in related work.",
            "novelty_of_chemicals": null,
            "application_specificity": "When used in referenced works, often applied to text-based molecule retrieval/translation; not described in this paper as being fine-tuned for property-driven molecule design.",
            "evaluation_metrics": null,
            "results_summary": "Mentioned as examples of LLMs being leveraged in molecular contexts (e.g., MolReGPT leverages ChatGPT without fine-tuning); no experimental details provided here.",
            "comparison_to_other_methods": "Referenced as off-the-shelf LLMs that can be used with retrieval/prompting versus fine-tuned multi-modal models like GIT-Mol.",
            "limitations_and_challenges": "Implicit limitations include lack of fine-tuning for domain specifics and potential need for retrieval/augmentation to perform well on molecule-specific tasks.",
            "uuid": "e8917.6",
            "source_info": {
                "paper_title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective",
            "rating": 2
        },
        {
            "paper_title": "Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins",
            "rating": 2
        },
        {
            "paper_title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
            "rating": 1
        },
        {
            "paper_title": "SwinOCSR: end-to-end optical chemical structure recognition using a swin transformer",
            "rating": 1
        }
    ],
    "cost": 0.01777175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</h1>
<p>Pengfei Liu ${ }^{a, b}$, Yiming Ren ${ }^{a}$, Jun Tao ${ }^{b}$ and Zhixiang Ren ${ }^{a}$<br>${ }^{a}$ Peng Cheng Laboratory, Shenzhen, 518055, Guangdong Province, China<br>${ }^{b}$ School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, 510006, Guangdong Province, China</p>
<h2>ARTICLE INFO</h2>
<p>Keywords:
Molecular Representation
Molecule Generation
Large Language Model
Multi-modality</p>
<h2>ABSTRACT</h2>
<p>Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multimodal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a $5 \%-10 \%$ accuracy increase in properties prediction and a $20.2 \%$ boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.</p>
<h2>1. Introduction</h2>
<p>Molecular science covers a broad spectrum of fields that study the structures, properties, and interactions of molecules. It is a interdisciplinary field that draws on chemistry, physics, biology, and computer science. Molecular science is pivotal in drug discovery applications, such as target identification and validation, structure-based drug design, and side effect prediction. However, most existing methods of discovering new molecules or tweaking existing ones can be time-consuming, expensive, and prone to failure (Rodrigues et al., 2016). More recently, computational methods have shown significant advantages in molecule generation and tweaking (Bilodeau et al., 2022). These techniques enable rapid identification and optimization of potential drug candidates. However, these computational methods are limited by substantial computational demands.</p>
<p>Fortunately, artificial intelligence (AI) and deep learning have emerged as powerful tools for molecular science. These technologies can potentially revolutionize the field by improving speed, accuracy, and scalability in molecular discovery and understanding. Large Language Models (LLMs) have made significant progress in Natural Language Processing (NLP) and molecular science. MolT5 (Edwards et al., 2022), based on the T5 (Raffel et al., 2020), which includes capabilities of molecule captioning (Mol2Cap) and text-based molecule generation (Cap2Mol). LLMs like MolT5 help describe molecules in words and generate structures from the text. However, these text-to-text models can not fully use the advantages of molecular structure data and understand molecular images. To fuse and understand multi-modal data, the multi-modal large language models (MLLMs) like CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), and BEIT-3 (Wang et al., 2022c) have laid</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the groundwork for adaptive learning across image-text modalities. In molecular science, SwinOCSR (Xu et al., 2022) is designed for image recognition, which significantly aids in document comprehension and multi-modal drug discovery database construction (Wang et al., 2022b). In addition, MoleculeSTM (Liu et al., 2022) and MoMu (Su et al., 2022) can combine the Simplified Molecular Input Line Entry System (SMILES) (Weininger, 1988) and graph representations for molecular property prediction tasks.</p>
<p>Existing language models excel in processing textual molecular data, but processing molecular graphs and images is difficult, typically relying on processed vector encodings. This limitation highlights the need for a modality data alignment process, where multi-modal models outperform language models by integrating diverse data types for enhanced feature representation. Moreover, the scalability of molecular representation and generation models remains a significant challenge. Models capable of effectively fusing three or more modalities are scarce, and the complexity of integrating these diverse modalities, often with missing or incomplete data, calls for advanced modeling techniques.</p>
<p>To address the significant scalability challenges in molecular science and to harness the potential of vast quantities of unlabeled multi-modal data, we have developed GIT-Mol, a robust neural network architecture. At the core of GIT-Mol is the GIT-Former, which incorporates an advanced cross-attention mechanism. It is capable of mapping data from various modalities into a unified multi-modal representation. This approach not only enhances the model's ability to process and integrate diverse modalities efficiently but also effectively bridges the scalability gap, providing a powerful tool for various molecular applications. Our main contributions are as follows:</p>
<ul>
<li>To utilize the available large amount of unlabeled multi-modal data, we develop a specialized multimodal large language model, GIT-Mol (700M), to</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Figure 1: An overview of GIT-Mol. (a) Internal Information</strong>, including sequence and graph structure representations, emphasizes inherent chemical properties and simple topology; <strong>(b) External Information</strong>, e.g., images and text descriptions, provide richer details and help the human understanding; <strong>(c) GIT-Former Multi-modal Encoder</strong>, architecture and Pre-train Strategy of GIT-Former, GIT-Former aligns graph, image, and text with the target text modality (SMILES strings or captions) using self-attention and cross-attention. The learnable queries interact with each other and the various modalities through these attention layers. Xmodal-Text Matching (XTM) and Xmodal-Text Contrastive Learning (XTC) represent our self-supervised learning strategies tailored for specific modalities (X) and target text modalities; <strong>(d) Multi-modal Molecular Tasks</strong>, in cross-modal tasks, GIT-Former generates different Embeddings based on various inputs, which MolT5 then decodes into the target text modality and the MLP model for property prediction tasks.</p>
<p>cover all three modalities in molecular science (graph, image, and text) for molecule generation, molecule captioning, molecular image recognition, and molecular property prediction.</p>
<ul>
<li>We present <strong>GIT-Former</strong>, a novel modality mixer designed with a cross-attention mechanism, enabling seamless fusion of three molecular modalities. Each fusion strategy within GIT-Former operates at the molecule level, ensuring optimal flexibility and scalability. The effectiveness of our multi-modal model is demonstrated in our extensive ablation studies, which provides an improvement of 10%-15% over the single-modality models.</li>
<li>The quantitative evaluations indicate that GIT-Mol exhibits commendable performance and surpasses state-of-the-art on certain metrics, as it outperforms</li>
</ul>
<p>the baseline $20 \%$ in the cross-modal molecule generation tasks regarding the validity of the generated molecules and by $5 \%-10 \%$ in molecular property prediction tasks.</p>
<p>In the subsequent sections, Section 2 primarily introduces multi-modal models and applications in molecular science. Section 3 introduces the principles of our GIT-Mol model. Section 4 focuses on the experimental setup and results for downstream tasks of the model. Sections 5, 6, and 7 respectively discuss the features of our model, the limitations, and the conclusions.</p>
<h2>2. Related Works</h2>
<h3>2.1. Multi-modal Large Language Models</h3>
<p>In recent years, Large Language Models (LLMs) like the GPT (Floridi and Chiriatti, 2020) family have received more attention due to their performance and potential applications, especially the ChatGPT(Radford et al., 2019) and GPT-4(OpenAI, 2023). Some variants of those models have been used in many scientific domains, such as BioGPT (Luo et al., 2022), DrugGPT (Li et al., 2023c), and MolReGPT (Li et al., 2023b), which have been adapted for molecular science tasks. Additionally, models like LLaMA and T5 have inspired a variety of variants and as the language model in multi-modal models.</p>
<p>Multi-modal models have been a primary focus on image captioning task (Stefanini et al., 2022). Modalityadaptive Learning models such as BEIT-3 (Wang et al., 2022c) and KOSMOS-1 (Huang et al., 2023) adopt the Mixture of Experts (MoE) strategy (Bao et al., 2022) and the MAGNETO transformer (Wang et al., 2022a), enhancing learning across different modalities using specialized encoders and shared self-attention modules. In contrast, Multimodal agents like Flamingo (Alayrac et al., 2022) and Gato (Reed et al., 2022) are crafted for real-world applications, leveraging multi-task and reinforcement learning to interpret and act within intricate environments. Additionally, models such as Visual ChatGPT (Wu et al., 2023) employ the Chain-of-Thoughts (CoT) strategy (Wei et al., 2022) approach, merging visual encoders with ChatGPT architectures for enriched, visually-informed dialogues. Lastly, the Cross-modal Learning paradigm, as seen in models like MiniGPT-4 (Zhu et al., 2023), emphasizes modality fusion and alignment using techniques like the Q-Former in BLIP2 (Li et al., 2023a) or adapter, promoting the modality integration.</p>
<p>In addition, text-image generation models like DALLE2 (Ramesh et al., 2022) and UniDiffuser (Bao et al., 2023) employ diffusion models (Yang et al., 2022) to generate images from text. These models have potential applications across art, design, and scientific visualization. Moreover, diffusion models are also used in molecular research for molecule structure graph generation tasks.</p>
<p>In multi-modal tasks, image-captioning and text-image generation stand out as significant directions. These models
integrate information across different modalities, highlighting the potential of multi-modal learning in comprehensively understanding the connections in modalities.</p>
<h3>2.2. Multi-modal Model in Molecular Science</h3>
<p>In molecular science, multi-modal models focus on molecule-caption translation tasks and utilize multi-modal representations for downstream tasks like molecular property and chemical reaction prediction.</p>
<p>Molecule-Caption Translation, in this task, our model can learn a shared semantic space from a dataset of molecules paired with their text descriptions. Like Text2Mol (Edwards et al., 2021), uses natural language descriptions to retrieve molecules. Moreover, models including KV-PLM (Zeng et al., 2022) and MolT5 (Edwards et al., 2022) significantly contribute to this area. KV-PLM builds a machine reading system, pre-trained on the domain-specific corpus, linking molecules and biomedical text. MolT5 is a self-supervised framework enhancing molecule-caption translation tasks. In addition, MoleculeSTM and MoMu bridge molecular graphs and text data through contrastive learning. MolReGPT applies a retrieval-based paradigm for molecule-caption translation, leveraging LLMs like ChatGPT without fine-tuning. These models, transforming between SMILES expressions and text descriptions, have the potential for further advancement, with possibilities including better modality alignment, fusion strategies, and fine-tuning methods.</p>
<p>Molecule Image Captioning, rule-based models such as MolVec2 (Peryea et al., 2019) and OSRA (Filippov and Nicklaus, 2009), along with machine learning-based ones like DECIMER (Rajan et al., 2021) and SwinOCSR. These models use image encoders like Vision Transformer (Dosovitskiy et al., 2020) or ResNet (He et al., 2016) and process image features using recurrent neural networks (RNNs) or transformers (Vaswani et al., 2017) to decode into SMILES strings. Unlike the general domain, their results mainly focus on transforming images into SMILES strings.</p>
<p>Molecular Property Prediction, GNN-based models such as GraphCL (Wang et al., 2022d) and GraphMAE (Hou et al., 2022) are used, leveraging contrastive learning and self-supervised graph autoencoders respectively. Models like Uni-mol (Zhou et al., 2023) and GraphMVP (Liu et al., 2021a) process 3D graph data effectively, while MoMu and MoleculeSTM combine SMILES and graph representations for downstream tasks.</p>
<p>The current challenge lies in enhancing modal fusion and merging data from various sources like text, graphs, and images. While these tasks demonstrate the vast potential of multi-modal learning, there remains significant scope for improvement, particularly in capturing the intricate relationships between different data sources. To address these challenges, our GIT-Mol model takes inspiration from the BLIP2's Q-Former approach, centering around text modalities while expanding the range of modal adaptability. For handling image modality data, in addition to employing the Swin Transformer, we have integrated contrastive learning</p>
<p>strategies into our training regime. Moreover, our dataset features more complex molecular structures compared to existing models. In contrast to MoMu's graph encoder, GIT-Mol not only employs contrastive learning but also combines it with cross-attention and modal fusion methods.</p>
<p>GIT-Mol is adept at mapping multi-modal tensors of varying lengths into a fixed-length unified latent space using the GIT-Former. Our model aligns at the entity level of molecules, providing a significant degree of flexibility. This adaptability ensures that our model remains versatile and applicable even in different scenarios, such as when replacing images or graphs with other modalities. These enhancements enable our model to more effectively capture and integrate the nuanced interplay between various modalities, thereby addressing the critical challenges in the field and pushing the boundaries of molecular science research.</p>
<h2>3. Methodology</h2>
<h3>3.1. Overview</h3>
<p>In this work, we present GIT-Mol, a multi-modal large language model for molecular science. As shown in Figure 1, molecular information is categorized into internal and external forms in our research. Internal information, including molecular SMILES and graph data, focuses on the intrinsic structure and rules of molecules. External information comprises molecular images and molecule captions, offering a more interpretable perspective. Furthermore, GIT-Mol can fuse data from diverse sources and present a comprehensive view of molecules. Specifically, we propose GIT-Former, a novel module capable of aligning all modalities into a unified latent space. It is designed to incorporate various molecular data types, including graphs, images, and text. In the pre-training phase, the model employs cross-attention and contrastive learning to align different modalities, enriching our understanding of the molecular data. Each fusion strategy within GIT-Former operates at the molecule level, ensuring optimal flexibility and scalability. During the finetuning stage, the process is guided by prompt learning that adapts to various tasks. Furthermore, these multi-modal representations can be directly processed through MLP to execute molecular properties prediction.</p>
<h3>3.2. Data and Preprocessing Strategy</h3>
<p>Data Modalities: The diversity of data modalities allows the model to learn and understand complex relationships across different modalities, which improves the performance in molecular property prediction and generation tasks. Molecular information is categorized as internal and external information, as shown in Figure 1. Internal data like SMILES strings and structural graphs are essential for predicting molecular properties and features. The SMILES strings provide a textual representation of molecular structures, concisely encoding vital connectivity and stereochemistry details. Furthermore, molecular structured graphs offer a topological view of molecules in two-dimensional space, where atoms are nodes and bonds are edges. In contrast, external data, including text descriptions and molecule images,
are user-friendly and easy to interpret. Molecular captions present textual descriptions that shed light on molecules' distinct characteristics and properties, offering a natural language context for the model. Furthermore, molecular images visually showcase atomic structures and bonding schemes, providing intuitive input for our model's molecular analysis.</p>
<p>Dataset: We collected approximately 4.8 million chemical compounds from the PubChem (Kim et al., 2019) database, providing a robust training dataset for our model. This dataset contains molecular images for image captioning tasks and serves as a rich resource for self-supervised learning with SMILES and molecular graph representations. In addition, we use the standard ChEBI-20 dataset (Edwards et al., 2021), consisting of 33,010 molecule-description pairs, for fine-tuning and evaluation. While the molecule captions in some databases are less complex than those in ChEBI-20, it is crucial to have concise and accurate descriptions of molecular characteristics. We constructed a dataset from ChEBI (Hastings et al., 2016) and PubChem. PubChem has over 320,000 molecule-description pairs, but many of them are too brief or contain non-informative content. To ensure high-quality captions, we analyzed the length distribution of captions in the CHEBI-20 dataset. We established a criterion for selecting captions that are longer than 96 characters and excluded those without any relevant information on their properties or functional groups.</p>
<p>Data Preprocessing: To ensure optimal model training and evaluation, we undertake several critical steps to refine raw data. Initially, we focus on data cleaning by rectifying inconsistencies, filling in missing values, or resolving data errors. It guarantees a high-quality data input for the model. Further, we remove compounds that feature low-frequency atoms by analyzing atom counts. It ensures the model focuses on patterns from prevalent and pertinent atomic structures. Lastly, using the RDKit toolkit (Bento et al., 2020), we validate and enhance the dataset's graph structures. We refine our collection to include only high-quality and representative compounds. By rigorously preprocessing the data, we assure data quality and bolster the model's learning efficiency and performance on targeted tasks.</p>
<h3>3.3. GIT-Former</h3>
<p>GIT-Former is an architecture that can map all modalities into a unified latent space, designed based on the QFormer architecture in BLIP2 (Li et al., 2023a). It leverages the strengths of various encoder and decoder models to suit the specific characteristics of different data modalities, such as SMILES strings, captions, images, and structure graphs. This section first introduces the model architecture. Then, it delineates the pre-training and fine-tuning in two stages: (1) the Xmodal-to-language representation learning stage with frozen encoders and (2) the Xmodal-to-language generative learning stage with the decoder. The GIT-Former model architecture and training strategy are shown in Figure 1. In the pre-training phase of GIT-Former, attention mechanisms and contrastive learning are used to align Xmodal data</p>
<p>with the target text modal. During the fine-tuning phase, Xmodal data is mapped onto learnable fixed-length queries, effectively aligning diverse modalities into a latent space.</p>
<p>GIT-Former model serves as a modality mixer to fuse molecular data. Unlike existing Visual Language (VL) models like BLIP2's Q-Former, our approach can address the graph and text modality translation tasks, providing the flexibility needed in bio-molecular multi-modal studies. To better fit our scientific scenario, we replace the BERT (Devlin et al., 2018) model with SciBERT (Beltagy et al., 2019), tailored explicitly to scientific text, and enhance it with cross-attention mechanisms for graph and text modalities. As shown in Figure 1, our model can adaptively adjust its training modules to achieve modality alignment effectively. Moreover, alternatives such as MolT5 can also be utilized as embedding and self-attention layers, providing flexibility to adapt to different molecular scenarios.</p>
<p>Encoder and Decoder: MolT5 is an advanced language model to translate molecule and molecular textual descriptions. We adopt it to serve as our text encoder and decoder. For image encoding, we adopt the Swin Transformer (Liu et al., 2021b) from SwinOCSR, and for graph encoding, we select the GIN model from the pre-trained MoMu model. This multi-encoder and decoder setup equips our model with the flexibility to adapt to the demands of each data modality, enhancing the model's performance and efficacy.</p>
<p>Cross-Attention Mechanism is central to GIT-Former. This module facilitates the alignment of image and graph modalities to text modalities, which encompass molecular captions and SMILES strings. This mechanism can be mathematically formulated as a multi-head attention operation, which captures inter-modal data relationships:</p>
<p>MultiHead $\left(Q_{T t}, K, V\right)=\operatorname{Concat}\left(\operatorname{head}<em h="h">{1}, \ldots, \operatorname{head}</em>$}\right) W_{O</p>
<p>$$
\text { head }<em T="T" t="t">{i}=\operatorname{Attention}\left(Q</em>\right)
$$} W_{Q i}, K W_{K i}, V W_{V i</p>
<p>In the context of GIT-Former, $Q$ represents queries, $Q_{T t}$ represents the queries from the target text modality (caption or SMILES string), $K$ stands for keys, $V$ denotes values, and $W$ refers to the learned weight matrices to project the input into appropriate spaces. For source modalities in equation (2), $i$ represent the graph, image, and source text modality. For each source modality, GIT-Former computes a separate set of cross-attention weights, aligning the $T t$ modality to each of the source modalities. This ensures that every detail of the image or graph is precisely reflected in its corresponding textual representation. The attention mechanism can be visualized as:</p>
<p>$$
\operatorname{Attention}\left(Q_{T t}, K, V\right)=\operatorname{softmax}\left(\frac{Q_{T t} K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>where $d_{k}$ is the dimensionality of the queries and keys.
By leveraging this cross-attention framework, GITFormer efficiently discerns and encodes intricate relationships and dependencies between modalities. This is shown
in Figure 1, emphasizing the alignment between image, graph and source text modalities to $T t$ modality.</p>
<h3>3.4. Pre-training Strategy</h3>
<p>Our research involves various training strategies. In the pre-training phase, we use frozen image and graph encoders and SciBERT for text encoding. Moreover, the GIT-Former aligns each modality with the target text by self-supervised learning, enhancing molecular translation tasks. We use a unique cross-attention method in pre-training to achieve inter-modal and contrastive learning.</p>
<p>Xmodal-Text Matching (XTM) aims to align different modal representations with corresponding text. It is a binary classification task that determines if a set of cross-modal texts matches. A bi-directional self-attention mask facilitates interaction between learnable queries and modality embeddings. The learnable query is an initialized, fixed-length tensor of zeros. The embeddings from different modalities are mapped onto this fixed-length tensor through cross-attention mechanisms. This process results in the generation of query embeddings enriched with multimodal information. These query embeddings are processed through a linear classifier to get a logit, and the average across queries gives the final matching score. For the different modalities (X-source-modal and text-target-modal), we convert the inputs into embeddings ( $E_{x}$ and $E_{t}$ ) and then pair them with matched and mismatched samples, creating a contrast for the model to recognize the correct combinations. Self-attention and cross-attention mechanisms are employed to understand relations within and across modalities, resulting in a unified embedding that contains multimodal information. This process can be represented with $f_{\text {att }}$ as the function for both attention mechanisms.</p>
<p>$$
E_{\text {fused }}=f_{\text {att }}\left(\operatorname{concat}\left(\left[E_{x}, E_{t}\right]\right)\right)
$$</p>
<p>Lastly, the fused embeddings $E_{\text {fused }}$ are processed through a linear layer with a weight of $W$, yielding a predictive score (logit). The cross-entropy loss between the logit and actual labels ( 1 for match, 0 for mismatch) is computed. This loss is a metric for the model's performance on the binary classification task, and its optimization helps the model effectively match information across different modalities.</p>
<p>$$
\operatorname{loss}<em _fused="{fused" _text="\text">{\text {xtm }}=\operatorname{cross} \text {_entropy }\left(W \cdot E</em>, y\right)
$$}</p>
<p>Xmodal-Text Contrastive Learning (XTC) aligns different information types with corresponding text representations. This technique contrasts the similarity of matched cross-modal text against mismatches. The representation from the task modality is aligned with the text, and the one with the highest similarity is selected as the crossmodal data pair. In the XTC approach, we first employ the GIT-Former to attain the learned representation of the Xmodal input. We then extract the Xmodal features $E_{x}$ from this representation. Subsequently, we compute the mutual information $I\left(E_{x}, E_{t}\right)$, individually for Xmodal and</p>
<p>text representations $E_{t}$. Finally, we use cross-entropy to calculate the loss function $L_{x t c}$. The operation $\cdot$ denotes the dot product between the two embeddings. With this in place, the mutual information is determined as:</p>
<p>$$
I\left(E_{x}, E_{t}\right)=E_{x} \cdot E_{t}
$$</p>
<p>Finally, the cross-entropy loss $L_{x t c}$ is computed as:</p>
<p>$$
L_{x t c}=\operatorname{cross} \operatorname{entropy}\left(I\left(E_{x}, E_{t}\right), y\right)
$$</p>
<h3>3.5. Fine-tuning Strategy</h3>
<p>During the fine-tuning phase, we classify our tasks into two main categories: Modality Translation and Molecular Property Prediction. Within the Modality Translation task, as illustrated in Figure 1, our focus areas include molecule captioning, molecule generation, and molecular image recognition. To enhance the flexibility of this task, we incorporate our prompt manager, which enables an any-to-language training mechanism, facilitating a more versatile modality translation process. In the Molecular Property Prediction task, we fine-tune our model using labeled SMILES and graph data from MoleculeNet (Wu et al., 2018), specifically classification tasks.</p>
<p>Prompt Tuning for Modality Translation: As shown in Figure 1, our model encodes data from each modality into embeddings, and then the GIT-Former maps embeddings into a unified latent space. This allows seamless interaction and translation among different modalities within a common representation. Moreover, the framework supports translating information across any modality to the language modality, providing a flexible setup for managing tasks such as prompt management, data loading, and model training. This is achieved by designing various task types and corresponding prompts according to the specific modality.</p>
<p>The any-to-language molecular translation strategy fundamentally revolves around a text-centric pretraining and finetuning approach. It can be implemented through a combination of prompts, GIT-Former, and Large Language Models (LLMs) to accomplish any-to-language tasks. The strength of this strategy lies in its flexibility in modal transformation and adaptability to various tasks. We test directive and guiding prompts for each task. Finally, we employ the following prompt for the SMILES string generation and recognition task: "Given the provided inputs, generate the corresponding SMILES string." Once this prompt is fused with the GIT-Former embeddings, the combined representation is channeled to the MolT5 decoder. Here, the training objective is to align the input data (a caption or an image) with its pertinent SMILES data. This design ensures that the model remains sensitive to the nuanced differences between modalities while still producing reliable SMILES representations, bridging the gap between visual or structural information and text.</p>
<p>In the molecule captioning task, our prompt choice is: "This is the inputs of the molecule, and the corresponding caption is: " Similar to the previously outlined procedure, upon amalgamation of this prompt with the GIT-Former
embeddings, the aggregate is routed to the MolT5 decoder. The overarching training goal in this scenario revolves around ensuring that the input data (SMILES string or graph) aligns impeccably with its associated caption data. This task reinforces the model's ability to understand molecular structures from various perspectives. It enriches its competency in generating descriptive text, demonstrating the model's broad applicability in multi-modal molecular informatics.</p>
<p>Fine-tuning for Property Prediction Tasks: We use MolT5 as embedding and self-attention layers to process SMILES strings and employ contrastive learning to pretrain SMILES string and graph embeddings. Our approach emphasizes utilizing multi-modal data integrating modality representations through attention mechanisms. After obtaining this enriched embedding, it is passed through a Multi-Layer Perceptron (MLP), which outputs the predicted molecular properties.</p>
<p>Language models have inherent strengths in understanding context and relationships, our model is particularly apt for benchmark tasks related to bio-activity and toxicity classifications. The model's ability to process and relate complex molecular data makes it well-suited for these specific prediction challenges.</p>
<h3>3.6. The Essentiality of Architecture and Strategy</h3>
<p>In molecular science, molecular data have multiple representation formats, such as molecular images, structural graphs, SMILES strings, and physical properties. Most existing models bridge two modalities into a unified space, like images with SMILES strings or graph structures with captions. In summary, accommodating an expanded set of modalities remains challenging for current models.</p>
<p>Architecture: GIT-Former provides a dynamic and scalable solution tailored for multi-modal molecular data. The prowess of GIT-Former emanates from its cross-attention mechanism, allowing it to integrate and adapt to new modalities within molecular representations effortlessly. The key feature of the GIT-Former is its ability to map multi-modal tensors of varying lengths into a fixed-length unified latent space. This mapping allows the model to align data at the molecular entity level, offering significant flexibility. Such adaptability ensures that GIT-Mol remains versatile and applicable in various scenarios, including those involving modalities beyond images and graphs.</p>
<p>Training strategies: The choice of XTC and XTM as training strategies, both strategies operate at the individual vector level, providing flexibility in model training. XTC is crucial for distinguishing between positive and negative molecular samples, especially since minor molecular changes can lead to significant shifts in bioactivity or drug interactions. Meanwhile, XTM ensures that each molecular representation aligns precisely with its corresponding text or description.</p>
<p>Multi-modal Representation: GIT-Former can extract information from each modality and ensure that this information effectively integrates within a unified representation</p>
<p>space. It ensures a deeper understanding of the nuances within each modality. Moreover, when paired with prompt learning and the pre-trained state-of-the-art MolT5 decoder, GIT-Mol effectively leverages this information in specific tasks, producing the corresponding molecular data.</p>
<h2>4. Results</h2>
<p>In our experiments, we delve into crucial tasks related to molecular multi-modal translation, including molecule captioning (Sec 4.1), text-based de novo molecule generation (Sec 4.2), and molecular image recognition (Sec 4.3). Additionally, we utilize the processed embeddings for molecular property prediction tasks for the molecular multimodal representation (Sec 4.4). Additionally, we have instituted ablation studies to ascertain the contributions of multimodal data, cross-attention, and prompt learning to the overall performance. To evaluate our model's efficacy, we compare embeddings pre-model and post-model processing, aiming to understand the model's impact on molecular data (Sec 4.5). Finally, we detail the training configurations and hyperparameters employed in our experiments (Sec 4.6).</p>
<h3>4.1. Molecule Captioning</h3>
<p>Experimental Setup: Molecule captioning primarily focuses on generating textual descriptions of molecules based on their inherent information, such as their SMILES strings, graph representations, and other molecular features. To evaluate the performance of our model, we conduct experiments using a set of established Natural Language Processing (NLP) metrics. These metrics include BLEU, which measures the n-gram overlap between generated and reference sentences; ROUGE, which emphasizes the overlap of various n-grams, word sequences, and word pairs; and METEOR, a comprehensive metric that takes into account precision, recall, synonymy, stemming, and word order. These collectively assess the quality and relevance of the generated text in molecule captioning. Within the framework of our GIT-Mol model, SciBERT and MolT5 serve as language model components, and then we choose SciBERT and MolT5 as the baselines for our model. This selection is particularly pertinent in our Molecule Generation experiments.</p>
<p>Molecule Caption Dataset: Our team procured a substantial dataset of 320,000 molecules accompanied by detailed descriptions from the PubChem Record Description category. We enhanced this dataset by conducting a thorough crawl of the corresponding captions (up to a maximum of 3) from each molecule page. We augmented our dataset by combining it with the ChEBI-20 Dataset, thus forming a highly comprehensive and informative multi-modal caption dataset.</p>
<p>We further carried out data cleaning to remove "invalid" descriptions in the captions, such as those that merely specify the source of the compound without any pertinent information about its properties or characteristics. However, we opted to retain succinct descriptions that provide essential information about the compound. This enriched dataset
consists of approximately 90,000 data points, incorporating multi-modal data such as images of the molecules, SMILES strings, and captions.</p>
<p>Results and Observations: Our model demonstrates superior performance in generating high-quality, relevant molecule captions. Detailed results are shown in Table 1. This experiment underscores the potential of multi-modality and suggests areas for further optimization and exploration.The GIT-Mol(graph) model delivers better performance than GIT-Mol(SMILES) across all metrics. It implies that graph representations might capture molecular structures more effectively than SMILES strings. However, neither of them outperforms the multi-modal model. In the ablation study, the employment of this multi-modal model resulted in an impressive $10 \%-15 \%$ improvement over single-modality models. It suggests that each modality brings complementary information to the task. As for pretraining strategies, the model pre-trained only with XTM shows room for enhancement, underlining the effectiveness of the XTC strategy in boosting overall performance.</p>
<p>Case Studies: We select a set of molecules, feed them to our model, and let it generate captions based on the learned representations. The results are shown in Figure 2. For each molecule, we present the ground truth, the captions generated by our model and the baseline model.</p>
<h3>4.2. Text-Based de novo Molecule Generation</h3>
<p>Experimental Setup: Text-Based de novo Molecule Generation aims to produce molecular SMILES representations based on provided textual captions of molecules. To gauge the accuracy and validity of the molecules generated by our model, we employ a mix of cheminformatics and NLP metrics:</p>
<ul>
<li>Fingerprint Tanimoto Similarity (FTS): We utilize this to measure the chemical similarity between the ground-truth molecules and the generated ones, considering multiple fingerprint types: MACCS, RDK, and Morgan fingerprints. FTS provides a decimal value between 0 (no similarity) and 1 (perfect similarity), offering a quantifiable measure of chemical resemblance.</li>
<li>BLEU: Used primarily in NLP tasks, BLEU scores evaluate the overlap of n-grams between generated and reference SMILES strings, signifying the closeness of the generated SMILES strings to the actual ones.</li>
<li>Exact Match: It simply checks if the generated SMILES string matches the reference SMILES string. An exact match score directly measures the model's precision in producing accurate molecular representations.</li>
<li>Levenshtein distance: This metric calculates the minimum number of single-character edits (i.e., insertions, deletions, or substitutions) needed to change one SMILES string into another. A smaller distance</li>
</ul>
<p>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BLEU-2</th>
<th>BLEU-4</th>
<th>ROUGH-1</th>
<th>ROUGH-2</th>
<th>ROUGH-L</th>
<th>METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td>SciBERT</td>
<td>0.184</td>
<td>0.113</td>
<td>0.412</td>
<td>0.327</td>
<td>0.397</td>
<td>0.367</td>
</tr>
<tr>
<td>MolT5-base</td>
<td>0.316</td>
<td>0.247</td>
<td>0.572</td>
<td>0.480</td>
<td>0.545</td>
<td>0.529</td>
</tr>
<tr>
<td>GIT-Mol(SMILES)</td>
<td>0.264</td>
<td>0.176</td>
<td>0.477</td>
<td>0.374</td>
<td>0.451</td>
<td>0.430</td>
</tr>
<tr>
<td>GIT-Mol(Graph)</td>
<td>0.290</td>
<td>0.210</td>
<td>0.540</td>
<td>0.445</td>
<td>0.512</td>
<td>0.491</td>
</tr>
<tr>
<td>GIT-Mol(XTM)</td>
<td>0.264</td>
<td>0.187</td>
<td>0.521</td>
<td>0.421</td>
<td>0.494</td>
<td>0.471</td>
</tr>
<tr>
<td>GIT-Mol op</td>
<td>0.312</td>
<td>0.237</td>
<td>0.556</td>
<td>0.468</td>
<td>0.535</td>
<td>0.525</td>
</tr>
<tr>
<td>GIT-Mol</td>
<td>$\mathbf{0 . 3 5 2}$</td>
<td>$\mathbf{0 . 2 6 3}$</td>
<td>$\mathbf{0 . 5 7 5}$</td>
<td>$\mathbf{0 . 4 8 5}$</td>
<td>$\mathbf{0 . 5 6 0}$</td>
<td>$\mathbf{0 . 5 3 3}$</td>
</tr>
</tbody>
</table>
<p>Table 1 Molecule captioning results. In the subsequent experimental results, op signifies models that do not utilize prompt learning, XTM represents models that only utilize XTM pre-training strategy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Molecule</th>
<th style="text-align: center;">MolT5</th>
<th style="text-align: center;">GIT-Mol</th>
<th style="text-align: center;">Ground truth</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The molecule is an alphaamino acid ester obtained by formal condensation of the carboxy group of glycyl-L-tryptophan with the amino group of glycine. It derives from a glycyl-L-tryptophan and a glycyl-L-tryptophan. It is a tautomer of a glycyl-L-tryptophan zwitterion.</td>
<td style="text-align: center;">The molecule is an organooxygen compound and an organonitrogen compound. It is functionally related to an alpha-amino acid.</td>
<td style="text-align: center;">The molecule is an alphaamino acid.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The molecule is a prostaglandin carboxylic acid anion that is the conjugate base of prostaglandin H 2 , obtained by deprotonation of the carboxy group; major species at pH 7.3 . It is a conjugate base of a prostaglandin H 2 .</td>
<td style="text-align: center;">The molecule is a docosanoid anion that is the conjugate base of (4Z,7Z,10Z,13Z,15E,19Z)docosahexaenoic acid, obtained by deprotonation of the carboxy group; major species at pH 7.3 . It is a conjugate base of a (4Z,7Z,10Z,13Z,15E,19Z)docosahexaenoic acid.</td>
<td style="text-align: center;">The molecule is a dihydroxydocosahexaenoat e that is the conjugate base of (7S,14S)-dihydroxy-(4Z,8E,10E,12Z,16Z,19Z)docosahexaenoic acid, obtained by deprotonation of the carboxy group; major species at pH 7.3 . It is a conjugate base of a (7S,14S)-dihydroxy-(4Z,8E,10E,12Z,16Z,19Z)docosahexaenoic acid.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The molecule is a neolignan isolated from the stems of Sinocalamus affinis. It has a role as a plant metabolite.</td>
<td style="text-align: center;">The molecule is a neolignan isolated from the stems of Sinocalamus affinis. It has a role as a plant metabolite. It is a neolignan, a dimethoxybenzene, a dimethoxybenzene and a member of methoxybenzenes.</td>
<td style="text-align: center;">The molecule is a neolignan isolated from the stems of Sinocalamus affinis. It has a role as a plant metabolite. It is a neolignan, a furofuran, a dimethoxybenzene, a member of phenols and a primary alcohol.</td>
</tr>
</tbody>
</table>
<p>Figure 2: Study case of Molecule Caption. The GIT-Mol model exhibits precise chemical characterization, aligning closely with ground truth information.
indicates a closer similarity between the generated and reference SMILES string.</p>
<ul>
<li>Validity: We employ RDKit, a cheminformatics software, to check the chemical validity of the generated SMILES string. By reporting the percentage of valid molecules, we ensure that our model does not just produce semantically accurate SMILES strings but also chemically feasible ones.</li>
</ul>
<p>To stay consistent with our methodology in the molecule captioning task, we employ MolT5-base as both the decoder and the baseline for comparison in our experimental setup.</p>
<p>Molecule Generation Dataset: We use the same ChEBI20 Dataset as MolT5 for fine-tuning our model on the molecule generation task. It contains various molecular structures represented in SMILES notation, providing a rich resource for training models to generate novel molecules.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BLEU</th>
<th>Exact</th>
<th>Levenshtein</th>
<th>MACCS FTS</th>
<th>RDK FTS</th>
<th>Morgan FTS</th>
<th>Validity</th>
</tr>
</thead>
<tbody>
<tr>
<td>SciBERT</td>
<td>0.459</td>
<td>0.005</td>
<td>55.459</td>
<td>0.499</td>
<td>0.344</td>
<td>0.254</td>
<td>0.915</td>
</tr>
<tr>
<td>MolT5-base</td>
<td>$\mathbf{0 . 7 6 9}$</td>
<td>$\mathbf{0 . 0 8 1}$</td>
<td>$\mathbf{2 4 . 4 5 8}$</td>
<td>0.721</td>
<td>$\mathbf{0 . 5 8 8}$</td>
<td>$\mathbf{0 . 5 2 9}$</td>
<td>0.772</td>
</tr>
<tr>
<td>GIT-Mol(XTM)</td>
<td>0.245</td>
<td>0.0</td>
<td>82.633</td>
<td>0.322</td>
<td>0.196</td>
<td>0.087</td>
<td>1.0</td>
</tr>
<tr>
<td>GIT-Mol $o p$</td>
<td>0.721</td>
<td>0.041</td>
<td>30.41</td>
<td>0.705</td>
<td>0.477</td>
<td>0.453</td>
<td>0.812</td>
</tr>
<tr>
<td>GIT-Mol</td>
<td>0.756</td>
<td>0.051</td>
<td>26.315</td>
<td>$\mathbf{0 . 7 3 8}$</td>
<td>0.582</td>
<td>0.519</td>
<td>$\mathbf{0 . 9 2 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2 Molecule generation results. Our model performs similarly to MolT5-base for generating molecular SMILES strings but excels in ensuring molecule validity. With a molecule validity of $\mathbf{9 2 . 8 \%}$, our model surpasses MolT5-base's validity of $77.2 \%$ by over $\mathbf{2 0 \%}$.</p>
<p>Results and Observations: Our model exhibits impressive results in generating chemically valid molecules that closely match the ground-truth molecules regarding chemical and structural characteristics. The results are shown in Table 2. Compared to the MolT5-base(caption) model, GIT-Mol(caption) slightly outperforms the MACCS FTS metric and presents closely competitive results on the other two molecular similarity measures, RDK FTS and Morgan FTS. Notably, the performance of GIT-Mol(caption) on the "Validity" metric reaches an impressive 0.928 , significantly higher than the other two models. This indicates that GITMol can generate a higher proportion of valid molecules while still preserving high molecular similarity. In the ablation study, the model pre-trained only with XTM displays a higher propensity for overfitting, with a lack of answer diversity and a tendency to generate homogenous molecular structures. Additionally, we demonstrate the necessity of prompt learning in influencing the performance of this task. We provide some examples of molecule generation based on text descriptions and detailed analysis.</p>
<p>Case Studies: In Figure 3, we showcase the outcomes for every molecule instance. Our model, the baseline model, and GPT4 all generate SMILES strings, which are compared to the ground truth. This comparison allows for a visual evaluation of the quality and relevance of the generated molecules, offering a better understanding of our model's effectiveness.</p>
<h3>4.3. Molecule Image Recognition</h3>
<p>Experimental Setup: Molecular Image Recognition focuses on converting 2D molecular images into their corresponding SMILES representations. We employ the identical set of metrics as those used in "Text-Based de novo Molecule Generation." For a fair and rigorous assessment, we juxtapose our model with SwinOCSR, an acknowledged efficient model in molecular image captioning.</p>
<p>Molecule Image Dataset: We employ the same dataset as the Molecule Caption Dataset utilized in the Molecule Captioning Task. The distinction lies in the training data composition, where the pairings are between SMILES strings and their corresponding molecular images rather than textual descriptions.</p>
<p>Results and Observations: The results are presented in Table 3. The empirical outcomes indicate that our model, whether employing prompt learning or not, the GIT-Mol model outperforms the SwinOCSR model across all evaluated metrics. It emphasizes the efficacy of the GIT-Mol architecture in the molecule image recognition task. In the ablation study, similar to findings in the molecule generation experiment, the model exclusively using the XTM pre-training strategy not only overfitted more easily but also converged more slowly, resulting in the generation of uniform molecular structures. Our model demonstrates the model's extensibility and effectiveness of prompt learning.</p>
<h3>4.4. Molecular Property Prediction</h3>
<p>Experimental Setup: In our study, we use six essential classification datasets related to molecular biological activity from MoleculeNet (Wu et al., 2018) , including Tox21, ToxCast, Sider, ClinTox, Bace, and BBBP. These datasets contribute to understanding a molecular properties and effects. We ensure the model's fairness and robustness by using a scaffold-based splitting approach for structuring training, validation, and test sets. This leads to diverse molecule evaluations. The pre-trained model on PubChem processes the datasets into molecule embeddings, and we optimize using the Binary Cross-Entropy (BCE) loss function.</p>
<p>MoleculeNet for Molecular Property Prediction. In our study, we chose a few classification task datasets from MoleculeNet as our fine-tuning datasets. These datasets include Tox21, ToxCast, Sider, ClinTox, BBBP, and Bace, each covering different molecular property prediction tasks.</p>
<ul>
<li>Tox21: A collaborative project to identify potential toxins, containing toxicity data for approximately 7800 compounds across 12 different pathways.</li>
<li>ToxCast: Contains biological activity data for about 8600 environmental compounds from over 600 in vitro experiments, used to assess potential toxicity.</li>
<li>Sider: A database of side effect information for around 1400 drugs, used for predicting potential drug side effects.</li>
<li>ClinTox: Includes toxicity information for known drugs and clinical toxicity predictions for unapproved compounds, primarily used to predict clinical toxicity.</li>
<li>BBBP: Utilized to predict whether compounds can penetrate the blood-brain barrier, aiding in understanding brain delivery capacity.</li>
</ul>
<h2>Description</h2>
<p>The molecule is a nucleotidesugar oxoanion that is a trianion arising from deprotonation of the carboxy and diphosphate OH groups of UDP-alpha-Dglucuronic acid; major species at pH 7.3. It has a role as a human metabolite. It is a nucleotidesugar oxoanion and a carbohydrate acid derivative anion. It is a conjugate base of an UDP-alpha-D-glucuronic acid.</p>
<p>The molecule is a indol-3-yl carboxylic acid anion that is the conjugate base of 3-(indol-3yl)pyruvic acid It has a role as a Saccharomyces cerevisiae metabolite and a plant metabolite. It is an indol-3-yl carboxylic acid anion and a 2oxo monocarboxylic acid anion. It is a conjugate base of a 3-(indol-3-yl)pyruvic acid.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>C1 $=\mathrm{CN}(\mathrm{C}(=\mathrm{O}) \mathrm{NC} 1=\mathrm{O})$ $[C @ H] 2[C @ @ H](<a href="[C @ H](O 2) C O P$ $(=\mathrm{O})([\mathrm{O}-]) \mathrm{OP}(=\mathrm{O})([\mathrm{O}-]$ )O $[C @ @ H] 3[C @ @ H]$ ([C@H]([C@@H]([C@H](O 3) C(=O)[O-])O)O) O">C @$ @H $</a> O$
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>C1=C(=C2C(=C1)C(= CN2)CC(=O)C(=O)[O-]
<img alt="img-3.jpeg" src="img-3.jpeg" />
(2R)-trihomocitric acid is a chiral tricarboxylic acid in which a central carbon carries hydroxy, carboxy, carboxymethyl and 4carboxybutyl substituents with R -configuration at the chiral centre. It is a conjugate acid of a (2R)-trihomocitrate(3-).
<img alt="img-4.jpeg" src="img-4.jpeg" />
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 3: Study case of Molecule Generation. GIT-Mol stands out for its ability to generate valid molecules. Besides, even if not identical to the ground truth, it still faithfully adhere to the features described in the textual captions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">Exact</th>
<th style="text-align: center;">Levenshtein</th>
<th style="text-align: center;">MACCS FTS</th>
<th style="text-align: center;">RDK FTS</th>
<th style="text-align: center;">Morgan FTS</th>
<th style="text-align: center;">Validity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SwinOCSR</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">9.157</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.827</td>
</tr>
<tr>
<td style="text-align: center;">GIT-Mol(XTM)</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">62.075</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.137</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">GIT-Mol op</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">8.675</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.850</td>
</tr>
<tr>
<td style="text-align: center;">GIT-Mol</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 1}$</td>
<td style="text-align: center;">$\mathbf{6 . 5 7 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 9}$</td>
</tr>
</tbody>
</table>
<p>Table 3
Molecule image recognition results. The results suggest that prompt learning aids in producing more accurate SMILES strings from molecule images.</p>
<ul>
<li>Bace: Contains inhibitory activity information on beta-secretase 1 (BACE1) for 1513 compounds, useful in identifying potential candidate drugs for disease treatment.</li>
</ul>
<p>Results and Observations: Our model performs commendably in predicting molecular properties, achieving high AUC scores across multiple runs. The results of our model and baselines are shown in Table 4. We demonstrate the effectiveness of multi-modal data combining graph and SMILES for molecular representation, which shows an average performance improvement of $5 \%-10 \%$.</p>
<h3>4.5. Embeddings Visualization</h3>
<p>GIT-Former Processing: We experimented on 80 molecules to create vector visualizations, as shown in Figure 4. The first image in Figure 4(a) shows the vector representations created by the encoders for each modality of molecular data. The second image displays the vector representations without the image embeddings. Our analysis of these images reveals that the graph vectors are more concentrated in the original vector representations, while the image representations are more diverse. Additionally, the SMILES and caption representations, both text-based, are not easily distinguishable.</p>
<p>Pre-training Effects: In Figure 4(b), we display the vectors processed by an untrained GIT-Former. All vectors</p>
<p>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Tox21 $\uparrow$</th>
<th>ToxCast $\uparrow$</th>
<th>Sider $\uparrow$</th>
<th>ClinTox $\uparrow$</th>
<th>BBBP $\uparrow$</th>
<th>Bace $\uparrow$</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>Molecules</td>
<td>7831</td>
<td>8575</td>
<td>1427</td>
<td>1478</td>
<td>2039</td>
<td>1513</td>
<td>-</td>
</tr>
<tr>
<td>Task</td>
<td>12</td>
<td>617</td>
<td>27</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>-</td>
</tr>
<tr>
<td>KV-PLM</td>
<td>$72.1 \pm 1.0$</td>
<td>$55.0 \pm 1.7$</td>
<td>$59.8 \pm 0.6$</td>
<td>-</td>
<td>$70.5 \pm 0.5$</td>
<td>$78.5 \pm 2.7$</td>
<td>67.20</td>
</tr>
<tr>
<td>GraphCL</td>
<td>$75.1 \pm 0.7$</td>
<td>$63.0 \pm 0.4$</td>
<td>$59.8 \pm 1.3$</td>
<td>$77.5 \pm 3.8$</td>
<td>$67.8 \pm 2.4$</td>
<td>$74.6 \pm 2.1$</td>
<td>69.64</td>
</tr>
<tr>
<td>GraphMVP</td>
<td>$74.9 \pm 0.5$</td>
<td>$63.1 \pm 0.2$</td>
<td>$60.2 \pm 0.13$</td>
<td>$79.1 \pm 2.8$</td>
<td>$70.8 \pm 0.5$</td>
<td>$79.3 \pm 1.5$</td>
<td>71.23</td>
</tr>
<tr>
<td>MoMu</td>
<td>$75.6 \pm 0.3$</td>
<td>$63.4 \pm 0.5$</td>
<td>$60.5 \pm 0.9$</td>
<td>$79.9 \pm 4.1$</td>
<td>$70.5 \pm 2.0$</td>
<td>$76.7 \pm 2.1$</td>
<td>71.1</td>
</tr>
<tr>
<td>Mole-BERT</td>
<td>$\mathbf{7 6 . 8} \pm \mathbf{0 . 5}$</td>
<td>$64.3 \pm 0.2$</td>
<td>$62.8 \pm 1.1$</td>
<td>$78.9 \pm 3.0$</td>
<td>$71.9 \pm 1.6$</td>
<td>$80.8 \pm 1.4$</td>
<td>72.58</td>
</tr>
<tr>
<td>GIT-Mol(SMILES)</td>
<td>$73.9 \pm 0.7$</td>
<td>$62.1 \pm 0.4$</td>
<td>$60.1 \pm 1.1$</td>
<td>$83.5 \pm 3.1$</td>
<td>$71.9 \pm 1.8$</td>
<td>$68.4 \pm 1.7$</td>
<td>70.0</td>
</tr>
<tr>
<td>GIT-Mol(Graph)</td>
<td>$75.4 \pm 0.5$</td>
<td>$65.3 \pm 0.7$</td>
<td>$58.2 \pm 0.9$</td>
<td>$78.9 \pm 2.5$</td>
<td>$71.1 \pm 1.5$</td>
<td>$65.8 \pm 1.8$</td>
<td>69.1</td>
</tr>
<tr>
<td>GIT-Mol(G+S)</td>
<td>$75.9 \pm 0.5$</td>
<td>$\mathbf{6 6 . 8} \pm \mathbf{0 . 5}$</td>
<td>$\mathbf{6 3 . 4} \pm \mathbf{0 . 8}$</td>
<td>$\mathbf{8 8 . 3} \pm \mathbf{1 . 2}$</td>
<td>$\mathbf{7 3 . 9} \pm \mathbf{0 . 6}$</td>
<td>$\mathbf{8 1 . 0 8} \pm \mathbf{1 . 5}$</td>
<td>$\mathbf{7 4 . 9 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4 Results for molecular property prediction (classification). The combined use of SMILES and 2D graphs enhances our multi-modal molecular representation, which outperforms both single-modal models and other multi-modal approaches. appear to move towards a uniform distribution. In contrast, Figure 4(c) shows the vectors processed by a pre-trained GIT-Former. The vector distribution shifts from dispersed to concentrated, with the outermost layer being the graph embeddings, the image embeddings, and the SMILES strings and captions in the text modality. This occurs because our GIT-Former's pre-training strategy involves aligning other modalities to the text modality. The SMILES strings and captions can be further distinguished from the original vector representations.</p>
<p>Clustering and Distribution of atoms: The image in Figure 4(d) displays the distribution of the number of atoms in a molecule. The color changes from dark to light, indicating an increase in the number of atoms. The distribution of the number of atoms is similar when viewed on the same vertical axis. Moving on to Figure 4(e), we conducted K-means clustering on the molecules. We observed the distribution of C, N, and O atoms in Figure 4(f) based on different groups. We can see some differences in the distribution among the different groups. However, by representing different molecules based on these differences, our GITFormer can differentiate between different modalities, data types within the same modality, and molecular properties within the same data type.</p>
<h3>4.6. Training Settings</h3>
<p>To achieve optimal performance with our GIT-Former model, we carefully designed both pretraining and finetuning stages. This section provides an overview of the training configurations, detailing the encoder settings, hyperparameter values, and the specific adjustments made for different molecular tasks. By meticulously orchestrating these settings, we ensure that GIT-Former can harness its full potential in varied application scenarios.</p>
<ul>
<li>Pre-training and Hyperparameters: The encoder settings freeze both the image and graph encoders. For modality selection, 1-3 arbitrary modalities and the target text modality are selected for pre-training. The learning rate ranges from $1 \mathrm{e}-4$ to $5 \mathrm{e}-5$, and the training process is constrained to less than 20 epochs. The batch size is determined by the number of input modalities, ranging from 32 to 128 (the more input modalities, the smaller the batch size).</li>
<li>Optional Settings: There are options for replacement in the GIT-Former model, such as replacing the Embedding layer with other models like BERT or MolT5's Encoder. Training strategies can also be tailored to the experiment, including Xmodal-Text Matching (XTM), Xmodal-Text Contrastive Learning (XTC), or both.</li>
<li>Molecular Translation Task: The task type determines the hyperparameters, including input and output modalities and batch size. The model has No parameters frozen, and the learning rate ranges from $1 \mathrm{e}-4$ to $5 \mathrm{e}-5$. The model is trained for less than 20 epochs, with a batch size between 32 and 64.</li>
<li>Molecular Property Prediction Task: The task determines specific hyperparameters, such as batch size. The model's Embedding layer utilizes MolT5-large. The learning rate is again set between $1 \mathrm{e}-4$ and $5 \mathrm{e}-5$, with less than 100 epochs in training. Patience is set to 10 , and batch size ranges between 16 and 256 .</li>
</ul>
<h2>5. Discussion</h2>
<p>Data Quality: In molecular science, the quality and diversity of data are crucial. Multi-modal data provides a more comprehensive perspective, enhancing model predictions. Furthermore, the datasets require benchmarks, validation subsets, and metrics to gauge their representativeness and challenges.</p>
<p>Model Techniques: Large language models, like the GPT series, have achieved significant success in natural language processing and multi-modal tasks. Applying pretrained models to specific molecular tasks and refining them can be highly beneficial. Moreover, distillation techniques can shrink models, fitting them for limited-resource settings, and data distillation targets training valuable subsets from large datasets. On the other hand, incorporating molecular science knowledge into model structures can pave a</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 4: Embeddings visualization. (a) Original vector representations from various molecular data modalities. (b) Vector distribution is processed by an untrained GIT-Former, illustrating a tendency towards uniformity. (c) Hierarchical vector distribution processed by a pre-trained GIT-Former showcasing the layered separation of modalities with the outermost layer representing graph embeddings, followed by image embeddings, and innermost containing SMILES strings and captions from the text modality. (d) Distribution of atoms in molecules, with color gradients indicating increasing atom numbers. (e) Results of K-means clustering applied to molecular data. (f) Distribution of $\mathrm{C}, \mathrm{N}$, and O atoms across different clusters. The pre-training effects demonstrate GIT-Former's ability to differentiate among modalities, subtypes within a modality, and specific properties within a given data type.
more robust learning pathway, demanding specially designed model layers or preprocessing steps.</p>
<p>Scientific Insights: Beyond vector visualization, visual outputs from models can aid scientists in comprehending decision-making processes. Attention mechanisms and other interpretability tools can reveal the data parts the</p>
<p>model relies on for predictions and which structures in the model focus on specific knowledge areas.</p>
<p>Practical Implications: The improvement of 5\%-10\% in accuracy for property prediction and a significant $20.2 \%$ enhancement in molecule generation validity demonstrate the superior generalizability of our multi-modal approach. The advancements benefit AI-aided drug discovery (AIDD). A model with higher predictive precision can speed up the process of drug discovery, enhancing both time and cost efficiency. In drug discovery, where expenses can reach billions, even a $5 \%$ increase in accuracy could mean saving millions, marking a substantial economic impact.</p>
<h2>6. Limitations</h2>
<p>While our model leverages the parameter-efficient training method of prompt learning, the speed of training is still a challenge. To address this, especially as tasks become complex and the model size grows, we're considering the use of Parameter-Efficient Fine-Tuning (PEFT) to reduce the number of training parameters and enhance performance. However, for tasks like molecular captioning and generation, we face another hurdle: the absence of appropriate evaluation to measure the model's ability. This limitation makes it challenging to assess the model's effectiveness in these specific areas.</p>
<h2>7. Conclusions</h2>
<p>We introduce GIT-Mol, a specialized multi-modal large language model tailored for molecular science, overcoming significant challenges in data collection, particularly in acquiring scarce molecular captions. Additionally, we faced and overcame difficulties in training large models, employing advanced parallel training algorithms. Compared to baseline models, it demonstrates relative advantages in tasks such as molecule captioning, generation, image captioning, and property prediction. In addition, a new multi-modal data mixer, GIT-Former, is proposed for fusing multi-modal molecular data. Our model outperforms existing methods and offers an any-to-language modality translation strategy, enhancing flexibility for various applications.</p>
<p>For future directions, we plan to integrate techniques such as Low-Rank Adaptation (LoRA) and Adapters to expedite the training process. Moreover, we will explore the extent to which prompts enhance model performance. As for new downstream tasks, such as compound name recognition and chemical reaction prediction, we will enhance the model's flexibility and adaptability. These improvements are expected to significantly impact fields like molecular science and drug development, potentially leading to more efficient and insightful research.</p>
<h2>Data and Software Availability:</h2>
<p>The data and software can be accessed at https:// github.com/AI-HPC-Research-Team/GIT-Mol. It is available for non-commercial use.</p>
<h2>Funding</h2>
<p>This work is supported by grants from the National Natural Science Foundation of China (61902446, 62172456, and 91937302) and the Peng Cheng Cloud-Brain of Peng Cheng Laboratory.</p>
<h2>CRediT authorship contribution statement</h2>
<p>Pengfei Liu: Conceptualization, Methodology, Data curation, Model traning, Writing-original draft. Yiming Ren: Model traning, Writing-original draft. Jun Tao: Writing review editing. Zhixiang Ren: Conceptualization, Formal analysis, Supervision, Funding acquisition, Writing review editing.</p>
<h2>Declaration of Competing Interest</h2>
<p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
<h2>Acknowledgments</h2>
<p>The authors appreciate Yue Zhou from Peng Cheng Laboratory for the technical advice. The research is supported by the Peng Cheng Cloud-Brain of Peng Cheng Laboratory.</p>
<h2>References</h2>
<p>Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al., 2022. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, 23716-23736.
Bao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., Yue, G., Cao, Y., Su, H., Zhu, J., 2023. One transformer fits all distributions in multi-modal diffusion at scale. arXiv preprint arXiv:2303.06555 .
Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O.K., Aggarwal, K., Som, S., Piao, S., Wei, F., 2022. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. Advances in Neural Information Processing Systems 35, 32897-32912.
Beltagy, I., Lo, K., Cohan, A., 2019. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676 .
Bento, A.P., Hersey, A., Félix, E., Landrum, G., Gaulton, A., Atkinson, F., Bellis, L.J., De Veij, M., Leach, A.R., 2020. An open source chemical structure curation pipeline using rdkit. Journal of Cheminformatics , $1-16$.
Bilodeau, C., Jin, W., Jaakkola, T., Barzilay, R., Jensen, K.F., 2022. Generative models for molecular discovery: Recent advances and challenges. Wiley Interdisciplinary Reviews: Computational Molecular Science 12, e1608.
Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 .
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al., 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 .
Edwards, C., Lai, T., Ros, K., Honke, G., Cho, K., Ji, H., 2022. Translation between molecules and natural language, in: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. $375-413$.
Edwards, C., Zhai, C., Ji, H., 2021. Text2mol: Cross-modal molecule retrieval with natural language queries, in: Proceedings of the 2021</p>
<p>Conference on Empirical Methods in Natural Language Processing, pp. $595-607$.
Filippov, I.V., Nicklaus, M.C., 2009. Optical structure recognition software to recover chemical information: Osra, an open source solution.
Floridi, L., Chiriatti, M., 2020. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines 30, 681-694.
Hastings, J., Owen, G., Dekker, A., Emtis, M., Kale, N., Muthukrishnan, V., Turner, S., Swainston, N., Mendes, P., Steinbeck, C., 2016. Chebi in 2016: Improved services and an expanding collection of metabolites. Nucleic Acids Research 44, D1214-D1219.
He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778.
Hou, Z., Liu, X., Cen, Y., Dong, Y., Yang, H., Wang, C., Tang, J., 2022. Graphmae: Self-supervised masked graph autoencoders, in: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 594-604.
Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Liu, Q., et al., 2023. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045 .
Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T., 2021. Scaling up visual and vision-language representation learning with noisy text supervision, in: International Conference on Machine Learning, PMLR. pp. 4904-4916.
Kim, S., Chen, J., Cheng, T., Gindulyte, A., He, J., He, S., Li, Q., Shoemaker, B.A., Thiessen, P.A., Yu, B., et al., 2019. Pubchem 2019 update: improved access to chemical data. Nucleic Acids Research 47, D1102-D1109.
Li, J., Li, D., Savarese, S., Hoi, S., 2023a. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 .
Li, J., Liu, Y., Fan, W., Wei, X.Y., Liu, H., Tang, J., Li, Q., 2023b. Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective. arXiv preprint arXiv:2306.06615 .
Li, Y., Gao, C., Song, X., Wang, X., Xu, Y., Han, S., 2023c. Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins. bioRxiv , 2023-06.
Liu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L., Tang, J., Xiao, C., Anandkumar, A., 2022. Multi-modal molecule structure-text model for text-based retrieval and editing. arXiv preprint arXiv:2212.10789 .
Liu, S., Wang, H., Liu, W., Lasenby, J., Guo, H., Tang, J., 2021a. Pretraining molecular graph representation with 3d geometry. arXiv preprint arXiv:2110.07728 .
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B., 2021b. Swin transformer: Hierarchical vision transformer using shifted windows, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012-10022.
Luo, R., Sun, L., Xia, Y., Qin, T., Zhang, S., Poon, H., Liu, T.Y., 2022. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics 23, bbac409.
OpenAI, 2023. GPT-4 technical report. CoRR abs/2303.08774.
Peryea, T., Katzel, D., Zhao, T., Southall, N., Nguyen, D.T., 2019. Molvec: Open source library for chemical structure recognition, in: Abstracts of Papers of the American Chemical Society, Amer Chemical Soc 1155 16TH ST, NW, WASHINGTON, DC 20036 USA.
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al., 2021. Learning transferable visual models from natural language supervision, in: International Conference on Machine Learning, PMLR. pp. 8748-8763.
Radford, A., Wu, J., Child, R., Luun, D., Amodei, D., Sutskever, I., et al., 2019. Language models are unsupervised multitask learners. OpenAI blog 1,9 .
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J., 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 5485-5551.</p>
<p>Rajan, K., Zielesny, A., Steinbeck, C., 2021. Decimer 1.0: deep learning for chemical image recognition using transformers. Journal of Cheminformatics 13, 1-16.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M., 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 .
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A., BarthMaron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.T., et al., 2022. A generalist agent. arXiv preprint arXiv:2205.06175 .
Rodrigues, T., Reker, D., Schneider, P., Schneider, G., 2016. Counting on natural products for drug design. Nature Chemistry 8, 531-541.
Stefanini, M., Cornia, M., Baraldi, L., Casciunelli, S., Fiameni, G., Cucchiara, R., 2022. From show to tell: A survey on deep learning-based image captioning. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 539-559.
Su, B., Du, D., Yang, Z., Zhou, Y., Li, J., Rao, A., Sun, H., Lu, Z., Wen, J.R., 2022. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481 .
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Advances in neural information processing systems 30 .
Wang, H., Ma, S., Huang, S., Dong, L., Wang, W., Peng, Z., Wu, Y., Bajaj, P., Singhal, S., Benhaim, A., et al., 2022a. Foundation transformers. arXiv preprint arXiv:2210.06423 .
Wang, J., Shen, Z., Liao, Y., Yuan, Z., Li, S., He, G., Lan, M., Qian, X., Zhang, K., Li, H., 2022b. Multi-modal chemical information reconstruction from images and texts for exploring the near-drug space. Briefings in Bioinformatics 23, bbac461.
Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O.K., Singhal, S., Som, S., et al., 2022c. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442 .
Wang, Y., Wang, J., Cao, Z., Barati Farimani, A., 2022d. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence 4, 279-287.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al., 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824-24837.
Weininger, D., 1988. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of Chemical Information and Computer Sciences 28, 31-36.
Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N., 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671.
Wu, Z., Ramsundar, B., Feinberg, E.N., Gomes, J., Geniesse, C., Pappu, A.S., Leswing, K., Pande, V., 2018. Moleculenet: a benchmark for molecular machine learning. Chemical Science 9, 513-530.
Xu, Z., Li, J., Yang, Z., Li, S., Li, H., 2022. Swinocsr: end-to-end optical chemical structure recognition using a swin transformer. Journal of Cheminformatics 14, 1-13.
Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Shao, Y., Zhang, W., Cui, B., Yang, M.H., 2022. Diffusion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796 .
Zeng, Z., Yao, Y., Liu, Z., Sun, M., 2022. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature Communications 13, 862.
Zhou, G., Gao, Z., Ding, Q., Zheng, H., Xu, H., Wei, Z., Zhang, L., Ke, G., 2023. Uni-mol: A universal 3d molecular representation learning framework, in: The Eleventh International Conference on Learning Representations. URL: https://openreview.net/ forum11d=6K28096wVqKu.
Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M., 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\text {a }}$ Corresponding author at: Peng Cheng Laboratory, Shenzhen, 518055, China
${ }^{\mathrm{m}}$ E-mail addresses: renzhx@pcl.ac.cn&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
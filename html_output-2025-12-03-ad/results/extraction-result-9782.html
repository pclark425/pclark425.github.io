<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9782 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9782</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9782</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-c7face35e84f2cb04fb1600d54298799aa0ed189</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c7face35e84f2cb04fb1600d54298799aa0ed189" target="_blank">ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps is introduced, and it is found that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.</p>
                <p><strong>Paper Abstract:</strong> When conducting literature reviews, scientists often create literature review tables—tables whose rows are publications and whose columns constitute a schema, a set of aspects used to compare and contrast the papers. Can we automatically generate these tables using language models (LMs)? In this work, we introduce a framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps. To enable experimentation, we address two main challenges: First, we overcome a lack of high-quality datasets to benchmark table generation by curating and releasing arxivDIGESTables, a new dataset of 2,228 literature review tables extracted from ArXiv papers that synthesize a total of 7,542 research papers. Second, to support scalable evaluation of model generations against human-authored reference tables, we develop DecontextEval, an automatic evaluation method that aligns elements of tables with the same underlying aspects despite differing surface forms. Given these tools, we evaluate LMs’ abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references). Finally, through a human evaluation study we find that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9782.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9782.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARXIVDIGESTables-SchemaSynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literature-review Schema and Value Synthesis via Language Models (ARXIVDIGESTABLES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses large language models to automatically synthesize literature-review tables from sets of scholarly papers by (1) generating a schema (shared aspects/columns) that generalizes across input papers and (2) extracting cell values for each paper-aspect pair from full text. The approach decomposes the task into schema-generation and value-generation, explores multiple context conditions, and evaluates using automatic and human metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5-Turbo, Mixtral 8x22, GPT-4-Turbo (for definitions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI, 2022) and Mixtral 8x22 (Mistral AI, 2024) are used as the primary generation engines for schema and value generation; GPT-4-Turbo is used specifically to generate column descriptions/definitions in the value-generation pipeline. Models are used via prompting (no reported fine-tuning) with decomposed prompting strategies to produce schemas and extractive-style answers.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scientific literature synthesis (primarily computer science/ArXiv papers)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>ARXIVDIGESTABLES: 2,228 curated literature-review tables scraped from ArXiv papers (April 2007–Nov 2023) that compare 7,542 unique papers; each table is linked to input papers' full text, table captions, and in-text reference sentences. Initial extraction began from ~2.5M tables from ~800k LaTeX-source ArXiv papers and was filtered heavily (character length limits, presence of citations, exclusion of numeric/experimental-result columns, full-text availability), yielding the high-quality set.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Shared comparative aspects / generalizable comparison dimensions (i.e., schema elements that summarize recurring properties across papers)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Example synthesized aspects include high-level comparison columns such as 'Task / Intended Application', 'Evaluation metric', 'Annotation method', or more specific aspects like 'Maximum resolution' or 'Training batch size' — these are generalizable comparison rules/attributes distilled across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Decomposed prompting: (1) Schema generation: prompt LMs with titles+abstracts (and optionally generated/gold captions, in-text references, or few-shot example tables) to output N column aspects; (2) Value generation: for each (paper, aspect) generate a concise query/definition (GPT-4-Turbo used to produce column definitions when context is provided), then prompt an LM (GPT-3.5-Turbo used for value extraction) over the paper full text to extract the cell value; retry policy with up to 4 reformulated queries reduces empty outputs from ~30% to ~7.5%. Batch splitting and fallback retries handle context-window and formatting errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic and human evaluation: (a) Schema alignment via DECONTEXTEVAL — featurizer that decontextualizes column names using an LM and a sentence-transformer scorer (all-MiniLM-L6-v2) with thresholding; (b) alternative scorers tested including exact match, Jaccard, sentence-transformers, and prompting Llama 3 (70B) as a scorer; (c) value evaluation using reference schemas to isolate value accuracy and same scorers; (d) human evaluation: Likert judgments (usefulness, specificity, insightfulness) on generated aspects and categorical judgments (complete/partial/none) on generated values, with inter-annotator agreement reported (~0.56 Krippendorff's alpha for aspects; ~0.55 Cohen's kappa for values).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLMs can synthesize useful, generalizable schema aspects and extract values but are far from perfect. Providing more context (table captions, in-text references, or few-shot examples) increases schema-reconstruction recall consistently across models. Novel generated aspects that do not match gold schemas are rated by humans as comparably useful, specific, and sometimes more useful than matched aspects. Automated schema aligner DecontextEval (decontext featurizer + sentence-transformer scorer, threshold 0.7) yields a practical operating point (reported precision ~70–85% on calibration sets). Using Llama 3 as a direct scorer resulted in high recall but low precision (human-verified precision ~37–55%), i.e., hallucinated matches. Value extraction performance remains low-moderate: human annotations across settings show e.g. (Column Names only) 21.13% complete / 22.54% partial / 56.34% none, while adding captions + in-text references improves partial matches (e.g. captions+IT-refs: 22.65% complete / 31.77% partial / 45.59% none). Models sometimes produce overly specific or mis-granular aspects and make errors in complex aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>The paper contrasts (a) joint generation (single LM call to produce schema+values) vs (b) decomposed generation. Decomposed generation (schema then values) was preferred for reducing hallucination and handling context-window limits. Additional context conditions (gold caption, caption+in-text refs, few-shot examples) outperform low-context baselines on schema reconstruction recall. DecontextEval (LM-based decontextualization then sentence-transformer scoring) outperforms naive exact-match and is more precise than direct LLM-based alignment (Llama 3) which had higher recall but substantially lower precision.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Scope limited to ArXiv papers (majority computer science); generated schemas often do not exactly match human-authored schemas — reconstruction is imperfect. Automated evaluation remains challenging due to lexical variability; DecontextEval improves but is not perfect. Value extraction can produce empty/incorrect values (~7.5% empty after retries) and many matches require inference beyond lexical overlap. Context helps schema reconstruction but does not fully solve the problem. The dataset and evaluation focus exclude numerical/experimental-result columns by design.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Observed hallucination and low-precision behavior when using LLMs as scorers (Llama 3 produced hallucinated column matches). Ethical discussion notes LLM-generated tables can misrepresent authors' work, produce hallucinatory or inaccurate cell values, and potentially discourage consulting original sources. The authors also note possible contamination as models may have seen older tables during training, which could inflate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9782.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9782.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DecontextEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DECONTEXTEVAL: LM-assisted Schema Alignment with Sentence-Transformer Scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation framework developed in this paper that uses a language model to 'decontextualize' column headers (expand terse header into a grounded description) and then uses a sentence-transformer similarity scorer to align generated and reference table aspects, improving precision over naive LM-only alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Mistral-8x7B-Instruct-v0.1 (used for decontextualization prompts); sentence-transformer all-MiniLM-L6-v2 used for scoring (not an LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Decontextualization is performed by prompting a smaller Mistral instruct model (Mistral-8x7B-Instruct-v0.1) to generate stand-alone descriptions of column names plus their column values; the generated descriptions are then encoded by a sentence-transformer for cosine-similarity scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Automated evaluation / schema alignment for literature-review table synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Calibration and evaluation on held-out subset (~25% used for calibration) of the ARXIVDIGESTABLES dataset (2,228 high-quality tables) and additional manual checks on ~50 tables for precision estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Interpretability-grounded alignment rules (i.e., whether two aspect headers convey the same comparative concept across papers)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Decontextualization turns terse headers like 'VQA' into stand-alone descriptions such as 'video quality assessment task', enabling matching with generated headers like 'Intended Application' when both decontextualized descriptions are semantically similar.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Featurizer options: 'name' (raw header), 'values' (concatenate column values), and 'decontext' (prompt LM to create a stand-alone description from header+values). Scorer options included exact match, Jaccard, sentence-transformers (all-MiniLM-L6-v2), and Llama 3 prompting; DECONTEXTEVAL selected the 'decontext' featurizer with sentence-transformer scoring and threshold t=0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Calibrated on 25% of ARXIVDIGESTABLES to select operating point; human evaluation on ~50 tables to estimate precision of different aligners and validate thresholds. Measured schema recall across models and context conditions and reported precision estimates for aligners.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DECONTEXTEVAL (decontext + sentence-transformer, t=0.7) provided a sound precision-recall tradeoff and was selected as the primary automatic metric. Reported precision for DECONTEXTEVAL was approximately 70–85% on calibration sets. In contrast, Llama 3-based aligner produced high recall but low precision (37–55% precision on predicted matches). DECONTEXTEVAL enabled more reliable automated assessment of schema reconstruction than direct LM-only alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared against exact-match (very low recall), Jaccard, raw sentence-transformer on names/values, and Llama 3 prompting. DECONTEXTEVAL outperforms exact match and naive name-based scorers and achieves substantially higher precision than direct LLM alignment (Llama 3) while maintaining acceptable yield.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>DECONTEXTEVAL is not perfect; precision is 70–85% (not near 100%), meaning some alignments remain incorrect. It relies on quality of LM decontextualization and sentence-transformer embeddings; featurizer and threshold choice affect recall/precision trade-offs. The method still requires human validation for high-stakes use.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>LM decontextualization can introduce spurious or overly generic descriptions; LLM-based aligners (e.g., Llama 3) were observed to hallucinate many false matches, motivating the combined approach. The authors caution about relying solely on LLM judgments for alignment due to hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model <em>(Rating: 2)</em></li>
                <li>Automatic generation of review matrices as multi-document summarization of scientific papers <em>(Rating: 2)</em></li>
                <li>Metro maps of science <em>(Rating: 2)</em></li>
                <li>A question answering framework for decontextualizing user-facing snippets from scientific documents <em>(Rating: 2)</em></li>
                <li>Decontextualization: Making sentences stand-alone <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9782",
    "paper_id": "paper-c7face35e84f2cb04fb1600d54298799aa0ed189",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "ARXIVDIGESTables-SchemaSynthesis",
            "name_full": "Literature-review Schema and Value Synthesis via Language Models (ARXIVDIGESTABLES)",
            "brief_description": "This paper uses large language models to automatically synthesize literature-review tables from sets of scholarly papers by (1) generating a schema (shared aspects/columns) that generalizes across input papers and (2) extracting cell values for each paper-aspect pair from full text. The approach decomposes the task into schema-generation and value-generation, explores multiple context conditions, and evaluates using automatic and human metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "GPT-3.5-Turbo, Mixtral 8x22, GPT-4-Turbo (for definitions)",
            "llm_model_description": "GPT-3.5-Turbo (OpenAI, 2022) and Mixtral 8x22 (Mistral AI, 2024) are used as the primary generation engines for schema and value generation; GPT-4-Turbo is used specifically to generate column descriptions/definitions in the value-generation pipeline. Models are used via prompting (no reported fine-tuning) with decomposed prompting strategies to produce schemas and extractive-style answers.",
            "application_domain": "Scientific literature synthesis (primarily computer science/ArXiv papers)",
            "input_corpus_description": "ARXIVDIGESTABLES: 2,228 curated literature-review tables scraped from ArXiv papers (April 2007–Nov 2023) that compare 7,542 unique papers; each table is linked to input papers' full text, table captions, and in-text reference sentences. Initial extraction began from ~2.5M tables from ~800k LaTeX-source ArXiv papers and was filtered heavily (character length limits, presence of citations, exclusion of numeric/experimental-result columns, full-text availability), yielding the high-quality set.",
            "qualitative_law_type": "Shared comparative aspects / generalizable comparison dimensions (i.e., schema elements that summarize recurring properties across papers)",
            "qualitative_law_example": "Example synthesized aspects include high-level comparison columns such as 'Task / Intended Application', 'Evaluation metric', 'Annotation method', or more specific aspects like 'Maximum resolution' or 'Training batch size' — these are generalizable comparison rules/attributes distilled across papers.",
            "extraction_methodology": "Decomposed prompting: (1) Schema generation: prompt LMs with titles+abstracts (and optionally generated/gold captions, in-text references, or few-shot example tables) to output N column aspects; (2) Value generation: for each (paper, aspect) generate a concise query/definition (GPT-4-Turbo used to produce column definitions when context is provided), then prompt an LM (GPT-3.5-Turbo used for value extraction) over the paper full text to extract the cell value; retry policy with up to 4 reformulated queries reduces empty outputs from ~30% to ~7.5%. Batch splitting and fallback retries handle context-window and formatting errors.",
            "evaluation_method": "Automatic and human evaluation: (a) Schema alignment via DECONTEXTEVAL — featurizer that decontextualizes column names using an LM and a sentence-transformer scorer (all-MiniLM-L6-v2) with thresholding; (b) alternative scorers tested including exact match, Jaccard, sentence-transformers, and prompting Llama 3 (70B) as a scorer; (c) value evaluation using reference schemas to isolate value accuracy and same scorers; (d) human evaluation: Likert judgments (usefulness, specificity, insightfulness) on generated aspects and categorical judgments (complete/partial/none) on generated values, with inter-annotator agreement reported (~0.56 Krippendorff's alpha for aspects; ~0.55 Cohen's kappa for values).",
            "results_summary": "LLMs can synthesize useful, generalizable schema aspects and extract values but are far from perfect. Providing more context (table captions, in-text references, or few-shot examples) increases schema-reconstruction recall consistently across models. Novel generated aspects that do not match gold schemas are rated by humans as comparably useful, specific, and sometimes more useful than matched aspects. Automated schema aligner DecontextEval (decontext featurizer + sentence-transformer scorer, threshold 0.7) yields a practical operating point (reported precision ~70–85% on calibration sets). Using Llama 3 as a direct scorer resulted in high recall but low precision (human-verified precision ~37–55%), i.e., hallucinated matches. Value extraction performance remains low-moderate: human annotations across settings show e.g. (Column Names only) 21.13% complete / 22.54% partial / 56.34% none, while adding captions + in-text references improves partial matches (e.g. captions+IT-refs: 22.65% complete / 31.77% partial / 45.59% none). Models sometimes produce overly specific or mis-granular aspects and make errors in complex aspects.",
            "comparison_to_baseline": "The paper contrasts (a) joint generation (single LM call to produce schema+values) vs (b) decomposed generation. Decomposed generation (schema then values) was preferred for reducing hallucination and handling context-window limits. Additional context conditions (gold caption, caption+in-text refs, few-shot examples) outperform low-context baselines on schema reconstruction recall. DecontextEval (LM-based decontextualization then sentence-transformer scoring) outperforms naive exact-match and is more precise than direct LLM-based alignment (Llama 3) which had higher recall but substantially lower precision.",
            "reported_limitations": "Scope limited to ArXiv papers (majority computer science); generated schemas often do not exactly match human-authored schemas — reconstruction is imperfect. Automated evaluation remains challenging due to lexical variability; DecontextEval improves but is not perfect. Value extraction can produce empty/incorrect values (~7.5% empty after retries) and many matches require inference beyond lexical overlap. Context helps schema reconstruction but does not fully solve the problem. The dataset and evaluation focus exclude numerical/experimental-result columns by design.",
            "bias_or_hallucination_issues": "Observed hallucination and low-precision behavior when using LLMs as scorers (Llama 3 produced hallucinated column matches). Ethical discussion notes LLM-generated tables can misrepresent authors' work, produce hallucinatory or inaccurate cell values, and potentially discourage consulting original sources. The authors also note possible contamination as models may have seen older tables during training, which could inflate performance.",
            "uuid": "e9782.0",
            "source_info": {
                "paper_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DecontextEval",
            "name_full": "DECONTEXTEVAL: LM-assisted Schema Alignment with Sentence-Transformer Scoring",
            "brief_description": "An automatic evaluation framework developed in this paper that uses a language model to 'decontextualize' column headers (expand terse header into a grounded description) and then uses a sentence-transformer similarity scorer to align generated and reference table aspects, improving precision over naive LM-only alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Mistral-8x7B-Instruct-v0.1 (used for decontextualization prompts); sentence-transformer all-MiniLM-L6-v2 used for scoring (not an LLM)",
            "llm_model_description": "Decontextualization is performed by prompting a smaller Mistral instruct model (Mistral-8x7B-Instruct-v0.1) to generate stand-alone descriptions of column names plus their column values; the generated descriptions are then encoded by a sentence-transformer for cosine-similarity scoring.",
            "application_domain": "Automated evaluation / schema alignment for literature-review table synthesis",
            "input_corpus_description": "Calibration and evaluation on held-out subset (~25% used for calibration) of the ARXIVDIGESTABLES dataset (2,228 high-quality tables) and additional manual checks on ~50 tables for precision estimates.",
            "qualitative_law_type": "Interpretability-grounded alignment rules (i.e., whether two aspect headers convey the same comparative concept across papers)",
            "qualitative_law_example": "Decontextualization turns terse headers like 'VQA' into stand-alone descriptions such as 'video quality assessment task', enabling matching with generated headers like 'Intended Application' when both decontextualized descriptions are semantically similar.",
            "extraction_methodology": "Featurizer options: 'name' (raw header), 'values' (concatenate column values), and 'decontext' (prompt LM to create a stand-alone description from header+values). Scorer options included exact match, Jaccard, sentence-transformers (all-MiniLM-L6-v2), and Llama 3 prompting; DECONTEXTEVAL selected the 'decontext' featurizer with sentence-transformer scoring and threshold t=0.7.",
            "evaluation_method": "Calibrated on 25% of ARXIVDIGESTABLES to select operating point; human evaluation on ~50 tables to estimate precision of different aligners and validate thresholds. Measured schema recall across models and context conditions and reported precision estimates for aligners.",
            "results_summary": "DECONTEXTEVAL (decontext + sentence-transformer, t=0.7) provided a sound precision-recall tradeoff and was selected as the primary automatic metric. Reported precision for DECONTEXTEVAL was approximately 70–85% on calibration sets. In contrast, Llama 3-based aligner produced high recall but low precision (37–55% precision on predicted matches). DECONTEXTEVAL enabled more reliable automated assessment of schema reconstruction than direct LM-only alignment.",
            "comparison_to_baseline": "Compared against exact-match (very low recall), Jaccard, raw sentence-transformer on names/values, and Llama 3 prompting. DECONTEXTEVAL outperforms exact match and naive name-based scorers and achieves substantially higher precision than direct LLM alignment (Llama 3) while maintaining acceptable yield.",
            "reported_limitations": "DECONTEXTEVAL is not perfect; precision is 70–85% (not near 100%), meaning some alignments remain incorrect. It relies on quality of LM decontextualization and sentence-transformer embeddings; featurizer and threshold choice affect recall/precision trade-offs. The method still requires human validation for high-stakes use.",
            "bias_or_hallucination_issues": "LM decontextualization can introduce spurious or overly generic descriptions; LLM-based aligners (e.g., Llama 3) were observed to hallucinate many false matches, motivating the combined approach. The authors caution about relying solely on LLM judgments for alignment due to hallucination risk.",
            "uuid": "e9782.1",
            "source_info": {
                "paper_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model",
            "rating": 2
        },
        {
            "paper_title": "Automatic generation of review matrices as multi-document summarization of scientific papers",
            "rating": 2
        },
        {
            "paper_title": "Metro maps of science",
            "rating": 2
        },
        {
            "paper_title": "A question answering framework for decontextualizing user-facing snippets from scientific documents",
            "rating": 2
        },
        {
            "paper_title": "Decontextualization: Making sentences stand-alone",
            "rating": 1
        }
    ],
    "cost": 0.013573749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ARXIVDIGESTABLES: Synthesizing Scientific Literature into Tables using Language Models</h1>
<p>Benjamin Newman ${ }^{\text {a }}$ Yoonjoo Lee ${ }^{\circ *}$<br>Aakanksha Naik ${ }^{\circ}$ Pao Siangliulue ${ }^{\circ}$ Raymond Fok ${ }^{\text {a }}$<br>Juho Kim ${ }^{\circ}$ Daniel S. Weld ${ }^{\text {a }}$ Joseph Chee Chang ${ }^{\circ}$ Kyle Lo ${ }^{\circ}$<br>${ }^{\text {a }}$ University of Washington ${ }^{\circ}$ KAIST ${ }^{\circ}$ Allen Institute for AI<br>blnewman@cs.washington.edu, yoonjoo.lee@kaist.ac.kr</p>
<h4>Abstract</h4>
<p>When conducting literature reviews, scientists often create literature review tables tables whose rows are publications and whose columns constitute a schema, a set of aspects used to compare and contrast the papers. Can we automatically generate these tables using language models (LMs)? In this work, we introduce a framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps. To enable experimentation, we address two main challenges: First, we overcome a lack of high-quality datasets to benchmark table generation by curating and releasing $\square$ ARXIVDIGESTABLES, a new dataset of 2,228 literature review tables extracted from ArXiv papers that synthesize a total of 7,542 research papers. Second, to support scalable evaluation of model generations against humanauthored reference tables, we develop DECONTEXTÉVAL, an automatic evaluation method that aligns elements of tables with the same underlying aspects despite differing surface forms. Given these tools, we evaluate LMs' abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references). Finally, through a human evaluation study we find that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.</p>
<p>○ blnewman/arxivDIGESTables
C bnewm0609/arxivDIGESTables</p>
<h2>1 Introduction</h2>
<p>Conducting literature reviews by reading and synthesizing information across a large set of documents is vital for scientists to stay abreast of their fields yet is increasingly laborious as the number of scientific publications grows exponentially (Jinha, 2010; Bornmann et al., 2021). At the core of this</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic of our literature review table generation task: (1) synthesize multiple input papers into a table with both (2) a schema (columns) and (3) values. Each row corresponds to an input paper.
sensemaking process is identifying a schema, a set of important aspects that are useful for comparing and contrasting prior literature (Russell et al., 1993). The results of this process are often presented in the form of literature review tables, whose rows are a set of papers and whose columns are a set of aspects that the papers share (Figure 1).</p>
<p>In this work, we conceptualize the task of literature review table generation by decomposing it into two sub-tasks: (1) Schema-generation: Determining a set of relevant shared aspects given a set of input papers, and (2) Value-generation: Determining the value given an aspect and a paper. For example, a table for a set of computer vision papers on video datasets (rows) might have a schema with aspects like "task" or "size" (columns); cell values under the "task" column may say "VQA" or "classification" (values).</p>
<p>Prior work has largely investigated each of the two sub-tasks independently. In particular, the large body of literature on document-grounded question-answering (Kwiatkowski et al., 2019; Dasigi et al., 2021; Lee et al., 2023), information extraction (Luan et al., 2018), and query (Zhong et al., 2021; Xu and Lapata, 2020) or aspect-based summarization (Yang et al., 2023; Ahuja et al.,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Side-by-side comparison of a reference literature review table from an ArXiv paper and a model-generated table given the same input papers. The generated table has reconstructed two gold aspects: the pink and blue aspects are the same, despite surface form differences (e.g., "Task" vs "Intended Application"). The generated table has also proposed two novel aspects that are still relevant and useful, like "evaluation metric" (green) or "Annotation method" (yellow) not to be confused with reference table's "Annotations".</p>
<p>2022) advances methods that are also suitable for generating values conditioned on an aspect. In our example above, values for aspect "size" can be answers to questions like "How many videos are in this dataset?".</p>
<p>In contrast, schema generation from a set of documents remains relatively under-explored, even though it is a crucial and effortful part of the manual literature review process. Prior work like Zhang and Balog (2018) infers new schemas from pre-existing ones, while recent work like Wang et al. (2024) assumes users can clearly articulate a schema in a short natural language query to infer aspects directly. This paper studies the use of language models for literature review table generation with a focus on unifying these two sub-tasks. This presents us with two research challenges:</p>
<p>First, we note a lack of large-scale, high-quality datasets of literature review tables to serve as a benchmark for this task. Second, similar to challenges faced in summarization and other grounded generation tasks, semantically similar content can be expressed with different surface forms, which makes automatic evaluation difficult even with a high-quality dataset. An example of these surface form differences is in Figure 2. To address these challenges:</p>
<ul>
<li>
<p>In §2, we curate and release ARXIVDIGESTABLES,<sup>1</sup> a dataset of 2,228 high-quality literature review tables scraped and filtered from 16 years of ArXiv papers uploaded between April 2007 and November 2023. These tables compare and contrast a total of 7,542 unique papers using a total of 7,634 columns and 43,905 values. This is the result of extensive filtering on an initial set of around 2.5 million extracted tables to ensure high quality, based on a strict set of desiderata. Finally, we link every table to rich paper content: (1) every input paper (row) has corresponding full text document, and (2) every table has its caption and in-line textual references extracted from the table's source paper for contextual information.</p>
</li>
<li>
<p>In §5, we present DECONTEXTEVAL, an au-</p>
</li>
</ul>
<p><sup>1</sup>DIGESTables stands for Document Information Gathering and Extraction for Scientific Tables</p>
<p>tomatic evaluation framework for comparing model-generated and human-authored tables. Our approach overcomes the difficulty in matching semantically-similar but lexicallydifferent column names by using a language model to expand column names into descriptions grounded in documents. Combining with a small textual similarity model results in a matcher that is nearly twice more precise than prompting Llama 3 (70B), which often hallucinates matches.</p>
<p>We formalize the literature review table generation task (§3) and introduce our framework for literature review table generation and detail our implementations using open and closed models (§4).</p>
<p>Finally in $\S 6$, we evaluate LMs on this generation task, addressing two key questions: (1) what contextual information is needed to steer language models to reconstruct human-authored schemas? and (2) are generated aspects that don't match gold still useful? For (1), we find that language models have higher recall by conditioning on more context that specifies the purpose of the table (e.g., captions, in-line references, other example tables). For (2), we find that novel aspects not in the reference tables can still be of comparable usefulness, specificity, and insightfulness.</p>
<h2>2 Creating $\square$ ARXIVDIGESTABLES</h2>
<p>Desiderata To enable research in synthesizing literature review tables, we first collect and curate a set of reference tables to ground our task and enable evaluation. To ensure this data is realistic, high-quality, and focused on supporting literature review, we decide on the following desiderata for including tables in our $\square$ ARXIVDIGESTABLES dataset:</p>
<ol>
<li>Tables should be ecologically valid-reflecting real syntheses authored by researchers rather than artificial annotation;</li>
<li>Tables should be focused on summarizing multiple aspects of a set of papers as opposed to tables for reporting empirical results;</li>
<li>Tables should follow a common structure where each row represents a single document and each column represents a specific aspect.
Based on these goals, we used the procedure below to construct $\square$ ARXIVDIGESTABLES:</li>
</ol>
<p>Data Source To ensure our task and benchmark are grounded in realistic cases, we collected a
dataset real-world literature review tables from open access ArXiv papers from April 2007 until November 2023. We subsequently filter these tables down to a high-quality set of 2,228 tables that meet our desiderata, as seen in Figure 3.</p>
<p>Extracting Tables The first step in our data collection pipeline is to extract the tables from papers published on the ArXiv preprint server. To start, we consider approximately 800,000 papers that have LaTeX source available. We then use unarXive (Saier et al., 2023) to convert the ArXiv source into XML. From these XML documents, we extract $\sim 2.5$ million tables.</p>
<p>Filtering Tables As a first filtering pass, we remove tables that are likely to be misparsed or unusable, filtering those with fewer than 400 or more than 15,000 characters. We also remove tables that have no table cell tags within them. Toward Desiderata 3, we filter out tables that have fewer than two citations, two rows, or two columns. We also remove any tables that have citations in more than one column, as these are often tables where papers are values rather than rows. This leaves approximately 211,000 tables.</p>
<p>Matching Rows to Papers We use heuristics to convert XML-formatted tables into JSON objects that allow us to directly index the tables by paper and aspect (See §A. 1 for details). At this stage, the citation information is usually contained within a cell in a table. For instance, an example cell with the header "Model" might have the value "BERT (Devlin et al., 2019)". We extract the citations from these cells and place them in their own column called "References". Rows without citations are assumed to refer to the source paper containing the table. After this step in the process we remove any tables where the algorithm failed and any tables that now have fewer than two rows, leaving 47,876 tables.</p>
<p>Obtaining Table Citation Metadata unarXive (Saier et al., 2023) helpfully links each citation in the table to a bibliography item. We use endpoints from the Semantic Scholar API (Kinney et al., 2023) to obtain titles and abstracts. This occasionally fails for various reasons (e.g., the bibliography text is missing information, the paper is missing from or could not be found in the Semantic Scholar database). We filter out any tables that have fewer than two matched citations, leaving us with 44,617 tables.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Pipeline for curating $\square$ ARXIVDIGESTABLES involves extensive data cleaning and filtering. The full pipeline filters from 2.5 million starting tables published in 800,000 papers to 2,228 tables published in 1,723 papers. Data pipeline described in §2.</p>
<p>Grounding to Paper Texts To meet Desiderata 2, we want to ensure that the information in the table actually comes from the cited paper. For instance, a common type of table reports experimental results whose values require actual experimentation and cannot be derived from the input papers' text alone. To filter such columns, we remove any that have math symbols or floating point numbers. Additionally, to make sure the generation task is tractable, we remove any rows whose papers do not have publicly-available full texts.</p>
<p>Final Filter and Manual Verification The last step applies a set of stringent filters and manually identifies and corrects any parsing errors (Details in §A.2). Finally, we produce a set of 2,228 highquality tables. (See Appendix §A. 5 for a sample instance.)</p>
<p>Dataset Statistics We present summary statistics in Table 1 of our high-quality set of $\square$ ARXIVDIGESTABLES. ${ }^{2}$ We are also interested in the types of aspects represented in the tables, the topics of the columns, and the fields the tables come from. To categorize the table aspects, we use simple heuristics (Table 2). We find $\sim 40 \%$ of the columns are categorical or boolean, which are more suitable for supporting inter-paper comparisons, while the other $\sim 60 \%$ are more descriptive. To obtain column topics, we manually annotate columns in $\sim 50$ tables- $\sim 38 \%$ are about datasets, $\sim 20 \%$ are about methods, and the rest are on other</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>topics such as applications or tasks. Finally, we use the ArXiv API to obtain which archive a table's paper was submitted to. We find a majority $(1,985)$ of the tables come from computer science publications, with others coming from Physics, Quantitative Biology, Statistics, Math, and other fields (See Appendix A.4).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Min</th>
<th style="text-align: center;">Max</th>
<th style="text-align: center;">Median</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Papers</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">4.944</td>
<td style="text-align: center;">11016</td>
</tr>
<tr>
<td style="text-align: left;">Aspects</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">3.426</td>
<td style="text-align: center;">7634</td>
</tr>
</tbody>
</table>
<p>Table 1: Number of papers (rows) and aspects (columns) in $\square$ ARXIVDIGESTABLES. Of the 11,0016 total rows there are 7,542 unique papers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Aspect Type</th>
<th style="text-align: center;">\% of Cols</th>
<th style="text-align: center;">Example Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Category</td>
<td style="text-align: center;">$35.5 \%$</td>
<td style="text-align: center;">"Open" vs "Proprietary"</td>
</tr>
<tr>
<td style="text-align: left;">Entity</td>
<td style="text-align: center;">$27.3 \%$</td>
<td style="text-align: center;">"CNN/Daily Mail", "Reddit"</td>
</tr>
<tr>
<td style="text-align: left;">Numeric</td>
<td style="text-align: center;">$21.7 \%$</td>
<td style="text-align: center;">" 10,000 "</td>
</tr>
<tr>
<td style="text-align: left;">Text</td>
<td style="text-align: center;">$9.7 \%$</td>
<td style="text-align: center;">" $\ldots$ collected via various $\ldots$ "</td>
</tr>
<tr>
<td style="text-align: left;">Boolean</td>
<td style="text-align: center;">$5.8 \%$</td>
<td style="text-align: center;">" $\checkmark$ " vs " $\boldsymbol{\Omega}$ "</td>
</tr>
</tbody>
</table>
<p>Table 2: Types of aspects in $\square$ ARXIVDIGESTABLES's columns.</p>
<h2>3 Literature Review Table Generation</h2>
<p>Equipped with our dataset, we formalize the task of generating literature review tables.</p>
<p>Task Definition We define our table generation task as follows: Given an input set of $M$ documents $d_{1}, \ldots, d_{M}$, generate a table with $M$ rows and any number of columns $N \geq 2$. Each row $r_{1}, \ldots, r_{M}$ corresponds to a unique input document. Each column $c_{1}, \ldots, c_{N}$ represents a unique aspect. Taken together, the columns constitute a schema. The</p>
<p>table then has $N \times M$ values, with one value in each cell. ${ }^{3}$ The cell values should be derived from the input documents.</p>
<p>Generation We consider two main approaches to generate a table given a set of input documents. (1) The schema and values could be jointly generated, e.g. in a single call to a language model. This approach is fast, but initial experiments found it more prone to hallucinations and generic column names (e.g., "Title" or "Year"). (2) The generation process can be decomposed into separate schema and value generation steps. This approach is slower but allows us to overcome context window limits and leverage prior work in aspect-based question answering to perform value generation.</p>
<p>Evaluation We evaluate our approaches by determining whether the generated schemas are useful and values are correct. We consider a generated schema to be useful if its aspects either match those in the corresponding human-authored table in $\square$ ARXIVDIGESTAbles or if human evaluators rate them to be useful. ${ }^{4}$ These two conditions allow us to measure how well systems reconstruct reference table aspects ( $\S 5.1$ ) and evaluate their ability to generate novel aspects (§6.1). Second, we evaluate correctness of values as we would for any information extraction or QA task: for a pair of aligned columns (and rows), we judge whether the predicted cell value is semantically equivalent to the gold cell value (see §5.2).</p>
<h2>4 Experiments</h2>
<p>We prompt language models to perform either joint or decomposed generation.</p>
<h3>4.1 Base Models</h3>
<p>We use two language models, one open-weight, Mixtral 8x22 (Mistral AI, 2024), and one closed weight, GPT-3.5-Turbo (Open AI, 2022). To avoid gaming our recall metric, we instruct all models to generate schemas with the same number of aspects as the corresponding reference tables. (More prompting details in Appendix §B.5).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.2 Joint Table Generation</h3>
<p>We represent input papers using their titles and abstracts, which usually have enough information to form useful schemas and are easier to fit in the context window of models. We use a zero-shot table generation prompt (Appendix §B.1). We treat this condition as our baseline.</p>
<h3>4.3 Decomposed Table Generation</h3>
<p>Step 1: Schema generation Like in joint generation, we represent input papers using their titles and abstracts. We explore a range of prompts, each including a different piece of additional context (detailed in §4.3.1).</p>
<p>Step 2: Value generation Similar to extractive QA, for each aspect-paper pair, we prompt the model to generate a cell value based on the aspect name and the full text of the paper. After generating values for each paper given an aspect, we instruct a model to rewrite the values to be shorter and more consistent in style for display in table format. For this step, we use GPT 3.5-Turbo for speed and accuracy (Open AI, 2022) (prompt in Appendix §B.3).</p>
<h3>4.3.1 Additional Context</h3>
<p>To further investigate what contextual information is needed to steer language models to reconstruct human-authored tables, we test the following additional contexts, which could be added to either schema and/or value generation (see Appendix $\S B$ for prompts): (1) a generated caption where GPT-3.5-Turbo generates a short description that is consistent with all input papers; (2) the gold caption from the reference table; (3) the gold caption and in-text references, which include referencing sentences from the table's source paper; and (4) few-shot in-context examples, consisting of five reference table examples from $\square$ ARXIVDIGESTAbles retrieved based on cosine similarity between caption embeddings (Reimers and Gurevych, 2019).</p>
<h2>5 Developing an Automatic Metric</h2>
<p>Below we describe the design of our automatic evaluation procedure with two components: evaluating the schema and values for a generated table.</p>
<h3>5.1 Schema Evaluation</h3>
<p>Challenges The key challenge in assessing how well a generated table reconstructs a reference</p>
<p>table lies in determining schema alignmentsidentifying which columns convey the same information despite different phrasing. Two issues make schema alignment difficult. First, reference tables tend to present information concisely, making column headers and values hard to interpret without additional context (e.g., a column might be named "VQA" instead of "video quality assessment"). Second, information in generated and reference tables might have low lexical overlap despite semantic similarity, a problem also observed in summarization evaluation (Lin, 2004).</p>
<p>Problem Definition To formalize the schema alignment problem, recall that a table schema is a set of $N$ aspects. Given a model-generated table schema, $S^{\prime n}=\left{a_{1}^{m}, \ldots, a_{N}^{m}\right}$, a reference table schema $S^{r}=\left{a_{1}^{r}, \ldots, a_{N}^{r}\right}$, and a threshold $0 \leq t \leq 1$, our goal is to construct a scoring function $f$ to score each pair of aspects, $\left(a_{i}^{m}, a_{j}^{r}\right)$, such that $f\left(a_{i}^{m}, a_{j}^{r}\right)&gt;t$ if and only if human raters would agree that $a_{i}^{m}$ and $a_{j}^{r}$ convey the same information.</p>
<p>Alignment Framework We propose to define $f$ as the composition of two functions: a featurizer $(\phi)$, and a scorer $(g)$. The goal of the featurizer is to improve aspect interpretability by incorporating additional context, while the goal of the scorer is to account for meaning-preserving lexical diversity, leading to better schema alignments.</p>
<p>Configurations of $f$ We study three featurizers $\phi$ : (1) "name" only takes the column name as-is, (2) "values" concatenates all values under a column to the name, and (3) "decontext" prompts a language model ${ }^{5}$ to generate a stand-alone description (Choi et al., 2021; Newman et al., 2023), given the column name and its values.
We also study four scoring functions $g$ :</p>
<ul>
<li>Exact Match, which assigns a score of 1 if $\phi\left(a_{i}^{m}\right)=\phi\left(a_{j}^{r}\right)$ and 0 otherwise.</li>
<li>Jaccard, which computes Jaccard similarity of the featurized aspects, with stopwords removed.</li>
<li>Sentence Transformers, which encodes featurized aspects using all-MiniLM-L6-v2 and computes cosine similarity between them (Reimers and Gurevych, 2019).</li>
<li>Llama 3, which prompts Llama 3 (70B) Chat with generated and reference tables, with the column headers replaced by featurized versions, in-</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>structions to output aligned columns, and ten in-context examples. All pairs of columns returned by the LLM are assigned a score of 1 , and 0 otherwise. Refer to $\S$ B. 4 for prompting details.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Recall averaged over different contexts and systems. The band represents $95 \%$ confidence interval. Llama3 scorers have high recall, but low precision. Sentence Transformers (decontext) has the best trade-off.</p>
<p>Calibrating Schema Alignment We first run various combinations of $(\phi, g, t)$ and compute schema recall (i.e., proportion of reference table aspects matched to generated table aspects) on $25 \%$ of the tables in ARXIVDIGESTAbLes. In Figure 4, we observe a wide range of recall trade-offs: (1) Exact match has very low recall, as expected, serving as our conservative bound. (2) Llama 3 aligners tend to predict many more matches than other configurations despite that half of the in-context examples are tables with no matches. Llama 3 aligners serve as our upper bound. We perform human evaluation on $\sim 50$ tables and find that Llama 3 aligners have between $37-55 \%$ precision on their predicted matches. ${ }^{6}$ (3) Focusing our attention on the configurations that yield recall between these two bounds, we evaluate a range of configurations on the same tables and arrive at DecontextEval, our best configuration with $\phi$ using decontext features, $g$ using sentence transformers, and $t=0.7$; we find DECONTExtEval performs at $70-85 \%$ precision with acceptable yield.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>5.2 Value Evaluation</h3>
<p>Automated value evaluation suffers from the same issues that complicate schema evaluation, but one issue specific to value evaluation is reliance on accurate schema alignments. If aspects are incorrectly matched by a schema alignment metric, performance on value evaluation might rise/drop undeservedly. Therefore, we propose evaluating value generation in isolation, instead of an end-to-end table evaluation setting.</p>
<p>Specifically, we use the reference table's schemas as input to our value generation module. This ensures that every value in the reference table has a corresponding generated value (barring generation failures), bypassing the need for schema alignment. Following $\S 4.3 .1$, we consider three settings using different types of contexts: (1) "Column Names" only, (2) "Caption Context" which adds the table caption, and (3) "All Context" which further adds in-text references. Prompts used for each setting are in Appendix §B.3. We then use the same suite of scorers from $\S 5.1$ (except Llama 3, which we observed was low-precision) to compute overlap between pairs of generated and reference table's values.</p>
<h2>6 Results</h2>
<h3>6.1 Schema Evaluation Results</h3>
<p>Automated Evaluation Figure 5 shows the ability of GPT-3.5-Turbo and Mixtral 8x22 to reconstruct schemas (as measured via DeconteXtEVAL) using various types of additional contexts described in §4.3.1. Turning back to the question: How does the amount of context provided affect table reconstruction? (1) We see that low context prompts (e.g., a baseline with no additional context, caption-only) perform the worst while high context prompts (e.g., in-text references, in-context examples) perform best. This trend is fairly stable across systems. (2) Interestingly, though adding context improves reconstruction, it does not make the task trivial - even the best performing systems are far from perfect.</p>
<p>One potential concern for this analysis is that the models we use may have seen the older tables during training, which could inflate performance. To address this, we compute recall separately on subsets of newer and older tables (those from before or after January 2023 constituting 30\% and 70\% of our data respectively) for the high context prompts. We find that there is minimal difference between
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Schema recall for GPT-3.5-Turbo and Mixtral $8 \times 22$, using various types of additional contexts. All scores are computed using our best metric: sentence transformer-based scorer with decontext featurizer. More context improves recall, but does not lead to completely reproducing reference table schemas.
these two sets (the newer tables have recalls on average 1-3 percentage points lower).</p>
<p>Human Evaluation Our automated evaluation measures how well LMs can recover the reference tables' aspects, but leaves an additional question: Are LM-generated novel aspects which do not match with gold aspects also useful? To investigate this, we collect human assessments of generated aspects. Annotators are provided a generated table and the titles of all input papers. They are then prompted to provide a 5-point Likert scale rating for each of the following aspects: (1) general usefulness for understanding the input papers, (2) specificity to input papers (i.e., would this aspect be applicable to any other set of papers), and (3) insightfulness of the generated aspect (i.e., capturing novelty). ${ }^{7}$ We also instruct annotators to only judge based on the quality of the aspects only, ignoring the values which are evaluated separately. After collecting these ratings, we separated the rated aspects into two groups-ones that matched a gold aspect (M), and ones that did not (NM). The annotators were blind to the conditions when rating the aspects, and inter-annotator agreement was 0.56 (Krippendorff's $\alpha$ ).</p>
<p>Comparing ratings on matched and unmatched aspects, we did not find aspects that matched to be rated significantly higher than ones that did not (Table 3; Mann-Whitney U tests). This suggests that novel generated aspects are of comparable quality</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Caption+In-text Ref</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{M}$</td>
<td style="text-align: center;">NM</td>
<td style="text-align: center;">$\mathbf{M}$</td>
<td style="text-align: center;">NM</td>
</tr>
<tr>
<td style="text-align: left;">Useful</td>
<td style="text-align: center;">$3.70(1.74)$</td>
<td style="text-align: center;">$4.07(1.06)$</td>
<td style="text-align: center;">$3.92(0.69)$</td>
<td style="text-align: center;">$3.73(1.17)$</td>
</tr>
<tr>
<td style="text-align: left;">Specific</td>
<td style="text-align: center;">$2.88(1.26)$</td>
<td style="text-align: center;">$3.06(1.34)$</td>
<td style="text-align: center;">$2.85(1.31)$</td>
<td style="text-align: center;">$2.75(1.35)$</td>
</tr>
<tr>
<td style="text-align: left;">Insightful</td>
<td style="text-align: center;">$1.86(1.04)$</td>
<td style="text-align: center;">$1.93(1.21)$</td>
<td style="text-align: center;">$2.34(1.25)$</td>
<td style="text-align: center;">$2.27(1.19)$</td>
</tr>
<tr>
<td style="text-align: left;"># Samples</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">208</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">283</td>
</tr>
</tbody>
</table>
<p>Table 3: Mean (SD) ratings from human assessments of generated aspects that match the gold schema (M) with those that do not (NM).
(usefulness, specificity, insightfulness) to gold aspects or even have a higher quality (usefulness of aspects from Caption+In-text References). Moreover, aspects from Caption+In-text Reference are shown to be more useful and specific than the Baseline's, but were less insightful. This suggests an interesting tradeoff between our reconstruction objective, and possibly a different objective like creativity.</p>
<p>Error Analysis Finally, we report some qualitative observations of errors in the generated schemas we used for human evaluation. These point to future areas of improvement. Comparing outputs from the baseline to the Caption+In-text References condition, we find that the latter tends to output more specific aspects. For example, for one table, the Mixtral baseline produces aspects "Model Architecture" and "Application", while the Caption+In-text References Mixtral system generates the more specific aspects "Maximum resolution" and "Training batch size". We also note a few differences between schemas generated in the Caption+In-text references setting the reference tables' schemas, as well as categories of aspects that can pose difficulty for generation in Table 5 and additional examples in Appendix §D.</p>
<h3>6.2 Value Evaluation Results</h3>
<p>Automated Evaluation. Figure 6 shows the performance of GPT-3.5-Turbo on value generation, using various types of additional contexts (described in §5.2.) We see that scorers continue to follow the same trend observed during schema alignment, with the sentence transformer scorer being fairly permissive while an exact match is overly strict. Interestingly, unlike schema reconstruction, we observe that incorporating additional context does not seem to improve value generation accuracy; we dig deeper into this during human evaluation. Finally, like schema alignment, models are far from perfect in value generation.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Value generation accuracy for GPT-3.5-Turbo using various types of additional contexts, as computed by different scorers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: center;">Complete</th>
<th style="text-align: center;">Partial</th>
<th style="text-align: center;">None</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Col. Names</td>
<td style="text-align: center;">$21.13 \%(75)$</td>
<td style="text-align: center;">$22.54 \%(80)$</td>
<td style="text-align: center;">$56.34 \%(200)$</td>
</tr>
<tr>
<td style="text-align: left;">+ Captions</td>
<td style="text-align: center;">$18.84 \%(65)$</td>
<td style="text-align: center;">$31.30 \%(108)$</td>
<td style="text-align: center;">$49.86 \%(172)$</td>
</tr>
<tr>
<td style="text-align: left;">+ IT-Refs</td>
<td style="text-align: center;">$22.65 \%(77)$</td>
<td style="text-align: center;">$31.77 \%(108)$</td>
<td style="text-align: center;">$45.59 \%(155)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Proportion of matched gold-generated value pairs for various context settings, according to human assessment.</p>
<p>Human Evaluation. We conduct additional human evaluation to investigate whether adding context indeed has no impact on value accuracy, or our automated metrics are not sensitive enough to capture differences. We randomly sample 30 tables and compare gold vs generated values for these tables under all three settings. For each goldgenerated value pair, we have two annotators label whether it is a complete match, partial match or unmatched. Partial matches include cases where values are lists of items and the generated value misses or adds some (e.g., "DPO" vs "DPO, PPO"), or cases where the gold and generated values have a hypernymy relationship (e.g., "graph neural networks" vs "GATs"). Inter-annotator agreement is 0.55 (Cohen's $\kappa$ ). Table 4 presents results from this assessment, showing that adding additional context leads to a significant improvement in partial matches. However, many matches have no lexical overlap (e.g., "X" vs "No") or require some inference (e.g., "Yes" under a column called "sensors deployed" should match a value like "sensors used to monitor air quality"). This indicates that there is scope for further research in developing more sensitive featurizers and scorers for value evaluation.</p>
<h2>7 Related Work</h2>
<h3>7.1 Schema Generation for Literature Review</h3>
<p>Synthesizing schemas from research papers has been previously studied in contexts like identifying relations between papers (Shahaf et al., 2012; Lee</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Challenge Type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Different Granularity</td>
<td style="text-align: left;">The generated schema might be a high-level category (e.g. "data types"), while the reference <br> schema includes more specific aspects (e.g. "image", "text", "audio", etc.)</td>
</tr>
<tr>
<td style="text-align: left;">Different topics</td>
<td style="text-align: left;">The generated schema might have a different variety of topics than the reference schema <br> (e.g. {"model architecture", "dataset used", "performance metric"} versus just dataset <br> properties {"color", "context"})</td>
</tr>
<tr>
<td style="text-align: left;">Complex Aspects</td>
<td style="text-align: left;">Aspects combine information from multiple cells, which can mislead the value generator. <br> E.g. "dataset size" leads to some values pertaining to training data and others to test data.</td>
</tr>
<tr>
<td style="text-align: left;">Overly Specific</td>
<td style="text-align: left;">A predicted aspect might only apply to one paper</td>
</tr>
</tbody>
</table>
<p>Table 5: Qualitative observations of challenges with generated tables
et al., 2024), organizing research threads (Kang et al., 2023), discovering papers for ideation (Hope et al., 2022; Kang et al., 2022), or constructing intermediate scaffolds for better multi-document summarization (Shah et al., 2021). These works often assume fixed or sparse schemas, focus on a sub-component of schema generation, or do not evaluate intermediate tables. More closely related to our work, SciDaSynth is an interactive interface for creating "data tables" from a set of papers (Wang et al., 2024), which infers aspects from users' questions about the papers. However, identifying and articulating good comparison aspects can be nontrivial for users, motivating our aim of automatically inducing salient aspects. Hashimoto et al. (2017) explore automated aspect extraction for literature review tables and point out that more specific aspects are useful but hard to generate.</p>
<h3>7.2 Datasets for Scientific Table Generation</h3>
<p>Prior work has also released datasets of tables (Bai et al., 2023; Gupta et al., 2023). Bai et al. (2023) build a dataset of numeric result tables, while Gupta et al. (2023) release 4.4 k distantly supervised and 1.5 k manually annotated tables with material compositions from papers. Unlike ARXIVDIGESTAbles, these datasets do not necessarily link tables to input papers. Multidocument summarization datasets, like MultiXScience (Lu et al., 2020) and MS’2 (DeYoung et al., 2021), are related to table generation but yield sparse tables or use fixed schemas. Finally, there are datasets for other table-related tasks such as table extraction from PDFs (Gemelli et al., 2023), table retrieval (Gao and Callan, 2017), column annotation (Korini et al., 2022), table-to-text generation (Moosavi et al., 2021), table transformation (Chen et al., 2021), and table generation (Wu et al., 2022). However, these datasets either do not focus on scientific tasks or comparing papers.</p>
<h3>7.3 Automated evaluation using LMs</h3>
<p>As LMs have improved, they have also increasingly been used for automatic evaluation across NLP tasks, including summarization and QA that our work is similar to (Zhang et al., 2020; Wang et al., 2023; Lu et al., 2024; Zheng et al., 2024; Murahari et al., 2024). Some work on table generation has used a combination of automated and human evaluation. Hashimoto et al. (2017) use ROUGE (Lin, 2004) and human evaluation (Nenkova et al., 2007) to evaluate generated summaries of a table. Zhang and Balog (2018) evaluates schema selection via automatic entity ranking using ground truth entities. These works largely focus on measuring content overlap, whereas our automated metric incorporates table structure and context and our human evaluation focuses on downstream utility.</p>
<h2>8 Conclusion</h2>
<p>Language models have the potential to help scientists organize papers during literature review by synthesizing tables with schemas that aid comparison. In this work, we curate ARXIVDIGESTABLES, a dataset of such tables and additional contexts that can be used to evaluate systems' abilities to produce such tables. We present DecontextEval, an automatic evaluation framework for comparing model-generated and human-authored reference tables. We then use this evaluation framework to investigate two research questions: what context is needed to reconstruct human-authored tables, and whether generated aspects that don't align with references are also useful, specific and insightful. We release our artifacts to help spur development of literature review table generation systems, and seed potential for their role in evaluating systems' scientific synthesis abilities.</p>
<h2>Limitations</h2>
<p>We only study scientific papers from ArXiv. While in theory, scientists in many fields produce literature review tables, we restrict our reference tables to ones that we can scrape from ArXiv. This means many of the papers in our dataset come from fields that are most represented on ArXiv (e.g. computer science) and fewer come from medicine, humanities, or social science publications. Additionally, all of the tables in our high quality set are in English, even though literature review tables may also be used in other languages.</p>
<p>Reconstructing tables is difficult. While DeCONTEXTEval is effective at matching generated and reference table columns, and we test providing different additional context to steer the table generation models, many generated table columns do not match with the reference columns. Though we presented a human evaluation protocol that showed utility for generated columns that do not match the reference columns, such evaluation is costly. Future work should investigate automatic metrics that correlate with human utility evaluations as well.</p>
<h2>Ethical Considerations and Broader Impact</h2>
<p>Generated literature review tables might misrepresent authors' work. Generating literature review tables requires taking aspects of papers out of their original context to show them to users. Similar to summarization, this process has the potential to misrepresent the original work either due to the table cell values not having enough context, or less accurate models introducing hallucinations. Additional checks would have to be implemented if such tables were to be deployed in user-facing situations.</p>
<p>Literature review tables may discourage reading original sources. The resource we present is meant to encourage the development of methods to construct literature review tables. If the field iterates on this task and develops systems that perform very well, the tables may have all of the information that a given reader wants to see. This could discourage readers from finding the original source of the claims. That said, the rows in the tables in our benchmark do include citations, so readers could trace values back to their sources. However, readers are not guaranteed to follow these citations, so generated tables could encourage poor scholarly practices.</p>
<h2>References</h2>
<p>OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and Red Avila et al. 2023. Gpt-4 technical report.</p>
<p>Ojas Ahuja, Jiacheng Xu, Akshay Gupta, Kevin Horecka, and Greg Durrett. 2022. ASPECTNEWS: Aspect-oriented summarization of news documents. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6494-6506, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, and Alan Ritter. 2023. Schema-driven information extraction from heterogeneous tables. ArXiv, abs/2305.14336.</p>
<p>Lutz Bornmann, Robin Haunschild, and Rüdiger Mutz. 2021. Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Humanities and Social Sciences Communications, 8(1):1-15.</p>
<p>Mingda Chen, Sam Wiseman, and Kevin Gimpel. 2021. WikiTableT: A large-scale data-to-text dataset for generating Wikipedia article sections. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 193-209, Online. Association for Computational Linguistics.</p>
<p>Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. 2021. Decontextualization: Making sentences stand-alone. Transactions of the Association for Computational Linguistics, 9:447-461.</p>
<p>Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599-4610, Online. Association for Computational Linguistics.</p>
<p>Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. MS'2: Multidocument summarization of medical studies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 74947513, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Kyle Yingkai Gao and Jamie Callan. 2017. Scientific table search using keyword queries. ArXiv, abs/1707.03423.</p>
<p>Andrea Gemelli, Emanuele Vivoli, and Simone Marinai. 2023. CTE: A dataset for contextualized table extraction. Preprint, arXiv:2302.01451.</p>
<p>Tanishq Gupta, Mohd Zaki, Devanshi Khatsuriya, Kausik Hira, N M Anoop Krishnan, and Mausam. 2023. DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13465-13483, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Hayato Hashimoto, Kazutoshi Shinoda, Hikaru Yokono, and Akiko Aizawa. 2017. Automatic generation of review matrices as multi-document summarization of scientific papers. In BIRNDL@ SIGIR.</p>
<p>Tom Hope, Ronen Tamari, Daniel Hershcovich, Hyeonsu B Kang, Joel Chan, Aniket Kittur, and Dafna Shahaf. 2022. Scaling creative inspiration with fine-grained functional aspects of ideas. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1-15.</p>
<p>Arif E Jinha. 2010. Article 50 million: an estimate of the number of scholarly articles in existence. Learned publishing, 23(3):258-263.</p>
<p>Hyeonsu B Kang, Xin Qian, Tom Hope, Dafna Shahaf, Joel Chan, and Aniket Kittur. 2022. Augmenting scientific creativity with an analogical search engine. ACM Transactions on Computer-Human Interaction, 29(6):1-36.</p>
<p>Hyeonsu B Kang, Tongshuang Wu, Joseph Chee Chang, and Aniket Kittur. 2023. Synergi: A mixed-initiative system for scholarly synthesis and sensemaking. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-19.</p>
<p>Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, et al. 2023. The semantic scholar open data platform. arXiv preprint arXiv:2301.10140.</p>
<p>Keti Korini, Ralph Peeters, and Christian Bizer. 2022. Sotab: the wdc schema. org table annotation benchmark. In CEUR Workshop Proceedings, volume 3320, pages 14-19. RWTH Aachen.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Yoonjoo Lee, Hyeonsu B Kang, Matt Latzke, Juho Kim, Jonathan Bragg, Joseph Chee Chang, and Pao Siangliulue. 2024. Paperweaver: Enriching topical paper alerts by contextualizing recommended papers with user-collected papers. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 1-19.</p>
<p>Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. 2023. QASA: advanced question answering on scientific articles. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yao Lu, Yue Dong, and Laurent Charlin. 2020. MultiXScience: A large-scale dataset for extreme multidocument summarization of scientific articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8068-8074, Online. Association for Computational Linguistics.</p>
<p>Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. 2024. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation. Advances in Neural Information Processing Systems, 36.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Mistral AI. 2024. Mixtral of experts. https: //mistral.ai/news/mixtral-of-experts/. Accessed on March 25, 2024.</p>
<p>Nafise Sadat Moosavi, Andreas Rücklé, Dan Roth, and Iryna Gurevych. 2021. SciGen: a dataset for reasoning-aware text generation from scientific tables. In NeurIPS Datasets and Benchmarks.</p>
<p>Vishvak Murahari, Ameet Deshpande, Peter Clark, Tanmay Rajpurohit, Ashish Sabharwal, Karthik Narasimhan, and Ashwin Kalyan. 2024. QualEval: Qualitative evaluation for model improvement. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 2093-2111, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The pyramid method: Incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing (TSLP), 4(2):4-es.</p>
<p>Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. 2023. A question answering framework for decontextualizing user-facing snippets from scientific documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3194-3212, Singapore. Association for Computational Linguistics.</p>
<p>Open AI. 2022. Introducing ChatGPT. https:// openai.com/index/chatgpt/. Accessed on March 25, 2024 .</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</p>
<p>Daniel M. Russell, Mark J. Stefik, Peter Pirolli, and Stuart K. Card. 1993. The cost structure of sensemaking. In Proceedings of the INTERACT '93 and CHI '93 Conference on Human Factors in Computing Systems, CHI '93, page 269-276, New York, NY, USA. Association for Computing Machinery.</p>
<p>Tarek Saier, Johan Krause, and Michael Färber. 2023. unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network. In 2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL), pages 66-70, Los Alamitos, CA, USA. IEEE Computer Society.</p>
<p>Darsh J. Shah, L. Yu, Tao Lei, and Regina Barzilay. 2021. Nutri-bullets hybrid: Consensual multidocument summarization. In North American Chapter of the Association for Computational Linguistics.</p>
<p>Dafna Shahaf, Carlos Guestrin, and Eric Horvitz. 2012. Metro maps of science. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1122-1130.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is ChatGPT a good NLG evaluator? a preliminary study. In Proceedings of the 4th New Frontiers in Summarization Workshop, pages 1-11, Singapore. Association for Computational Linguistics.</p>
<p>Xingbo Wang, Samantha L Huey, Rui Sheng, Saurabh Mehta, and Fei Wang. 2024. SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model. arXiv preprint arXiv:2404.13765.</p>
<p>Xueqing Wu, Jiacheng Zhang, and Hang Li. 2022. Text-to-table: A new way of information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2518-2533, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yumo Xu and Mirella Lapata. 2020. Coarse-to-fine query focused multi-document summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3632-3645, Online. Association for Computational Linguistics.</p>
<p>Xianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Xiaoman Pan, Linda Petzold, and Dong Yu. 2023. OASum: Large-scale open domain aspectbased summarization. In Findings of the Association
for Computational Linguistics: ACL 2023, pages 4381-4401, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Shuo Zhang and Krisztian Balog. 2018. On-the-fly table generation. The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A new benchmark for querybased multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905-5921, Online. Association for Computational Linguistics.</p>
<h2>A Data Processing</h2>
<h2>A. 1 XML Parsing</h2>
<p>At this point in the pipeline, the tables we are considering are represented in XML format. Unfortunately, sometimes XML-formatted tables have column headers that span multiple rows, rows can have insufficient numbers of columns, cells may span multiple columns rows, etc. This makes it hard to enforce Desiderata 3. To address these difficulties, we design heuristics to parse the XML formatted tables into a JSON object that allows us to directly index the tables by paper and aspect. Our heuristics cannot be completed for all tablessometimes they fail completely, and other times they fail on particular rows. We also experimented using GPT-4 (Achiam et al., 2023) for these difficult cases, but still found errors due to insufficient layout information being maintained in the conversion from LaTeX source to XML.</p>
<h2>A. 2 High Quality Data Filters</h2>
<p>To achieve our set of tables, we apply a number of stringent filters. We remove any tables whose headers came from merging two rows or that have a</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Field</th>
<th style="text-align: center;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Computer Science</td>
<td style="text-align: center;">1985</td>
</tr>
<tr>
<td style="text-align: center;">Electrical Engineering and Systems Science</td>
<td style="text-align: center;">131</td>
</tr>
<tr>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Quantitative Biology</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: center;">Statistics</td>
<td style="text-align: center;">19</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">Quantitative Finance</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Economics</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 6: Fields of study represented in the high-quality dataset.
row without a citation to avoid misformatted tables. We also deduplicate the tables using an exact string match on all columns minus the references column, and deduplicate individual rows (which includes citations). To meet Desiderata 2, and avoid filtering out tables that have empirical results, we filter out any columns that have floating point numbers, formulas, or figures. After these steps, we remove any tables that have fewer than two citations, rows or columns, leaving us with our final set.</p>
<h2>A. 3 Medium Quality Data Filters</h2>
<p>In addition to our high-quality dataset that is likely to meet our desiderata, we also release a larger set of 22,283 tables with fewer filters. These tables are not manually checked, are filtered less stringently, and do not have linked full-texts. In particular:</p>
<ul>
<li>Papers in rows are required to have titles and abstracts, but not required to have full-texts. This potentially makes value generation difficult because all of the values have to come from the title and abstract.</li>
<li>Tables are not required to have in-text references. This potentially makes schema generation difficult, as any additional context has to come from the caption (if present).</li>
<li>Tables with at most one row with no citation are allowed, as opposed to all rows having citations.</li>
<li>Tables with multi-row or hierarchical headers are allowed. These can sometimes lead to misformatted tables.</li>
</ul>
<h2>A. 4 Field of Study</h2>
<p>A full break-down of the fields of study represented in the high-quality dataset is in Table 6.</p>
<h2>A. 5 Example Data Instance</h2>
<p>Below is an example instance from ARXIVDIGESTAbles. (Some of the keys
rephrased and values are elided for clarity)</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="nx">Table</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span><span class="w"> </span><span class="mi">53648</span><span class="nx">c28</span><span class="o">-</span><span class="nx">a2b2</span><span class="o">-</span><span class="mi">4</span><span class="nx">e41</span><span class="o">-...</span>
<span class="nx">Paper</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span><span class="w"> </span><span class="m m-Double">2305.14525</span><span class="nx">v1</span>
<span class="nx">Caption</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;A categorization of</span>
<span class="s">    scope regarding design</span>
<span class="s">    variations observed in</span>
<span class="s">    collected corpora. The three</span>
<span class="s">    columns are high-level design</span>
<span class="s">    variation types, low-level</span>
<span class="s">    details assumptions over</span>
<span class="s">    visual designs...&quot;</span>
<span class="nx">In</span><span class="o">-</span><span class="nx">Text</span><span class="w"> </span><span class="nx">References</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span><span class="nx">Section</span><span class="p">:</span><span class="w"> </span><span class="nx">Design</span><span class="w"> </span><span class="nx">Variations</span>
<span class="w">    </span><span class="nx">Text</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;In addition to chart</span>
<span class="s">        type, we have also</span>
<span class="s">        observed scope...in Table</span>
<span class="s">        {{table:&lt;table id&gt;}}...&quot;</span><span class="p">},</span>
<span class="w">    </span><span class="o">...</span>
<span class="p">],</span>
<span class="nx">Table</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">References</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;{{cite:9a81b16</span>
<span class="s">        }}&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;{{cite:d5b4bb4}}&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;{{cite:342c0c4}}&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;{{</span>
<span class="s">        cite:6697498}}&quot;</span><span class="p">],</span>
<span class="w">    </span><span class="s">&quot;Design Variation Type&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;</span>
<span class="s">        composite arrangement&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;</span>
<span class="s">        mark and glyph&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;mark and</span>
<span class="s">        glyph&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;coordinate space</span>
<span class="s">        &quot;</span><span class="p">],</span>
<span class="w">    </span><span class="s">&quot;Assumption&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;only multiple</span>
<span class="s">        -view charts&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;only</span>
<span class="s">        proportion-related charts</span>
<span class="s">        &quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;only timeline-related</span>
<span class="s">        infographics&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;in</span>
<span class="s">        Cartesian coordinate space</span>
<span class="s">        &quot;</span><span class="p">]</span>
<span class="p">},</span>
<span class="nx">Citation</span><span class="w"> </span><span class="nx">Info</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="nx">Cite</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span><span class="w"> </span><span class="mi">9</span><span class="nx">a81b16</span><span class="p">,</span>
<span class="nx">Title</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Composition and</span>
<span class="s">    Configuration Patterns in</span>
<span class="s">    Multiple-View Visualizations&quot;</span><span class="p">,</span>
<span class="nx">Abstract</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Multiple-view</span>
<span class="s">    visualization (MV) is a layout</span>
<span class="s">    design technique...&quot;</span><span class="p">,</span>
<span class="nx">Full</span><span class="w"> </span><span class="nx">Text</span><span class="p">:</span><span class="w"> </span><span class="err">&quot;</span><span class="mi">1</span><span class="w"> </span><span class="nx">Introduction</span><span class="w"> </span><span class="nx">We</span>
<span class="w">    </span><span class="nx">present</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="k">in</span><span class="o">-</span><span class="nx">depth</span><span class="w"> </span><span class="nx">study</span><span class="w"> </span><span class="nx">on</span>
<span class="w">    </span><span class="nx">how</span><span class="w"> </span><span class="nx">multiple</span><span class="w"> </span><span class="nx">views</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">used</span><span class="w"> </span><span class="k">in</span>
<span class="w">        </span><span class="nx">practice</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">integrate</span><span class="w"> </span><span class="nx">our</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">results</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">recommendation</span>
<span class="w">    </span><span class="nv">system</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">layout</span><span class="w"> </span><span class="nv">design</span>
<span class="w">    </span>...<span class="err">&quot;</span>
<span class="err">}, ...</span>
<span class="err">}</span>
</code></pre></div>

<h2>B Prompts</h2>
<h2>B. 1 Prompt for table generation (Baseline)</h2>
<p>System Prompt: You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers.</p>
<p>User Prompt: [System]</p>
<p>We would like you to build a table that has each paper as a row and, as each column, a dimension that compares between the papers. You will be given multiple papers labeled Paper 1, 2, and so on. You will be provided with the title and content of each paper. Please create a table that compares and contrasts the given papers. Make { col_num} dimensions which are phrases that can compare multiple papers, so that the table has {col_num} columns. The table should also have { paper_num} papers as rows. Return a JSON object of the following format:</p>
<div class="codehilite"><pre><span></span><code>    json
{json_format}
</code></pre></div>

<p><strong>Check that the table has { paper_num} papers as rows and {column_num} dimensions as
columns.</strong>.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Paper Content]</span>
<span class="na">{paper1} {paper2} ... {paperN}</span>
</code></pre></div>

<h2>B. 2 Prompt for schema generation</h2>
<p>System prompt is the same as the one from table generation.</p>
<h2>B.2.1 Schema generation with generated captions</h2>
<div class="codehilite"><pre><span></span><code><span class="k">User</span><span class="w"> </span><span class="nl">Prompt</span><span class="p">:</span><span class="w"> </span><span class="o">[</span><span class="n">System</span><span class="o">]</span>
<span class="n">Imagine</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="nl">scenario</span><span class="p">:</span><span class="w"> </span><span class="n">A</span>
<span class="w">    </span><span class="k">user</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">making</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nc">table</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">a</span>
<span class="w">    </span><span class="n">scholarly</span><span class="w"> </span><span class="n">paper</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">contains</span>
<span class="w">    </span><span class="n">information</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">multiple</span>
<span class="w">    </span><span class="n">papers</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">compares</span><span class="w"> </span><span class="n">these</span>
<span class="w">    </span><span class="n">papers</span><span class="p">.</span><span class="w"> </span><span class="k">To</span><span class="w"> </span><span class="n">compare</span><span class="w"> </span><span class="ow">and</span>
<span class="w">    </span><span class="n">contrast</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">papers</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">user</span>
<span class="w">    </span><span class="n">provides</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">title</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">content</span>
<span class="w">        </span><span class="k">of</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">paper</span><span class="p">.</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="k">is</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="nl">following</span><span class="p">:</span><span class="w"> </span><span class="n">Given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span>
<span class="w">        </span><span class="n">papers</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">find</span>
<span class="w">        </span><span class="n">aspects</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">shared</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span>
<span class="w">        </span><span class="n">given</span><span class="w"> </span><span class="n">research</span><span class="w"> </span><span class="n">papers</span><span class="p">.</span><span class="w"> </span><span class="k">Then</span><span class="p">,</span>
<span class="w">        </span><span class="k">within</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">aspect</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span>
<span class="w">        </span><span class="n">identify</span><span class="w"> </span><span class="err">{</span><span class="n">num_columns</span><span class="err">}</span>
<span class="w">        </span><span class="n">attributes</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="k">to</span>
<span class="w">        </span><span class="n">compare</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">papers</span><span class="p">.</span>
</code></pre></div>

<p>First, you should return the list of similar aspects as a Python list as follows: "["&lt; similar aspect that all given papers shared&gt;", ...]". Then, think of each aspect as the topic for the Related Work section of the user's paper. Finally, find attributes that can compare the given papers within the Related Work section. Return a JSON object in the following format:</p>
<div class="codehilite"><pre><span></span><code>    json
{{
    &quot;&lt;attribute 1&gt;&quot;: [&quot;&lt;comparable
        attribute within the aspect
        1&gt;&quot;, &quot;&lt;comparable attribute
        within the aspect 1&gt;&quot;, ...],
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    ...
]}
~..
</code></pre></div>

<p>[Paper Content]
{paper1} {paper2} ... {paperN}</p>
<div class="codehilite"><pre><span></span><code>Please ensure that your response
    strictly follows the given
    format. Adherence to the
    specified structure is
    mandatory.
</code></pre></div>

<h2>B.2.2 Schema generation with caption and in-text references</h2>
<p>Generation of schemas with captions does not include the in-text references part in the prompt below. This prompt is when the number of in-text references is $K$</p>
<h2>User Prompt: [System]</h2>
<p>Imagine the following scenario: A user is making a table for a scholarly paper that contains information about multiple papers and compares these papers. To compare and contrast the papers, the user provides the title and content of each paper. To help you build the table, the user provides a caption of this table, which is referred to in the paper as additional information.</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">Caption</span><span class="o">]</span>
</code></pre></div>

<p>{caption}
[In-text reference]
{section header 1: in-text
reference 1}{section header 2:
in-text reference 2}...{ section header K: in-text reference K}</p>
<p>Your task is the following: Given a list of papers and table caption, you should identify { num_columns } table columns to compare given research papers.</p>
<p>Return a list in the following format:</p>
<div class="codehilite"><pre><span></span><code>    \List
[&quot;&lt;comparable attribute within
    the table caption&gt;&quot;, &quot;&lt;
    comparable attribute within
    the table caption&gt;&quot;, ...]
</code></pre></div>

<p>[Paper Content]
{paper1} {paper2} ... {paperN}</p>
<div class="codehilite"><pre><span></span><code>Please ensure that your response
    strictly follows the given
    format. Adherence to the
    specified structure is
    mandatory.
</code></pre></div>

<h2>B.2.3 Schema generation with few-shot examples</h2>
<div class="codehilite"><pre><span></span><code><span class="k">User</span><span class="w"> </span><span class="nl">Prompt</span><span class="p">:</span><span class="w"> </span><span class="o">[</span><span class="n">System</span><span class="o">]</span>
</code></pre></div>

<p>Imagine the following scenario: A user is making a table for a scholarly paper that contains information about multiple papers and compares these papers. To compare and contrast the papers, the user provides the title and content of each paper. To help you build the table, the user provides similar tables that you can refer to as follows:
{Table 1: few-shot example table 1}{Table 2: few-shot example table 2}...{Table 5: few-shot example table 5}</p>
<p>Your task is the following: Given a list of papers and table examples, you should identify {num_columns} table columns to compare given research papers Return a list in the following format:
[List]</p>
<p>["<comparable attribute>", "&lt; comparable attribute&gt;", ...]
[List]
{paper1} {paper2} ... {paperN}
Please ensure that your response strictly follows the given format. Adherence to the specified structure is mandatory.</p>
<h2>B. 3 Prompt for value generation</h2>
<p>Answer a question using the provided scientific paper.</p>
<p>Your response should be a JSON object with the following fields:</p>
<ul>
<li>answer: The answer to the question. The answer should use concise language, but be comprehensive. Only provide answers that are objectively supported by the text in paper</li>
<li>excerpts: A list of one or more <em>EXACT</em> text spans extracted from the paper that support the answer. Return between at most ten spans, and no more that 800 words. Make sure to cover all aspects of the answer above.</li>
</ul>
<p>If there is no answer, return an empty dictionary, i.e., '{}'.</p>
<h2>Paper:</h2>
<p>{ full_text }
Given the information above, please answer the question: "{ question }".</p>
<p>Using this strategy to generate values for columns requires the creation of questions describing the corresponding columns, for which we follow a two-step generation process. First, we
prompt an LLM, specifically GPT-4-Turbo to generate descriptions for every column conditioned on additional context (either reference captions, or reference captions and in-text references). For the setting that does not use any additional context, this step is skipped.</p>
<div class="codehilite"><pre><span></span><code>CAPTION_PROMPT = &quot;&quot;&quot;
A user is making a table for a
    scholarly paper that contains
    information about multiple
    papers and compares these
    papers.
This table contains a column
    called {column}. Please write
    a brief definition for this
    column.
</code></pre></div>

<p>Here is the caption for the table : {caption}.</p>
<div class="codehilite"><pre><span></span><code>Definition:
&quot;&quot;&quot;
</code></pre></div>

<p>CAPTION_WITH_REF_PROMPT = """
A user is making a table for a scholarly paper that contains information about multiple papers and compares these papers.
This table contains a column called {column}. Please write a brief definition for this column.</p>
<p>Here is the caption for the table : {caption}.</p>
<p>Following is some additional information about this table: {in_text_ref}.</p>
<h2>Definition:</h2>
<p>" " "</p>
<p>Then, LLMs are prompted to rewrite generated definitions as concise queries. For the nocontext setting, we use a simple template to produce queries containing the column name.</p>
<div class="codehilite"><pre><span></span><code>CONTEXT_QUERY = &quot;Rewrite this
    description as a one-line
    question.&quot;
</code></pre></div>

<p>NO_CONTEXT_QUERY = "From the provided paper full-text, can you extract {column}?"</p>
<p>Our preliminary experiments show that the value generation module often returns empty values (in $30 \%$ cases on average), which motivates us to add a retry policy. Under this policy, we generate four additional queries with minor rephrasing and retry value generation with them. We observe that this reduces the proportion of empty values to $\sim 7.5 \%$. If all retries produce empty values, we return an empty value.</p>
<div class="codehilite"><pre><span></span><code>CONTEXT_RETRY_QUERIES
original_query + &quot;Return a
    summary of this information&quot;
original_query + &quot;Try to extract
    this information.&quot;
original_query + &quot;Summarize
    information about this.&quot;
original_query + &quot;What
    information can you find about
        this?&quot;
</code></pre></div>

<p>NO_CONTEXT_RETRY _QUERIES
Extract information about {column } aspect from this paper.
What information can you find about {column}?
We want to create a table comparing papers. Extract the information from this paper that goes in the column called {column}.
In a literature review table comparing multiple papers, what information from this paper would go under column { column}?</p>
<h2>B. 4 Prompt for Llama-3 Scorer for Automatic Evaluation</h2>
<div class="codehilite"><pre><span></span><code><span class="nv">Given</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">tables</span>,<span class="w"> </span><span class="nv">match</span><span class="w"> </span><span class="nv">column</span>
<span class="w">    </span><span class="nv">headers</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">their</span><span class="w"> </span><span class="nv">columns</span><span class="w"> </span><span class="nv">have</span>
<span class="w">    </span><span class="nv">very</span><span class="w"> </span><span class="nv">similar</span><span class="w"> </span><span class="nv">values</span>.<span class="w"> </span><span class="nv">Most</span>
<span class="w">    </span><span class="nv">columns</span><span class="w"> </span><span class="nv">will</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">match</span>.
<span class="nv">Respond</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">json</span><span class="w"> </span><span class="nv">list</span>,<span class="w"> </span><span class="nv">whose</span>
<span class="w">    </span><span class="nv">elements</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">element</span><span class="w"> </span><span class="nv">lists</span>
</code></pre></div>

<p>. The first element is the key of Object 1 and the matching key of Object 2.
For example, if the key 'Dataset size' and 'Number of training examples' are matched, you should return '[['Dataset size ', 'Number of training examples']]. If no keys contain the same information, then just output an empty list '[]'</p>
<p>Table 1:
[In-context example humanauthored table]</p>
<p>Table 2:
[In-context example generated table]</p>
<p>Response: [In-context example human-aligned aspects]</p>
<h2>B. 5 LM Prompting Details</h2>
<p>Truncation and Error Handling. As our evaluation tests language models' capabilities of schema rediscovery, we implemented strategies for handling other types of errors from language model generation (e.g., a different number number of schemas between generated and reference tables, or the format of the generated output not matching with a format given in the prompt). We take both preventative as well as fall-back measures to deal with these errors:</p>
<ol>
<li>Preventative: To address the issue of generating tables when context window might be insufficient due to the large number of input papers, we adopted the following approach by: (1) dividing the paper sets into smaller batches to ensure the total length of input papers does not exceed the context window size, (2) dividing the columns that need to be created into smaller batches to ensure the total number of columns from whole batches does not exceed the number of columns in human-authored tables, and (3) subsequently joining these smaller tables together without the need for further generation. The batch size is chosen based on the model and input paper</li>
</ol>
<p>Conditions for Paper Representation
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Diagram of prompting methods under experiment conditions.
representation. In general, though, we used a threshold of 20 abstracts per batch, determined by using the average length of the top $20 \%$ longest abstracts to ensure that even long abstracts could fit within the context along with the in-context examples and prompts. We also set the number of max tokens as high as the model can handle.
2. Fall-back: When encountering an error, we retry querying the model with the same prompt, and due to stochasticity in the generation process, models occasionally recover. The errors we handled with fallback strategies are as follows: (1) when the output doesn't align with the format specified in the prompt, (2) when the number of schemas, papers, and values don't match the reference table, and (3) when the entire context exceeds the context window of the base model.
3. Removal: We allow up to five retries before abandoning the input.</p>
<h2>C Human Evaluation</h2>
<p>When performing human evaluation for the novel schema, we assessed each column based on the following criteria:</p>
<ul>
<li>Usefulness: the degree to which this column helps in understanding and comparing the set of input papers.</li>
<li>Specificity: the degree to which a column is specific to the particular set of input papers, rather than applying to any generic set of papers.</li>
<li>Insightfulness: the degree to which a column is about novel and deep aspects. An insightful column goes beyond surface-level information and captures novel or unexpected aspects (e.g., "Method" column may be useful, but it may not be considered highly insightful.)</li>
</ul>
<p>The annotation interface used was created using Streamlit ${ }^{8}$ and can be found in Figure 8.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The interface used for annotating generated table column quality.</p>
<h1>D Qualitative Error Analysis</h1>
<p>For each of the error types listed in Table 5, we include a generated table that illustrates the error and a reference table when appropriate.</p>
<h2>1. Different Topics see Table 7</h2>
<ol>
<li>Different Granularity see Table 8</li>
<li>Complex aspects see Table 9</li>
<li>Overly Specific see Table 10</li>
</ol>
<p>Reference:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;"># categories</th>
<th style="text-align: center;">evaluation metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">765 adbf</td>
<td style="text-align: center;">fine-grained</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">mean accuracy</td>
</tr>
<tr>
<td style="text-align: center;">4 fe 680 c</td>
<td style="text-align: center;">face</td>
<td style="text-align: center;">9,131</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Generated:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source dataset</th>
<th style="text-align: center;">Target dataset</th>
<th style="text-align: center;">Number of images</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">765 adbf</td>
<td style="text-align: center;">FGVC-Aircraft</td>
<td style="text-align: center;">FGVC-Aircraft</td>
<td style="text-align: center;">10,000 images of airplanes</td>
</tr>
<tr>
<td style="text-align: center;">4 fe 680 c</td>
<td style="text-align: center;">VGGFace2</td>
<td style="text-align: center;">VGGFace2</td>
<td style="text-align: center;">3.31 million images</td>
</tr>
</tbody>
</table>
<p>Table 7: Different Topics: Reference table (top), Predicted table (bottom). We can see that our system generates a different (and redundant) set of aspects compared to the reference.</p>
<p>Reference:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Year</th>
<th style="text-align: center;">Data size</th>
<th style="text-align: center;">Image</th>
<th style="text-align: center;">Text</th>
<th style="text-align: center;">Tags</th>
<th style="text-align: center;">Video</th>
<th style="text-align: center;">Audio</th>
<th style="text-align: center;">3D Model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3919117</td>
<td style="text-align: center;">Twitter100K</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">15514398</td>
<td style="text-align: center;">Xmedia</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">12,000</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Generated:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset Name</th>
<th style="text-align: center;">Dataset Size</th>
<th style="text-align: center;">Data Types</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3919117</td>
<td style="text-align: center;">Twitter100k</td>
<td style="text-align: center;">100,000 image-text pairs</td>
<td style="text-align: center;">LDA, Bag-of-Word (BoW), ...</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">15514398</td>
<td style="text-align: center;">XMedia, Wikipedia, $\ldots$</td>
<td style="text-align: center;">12,000 media instances</td>
<td style="text-align: center;">text, image, video, audio, 3D model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 8: Different Granularities: Reference table (top), Predicted table (bottom). Some aspects are removed from each table to highlight the difference in granularity. The reference table separately splits out the various data types while the generated one has a single "Data Types" column.</p>
<p>Reference:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Classes</th>
<th style="text-align: center;">Signer</th>
<th style="text-align: center;">Videos</th>
<th style="text-align: center;">Videos per Class</th>
<th style="text-align: center;">Controlled</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">64745485</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3200</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">54446047</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">$11-45$</td>
<td style="text-align: center;">25513</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$\times$</td>
</tr>
</tbody>
</table>
<p>Generated:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sign Language</th>
<th style="text-align: center;">Dataset Size</th>
<th style="text-align: center;">Number of Subjects</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">64745485</td>
<td style="text-align: center;">Dataset of Argentinian Sign</td>
<td style="text-align: center;">3200 videos, 64 LSA signs,</td>
<td style="text-align: center;">10 subjects</td>
</tr>
<tr>
<td style="text-align: center;">54446047</td>
<td style="text-align: center;">Language (LSA) presented Large-scale sign language dataset created</td>
<td style="text-align: center;">10 subjects over 25,000 annotated videos</td>
<td style="text-align: center;">222 subjects</td>
</tr>
</tbody>
</table>
<p>Table 9: Complex Aspects: Reference table (top), Predicted table (bottom).</p>
<p>Generated:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hate Speech Dataset</th>
<th style="text-align: center;">Misinformation Dataset</th>
<th style="text-align: center;">Number of Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">253018764</td>
<td style="text-align: center;">Mentions [...] Hate Speech Dataset</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">multiple hate speech datasets</td>
</tr>
<tr>
<td style="text-align: center;">10326133</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Introduces LIAR dataset...</td>
<td style="text-align: center;">12,836</td>
</tr>
</tbody>
</table>
<p>Table 10: Overly Specific: The table shown is the predicted table. Note that the aspects "Hate Speech Dataset" and "Misinformation Dataset" only apply to a single paper each.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://streamlit.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ Predicted matches are rated either as incorrect, partially, or completely correct. The lower bound only counts complete matches and the upper bound includes partial matches.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
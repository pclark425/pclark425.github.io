<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7708 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7708</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7708</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-270148668</p>
                <p><strong>Paper Title:</strong> <a href="https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-56780-accepted.pdf" target="_blank">Potential roles of large language models in production of systematic reviews and meta-analyses</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) like ChatGPT have become widely applied in the field of medical research. In the process of conducting systematic reviews, similar tools can be employed to expedite various steps, including defining clinical questions, literature search, document screening, information extraction</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7708.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7708.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT (Meta programming for multi-agent collaborative framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent, LLM-centered framework for coordinating specialized agents (e.g., retrieval, extraction, analysis, writing) to perform complex, multi-step tasks such as end-to-end systematic review automation; presented as a proof-of-concept demo for multi-agent collaboration driven by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metagpt: Meta programming for multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Metagpt: Meta programming for multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses an LLM as the core 'controller' that programs and coordinates multiple specialized agents (retrievers, extractors, analyzers, writers). The LLM issues prompts/tasks to agents, ingests their outputs, and iteratively composes higher-level artifacts (extractions, analyses, draft text).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-text scholarly articles / PDFs and search results (literature corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured extractions, analysis outputs, and draft manuscript components (multi-step artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Multi-agent prompt orchestration and prompt engineering (meta‑programming), iterative/hierarchical prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited in the paper as a proof-of-concept demo showing feasibility of multi-agent LLM workflows for complex synthesis tasks; no quantitative results provided in this review article.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Presented as proof-of-concept only; issues noted in the review include hallucination risk, model knowledge staleness, token/context limits, and need for human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Potential roles of large language models in production of systematic reviews and meta-analyses', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7708.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7708.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-driven autonomous agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language model based autonomous agents for systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of approaches that instantiate autonomous agents under LLM control to perform literature search, screening, data extraction, synthesis, and drafting in a (semi-)automated pipeline for reviews and meta-analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A survey on large language model based autonomous agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A survey on large language model based autonomous agents</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-driven autonomous agents</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Frameworks that leverage LLMs to plan, delegate, and iteratively perform tasks via agents (retrieval, screening, extraction, analysis), enabling end-to-end or modular automation of literature synthesis workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Search results, abstracts, full-text PDFs, and possibly metadata/structured records</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screened study lists, structured data extractions, synthesized summaries, and draft manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Agent orchestration, hierarchical/iterative prompting, prompt engineering; may combine retrieval augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reviewed as an emerging direction with proof-of-concept systems; the article cites the promise but no standardized benchmarks reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Challenges highlighted include updating knowledge, personalization, strategic planning, hallucination, token limits, and need for human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Potential roles of large language models in production of systematic reviews and meta-analyses', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7708.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7708.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polak2024 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting accurate materials data with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method using conversational LLMs and prompt engineering to extract structured materials-science data from research papers, showing that tailored prompts and conversational interaction can improve extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>M. P. Polak, D. Morgan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Conversational LLM extraction with prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Employs conversational LLMs with engineered prompts (and iterative clarification) to parse paper content and output structured materials data; emphasizes prompt design and interaction to improve fidelity of extracted facts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-text research papers (PDFs) and extracted textual content</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured materials data (tabular fields), i.e., machine-readable data extracted from papers</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Iterative conversational prompting and prompt engineering (few-shot examples, clarification queries)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as a study demonstrating improved accuracy of material data extraction via conversational LLMs and prompt engineering (qualitative and empirical evidence reported in original paper; this review does not reproduce numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The review highlights the general risks: hallucinations, difficulty with figures/tables, need for manual verification, and possible domain adaptation issues.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Potential roles of large language models in production of systematic reviews and meta-analyses', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7708.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7708.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-agent data analyst (arXiv:2402.01386)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent assisted approach that leverages LLMs in coordinated roles to perform qualitative data analysis, illustrating how agents can divide tasks and aggregate results for complex analytic workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Multi-agent assisted LLM data analysis</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses multiple LLM-driven agents to perform different aspects of data analysis (e.g., coding, theme extraction, synthesis) and aggregates their outputs to produce structured analytic products.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Qualitative datasets and textual documents (could include scholarly texts)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Coded qualitative data, themes, synthesized analytic summaries</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Multi-agent orchestration, prompt engineering, iterative consensus/aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned in the review as an approach exploring LLMs as analysts; the review does not report quantitative metrics from the original preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper-level and review-level concerns include reliability, need for human oversight, and limitations in complex problem-solving and longitudinal planning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Potential roles of large language models in production of systematic reviews and meta-analyses', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7708.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7708.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 systematic review eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical evaluation of GPT-4 for screening and data extraction tasks in systematic reviews across languages, assessing recall/accuracy relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GPT-4 screening & extraction evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Applies GPT-4 to title/abstract screening and data extraction tasks from heterogeneous literature to measure its performance against human curators, including multi-language sources.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Peer-reviewed and grey literature (titles, abstracts, full-texts)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Inclusion/exclusion decisions and extracted data fields</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Task-specific prompting for screening and extraction; evaluation-focused prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Mixed peer-reviewed and grey literature collections (as described in the cited study)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall/precision for screening and extraction accuracy compared to human curators (as described in source study)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as an evaluation showing GPT-4's capabilities for screening and extraction across languages; the review notes workload reduction claims but emphasizes need for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Concerns noted include hallucinations, variable reliability across tasks and languages, and requirement for human oversight and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Potential roles of large language models in production of systematic reviews and meta-analyses', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7708.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7708.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT Code Interpreter (analysis/graphing plugin within ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive environment within ChatGPT that can execute code, perform analyses, and generate plots from data extracted or supplied during a session, used as an aid for meta-analysis computations and visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChatGPT Code Interpreter assisted analysis</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Used after LLM-assisted extraction to run statistical analyses, produce plots, and assist with data processing within an interactive ChatGPT session (noted as available to ChatGPT Plus subscribers).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Structured extracted data (CSV/JSON) from studies or user-provided data</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Analytic results, statistical outputs, and graphical figures</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Interactive coding prompts and iterative refinement within the Code Interpreter environment</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as a tool that can assist in analysis and plotting after extraction; no quantitative evaluation provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependence on accurate extracted inputs, data privacy/ethical considerations when uploading sensitive data, and subscription/access constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Potential roles of large language models in production of systematic reviews and meta-analyses', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7708.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7708.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RobotReviewer (automatic risk-of-bias assessment system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated system to assess risk of bias in clinical trials by extracting trial characteristics and mapping them to bias domains, intended to speed up bias assessment with batch processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>I. J. Marshall, J. Kuiper, B. C. Wallace</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2016</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Automates extraction of trial details from texts and predicts domain-level risk-of-bias judgments to assist human reviewers; integrates NLP to populate bias assessment tools.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-text trial reports and extracted text passages (PDF/HTML)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Risk-of-bias judgments and supporting textual evidence (structured annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Rule-based/NLP pipelines rather than interactive prompting (historical system predating modern conversational LLM workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Comparisons to human assessments (agreement metrics) reported in original evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as an automated tool that streamlines bias extraction with improved efficiency; review emphasizes that manual verification remains necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires manual verification; not a full replacement for human judgment; earlier-generation NLP methods may lack deep contextual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Potential roles of large language models in production of systematic reviews and meta-analyses', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Metagpt: Meta programming for multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>A survey on large language model based autonomous agents <em>(Rating: 2)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages <em>(Rating: 2)</em></li>
                <li>Assessing the Ability of ChatGPT to Screen Articles for Systematic Reviews <em>(Rating: 1)</em></li>
                <li>Enhancing title and abstract screening for systematic reviews with GPT-3.5 turbo <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7708",
    "paper_id": "paper-270148668",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT (Meta programming for multi-agent collaborative framework)",
            "brief_description": "A multi-agent, LLM-centered framework for coordinating specialized agents (e.g., retrieval, extraction, analysis, writing) to perform complex, multi-step tasks such as end-to-end systematic review automation; presented as a proof-of-concept demo for multi-agent collaboration driven by LLMs.",
            "citation_title": "Metagpt: Meta programming for multi-agent collaborative framework",
            "mention_or_use": "mention",
            "paper_title": "Metagpt: Meta programming for multi-agent collaborative framework",
            "authors": null,
            "year": 2023,
            "method_name": "MetaGPT",
            "method_description": "Uses an LLM as the core 'controller' that programs and coordinates multiple specialized agents (retrievers, extractors, analyzers, writers). The LLM issues prompts/tasks to agents, ingests their outputs, and iteratively composes higher-level artifacts (extractions, analyses, draft text).",
            "input_type": "Full-text scholarly articles / PDFs and search results (literature corpus)",
            "output_type": "Structured extractions, analysis outputs, and draft manuscript components (multi-step artifacts)",
            "prompting_technique": "Multi-agent prompt orchestration and prompt engineering (meta‑programming), iterative/hierarchical prompting",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited in the paper as a proof-of-concept demo showing feasibility of multi-agent LLM workflows for complex synthesis tasks; no quantitative results provided in this review article.",
            "limitations": "Presented as proof-of-concept only; issues noted in the review include hallucination risk, model knowledge staleness, token/context limits, and need for human verification.",
            "counterpoint": true,
            "uuid": "e7708.0",
            "source_info": {
                "paper_title": "Potential roles of large language models in production of systematic reviews and meta-analyses",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-driven autonomous agents",
            "name_full": "Large language model based autonomous agents for systematic reviews",
            "brief_description": "A class of approaches that instantiate autonomous agents under LLM control to perform literature search, screening, data extraction, synthesis, and drafting in a (semi-)automated pipeline for reviews and meta-analyses.",
            "citation_title": "A survey on large language model based autonomous agents",
            "mention_or_use": "mention",
            "paper_title": "A survey on large language model based autonomous agents",
            "authors": null,
            "year": 2024,
            "method_name": "LLM-driven autonomous agents",
            "method_description": "Frameworks that leverage LLMs to plan, delegate, and iteratively perform tasks via agents (retrieval, screening, extraction, analysis), enabling end-to-end or modular automation of literature synthesis workflows.",
            "input_type": "Search results, abstracts, full-text PDFs, and possibly metadata/structured records",
            "output_type": "Screened study lists, structured data extractions, synthesized summaries, and draft manuscripts",
            "prompting_technique": "Agent orchestration, hierarchical/iterative prompting, prompt engineering; may combine retrieval augmentation",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Reviewed as an emerging direction with proof-of-concept systems; the article cites the promise but no standardized benchmarks reported in this review.",
            "limitations": "Challenges highlighted include updating knowledge, personalization, strategic planning, hallucination, token limits, and need for human oversight.",
            "counterpoint": true,
            "uuid": "e7708.1",
            "source_info": {
                "paper_title": "Potential roles of large language models in production of systematic reviews and meta-analyses",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Polak2024 extraction",
            "name_full": "Extracting accurate materials data with conversational language models and prompt engineering",
            "brief_description": "A method using conversational LLMs and prompt engineering to extract structured materials-science data from research papers, showing that tailored prompts and conversational interaction can improve extraction accuracy.",
            "citation_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "mention_or_use": "mention",
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "authors": "M. P. Polak, D. Morgan",
            "year": 2024,
            "method_name": "Conversational LLM extraction with prompt engineering",
            "method_description": "Employs conversational LLMs with engineered prompts (and iterative clarification) to parse paper content and output structured materials data; emphasizes prompt design and interaction to improve fidelity of extracted facts.",
            "input_type": "Full-text research papers (PDFs) and extracted textual content",
            "output_type": "Structured materials data (tabular fields), i.e., machine-readable data extracted from papers",
            "prompting_technique": "Iterative conversational prompting and prompt engineering (few-shot examples, clarification queries)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as a study demonstrating improved accuracy of material data extraction via conversational LLMs and prompt engineering (qualitative and empirical evidence reported in original paper; this review does not reproduce numbers).",
            "limitations": "The review highlights the general risks: hallucinations, difficulty with figures/tables, need for manual verification, and possible domain adaptation issues.",
            "counterpoint": true,
            "uuid": "e7708.2",
            "source_info": {
                "paper_title": "Potential roles of large language models in production of systematic reviews and meta-analyses",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Multi-agent data analyst (arXiv:2402.01386)",
            "name_full": "Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis",
            "brief_description": "A multi-agent assisted approach that leverages LLMs in coordinated roles to perform qualitative data analysis, illustrating how agents can divide tasks and aggregate results for complex analytic workflows.",
            "citation_title": "Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis",
            "mention_or_use": "mention",
            "paper_title": "Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis",
            "authors": null,
            "year": 2024,
            "method_name": "Multi-agent assisted LLM data analysis",
            "method_description": "Uses multiple LLM-driven agents to perform different aspects of data analysis (e.g., coding, theme extraction, synthesis) and aggregates their outputs to produce structured analytic products.",
            "input_type": "Qualitative datasets and textual documents (could include scholarly texts)",
            "output_type": "Coded qualitative data, themes, synthesized analytic summaries",
            "prompting_technique": "Multi-agent orchestration, prompt engineering, iterative consensus/aggregation",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Mentioned in the review as an approach exploring LLMs as analysts; the review does not report quantitative metrics from the original preprint.",
            "limitations": "Paper-level and review-level concerns include reliability, need for human oversight, and limitations in complex problem-solving and longitudinal planning.",
            "counterpoint": true,
            "uuid": "e7708.3",
            "source_info": {
                "paper_title": "Potential roles of large language models in production of systematic reviews and meta-analyses",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 systematic review eval",
            "name_full": "Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages",
            "brief_description": "An empirical evaluation of GPT-4 for screening and data extraction tasks in systematic reviews across languages, assessing recall/accuracy relative to humans.",
            "citation_title": "Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages",
            "mention_or_use": "mention",
            "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages",
            "authors": null,
            "year": 2024,
            "method_name": "GPT-4 screening & extraction evaluation",
            "method_description": "Applies GPT-4 to title/abstract screening and data extraction tasks from heterogeneous literature to measure its performance against human curators, including multi-language sources.",
            "input_type": "Peer-reviewed and grey literature (titles, abstracts, full-texts)",
            "output_type": "Inclusion/exclusion decisions and extracted data fields",
            "prompting_technique": "Task-specific prompting for screening and extraction; evaluation-focused prompts",
            "model_name": "GPT-4",
            "model_size": null,
            "datasets_used": "Mixed peer-reviewed and grey literature collections (as described in the cited study)",
            "evaluation_metric": "Recall/precision for screening and extraction accuracy compared to human curators (as described in source study)",
            "reported_results": "Cited as an evaluation showing GPT-4's capabilities for screening and extraction across languages; the review notes workload reduction claims but emphasizes need for verification.",
            "limitations": "Concerns noted include hallucinations, variable reliability across tasks and languages, and requirement for human oversight and validation.",
            "counterpoint": true,
            "uuid": "e7708.4",
            "source_info": {
                "paper_title": "Potential roles of large language models in production of systematic reviews and meta-analyses",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChatGPT Code Interpreter",
            "name_full": "ChatGPT Code Interpreter (analysis/graphing plugin within ChatGPT)",
            "brief_description": "An interactive environment within ChatGPT that can execute code, perform analyses, and generate plots from data extracted or supplied during a session, used as an aid for meta-analysis computations and visualization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": null,
            "year": null,
            "method_name": "ChatGPT Code Interpreter assisted analysis",
            "method_description": "Used after LLM-assisted extraction to run statistical analyses, produce plots, and assist with data processing within an interactive ChatGPT session (noted as available to ChatGPT Plus subscribers).",
            "input_type": "Structured extracted data (CSV/JSON) from studies or user-provided data",
            "output_type": "Analytic results, statistical outputs, and graphical figures",
            "prompting_technique": "Interactive coding prompts and iterative refinement within the Code Interpreter environment",
            "model_name": "ChatGPT Code Interpreter",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Mentioned as a tool that can assist in analysis and plotting after extraction; no quantitative evaluation provided in this review.",
            "limitations": "Dependence on accurate extracted inputs, data privacy/ethical considerations when uploading sensitive data, and subscription/access constraints.",
            "counterpoint": true,
            "uuid": "e7708.5",
            "source_info": {
                "paper_title": "Potential roles of large language models in production of systematic reviews and meta-analyses",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RobotReviewer",
            "name_full": "RobotReviewer (automatic risk-of-bias assessment system)",
            "brief_description": "An automated system to assess risk of bias in clinical trials by extracting trial characteristics and mapping them to bias domains, intended to speed up bias assessment with batch processing.",
            "citation_title": "RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials",
            "mention_or_use": "mention",
            "paper_title": "RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials",
            "authors": "I. J. Marshall, J. Kuiper, B. C. Wallace",
            "year": 2016,
            "method_name": "RobotReviewer",
            "method_description": "Automates extraction of trial details from texts and predicts domain-level risk-of-bias judgments to assist human reviewers; integrates NLP to populate bias assessment tools.",
            "input_type": "Full-text trial reports and extracted text passages (PDF/HTML)",
            "output_type": "Risk-of-bias judgments and supporting textual evidence (structured annotations)",
            "prompting_technique": "Rule-based/NLP pipelines rather than interactive prompting (historical system predating modern conversational LLM workflows)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": "Comparisons to human assessments (agreement metrics) reported in original evaluation",
            "reported_results": "Cited as an automated tool that streamlines bias extraction with improved efficiency; review emphasizes that manual verification remains necessary.",
            "limitations": "Requires manual verification; not a full replacement for human judgment; earlier-generation NLP methods may lack deep contextual reasoning.",
            "counterpoint": false,
            "uuid": "e7708.6",
            "source_info": {
                "paper_title": "Potential roles of large language models in production of systematic reviews and meta-analyses",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Metagpt: Meta programming for multi-agent collaborative framework",
            "rating": 2,
            "sanitized_title": "metagpt_meta_programming_for_multiagent_collaborative_framework"
        },
        {
            "paper_title": "A survey on large language model based autonomous agents",
            "rating": 2,
            "sanitized_title": "a_survey_on_large_language_model_based_autonomous_agents"
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages",
            "rating": 2,
            "sanitized_title": "can_large_language_models_replace_humans_in_systematic_reviews_evaluating_gpt4s_efficacy_in_screening_and_extracting_data_from_peerreviewed_and_grey_literature_in_multiple_languages"
        },
        {
            "paper_title": "Assessing the Ability of ChatGPT to Screen Articles for Systematic Reviews",
            "rating": 1,
            "sanitized_title": "assessing_the_ability_of_chatgpt_to_screen_articles_for_systematic_reviews"
        },
        {
            "paper_title": "Enhancing title and abstract screening for systematic reviews with GPT-3.5 turbo",
            "rating": 1,
            "sanitized_title": "enhancing_title_and_abstract_screening_for_systematic_reviews_with_gpt35_turbo"
        }
    ],
    "cost": 0.014261249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Potential roles of large language models in production of systematic reviews and meta-analyses
February 20, 2024</p>
<p>PhDXufei Luo 
Evidence-Based Medicine Center
School of Basic Medical Sciences
Lanzhou University
LanzhouChina. Lanzhou City CN</p>
<p>Evidence-Based Medicine Center
School of Basic Medical Sciences
Lanzhou University
LanzhouChina</p>
<p>World Health Organization Collaboration Center for Guideline Implementation and Knowledge Translation
LanzhouChina</p>
<p>Institute of Health Data Science
Lanzhou University
LanzhouChina</p>
<p>PhDFengxian Chen 
Key Laboratory of Evidence Based Medicine and Knowledge Translation of Gansu Province
Lanzhou University
LanzhouChina</p>
<p>School of Basic Medical Sciences
Research Unit of Evidence-Based Evaluation and Guidelines
Chinese Academy of Medical Sciences (2021RU017)
Lanzhou University
LanzhouChina</p>
<p>School of Information Science &amp; Engineering
LanzhouChina</p>
<p>; Fengxian Chen 
School of Information Science &amp; Engineering
LanzhouChina. Lanzhou City CN</p>
<p>MPHDi Zhu 
School of Public Health
Lanzhou University Lanzhou City CN</p>
<p>School of Public Health
Lanzhou University
LanzhouChina</p>
<p>MPHLing Wang 
School of Public Health
Lanzhou University Lanzhou CN</p>
<p>School of Public Health
Lanzhou University
LanzhouChina</p>
<p>PhDZijun Wang 
Evidence-Based Medicine Center
School of Basic Medical Sciences
Lanzhou University Lanzhou</p>
<p>Evidence-Based Medicine Center
School of Basic Medical Sciences
Lanzhou University
LanzhouChina</p>
<p>World Health Organization Collaboration Center for Guideline Implementation and Knowledge Translation
LanzhouChina</p>
<p>PhDHui Liu 
Evidence-Based Medicine Center
School of Basic Medical Sciences
Lanzhou University Lanzhou</p>
<p>Evidence-Based Medicine Center
School of Basic Medical Sciences
Lanzhou University
LanzhouChina</p>
<p>World Health Organization Collaboration Center for Guideline Implementation and Knowledge Translation
LanzhouChina</p>
<p>Institute of Health Data Science
Lanzhou University
LanzhouChina</p>
<p>Key Laboratory of Evidence Based Medicine and Knowledge Translation of Gansu Province
Lanzhou University
LanzhouChina</p>
<p>School of Basic Medical Sciences
Research Unit of Evidence-Based Evaluation and Guidelines
Chinese Academy of Medical Sciences (2021RU017)
Lanzhou University
LanzhouChina</p>
<p>MPHMeng Lyu 
School of Public Health
Lanzhou University Lanzhou City CN</p>
<p>Institute of Health Data Science
Lanzhou University
LanzhouChina</p>
<p>Key Laboratory of Evidence Based Medicine and Knowledge Translation of Gansu Province
Lanzhou University
LanzhouChina</p>
<p>School of Basic Medical Sciences
Research Unit of Evidence-Based Evaluation and Guidelines
Chinese Academy of Medical Sciences (2021RU017)
Lanzhou University
LanzhouChina</p>
<p>School of Public Health
Lanzhou University
LanzhouChina</p>
<p>MPHYe Wang 
School of Public Health
Lanzhou University Lanzhou City CN</p>
<p>School of Public Health
Lanzhou University
LanzhouChina</p>
<p>PhDQi Wang 
Department of Health Research Methods, Evidence and Impact
Faculty of Health Sciences
McMaster University Hamilton CA</p>
<p>Department of Health Research Methods, Evidence and Impact
Faculty of Health Sciences
McMaster University
HamiltonOntarioCanada</p>
<p>McMaster Health Forum
McMaster University
L8S4L8HamiltonCanada</p>
<p>ProfessorYaolong Chen 
Lanzhou University
Lanzhou CityCN</p>
<p>Evidence-Based Medicine Center
School of Basic Medical Sciences
Lanzhou University
LanzhouChina</p>
<p>World Health Organization Collaboration Center for Guideline Implementation and Knowledge Translation
LanzhouChina</p>
<p>Institute of Health Data Science
Lanzhou University
LanzhouChina</p>
<p>Key Laboratory of Evidence Based Medicine and Knowledge Translation of Gansu Province
Lanzhou University
LanzhouChina</p>
<p>School of Basic Medical Sciences
Research Unit of Evidence-Based Evaluation and Guidelines
Chinese Academy of Medical Sciences (2021RU017)
Lanzhou University
LanzhouChina</p>
<p>Di Zhu
Ling Wang, Zijun Wang, Hui Liu, Meng Lyu, Ye Wang, Qi WangYaolong Chen</p>
<p>Lanzhou University No
199 Donggang West Road</p>
<p>Chengguan District Lanzhou City CN</p>
<p>Evidence-Based Medicine Center
School of Basic Medical Sciences
Lanzhou University
No.199, Donggang West Road, Chengguan District730000LanzhouChina</p>
<p>Potential roles of large language models in production of systematic reviews and meta-analyses
February 20, 202422FEEF4D4FC7B60C97E4305DA733405010.2196/preprints.56780Submitted to: Journal of Medical Internet Researchlarge language modelChatGPTsystematic reviewchatbotmeta-analysis
Large language models (LLMs) like ChatGPT have become widely applied in the field of medical research.In the process of conducting systematic reviews, similar tools can be employed to expedite various steps, including defining clinical questions, literature search, document screening, information extraction, and language refinement, etc, thereby conserving resources and enhancing efficiency.However, when utilizing LLMs, attention should be given to transparent reporting, distinguishing between genuine and false content, and avoiding academic misconduct.This article reviews the potential roles of LLMs in the creation of systematic reviews and meta-analyses, elucidating their advantages, limitations, and future research directions, aiming to provide insights and guidance for authors involved in systematic reviews and meta-analyses.</p>
<p>Table of Contents
Original</p>
<p>Preprint Settings</p>
<p>1) Would you like to publish your submitted manuscript as preprint?Please make my preprint PDF available to anyone at any time (recommended).Please make my preprint PDF available only to logged-in users; I understand that my title and abstract will remain visible to all users.Only make the preprint title and abstract visible.</p>
<p>No, I do not wish to publish my submitted manuscript as a preprint.2) If accepted for publication in a JMIR journal, would you like the PDF to be visible to the public?</p>
<p>Yes, please make my accepted manuscript PDF available to anyone at any time (Recommended).</p>
<p>Yes, but please make my accepted manuscript PDF available only to logged-in users; I understand that the title and abstract will remain v Yes, but only make the title and abstract visible (see Important note, above).I understand that if I later pay to participate in &lt;a href="http</p>
<p>Email address:</p>
<p>Xufei Luo: luoxf22@lzu.edu.cnFengxian Chen: chenfx@lzu.edu.cnDi Zhu: zhudiyx@163.comLing Wang: wangling_working@163.com Zijun Wang: bdwzj_0312@163.comHui Liu: sxyafxlh@163.comMeng Lyu: meng.lyu0908@gmail.comYe Wang: wye2023@lzu.edu.cnQi Wang: wangq87@mcmaster.caYaolong Chen: chevidence@lzu.edu.cn</p>
<p>Word Counts: 2431</p>
<p>Figure : 1 Table : 1 eFigure: 12</p>
<p>Introduction</p>
<p>A systematic review is the result of a systematic and rigorous evaluation of evidence, and a metaanalysis may or may not be a part of it [1].Due to its strict methodology and comprehensive summary of evidence, high-quality systematic reviews are considered the highest level of evidence in the hierarchy of evidence [2].They are positioned at the top of the evidence pyramid [2].</p>
<p>Additionally, high-quality systematic reviews and meta-analyses are often used to support the development of clinical practice guidelines, aid clinical decision-making, and inform healthcare policy formulation [3].Currently, the methods of systematic reviews and meta-analyses are also applied in various disciplines beyond medicine, such as law [4], management [5], economics [6], and have yielded positive results, contributing to the continuous advancement of these fields [7].</p>
<p>The process of conducting systematic reviews demands a substantial investment in terms of time, resources, human effort, and financial capital [8].To expedite the development of systematic reviews and meta-analyses, various (semi)automated tools, such as Covidence, have also come into play [9,10].However, the emergence of large language models (LLMs), particularly Chatbots such as GPT, presents a set of challenges and opportunities in the realm of systematic review and metaanalysis [11].This article conducts a comprehensive review of relevant literature, aiming to investigate the potential for harnessing LLMs to accelerate the production of systematic review and meta-analysis, while also scrutinizing the potential impacts and delineating the crucial steps involved in this process.</p>
<p>The process and challenges of conducting a systematic review and meta-analysis</p>
<p>The procedures and workflows for conducting systematic reviews and meta-analyses are wellestablished.Currently, researchers often refer to the Cochrane Handbooks recommended by the Cochrane Library for intervention or diagnostic reviews [12,13].In addition, some scholars and institutions have also developed detailed guidelines on the steps and methodology for performing systematic reviews and meta-analyses [14][15][16][17].Generally speaking, researchers should take the following steps to produce a high-quality systematic review and meta-analysis: determine the clinical question, register and draft a protocol, set inclusion and exclusion criteria, develop and implement a search strategy, screen literature, extract data from included studies, assess the quality and risk of bias of included studies, analyze and process data, write up the full text, and submit for publication, as illustrated in Figure 1.These different steps contain many sub-tasks, therefore</p>
<p>Luo et al conducting a complete systematic review and meta-analysis requires fairly complex and timeconsuming work.</p>
<p>Figure 1</p>
<p>The process of conducting a systematic review and meta-analysis</p>
<p>Although systematic reviews and meta-analyses have been widely applied and play an important role in developing guidelines and informing clinical decision-making, their production process faces many challenges.One of them is the long production time and large resource requirements.Studies suggest that the average estimated time to complete and publish a systematic review is 67.3 weeks, requiring five researchers and costing around $140,000 [18][19].For some time, (semi-)automated tools utilizing natural language processing and machine learning have accelerated systematic review and meta-analysis production to some extent [20], with studies showing such tools can produce a systematic review and meta-analysis within two weeks [21].However, these tools also have some limitations.First, no single tool can fully accelerate the entire production process of systematic reviews and meta-analyses.Second, these tools cannot process and analyze literature in different languages.Finally, the reliability of results generated by these (semi-)automated tools needs further validation as they are not yet widely adopted.</p>
<p>Large language models in medical research</p>
<p>Chatbots based on LLMs, such as ChatGPT, Google Gemini, and Claud, have become widely applied in medical research.These chatbots prove valuable in tasks ranging from knowledge retrieval, language refinement, content generation, and medical exam preparation to literature assessment.</p>
<p>Research indicates that ChatGPT excels in accuracy, completeness, nuance, and speed when generating responses to clinical inquiries in psychiatry [22].Moreover, LLMs like ChatGPT play a pivotal role in automating the evaluation of medical literature, facilitating the identification of accurately reported research findings [23].Despite their significant contributions, these chatbots are not without limitations.Challenges such as the potential for generating misleading content and susceptibility to academic deception necessitate further scholarly discourse on effective mitigation strategies.Standardized reporting practices may contribute to delineating the applications of</p>
<p>ChatGPT and mitigating research biases [24].</p>
<p>In the process of conducting systematic reviews and meta-analyses, ChatGPT demonstrates significant application potential and promise.Existing studies [11,[25][26][27][28][29][30][31][32] indicate that ChatGPT can play a pivotal role in formulating clinical questions, determining inclusion and exclusion criteria, screening literature, assessing publications, generating meta-analysis code, and assisting in full-text composition, etc.In this context, we will provide a detailed exposition of these capabilities (Table 1).</p>
<p>The potential roles of LLMs in systematic review and meta-analysis</p>
<p>Determine the research topic/question</p>
<p>Determining the clinical question represents the initial and paramount step in the process of conducting systematic reviews and meta-analyses.At this juncture, it is crucial to ascertain whether comparable systematic reviews and meta-analyses have already been published and to delineate the scope of the forthcoming review and meta-analysis.Generally, for interventional systematic reviews, the patient, intervention, comparison, outcome (PICO) framework is considered for defining the scope and objectives of the research question [60].In this context, ChatGPT serves a dual role.On one hand, it expeditiously aids in searching for published systematic reviews and meta-analyses related to the relevant topics (See Figure S1 and S2) [34].On the other hand, it assists in refining the clinical question that needs to be addressed (See Figure S3), facilitating researchers in promptly determining the feasibility of undertaking the proposed study.However, it is important to be cautious of false literature [35].</p>
<p>Register and write a research proposal</p>
<p>The registration and proposal writing process constitutes a pivotal preparatory phase for the conducting of systematic reviews and meta-analyses.Registration enhances research transparency, fosters collaboration among investigators, and mitigates the redundancy of research endeavors.</p>
<p>Drafting a proposal helps in elucidating the research objectives and methods, providing robust support for the smooth execution of the study.For LLMs, generating preliminary registration information and initial proposal content is remarkably convenient and facile (see Figures S4 and S5).</p>
<p>For example, ChatGPT can assist researchers in generating the statistical methods for a research proposal [37].However, considering that LLMs often generate fictitious literature, the content they produce may be inaccurate, thus discernment and validation of the generated content remain essential considerations.</p>
<p>Define inclusion an exclusion criterion</p>
<p>The inclusion and exclusion criteria for systematic review and meta-analyses are instrumental in determining the screening standards for studies.Therefore, strict and detailed inclusion and exclusion criteria contribute to the smooth and high-quality conduct of systematic reviews and meta-analyses.The use of a chatbot based on LLMs can help in establishing the inclusion and exclusion criteria (see Figure S6) [38], however, the inclusion criteria need to be optimized and adjusted according to the specific research objectives, and the exclusion criteria should be based on the foundation of the inclusion criteria.Therefore, manual adjustments and optimizations are also necessary.</p>
<p>Develop a search strategy and conduct searches</p>
<p>ChatGPT can assist in formulating search strategies, using PubMed as an example [40].Researchers can simply list their questions using the PICO framework, and a search strategy can be quickly generated (Figure S1 and S2).Based on the generated search strategy, one method is to copy the strategy into the PubMed search box for direct retrieval [40][41].Another approach involves utilizing the OpenAI application programming interfaces (APIs) to invoke PubMed APIs with the search strategy generated by GPT.This allows for searching the PubMed database, obtaining search results, and applying predetermined inclusion and exclusion criteria.Subsequently, GPT is used to filter the search results, exporting and recording the filtered results in JSON format.This integrated process encompasses search strategy formulation, retrieval, and filtering.However, the direct use of LLMs to generate search strategies and complete the one-stop process of searching and screening may not be mature at present, and poses a significant challenge for generating the PRISMA flowchart.</p>
<p>Therefore, we suggest using LLMs to generate search strategies, which are then optimized and modified by librarians and computer experts (specializing in large language models) before manually searching the databases.Additionally, to use search strategies transparently and reproducibly, detailed prompts should be reported [40,42].thesesearch strategies also need validation, refinement, and modification.</p>
<p>Screen the literature</p>
<p>Literature screening is one of the most time-consuming steps in the creation of systematic reviews and meta-analyses.Prior to the advent of ChatGPT, there were already many (semi) automated tools available for literature screening, such as Coevidence, EPPI-Reviewer, DistillerSR, and others [39].With the emergence of ChatGPT, researchers can now train the model based on pre-defined inclusion criteria.Subsequently, they can utilize ChatGPT to automatically screen records retrieved from databases, obtaining the filtered results .Previous studies suggested that utilizing ChatGPT in the literature selection process for meta-analysis substantially diminishes the workload while preserving a recall rate on par with manual curation [28,[44][45][46][47].</p>
<p>Extract the data</p>
<p>Data extraction involves obtaining information from primary studies and serves as a primary source for systematic reviews and meta-analyses.Generally, when conducting systematic reviews and meta-analyses, we need to extract basic information from the original studies, such as publication date, country of conduct, and the journal of publication.Additionally, characteristics of the population, such as patient samples, age, gender, and outcome data, including event occurrences, mean change values, and total sample size, are also extracted.Currently, tools based on natural language processing and LLMs, such as ChatGPT and Claude, demonstrate high accuracy in extracting information from Portable Document Format (PDF) documents (Figure S7) [47][48][49][50].</p>
<p>However, it is important to note that despite the promising capabilities of these tools, manual verification remains a necessary step in the data extraction process when utilizing AI tools [61].</p>
<p>Using large language models to extract data can help avoid random errors; however, caution is still required when extracting data from figures or tables [47][48][49][50].</p>
<p>Assess the risk of bias</p>
<p>Assessing the bias of risk involves evaluating the internal validity of studies included in research.For randomized controlled trials, we typically use tools like Risk of Bias (RoB) [62] or RoB 2 tools [63],</p>
<p>with an estimated review time of 10-15 minutes per trial.However, automated tools such as RobotReviewer can streamline the extraction and evaluation process in batches [51][52][53], improving efficiency-though manual verification is still necessary.Additionally, chatbots based on LLMs can aid in risk of bias assessment (see Figure S8), and studies indicate that their accuracy is comparable to human evaluations [23].</p>
<p>Analyze the data/meta-analysis</p>
<p>Data analysis serves as the source of systematic review results, typically encompassing basic information and outcome findings.Meta-analysis may be one outcome, along with potential components like subgroup analysis, sensitivity analysis, meta-regression, and detection of publication bias.Numerous software options are available to facilitate these data analyses, including STATA, RevMan, Rstudio, and others [43].Currently, it appears that chatbots based on LLMs may not fully execute data analysis independently, but they can extract relevant information.Subsequently, one can employ corresponding software for comprehensive data analysis.Alternatively, after extracting information with chatbots, the ChatGPT Code Interpreter can assist in analysis and generating graphical results, contingent upon being a ChatGPT Plus subscriber.Moreover, LLM markedly accelerates the data analysis process, empowering researchers to handle larger datasets with greater efficacy [54].</p>
<p>Draft the full manuscript</p>
<p>The complete drafting of systematic reviews and meta-analyses should adhere to the PRISMA reporting guidelines [64].It is not advisable to use chatbots like ChatGPT for article composition.On one hand, the accuracy and integrity of content generated by GPT require human verification.On the other hand, various research types and journals have different requirements for full-text articles, making it challenging to achieve uniformity in generated content.However, utilizing tools like GPT for language refinement and adjusting content logic can be considered to enhance the quality and readability of the article [33,55].It is important to declare the use of GPT-related tools in the methods, acknowledgments, or appendices to ensure transparency [24,65].</p>
<p>Submit and publish</p>
<p>Submission and publication represent the final steps in the process of conducting systematic reviews and meta-analyses, aside from subsequent updates.At this stage, the potential role of tools related to LLMs can assist authors in recommending suitable journals (Figure S9).They might also aid in crafting components such as cover letters and highlights [59].However, it is imperative to emphasize that content generated by these tools requires manual verification to ensure the accuracy of the content, and all authors should be accountable for the content generated by LLMs.</p>
<p>Benefits and drawbacks of using large language models</p>
<p>Systematic reviews and meta-analyses are crucial evidence types that support the development of guidelines [3].The benefits of employing LLM chatbots in the production of systematic reviews and meta-analyses include increased speed, such as in the stages of evidence searching, data extraction, and assessment of bias risk; they can also enhance accuracy by reducing human errors, for instance, in extracting essential information and pooling data.However, there are also drawbacks, such as the potential for generating hallucinations, the reliability of the models requires human verification and the entire systematic review process is not replicable.Moreover, when interacting with large language model chatbots, it is important to manage data privacy; when using LLMs to analyze data, especially when it involves patient privacy, ethical approval and management must be properly addressed.</p>
<p>Challenges and solutions</p>
<p>While LLMs can assist in accelerating the production of systematic reviews and meta-analyses in some steps, enhancing accuracy and transparency, and saving resources, they also face several challenges.For instance, LLMs cannot promptly update their versions and information.Currently, ChatGPT 3.5 is trained on data from around 2021.Limitations such as the length of prompts and token constraints, as well as restrictions related to context associations, may potentially impact overall results and user experience [25].Although LLM-based autonomous agents have made strides in systematic reviews and meta-analyses, LLMs still face numerous challenges due to issues related to personalization, updating knowledge, strategic planning, and complex problem-solving.To better facilitate the utilization of tools such as ChatGPT in systematic reviews and meta-analyses, we believe that, first and foremost, authors should understand the scope and scenarios for applying ChatGPT, clearly defining which steps can benefit from these tools.Secondly, for researchers, collaboration with computer scientists and artificial intelligence engineers is crucial to optimize the prompts and develop integrated tools based on LLMs, such as web applications.These tools can assist in seamless transitions between different tasks in the systematic review process.Lastly, for journal editors, collaboration with authors and reviewers is essential to adhere to reporting and ethical principles associated with the use of GPT [24,68].This collaboration aims to promote transparency and integrity, and to prevent indiscriminate overuse in the application of LLMs in systematic reviews and meta-analyses.</p>
<p>Future perspectives and conclusion</p>
<p>The emergence of LLMs could have a significant impact on the production of systematic reviews and meta-analyses.In this process, the application of chatbots like ChatGPT have the potential to speed up certain steps, such as literature screening, data extraction, and bias risk assessment-processes that typically consume a considerable amount of time.However, it is important to note that if artificial intelligence methods such as GPT are employed in the performing of systematic reviews, disclosure and declaration are essential.This includes specifying the AI tools utilized, their roles, and the areas of application within the review process, etc [24].In this context, developing a reporting guideline is warranted to guide the application of LLM tools in systematic reviews and metaanalyses.While PRISMA 2020 briefly addresses the use of automation technologies, its coverage is limited to steps such as screening, and lacks comprehensive guidance on the broader spectrum of</p>
<p>Manuscript ....................................................................................................................................................................... Supplementary Files ..................................................................................................................................................................... Figures ......................................................................................................................................................................................... Figure 1 ...................................................................................................................................................................................... Multimedia Appendixes ................................................................................................................................................................. Multimedia Appendix 1 .................................................................................................................................................................. Multimedia Appendix 2 .................................................................................................................................................................. Multimedia Appendix 3 .................................................................................................................................................................. Multimedia Appendix 4 .................................................................................................................................................................. Multimedia Appendix 5 .................................................................................................................................................................. Multimedia Appendix 6 .................................................................................................................................................................. Multimedia Appendix 7 .................................................................................................................................................................. Multimedia Appendix 8 .................................................................................................................................................................. Multimedia Appendix 9 ..................................................................................................................................................................</p>
<p>The</p>
<p>development of LLM-driven autonomous agents adept at systematic reviews and metaanalyses warrants exploration [66].The use of LLMs as centrally controlled intelligent agents encompasses the ability to handle precise literature screening, extract and analyze complex data, and assist in manuscript composition, as demonstrated by proof-of-concept demos like MetaGPT [67].Moreover, as the use of LLMs continues to grow ， ensuring the accuracy of information provided in systematic reviews becomes a significant challenge, particularly if LLMs are indiscriminately overused.</p>
<p>AbbreviationsAPI: Application Programming Interfaces ChatGPT: Chat Generative Pre-trained Transformer LLM: Large language model PDF: Portable Document Format PICO: Population, Intervention, Comparison, Outcome PRISMA: Preferred Reporting Items for Systematic reviews and Meta-Analyses RoB: Risk of Bias</p>
<p>Table 1 The possible functions of chatbots in the creation of systematic reviews and meta- analyses encompass separate stages.
1No.TasksPotential rolesReferencesand applicationsteps of chatbotsDeterminethe Identify previouslyresearchpublishedtopic/questionsystematic reviewsand meta-analyseson the same topic. Assistin1determining rationale for the theresearch question. Clarify the PICO(Population,Intervention,Comparison,Outcome) question.
[11,[36][37][35]2Register and write a research proposal  Generate preliminary, unverified[11,[36][37]https://preprints.jmir.org/preprint/56780[unpublished, peer-reviewed preprint]</p>
<p>https://preprints.jmir.org/preprint/56780[unpublished, peer-reviewed preprint]
AcknowledgementsWe would like to thank ChatGPT 3.5 designed by OpenAI for its assistance in language editing.We take the ultimate responsibility for the content of this publication.Data availability statementNo specific data collected for the above manuscript.Patient and Public InvolvementIt was not appropriate or possible to involve patients or the public in the design, or conduct, or reporting, or dissemination plans of our research.Ethical approvalNot applicable.Sources of fundingNo funding.Author contributionsConflicts of interest disclosureNo potential competing interest was reported by the author(s).GuarantorYaolong Chen (corresponding author) takes full responsibility for the work and/or the conduct of the study, has access to the data, and controls the decision to publish.
How to Conduct a Systematic Review: A Narrative Literature Review. Cureus. N Jahan, S Naveed, M Zeshan, 10.7759/cureus.86427924252PMC51379942016 Nov 48e864</p>
<p>Hierarchy of Evidence Within the Medical Literature. S S Wallace, G Barak, G Truong, M W Parker, 10.1542/hpeds.2022-00669035909178Hosp Pediatr. 2022 Aug 112</p>
<p>Institute of Medicine (US) Committee on Standards for Developing Trustworthy Clinical Practice Guidelines. </p>
<p>R Graham, M Mancher, D Miller Wolman, 10.17226/13058Clinical Practice Guidelines We Can Trust. Washington (DC)National Academies Press2011</p>
<p>Anchoring effect in legal decision-making: A meta-analysis. Law Hum Behav. P Bystranowski, B Janik, M Próchnicki, P Skórska, 10.1037/lhb0000438337347462021 Feb45</p>
<p>A review and evaluation of meta-analysis practices in management research. I Geyskens, R Krishnan, J Steenkamp, Journal of Management. 3522009</p>
<p>Meta-analysis of economic evaluation studies: data harmonisation and methodological issues. B S Bagepally, U Chaikledkaew, N Chaiyakunapruk, J Attia, A Thakkinstian, 10.1186/s12913-022-07595-135168619PMC8845252BMC Health Serv Res. 2212022022 Feb 15</p>
<p>Meta-analysis and the science of research synthesis. J Gurevitch, J Koricheva, S Nakagawa, G Stewart, 10.1038/nature2575329517004Nature. 55576952018 Mar 7</p>
<p>How to conduct systematic reviews more expeditiously? Syst Rev. A Tsertsvadze, Y F Chen, D Moher, P Sutcliffe, N Mccarthy, 10.1186/s13643-015-0147-726563648PMC46435002015 Nov 124160</p>
<p>Systematic review automation tools improve efficiency but lack of knowledge impedes their adoption: a survey. A M Scott, C Forbes, J Clark, M Carter, P Glasziou, Z Munn, 10.1016/j.jclinepi.2021.06.03034242757J Clin Epidemiol. 1382021 Oct. 2021 Jul 7</p>
<p>Tools to support the automation of systematic reviews: a scoping review. H Khalil, D Ameen, A Zarnegar, 10.1016/j.jclinepi.2021.12.00534896236J Clin Epidemiol. 1442022 Apr. 2021 Dec 8</p>
<p>Are ChatGPT and large language models "the answer" to bringing us closer to systematic review automation? Syst Rev. R Qureshi, D Shaughnessy, Kar Gill, K A Robinson, T Li, E Agai, 10.1186/s13643-023-02243-z37120563PMC101484732023 Apr 291272</p>
<p>Cochrane Handbook for Systematic Reviews of Interventions version. Jpt Higgins, J Thomas, J Chandler, M Cumpston, T Li, M J Page, V A Welch, August 2023. 2023Cochrane6</p>
<p>Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy. Version 2.0 (updated. J J Deeks, P M Bossuyt, M M Leeflang, Y Takwoingi, July 2023. 2023Cochrane</p>
<p>Guidance on Conducting a Systematic Literature Review. Y Xiao, M Watson, 10.1177/0739456X17723971Journal of Planning Education and Research. 3912019</p>
<p>A 24-step guide on how to design, conduct, and successfully publish a systematic review and meta-analysis in medical research. T Muka, M Glisic, J Milic, S Verhoog, J Bohlius, W Bramer, R Chowdhury, O H Franco, 10.1007/s10654-019-00576-531720912Eur J Epidemiol. 3512020 Jan. 2019 Nov 13</p>
<p>How to Do a Systematic Review: A Best Practice Guide for Conducting and Reporting Narrative Reviews, Meta-Analyses, and Meta-Syntheses. A P Siddaway, A M Wood, L V Hedges, 10.1146/annurev-psych-010418-10280330089228Annu Rev Psychol. 702019 Jan 4. 2018 Aug 8</p>
<p>A step by step guide for conducting a systematic review and meta-analysis with simulation data. G M Tawfik, Kas Dila, Myf Mohamed, Dnh Tam, N D Kien, A M Ahmed, N T Huy, 10.1186/s41182-019-0165-631388330PMC6670166Trop Med Health. 47462019 Aug 1</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry. R Borah, A W Brown, P L Capers, K A Kaiser, 10.1136/bmjopen-2016-01254528242767PMC5337708BMJ Open. 72e0125452017 Feb 27</p>
<p>The significant cost of systematic reviews and meta-analyses: A call for greater involvement of machine learning to assess the promise of clinical trials. M Michelson, K Reuter, 10.1016/j.conctc.2019.100443Contemp Clin Trials Commun. 161004432019 Aug 25</p>
<p>Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. I J Marshall, B C Wallace, 10.1186/s13643-019-1074-931296265PMC6621996Syst Rev. 811632019 Jul 11</p>
<p>A full systematic review was completed in 2 weeks using automation tools: a case study. J Clark, P Glasziou, Del Mar, C Bannach-Brown, A Stehlik, P Scott, A M , 10.1016/j.jclinepi.2020.01.00832004673J Clin Epidemiol. 1212020 May. 2020 Jan 28</p>
<p>The performance of ChatGPT in generating answers to clinical questions in psychiatry: a two-layer assessment. J J Luykx, F Gerritse, P C Habets, C H Vinkers, 10.1002/wps.2114537713576PMC10503909World Psychiatry. 2232023 Oct</p>
<p>Comparative study of ChatGPT and human evaluators on the assessment of medical literature according to recognised reporting standards. R H Roberts, S R Ali, H A Hutchings, T D Dobbs, I S Whitaker, 10.1136/bmjhci-2023-10083037827724PMC10583079BMJ Health Care Inform. 301e1008302023 Oct</p>
<p>The use of ChatGPT in medical research: do we need a reporting guideline?. X Luo, J Estill, Y Chen, 10.1097/JS9.000000000000073737707517Int J Surg. 2023 Sep 14</p>
<p>Harnessing the Power of ChatGPT for Automating Systematic Review Process: Methodology, Case Study, Limitations, and Future Directions. A Alshami, M Elsayed, E Ali, Aee Eltoukhy, T Zayed, 10.3390/systems11070351Systems. 1173512023</p>
<p>Application ChatGPT in conducting systematic reviews and meta-analyses. S A Mahuli, A Rai, A V Mahuli, A Kumar, 10.1038/s41415-023-6132-y37500847Br Dent J. 23522023 Jul</p>
<p>Artificial intelligence in systematic reviews: promising when appropriately used. Shb Van Dijk, Mgj Brusse-Keizer, C C Bucsán, J Van Der Palen, Cjm Doggen, A Lenferink, 10.1136/bmjopen-2023-07225437419641PMC10335470BMJ Open. 137e0722542023 Jul 7</p>
<p>Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. Q Khraisha, S Put, J Kappenberg, A Warraitch, K Hadfield, 10.1002/jrsm.171538484744Res Synth Methods. 2024 Mar 14</p>
<p>The Use of Generative AI for Scientific Literature Searches for Systematic Reviews: ChatGPT and Microsoft Bing AI Performance Evaluation. Y N Gwon, J H Kim, H S Chung, E J Jung, J Chun, S Lee, S R Shim, 10.2196/51187JMIR Med Inform. 12e511872024</p>
<p>Using ChatGPT and other forms of generative AI in systematic reviews: Challenges and opportunities. M M Hossain, 10.1016/j.jmir.2023.11.00538040497J Med Imaging Radiat Sci. 5512024 Mar. 2023 Nov 30</p>
<p>Cocreating an Automated mHealth Apps Systematic Review Process With Generative AI: Design Science Research Approach. G Giunti, C P Doherty, 10.2196/4894938345839PMC10897815JMIR Med Educ. 10e489492024 Feb 12</p>
<p>Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation. A J Nashwan, J H Jaradat, 10.7759/cureus.4302337674957PMC10478591Cureus. 158e430232023 Aug 6</p>
<p>The role of ChatGPT in scientific communication: writing better scientific review articles. J Huang, M Tan, 37168339PMC10164801Am J Cancer Res. 1342023 Apr 15</p>
<p>Methodological insights into ChatGPT's screening performance in systematic reviews. M Issaiy, H Ghanaati, S Kolahi, M Shakiba, A H Jalali, D Zarei, S Kazemian, M A Avanaki, K Firouznia, 10.1186/s12874-024-02203-838539117PMC10976661BMC Med Res Methodol. 241782024 Mar 27</p>
<p>Can ChatGPT Accurately Answer a PICOT Question? Assessing AI Response to a Clinical Question. C Branum, M Schiavenato, 10.1097/NNE.000000000000143637130197Nurse Educ. 4852023 Sep-Oct 01. 2023 Apr 28</p>
<p>Can ChatGPT draft a research article? An example of population-level vaccine effectiveness analysis. C Macdonald, D Adeloye, A Sheikh, I Rudan, 10.7189/jogh.13.0100336798998PMC9936200J Glob Health. 1310032023 Feb 17</p>
<p>Using ChatGPT to Develop the Statistical Analysis Plan for a Randomized Controlled Trial: A Case Report. Richard Evans, Antonio Pozzi, 10.21203/rs.3.rs-3433956/v117 October 2023PREPRINT (Version 1) available at Research Square</p>
<p>How AI is being used to accelerate clinical trials. M Hutson, 10.1038/d41586-024-00753-x38480968Nature. 62780032024 Mar</p>
<p>Software tools for literature screening in systematic reviews in biomedical research. S Van Der Mierden, K Tsaioun, A Bleich, Chc Leenaars, 10.14573/altex.190213131113000ALTEX. 3632019. 2019 May 16</p>
<p>Can ChatGPT write a good Boolean query for systematic review literature search. S Wang, H Scells, B Koopman, G Zuccon, online February 9, 2023. May 21, 2024Published</p>
<p>The Utility of Artificial Intelligence for Systematic Reviews and Boolean Query Formulation and Translation. Plast Reconstr Surg Glob Open. L Alaniz, C Vu, M J Pfaff, 10.1097/GOX.000000000000533937908326PMC106155382023 Oct 3011e5339</p>
<p>Development of search strategies for systematic reviews in health using ChatGPT: a critical analysis. N S Guimarães, J V Joviano-Santos, M G Reis, Rrm Chaves, 10.1186/s12967-023-04371-538167166PMC10759630J Transl Med. 2212024. 2024 Jan 2Observatory of Epidemiology, Nutrition, Health Research (OPENS). Published</p>
<p>Self-learning software tools for data analysis in metaanalysis. T P Tantry, H Karanth, P K Shetty, D Kadam, 10.4097/kja.2108033677944PMC8497909Korean J Anesthesiol. 7452021 Oct. 2021 Mar 8</p>
<p>Utilizing ChatGPT to select literature for meta-analysis shows workload reduction while maintaining a similar recall level as manual curation. Xiangming Cai, Yuanming Geng, Yiming Du, Bart Westerman, Duolao Wang, Chiyuan Ma, Juan J , Garcia Vallejo, org/10.1101/2023.09.06.23295072</p>
<p>Assessing the Ability of ChatGPT to Screen Articles for Systematic Reviews. Eugene Syriani, Istvan David, Gauransh Kumar, 10.48550/arXiv.2307.06464arXiv:2307.06464</p>
<p>Enhancing title and abstract screening for systematic reviews with GPT-3.5 turbo. Kohandel Gargari, O Mahmoudi, M H Hajisafarali, M Samiee, R , 10.1136/bmjebm-2023-11267837989538PMC10850650BMJ Evid Based Med. 2912024 Jan 19</p>
<p>Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study. E Guo, M Gupta, J Deng, Y J Park, M Paget, C Naugler, 10.2196/4899638214966PMC10818236J Med Internet Res. 26e489962024 Jan 12</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, 10.1038/s41467-024-45914-838383556PMC10882009Nat Commun. 15115692024 Feb 21</p>
<p>A Critical Assessment of Large Language Models for Systematic Reviews: Utilizing ChatGPT for Complex Data Extraction. Hesam Mahmoudi, Doris Chang, Lee Hannah And Ghaffarzadegan, Navid Jalali, Mohammad S , 10.2139/ssrn.4797024SSRN. April 17, 2024</p>
<p>How good are large language models for automated data extraction from randomized trials?. Zhuanlan Sun, Ruilin Zhang, Suhail A Doi, Luis Furuya-Kanamori, Tianqi Yu, Lifeng Lin, Chang Xu, 10.1101/2024.02.20.24303083medRxiv 2024.02.20.24303083</p>
<p>RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials. I J Marshall, J Kuiper, B C Wallace, 10.1093/jamia/ocv04426104742PMC4713900J Am Med Inform Assoc. 2312016 Jan. 2015 Jun 22</p>
<p>Assessing the Risk of Bias in Randomized Clinical Trials With Large Language Models. H Lai, L Mm; Ge, M Sun, B Pan, J Huang, L Hou, Q Yang, J Liu, J Liu, Z Ye, D Xia, W Zhao, X Wang, M Liu, J R Talukdar, J Tian, K Yang, J Estill, 10.1001/jamanetworkopen.2024.12687JAMA Network Open. 75e24126872024</p>
<p>ChatGPT for assessing risk of bias of randomized trials using the RoB 2.0 tool: A methods study. Tyler Pitre, Tanvir Jassal, Mahnoor Jhalok Ronjan Talukdar, Michael Shahab, Dena Ling, Zeraatkar, 10.1101/2023.11.19.23298727medRxiv 2023.11.19.23298727</p>
<p>Zeeshan Rasheed, Muhammad Waseem, Aakash Ahmad, Kai-Kristian Kemell, Wang Xiaofeng, Anh Nguyen Duc, Pekka Abrahamsson, 10.48550/arXiv.2402.01386arXiv:2402.01386Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis. </p>
<p>Using ChatGPT for language editing in scientific articles. S G Kim, 10.1186/s40902-023-00381-x36882591PMC9992464Maxillofac Plast Reconstr Surg. 451132023 Mar 8</p>
<p>Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers. C A Gao, F M Howard, N S Markov, E C Dyer, S Ramesh, Y Luo, A T Pearson, 10.1038/s41746-023-00819-637100871PMC10133283NPJ Digit Med. 61752023 Apr 26</p>
<p>ChatGPT and large language model (LLM) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine. J K Kim, M Chua, M Rickard, A Lorenzo, 10.1016/j.jpurol.2023.05.01837328321J Pediatr Urol. 1952023 Oct. 2023 Jun 2</p>
<p>Evaluation of Large Language Model Performance and Reliability for Citations and References in Scholarly Writing: Cross-Disciplinary Study. J Mugaanyi, L Cai, S Cheng, C Lu, J Huang, 10.2196/5293538578685PMC11031695J Med Internet Res. 26e529352024</p>
<p>Will ChatGPT bring a New Paradigm to HR World? A Critical Opinion Article. I F Nuzula, M M Amri, Journal of Management Studies and Development. 202302</p>
<p>The impact of patient, intervention, comparison, outcome (PICO) as a search strategy tool on literature search quality: a systematic review. M B Eriksen, T F Frandsen, 10.5195/jmla.2018.34530271283PMC6148624J Med Libr Assoc. 10642018 Oct. 2018 Oct 1</p>
<p>I tested how well ChatGPT can pull data out of messy PDFs. Brandon Roberts, </p>
<p>The Cochrane Collaboration's tool for assessing risk of bias in randomised trials. J P Higgins, D G Altman, P C Gøtzsche, P Jüni, D Moher, A D Oxman, J Savovic, K F Schulz, L Weeks, J A Sterne, 10.1136/bmj.d592822008217PMC3196245BMJ. 34359282011 Oct 18Cochrane Bias Methods Group; Cochrane Statistical Methods Group</p>
<p>RoB 2: a revised tool for assessing risk of bias in randomised trials. Jac Sterne, J Savović, M J Page, R G Elbers, N S Blencowe, I Boutron, C J Cates, H Y Cheng, M S Corbett, S M Eldridge, J R Emberson, M A Hernán, S Hopewell, A Hróbjartsson, D R Junqueira, P Jüni, J J Kirkham, T Lasserson, T Li, A Mcaleenan, B C Reeves, S Shepperd, I Shrier, L A Stewart, K Tilling, I R White, P F Whiting, Jpt Higgins, 10.1136/bmj.l489831462531BMJ. 36648982019 Aug 28</p>
<p>The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, I Boutron, T C Hoffmann, C D Mulrow, L Shamseer, J M Tetzlaff, E A Akl, S E Brennan, R Chou, J Glanville, J M Grimshaw, A Hróbjartsson, M M Lalu, T Li, E W Loder, E Mayo-Wilson, S Mcdonald, L A Mcguinness, L A Stewart, J Thomas, A C Tricco, V A Welch, P Whiting, D Moher, 10.1136/bmj.n7133782057PMC8005924BMJ. 372712021 Mar 29</p>
<p>Ethics: disclose use of AI in scientific manuscripts. A Gaggioli, 10.1038/d41586-023-00381-x36788370Nature. 61479484132023 Feb</p>
<p>A survey on large language model based autonomous agents. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J Wen, 10.1007/s11704-024-40231-1Front Comput Sci. 181863452024</p>
<p>Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Chenyu Ran, Lingfeng Xiao, Chenglin Wu, Jürgen Schmidhuber. Metagpt: Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Reporting Use of AI in Research and Scholarly Publication-JAMA Network Guidance. A Flanagin, R Pirracchio, R Khera, M Berkwits, Y Hswen, K Bibbins-Domingo, 10.1001/jama.2024.347138451540JAMA. 331132024 Apr 2</p>
<p>The inclusion and exclusion criteria for a systematic review and meta-analysis on exercise therapy for osteoarthritis based on GPT-4. Luo et al Using ChatGPT 4 to assist in generating PubMed search strategies for assessing systematic reviews. Using GPT-4 to assist in selecting submitted journals. unpublished, peer-reviewed preprint</p>            </div>
        </div>

    </div>
</body>
</html>
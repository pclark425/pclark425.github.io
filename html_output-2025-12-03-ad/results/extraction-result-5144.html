<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5144 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5144</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5144</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-263831328</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.06083v1.pdf" target="_blank">Transformers and Large Language Models for Chemistry and Drug Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology. In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration. The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language. A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language. As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5144.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5144.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive language model used via fine-tuning and in-context learning to perform low-data discovery tasks in chemistry, including inverse design and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is GPT-3 all you need for low-data discovery in chemistry?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive Transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large web-scale text corpora; in chemistry use-cases fine-tuning or in-context examples applied to molecular tasks (review does not specify exact chemical pretraining corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>chemistry / materials discovery; inverse molecular design and property prediction (low-data regimes)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning and in-context learning (few-shot prompting) to produce molecular candidates or predict properties</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Typically SMILES/SELFIES or textual descriptions (review: models used in textual molecular representations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in review for GPT-3 experiments; generally tasks evaluated by predictive accuracy on property regression and success in inverse design</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Review reports that Jablonka et al. found GPT-3 (via fine-tuning and in-context learning) can solve various chemistry and materials tasks in limited-data settings, sometimes matching or outperforming specialized methods for property prediction and inverse design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Reported to perform on par with or sometimes better than specialized, heavily engineered algorithms in low-data settings (per Jablonka et al.), especially when using in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Review notes typical LLM issues (hallucinations, lack of grounded chemical validity) and that details like synthesizability and domain-specific calibration remain challenges; specific GPT-3 methodological limitations not fully detailed in review.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5144.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5144.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-based BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT models used for Bayesian Optimization with in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of GPT-family models to perform regression with calibrated uncertainty enabling Bayesian optimization for catalyst and molecular optimization from synthesis-procedure text inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian Optimization of Catalysts With In-context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT (unspecified family/version)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive Transformer (decoder-only) used with in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained language model weights; regression performed via in-context prompting on task-specific examples (review does not list chemical training corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>catalysis and molecular optimization (chemistry/materials)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context learning to perform uncertainty-aware regression; integrated into Bayesian Optimization loop</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual synthesis procedures mapped to property predictions; molecular candidates implied to be derived/optimized (representation not strictly specified in review)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in review; BO success criteria (e.g., finding improved catalytic performance) implied</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Ramos et al. demonstrated BO workflows using GPT models for catalyst and molecular optimization, mapping synthesis-procedure text directly into property space and enabling optimization without feature engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>This approach obviates feature engineering required by classic BO and structured regression, enabling BO from textual inputs; review implies improved practicality but does not provide quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Uncertainty calibration and robustness are challenging for LLMs; review notes general LLM propensity to hallucinate and that grounding with tools/data is important for reliability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5144.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5144.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT: Molecular Generation Using a Transformer-Decoder Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-decoder based generative model trained to produce molecular structures (typically as SMILES) for de novo molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolGPT: Molecular Generation Using a Transformer-Decoder Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-decoder (autoregressive) generative model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on molecular string representations (SMILES) — review references MolGPT among SMILES-based generative transformer approaches but does not list the exact training dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular design (drug discovery / chemical space exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive generation of SMILES strings from learned token distribution; typical generative sampling strategies (review does not detail specifics)</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES (linear molecular string representations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Review does not list specific metrics for MolGPT; typical metrics include validity, uniqueness, novelty, and property-based objectives</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reviewed as an example of transformer-decoder models successfully applied to molecular generation using SMILES; specifics of performance are not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned among other SMILES-based generative models; review notes that SMILES-based generators are easy to train but can produce invalid molecules unless robustness measures (e.g., SELFIES) are used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SMILES fragility can yield invalid molecules; synthesizability and representation limits for macromolecules/materials remain concerns; review does not present MolGPT-specific failure cases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5144.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5144.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flam-Shepherd-3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models for direct 3D structure generation (Flam-Shepherd & Aspuru-Guzik)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Language models that generate three-dimensional molecular/material/protein structures directly as XYZ/CIF/PDB files, overcoming limitations of string-based representations for macromolecules and materials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Language-model-based 3D generator (unnamed in review)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer / language-model formulation adapted to 3D coordinate generation (sequence generation of 3D tokens/files)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>3D structural files (XYZ/CIF/PDB) for crystals, proteins, and molecules — review mentions training on formats used for crystals, proteins, etc., but does not give dataset names</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>materials, macromolecules, protein binding site design, and molecular discovery requiring explicit 3D structures</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive language-model generation of textual 3D file formats (direct generation of atomic positions and other metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>3D coordinate file formats (XYZ, CIF, PDB)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Review reports comparable performance to expert-designed state-of-the-art graph-based generation algorithms; exact metrics not listed here</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Flam-Shepherd & Aspuru-Guzik's approach can directly generate 3D structures and shows performance comparable to state-of-the-art graph-based generative models while addressing representation limits of SMILES/SELFIES for large/periodic/macromolecular systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared favorably to graph-based generation methods on the tasks reviewed (per the paper's claim); advantage is broader applicability to crystals, proteins, and materials where string representations fail.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Review notes general challenges include model grounding, chemical validity, and ensuring physically realistic geometries; specifics of failure modes not detailed in review.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5144.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5144.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Transformer (Born & Manica)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that combines sequence regression and autoregressive generation within a single transformer architecture, enabling simultaneous property prediction and molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer combining regression head and generative decoder (sequence-to-sequence / multi-task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecular language data (SMILES/SELFIES) paired with property/regression targets — review does not specify exact datasets</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecular generation conditioned on properties (drug discovery / molecular design)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multi-task training to perform sequence regression and conditional sequence generation (can generate molecules with desired property values)</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES/other molecular string representations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in review; generally evaluated on ability to generate molecules matching target properties and standard generative validity/novelty metrics</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Presented as a promising approach enabling concurrent regression and generation for molecular language modelling, facilitating property-conditioned generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Offers a unified approach versus separate property predictors and generators; review does not provide quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Challenges include ensuring generated molecules are synthesizable and valid; review does not provide paper-specific failure analyses.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5144.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5144.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-DeNovo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Transformer-based Generative Model for De Novo Molecular Design (Wang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based generative model designed for de novo molecular generation to explore chemical space for drug discovery and related applications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Transformer-based Generative Model for De Novo Molecular Design.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based generative model (Wang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer generative architecture (likely decoder-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecular string representations (SMILES) — review references this among transformer generative work but does not give dataset specifics</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular design (drug discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive generation of molecular strings using transformer architecture; likely uses sampling strategies for candidate generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES or similar molecular string formats</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in review; typical evaluation would include validity, novelty, drug-likeness, and property optimization</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of transformer generative models applied successfully to de novo molecular design; review does not detail quantitative outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Reviewed within the context of transformer generative methods; no explicit performance comparison reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>General limitations around string representations (invalid molecules), synthesizability, and applicability to non-small-molecule domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5144.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5144.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C5T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>C5T5: Controllable Generation of Organic Molecules with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based system enabling controllable generation of organic molecules conditioned on desired attributes or constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>C5T5: Controllable Generation of Organic Molecules with Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>C5T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer generative model with controllable conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecular strings and associated attribute labels (review does not specify datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>organic molecule generation with control over attributes (drug discovery / chemical design)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional generation via transformer with control tokens or conditioning mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES or molecular string format</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in review; typical metrics include adherence to conditioning, validity, novelty, and property distributions</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Listed as an example of controllable transformer molecular generators; review does not provide experimental numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presents controllability advantages over unconditional generators; no quantitative comparisons in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same general issues: valid/synthesizable output, distributional shift when conditioning on rare attributes, and need for grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5144.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5144.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered agent framework that composes computational chemistry tools to plan and execute chemical tasks including molecular generation and synthesis planning, improving grounding and reducing hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemCrow: Augmenting largelanguage models with chemistry tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM (unspecified; framework augments LLMs with toolset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM-based agent architecture integrating external tools (tool-augmented Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>ChemCrow compiles computational chemistry tools and databases; the underlying LLM is pretrained on general corpora (review does not list exact datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug and materials design, synthesis planning and execution, chemical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM acts as controller/agent that invokes specialized tools (e.g., generative models, retrosynthesis planners) to generate and validate molecules and plans</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Depends on invoked tools; could be SMILES, retrosynthetic routes, or experimental procedures</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Review reports improved performance/grounding and reduced hallucinations qualitatively; specific quantitative metrics not provided here</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemCrow demonstrates that augmenting LLMs with curated chemistry tools substantially improves performance on diverse chemistry tasks including generation and synthesis planning by grounding outputs and enabling tool composability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Improves over plain LLM prompting by reducing hallucinations and grounding results with computational tools; compared qualitatively in review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Framework complexity, dependency on tool quality, and remaining risks of erroneous outputs if tools or their integration are flawed; review cautions about LLM hallucinations and need for grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is GPT-3 all you need for low-data discovery in chemistry? <em>(Rating: 2)</em></li>
                <li>Bayesian Optimization of Catalysts With In-context Learning <em>(Rating: 2)</em></li>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. <em>(Rating: 2)</em></li>
                <li>MolGPT: Molecular Generation Using a Transformer-Decoder Model. <em>(Rating: 2)</em></li>
                <li>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. <em>(Rating: 2)</em></li>
                <li>A Transformer-based Generative Model for De Novo Molecular Design. <em>(Rating: 2)</em></li>
                <li>C5T5: Controllable Generation of Organic Molecules with Transformers. <em>(Rating: 2)</em></li>
                <li>ChemCrow: Augmenting largelanguage models with chemistry tools. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5144",
    "paper_id": "paper-263831328",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3",
            "brief_description": "A large autoregressive language model used via fine-tuning and in-context learning to perform low-data discovery tasks in chemistry, including inverse design and property prediction.",
            "citation_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_type": "autoregressive Transformer (decoder-only)",
            "model_size": null,
            "training_data": "Pretrained on large web-scale text corpora; in chemistry use-cases fine-tuning or in-context examples applied to molecular tasks (review does not specify exact chemical pretraining corpus).",
            "application_domain": "chemistry / materials discovery; inverse molecular design and property prediction (low-data regimes)",
            "generation_method": "Fine-tuning and in-context learning (few-shot prompting) to produce molecular candidates or predict properties",
            "output_representation": "Typically SMILES/SELFIES or textual descriptions (review: models used in textual molecular representations)",
            "evaluation_metrics": "Not specified in review for GPT-3 experiments; generally tasks evaluated by predictive accuracy on property regression and success in inverse design",
            "benchmarks_or_datasets": null,
            "results_summary": "Review reports that Jablonka et al. found GPT-3 (via fine-tuning and in-context learning) can solve various chemistry and materials tasks in limited-data settings, sometimes matching or outperforming specialized methods for property prediction and inverse design.",
            "comparison_to_other_methods": "Reported to perform on par with or sometimes better than specialized, heavily engineered algorithms in low-data settings (per Jablonka et al.), especially when using in-context learning.",
            "limitations_or_challenges": "Review notes typical LLM issues (hallucinations, lack of grounded chemical validity) and that details like synthesizability and domain-specific calibration remain challenges; specific GPT-3 methodological limitations not fully detailed in review.",
            "uuid": "e5144.0"
        },
        {
            "name_short": "GPT-based BO",
            "name_full": "GPT models used for Bayesian Optimization with in-context learning",
            "brief_description": "Use of GPT-family models to perform regression with calibrated uncertainty enabling Bayesian optimization for catalyst and molecular optimization from synthesis-procedure text inputs.",
            "citation_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "mention_or_use": "mention",
            "model_name": "GPT (unspecified family/version)",
            "model_type": "autoregressive Transformer (decoder-only) used with in-context learning",
            "model_size": null,
            "training_data": "Pretrained language model weights; regression performed via in-context prompting on task-specific examples (review does not list chemical training corpus)",
            "application_domain": "catalysis and molecular optimization (chemistry/materials)",
            "generation_method": "In-context learning to perform uncertainty-aware regression; integrated into Bayesian Optimization loop",
            "output_representation": "Textual synthesis procedures mapped to property predictions; molecular candidates implied to be derived/optimized (representation not strictly specified in review)",
            "evaluation_metrics": "Not specified in review; BO success criteria (e.g., finding improved catalytic performance) implied",
            "benchmarks_or_datasets": null,
            "results_summary": "Ramos et al. demonstrated BO workflows using GPT models for catalyst and molecular optimization, mapping synthesis-procedure text directly into property space and enabling optimization without feature engineering.",
            "comparison_to_other_methods": "This approach obviates feature engineering required by classic BO and structured regression, enabling BO from textual inputs; review implies improved practicality but does not provide quantitative comparisons.",
            "limitations_or_challenges": "Uncertainty calibration and robustness are challenging for LLMs; review notes general LLM propensity to hallucinate and that grounding with tools/data is important for reliability.",
            "uuid": "e5144.1"
        },
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
            "brief_description": "A Transformer-decoder based generative model trained to produce molecular structures (typically as SMILES) for de novo molecular design.",
            "citation_title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model.",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "Transformer-decoder (autoregressive) generative model",
            "model_size": null,
            "training_data": "Trained on molecular string representations (SMILES) — review references MolGPT among SMILES-based generative transformer approaches but does not list the exact training dataset.",
            "application_domain": "de novo molecular design (drug discovery / chemical space exploration)",
            "generation_method": "Autoregressive generation of SMILES strings from learned token distribution; typical generative sampling strategies (review does not detail specifics)",
            "output_representation": "SMILES (linear molecular string representations)",
            "evaluation_metrics": "Review does not list specific metrics for MolGPT; typical metrics include validity, uniqueness, novelty, and property-based objectives",
            "benchmarks_or_datasets": null,
            "results_summary": "Reviewed as an example of transformer-decoder models successfully applied to molecular generation using SMILES; specifics of performance are not reported in this review.",
            "comparison_to_other_methods": "Positioned among other SMILES-based generative models; review notes that SMILES-based generators are easy to train but can produce invalid molecules unless robustness measures (e.g., SELFIES) are used.",
            "limitations_or_challenges": "SMILES fragility can yield invalid molecules; synthesizability and representation limits for macromolecules/materials remain concerns; review does not present MolGPT-specific failure cases.",
            "uuid": "e5144.2"
        },
        {
            "name_short": "Flam-Shepherd-3D",
            "name_full": "Language models for direct 3D structure generation (Flam-Shepherd & Aspuru-Guzik)",
            "brief_description": "Language models that generate three-dimensional molecular/material/protein structures directly as XYZ/CIF/PDB files, overcoming limitations of string-based representations for macromolecules and materials.",
            "citation_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files.",
            "mention_or_use": "mention",
            "model_name": "Language-model-based 3D generator (unnamed in review)",
            "model_type": "Transformer / language-model formulation adapted to 3D coordinate generation (sequence generation of 3D tokens/files)",
            "model_size": null,
            "training_data": "3D structural files (XYZ/CIF/PDB) for crystals, proteins, and molecules — review mentions training on formats used for crystals, proteins, etc., but does not give dataset names",
            "application_domain": "materials, macromolecules, protein binding site design, and molecular discovery requiring explicit 3D structures",
            "generation_method": "Autoregressive language-model generation of textual 3D file formats (direct generation of atomic positions and other metadata)",
            "output_representation": "3D coordinate file formats (XYZ, CIF, PDB)",
            "evaluation_metrics": "Review reports comparable performance to expert-designed state-of-the-art graph-based generation algorithms; exact metrics not listed here",
            "benchmarks_or_datasets": null,
            "results_summary": "Flam-Shepherd & Aspuru-Guzik's approach can directly generate 3D structures and shows performance comparable to state-of-the-art graph-based generative models while addressing representation limits of SMILES/SELFIES for large/periodic/macromolecular systems.",
            "comparison_to_other_methods": "Compared favorably to graph-based generation methods on the tasks reviewed (per the paper's claim); advantage is broader applicability to crystals, proteins, and materials where string representations fail.",
            "limitations_or_challenges": "Review notes general challenges include model grounding, chemical validity, and ensuring physically realistic geometries; specifics of failure modes not detailed in review.",
            "uuid": "e5144.3"
        },
        {
            "name_short": "Regression Transformer",
            "name_full": "Regression Transformer (Born & Manica)",
            "brief_description": "A model that combines sequence regression and autoregressive generation within a single transformer architecture, enabling simultaneous property prediction and molecule generation.",
            "citation_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling.",
            "mention_or_use": "mention",
            "model_name": "Regression Transformer",
            "model_type": "Transformer combining regression head and generative decoder (sequence-to-sequence / multi-task)",
            "model_size": null,
            "training_data": "Molecular language data (SMILES/SELFIES) paired with property/regression targets — review does not specify exact datasets",
            "application_domain": "molecular generation conditioned on properties (drug discovery / molecular design)",
            "generation_method": "Multi-task training to perform sequence regression and conditional sequence generation (can generate molecules with desired property values)",
            "output_representation": "SMILES/other molecular string representations",
            "evaluation_metrics": "Not specified in review; generally evaluated on ability to generate molecules matching target properties and standard generative validity/novelty metrics",
            "benchmarks_or_datasets": null,
            "results_summary": "Presented as a promising approach enabling concurrent regression and generation for molecular language modelling, facilitating property-conditioned generation.",
            "comparison_to_other_methods": "Offers a unified approach versus separate property predictors and generators; review does not provide quantitative comparisons.",
            "limitations_or_challenges": "Challenges include ensuring generated molecules are synthesizable and valid; review does not provide paper-specific failure analyses.",
            "uuid": "e5144.4"
        },
        {
            "name_short": "Transformer-DeNovo",
            "name_full": "A Transformer-based Generative Model for De Novo Molecular Design (Wang et al.)",
            "brief_description": "Transformer-based generative model designed for de novo molecular generation to explore chemical space for drug discovery and related applications.",
            "citation_title": "A Transformer-based Generative Model for De Novo Molecular Design.",
            "mention_or_use": "mention",
            "model_name": "Transformer-based generative model (Wang et al.)",
            "model_type": "Transformer generative architecture (likely decoder-based)",
            "model_size": null,
            "training_data": "Molecular string representations (SMILES) — review references this among transformer generative work but does not give dataset specifics",
            "application_domain": "de novo molecular design (drug discovery)",
            "generation_method": "Autoregressive generation of molecular strings using transformer architecture; likely uses sampling strategies for candidate generation",
            "output_representation": "SMILES or similar molecular string formats",
            "evaluation_metrics": "Not provided in review; typical evaluation would include validity, novelty, drug-likeness, and property optimization",
            "benchmarks_or_datasets": null,
            "results_summary": "Cited as an example of transformer generative models applied successfully to de novo molecular design; review does not detail quantitative outcomes.",
            "comparison_to_other_methods": "Reviewed within the context of transformer generative methods; no explicit performance comparison reported here.",
            "limitations_or_challenges": "General limitations around string representations (invalid molecules), synthesizability, and applicability to non-small-molecule domains.",
            "uuid": "e5144.5"
        },
        {
            "name_short": "C5T5",
            "name_full": "C5T5: Controllable Generation of Organic Molecules with Transformers",
            "brief_description": "A transformer-based system enabling controllable generation of organic molecules conditioned on desired attributes or constraints.",
            "citation_title": "C5T5: Controllable Generation of Organic Molecules with Transformers.",
            "mention_or_use": "mention",
            "model_name": "C5T5",
            "model_type": "Transformer generative model with controllable conditioning",
            "model_size": null,
            "training_data": "Molecular strings and associated attribute labels (review does not specify datasets)",
            "application_domain": "organic molecule generation with control over attributes (drug discovery / chemical design)",
            "generation_method": "Conditional generation via transformer with control tokens or conditioning mechanism",
            "output_representation": "SMILES or molecular string format",
            "evaluation_metrics": "Not specified in review; typical metrics include adherence to conditioning, validity, novelty, and property distributions",
            "benchmarks_or_datasets": null,
            "results_summary": "Listed as an example of controllable transformer molecular generators; review does not provide experimental numbers.",
            "comparison_to_other_methods": "Presents controllability advantages over unconditional generators; no quantitative comparisons in the review.",
            "limitations_or_challenges": "Same general issues: valid/synthesizable output, distributional shift when conditioning on rare attributes, and need for grounding.",
            "uuid": "e5144.6"
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large language models with chemistry tools",
            "brief_description": "An LLM-powered agent framework that composes computational chemistry tools to plan and execute chemical tasks including molecular generation and synthesis planning, improving grounding and reducing hallucinations.",
            "citation_title": "ChemCrow: Augmenting largelanguage models with chemistry tools.",
            "mention_or_use": "mention",
            "model_name": "LLM (unspecified; framework augments LLMs with toolset)",
            "model_type": "LLM-based agent architecture integrating external tools (tool-augmented Transformer)",
            "model_size": null,
            "training_data": "ChemCrow compiles computational chemistry tools and databases; the underlying LLM is pretrained on general corpora (review does not list exact datasets)",
            "application_domain": "drug and materials design, synthesis planning and execution, chemical reasoning",
            "generation_method": "LLM acts as controller/agent that invokes specialized tools (e.g., generative models, retrosynthesis planners) to generate and validate molecules and plans",
            "output_representation": "Depends on invoked tools; could be SMILES, retrosynthetic routes, or experimental procedures",
            "evaluation_metrics": "Review reports improved performance/grounding and reduced hallucinations qualitatively; specific quantitative metrics not provided here",
            "benchmarks_or_datasets": null,
            "results_summary": "ChemCrow demonstrates that augmenting LLMs with curated chemistry tools substantially improves performance on diverse chemistry tasks including generation and synthesis planning by grounding outputs and enabling tool composability.",
            "comparison_to_other_methods": "Improves over plain LLM prompting by reducing hallucinations and grounding results with computational tools; compared qualitatively in review.",
            "limitations_or_challenges": "Framework complexity, dependency on tool quality, and remaining risks of erroneous outputs if tools or their integration are flawed; review cautions about LLM hallucinations and need for grounding.",
            "uuid": "e5144.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "rating": 2,
            "sanitized_title": "is_gpt3_all_you_need_for_lowdata_discovery_in_chemistry"
        },
        {
            "paper_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_of_catalysts_with_incontext_learning"
        },
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files.",
            "rating": 2,
            "sanitized_title": "language_models_can_generate_molecules_materials_and_protein_binding_sites_directly_in_three_dimensions_as_xyz_cif_and_pdb_files"
        },
        {
            "paper_title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model.",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling.",
            "rating": 2,
            "sanitized_title": "regression_transformer_enables_concurrent_sequence_regression_and_generation_for_molecular_language_modelling"
        },
        {
            "paper_title": "A Transformer-based Generative Model for De Novo Molecular Design.",
            "rating": 2,
            "sanitized_title": "a_transformerbased_generative_model_for_de_novo_molecular_design"
        },
        {
            "paper_title": "C5T5: Controllable Generation of Organic Molecules with Transformers.",
            "rating": 2,
            "sanitized_title": "c5t5_controllable_generation_of_organic_molecules_with_transformers"
        },
        {
            "paper_title": "ChemCrow: Augmenting largelanguage models with chemistry tools.",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        }
    ],
    "cost": 0.014048749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transformers and Large Language Models for Chemistry and Drug Discovery
9 Oct 2023</p>
<p>Andres M Bran 
Laboratory of Artificial Chemical Intelligence (LIAC)
ISIC
EPFL
1050LausanneVDSwitzerland</p>
<p>EPFL
National Centre of Competence in Research (NCCR) Catalysis
1050LausanneVDSwitzerland</p>
<p>Philippe Schwaller philippe.schwaller@epfl.ch 
Laboratory of Artificial Chemical Intelligence (LIAC)
ISIC
EPFL
1050LausanneVDSwitzerland</p>
<p>EPFL
National Centre of Competence in Research (NCCR) Catalysis
1050LausanneVDSwitzerland</p>
<p>Transformers and Large Language Models for Chemistry and Drug Discovery
9 Oct 2023FCD2F3DED513E77F7E69995F08D6E6A5arXiv:2310.06083v1[cs.LG]TransformersAccelerated discoveryLanguage ModelsRetrosynthesisComputational tools
Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology.In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration.The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language.A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language.As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
<p>Introduction</p>
<p>The capacity to process and accurately model human language has been a persistent pursuit within the machine learning community [1][2][3][4][5].The belief is that language is intrinsic to human reasoning capabilities, thus successful language modeling could open the door to numerous applications, enhancing various information processing tasks with the potential to revolutionize several industries [6,7].The field of natural language processing has witnessed significant advancements in recent years, thanks to improved computing infrastructure, breakthroughs in algorithms, and the proliferation of abundant and accessible data [8].Language and technical terminology also play a pivotal role in the domain of chemistry, which serves as the fundamental basis for drug discovery and development.Analogous to human language, understanding and accurately modeling the language of chemistry is crucial for effective research and development in the pharmaceutical industry.By applying the advancements in language modeling and processing from the machine learning community to the domain of chemistry, it is possible to facilitate drug discovery by efficiently analyzing and interpreting vast amounts of chemical data and literature.</p>
<p>Introduced in 2017, the Transformer architecture revolutionized natural language processing [5].The Transformer is a type of neural network initially developed for neural machine translation.In its original architecture, the Transformer consists of an encoder, which encodes a sentence in the source language (e.g., French) and a decoder, which wordby-word decodes the translated sentence in the target language (e.g.English).The power of the Transformer model comes from its core building blocks, so-called attention layers [1].Those attention layers are excellent at capturing the meaning of words and subwords in their context.For this to work, text is split into subwords and inputted into the Transformer, which encodes the sequence of subwords and learns to use its attention layers to connect relevant pieces of information.The Transformer then generates another sequence of tokens as output, using its decoder part.</p>
<p>The original Transformer and variants of it, such as encoder-based and decoder-based architectures, have shown remarkable results in a range of language modeling tasks, including translation [9], sentiment classification [10,11], and summarization [12,13], to name a few.Over time, the architecture of these models has evolved, with the most popular variations being decoder-only structures, which make up the foundation of many contemporary, large language models (LLMs), such as ChatGPT and GPT-4 [14].</p>
<p>Similar to human language, where the tokens -the subunits in which the text is broken down -are words and subwords, we can imagine splitting proteins and peptides into sequences of amino acids, or molecules and chemical reactions into sequences of atoms (Figure 1).Such analogies between human language and (bio)chemistry extended the influence of Transformers in fields far beyond the boundaries of natural language processing impacting numerous scientific disciplines.</p>
<p>A prime example of Transformers' applications is their pivotal role in AlphaFold2, a leading system for predicting the 3D structure of proteins [15].The emergence of AlphaFold2 not only addressed a long-standing challenge in biochemistry but also catalyzed a surge of research, thereby becoming one of the most prevalent tools in the biochemistry community [16,17].</p>
<p>Chemistry has also benefited from these advancements.Inspired by developments in other areas, researchers have adapted a number of chemical tasks in the form of text sequences (Figure 1).This, together with the rise of open datasets and benchmarks [18][19][20][21], sparked a revolution that started with clearly defined chemical challenges fundamental to the drug development process, like reaction outcome prediction and retrosynthetic planning.Models at this stage are trained with only molecules as both input and output, making them single-modal models.</p>
<p>The revolution then continued in a new direction, with attempts to include additional types of data like spectra from analytical techniques, and human language as in synthesis procedures, giving rise to a wave of chemical multimodal models.The current stage aims to definitively bridge the gap between the modeling of chemical and natural language (Figure 1).With applications based on large language models, researchers have demonstrated how natural language can serve as interfaces for chemical reasoning and task solving across fields.</p>
<p>We are now in an era marked by significant progress in a host of tasks that are fundamental to the drug development process, with systems that go beyond single task solving and are capable of implementing complete pipelines in this process, accelerating chemical research.The influence of these Transformer models in the realm of chemistry is thus profound, highlighting their pivotal role in shaping the future of chemistry and drug discovery.The next sections will briefly introduce textual representations of molecules and reactions, and then continue to discuss task-specific Transformers for single-modality and multi-modality tasks.Lastly, we will address large language models and potential uses in chemistry and drug discovery.</p>
<p>Natural</p>
<p>2 Modeling the language of organic chemistry Chemistry, in many respects, resembles a language [22].Not only is most of the information in chemistry conveyed through human language, but the rules governing chemical transformations also form a distinct language themselves.Accurately modeling this language advances our understanding of its rules, opening up applications such as automatic retrosynthetic planning and efficient chemical space exploration [23], while also shedding light on the intricate grammar of organic chemistry [24].</p>
<p>However, the chemical language is not a conventional language like English or Mandarin, which operate on text.In organic chemistry, grammar operates on a spectrum of molecular graphs and reaction conditions, making the direct application of Transformers to chemistry a challenge.Overcoming this obstacle is thus vital and, as we will explore, can be accomplished using linear string molecular representations that have been in use for decades, with new revisions and proposals in recent years [25][26][27][28][29][30]</p>
<p>Text Representations of Molecules and Reactions</p>
<p>Chemistry has long been acknowledged as a fundamentally inventorial science, wherein new molecules and reactions are discovered, analyzed, and to a certain extent cataloged in databases [31].Researchers rely not only on scientific articles and patents but also on resources like handbooks and, more recently, computational databases, to advance their work.To streamline the storage and querying of these data, simplified molecular-input line-entry system (SMILES) strings have been proposed and utilized since the 1980s [25].</p>
<p>The process of linearizing molecular graphs involves choosing an atom and sequentially enumerating all other atoms in the molecule from this point (Figure 2a).Special characters are assigned to specify bond types, branches, rings, stereochemistry, and other pertinent information for molecular representation.SMILES leverage the fact that molecular graphs adhere to certain chemical rules, such as different types of atoms needing to respect their valence.This makes it feasible to represent a large portion of organic chemistry as sequences of characters.</p>
<p>As molecular machine learning applications have emerged, the limitations of these representations -such as their lack of robustness-have become increasingly apparent.In generative models, this flaw can lead to the production of entirely invalid molecules, becoming an additional hurdle in molecular discovery applications [32,33].To mitigate this issue, researchers have introduced Self-Referencing Embedded Strings (SELFIES), a novel string-based representation that effectively addresses the robustness concern [28].This new representation has found numerous applications, particularly in drug discovery and molecular generation, due to its unique formulation that guarantees the correct mapping of any such string to a valid molecule.While other representations have been proposed in the past to standardize the language (IUPAC names, InChI [29]), or to address the limitations of current representations for deep learning applications (DeepSMILES [26]), we will not delve further into these representations in this discussion [28,34].</p>
<p>With these molecular text representations in hand, chemical reactions can easily be encoded by leveraging the syntax of chemical equations.In that sense, sets of molecules -e.g. the set of reactants-are represented by listing all molecules with a dot "." between them.Reactants are separated from the products using the symbol "&gt;", representing the arrow, and other details like catalysts and reagents are placed in between the "&gt;&gt;" symbol.The resulting reaction representation has the form "A.B&gt;catatlyst.reagent&gt;C.D", where A and B are reactants and C and D products.This notation is commonly used in SMILES and called a "reaction SMILES".</p>
<p>Task-specific Transformers</p>
<p>The conversion of chemical problems into sequences of tokens -in the form of a languagehas unlocked the transformative potential of the Transformer architecture within the chemistry domain.This shift has led to remarkable model performances in prediction tasks in</p>
<p>CNC(C)CC1=CC=C(OCO2)C2=C1 SMILES SELFIES InChI [C][N][Branch1][C][C][C][C][=C][C] [=C][C][Branch1][Ring2][=C] [Ring1][=Branch1][O][=C][O] [Ring1][=Branch1]</p>
<p>IUPAC name Fig. 2 The language of organic chemistry operates on molecular graphs, which can easily be converted into multiple text formats (a), facilitating the use of Transformers in their multiple forms, for a variety of tasks in Chemistry (b).</p>
<p>chemistry, such as retro-and forward-synthesis [35][36][37], as well as molecular regression [38][39][40] and reaction classification [41].Moreover, it has set the stage for other, more diverse tasks, like inferring experimental procedures [42,43], that go beyond operations on molecular graphs and require a deeper understanding of experimental conditions and standard procedures, a feat only achievable through modeling human language.Achieving success in this wide range of tasks has been facilitated by different variations of the Transformer architecture.Depending on the application, these variations leverage different parts of the architecture (Figure 2b), leading to encoder-decoder models -useful for tasks involving conversion of a sequence into another, like translation-, encoder-only -to extract rich representations from data-, and decoder-only architectures, mostly used in generative applications (Figure 2b).</p>
<p>Chemical translation tasks</p>
<p>The primary motivation for the formulation of the Transformer architecture by Vaswani et al. [5] was translation.In this task, input text in one language is converted into corresponding text in another language, logically leading to an architecture with an encoder part linked to a decoder (Figure 2b).</p>
<p>Steps to exploit this design were taken by Schwaller et al. [36], they introduced the Molecular Transformer and treated the task of reaction prediction as a translation task.In this perspective, a Transformer model learns to "translate" from precursors to products SMILES.The approach proved highly successful, establishing itself as one of the state-ofthe-art methods for this task [44].Subsequent attempts applied a similar approach to other tasks, like retrosynthetic planning [37], where a model learns to predict the reactants (and reagents) necessary to produce a given product.The model is then successively applied to the predicted reactants iteratively, to construct a retrosynthetic tree that maps to commercially available building blocks.Recent work used similar retrosynthesis models to tackle the challenge of suggesting diverse candidate reactions [45], as well as prompting, steering and unbiasing the proposed disconnections [46].</p>
<p>The Molecular Transformer approach was further extended by Irwin et al. [47], who proposed the Chemformer, a model pre-trained on a variety of chemical tasks, that can then be specialized for specific applications.This methodology achieved even greater success while offering additional flexibility and transferability to new tasks.</p>
<p>An interesting variation of the architecture came from an attempt to directly encode molecules as molecular graphs, for translation tasks like reaction outcome prediction.Tu and Coley [48] used a custom-designed graph encoder, with a Transformer decoder, trained to translate molecular graphs to SMILES.This approach proved valuable in forwardand retrosynthesis tasks, improving over other Transformer-based baselines, due to the permutational invariance of the novel graph-encoder.</p>
<p>Unsupervised learning and feature extraction.</p>
<p>Transformers have also been applied in various fields to generate rich vectorial representations of text that encapsulate context and general features [11,49].Such methods have been used for sentiment analysis, where a given text is classified based on the sentiment it conveys, a useful tool for assessing customer satisfaction with a specific product.The process of encoding this text as vectors is known as representation learning.For this task, only the encoder part of the Transformer architecture is used, as there is no need for text generation.</p>
<p>In the realm of chemistry, representation learning is a critical issue [50].It enables the transformation of molecules and reactions into vector representations, thus enabling a multitude of downstream applications.These include similarity assessment for database lookups, as well as regression and classification for property prediction tasks, such as predicting reaction yields or identifying toxic compounds -both of which are crucial to the drug development process.</p>
<p>Adopting similar techniques, Wang et al. [51] trained a model to generate reaction representations, and compared them against commonly used hand-engineered molecular representations, showing gains in accuracy in a number of downstream regression tasks, demonstrating the power of Transformer encoders for tasks in chemistry.</p>
<p>In another study, the decoder side of the Transformer was replaced by a classification layer, and the model was trained to predict the class of chemical reactions [41].The resulting vector representations, in addition to achieving high classification accuracies, were used for the visualization and exploration of a database of chemical reactions from patents.This revealed patterns in the data that grouped reactions by class, but also by data source, and relevant product properties, like logP, number of H donors, and others [52].This study showcased how such numerical representations can encode intricate details of chemical reactions, thereby facilitating effective clustering and classification of reactions.Importantly, this was achieved without the need for hand-crafted molecular or reaction features, demonstrating the model's capacity to learn purely from extensive reaction databases.In a similar vein, a regression layer, instead of a decoder layer, was used for predicting reaction yields, achieving outstanding results on datasets from high-throughput-experimentation platforms [53,54].</p>
<p>Other works have further advanced the applications of encoder-only Transformers, with enhancements in training procedures [38][39][40], modifications in the architecture [55], and applications in larger machine learning systems towards the solution of more specific objectives [56].</p>
<p>Similar ideas have also been implemented, with a focus on biochemistry.Rives et al. [57] trained a transformer model on 250 million unlabeled protein sequences, with the goal of learning the "language of proteins", an approach known as unsupervised learning.The resulting representations not only enabled state-of-the-art property predictions for proteins, but also predictions of variant effects and protein folding [58].The model was further shown to generalize beyond naturally occurring proteins [59], paving the way for applications in de novo generation of proteins for specific applications.</p>
<p>Perhaps one of the most intriguing applications of Transformers in chemical representation learning, also leveraging unsupervised learning, emerged from an attempt to interpret their inner workings.Through a detailed examination of the attention weights in a Transformer model trained on unlabeled chemical reactions, Schwaller et al. [24] discovered that these models create internal representations that connect atoms from the reactants to other atoms in the products.This almost serendipitous discovery led to the creation of RXN-Mapper, an open-source algorithm that accurately calculates the atom mapping in a given chemical reaction.RXNMapper has been shown to outperform other approaches -many of which were under closed licenses-in terms of speed, parallelizability, and accuracy.A similar approach was recently investigated on enzymatic reactions, where the attention weights could be used to identify active sites in protein sequences [60].</p>
<p>Articulating chemical language and other modalities</p>
<p>Chemical transformations are not confined solely to the realm of chemical structures.They are enhanced and accessed through a variety of other data types, or modalities.These include human language used for describing molecules and experimental results, as well as the experimental results themselves, which may be presented in formats such as numerical arrays or images, among others.</p>
<p>Considering this, scientists have proposed tasks designed to bridge the divide between the language of molecules and human language.One such task is known as molecular captioning, which involves describing a specific molecule in natural language [61].The description can cover a wide range of features such as molecular scaffolds, the source of the substance, drug interactions, and more, all expressed in simple English.</p>
<p>Some researchers have expanded on this concept [37,62], enabling a smooth interconversion from molecules to natural language and vice versa.This advancement has given rise to versatile models capable of not just molecular captioning, but also generating molecules in response to text queries, or carrying out molecule-to-molecule conversion tasks, like predicting reaction outcomes and retrosynthesis [62].While these models are still in their early stages of development, they show immense potential for the future of drug discovery as they continue to be developed and refined.</p>
<p>Another task, highly relevant for the design of automatic robotic platforms for synthesis, is the prediction of experimental steps.A step missing from retrosynthetic planning is the experimental realization, which involves steps such as adding substances, stirring, and purifying, details not contained in predicted reactions.To bridge this gap, Vaucher et al. [42,43] compiled a database of reactions with associated action sequences, and trained models to generate actionable sequences of steps from a given reaction in SMILES format.Other tasks aim to link experimental results with molecular structures, effectively addressing the challenge of structural elucidation.A pioneering attempt using Transformers was recently made by Alberts et al. [63], who created a database of molecules and computationally generated IR spectra to train a Transformer model for this purpose.IR spectra, often overlooked for this task due to their complexity, were represented as a sequence of numerical values encoded in text alongside the chemical formula.These were then fed to the model, which was tasked with predicting the SMILES string of the underlying molecular structure.This method achieved a 45% accuracy rate in structure prediction, while also surpassing all previous efforts in predicting functional groups from IR spectra.This illustrates how underutilized data, such as IR spectra, can be harnessed for tasks as intricate as structural elucidation, despite their initial difficulty in interpretation.</p>
<p>These excellent applications of Transformers across a range of tasks in chemistry underscore just a fraction of the potential these models possess, as well as the wealth of latent knowledge embedded in chemical databases.So far, our discussion has focused on taskspecific applications, where a predefined task is established and models are specifically trained to solve that task.One of the advantages of language is its flexibility, enabling it to express various tasks in similar forms of the same language.The upcoming section will delve into the potential of more generalized language models for chemistry.</p>
<p>Advanced applications: Beyond task-specific models</p>
<p>The last few years have seen a remarkable rise in the power and popularity of foundation models: models pre-trained on vast amounts of data that can easily be adapted to other, more specific tasks [6,14].Such pre-training is conducted using rich databases of human text extracted from the internet, among others, to make the model learn the language along with general knowledge about the world.This rise has been achieved mainly through the escalation of Transformer architectures to large computational budgets and vast databases, leading to models capable of producing text that matches human-level quality in a wide range of scenarios [6,14].</p>
<p>The process of fine-tuning these models leverages their pre-learned language abilities while tailoring them toward a specific objective where less data might be available [64,65].A prime example of this is ChatGPT, a large language model fine-tuned for conversation.Its release has not only catapulted these models into popularity but has also reignited profound philosophical discussions about the nature of intelligence [7].However, it has also raised serious concerns about potential issues such as the spread of misinformation.The power and accessibility of ChatGPT have set the world on a new trajectory, prompting us to rethink how we produce and consume media, while also highlighting the need for careful consideration of the potential implications.The success and popularity of ChatGPT can be attributed to two key factors.Firstly, its freely available and highly user-friendly chat interface, which makes interaction with the model straightforward, and secondly, its impressive capabilities, which often extend beyond the tasks the model was initially trained for.These capabilities not only demonstrate the power of ChatGPT and models of its kind but also hint at their potential for innovative applications.</p>
<p>On the capabilities of Large Language Models</p>
<p>The rise of data-hungry machine learning algorithms, coupled with the increasing availability of data, has set a trend for scaling models to the maximum sizes that hardware constraints allow.As these models increase in size, they correspondingly improve in their capacity to perform the tasks they were originally trained for.This trend is particularly predictable in Their capabilities allow for a. general task solvers thanks to their flexibility and knowledge transferability [66,67] and b. agent architectures, capable of integrating virtually unlimited modalities, in the form of computational tools [68][69][70].</p>
<p>the realm of language models, and it manifests in the form of what researchers refer to as scaling laws [71].These scaling laws serve as a valuable tool for researchers, enabling them to identify performance trends and make accurate predictions about the capabilities of significantly larger models.However, the process of scaling does not simply enhance existing capabilities.As these models grow, they reach certain critical thresholds where not only their existing capabilities are enhanced, but entirely new capabilities are observed.</p>
<p>These new abilities are collectively referred to as emergent capabilities [72].They represent a fascinating aspect of model scaling, as they are not present or even predictable in smaller models, but suddenly appear as the models increase in size.These emergent capabilities offer exciting potential for the future of machine learning and its application in various fields, including chemistry.</p>
<p>Wei et al. [72] demonstrated how language models below certain computing budgets display somewhat random behavior across a range of tasks.However, once a certain model size is reached, sudden and significant improvements in performance are observed.Other capabilities are observed in the form of augmented prompting strategies, such as Chain of Thought (CoT) reasoning [73].In this approach, models are instructed to solve a task by following a step by step reasoning sequence.Another emergent capability is instruction following [74], where LMs are given a task in the form of a set of instructions to follow.</p>
<p>Interestingly, these techniques generally have a negative impact on the performance of smaller models [73].However a positive effect on performance is observed once models reach certain threshold sizes [72].</p>
<p>Emergent capabilities, therefore, enable language models to effectively tackle a variety of tasks that involve reasoning.This is achieved without explicit training and with the flexibility of a text query in natural language.Considering the remarkable capabilities and transformative potential of large language models in various fields, it naturally leads us to question what benefits these models could bring to the field of chemistry.This will be the focus of our exploration in the following sections.</p>
<p>Large Language Models in chemistry</p>
<p>Much of this chapter has been devoted to the application of the Transformer architecture to ingeniously-encoded specific chemical tasks.However, it is important to note that a significant portion of information in chemistry is conveyed through human language, which includes explanations of reaction mechanisms, descriptions of modes of action of drugs, to name a few.Reasoning in chemistry is thus fundamentally articulated in human language, even though it is complemented by other non-text objects like graphs and images.This observation raises the question of whether Large Language Models (LLMs) can replicate this level of chemical reasoning and, if so, to what extent.</p>
<p>Tailoring techniques: fine-tuning and in-context learning</p>
<p>One of the main goals of developing general, large pretrained language models, is being able to further tune them for specific applications.Techniques for this include fine-tuning, where a model's parameters are further optimized for a new task.This technique has been the dominating paradigm in machine learning for years, with excellent results in a number of applications [75][76][77].</p>
<p>A new paradigm has also become the target of investigations with the rise of LLMs, so-called in-context learning.This feature can be activated simply by providing the model with a task description, along with a small set of examples that serve as "training data".This method proves particularly useful when data is scarce or obtaining it is costly or time-consuming, as is often the case in chemistry laboratories.LLMs have demonstrated impressive performance across a range of tasks using this method, with their strong performance believed to stem from knowledge transfer across different pre-training sources.</p>
<p>In line with this, Jablonka et al. [66] demonstrated that LLMs like GPT-3 can effectively solve various tasks in chemistry and materials science by fine-tuning LLMs, to a good level of approximation.The authors chose a range of tasks for which datasets are typically limited, such as predicting the transition wavelength of molecules, the phase of solid solutions in high entropy alloys, or even inverse design.Given the limited datasets available for these tasks, machine learning solutions have traditionally relied on heavily engineered and specialized algorithms [50,78,79].These algorithms aim to directly incorporate chemical knowledge into the model architectures.In parallel, extensive research has been devoted to molecular and reaction representations, with some efforts also attempting to bias the representations using prior chemical knowledge to better encode relevant features in data [80].All these endeavors aim to make the most of existing knowledge, enabling models to learn as much as possible from the few available data points.</p>
<p>Interestingly, Jablonka et al. [66] demonstrated that fine-tuning and in-context learning can perform on par with, and in some instances even outperform, these specialized techniques, particularly when data is limited.The high performance of in-context learning, combined with its ease of use and flexibility, makes it one of the most impressive applications of LLMs in chemistry to date.This technique holds the potential to revolutionize the way machine learning is utilized in the scientific field, by rapidly highlighting complex correlations in data.</p>
<p>Another application of regression, which holds significant interest in chemistry and drug discovery cycles, is optimization.This process involves modifying an object until a property of interest reaches a desired value.Typically, this requires a large number of measurements of the desired property, which can be quite costly in chemistry use-cases.Applications of this include yield/selectivity optimization in chemical reactions and the generation of molecular candidates with target properties.Bayesian Optimization (BO) has recently been proposed as a solution to such problems in chemistry [81,82], particularly in situations where data is small.However, BO requires uncertainty-calibrated regression methods, which sets it apart from conventional regression.</p>
<p>In line with the concept of in-context learning, Ramos et al. [67] proposed a system that utilizes GPT models to perform regression while also incorporating uncertainty.This approach enables BO without the need for any feature engineering or fine-tuning.The flexibility of this method allowed the team to perform catalyst and molecular optimization using only the synthesis procedure of the catalyst as input.This work represents a paradigm shift in drug discovery and molecular design.For the first time, it showcases a direct map from synthesis procedure into property space, effectively overcoming issues like the synthesizability of proposed molecules, a key limitation of structure-based generative models.</p>
<p>Molecular generation</p>
<p>Another fascinating application of the generative capabilities of language models is molecular generation.This area, which is of significant importance in the drug discovery process, has been largely dominated by models that generate molecules in the form of linear string representations, such as SMILES or SELFIES [83][84][85].While this approach has proven successful due to the ease of training these models [66,86,87], its successful application is contingent on the ability to specify substances as graphs and their subsequent conversion to a linear string representation.However, this approach is only suitable for a subset of organic molecules.Other substances, such as macromolecules and materials, necessitate more comprehensive representations.A complete and accurate representation of these substances can only be achieved by specifying atomic positions, boundary conditions, and other factors.This requirement presents a significant challenge and limitation to the current methods of molecular generation.To address these limitations, Flam-Shepherd and Aspuru-Guzik [88] proposed using language models for structure generation, directly generating them with three-dimensional atomic positions.Besides being innovative and valid, the generated structures can be obtained by training models in a variety of formats used for crystals, proteins, and more.This work also demonstrates performance comparable to expert-designed, state-of-the-art algorithms for molecular generation based on graphs, while overcoming the limitations mentioned earlier.</p>
<p>Language Model-powered Agents</p>
<p>Among the most useful emergent abilities of language models are step-by-step reasoning, activated through chain-of-thought (CoT) prompting, and their capacity to effectively use tools [89].These capabilities have been the subject of extensive research in recent years, and their application has been shown to significantly enhance the performance of LLMs across a variety of tasks.CoT prompting is a technique where language models are instructed to solve a task by following a sequence of reasoning steps, rather than providing an answer in a single response [73].Instructing language models in this way effectively allows them to perform symbolic operations, much like humans perform arithmetic operations by keeping track of intermediate steps.</p>
<p>The ability to use tools is another significant capability of language models [89].This allows them to invoke external computational tools, thereby enriching their knowledge through querying search engines, accessing calculators, and so on [89].These capabilities have been demonstrated to enhance the performance of large language models in a range of tasks that were previously inaccessible.The recent advancements and results in revealing and exploiting the capabilities of LLMs suggest the potential for combining some of these capabilities to create more powerful and useful possibilities.This concept has been recently explored, leading to the development of the Modular Reasoning, Knowledge and Language (MRKL [90]) and Reason+Act (ReAct [91]) systems, which combine the CoT and tool-using capabilities of modern LLMs.By incorporating external tools into a CoT setting, agents of this type have recently been shown to outperform other methods based on large language models.</p>
<p>One direct benefit of effective tool usage is that it partially overcomes the unimodality issue of LLMs.Under this setting, they become capable of processing different types of input data, making real-time decisions in simulated environments, and even interacting with real-world robotic platforms.The solutions provided by LLMs to tasks also become more grounded in reality, as access to certain tools provides them with real, up-to-date information relevant to the task.This can, to some extent, limit the tendency of these models to generate unrealistic or "hallucinated" responses.</p>
<p>Agents in chemistry: Unleashing the power from tools</p>
<p>Despite their strengths as text generators and task solvers, and their remarkable few-shot and zero-shot performance, these models are also well known for their high propensity to generate false and inaccurate content, an issue that extends to easily verifiable matters such as basic arithmetic [89] and chemical operations [92].These limitations make the direct application of LLMs to chemistry a challenging matter.</p>
<p>The potential applications of large language models in chemistry were first explored in a large-scale collaboration involving researchers from around the world, an effort that resulted in the demonstration of 14 use-cases [68].The applications range from wrappers for computational tools, which enhance their accessibility by allowing natural language inputs to modify behaviors, to assistants for reaction optimization, and knowledge parsers and synthesizers for scientific question answering, among others.These are just a few of the possibilities that LLMs offer in chemistry, which, when combined with existing chemistry tools and databases, significantly increase the applicability and accessibility of computational applications.</p>
<p>More recently, Bran and Cox et al. [69] extended the concept of LLM-powered agents for chemistry by curating and compiling a set of computational chemistry tools.Their system, ChemCrow, has been shown to be capable of planning and executing tasks in chemistry, effectively streamlining the reasoning process for several common chemical tasks across areas such as drug and materials design and synthesis.The authors demonstrate that this approach has a highly positive effect on LLM's performance for tasks in chemistry, overcoming hallucinations and grounding their responses with data from reliable sources.Complementary approaches exist, with a sharper focus on cloud lab operability [70].</p>
<p>The power of platforms like ChemCrow extends beyond merely serving as independent task solvers.They can be viewed as general chemistry assistants with the ultimate goal of making computational tools more accessible to chemists, thereby accelerating discovery.An additional highlight is the seamless exploitation of tool composability that this allows.It makes it straightforward to enrich the results of one tool with another, or to construct custom tool pipelines, all through simple requests in natural language.</p>
<p>Outlook and final remarks</p>
<p>Advancements in neural translation models, and specially with the introduction of the Transformer architecture, has sparked a revolution in machine learning for applications in chemistry and drug development.Analogies between chemical and natural language, and the publication of open databases and benchmarks, have inspired the representation of chemical tasks in the form of text, allowing straightforward application of Transformers to problems in this field.This revolution has occurred in three stages, differentiated by the specificity of tasks.In a first stage, characterized by task-specificity and use of singlemodality models, applications spanned molecule-to-molecule conversion tasks, like reaction outcome prediction and retrosynthetic planning, along with representation learning and downstream tasks like regression and classification.Their excellent performance and relative simplicity made them de-facto models in an array of applications.</p>
<p>In a second stage, researchers attempted to connect multiple additional modalities relevant to chemistry, like spectra from experiments, sequences of experimental actions, and even natural language, opening the way for an expanded number of applications involving modalities of any sort, however still task-specific.More recently, powered by vertiginous advancements in training and tuning of large language models, a series of works have been published that leverage a number of capabilities from such models.Among others, these contributions showcase applications in regression, classification, molecular generation and reaction optimization, all with unprecedented flexibility and usually improved performance over other methods.Another direction explores the integration of virtually unlimited modalities -in the form of tools-into agents powered by LLMs.The power of these agents has been demonstrated through a number of diverse tasks, ranging from molecular generation to automated organic synthesis, in an open-ended, highly customizable fashion.</p>
<p>By leveraging the expressivity and flexibility of natural language, this last wave of applications aims to bridge the gap between the chemical and natural languages.As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
<p>Fig. 1
1
Fig.1Advances in Natural Language Processing have inspired applications in Chemistry.Over time, the gap between chemical Language and natural Language is being closed by including additional modalities.Most recent works present general task solvers for chemistry, capable of chemical reasoning and automatic synthesis, among others.</p>
<p>Fig. 3
3
Fig. 3 Recent advances in Large Language Models have launched a new era of Transformers in chemistry.Their capabilities allow for a. general task solvers thanks to their flexibility and knowledge transferability[66,67] and b. agent architectures, capable of integrating virtually unlimited modalities, in the form of computational tools[68][69][70].</p>
<p>AcknowledgementsThis work was created as part of NCCR Catalysis (grant number 180544), a National Centre of Competence in Research funded by the Swiss National Science Foundation.
D Bahdanau, K Cho, Y Bengio, 10.48550/arXiv.1409.0473arXiv.arXiv:1409.0473stat]Neural Machine Translation by Jointly Learning to Align and Translate. 2016</p>
<p>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. K Cho, B Merrienboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, 10.48550/arXiv.1406.1078arXiv.arXiv:1406.10782014cs, stat</p>
<p>Long Short-Term Memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Computation. 981997</p>
<p>I Sutskever, O Vinyals, Q V Le, 10.48550/arXiv.1409.3215arXiv.arXiv:1409.3215Sequence to Sequence Learning with Neural Networks. 2014</p>
<p>Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 10.48550/arXiv.1706.03762arXiv.arXiv:1706.037622017</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N Chatterji, A Chen, K Creel, J Q Davis, D Demszky, C Donahue, M Doumbouya, E Durmus, S Ermon, J Etchemendy, K Ethayarajh, L Fei-Fei, C Finn, T Gale, L Gillespie, K Goel, N Goodman, S Grossman, N Guha, T Hashimoto, P Henderson, J Hewitt, D E Ho, J Hong, K Hsu, J Huang, T Icard, S Jain, D Jurafsky, P Kalluri, S Karamcheti, G Keeling, F Khani, O Khattab, P W Koh, M Krass, R Krishna, R Kuditipudi, A Kumar, F Ladhak, M Lee, T Lee, J Leskovec, I Levent, X L Li, X Li, T Ma, A Malik, C D Manning, S Mirchandani, E Mitchell, Z Munyikwa, S Nair, A Narayan, D Narayanan, B Newman, A Nie, J C Niebles, H Nilforoshan, J Nyarko, G Ogut, L Orr, I Papadimitriou, J S Park, C Piech, E Portelance, C Potts, A Raghunathan, R Reich, H Ren, F Rong, Y Roohani, C Ruiz, J Ryan, C Ré, D Sadigh, S Sagawa, K Santhanam, A Shih, K Srinivasan, A Tamkin, R Taori, A W Thomas, F Tramèr, R E Wang, W Wang, B Wu, J Wu, Y Wu, S M Xie, M Yasunaga, J You, M Zaharia, M Zhang, T Zhang, X Zhang, Y Zhang, L Zheng, K Zhou, P Liang, arXiv.arXiv:2108.07258On the Opportunities and Risks of Foundation Models. 2022</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv. 2023</p>
<p>L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, S Presser, C Leahy, 10.48550/arXiv.2101.00027arXiv.arXiv:2101.00027The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, 10.48550/arXiv.1910.10683arXiv:1910.106832020Text Transformer. arXiv.cs, stat</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 10.48550/arXiv.1810.04805arXiv.arXiv:1810.048052019</p>
<p>N Reimers, I Gurevych, 10.48550/arXiv.1908.10084arXiv.arXiv:1908.10084Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. 2019</p>
<p>P J Liu, M Saleh, E Pot, B Goodrich, R Sepassi, L Kaiser, N Shazeer, arXiv.arXiv:1801.10198[cs]Generating Wikipedia by Summarizing Long Sequences. 2018</p>
<p>Neural Text Summarization: A Critical Evaluation. W Kryscinski, N S Keskar, B Mccann, C Xiong, R Socher, 10.18653/v1/D19-1051Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong; ChinaAssociation for Computational Linguistics2019</p>
<p>. 10.48550/arXiv.2303.08774arXiv.arXiv:2303.087742023Technical Report</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-22023-08-07Nature. 59678732021Nature Publishing Group</p>
<p>AlphaFold2 and its applications in the fields of biology and medicine. Z Yang, X Zeng, Y Zhao, R Chen, 10.1038/s41392-023-01381-zSignal Transduction and Targeted Therapy. 812023Nature Publishing Group</p>
<p>M Akdel, D E V Pires, E P Pardo, J Jänes, A O Zalevsky, B Mészáros, P Bryant, L L Good, R A Laskowski, G Pozzati, A Shenoy, W Zhu, P Kundrotas, V R Serra, C H M Rodrigues, A S Dunham, D Burke, N Borkakoti, S Velankar, A Frost, J Basquin, K Lindorff-Larsen, A Bateman, A V Kajava, A Valencia, S Ovchinnikov, J Durairaj, D B Ascher, J M Thornton, N E Davey, A Stein, A Elofsson, T I Croll, P Beltrao, 10.1038/s41594-022-00849-wA structural biology community assessment of AlphaFold2 applications. Nature Publishing Group202229</p>
<p>K Huang, T Fu, W Gao, Y Zhao, Y Roohani, J Leskovec, C Coley, C Xiao, J Sun, M Zitnik, Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development. Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. 2021</p>
<p>The Open Reaction Database. S M Kearnes, M R Maser, M Wleklinski, A Kast, A G Doyle, S D Dreher, J M Hawkins, K F Jensen, C W Coley, 10.1021/jacs.1c09820Journal of the American Chemical Society. 143452021American Chemical Society</p>
<p>Extraction of chemical structures and reactions from the literature. D Lowe, 10.17863/CAM.162932012University of CambridgePhD thesis</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, 10.1039/C7SC02664AChemical Science. 922018The Royal Society of Chemistry</p>
<p>Organic Chemistry as a Language and the Implications of Chemical Linguistics for Structural and Retrosynthetic Analyses. A Cadeddu, E K Wylie, J Jurczak, M Wampler-Doty, B A Grzybowski, 10.1002/anie.201403708Angewandte Chemie International Edition. 53312014</p>
<p>Computer-designed repurposing of chemical wastes into drugs. A Wo Los, D Koszelewski, R Roszak, S Szymkuć, M Moskal, R Ostaszewski, B T Herrera, J M Maier, G Brezicki, J Samuel, J A M Lummiss, D T Mcquade, L Rogers, B A Grzybowski, 10.1038/s41586-022-04503-9Nature. 60479072022Nature Publishing Group</p>
<p>Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. P Schwaller, B Hoover, J.-L Reymond, H Strobelt, T Laino, 10.1126/sciadv.abe41662023-02-07Science Advances. 71541662021American Association for the Advancement of Science</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, 10.1021/ci00057a005Journal of Chemical Information and Computer Sciences. 2811988American Chemical Society</p>
<p>DeepSMILES: An Adaptation of SMILES for Use in Machine-Learning of Chemical Structures. N O'boyle, A Dalke, 10.26434/chemrxiv.7097960.v1ChemRxiv. 2018</p>
<p>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. M Krenn, F Häse, A Nigam, P Friederich, A Aspuru-Guzik, 10.1088/2632-2153/aba947Machine Learning: Science and Technology. 142020Publisher: IOP Publishing</p>
<p>M Krenn, Q Ai, S Barthel, N Carson, A Frei, N C Frey, P Friederich, T Gaudin, A A Gayle, K M Jablonka, R F Lameiro, D Lemm, A Lo, S M Moosavi, J M Nápoles-Duarte, A Nigam, R Pollice, K Rajan, U Schatzschneider, P Schwaller, M Skreta, B Smit, F Strieth-Kalthoff, C Sun, G Tom, G F Rudorff, A Wang, A White, A Young, R Yu, A Aspuru-Guzik, 10.1016/j.patter.2022.100588arXiv:2204.000562023-08-07SELFIES and the future of molecular string representations. 20223100588</p>
<p>InChI, the IUPAC International Chemical Identifier. S R Heller, A Mcnaught, I Pletnev, S Stein, D Tchekhovskoi, 10.1186/s13321-015-0068-4Journal of Cheminformatics. 712015</p>
<p>TUCAN: A molecular identifier and descriptor applicable to the whole periodic table from hydrogen to oganesson. J C Brammer, G Blanke, C Kellner, A Hoffmann, S Herres-Pawlis, U Schatzschneider, 10.1186/s13321-022-00640-5Journal of Cheminformatics. 1412022</p>
<p>Chemical space: limits, evolution and modelling of an object bigger than our universal library. G Restrepo, 10.1039/D2DD00030JDigital Discovery. 152022Royal Society of Chemistry</p>
<p>Grammar Variational Autoencoder. M J Kusner, B Paige, J M Hernández-Lobato, 10.48550/arXiv.1703.01925arXiv.arXiv:1703.019252017</p>
<p>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, A Aspuru-Guzik, 10.1021/acscentsci.7b00572ACS Central Science. 422018American Chemical Society</p>
<p>Exploring chemical space using natural language processing methodologies for drug discovery. H Öztürk, A Özgür, P Schwaller, T Laino, E Ozkirimli, 10.1016/j.drudis.2020.01.020Drug Discovery Today. 2542020</p>
<p>Transfer learning enables the molecular transformer to predict regio-and stereoselective reactions on carbohydrates. G Pesciullesi, P Schwaller, T Laino, J.-L Reymond, 10.1038/s41467-020-18671-7Nature Communications. 1112020Nature Publishing Group</p>
<p>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction. P Schwaller, T Laino, T Gaudin, P Bolgar, C A Hunter, C Bekas, A A Lee, 10.1021/acscentsci.9b005762023-02-07ACS Central Science. 592019American Chemical Society</p>
<p>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. P Schwaller, R Petraglia, V Zullo, V H Nair, R A Haeuselmann, R Pisoni, C Bekas, A Iuliano, T Laino, 10.1039/C9SC05704HChemical Science. 11122020. 2023-07-20The Royal Society of Chemistry</p>
<p>Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction. J Li, X Jiang, 10.1155/2021/7181815Wireless Communications and Mobile Computing 2021. 2021. 2023-07-277181815</p>
<p>ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. S Chithrananda, G Grand, B Ramsundar, 10.48550/arXiv.2010.09885arXiv.arXiv:2010.098852020physics, q-bio</p>
<p>W Ahmad, E Simon, S Chithrananda, G Grand, B Ramsundar, 10.48550/arXiv.2209.01712arXiv.arXiv:2209.01712ChemBERTa-2: Towards Chemical Foundation Models. 2022cs, q-bio</p>
<p>Mapping the space of chemical reactions using attention-based neural networks. P Schwaller, D Probst, A C Vaucher, V H Nair, D Kreutter, T Laino, J.-L Reymond, 10.1038/s42256-020-00284-wNature Machine Intelligence. 322021. 2023-07-25Nature Publishing Group</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. A C Vaucher, F Zipoli, J Geluykens, V H Nair, P Schwaller, T Laino, 10.1038/s41467-020-17266-6Nature Communications. 1112020Nature Publishing Group</p>
<p>Inferring experimental procedures from text-based representations of chemical reactions. A C Vaucher, P Schwaller, J Geluykens, V H Nair, A Iuliano, T Laino, 10.1038/s41467-021-22951-1Nature Communications. 1212021Nature Publishing Group</p>
<p>State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis. I V Tetko, P Karpov, R Van Deursen, G Godin, 10.1038/s41467-020-19266-yNature Communications. 1112020Nature Publishing Group</p>
<p>Enhancing diversity in language based models for single-step retrosynthesis. A Toniato, C Vaucher, A Schwaller, P Laino, T , 10.1039/D2DD00110A2023-08-03Digital Discovery. 222023Royal Society of Chemistry</p>
<p>Unbiasing Retrosynthesis Language Models with Disconnection Prompts. A Thakkar, A C Vaucher, A Byekwaso, P Schwaller, A Toniato, T Laino, 10.1021/acscentsci.3c003722023-08-03ACS Central Science. 972023American Chemical Society</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, 10.1088/2632-2153/ac3ffb2023-02-15Machine Learning: Science and Technology. 31150222022Publisher: IOP Publishing</p>
<p>Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction. Z Tu, C W Coley, 10.48550/arXiv.2110.09681arXiv.arXiv:2110.096812021</p>
<p>Distributed Representations of Words and Phrases and their Compositionality. T Mikolov, I Sutskever, K Chen, G Corrado, J Dean, ArXiv. 2013</p>
<p>D Duvenaud, D Maclaurin, J Aguilera-Iparraguirre, R Gómez-Bombarelli, T Hirzel, A Aspuru-Guzik, R P Adams, 10.48550/arXiv.1509.09292arXiv.arXiv:1509.09292Convolutional Networks on Graphs for Learning Molecular Fingerprints. 2015cs, stat</p>
<p>SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction. S Wang, Y Guo, Y Wang, H Sun, J Huang, 10.1145/3307339.3342186Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics. the 10th ACM International Conference on Bioinformatics, Computational Biology and Health InformaticsNiagara Falls NY USAACM2019</p>
<p>Machine intelligence for chemical reaction space. P Schwaller, A C Vaucher, R Laplaza, C Bunne, A Krause, C Corminboeuf, T Laino, 10.1002/wcms.1604WIREs Computational Molecular Science. 1252022</p>
<p>Prediction of chemical reaction yields using deep learning. P Schwaller, A C Vaucher, T Laino, J.-L Reymond, 10.1088/2632-2153/abc81dMachine Learning: Science and Technology. 212021Publisher: IOP Publishing</p>
<p>Global reactivity models are impactful in industrial synthesis applications. P Neves, K Mcclure, J Verhoeven, N Dyubankova, R Nugmanov, A Gedich, S Menon, Z Shi, J K Wegner, 10.1186/s13321-023-00685-0Journal of Cheminformatics. 1512023</p>
<p>J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, 10.48550/arXiv.2106.09553arXiv.arXiv:2106.09553Large-Scale Chemical Language Representations Capture Molecular Structure and Properties. 2022cs, q-bio</p>
<p>Molformer: Motif-based Transformer on 3D Heterogeneous Molecular Graphs. F Wu, D Radev, S Z Li, 10.48550/arXiv.2110.01191arXiv.arXiv:2110.011912023cs, q-bio</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. A Rives, J Meier, T Sercu, S Goyal, Z Lin, J Liu, D Guo, M Ott, C L Zitnick, J Ma, R Fergus, 10.1073/pnas.20162391182023-08-07Proceedings of the National Academy of Sciences. 118152016239118. 2021Proceedings of the National Academy of Sciences</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, H Akin, R Rao, B Hie, Z Zhu, W Lu, N Smetanin, R Verkuil, O Kabeli, Y Shmueli, A Santos Costa, M Fazel-Zarandi, T Sercu, S Candido, 10.1126/science.ade25742023-08-07Science. 37966372023American Association for the Advancement of Science</p>
<p>Language models generalize beyond natural proteins. R Verkuil, O Kabeli, Y Du, B I M Wicky, L F Milles, J Dauparas, D Baker, S Ovchinnikov, T Sercu, A Rives, 10.1101/2022.12.21.521521v1bioRxiv. 2022Pages: 2022.12.21.521521 Section: New Results</p>
<p>Language models can identify enzymatic active sites in protein sequences. Y G N Teukam, L K Dassi, M Manica, D Probst, T Laino, </p>
<p>C Edwards, T Lai, K Ros, G Honke, K Cho, H Ji, 10.48550/arXiv.2204.11817arXiv.arXiv:2204.11817Translation between Molecules and Natural Language. 2022</p>
<p>Unifying Molecular and Textual Representations via Multi-task Language Modelling. D Christofidellis, G Giannone, J Born, O Winther, T Laino, M Manica, 10.48550/arXiv.2301.12586arXiv.arXiv:2301.125862023</p>
<p>Leveraging Infrared Spectroscopy for Automated Structure Elucidation. M Alberts, T Laino, A C Vaucher, 10.26434/chemrxiv-2023-5v27fChemistry. May 2023preprint</p>
<p>Finetuning Large Language Models. S Raschka, 2023</p>
<p>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y Qiao, arXiv.arXiv:2303.161992023</p>
<p>Is GPT-3 all you need for low-data discovery in chemistry?. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.26434/chemrxiv-2023-fw8n4ChemRxiv. 2023</p>
<p>M C Ramos, S S Michtavy, M D Porosoff, A D White, 10.48550/arXiv.2304.05341arXiv.arXiv:2304.05341Bayesian Optimization of Catalysts With In-context Learning. 2023</p>
<p>K M Jablonka, Q Ai, A Al-Feghali, S Badhwar, J D Bocarsly, A M Bran, S Bringuier, L C Brinson, K Choudhary, D Circi, S Cox, W A Jong, M L Evans, N Gastellu, J Genzling, M V Gil, A K Gupta, Z Hong, A Imran, S Kruschwitz, A Labarre, J Lála, T Liu, S Ma, S Majumdar, G W Merz, N Moitessier, E Moubarak, B Mouriño, B Pelkie, M Pieler, M C Ramos, B Ranković, S G Rodriques, J N Sanders, P Schwaller, M Schwarting, J Shi, B Smit, B E Smith, J Van Herck, C Völker, L Ward, S Warren, B Weiser, S Zhang, X Zhang, G A Zia, A Scourtas, K J Schmidt, I Foster, A D White, B Blaiszik, 10.48550/arXiv.2306.06283arXiv.arXiv:2306.0628314 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. 2023cond-mat, physics:physics</p>
<p>A M Bran, S Cox, A D White, P Schwaller, 10.48550/arXiv.2304.05376arXiv.arXiv:2304.05376ChemCrow: Augmenting largelanguage models with chemistry tools. 2023physics, stat</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, 10.48550/ARXIV.2304.053322023Publisher: arXiv Version Number: 1</p>
<p>J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, T Hennigan, E Noland, K Millican, G V D Driessche, B Damoc, A Guy, S Osindero, K Simonyan, E Elsen, J W Rae, O Vinyals, L Sifre, arXiv.arXiv:2203.15556Training Compute-Optimal Large Language Models. 2022</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, 10.48550/ARXIV.2206.07682Emergent Abilities of Large Language Models. 2022Publisher: arXiv Version Number: 2</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 10.48550/arXiv.2201.11903arXiv.arXiv:2201.119032023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 10.48550/arXiv.2203.02155arXiv.arXiv:2203.021552022</p>
<p>Finetuned Language Models are Zero-Shot Learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, 2021</p>
<p>Fine-tuning and visualization of convolutional neural networks. X Yin, W Chen, X Wu, H Yue, 10.1109/ICIEA.2017.828304112th IEEE Conference on Industrial Electronics and Applications (ICIEA). 2017. 2017</p>
<p>Universal Language Model Fine-tuning for Text Classification. J Howard, S Ruder, 10.18653/v1/P18-1031Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia20181Association for Computational Linguistics</p>
<p>Retrosynthesis Prediction with Conditional Graph Logic Network. H Dai, C Li, C W Coley, B Dai, L Song, 10.48550/arXiv.2001.01408arXiv.arXiv:2001.014082020cs, stat</p>
<p>Chemistry-informed molecular graph as reaction descriptor for machine-learned retrosynthesis planning. B Zhang, X Zhang, W Du, Z Song, G Zhang, G Zhang, Y Wang, X Chen, J Jiang, Y Luo, 10.1073/pnas.22127111192023-08-07Company: National Academy of Sciences Distributor: National Academy of Sciences Institution. 1194122127111192022National Academy of Sciences Label: National Academy of Sciences Publisher: Proceedings of the National Academy of SciencesProceedings of the National Academy of Sciences</p>
<p>. K Jorner, L Turcani, 10.5281/zenodo.70175992022</p>
<p>Bayesian optimisation for additive screening and yield improvements in chemical reactions -beyond one-hot encoding. B Ranković, R.-R Griffiths, H B Moss, P Schwaller, 10.26434/chemrxiv-2022-nll2j-v3ChemRxiv. 2023</p>
<p>Bayesian reaction optimization as a tool for chemical synthesis. B J Shields, J Stevens, J Li, M Parasram, F Damani, J I M Alvarado, J M Janey, R P Adams, A G Doyle, 10.1038/s41586-021-03213-y2023-02-08Nature. 59078442021Nature Publishing Group</p>
<p>A Transformer-based Generative Model for De Novo Molecular Design. W Wang, Y Wang, H Zhao, S Sciabola, 10.48550/arXiv.2210.08749arXiv.arXiv:2210.087492022cs, q-bio</p>
<p>C5T5: Controllable Generation of Organic Molecules with Transformers. D Rothchild, A Tamkin, J Yu, U Misra, J Gonzalez, 10.48550/arXiv.2108.10307arXiv.arXiv:2108.103072021</p>
<p>MolGPT: Molecular Generation Using a Transformer-Decoder Model. V Bagal, R Aggarwal, P K Vinod, U D Priyakumar, 10.1021/acs.jcim.1c006002023-08-05Journal of Chemical Information and Modeling. 6292022American Chemical Society</p>
<p>Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation. E Bengio, M Jain, M Korablyov, D Precup, Y Bengio, 10.48550/arXiv.2106.04399arXiv.arXiv:2106.043992021</p>
<p>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, 10.1038/s42256-023-00639-z2023-07-30Nature Machine Intelligence. 542023Nature Publishing Group</p>
<p>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. D Flam-Shepherd, A Aspuru-Guzik, 10.48550/arXiv.2305.05708arXiv.arXiv:2305.057082023cs, q-bio</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, 10.48550/arXiv.2302.04761arXiv:2302.04761Toolformer: Language Models Can Teach Themselves to Use Tools. arXiv. 2023</p>
<p>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. E Karpas, O Abend, Y Belinkov, B Lenz, O Lieber, N Ratner, Y Shoham, H Bata, Y Levine, K Leyton-Brown, D Muhlgay, N Rozen, E Schwartz, G Shachaf, S Shalev-Shwartz, A Shashua, M Tenenholtz, arXiv.arXiv:2205.004452022</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv.arXiv:2210.03629ReAct: Synergizing Reasoning and Acting in Language Models. 2023</p>
<p>Assessment of chemistry knowledge in large language models that generate code. D White, A , M Hocky, G , A Gandhi, H Ansari, M Cox, S , P Wellawatte, G Sasmal, S Yang, Z Liu, K Singh, Y Ccoa, W J P , 10.1039/D2DD00087CDigital Discovery. 222023. 2023-07-25Royal Society of Chemistry</p>            </div>
        </div>

    </div>
</body>
</html>
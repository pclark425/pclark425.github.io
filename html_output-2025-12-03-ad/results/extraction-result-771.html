<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-771 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-771</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-771</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-268666998</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.14843v1.pdf" target="_blank">Local Causal Discovery with Linear non-Gaussian Cyclic Models</a></p>
                <p><strong>Paper Abstract:</strong> Local causal discovery is of great practical significance, as there are often situations where the discovery of the global causal structure is unnecessary, and the interest lies solely on a single target variable. Most existing local methods utilize conditional independence relations, providing only a partially directed graph, and assume acyclicity for the ground-truth structure, even though real-world scenarios often involve cycles like feedback mechanisms. In this work, we present a general, unified local causal discovery method with linear non-Gaussian models, whether they are cyclic or acyclic. We extend the application of independent component analysis from the global context to independent subspace analysis, enabling the exact identification of the equivalent local directed structures and causal strengths from the Markov blanket of the target variable. We also propose an alternative regression-based method in the particular acyclic scenarios. Our identifiability results are empirically validated using both synthetic and real-world datasets.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e771.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e771.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Local ISA-LiNG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Independent Subspace Analysis for LiNG models (Local ISA-LiNG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed local causal discovery algorithm that applies Independent Subspace Analysis (ISA) on the target and its Markov blanket to recover all identifiable directed edges into the target and its children even in presence of hidden variables and cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Local ISA-LiNG (Algorithm 1)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Performs ISA on the variable set S = {T} ∪ MB(T) to demix observed variables into independent irreducible subspaces, finds admissible row-permutations (block-wise invertible diagonal blocks) to align ISA output to the ISA characterization A^{-1}_{S,S}, and then reads off rows corresponding to the target and its children to recover incoming causal strengths (up to scaling indeterminacies resolved per-block). It includes post-processing to enumerate all admissible permutations and to extract the local equivalence-class of directed edges; a stability filter may be applied to prefer convergent (stable) global models.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear non-Gaussian SEM simulations (LiNG) and Sachs protein-signaling data</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Evaluated on synthetic LiNG SEMs (both cyclic and acyclic: simulated 50-node graphs with varied degree, typically 2000 samples) and on a real-world protein-signaling observational dataset (Sachs et al.); environments are observational simulation datasets (not interactive/online experimental labs).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Independent Subspace Analysis to group signals that share common hidden confounders (latent distractors), admissible permutation detection via block-rank checks to eliminate spurious nonzeros, nodewise Lasso for MB estimation to block confounding paths; optional stability filtering to prefer stable models.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounders (latent variables outside the local variable set S that cause overcomplete mixing), external cyclic paths that appear as self-loops locally, spurious nonzero entries due to subspace scaling/permute indeterminacies.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects confounding/distractors by (1) ISA: presence of multi-dimensional irreducible subspaces indicates shared hidden sources; (2) support of target column in ISA demixing W indicates which rows correspond to T and its children; (3) rank-deficient diagonal blocks under incorrect permutations reveal spurious nonzeros (Definition 4).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Subspace-wise scaling normalization (general scaling matrices consistent with ISA partition) and thresholding small demixing/weight entries (implementation uses absolute-value threshold 0.05); nodewise Lasso regularization used in MB estimation to shrink irrelevant variables.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Independence tests to validate separated components (HSIC) and regression residual independence checks in post-processing; optional stability check that discards non-convergent candidate B' matrices (filtering by spectral radius < 1).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>The method achieves substantially lower structural Hamming distance (SHD) on local DCG/DAG recovery compared to baselines (GSBN, Local A*, CMB, LDECC) in the presented cyclic and acyclic simulations; asymptotically both structure and parameter estimates converge (SHD and L2 error decrease with sample size). (Results shown in paper figures; no single scalar metric is given in-text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines that do not use ISA or the MB-blocking (e.g., GSBN, Local A*, CMB, LDECC) have higher SHD on local structure recovery in the same synthetic experiments; also ICA-LiNG on all variables (global) is not applicable locally when latent sources exist. (Paper reports relative worsening but exact numbers are in figures.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Local ISA-LiNG can recover all incoming causal effects to the target and its children from only the local variables even when latent confounders and cycles exist outside the local set S; ISA groups variables sharing hidden confounders into multi-dimensional subspaces so that 1-dim components (target and unconfounded children) can be identified and their causal weights recovered; admissible permutation via invertible block diagonal requirement eliminates spurious permutations. Stability filtering recovers unique stable local model when cycles are disjoint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local Causal Discovery with Linear non-Gaussian Cyclic Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e771.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e771.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ISA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Independent Subspace Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalization of ICA that decomposes observed mixtures into mutually independent irreducible subspaces (multi-dimensional independent components) rather than strictly 1-D independent components, used here to handle cases where observed signals are fewer than latent sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Independent Subspace Analysis (ISA) as applied to LiNG models</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Estimate an invertible demixing W so that WY = (Z_1^T, ..., Z_k^T)^T where Z_i are mutually independent irreducible random vectors; estimation is unique up to block-permutation and block-wise scaling. In this paper, theorem shows A^{-1}_{S,S} (inverse principal submatrix of mixing matrix) is an ISA solution for X_S in LiNG models, enabling recovery of subspaces corresponding to variables whose parents are contained in S.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Local observational subsets of LiNG SEMs (synthetic simulations and applied to real data)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied on the local variable subset S = {T} ∪ MB(T) drawn from global LiNG SEMs; non-interventional observational environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Groups observed signals into subspaces corresponding to shared latent sources (hidden confounders) so that confounded variables form a multi-dimensional subspace while unconfounded variables form 1-D components; this separation isolates distractor influence.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden latent confounders (overcomplete mixing), multi-variable confounding, external cycles appearing as self-loops locally.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Presence of multi-dimensional subspaces (partition Γ_W) indicates shared confounders; the ISA partition is detected via independence tests (HSIC) on ICA components in implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Block-wise scaling indeterminacy is resolved via computing block inverse on permuted W to align to A^{-1}_{S,S}; small entries thresholded in practice. No specific probabilistic downweighting beyond scaling and thresholding is introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Validate 1-D components via independence to regressors or HSIC tests; use theoretical identifiability results (Theorem 2 and Lemma 1) to rule out spurious interpretations when parents of a node are in S.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>ISA-based demixing recovers irreducible subspaces that allow exact identification of target incoming effects (up to per-subspace scaling and permutation) and leads to lower SHD in experiments versus methods not using ISA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ISA provides the right indeterminacy structure for local LiNG models with latent variables: A^{-1}_{S,S} is an ISA solution and enables extraction of exogenous noise components for nodes whose parents are included in S; multi-dimensional subspaces correspond to groups of variables entangled by shared hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local Causal Discovery with Linear non-Gaussian Cyclic Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e771.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e771.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Admissible permutations / Rank-deficiency test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Admissible Row Permutations via invertible block diagonals (rank-deficiency detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing criterion that retains only those row permutations of the ISA demixing matrix where each block corresponding to an ISA subspace has an invertible diagonal block, which detects and removes spurious nonzero patterns caused by incorrect permutation/scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Admissible permutations (Definition 4 and related post-processing)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given an ISA solution W and partition Γ_W, consider row permutations π; π is admissible if for every block S_i in Γ_W the submatrix (P_π W)_{π[S_i],π[S_i]} has full rank |S_i| (i.e., invertible diagonal blocks after grouping). This detects permutations that create spurious nonzeros (which appear rank-deficient) and enumerates valid alignments to A^{-1}_{S,S}.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Local ISA demixing outputs on LiNG SEM subsets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied as a deterministic numerical post-processing step; no interaction or active experiments involved.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects spurious nonzeros arising from mixing with hidden confounders and incorrect permutation/scaling by checking block-rank; preserves permutations that map ISA subspaces to correct variable groups, thereby isolating distractor-induced mixing.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious nonzero entries in demixing rows due to subspace-scaling ambiguity; fake edges arising from incorrect row permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Rank checks on diagonal blocks: if a candidate permutation yields rank-deficient diagonal block(s), it is flagged as spurious and rejected.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Rejects candidate permutations that would create inconsistent (rank-deficient) diagonal blocks, effectively refuting spurious edge assignments derived from those permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Using admissible permutations eliminates incorrect permutations that would otherwise produce wrong local edges (shown in illustrative examples), improving structural recovery (lower SHD) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without enforcing block-invertibility, naive nonzero-diagonal permutation criteria can accept entirely incorrect permutations and lead to wrong edges (demonstrated in Example 4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Block-rank/invertibility is a principled criterion to rule out spurious permutations of ISA outputs; it correctly recovers the row alignment to A^{-1}_{S,S} and thereby reduces false-positive edges resulting from permutation indeterminacy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local Causal Discovery with Linear non-Gaussian Cyclic Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e771.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e771.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inverse Direct-LiNGAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Direct-LiNGAM (regression-based local variant for acyclic case)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A regression-based local algorithm (Algorithm 2) that identifies 'local leaves' via independent residuals to recover edges into the target and its children in acyclic LiNGAMs when only a local subset is available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Inverse Direct-LiNGAM (Algorithm 2)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Recursively searches bottom-up for variables j in U = {T} ∪ MB(T) such that the OLS regression residual R_{U\{j} → j} is independent of regressors X_{U\{j}} (guaranteed when parents of j are contained in U and none of j's descendants remain in U). For such j, regression coefficients equal true direct effects B_{j,·}. The algorithm removes identified leaves and iterates; extra checks detect spouses by zero-coefficient tests and avoid erroneous inclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Acyclic LiNGAM local search (simulated DAGs and synthetic datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates on observational data from linear non-Gaussian acyclic models restricted to local variable sets; not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Uses the Markov blanket property: by working on S that contains parents of target and its children, confounding paths are blocked so residuals reveal exogenous noise; spouses and hidden confounders outside S are handled via removing variables and zero-coefficient checks.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounders outside the local set, spouses (variables that share children with T), and confounding paths causing dependent residuals.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects non-leaf (confounded) variables because regression residuals fail independence tests; detects spouses via checking whether coefficient β_{T, U\{k} → k} is zero.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Independent residual test (if residual is independent, it refutes presence of descendants/confounders in U) and coefficient-zero tests to refute spurious parent assignments; iterative removal of identified leaves corrects for spouses to avoid false edges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Provably correct for acyclic LiNGAMs: Theorem 4 states Algorithm 2 recovers all incoming edges to T and its children; experiments on simulated DAGs show lower SHD relative to baselines when used with appropriate MB estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>While Direct-LiNGAM fails locally due to latent confounders, reversing to identify 'local leaves' via independent residuals successfully recovers target-adjacent edges when parents of leaves are contained in the local set; spouse-corrections are necessary to avoid false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local Causal Discovery with Linear non-Gaussian Cyclic Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e771.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e771.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MB via inverse-covariance / nodewise Lasso</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Markov Blanket discovery by inverse-covariance support and nodewise Lasso (generalized to cyclic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure to estimate the Markov blanket of the target in linear cyclic SEMs by relating the support of the inverse covariance (precision) matrix to the moral graph and using nodewise Lasso to estimate support in finite samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MB discovery via inverse covariance support and nodewise Lasso</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Under Assumption 1, the support of the inverse covariance Θ equals the moral graph of the directed graph G; therefore MB(T) can be recovered from nonzero entries in Θ. Practically, nodewise Lasso regression is used to estimate conditional dependencies and infer MB(T) in finite samples, extended here to cyclic LiNG models.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear SEMs with cycles (simulated data); used as preprocessing for local methods</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static observational datasets from linear cyclic SEMs; not an interactive environment. MB estimation is a preprocessing step to provide local S for Local ISA-LiNG and Algorithm 2.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>By estimating MB(T), the method ensures that parents of target and its children are included in S, which blocks confounding paths from variables outside S (distractors) and enables reliable local identification.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Variables outside local MB acting as hidden confounders, indirect associations due to shared children (spouses).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects adjacency through nonzero entries in estimated inverse covariance (Θ) or nonzero nodewise Lasso coefficients; relies on Proposition 1 linking Θ-support to moral graph under Assumption 1.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Lasso regularization in nodewise regression reduces influence of irrelevant variables by shrinking coefficients to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Theoretical equivalence (under Assumption 1) and empirical thresholding of estimated precision support refute spurious adjacencies; in finite samples regularization helps avoid false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Using estimated MB (via nodewise Lasso) as input, Local ISA-LiNG achieves low SHD; experiments compare oracle MB vs estimated MB showing method is robust though performance improves with oracle MB.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>If MB is inaccurately estimated (e.g., missing parents), ISA-based local identification can fail; the paper reports both oracle and estimated MB experiments (figures) showing degraded performance with estimated MB but still better than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generalizing Loh & Bühlmann's inverse-covariance idea to cyclic graphs allows MB estimation via nodewise Lasso; accurate MB estimation is crucial because it blocks confounding paths and is a key enabler for the ISA and regression-based local identification methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local Causal Discovery with Linear non-Gaussian Cyclic Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e771.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e771.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HSIC grouping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HSIC-based subspace identification (ICA + HSIC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An implementation detail: estimate ICA components, then use HSIC independence tests to group components into ISA subspaces (identify which components belong to the same independent subspace).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A kernel statistical test of independence</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ICA followed by HSIC to detect ISA partitions</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Run ICA to get one-dimensional components, then apply pairwise or group-level HSIC (Hilbert-Schmidt Independence Criterion) independence tests to decide which ICA components are dependent and should be grouped into the same ISA subspace; yields partition Γ_W for ISA post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied on demixed components from ISA step on LiNG SEM local subsets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Used as a computational procedure on observational datasets; no active experiment selection.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Groups ICA components that are dependent due to shared latent distractors into multi-dimensional subspaces, preventing incorrect assumption of mutual independence among components.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Shared latent sources causing dependence between ICA components (hidden confounders).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>HSIC independence testing to find dependent ICA components; components grouped if dependence is detected (significance level used in implementation: 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Independence test refutes the hypothesis that two components are independent and therefore supports grouping them into same subspace.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Practical implementation found to work well: HSIC grouping produced reliable ISA partitions that enabled downstream admissible-permutation processing and accurate local edge estimation (reported in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using HSIC on ICA outputs is an effective and practical method to obtain ISA partitions in the paper's pipeline; it operationalizes ISA estimation for causal identification with latent confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local Causal Discovery with Linear non-Gaussian Cyclic Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e771.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e771.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stability filtering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local stability-based filtering of candidate LiNG solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to select locally-identifiable solutions that correspond to globally stable LiNG models by checking convergence (spectral radius) of candidate adjacency matrices and discarding non-convergent ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Stability filtering (spectral-radius-based model selection)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>From the set B of global LiNG equivalence-class adjacency matrices consistent with an ISA-demixed solution, compute each candidate B' and test whether it is convergent (lim_{t→∞} B'^t = 0), typically by checking spectral radius < 1; discard non-convergent candidates. Under disjoint cycles, this yields a unique global stable model locally.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated LiNG SEMs with disjoint cycles (synthetic experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied as an additional selection criterion on candidate local models produced by ISA post-processing; not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Not directly a distractor-removal method, but helps reject candidate models that are dynamically unstable (which may correspond to spurious parameter configurations arising from ambiguous permutations).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Ambiguous equivalent LiNG solutions that are dynamically unstable (explosive cycles) which are unlikely to be the true data-generating stable system.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compute spectral radius (largest eigenvalue magnitude) of candidate adjacency B'; if ≥1 mark as non-convergent and skip.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refute candidate B' as plausible global model if it is not convergent (unstable); keep only convergent ones.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When cycles are disjoint and true global model is stable, adding the stability check identifies the unique global stable solution locally (Corollary 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Local stability check (spectral radius test) can uniquely pick out the global stable solution from the local equivalence class when cycles are disjoint; however, when cycles intersect, local stability tests cannot reliably identify global stable solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local Causal Discovery with Linear non-Gaussian Cyclic Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e771.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e771.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thresholding / Lasso regularization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coefficient thresholding and nodewise Lasso regularization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practical numerical regularization techniques used in implementation: nodewise Lasso for MB estimation and absolute thresholding of small demixing/weight entries to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Nodewise Lasso + thresholding</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Nodewise Lasso regression (Meinshausen & Bühlmann style) is used to estimate support of precision matrix / MB; small-magnitude entries in demixing/adjacency estimates are thresholded (paper uses threshold 0.05) to reduce false positives and numerical noise.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite-sample implementations on synthetic LiNG SEMs and the Sachs dataset</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Practical preprocessing/regularization steps applied to batch observational datasets; no active experiment selection.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Regularization (Lasso) shrinks coefficients of irrelevant/distractor variables towards zero; thresholding removes small spurious demixing/adjacency values that could be due to noise or estimation error.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Measurement noise, finite-sample estimation noise, irrelevant variables producing small spurious associations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Nonzero Lasso coefficients indicate candidate neighbors; small absolute values below threshold considered spurious and removed.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>L1 regularization (Lasso) and hard thresholding reduce influence of weak/spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Regularization and thresholding improve finite-sample robustness: experimental SHD and weight-estimation errors decrease with these practical choices (as reported in implementation notes and figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without regularization/thresholding, estimation is more noisy and produces higher SHD and worse parameter estimates in finite samples (implied by comparisons using estimated vs oracle MB and across sample sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Lasso-based MB estimation and thresholding of small entries are effective practical measures to reduce false positives arising from finite-sample noise and help pipeline stability and accuracy in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Local Causal Discovery with Linear non-Gaussian Cyclic Models', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Discovering cyclic causal models by independent components analysis <em>(Rating: 2)</em></li>
                <li>Estimation of causal effects using linear non-Gaussian causal models with hidden variables <em>(Rating: 2)</em></li>
                <li>High-dimensional learning of linear causal networks via inverse covariance estimation <em>(Rating: 2)</em></li>
                <li>DirectLiNGAM: A method for learning a linear non-Gaussian acyclic model <em>(Rating: 2)</em></li>
                <li>Independent subspace analysis and related works (Theis 2006 / Hyvärinen & Hoyer 2000) <em>(Rating: 2)</em></li>
                <li>Local causal discovery of direct causes and effects <em>(Rating: 1)</em></li>
                <li>Local causal discovery for estimating causal effects <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-771",
    "paper_id": "paper-268666998",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "Local ISA-LiNG",
            "name_full": "Local Independent Subspace Analysis for LiNG models (Local ISA-LiNG)",
            "brief_description": "The paper's proposed local causal discovery algorithm that applies Independent Subspace Analysis (ISA) on the target and its Markov blanket to recover all identifiable directed edges into the target and its children even in presence of hidden variables and cycles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Local ISA-LiNG (Algorithm 1)",
            "method_description": "Performs ISA on the variable set S = {T} ∪ MB(T) to demix observed variables into independent irreducible subspaces, finds admissible row-permutations (block-wise invertible diagonal blocks) to align ISA output to the ISA characterization A^{-1}_{S,S}, and then reads off rows corresponding to the target and its children to recover incoming causal strengths (up to scaling indeterminacies resolved per-block). It includes post-processing to enumerate all admissible permutations and to extract the local equivalence-class of directed edges; a stability filter may be applied to prefer convergent (stable) global models.",
            "environment_name": "Linear non-Gaussian SEM simulations (LiNG) and Sachs protein-signaling data",
            "environment_description": "Evaluated on synthetic LiNG SEMs (both cyclic and acyclic: simulated 50-node graphs with varied degree, typically 2000 samples) and on a real-world protein-signaling observational dataset (Sachs et al.); environments are observational simulation datasets (not interactive/online experimental labs).",
            "handles_distractors": true,
            "distractor_handling_technique": "Independent Subspace Analysis to group signals that share common hidden confounders (latent distractors), admissible permutation detection via block-rank checks to eliminate spurious nonzeros, nodewise Lasso for MB estimation to block confounding paths; optional stability filtering to prefer stable models.",
            "spurious_signal_types": "Hidden confounders (latent variables outside the local variable set S that cause overcomplete mixing), external cyclic paths that appear as self-loops locally, spurious nonzero entries due to subspace scaling/permute indeterminacies.",
            "detection_method": "Detects confounding/distractors by (1) ISA: presence of multi-dimensional irreducible subspaces indicates shared hidden sources; (2) support of target column in ISA demixing W indicates which rows correspond to T and its children; (3) rank-deficient diagonal blocks under incorrect permutations reveal spurious nonzeros (Definition 4).",
            "downweighting_method": "Subspace-wise scaling normalization (general scaling matrices consistent with ISA partition) and thresholding small demixing/weight entries (implementation uses absolute-value threshold 0.05); nodewise Lasso regularization used in MB estimation to shrink irrelevant variables.",
            "refutation_method": "Independence tests to validate separated components (HSIC) and regression residual independence checks in post-processing; optional stability check that discards non-convergent candidate B' matrices (filtering by spectral radius &lt; 1).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "The method achieves substantially lower structural Hamming distance (SHD) on local DCG/DAG recovery compared to baselines (GSBN, Local A*, CMB, LDECC) in the presented cyclic and acyclic simulations; asymptotically both structure and parameter estimates converge (SHD and L2 error decrease with sample size). (Results shown in paper figures; no single scalar metric is given in-text.)",
            "performance_without_robustness": "Baselines that do not use ISA or the MB-blocking (e.g., GSBN, Local A*, CMB, LDECC) have higher SHD on local structure recovery in the same synthetic experiments; also ICA-LiNG on all variables (global) is not applicable locally when latent sources exist. (Paper reports relative worsening but exact numbers are in figures.)",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Local ISA-LiNG can recover all incoming causal effects to the target and its children from only the local variables even when latent confounders and cycles exist outside the local set S; ISA groups variables sharing hidden confounders into multi-dimensional subspaces so that 1-dim components (target and unconfounded children) can be identified and their causal weights recovered; admissible permutation via invertible block diagonal requirement eliminates spurious permutations. Stability filtering recovers unique stable local model when cycles are disjoint.",
            "uuid": "e771.0",
            "source_info": {
                "paper_title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ISA",
            "name_full": "Independent Subspace Analysis",
            "brief_description": "A generalization of ICA that decomposes observed mixtures into mutually independent irreducible subspaces (multi-dimensional independent components) rather than strictly 1-D independent components, used here to handle cases where observed signals are fewer than latent sources.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Independent Subspace Analysis (ISA) as applied to LiNG models",
            "method_description": "Estimate an invertible demixing W so that WY = (Z_1^T, ..., Z_k^T)^T where Z_i are mutually independent irreducible random vectors; estimation is unique up to block-permutation and block-wise scaling. In this paper, theorem shows A^{-1}_{S,S} (inverse principal submatrix of mixing matrix) is an ISA solution for X_S in LiNG models, enabling recovery of subspaces corresponding to variables whose parents are contained in S.",
            "environment_name": "Local observational subsets of LiNG SEMs (synthetic simulations and applied to real data)",
            "environment_description": "Applied on the local variable subset S = {T} ∪ MB(T) drawn from global LiNG SEMs; non-interventional observational environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Groups observed signals into subspaces corresponding to shared latent sources (hidden confounders) so that confounded variables form a multi-dimensional subspace while unconfounded variables form 1-D components; this separation isolates distractor influence.",
            "spurious_signal_types": "Hidden latent confounders (overcomplete mixing), multi-variable confounding, external cycles appearing as self-loops locally.",
            "detection_method": "Presence of multi-dimensional subspaces (partition Γ_W) indicates shared confounders; the ISA partition is detected via independence tests (HSIC) on ICA components in implementation.",
            "downweighting_method": "Block-wise scaling indeterminacy is resolved via computing block inverse on permuted W to align to A^{-1}_{S,S}; small entries thresholded in practice. No specific probabilistic downweighting beyond scaling and thresholding is introduced.",
            "refutation_method": "Validate 1-D components via independence to regressors or HSIC tests; use theoretical identifiability results (Theorem 2 and Lemma 1) to rule out spurious interpretations when parents of a node are in S.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "ISA-based demixing recovers irreducible subspaces that allow exact identification of target incoming effects (up to per-subspace scaling and permutation) and leads to lower SHD in experiments versus methods not using ISA.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "ISA provides the right indeterminacy structure for local LiNG models with latent variables: A^{-1}_{S,S} is an ISA solution and enables extraction of exogenous noise components for nodes whose parents are included in S; multi-dimensional subspaces correspond to groups of variables entangled by shared hidden confounders.",
            "uuid": "e771.1",
            "source_info": {
                "paper_title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Admissible permutations / Rank-deficiency test",
            "name_full": "Admissible Row Permutations via invertible block diagonals (rank-deficiency detection)",
            "brief_description": "A post-processing criterion that retains only those row permutations of the ISA demixing matrix where each block corresponding to an ISA subspace has an invertible diagonal block, which detects and removes spurious nonzero patterns caused by incorrect permutation/scaling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Admissible permutations (Definition 4 and related post-processing)",
            "method_description": "Given an ISA solution W and partition Γ_W, consider row permutations π; π is admissible if for every block S_i in Γ_W the submatrix (P_π W)_{π[S_i],π[S_i]} has full rank |S_i| (i.e., invertible diagonal blocks after grouping). This detects permutations that create spurious nonzeros (which appear rank-deficient) and enumerates valid alignments to A^{-1}_{S,S}.",
            "environment_name": "Local ISA demixing outputs on LiNG SEM subsets",
            "environment_description": "Applied as a deterministic numerical post-processing step; no interaction or active experiments involved.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects spurious nonzeros arising from mixing with hidden confounders and incorrect permutation/scaling by checking block-rank; preserves permutations that map ISA subspaces to correct variable groups, thereby isolating distractor-induced mixing.",
            "spurious_signal_types": "Spurious nonzero entries in demixing rows due to subspace-scaling ambiguity; fake edges arising from incorrect row permutations.",
            "detection_method": "Rank checks on diagonal blocks: if a candidate permutation yields rank-deficient diagonal block(s), it is flagged as spurious and rejected.",
            "downweighting_method": null,
            "refutation_method": "Rejects candidate permutations that would create inconsistent (rank-deficient) diagonal blocks, effectively refuting spurious edge assignments derived from those permutations.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Using admissible permutations eliminates incorrect permutations that would otherwise produce wrong local edges (shown in illustrative examples), improving structural recovery (lower SHD) in experiments.",
            "performance_without_robustness": "Without enforcing block-invertibility, naive nonzero-diagonal permutation criteria can accept entirely incorrect permutations and lead to wrong edges (demonstrated in Example 4).",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Block-rank/invertibility is a principled criterion to rule out spurious permutations of ISA outputs; it correctly recovers the row alignment to A^{-1}_{S,S} and thereby reduces false-positive edges resulting from permutation indeterminacy.",
            "uuid": "e771.2",
            "source_info": {
                "paper_title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Inverse Direct-LiNGAM",
            "name_full": "Inverse Direct-LiNGAM (regression-based local variant for acyclic case)",
            "brief_description": "A regression-based local algorithm (Algorithm 2) that identifies 'local leaves' via independent residuals to recover edges into the target and its children in acyclic LiNGAMs when only a local subset is available.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Inverse Direct-LiNGAM (Algorithm 2)",
            "method_description": "Recursively searches bottom-up for variables j in U = {T} ∪ MB(T) such that the OLS regression residual R_{U\\{j} → j} is independent of regressors X_{U\\{j}} (guaranteed when parents of j are contained in U and none of j's descendants remain in U). For such j, regression coefficients equal true direct effects B_{j,·}. The algorithm removes identified leaves and iterates; extra checks detect spouses by zero-coefficient tests and avoid erroneous inclusions.",
            "environment_name": "Acyclic LiNGAM local search (simulated DAGs and synthetic datasets)",
            "environment_description": "Operates on observational data from linear non-Gaussian acyclic models restricted to local variable sets; not interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Uses the Markov blanket property: by working on S that contains parents of target and its children, confounding paths are blocked so residuals reveal exogenous noise; spouses and hidden confounders outside S are handled via removing variables and zero-coefficient checks.",
            "spurious_signal_types": "Hidden confounders outside the local set, spouses (variables that share children with T), and confounding paths causing dependent residuals.",
            "detection_method": "Detects non-leaf (confounded) variables because regression residuals fail independence tests; detects spouses via checking whether coefficient β_{T, U\\{k} → k} is zero.",
            "downweighting_method": null,
            "refutation_method": "Independent residual test (if residual is independent, it refutes presence of descendants/confounders in U) and coefficient-zero tests to refute spurious parent assignments; iterative removal of identified leaves corrects for spouses to avoid false edges.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Provably correct for acyclic LiNGAMs: Theorem 4 states Algorithm 2 recovers all incoming edges to T and its children; experiments on simulated DAGs show lower SHD relative to baselines when used with appropriate MB estimation.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "While Direct-LiNGAM fails locally due to latent confounders, reversing to identify 'local leaves' via independent residuals successfully recovers target-adjacent edges when parents of leaves are contained in the local set; spouse-corrections are necessary to avoid false positives.",
            "uuid": "e771.3",
            "source_info": {
                "paper_title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MB via inverse-covariance / nodewise Lasso",
            "name_full": "Markov Blanket discovery by inverse-covariance support and nodewise Lasso (generalized to cyclic graphs)",
            "brief_description": "A procedure to estimate the Markov blanket of the target in linear cyclic SEMs by relating the support of the inverse covariance (precision) matrix to the moral graph and using nodewise Lasso to estimate support in finite samples.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "MB discovery via inverse covariance support and nodewise Lasso",
            "method_description": "Under Assumption 1, the support of the inverse covariance Θ equals the moral graph of the directed graph G; therefore MB(T) can be recovered from nonzero entries in Θ. Practically, nodewise Lasso regression is used to estimate conditional dependencies and infer MB(T) in finite samples, extended here to cyclic LiNG models.",
            "environment_name": "Linear SEMs with cycles (simulated data); used as preprocessing for local methods",
            "environment_description": "Static observational datasets from linear cyclic SEMs; not an interactive environment. MB estimation is a preprocessing step to provide local S for Local ISA-LiNG and Algorithm 2.",
            "handles_distractors": true,
            "distractor_handling_technique": "By estimating MB(T), the method ensures that parents of target and its children are included in S, which blocks confounding paths from variables outside S (distractors) and enables reliable local identification.",
            "spurious_signal_types": "Variables outside local MB acting as hidden confounders, indirect associations due to shared children (spouses).",
            "detection_method": "Detects adjacency through nonzero entries in estimated inverse covariance (Θ) or nonzero nodewise Lasso coefficients; relies on Proposition 1 linking Θ-support to moral graph under Assumption 1.",
            "downweighting_method": "Lasso regularization in nodewise regression reduces influence of irrelevant variables by shrinking coefficients to zero.",
            "refutation_method": "Theoretical equivalence (under Assumption 1) and empirical thresholding of estimated precision support refute spurious adjacencies; in finite samples regularization helps avoid false positives.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Using estimated MB (via nodewise Lasso) as input, Local ISA-LiNG achieves low SHD; experiments compare oracle MB vs estimated MB showing method is robust though performance improves with oracle MB.",
            "performance_without_robustness": "If MB is inaccurately estimated (e.g., missing parents), ISA-based local identification can fail; the paper reports both oracle and estimated MB experiments (figures) showing degraded performance with estimated MB but still better than baselines.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Generalizing Loh & Bühlmann's inverse-covariance idea to cyclic graphs allows MB estimation via nodewise Lasso; accurate MB estimation is crucial because it blocks confounding paths and is a key enabler for the ISA and regression-based local identification methods.",
            "uuid": "e771.4",
            "source_info": {
                "paper_title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "HSIC grouping",
            "name_full": "HSIC-based subspace identification (ICA + HSIC)",
            "brief_description": "An implementation detail: estimate ICA components, then use HSIC independence tests to group components into ISA subspaces (identify which components belong to the same independent subspace).",
            "citation_title": "A kernel statistical test of independence",
            "mention_or_use": "use",
            "method_name": "ICA followed by HSIC to detect ISA partitions",
            "method_description": "Run ICA to get one-dimensional components, then apply pairwise or group-level HSIC (Hilbert-Schmidt Independence Criterion) independence tests to decide which ICA components are dependent and should be grouped into the same ISA subspace; yields partition Γ_W for ISA post-processing.",
            "environment_name": "Applied on demixed components from ISA step on LiNG SEM local subsets",
            "environment_description": "Used as a computational procedure on observational datasets; no active experiment selection.",
            "handles_distractors": true,
            "distractor_handling_technique": "Groups ICA components that are dependent due to shared latent distractors into multi-dimensional subspaces, preventing incorrect assumption of mutual independence among components.",
            "spurious_signal_types": "Shared latent sources causing dependence between ICA components (hidden confounders).",
            "detection_method": "HSIC independence testing to find dependent ICA components; components grouped if dependence is detected (significance level used in implementation: 0.05).",
            "downweighting_method": null,
            "refutation_method": "Independence test refutes the hypothesis that two components are independent and therefore supports grouping them into same subspace.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Practical implementation found to work well: HSIC grouping produced reliable ISA partitions that enabled downstream admissible-permutation processing and accurate local edge estimation (reported in experiments).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Using HSIC on ICA outputs is an effective and practical method to obtain ISA partitions in the paper's pipeline; it operationalizes ISA estimation for causal identification with latent confounders.",
            "uuid": "e771.5",
            "source_info": {
                "paper_title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Stability filtering",
            "name_full": "Local stability-based filtering of candidate LiNG solutions",
            "brief_description": "A technique to select locally-identifiable solutions that correspond to globally stable LiNG models by checking convergence (spectral radius) of candidate adjacency matrices and discarding non-convergent ones.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Stability filtering (spectral-radius-based model selection)",
            "method_description": "From the set B of global LiNG equivalence-class adjacency matrices consistent with an ISA-demixed solution, compute each candidate B' and test whether it is convergent (lim_{t→∞} B'^t = 0), typically by checking spectral radius &lt; 1; discard non-convergent candidates. Under disjoint cycles, this yields a unique global stable model locally.",
            "environment_name": "Simulated LiNG SEMs with disjoint cycles (synthetic experiments)",
            "environment_description": "Applied as an additional selection criterion on candidate local models produced by ISA post-processing; not interactive.",
            "handles_distractors": null,
            "distractor_handling_technique": "Not directly a distractor-removal method, but helps reject candidate models that are dynamically unstable (which may correspond to spurious parameter configurations arising from ambiguous permutations).",
            "spurious_signal_types": "Ambiguous equivalent LiNG solutions that are dynamically unstable (explosive cycles) which are unlikely to be the true data-generating stable system.",
            "detection_method": "Compute spectral radius (largest eigenvalue magnitude) of candidate adjacency B'; if ≥1 mark as non-convergent and skip.",
            "downweighting_method": null,
            "refutation_method": "Refute candidate B' as plausible global model if it is not convergent (unstable); keep only convergent ones.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When cycles are disjoint and true global model is stable, adding the stability check identifies the unique global stable solution locally (Corollary 1).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Local stability check (spectral radius test) can uniquely pick out the global stable solution from the local equivalence class when cycles are disjoint; however, when cycles intersect, local stability tests cannot reliably identify global stable solutions.",
            "uuid": "e771.6",
            "source_info": {
                "paper_title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Thresholding / Lasso regularization",
            "name_full": "Coefficient thresholding and nodewise Lasso regularization",
            "brief_description": "Practical numerical regularization techniques used in implementation: nodewise Lasso for MB estimation and absolute thresholding of small demixing/weight entries to zero.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Nodewise Lasso + thresholding",
            "method_description": "Nodewise Lasso regression (Meinshausen & Bühlmann style) is used to estimate support of precision matrix / MB; small-magnitude entries in demixing/adjacency estimates are thresholded (paper uses threshold 0.05) to reduce false positives and numerical noise.",
            "environment_name": "Finite-sample implementations on synthetic LiNG SEMs and the Sachs dataset",
            "environment_description": "Practical preprocessing/regularization steps applied to batch observational datasets; no active experiment selection.",
            "handles_distractors": true,
            "distractor_handling_technique": "Regularization (Lasso) shrinks coefficients of irrelevant/distractor variables towards zero; thresholding removes small spurious demixing/adjacency values that could be due to noise or estimation error.",
            "spurious_signal_types": "Measurement noise, finite-sample estimation noise, irrelevant variables producing small spurious associations.",
            "detection_method": "Nonzero Lasso coefficients indicate candidate neighbors; small absolute values below threshold considered spurious and removed.",
            "downweighting_method": "L1 regularization (Lasso) and hard thresholding reduce influence of weak/spurious signals.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Regularization and thresholding improve finite-sample robustness: experimental SHD and weight-estimation errors decrease with these practical choices (as reported in implementation notes and figures).",
            "performance_without_robustness": "Without regularization/thresholding, estimation is more noisy and produces higher SHD and worse parameter estimates in finite samples (implied by comparisons using estimated vs oracle MB and across sample sizes).",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Lasso-based MB estimation and thresholding of small entries are effective practical measures to reduce false positives arising from finite-sample noise and help pipeline stability and accuracy in experiments.",
            "uuid": "e771.7",
            "source_info": {
                "paper_title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Discovering cyclic causal models by independent components analysis",
            "rating": 2,
            "sanitized_title": "discovering_cyclic_causal_models_by_independent_components_analysis"
        },
        {
            "paper_title": "Estimation of causal effects using linear non-Gaussian causal models with hidden variables",
            "rating": 2,
            "sanitized_title": "estimation_of_causal_effects_using_linear_nongaussian_causal_models_with_hidden_variables"
        },
        {
            "paper_title": "High-dimensional learning of linear causal networks via inverse covariance estimation",
            "rating": 2,
            "sanitized_title": "highdimensional_learning_of_linear_causal_networks_via_inverse_covariance_estimation"
        },
        {
            "paper_title": "DirectLiNGAM: A method for learning a linear non-Gaussian acyclic model",
            "rating": 2,
            "sanitized_title": "directlingam_a_method_for_learning_a_linear_nongaussian_acyclic_model"
        },
        {
            "paper_title": "Independent subspace analysis and related works (Theis 2006 / Hyvärinen & Hoyer 2000)",
            "rating": 2,
            "sanitized_title": "independent_subspace_analysis_and_related_works_theis_2006_hyvärinen_hoyer_2000"
        },
        {
            "paper_title": "Local causal discovery of direct causes and effects",
            "rating": 1,
            "sanitized_title": "local_causal_discovery_of_direct_causes_and_effects"
        },
        {
            "paper_title": "Local causal discovery for estimating causal effects",
            "rating": 1,
            "sanitized_title": "local_causal_discovery_for_estimating_causal_effects"
        }
    ],
    "cost": 0.021090249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Local Causal Discovery with Linear non-Gaussian Cyclic Models</p>
<p>Haoyue Dai 
Carnegie Mellon University</p>
<p>Ignavier Ng 
Carnegie Mellon University</p>
<p>Yujia Zheng 
Carnegie Mellon University</p>
<p>Zhengqing Gao 
Zayed University of Artificial Intelligence</p>
<p>Kun Zhang 
Carnegie Mellon University</p>
<p>Zayed University of Artificial Intelligence</p>
<p>Local Causal Discovery with Linear non-Gaussian Cyclic Models
16D3517231384C33B748DE44C3A699C9
Local causal discovery is of great practical significance, as there are often situations where the discovery of the global causal structure is unnecessary, and the interest lies solely on a single target variable.Most existing local methods utilize conditional independence relations, providing only a partially directed graph, and assume acyclicity for the ground-truth structure, even though realworld scenarios often involve cycles like feedback mechanisms.In this work, we present a general, unified local causal discovery method with linear non-Gaussian models, whether they are cyclic or acyclic.We extend the application of independent component analysis from the global context to independent subspace analysis, enabling the exact identification of the equivalent local directed structures and causal strengths from the Markov blanket of the target variable.We also propose an alternative regression-based method in the particular acyclic scenarios.Our identifiability results are empirically validated using both synthetic and real-world datasets.</p>
<p>INTRODUCTION</p>
<p>Causal discovery aims to identify causal relations among variables from data.In many real-world scenarios, it is not essential to determine the causal structure across all variables.Rather, the primary interest is often in unveiling the causes and effects related to specific target variables.Allocating resources to estimate a global structure for such narrowed objectives can be computationally excessive.This is exemplified in scRNA-seq data, where attempting global causal discovery to derive the gene regulatory network amongst approximately 20k genes is not only computationally expensive but also often redundant (Levine and Davidson, 2005).Local causal discovery, emphasizing the causal relations of a target variable and its neighbors, stands out as a more grounded and efficient approach.Additionally, techniques like divide-and-conquer and parallelization, when applied through local causal discovery, can often enhance the efficiency of identifying the global causal structure (Ma et al., 2023).</p>
<p>Building on this motivation, several studies have delved into the discovery of local structures within a select subset of variables (Margaritis and Thrun, 1999;Yin et al., 2008;Zhou et al., 2010;Niinimaki and Parviainen, 2012;Wang et al., 2014;Gao and Ji, 2015;Gao et al., 2017;Ling et al., 2020;Ng et al., 2021;Yu et al., 2021;Gupta et al., 2023).The distinction in this line of research lies in the estimation approaches used to estimate these local structures, such as parent-child sets.These approaches range from testing conditional independence relations to employing likelihood-based score functions.With appropriate tests or scores, they can offer nonparametric guarantees.Yet, without parametric assumptions, both independence tests and score functions cannot uniquely determine all directions, leading to some edges being undirected.</p>
<p>Moreover, most existing work in local causal discovery assume that there are no cycles in the ground-truth structure.This constrains its applicability given that cycles frequently appear in real-world contexts.These cycles can arise from various origins, including feedback mechanisms in biological systems (Benito et al., 2007), electrical engineering (Mason, 1953), or economic processes (Haavelmo, 1943).Such cyclic relationships can have profound implications, reshaping our understanding of the systems under consideration.Furthermore, in local context, one often cannot make the assumption of global acyclicity, since there is no way for the acyclicity beyond the considered subset of variables to be testable.While there has been a steady progress on causal discovery with cycles (Spirtes, 1995;Richardson, 1996;Lacerda et al., 2008;Hyttinen et al., 2012;Mooij and Heskes, 2013;Ghassami et al., 2020), none have offered methodologies with theoretical guarantees in the context of local search.</p>
<p>arXiv:2403.14843v1 [cs.LG] 21 Mar 2024</p>
<p>Contributions.To our knowledge, this work is the first to tackle local causal discovery in cyclic models, crucial for gene regulatory networks with prevalent feedback loops and numerous genes.Moreover, we allow intersecting cycles, a known challenging case.By leveraging non-Gaussianity, our approach determines causal directions and strengths, standing in contrast to most previous local methods that only identify partially directed edges.Notably, this work offers a unified perspective on acyclic (Shimizu et al., 2006(Shimizu et al., , 2011) ) and cyclic (Lacerda et al., 2008) cases within the local context.We establish identifiability guarantees for all proposed methods, and our theoretical results have been validated in both synthetic and real-world data.</p>
<p>2 Problem Setup 2.1 Notations, Definitions, and the Goal Let G = (V, E) be a directed graph with the vertex set V = [d] := {1, 2, . . ., d} and the edge set E. Denote a directed edge from vertex i to vertex j as i → j.</p>
<p>Random variables X = (X i ) d i=1 are generated by a linear non-Gaussian (LiNG) structural equation model (SEM) (Lacerda et al., 2008) w.r.t. the graph G, described in the matrix form as
X = BX + E,(1)
where E = (E i ) d i=1 are mutually independent non-Gaussian exogenous noise components, and B is the adjacency matrix, with the entry B j,i representing the direct causal effect of X i on X j .B j,i ̸ = 0 if and only if i → j ∈ E. Solving for X in Equation (1) gives
X = AE, with A := (I − B) −1 ,(2)
i.e., X can also be expressed directly as a linear combination of the noises, through the mixing matrix A.</p>
<p>Following (Lacerda et al., 2008), we allow cycles in G under some mild assumptions (see Section 3), and interpret X as the equilibrium of the dynamic system.</p>
<p>For a vertex i ∈ V, denote its Markov blanket (MB) in G as mb G (i) := pa G (i) ∪ ch G (i) ∪ sps G (i), the union of its parents pa G (i) := {j ∈ V : j → i ∈ E}, children ch G (i) := {j ∈ V : i → j ∈ E}, and spouses sps G (i) := {j ∈ V(pa G (i)∪ch G (i)) : ch G (i)∩ch G (j) ̸ = ∅}.Assuming faithfulness, mb G (i) corresponds to the minimal set of variables conditioned on which all other variables are independent of X i .Consequently, it is an appropriate starting point for the local search on vertex i.</p>
<p>For a target vertex T , we provide a method in Appendix A to efficiently estimate mb G (T ) from X even in the presence of cycles.Specifically, we generalize the method developed by Loh and Bühlmann (2014) in the acyclic case based on inverse covariance matrix of the distributions.Hence in the following main results (Sections 3 and 4), we assume the oracle mb G (T ) is available, and focus on the problem of further discovering causal effects related to T from T, mb G (T ), and their corresponding variables.</p>
<p>LiNG SEM and its Global Estimation</p>
<p>Our definition of a linear non-Gaussian (LiNG) cyclic model precisely follows (Lacerda et al., 2008).We allow cycles in G, interpret X as the equilibrium of the dynamic system.We allow overlapped cycles, but only assume that there are no "self-loops", i.e., B has all zeros in the diagonal, because by trivial scaling, any equilibrium even with self-loops can be equivalently entailed by another LiNG model without self-loops, as long as the self-loop strengths B i,i ̸ = 1.Moreover, we assume no cycles with strength exactly 1, i.e., B has no eigenvalues of 1, rendering I − B invertible.See Section 1.2 of (Lacerda et al., 2008) for details.</p>
<p>Recall that Equation ( 2), X = AE, is in the exact form of independent component analysis (ICA) (Comon, 1994;Hyvärinen and Oja, 2000), where observed data X (signals) is an unknown linear invertible mixture of unknown non-Gaussian independent components E (blind sources).When all the variables in X are involved, namely, with causal sufficiency, ICA can estimate a demixing matrix W to separate X into independent components WX.It is shown that W identifies A −1 = I − B up to rows permutation and scaling.Interestingly, with the structural constraint of zero diagonals in B (i.e., diagonal ones in A −1 ), these indeterminacies can be further reduced.A row permutation is called admissible if it makes W have diagonal ones with corresponding scaling.When G is acyclic, ICA-LiNGAM (Shimizu et al., 2006) shows that the admissible permutation is unique, resulting in the exact identification of B. This is because for acyclic G, its B can be simultaneously row and column permuted to be strictly lower triangular.Lacerda et al. (2008) generalizes ICA-LiNGAM to cyclic cases with almost a same algorithmic procedure: it begins with an ICA on X to obtain a demixing matrix W, and then identifies the admissible row permutations.The key distinction is that, in the presence of cycles in G, there can be multiple admissible permutations.Denote the set of adjacency matrices recovered by all admissible permutations as B, i.e., Definition 1.For a LiNG model X = BX+E, denote
B := {B ′ : B ′ = I − P π DA −1 , diag(B ′ ) = 0},
where A −1 = I − B, D is an d-dim scaling matrix, and P π is a permutation matrix (see Section 3 for details) with π enumerating permutations of
V = [d].
Two different LiNG models from B entail a same equilibrium distribution and are termed distributionally equivalent, though they share different graph structures; see Figure 8 in Appendix B for an example.Note that with linearity and non-Gaussianity, no two different acyclic SEMs are distributionally equivalent, guaranteeing the unique identification of B in LiNGAM, but there are different cyclic SEMs that are distributionally equivalent, and thus the true B can be identified up to an equivalence class.Lacerda et al. (2008) shows that B, defined above from all admissible permutations, characterizes exactly the LiNG equivalence class for X.</p>
<p>LOCAL LING DISCOVERY</p>
<p>We develop a local causal discovery method based on independent subspace analysis (ISA), which enables the exact identification of the equivalent local directed structures and causal strengths from the MB of the target variable.We first explain how the commonly used ICA approach for discovering global causal structure (Shimizu et al., 2006) fails in local context.</p>
<p>We then describe the key identifiability result of ISA that we exploit, and provide a specific characterization of the ISA solution.Finally, we describe our proposed Local ISA-LiNG method, which involves (1) performing ISA on the local variables, (2) finding admissible permutations on the ISA solutions, and (3) identifying local structures and coefficients from the permuted solutions.We prove that, interestingly, with only local variables, our proposed algorithm can identify exactly what can be identified globally with all variables.</p>
<p>Independent Subspace Analysis</p>
<p>Having introduced the cyclic LiNG and its ICA-based global estimation method, we now turn to our local case.When only a subset of variables (e.g., a target T and its mb G (T )) is involved, the main challenge lies in causal insufficiency: with hidden confounders, ICA cannot demix mutually independent components.</p>
<p>Example 1.In Figure 1(i), consider a target T = 4 with mb G (T ) = {2, 3}.With a confounder X 1 outside of T 's MB, ICA is not applicable on {X 2 , X 3 , X 4 }, as these three signals mix four sources ({E 1 , E 2 , E 3 , E 4 }), and no invertible matrix W ∈ Gl(3) can separate out any three mutually independent components.△ Such an issue is typically pronounced in overcomplete ICA (OICA) (Hyvärinen and Oja, 2000), where the number of observed signals is less than the number of mixed sources.There are indeed work on LiNGAM with hidden confounders using OICA (Hoyer et al., 2008), but OICA is known to be both computationally and statistically ineffective.In this work, our methods do not involve OICA, away from the difficulties of trying to separate out that many mutually independent sources from only a few signals.Instead, we only seek the separation "as independent as possible", and show that it is informative enough.To achieve this, independence subspace analysis (ISA) (Hyvärinen and Hoyer, 2000;Theis, 2006) comes into play.</p>
<p>Definition 2. An m-dim random vector Z is called irreducible if it contains no lower-dim independent components, i.e., no invertible matrix
W ∈ Gl(m) can decompose WZ = (Z ′ 1 , Z ′ 2 ) into independent Z ′ 1 ⊥ ⊥ Z ′ 2 . Definition 3. For an m-dim random vector Y, an invertible matrix W is called an independent subspace analysis (ISA) solution of Y if WY = (Z ⊺ 1 , . . . , Z ⊺ k ) ⊺ consists of mutually independent, irreducible random vectors Z i . The corresponding partition Γ W of indices [m] is called the ISA partition associated with W.
Given a random vector Y with existing covariance and no Gaussian components, Theis (2006) shows that an ISA solution of Y exists and, similar to ICA, is unique except for general scaling and permutation.</p>
<p>Before stating the result of ISA, we first introduce some notations.For a permutation π : {1, . . ., m} → {1, . . ., m} of m elements, let π i be the i-th element in π, and π[j] be the index of element j in π, i.e., π π[j] = j.For an ordered subset S ⊂ [m], denote π S := (π i : i ∈ S) and π[S] := (π[j] : j ∈ S), where (•) means ordered sets.Define the m×m permutation matrix P π by P π i,j = 1 if π i = j and 0 otherwise.Given a partition Γ of [m], an m × m block diagonal matrix D is said to be a general scaling matrix consistent with Γ, if ∀S ∈ Γ, rank(D S,S ) = |S|, and D S,[m]\S = 0.Here the notation like D S1,S2 means the submatrix of D with rows and columns indexed by ordered sets S 1 and S 2 respectively.Subscripts on random vectors denotes indexing similarly, e.g., X S = (X i : i ∈ S).We have:</p>
<p>Theorem 1 (Indeterminacy of ISA; Theorem 1.8 of (Theis, 2006)).Given an m-dim random vector Y, if both W 1 and W 2 are ISA solutions of Y with partitions Γ W1 , Γ W2 , then there exists a permutation π of [m] and a general scaling matrix D consistent with Γ W1 s.t.W 2 = P π DW 1 , and ∀S 1 ∈ Γ W1 , ∃S 2 ∈ Γ W2 , with S 2 and π[S 1 ] having the same elements.</p>
<p>ISA can be identified up to such indetermincaies, and can be estimated as efficiently as square ICA (Theis, 2006).ICA can then be viewed as a special case of ISA, where all subspaces are of one-dimension.</p>
<p>One Specific ISA Characterization</p>
<p>Given a vertex subset S ⊂ V and the corresponding variables X S , Section 3.1 shows that although ICA on
(i) (ii) (iii)
Figure 1: For Examples 1 to 3. On each G, the target T is colored red, and its mb G (T ) is circled by blue and colored dark.Same marks apply henceforth.</p>
<p>X S may not be applicable, an ISA solution exists and is unique up to some indeterminacies.However, what is such an ISA solution?In the causally sufficient (i.e, ICA) case, a demixing matrix A −1 = I−B follows naturally from Equation (2), while this is less obvious in the ISA case.Below we give a specific characterization.</p>
<p>Theorem 2 (One characterization of ISA in LiNG model).Assume X follows a LiNG SEM X = AE.For any vertex subset S ⊂ V, the inverse of the principal submatrix of the mixing matrix A indexed by S, denoted by A −1 S,S , is an ISA solution of X S .</p>
<p>Theorem 2 characterizes a specific ISA matrix
A −1 S,S
that separates X S "as independent as possible", i.e., A −1 S,S X S produces irreducible independent subspaces.The proof is in Appendix D.1.However, before delving into further identification of A −1 S,S , let us first closely examine and understand what it represents.</p>
<p>Recall that in ICA, the adjacency matrix B that represents the causal structure and strengths can be directly read off of the demixing characterization, A −1 = I−B.However, in ISA, the local adjacencies may not be as so straightforward.A −1 S,S is the Schur complement of [d]\S block in I − B. Typically, A −1 S,S does not equal I − B S,S , and I − B S,S is not an ISA solution either:</p>
<p>Example 2. In Figure 1(ii), consider S = (1, 3, 4), i.e., a target T = 4 and its mb G (T ) = {1, 3}.The ISA characterization A −1 S,S separates X S into three independent irreducible subspaces (1-dim components):
A −1 S,S X S =   1 0 0 −ac 1 0 −b −d 1     X 1 X 3 X 4   =   E 1 cE 2 + E 3 E 4   ,
but the local adjacencies I − B S,S ̸ = A −1 S,S , and by
(I − B S,S )X S =   1 0 0 0 1 0 −b −d 1     X 1 X 3 X 4   =   X 1 X 3 E 4   ,
it is not an ISA solution, as it produces only two independent subspaces, of which the first one (X ⊺ 1 , X ⊺ 3 ) ⊺ is not irreducible with a decomposition 1 0 −ac 1 .△</p>
<p>Write the matrix inverse in block form we will have:
I − A −1 S,S = B S,S + B S, S(I − BS , S) −1 BS ,S ,(3)
where S := V\S.By Equation (3), the (i, j)-th entry of I − A −1 S,S corresponds not only to the direct causal effect from j to i, but also the total causal effect from j to i through all other variables outside of S.</p>
<p>With this in mind, we note an issue on diagonals: while the global demixing matrix A −1 = I − B always has diagonal ones as we assume no self-loops, it may not be the case locally.Specifically, if G is acyclic, A −1 S,S still has diagonal ones, but this does not hold for cyclic G:</p>
<p>Example 3. Consider the LiNG SEM in Figure 1(iii).
B =   0 b 0 a 0 0 0 c 0   ; A = 1 1 − ab   1 b 0 a 1 0 ac c 1 − ab   .
Let S = (2, 3), i.e., T = 3 and its mb G (T ) = {2},
A −1 S,S = 1 − ab 0 −c 1 ,
where the diagonal entry on X 2 is not one.This is because X 2 is on a cycle outside of S, which, from the local view of S, is equivalent to a self-loop on X 2 .The strength of this "self-loop" is thus unidentifiable.△</p>
<p>Local LiNG Identification from ISA</p>
<p>Having defined a specific ISA characterization A −1 S,S , we are now left with the task to post-process any general ISA solution to this specific one (and its equivalence class, if any).Recall that in the global ICA case, the adjacency matrix equivalence class B directly stems from row permutations on any demixing matrix W.However, with ISA, we face more complex crossrows indeterminacies (Theorem 1).How to reduce them?Moreover, even if A −1 S,S is exactly recovered, a challenge lies still in translating it back into LiNG model parameters, as it may not directly represent adjacencies and may be unidentifiable due to external paths (Examples 2 and 3).Then, what is identifiable locally, and how?We address these questions below.</p>
<p>Consider a target vertex T and its mb G (T ).Let S be {T } ∪ mb G (T ) with m elements.W.l.o.g., we rename vertices s.t.S reads 1 to m, i.e., S = [m] ⊂ [d] = V.Assume a LiNG SEM X = BX + E = AE.Perform ISA on X S , we obtain a solution W and the associated subspace partition Γ W .By Theorems 1 and 2, W can be row-permuted and subspace-scaled into A −1 S,S .</p>
<p>Admissible Permutations</p>
<p>So the first step is to "re-permute" W. Since columns of W correspond exactly to X 1 through X m , rows permutation of W can be seen as assigning names to each row, thereby forming their one-to-one correspondence to X 1 through X m also.Intuitively, rows within a same multi-dim subspace always correspond to variables with common hidden confounders and are thus mutually unidentifiable.However, different subspaces collectively, especially singleton subspaces (components), should be re-identified to their correct locations.Nonetheless, we first note that the nonzerodiagonal permutation as in ICA, is incorrect here:</p>
<p>Example 4. Consider an acyclic G as in Figure 2.</p>
<p>Let S be (1, 2, 3, 4, 5), i.e., a target T = 3 and its mb G (T ).The true but unknown A −1 S,S is provided for reference.We are only given an ISA output W, as in (i), and its Γ W = {(1, 2), (3), (4), (5)}.Actually, W is just scaled from A −1 S,S without permutation, though this is unknown.Comparing W to A −1 S,S , we notice that within the subspace of the 1st and 2nd rows, the nonzero entries are mixed by the general scaling.If we were to still follow the "admissible" criteria of nonzero diagonals as in ICA, we see that W is already satisfied (and is indeed correct).However, another permutation W ′ as in (ii), is also satisfied but is entirely incorrect (even on singletons' locations), leading to incorrect edges like 2 → 4, 3 → 1.</p>
<p>Why does incorrect permutation (ii) occur?Note that A −1 S,S possesses a unique row permutation (itself) with nonzero diagonals, so the blame falls on the scaling to 1st and 2nd rows with more nonzeros.Fortunately, these spurious nonzeros reveal themselves via rank deficiency.In (ii), even though nonzero diagonals exist, the diagonal block of the first subspace, W ′</p>
<p>(2,4),(2,4) , is proportional to [1, −c] and has rank 1. Inspired by this, we can eliminate spurious nonzeros by forcing not the nonzero diagonal entries as in ICA, but the invertible diagonal blocks, formally described below.△ Definition 4. Given an ISA solution W and the associated partition
Γ W , a permutation π is called ad- missible, if ∀S i ∈ Γ W , rank((P π W) π[Si],π[Si] ) = |S i |.
Admissible permutations defined in Definition 4 are "sound and complete".See Appendix D.4 for formal definition and proof.Roughly speaking, all such admissible rows permutations correspond exactly to all rows permutations on A −1 S,S with nonzero diagonals (viewing each subspace collectively), which then correspond exactly to the LiNG equivalence class on S.</p>
<p>Identifiable Local Causal Effects</p>
<p>Having admissible permutations, now we proceed to identify local causal structures and coefficients.As demonstrated in Examples 2 and 3, ISA matrices is not overall reliable.However, note that the misidentification of an X i on both examples can be attributed to an incoming path (either in a cycle or not) outside of S. This immediately sparks us that if all of i's parents are included in S, such issues should not arise:
Lemma 1. Given an ISA solution W and Γ W on X S , ∀i ∈ S, if pa G (i) ⊂ S, then its exogenous noise compo- nent E i is separated out, i.e., ∃j ∈ [m] s.t. (j) ∈ Γ W
and (WX S ) j = cE i with a scaling factor c.Moreover, the incoming causal strengths to X i are identified up to c, i.e., the row vector
W j = c(I − B S,S ) i .
Lemma 1 becomes especially helpful in our case: by definition of MB, for any of T and its children, all its parents are included in {T } ∪ mb G (T ), thus blocking all confounding paths, enabling recovery of exogenous noise, and moreover, the exact causal strengths.Once all edges into T and T 's children are identified, we've attained the goal of local causal discovery, as these edges include all edges to and from T .As for other variables in MB, e.g., parents and spouses, they may be entangled within subspaces and remain unidentifiable, but this does not pose a concern anymore.</p>
<p>By Lemma 1, T and its children produce independent components (1-dim subspaces) by ISA.But conversely, an unconfounded parent or spouse can also produce a 1-dim subspace.Then, which of these components correspond exactly to our main focus, T and its children?</p>
<p>The answer can be read off of T 's column in W:
Lemma 2. Given an ISA solution W and Γ W on X S for S = {T } ∪ mb G (T ). Denote by C := supp(W :,T ) = (i ∈ [m] : W i,T ̸ = 0). Then ∀i ∈ C, W i must produce a single component, i.e., (i) ∈ Γ W . Moreover, {π[C] : π admissible to W} = {supp(B ′ :,T ) : B ′ ∈ B}.
In essence, Lemma 2 interprets the nonzero row indices on T 's column vector in W as T and T 's children.Note that there can be multiple directed graphs in the LiNG equivalence class, leaving different choices of S subsets as T 's children.Any such choice can be interpreted by an admissible row permutation, and vice versa.Set scaling matrix D:
∀S i ∈ Γ W , D π[Si],π[Si] := (W ′ π[Si],π[Si] ) −1 and D π[Si],[m]\π[Si] := 0; 8: Set B ′ := I − DW ′ ; 9:
for i ∈ C do 10:
Assert (i) ∈ Γ W ; 11:
Add to K a weighted edge denoted as (j →
π[i], B ′ π[i],j ), for each j ∈ [m] with B ′ π[i],j ̸ = 0; 12:
end for 13:</p>
<p>Set K := K ∪ {K}; 14: end for 15: Return K;</p>
<p>The Local ISA-LiNG Algorithm</p>
<p>Finally, we have the local ISA-LiNG Algorithm 1. Below we give an illustrative example on how it works:</p>
<p>Example 5. Consider the example in Figure 3.There are two graphs in the global equivalence class B, as shown in the upper row.Let S be (1, 2, 3, 4, 5), i.e., a target T = 3 and its mb G (T ).An ISA on X S gives W with nonzero patterns as in the lower left matrix, where specifically, the striped entries are nonzero but rank deficient (see Definition 4).The 3rd (T -th) column has three nonzero entries, corresponding to T and its two children, which are yet unknown and can be different in different equivalent graphs.Two admissible rows permutations (the lower row) reveal their variable correspondences, with all edges into T and its children (dark edges in the graphs) recovered correctly for all equivalent graphs with different global directed cycles, while note these local edges themselves are acyclic.△ Theorem 3 (Correctness of local ISA-LiNG).For any T ∈ V, let K be set of weighted edge sets returned by Algorithm 1 on T , mb G (T ), and X.We have:
K = {{(i → j, B ′ j,i ) : ∀j ∈ {T } ∪ ch G ′ (T ), ∀i ∈ pa G ′ (j)} : ∀B ′ ∈ B, and the graph G ′ defined by B ′ }.
The local ISA-LiNG algorithm (Algorithm 1) correctly identifies all the causal effects into the target T and all its children, for all LiNG models that equivalently entails the distribution of X.That is, with only local variables, we identify exactly what can be identified globally.Note that this identification is unique (i.e.,</p>
<p>With the Notion of Stability</p>
<p>The B defined in Definition 1 characterizes the entire global LiNG equivalence class, yet not all models within it are "stable".In dynamical systems, stability refers to "the dissipation of the effects of one-time noise in models" (Lacerda et al., 2008).Applied to causal models, a model is "stable" when any infinitely long path (after traversing loops) result in zero causal effect.For example, in a simple cycle with two variables and two edges both carrying weights 2, the model is unstable, with the cycle product of 4 &gt; 1 leading to explosion.When both weights are 0.5, the model remains LiNG equivalent to the former one but achieves stability, with the cycle product of 0.25 &lt; 1. Formally, a global LiNG model is said to be stable when its adjacency matrix B is convergent, i.e., lim t→∞ B t = 0. Note that here the entry (B t ) i,j represents the summed causal effect from j to i along all paths of length t.</p>
<p>In practical global causal discovery scenarios, an oftenmade assumption is the stability of the underlying LiNG model, and people usually focus on identifying the stable LiNG model(s), instead of the entire equivalence class B. This is straightforward in ICA-LiNG (Lacerda et al., 2008): as the entire B can be recovered first, we then only need to check the convergence of each item within B. However, when we only have local variables, can we still recover the local part corresponding to the global stable model(s)?</p>
<p>The answer is affirmative but with constraints: our method can still correctly find stable solutions locally, as long as this local stable solution is identifiable.Denote the stable sub-equivalence class as B * := {B ∈ B : lim t→∞ B t = 0}.When cycles in the ground-truth G are disjoint, there exists a unique global stable model, i.e., |B * | = 1.Let B * be this unique stable adjacency matrix, and G * be the corresponding graph.In this case, simply by adhering to an additional local stability condition, the stable solution can be identified locally: Corollary 1 (Identifying stable solutions locally, with disjoint cycles).Suppose the cycles are disjoint in G. Consider a modified version of Algorithm 1 in which the line "if B ′ is not convergent: skip" is added between lines 8 and 9.Then, this modified version of Algorithm 1 will yield a single local model, corresponding exactly to the unique global stable model.That is, the returned K consists of a single item K, with
K = {(i → j, B * j,i ) : ∀j ∈ {T }∪ch G * (T ), ∀i ∈ pa G * (j)}.
Corollary 1 is valid as here stability is determined sufficiently and necessarily by the cycle products, which is preserved locally.However, when some cycles in G intersect, the situation becomes more complex.</p>
<p>Globally, there may be none or multiple global stable models in B * .Locally, while in this case, our Algorithm 1 can still identify local correspondences of all equivalent solutions (Theorem 3), the exact identification of the global stable solutions from local variables alone becomes inherently impossible.Intuitively, this is because that external cycles appear as self-loops on the local variables.More details are in Appendix D.</p>
<p>REGRESSION-BASED VARIANT</p>
<p>In Section 3 we propose a local ISA-based method suitable for both acyclic and cyclic graphs.In this section, we focus on the specific scenario where there are no cycles in G, i.e., X follows a linear non-Gaussian acyclic model (LiNGAM (Shimizu et al., 2006)), and propose an alternative local regression-based method.The relationship between this section and Section 3 can be likened to that of Direct-LiNGAM (Shimizu et al., 2011) and ICA-LiNG (Lacerda et al., 2008) in the global context, with the former utilizing non-Gaussianity by ICA, and the latter by Darmois-Skitovitch theorem (Darmois, 1953;Skitovitch, 1953).</p>
<p>Acyclicity renders the existence of a causal ordering, i.e., vertices V can be ordered so that no later vertex has a direct edge onto any earlier variable.When all the variables in X are involved, namely, with causal sufficiency, Shimizu et al. (2011) gives the method named Direct-LiNGAM to uniquely identify the DAG G by estimating its causal ordering: Regress X j on X i , if the residual is statistically independent with the regressor X i , then i is causally earlier than j.If such an independence holds for X i on all its pairwise regressions with the remaining X j s, i must be a root vertex.Subroots are then recursively identified in a same way, forming the causal ordering.</p>
<p>However, when only a subset of variables (as of here, {T } ∪ mb G (T )) is involved, Direct-LiNGAM does not work, as causal sufficiency is violated, and there can be no independent residual due to hidden confounders, just like the absent independent components in ICA.Example 6.In Figure 1(i), with a confounder X 1 outside of T 's MB, neither regressing X 2 on X 3 nor the converse results in independent residuals, making it impossible to identify any "local root" in mb G (T ).△ While identifying "local roots" is impossible, can we reverse our perspective from top-down to bottom-up and identify "local leaves" instead?Interestingly, the answer seems affirmative: In Example 6, regressing X 4 on {X 2 , X 3 }, the residual is exactly the exogenous noise E 4 and is independent to {X 2 , X 3 }.Formally, for any vertex subset S ⊂ V, we denote the corresponding random vector as X S := [X i : i ∈ S] ⊺ .Perform ordinary least square error linear regression of a random variable X i on a random vector X S , the asymptotic coefficients of fit is
β S→i := cov(X S , X S ) −1 cov(X S , X i ),
where for j ∈ S, β j S→i is the coefficient on X j .Denote the regression residual as
R S→i := X i − β ⊺ S→i X S . Denote i's descendants in G as des G (i). We have: Lemma 3. For any i ∈ V, S ⊂ V{i}, if R S→i ⊥ ⊥ X S , i.e., independent residual, then S ∩ des G (i) = ∅.
Lemma 3 generalizes regressions in (Shimizu et al., 2011) from single variables to multi-dim vectors, but with a same idea: independent residuals imply causal ordering.While as in Example 6, independent residuals may be absent for "local roots" due to confounders (echoed as multi-dim subspaces in ISA), they must exist for "local leaves" (echoed as the 1-dim components in ISA).This is because, again, as in Lemma 1, that parents of T and its children are included in the MB, thus blocking all confounding paths, enabling recovery of exogenous noise and the exact causal strengths:
Lemma 4. ∀i, S in V, if pa G (i) ⊂ S ⊂ V\ des G (i), then ∀j ∈ S, β j S→i = B i,j
, and R S→i = E i (so ⊥ ⊥ X S ).Lemma 4 holds generally for linear acyclic SEMs, echoing the local Markov property: given all its parents, a variable is independent of other non-descendants, enabling accurate estimation of direct effects to it.Lemmas 3 and 4 then readily leads to Algorithm 2.</p>
<p>The basic idea of Algorithm 2 is to recursively identify "local leaves", i.e., all T 's children until T , via an "inverse causal ordering".Independent residuals must exist for these variables, as all their parents are included locally (lines 7-9).Lines 3-6 serve to avoid errors due to spouses, which could be hidden confounded: Example 7. In Figure 4(i), T = 1, mb G (T ) = {2, 3, 4, 5}.After X 5 is first identified and removed, the remaining two "last leaves" X 3 , X 4 are confounded by X 6 hidden outside of mb G (T ), and thus neither can produce independent residual.The iteration cannot proceed, unless these two spouses are removed.△</p>
<p>Algorithm 2 Inverse Direct-LiNGAM Input: A target vertex T ∈ V, its oracle MB mb G (T ), and their corresponding variables in X Output: A set of directed edges with weights 1: Initialize the remaining vertex set U := {T } ∪ mb G (T ), and the output edge set K := ∅; 2: while U ̸ = ∅ do 3:
if ∃k ∈ U{T } s.t. β T U{k}→k = 0 then 4:
Set U := U{k};</p>
<p>5: continue to line 2;</p>
<p>6:
end if 7: Assert ∃j ∈ U s.t. R U{j}→j ⊥ ⊥ X U{j} ; 8:
Set j as any one found in line 7;  Even without confounders, edges produced by independent residuals may still be incorrect due to spouses: Example 8.In Figure 4(ii), T = 1, mb G (T ) = {2, 4, 5}.After X 5 is first identified and removed, though the "last leaf" X 4 produces independent residual regressing on X 1 , X 2 , due to hidden X 3 , the coefficient on X 1 is nonzero, yielding an incorrect edge 1 → 4. Correction requires removing this spouse X 4 .△</p>
<p>With spouses corrected, Algorithm 2 accurately estimate all edges into T and its children, including edges adjacent to T (the purpose of local search).Formally, Theorem 4 (Correctness of Algorithm 2).For any T ∈ V, let K be the weighted edge set returned by Algorithm 2 on T , mb G (T ), and X.We have:
K = {(i → j, B j,i ) : ∀j ∈ {T } ∪ ch G (T ), ∀i ∈ pa G (j)}.
Theorem 4 is similar to Theorem 3, except that the DAG can be uniquely identified.See Appendix D.8 for the proof, and Appendix C for also an alternative postprocessing of ISA, with the same "ordering" idea here.</p>
<p>EXPERIMENTS</p>
<p>We assess the effectiveness of our method for cyclic and acyclic cases in Sections 5.1 and 5.2, respectively.We provide an analysis of how our method performs under different sample sizes in Section 5.3, and an experiment on real data in Section 5.4.The implementation details and running times are discussed in Appendices E and F.1, respectively.</p>
<p>Cyclic Case</p>
<p>We conduct experiments to illustrate the output of our method, by adopting the left cyclic graph in One observes that our method correct identifies the edges according to Theorem 3, and that the estimated edge weights are close to the true ones.</p>
<p>With stability.To conduct a systematic validation, we restrict the cycles in the true graphs to be disjoint and the true B matrices to be stable using an acceptreject approach; that is, the spectral radius of B has to be strictly smaller than one.In this case, Corollary 1 indicates that the stable solution can be uniquely identified locally.We simulate 50-node directed cyclic graphs (DCGs) with maximum degree of 4, and 2000 samples from the LiNG SEM in Equation (1).We use the same setup described above for the edge weights and noise distributions.To perform local causal discovery, we randomly select a target T that is part of a cycle in the 50-node DCGs.Due to the lack of local causal discovery baselines that handle cyclic graphs, we compare our method with those for acyclic cases, including GSBN (Margaritis and Thrun, 1999), Local A<em> (Ng et al., 2021), CMB (Gao and Ji, 2015), and LDECC (Gupta et al., 2023).Note that GSBN and Local A</em> require information of two-step MBs (i.e., mb G (T ) and MB of each variable in mb G (T )), which are not directly comparable to our method that requires only mb G (T ); thus, we consider modifications of these methods, described in Appendix E. We report the structural Hamming distance (SHD) of local DCG, which is explained in details in Appendix E.3.</p>
<p>We provide the results for the methods using estimated MB in Figure 5, and using oracle MB in Figure 11 in Appendix F.2.It is observed that our method achieves much lower SHD in both settings, thereby demonstrating its effectiveness for identifying the local structure.</p>
<p>Acyclic Case</p>
<p>We consider the acyclic setting where the ground truths are DAGs.In the acyclic case , we use a more efficient post-processing procedure for demixing matrix W, described in Appendix C. We simulate 50-node Erdös-Rényi (Erdös and Rényi, 1959) DAGs, and 2000 samples from the LiNG SEM in Equation ( 1) using the same setting (including edge weights and noise distributions) as that of Section 5.1.To perform local causal discovery, we consider target T from 50node DAGs with expected degrees of 3 and 5, leading to roughly 14 and 20 variables in the MB mb G (T ), respectively.We report the SHD of local DAG and partially DAG (PDAG), explained in Appendix E.3.</p>
<p>For degree of 3, the SHDs of local DAG for the methods using estimated MB are shown in Figure 12, while the complete results using estimated MB and oracle MB are given in Figures 12 and 13 in Appendix F.2, respectively, due to space limit.We provide the results for degree of 5 in Figure 14 in Appendix F.2. Similar to the cyclic case, our method achieves much lower SHD for both local DAG and PDAG as compared to the baselines.One also observes that GSBN and Local A* performs better than CMB and LDECC.</p>
<p>Analysis of Different Sample Sizes</p>
<p>We provide an analysis of the proposed method across sample sizes n ∈ {100, 300, 1000, 3000, 10000}, following the data generating procedure in Section 5.2.We report the SHD of local DAG and the Euclidean distance between the estimated edge weights and the true ones.The results using oracle MB is shown in Figure 7, while those using estimated MB are given in Figure 15 in Appendix F.2.As the sample size increases, both metrics decrease to small values close to zero, which help validate the asymptotic correctness of our method in terms of both structure and parameter estimation.This also demonstrates the possibility of reliable estimation even when the sample size is rather limited.</p>
<p>Moreover, we provide the scatter plots of the estimated and true edge weights in Figures 16 and 17 in Ap-pendix F.2.For larger sample sizes, the data points are increasingly grouped onto the main diagonal, showing that the estimated weights become more accurate.</p>
<p>Real Data</p>
<p>We  , 1994).In this section, we provide a method to estimate the MB of a variable from a linear cyclic SEM.Specifically, we build upon the method proposed by Loh and Bühlmann (2014) that, similar to methods based on conditional independence tests, makes the acyclicity assumption, and further generalize it to handle cycles.</p>
<p>We first define the moral graph of a directed cyclic graph the same way as that of a DAG.Specifically, the moral graph of directed graph G is an undirected graph that contains an edge between two nodes if (1) they are adjacent in G, or (2) they share the same children.Clearly, the MB of a variable is simply its neighbors in the moral graph of G. Here, we provide a method to estimate such moral graph, which informs us about mb G (T ).</p>
<p>Considering the linear SEM in Equation ( 1), the inverse covariance matrix of the distribution of variables X is given by Θ = (I − B)Ω −1 (I − B) ⊺ , where Ω := diag(σ 2 1 , . . ., σ 2 d ) := cov(E) is the covariance matrix of exogenous noise components E. Inspired by Loh and Bühlmann (2014, Assumption 1) in the acyclic case, we make the following assumption in the cyclic case.</p>
<p>Assumption 1.Let B and Ω be the weighted adjacency matrix and noise covariance matrix, respectively, of the linear SEM in Equation (1).For every j &lt; i, we have
−σ −2 j B i,j − σ −2 i B j,i + ℓ̸ =j,i σ −2 ℓ B j,ℓ B i,ℓ = 0,(4)
only if B i,j = B j,i = 0 and B j,ℓ B i,ℓ = 0 for all ℓ ̸ = j, i.</p>
<p>As we will show in the proof, the LHS of Equation ( 4) is equal to Θ j,i .It is worth noting that if the nonzero coefficients of B are randomly drawn from a distribution that is absolutely continuous with respect to Lebesgue measure, then the above assumption is only violated for a set of matrices B with zero Lebesgue measure.We then have the following proposition, with a proof given in Appendix D.9.Note that the proposition and its proof are built upon Loh and Bühlmann (2014, Theorem 2) in the acyclic case, which we generalize to the cyclic case.</p>
<p>Proposition 1. Suppose X follows the linear SEM in Equation (1) with directed cyclic graph G and inverse covariance matrix Θ.Under Assumption 1, the structure defined by the support of Θ is the same as the moral graph of G.</p>
<p>Asymptotically speaking, the true inverse covariance Θ can be estimated by computing the inverse of empirical covariance matrix.For finite samples, Ravikumar et al. ( 2011) established high dimensional guarantee for estimating the support of Θ using graphical Lasso (Friedman et al., 2008).An alternative approach (Meinshausen and Bühlmann, 2006) is to perform nodewise regression with Lasso (Tibshirani, 1996), which we adopt in this work.That is, we regress the target T on the other variables [d] \ {T } with Lasso, from which the nonzero coefficients determine the MB of T .&gt; &gt; &lt; &gt; &gt; :
X 1 = E 1 X 2 = aX 1 + dX 4 + E 2 X 3 = bX 2 + E 3 X 4 = cX 3 + E 4 8 &gt; &gt; &lt; &gt; &gt; : X 1 = E 1 X 2 = 1 b X 3 + 1 b E 3 X 3 = 1 c X 4 + 1 c E 4 X 4 = a d X 1 + 1 d X 2 + d A.2⇥ jk = 2 j B kj 2 k B jk + X <code>6 =j,k 2</code>Bj<code>Bk</code>, 8j 6 = ⇥ jj = 2 j + X <code>6 =j 2</code>B2 j`,8j
.</p>
<p>B MARKOV BLANKET DISCOVERY FOR CYCLIC GR</p>
<p>The local causal discovery procedures presented in Sections 3 and 4 rely on knowled variable T i.e., its parents, children, and spouses.To the best of our knowledge, methods, such as those based on nonparametric conditional independence test, Thrun, 1999a), IAMB (Tsamardinos et al., 2003), and MMMB (Tsamardinos et al network (i.e., acyclic) setting.That is, it may not be immediately clear how thei true one mb G (T ) in the presence of cycles, partly owing to the extra complications with conditional independence tests (Spirtes, 1994).In this section, we provide of a variable from a linear cyclic SEM.Specifically, we build upon the method pr (2014) that, similar to methods based on conditional independence tests, makes further generalize it to handle cycles.</p>
<p>Manuscript under review by AISTATS 2024</p>
<p>Local Cyclic Causal Discovery with Independent Su Supplementary Materials</p>
<p>A PROOFS OF MAIN RESULTS</p>
<p>A.1 Proof of Theorem 2</p>
<p>Theorem 2 (One characterization of ISA in LiNG).Assume X follows a LiNG X S ⇢ V, the inverse of the principal submatrix of B indexed by S, denoted by B 1 S,S ,
8 &gt; &gt; &lt; &gt; &gt; : X 1 = E 1 X 2 = aX 1 + dX 4 + E 2 X 3 = bX 2 + E 3 X 4 = cX 3 + E 4 8 &gt; &gt; &lt; &gt; &gt; : X 1 = E 1 X 2 = 1 b X 3 + 1 b E 3 X 3 = 1 c X 4 + 1 c E 4 X 4 = a d X 1 + 1 d X 2 + 1 d E A.2⇥ jk = 2 j B kj 2 k B jk + X <code>6 =j,k 2</code>Bj<code>Bk</code>, 8j 6 = k ⇥ jj = 2 j + X <code>6 =j 2</code>B2 j`,8j
.</p>
<p>B MARKOV BLANKET DISCOVERY FOR CYCLIC GRA</p>
<p>The local causal discovery procedures presented in Sections 3 and 4 rely on knowledg variable T i.e., its parents, children, and spouses.To the best of our knowledge, m methods, such as those based on nonparametric conditional independence test, e Thrun, 1999a), IAMB (Tsamardinos et al., 2003), and MMMB (Tsamardinos et al., network (i.e., acyclic) setting.That is, it may not be immediately clear how their true one mb G (T ) in the presence of cycles, partly owing to the extra complications in with conditional independence tests (Spirtes, 1994).In this section, we provide a of a variable from a linear cyclic SEM.Specifically, we build upon the method prop (2014) that, similar to methods based on conditional independence tests, makes th further generalize it to handle cycles.</p>
<p>Manuscript under review</p>
<p>Local Cyclic Causal Discovery with Supplementary A PROOFS OF MAIN RESULTS</p>
<p>A.1 Proof of Theorem 2</p>
<p>Theorem 2 (One characterization of ISA in LiNG).Assum S ⇢ V, the inverse of the principal submatrix of B indexed 8 &gt; &gt; &lt; &gt; &gt; :
X 1 = E 1 X 2 = aX 1 + dX 4 + E 2 X 3 = bX 2 + E 3 X 4 = cX 3 + E 4 8 &gt; &gt; &lt; &gt; &gt; : X X X X A.2⇥ jk = 2 j B kj 2 k B jk + X <code>6 = ⇥ jj = 2 j + X</code>6 =j 2 <code>B2 j</code>,</p>
<p>B MARKOV BLANKET DISCOVERY F</p>
<p>The local causal discovery procedures presented in Sections variable T i.e., its parents, children, and spouses.To the methods, such as those based on nonparametric conditio Thrun, 1999a), IAMB (Tsamardinos et al., 2003), and MM network (i.e., acyclic) setting.That is, it may not be imm true one mb G (T ) in the presence of cycles, partly owing to t with conditional independence tests (Spirtes, 1994).In th of a variable from a linear cyclic SEM.Specifically, we bui (2014) that, similar to methods based on conditional indep further generalize it to handle cycles.</p>
<p>Local Cyclic Causa A PROOFS OF MAI</p>
<p>A.1 Proof of Theorem 2</p>
<p>Theorem 2 (One characteriza S ⇢ V, the inverse of the princ  Local Cyclic Causal Discovery with Independent Subspace Analysis: Supplementary Materials
8 &gt; &gt; &lt; &gt; &gt; : X 1 X 2 X 3 X 4 A.2</p>
<p>A PROOFS OF MAIN RESULTS</p>
<p>A.1 Proof of Theorem 2</p>
<p>Theorem 2 (One characterization of ISA in LiNG).Assume X follows a LiNG X = BE.For any vertex subset S ⇢ V, the inverse of the principal submatrix of B indexed by S, denoted by B 1 S,S , is an ISA of X S .</p>
<p>Proof.</p>
<blockquote>
<blockquote>
<blockquote>
<p>&lt;</p>
<p>:
X 1 = E 1 X 2 = aX 1 + dX 4 + E 2 X 3 = bX 2 + E 3 X 4 = cX 3 + E 4 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; : X 1 = E 1 X 2 = 1 b X 3 + 1 b E 3 X 3 = 1 c X 4 + 1 c E 4 X 4 = a d X 1 + 1 d X 2 + 1 d E 2(5)
For convenience denote W := B 1 S,S .Write the ISA demixed subspaces as exogenous noise combinations:
WX S = 2 4 W 3 5 • 2 4 S B S,: 3 5 V := [d] z }| { 8 &lt; : • E (6) = 2 4 B 1 S,S 3 5 • 2 4 S B S,S B S, S 3 5 S z }| { S := V\S z }| { 8 &lt; : • E (7) = 2 4 S I • • • 3 5 S z }| { S := V\S z }| { 8 &lt; : • E(8)
where
E = (E | 1 , • • • , E | d )
| are the mutually independent exogenous non-Gaussian noise components.To show that W is an ISA of X S , we want to show that for any subspace
Z i 2 (Z | 1 , . . . , Z | k ) | = WX S with m := |Z i | 2 (
otherwise it's already a single component; | • | denotes dimension or cardinality), Z i is irreducible (Definition 1), i.e., there exists no invertible matrix H 2 Gl(m) s.t.HZ i produces two or more independent subspaces (random vectors).For convenience, we denote the row indices corresponding to the row-submatrix of W that produces the subspace Z i as M (M ⇢ S), i.e., Z i = W M,: X S .Similarly, rewrite it to noise combinations:</p>
</blockquote>
</blockquote>
</blockquote>
<p>C POST-PROCESSING FOR LOCAL ISA-LING</p>
<p>In this section, we provide an alternative post-processing procedure to obtain the estimated structures and edge weights from the ISA solution W, described in Algorithm 3.This procedure assumes that none of T and ch G (T ) is part of any cycles in G (including the case where G is acyclic).The overall idea is similar to that of the regression-based approach described in Algorithm 2. That is, Algorithm 3 iteratively finds the "sink" node from the ISA solution that is not an ancestor of the other nodes in the remaining vertex set.</p>
<p>Algorithm 3 Alternative post-processing procedure of ISA solution Input: A target vertex T ∈ V, its oracle MB mb G (T ), and ISA solution W Output: A set of directed edges with weights 1: Initialize the remaining vertex set U 1 , U 2 := {T } ∪ mb G (T ), and the output edge set K := ∅; 2: while U 1 ̸ = ∅ do 3:
Assert ∃j ∈ U 1 s.t. ∥W U2,j ∥ 0 = 1; 4:
Set j as any one found in line 7;
5: Let k ∈ U 2 be s.t. W k,j = 1; 6: if W k,T ̸ = 0 then 7: Add to K an edge i → j with weight W k,i for each i ∈ U 1 {j} with W k,i ̸ = 0; 8: end if 9:
break if j = T ; Otherwise set U 1 := U 1 {j} and U 2 := U 2 {k}; 10: end while 11: Return K;</p>
<p>D PROOFS OF MAIN RESULTS</p>
<p>D.1 Proof of Theorem 2</p>
<p>Theorem 2 (One characterization of ISA in LiNG model).Assume X follows a LiNG SEM X = AE.For any vertex subset S ⊂ V, the inverse of the principal submatrix of the mixing matrix A indexed by S, denoted by A −1 S,S , is an ISA solution of X S .</p>
<p>Proof.For convenience denote W := A −1 S,S .Write the ISA demixed subspaces as exogenous noise combinations:
WX S =   W   •   S A S,:   V := [d]    • E =   A −1 S,S   •   S A S,S A S, S   S S := V\S    • E =   S I • • •   S S := V\S    • E where E = (E ⊺ 1 , • • • , E ⊺ d )
⊺ are the mutually independent exogenous non-Gaussian noise components.To show that W is an ISA of X S , we want to show that for any subspace
Z i ∈ (Z ⊺ 1 , . . . , Z ⊺ k ) ⊺ = WX S with m := |Z i | ≥ 2 (otherwise it's already a single component; | • | denotes dimension or cardinality), Z i is irreducible (Definition 2), i.e.
, there exists no invertible matrix H ∈ Gl(m) s.t.HZ i produces two or more independent subspaces (random vectors).For convenience, we denote the row indices corresponding to the row-submatrix of W that produces the subspace Z i as M (M ⊂ S), i.e., Z i = W M,: X S .Similarly, rewrite it to noise combinations:
WX S = M W M,: S •   S A S,:   V := [d]    • E = M I • • • M M := V\M • E (5) =: C M • E,
where we denote the m × d rectangle mixing submatrix in Equation ( 5) as C M .We do not use letter A for distinguishment, as it is multiplied by W, and is different from submatrix from the original A mixing matrix.</p>
<p>Suppose for contradiction that Z i is irreducible, i.e., there exists an invertible matrix H ∈ Gl(m) s.t.HZ i produces at least two independent subspaces, then, there must exist a partition P 1 , P 2 of [d] s.t.,
HZ i = H • C M • E = H 1 H 2 • M C M,P1 C M,P2 P 1 P 2 = M\P 1 • E (6) = 0 • • • • • • 0 P 1 P 2 = M\P 1 ,(7)
i.e., H 1 Z i and H 2 Z i are linear combinations of disjoint sets of exogenous noise components, and thus by the Darmois-Skitovitch theorem (Darmois, 1953;Skitovitch, 1953), they are mutually independent.</p>
<p>By Equations ( 6) and ( 7) we have that row vectors of H 1 lie in nullspace(C ⊺ M,P1 ), and row vectors of H 2 lie in nullspace(C ⊺ M,P2 ).Also, since C M,M = I, rank(C M ) = m (i.e., full row rank), so,
rank(C M,P1 ) + rank(C M,P2 ) ≥ rank(C M,P1 |C M,P2 ) = m,
and thus
m − nullity(C ⊺ M,P1 ) + m − nullity(C ⊺ M,P2 ) ≥= m, i.e., nullity(C ⊺ M,P1 ) + nullity(C ⊺ M,P2 ) ≤ m
Consider the following two cases:
1 • When nullity(C ⊺ M,P1 ) + nullity(C ⊺ M,P2
) &lt; m, even when these two nullspaces are linearly independent, the number of their supports is less than m and there are not enough number of linearly independent row vectors to fill into H 1 and H 2 to form an invertible H. Contradicted with our hypothesis.
2 • When nullity(C ⊺ M,P1 ) + nullity(C ⊺ M,P2 ) = m, the above independence condition H 1 Z i ⊥ ⊥ H 2 Z i is nontrivial (i.e.
, both are still random vectors with covariance, instead of a collapsing constant zero) only when:
nullity(C ⊺ M,P1 ) &gt; 0 nullity(C ⊺ M,P2 ) &gt; 0 However, this is impossible: Suppose for contradiction that 0 &lt; rank(C M,P1 ), rank(C M,P2 ) &lt; m, then at least the C M,M = I part must be separated, i.e., M ̸ ⊂ P 1 M ̸ ⊂ P 2 .
Then, there must be a partition of M into into smaller respective subsets
(M u , M v ) (we do not use M 1 , M 2 to distinguish from the row indices for H 1 , H 2 ) s.t. M u ⊂ P 1 M v ⊂ P 2 , then, since rank(C M,P1 ) = |M u | and C Mu,Mu = I, C Mv,
Mu must be all zeros.Further, since the M v rows are linear combinations of the M u rows, C Mv,P1\Mu must also be all zeros.Same applies to C M,P2 .We have:
C M,P1 = M u I • • • M v 0 0 M u P 1 \M u and C M,P2 = M u 0 0 M v I • • • M v P 2 \M v ,
However, in this case, C Mu,: E ⊥ ⊥ C Mv,: E, as they share disjoint non-Gaussian E components.This contradicts with the initial hypothesis on a nontrivial subspace Z i , as M u and M v in S will not be mixed in M, but rather produce two independence subspaces at the very beginning.</p>
<p>From the above contradiction, every Z i must be irreducible.So A −1 S,S is an ISA.</p>
<p>Note that while we are not the first to use ISA in linear non-Gaussian models with latent variables, this work is, to the best of our knowledge, the first with an identifiability guarantee.As shown in Examples 1 to 4, the characterization and post-processing of ISA are highly nontrivial.Some prior works (Sanchez-Romero et al., 2019) simply treated ISA solutions like ICA solutions and applied the same post-processing, resulting in inaccuracies.</p>
<p>Other works used ISA mainly for downstream steps e.g., OICA (Hoyer et al., 2008) or independence tests (Dai et al., 2022), but not directly for the LiNG model identification.We believe that the generalized characterization of ISA solutions provided in Theorem 2 can be helpful for future works on causal discovery with latent variables.</p>
<p>D.2 Proof of Lemma 1</p>
<p>Lemma 1.Given an ISA solution W and Γ W on X S , ∀i ∈ S, if pa G (i) ⊂ S, then its exogenous noise component E i is separated out, i.e., ∃j ∈ [m] s.t.(j) ∈ Γ W and (WX S ) j = cE i with a scaling factor c.Moreover, the incoming causal strengths to X i are identified up to c, i.e., the row vector W j = c(I − B S,S ) i .</p>
<p>Proof.For any vertex i and vertex set S of V with i ∈ S and pa G (i) ⊂ S, we can write the variable X i as
X i = A i E (8) = B i,pa G (i) X pa G (i) + E i (9) = B i,S X S + E i (10) = B i,S A S E + E i (11)
where subscripts of index of indices denote the corresponding row/column submatrices.Equation ( 9) to Equation ( 10) is trivial because i has no parents from outside of S, i.e., B i,S\ pa G (i) = 0.</p>
<p>By Equation ( 8)=Equation (11), we have
A i = B i,S A S + 1 |S| i ,(12)
where 1 |S| i denotes the row vector of dimension |S| with only the i-th indexed entry being one, and elsewhere zeros.Equation ( 12) tells that all noise components ("ancestors") coming into X i , except for the E i itself, must go through pa G (i).</p>
<p>Keep only the columns of S on Equation (12), we have
A i,S = B i,S A S,S + 1 |S| i ,(13)
By Equation (13) we have
(A i,S − 1 |S| i )A −1 S,S = B i,S ,(14)
where note that A −1 S,S is exactly the ISA characterization (Section 3.2) for X S .Expand Equation ( 14) we have
A i,S A −1 S,S − 1 |S| i A −1 S,S = B i,S , i.e., 1 |S| i − (A −1 S,S ) i = B i,S ,(15)
Equation ( 15) tells that the i-th row of the ISA characterization A −1 S,S is exactly 1 |S| i − B i,S , i.e., the i-th row of I − B S,S .In other words, (A −1 S,S ) i A S = 1 |S| i .Then, substitute Equation (15) into the demixed subspaces,
(A −1 S,S ) i X S = X i − B i,S X S = E i ,
i.e., the indpendent component (1-dim subspace) of E i is exactly recovered.</p>
<p>Finally, with the subspace-wise permutation and scaling indeterminacies of ISA (Theorem 1), there must be a row in any ISA solution W being proportional to (I − A S,S ) i , and the decomposed component also.The proof is apparent and is almost the same as the above for Lemma 1: since all of T 's children is included (as the "all parents" in that of Lemma 1), all the weights outgoing from T can also be correctly estimated.This can also be seen from the expression of (A −1 S,S ) i,T entries in Equation ( 3), that ISA has indeterminacies of subspace-wise permutations and scalings, and that rows permutations of (A −1 S,S ) i,T with nonzero diagonal entries directly correspond to each of that on A −1 S,S (the equivalence class B in Definition 1).</p>
<p>D.4 Proof of Theorem 3</p>
<p>Theorem 3 (Correctness of local ISA-LiNG).For any T ∈ V, let K be set of weighted edge sets returned by Algorithm 1 on T , mb G (T ), and X.We have:
K = {{(i → j, B ′ j,i ) : ∀j ∈ {T } ∪ ch G ′ (T ), ∀i ∈ pa G ′ (j)} : ∀B ′ ∈ B,
and the graph G ′ defined by B ′ }.</p>
<p>To show the correctness of Algorithm 1, we first show the correctness of the "admissible" block permutations defined in Definition 4. To put it formally, we have the following lemma:
C;Γ , ∃π ∈ Π C , s.t. π ∼ Γ τ ; 2. ∀π ∈ Π C , if ∀S ∈ Γ, (P π C) π[S],π[S] is invertible, then ∃τ ∈ Π C;Γ , s.t. π ∼ Γ τ .
Lemma 5 tells that all permutations that can result in nonzero diagonal entries on an invertible matrix are groupwise equivalent to all permutations that in result in invertible diagonal blocks on the same matrix corresponding to a given partition of the row indices.Note that 1. is universally true, while 2. needs an additional mild assumption that the partition and nonzero-diagonal permutation itself result in invertible diagonal blocks.Clearly the identity π (i.e., P π = I) is in Π C , with C already having nonzero diagonal entries.However, we cannot find any τ ∈ Π C;Γ with π ∼ Γ τ :</p>
<p>Consider a counterexample: C
=   1 1 2 1 1 3 1 0 1   is invertible,D Γ C =   a b 0 c d 0 0 0 e     1 1 2 1 1 3 1 0 1   =   a + b a + b 2a + 3b c + d c + d 2c + 3d e 0 e   , either τ = (1, 2, 3) or (2, 1, 3) is not in Π C;Γ , because a + b a + b c + d c + d
is already not invertible itself.Therefore, we make an additional assumption for 2., which is, as we can see later, trivially satisfied for our choice of invertible C. Now we prove the correctness of Lemma 5:</p>
<p>Proof.First, Π C is nonempty, because C is invertible, det(C) = π sgn(π)Π m i=1 C i,πi ̸ = 0, then at least there is one π s.t.∀i = 1, • • • , m, C i,πi ̸ = 0, and so the inverse of this π will do a rows permutation with nonzero diagonals.The nonemptiness of Π C;Γ can be proved using a similar idea (i.e., ∀Γ, ∀C, ∃τ s.t.P τ D Γ C has invertible diagonal blocks), but using group decomposition of the determinant expression, called "Generalized Laplacian expansion" (Janjic, 2008).</p>
<p>To prove 1., for any τ ∈ Π C;Γ , initialize a new empty π.For any group S ∈ Γ, by definition, the block
(P τ D Γ C) τ [S],τ [S] is invertible. Note that (P τ D Γ C) τ [S],τ [S] = (P τ ) τ [S],S •(D Γ ) S,S •C S,τ [S] , so C S,τ [S] must also be X 1 X 2 X 3 a b c d (i) X 1 X 2 X 3 1/c 1/d b −a/d (ii) X 1 X 2 X 3 1/a 1/b c −d/a (iii)
Remark.</p>
<p>However, note that the above proof relies on the assumption that the cycles in G are disjoint.However, when some cycles in G intersect, things become more complex.The figure to the right shows an example of three cyclic graphs in a LiNG equivalence class with intersected cycles.Globally, there may be none (e.g., when a = b = c = d = 0.8) or multiple (e.g., both (i) and (ii) when a = b = c = 0.8, d = −2) global stable models.In this case, while our method can still identify local correspondings of all equivalent models (Theorem 3), and some unstable solutions may be partially eliminated (using the local stability constraint), we show that the exact identification of the global stable solutions from local variables alone becomes inherently impossible, because intuitively, external cycles appear as self-loops on the local variables.Note that when cycles intersect, the simple cycles' products cannot be related to the stability directly anymore: a LiNG model can be stable with some cycle products larger than one, and a LiNG model with all abs(cycle products) less than one can also be unstable.Even if we force to use cycle products to define stability (as some papers (Rothenhäusler et al., 2015) do), the abovementioned unidentifiability issue of global stable solutions from local variables still remains.
D.6 Proof of Lemma 3 Lemma 3. For any i ∈ V, S ⊂ V{i}, if R S→i ⊥ ⊥ X S , i.e., independent residual, then S ∩ des G (i) = ∅.
The proof can be referred to (Shimizu et al., 2011), by using Darmois-Skitovitch theorem (Darmois, 1953;Skitovitch, 1953).
D.7 Proof of Lemma 4 Lemma 4. ∀i, S in V, if pa G (i) ⊂ S ⊂ V\ des G (i), then ∀j ∈ S, β j S→i = B i,j , and R S→i = E i (so ⊥ ⊥ X S ).
The proof follows naturally from Lemma 1 (in the acyclic graph case).</p>
<p>D.8 Proof of Theorem 4</p>
<p>Theorem 4 (Correctness of Algorithm 2).For any T ∈ V, let K be the weighted edge set returned by Algorithm 2 on T , mb G (T ), and X.We have:
K = {(i → j, B j,i ) : ∀j ∈ {T } ∪ ch G (T ), ∀i ∈ pa G (j)}.
Proof.At every iteration of Algorithm 2, consider a 'last' remaining vertex j ∈ U s.t.there exists no other j ′ ∈ U as j's descendant on G. 1 • if j is T or T 's child, since pa G (j) ⊂ mb G (T ), and since none of pa G (j) can be removed earlier, we have pa G (j) ⊂ U.With all the parents in U and no descendants in U, regress j on U{j} will produce independent residual, and the nonzero coefficients correspond to the true direct parents with true weights, i.e., β i U{j}→j = B j,i ; 2 • if j is T 's spouse, since j is 'last', none of j and T 's common children and descendants are in U. Also since pa G (T ) ⊂ U, every confounding path between j and T is blocked, and thus β T U{j}→j = 0; 3 • it is impossible for j to be T 's parents, since when T pops out, the program breaks.</p>
<p>D.9 Proof of Proposition 1</p>
<p>We first state the following lemmas from Ng et al. (2021, Lemmas 1 &amp; 2) (which were based on Loh and Bühlmann (2014)).The original lemmas in Ng et al. ( 2021) focus on linear acyclic SEM, but their proofs do not make use of the acyclicity constraint.Therefore, we restate the lemmas here for cyclic graphs, whose proofs are similar to the acyclic case and omitted.</p>
<p>Lemma 6. Suppose X follows the linear SEM in Equation (1) with directed cyclic graph G and inverse covariance matrix Θ.The entries of Θ are given by
Θ j,i = −σ −2 j B i,j − σ −2 i B j,i + ℓ̸ =j,i σ −2 ℓ B j,ℓ B i,ℓ , ∀j ̸ = k, Θ j,j = σ −2 j + ℓ̸ =j σ −2 ℓ B 2 j,ℓ , ∀j.
Lemma 7. Suppose X follows the linear SEM in Equation (1) with directed cyclic graph G and inverse covariance matrix Θ.Then, the structure defined by the support of Θ is a subgraph of the moral graph of G.</p>
<p>We then provide the proof for the following proposition.</p>
<p>Proposition 1. Suppose X follows the linear SEM in Equation (1) with directed cyclic graph G and inverse covariance matrix Θ.Under Assumption 1, the structure defined by the support of Θ is the same as the moral graph of G.</p>
<p>Proof.By Lemma 7, the structure defined by the support of Θ is a subgraph of the moral graph of G.By Assumption 1 and Lemma 6, if Θ j,i = 0, then we have B i,j = B j,i = 0 and B j,ℓ B i,ℓ = 0 for all ℓ ̸ = j, i, which, by definition, indicates that i and j are not adjacent in the moral graph of G.This indicates that the moral graph of G is a subgraph of the structure defined by the support of Θ.</p>
<p>E SUPPLEMENTARY EXPERIMENTS DETAILS</p>
<p>We provide additional details for the experiments conducted in Section 5. Specifically, we provide the implementation details of our method and the baselines in Appendices E.1 and E.2, respectively.We then describe in Appendix E.3 the performance metrics used in our experiments.</p>
<p>the method then identifies all (1) undirected edges involving T and (2) v-structures involving T to be the final local structure around T .In our modification, we apply A<em> on only T and its MB mb G (T ).In this case, the modified method identifies all (1) undirected edges involving T and (2) v-structures of which T is not the collider to be the final estimated local structure around T .The resulting algorithm performs local discovery on only T and its MB mb G (T ).Here, we use BIC score (Schwarz, 1978) for the A</em> search.</p>
<p>GSBN.The original GSBN method (Margaritis and Thrun, 1999) applies certain rules based on conditional independence tests to identify all (1) undirected edges involving T and (2) v-structures of which T is the collider.The latter requires information of two-step MBs, i.e., mb G (T ) and MB of each variable in mb G (T ).In our modification, we adopt different but similar rules to identify (1) undirected edges involving T and (2) v-structures of which T is not the collider:</p>
<p>1.For each X ∈ mb G (T ), determine X to be a (direct) neighbor of T if T ̸ ⊥ ⊥ X|S for all S ⊆ mb G (T ) \ {Y }.</p>
<ol>
<li>Given the neighbors of target T identified in the first step, we use the following rule to identify the vstructures: for each Z ∈ mb G (T ) and Y being a neighbor of T , determine
T → Y ← Z to be a v-structure if T ̸ ⊥ ⊥ Z|S ∪ {Y } for all S ⊆ mb G (T ) \ {Y, Z}.
The resulting algorithm performs local discovery on only T and its MB mb G (T ).Here, we use Fisher Z test with significance level of 0.05 for identifying conditional independence relations.</li>
</ol>
<p>CMB.For the CMB method (Gao and Ji, 2015), we use an implementation through the pyCausalFS package.</p>
<p>LDECC.We use the default implementation provided by Gupta et al. (2023).</p>
<p>E.3 Performance Metrics</p>
<p>In the acyclic case, we report two performance matrics, namely SHD of local DAG and PDAG.In the cyclic case, we report the SHD of local DCG.These metrics are explained in details below.For each setting, the metrics are calculated over 8 random simulations.</p>
<p>SHD of local DAG.For this metric, the ground-truth and estimated structures contain all incoming directed edges of T and its children.We then compute the SHD between the ground-truth and estimated local structures.Such a metric is used to validate our method (see Theorem 3) that can estimate more edges and directions than the baselines.Note that the estimated output by Local A* and GSBN may contain undirected edges; therefore, we enumerate all possible combinations of directed edges from these undirected ones, and compute the final averaged SHD for these two methods.</p>
<p>SHD of local PDAG.Since Local A* and GSBN return PDAG around the target T , we design this metric specifically for these baselines.In particular, the ground-truth and estimated local structures contain (1) undirected edges involving T and (2) v-structures of which T is not the collider.We then compute the SHD between the ground-truth and estimated local structures.Note that since our method returns DAG around the target T that contains additional edges, we convert it into the same format of PDAG as well.</p>
<p>SHD of local DCG.This metric is similar to the SHD of local DAG explained above, except that the ground truth may contain cycles.</p>
<p>F SUPPLEMENTARY EXPERIMENTS RESULTS</p>
<p>F.1 Running Time</p>
<p>We report the running times for different sizes of MBs.Specifically, we follow the data generating procedure in Section 5.2.We generate random DAGs with expected degrees of 3, 5, and 7, and select target with a relatively large MB.The running times of different methods, including those with and without oracle MBs, are shown in Figure 9.Note that, for Local A<em>, instances with MBs exceeding 19 variables are omitted due to the long running time.It is observed that our method has a longer running time than that of LDECC and CMB.Furthermore, when size of MB increases, the running time of our method is shorter and increases much slowly compared to GSBN and Local A</em>.</p>
<p>Local Causal Discovery with Linear non-Gaussian Cyclic Models</p>
<p>F.2 Additional Figures</p>
<p>This section provides additional figures for Section 5, namely Figures 10,11,12,13,14,15,16,17,and 18.</p>
<p>-0 .7 2 -0 .3) GSBN, respectively.We use underlined vertices to denote target variables.For the second column, green and blue arrows denote correct directed edges and wrong directed edges discovered by our method, respectively.For the third and fourth columns, blue lines denote correct undirected edges discovered by the baselines.</p>
<p>Figure 2 :
2
Figure 2: For Example 4: row permutation on ISA matrices with nonzero diagonals can be entirely incorrect.</p>
<p>Algorithm 1
1
Local ISA-LiNG Input: A target T ∈ V, its oracle MB mb G (T ), and data X.Assume w.l.o.g.S := {T } ∪ mb G (T ) = [m] Output: A set of directed weighted edge sets 1: Initialize the output set K := ∅; 2: Obtain an ISA solution W with Γ W on X S ; 3: Set C := supp(W :,T ) = (i ∈ [m] : W i,T ̸ = 0); 4: for any permutation π admissible to W, Γ W do 5: Initialize K := ∅; 6: Set W ′ := P π W; 7:</p>
<p>Figure 3 :
3
Figure 3: For Example 5, to illustrate Algorithm 1.</p>
<p>9 :
9
Add to K an edge i → j with weight β i U{j}→j for each i ∈ U{j} with β i U{j}→j ̸ = 0; 10: break if j = T ; Otherwise set U := U{j}; 11: end while 12: Return K;(ii) (i)</p>
<p>Figure 4 :
4
Figure 4: Examples to illustrate Algorithm 2.</p>
<p>Figure 3 as ground truth.We simulate 2000 samples from the LiNG SEM in Equation (1), of which the nonzero weights of B are sampled uniformly from [−0.9, −0.5] ∪ [0.5, 0.9], and each exogenous noise E i is sampled uniformly from [−c i , c i ] to the power of 5, with c i sampled randomly from [0.75, 1.25].We first run the ICA-LiNG method byLacerda et al. (2008) on all variables.To perform local causal discovery, we also run our Local ISA-LiNG method on target T = 3 and its MB {1, 2, 4, 5}.An example of the outputs by both methods, including the estimated edge weights, are provided in Figure10in Appendix F.2.</p>
<p>Figure 7 :
7
Figure 5: SHD of local DCG under estimated MB.</p>
<p>We first state the following lemmas (and proofs) fromNg et al. (2021, Lemmas 1  andBühlmann (2014)).The original lemmas in Ng et al. (2021) focus on linear do not make use of the acyclicity constraint.Therefore, we state the lemmas here Lemma 5. Consider a directed cyclic graph G and distribution P that follow a linea matrix ⇥.Let (B, ⌦) be the weighted adjacency matrix and noise covariance ma ⌦ = diag( 2 1 , . . ., 2 d ).The entries of ⇥ are given by</p>
<p>We first state the following lemmas (and proofs) fromNg et al. (2021, Lemmas 1 &amp;  andBühlmann (2014)).The original lemmas inNg et al. (2021)  focus on linear a do not make use of the acyclicity constraint.Therefore, we state the lemmas here f Lemma 5. Consider a directed cyclic graph G and distribution P that follow a linear matrix ⇥.Let (B, ⌦) be the weighted adjacency matrix and noise covariance matr ⌦ = diag( 2 1 , . . ., 2 d ).The entries of ⇥ are given by</p>
<p>We first state the following lemmas (and proofs) from Ng and Bühlmann (2014)).The original lemmas in Ng et al. do not make use of the acyclicity constraint.Therefore, we Lemma 5. Consider a directed cyclic graph G and distribut matrix ⇥.Let (B, ⌦) be the weighted adjacency matrix an ⌦ = diag( 2 1 , . . ., 2 d ).The entries of ⇥ are given by</p>
<p>of PropositionWe first state the following lem and Bühlmann (2014)).The o do not make use of the acyclic Lemma 5. Consider a directed matrix ⇥.Let (B, ⌦) be the w ⌦</p>
<p>Figure 8 :
8
Figure 8: Example of two equivalent cyclic LiNG models.</p>
<p>D. 3
3
Proof of Lemma 2 Lemma 2. Given an ISA solution W and Γ W on X S for S = {T } ∪ mb G (T ).Denote by C := supp(W :,T ) = (i ∈ [m] : W i,T ̸ = 0).Then ∀i ∈ C, W i must produce a single component, i.e., (i) ∈ Γ W .Moreover, {π[C] : π admissible to W} = {supp(B ′ :,T ) : B ′ ∈ B}.</p>
<p>Lemma 5 .
5
Let C be an arbitrary m × m invertible matrix, Γ be an arbitrary partition of [m].Denote by Π C as the set of all the rows permutations of C that result in nonzero diagonal entries, i.e., Π C := {π : P π C has all the nonzero diagonal entries.},Denote by Π C;Γ as all the rows permutations that result in invertible diagonal blocks on general scaled C, i.e., Π C;Γ := {π : ∃D Γ , ∀S ∈ Γ, rank((P π D Γ C) π[S],π[S] ) = |S|}, where D Γ is any general scaling matrix (defined in Section 3.1) consistent with Γ.For any two permutations π and τ of [m], we say they are groupwise equivalent regarding a partition Γ of [m], denoted by π ∼ Γ τ , if and only if ∀S ∈ Γ, π[S] and τ [S] have exactly the same elements.Then, Π C and Π C;Γ are equivalent up to groupwise permutations, i.e., 1. ∀τ ∈ Π</p>
<p>and a partition of row indices Γ = {(1, 2), (3, )}.</p>
<p>Figure 9 :
9
Figure 9: Running time of different methods.Y-axis is in log scale.</p>
<p>Estimated graphs and edge weights by Local ISA-LiNG using T = 3 as target.</p>
<p>Figure 10 :
10
Figure 10: Ground truth and estimated cyclic graphs and edge weights with 2000 samples.</p>
<p>Figure 12 :
12
Figure 12: Results of local causal discovery with 2000 samples and degree of 3 under estimated MB.</p>
<p>SHD of local PDAG.</p>
<p>Figure 13 :
13
Figure 13: Results of local causal discovery with 2000 samples and degree of 3 under oracle MB.</p>
<p>Figure 14 :Figure 15 :
1415
Figure 14: Results of local causal discovery with 2000 samples and degree of 5 under estimated MB.</p>
<p>Figure 16 :
16
Figure 16: Edge weights estimated by Local ISA-LiNG under oracle MB.</p>
<p>Figure 17 :Figure 18 :
1718
Figure 17: Edge weights estimated by Local ISA-LiNG under estimated MB.</p>
<p>Table 1 :
1
(Margaritis and Thrun, 1999) and Local A * on a standard real-world dataset that collects continuous expression levels of proteins and phospholipids within human immunological cells(Sachs et al., 2005), characterized by 853 observational samples and a ground truth DAG with 11 variables and 17 edges.Here, we select PIP2, PIP3, and Akt as target variables, and compute the SHD of local DAG obtained by all three methods.As shown in the Table1, our method achieves lower SHD in most cases.A detailed comparison of ground-truth and estimated local causal structures can be found in Figure18in Appendix F.2. SHD of different local causal discovery methods on real data by Sachs et al. (2005).The local causal discovery procedures presented in Sections 3 and 4 rely on knowledge about the MB of the target variable T i.e., its parents, children, and spouses.To the best of our knowledge, many existing MB estimation methods, such as those based on nonparametric conditional independence test, e.g., GSMB(Margaritis and Thrun, 1999),IAMB (Tsamardinos et al., 2003), and MMMB (Tsamardinos et al., 2006), focus on the Bayesian network (i.e., acyclic) setting.That is, it may not be immediately clear how their estimated MB relates to the true one mb G (T ) in the presence of cycles, partly owing to the extra complications involved when handling cycles with conditional independence tests (Spirtes
Target Ours GSBN Local A  *PIP211.53.6PIP3114Akt11.31.36 CONCLUSIONWe have expanded local causal discovery to includecyclic scenarios by generalizing the classic LiNGAM-based methods. Notably, while previous local searchmethods based on conditional independence tests orlikelihood-based scores often fail to determine the di-rection of certain edges, our method leverages non-Gaussianity to enable more precise edge orientations.This leads to a more comprehensive representation ofthe causal graph, even in cyclic contexts. Addition-
ally, we have established identifiability guarantees for all our proposed methods.These theoretical findings have been validated using various datasets in both synthetic and real-world settings.Future work includes characterizing the number of possible structures in the cyclic equivalence class estimated by our method.T. S. Richardson.Discovering cyclic causal structure.Carnegie Mellon [Department of Philosophy], 1996.D. Rothenhäusler, C. Heinze, J. Peters, and N. Meinshausen.Backshift: Learning causal cyclic graphs from unknown shift interventions.Advances in Neural Information Processing Systems, 28, 2015.K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger, and G. P. Nolan.Causal protein-signaling networks derived from multiparameter single-cell data.V. P. Skitovitch.On a property of the normal distribution.DAN SSSR, 89:217-219, 1953.P. Spirtes.Conditional independence in directed cyclic graphical models representing feedback or mixtures.Technical report, Philosophy, Methodology and Logic Technical Report 59, CMU, 1994.J. Yin, Y. Zhou, C. Wang, P. He, C. Zheng, and Z. Geng.Partial orientation and local structural learning of causal networks for prediction.In Causation and Prediction Challenge, pages 93-105.PMLR, 2008.K. Yu, Z. Ling, L. Liu, P. Li, H. Wang, and J. Li.</p>
<p>AcknowledgementsThe authors would like to thank reviewers for their helpful comments.This material is based upon work supported by the AI Research Institutes Program funded by the National Science Foundation under AI Institute for Societal Decision Making (AI-SDM), Award No. 2229881.The project is also partially supported by the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Apple Inc., KDDI Research Inc., Quris AI, and Infinite Brain Technology.B Illustrative ExamplesManuscript under review by AISTATS 2024Local Cyclic Causal Discovery with Independent S Supplementary MaterialsA PROOFS OF MAIN RESULTSA.1 Proof of Theorem 2Theorem 2 (One characterization of ISA in LiNG).Assume X follows a LiNG X S ⇢ V, the inverse of the principal submatrix of B indexed by S, denoted by B 1 S,S invertible.By above nonemptiness, we know that C S,τ [S] can be row permutated to one with nonzero diagonal entries.Then, we set the corresponding indices as this permutation, i.e., set (P π ) τ [S],S as this permutation submatrix.Then P π C has nonzero diagonals, and for any S, π[S] and τ [S] have the same elements (row indices).To prove 2., it sufficies to show that for any π ∈ Π C , there is also π ∈ Π C;Γ .For any S ∈ Γ, consider the principal submatrix (P π C) π[S],π[S], which is assumed to be invertible.Note that (Pand so C S,π[S] is invertible.For any D Γ , we have (, where each factor is invertible, soBy setting the invertible matrix C as the ISA characterization A −1 S,S (Section 3.2), we know that the admissible post-processing of rows permutation on any general ISA solution matrices W will make all subspaces at the correct location.The additional assumption for 2. that diagonal blocks of C are invertible echoes the 'weak stability' assumption mentioned in(Hyttinen et al., 2012).For correctness, specifically, the 1-dim subspaces (independent components), including the T and T 's children that we are interested in, can be identified at the correct location (for each LiNG model in the equivalence class B).The last step left is to associate the A −1 S,S to the local adjacencies B S,S .By Equation (3), though in general A −1 S,S ̸ = B S,S , they must be equal on the rows of T and T 's children, as their parents are all included in S. Finally, the correctness of Algorithm 1 is proved.D.5 Proof of Corollary 1Corollary 1 (Identifying stable solutions locally, with disjoint cycles).Suppose the cycles are disjoint in G. Consider a modified version of Algorithm 1 in which the line "if B ′ is not convergent: skip" is added between lines 8 and 9.Then, this modified version of Algorithm 1 will yield a single local model, corresponding exactly to the unique global stable model.That is, the returned K consists of a single item K, with From Example 3 we know that A −1 S,S does not necessarily have all diagonals as ones, so we further remove all self-loops on G (S) .Then, all cycles in this local G (S) , if any, must be disjoint.Further, for each local cycle in G (S) consisting of vertices C (S) ⊂ S, we have the followings:1.All vertices on the local cycle need no row scalings, i.e., ∀i ∈ C (S) , (A −1 S,S ) i,j = 1, and 2. There exists a global cycle in G consisting of vertices C with C (S) ⊂ C (in a consistent ordering), and, 3. The cycle product of this local cycle C (S) on G (S) equals the cycle product of the global cycle C on G.The above statement follows from Equation (3): the (i, j)-th entry of I − A −1 S,S corresponds not only to the direct causal effect from j to i, but also the total causal effect from j to i through all other variables outside of S. For the above point 1., a vertex i has non-unit diagonal A −1 S,S ̸ = 1 only when i is involved in a cycle where all the remaining vertices in this cycle is not in S (see Example 3), so that this cycle appear as a self-loop on i relative to S. As cycles are disjoint, i cannot belong to any cycle in G (S) .Point 1 shows that whenever a cycle can appear locally, the edge weights on this cycle must follow exactly from A −1 S,S , without any scalings.Then, using the characterization in Equation (3), points 2 and 3 show that the cycle products can be preserved locally.A local stable model (with disjoint cycles and abs(cycle products)&lt; 1) must correspond to a global model with those stable cycles, which, in Algorithm 1's case, implies the correct local stable model among {T } ∪ ch G * (T ).E.1 Implementation Details of Local ISA-LiNGThe proposed Local ISA-LiNG method involves estimating the demixing matrix with ISA (see Algorithm 1).One could use the ISA procedure developed byTheis (2006).In this work, we use ICA to first estimate the components, and then use independence test, i.e. the Hilbert-Schmidt independence criterion (HSIC)(Gretton et al., 2007), to identify the subspaces from the components estimated by ICA.Such a procedure is found to work well in practice.For the HSIC test in our experiments, we use a significance level of 0.05.Furthermore, as suggested byLacerda et al. (2008), we adopt ICA with sparse connection(Zhang and Chan, 2006)for the specific ICA procedure.In the acyclic case , we use a more efficient post-processing procedure to identify the edges from the demixing matrix W, described in Appendix C.We perform nodewise regression with Lasso(Meinshausen and Bühlmann, 2006)to estimate the MB of each target variable (see Appendix A for more details).Moreover, when applying Algorithms 1 and 3 to identify the edges from the estimated demixing matrix, we use a threshold of 0.05 to set the entries with small absolute values to zero.All experiments are conducted on 2 CPUs with 4GB of memory.The code is available at https://github.com/MarkDana/local-ling-discovery.E.2 Implementation Details of Existing MethodsWe provide the implementation details of several existing methods considered in our experiments.All experiments are conducted on 2 CPUs with 4GB of memory.For Local A<em> and GSBN, we perform nodewise regression with Lasso(Meinshausen and Bühlmann, 2006)to estimate the MB of each target variable, similar to our Local ISA-LiNG method.For CMB and LDECC, we use their default method for MB estimation.In the following, we describe the implementation details of each method.ICA-LiNG.As suggested byLacerda et al. (2008), we use ICA with sparse connection(Zhang and Chan, 2006)for the specific ICA procedure, and a threshold of 0.05 for the demixing matrix, similar to our method.Local A</em>.The original Local A<em> method(Ng et al., 2021)applies exact search strategy like A</em> (Yuan and Malone, 2013) on the target T , its MB mb G (T ), and MB of each variable in mb G (T ).In the estimated structure,
Transcriptional feedback loop regulation, function and ontogeny in Drosophila. J Benito, H Zheng, F S Ng, P E Hardin, Cold Spring Harbor symposia on quantitative biology. NIH Public Access200772437</p>
<p>Independent component analysis, a new concept? Signal processing. P Comon, 199436</p>
<p>Independence testing-based approach to causal discovery under measurement error and linear Non-Gaussian models. H Dai, P Spirtes, K Zhang, Advances in Neural Information Processing Systems. 202235</p>
<p>Analyse générale des liaisons stochastiques: etude particulière de l'analyse factorielle linéaire. Revue de l'Institut international de statistique. G Darmois, 1953</p>
<p>On random graphs I. Publicationes Mathematicae. P Erdös, A Rényi, 19596</p>
<p>Sparse inverse covariance estimation with the graphical Lasso. J Friedman, T Hastie, R Tibshirani, Biostatistics. 92008</p>
<p>Local causal discovery of direct causes and effects. T Gao, Q Ji, Advances in Neural Information Processing Systems. 201528</p>
<p>Local-toglobal Bayesian network structure learning. T Gao, K Fadnis, M Campbell, International Conference on Machine Learning. PMLR2017</p>
<p>Characterizing distribution equivalence and structure learning for cyclic and acyclic directed graphs. A Ghassami, A Yang, N Kiyavash, K Zhang, International Conference on Machine Learning. PMLR2020</p>
<p>A kernel statistical test of independence. A Gretton, K Fukumizu, C Teo, L Song, B Schölkopf, A Smola, Advances in Neural Information Processing Systems. 2007</p>
<p>Local causal discovery for estimating causal effects. S Gupta, D Childers, Z C Lipton, Conference on Causal Learning and Reasoning. PMLR2023</p>
<p>The statistical implications of a system of simultaneous equations. T Haavelmo, Journal of the Econometric Society. 1943Econometrica</p>
<p>Estimation of causal effects using linear non-Gaussian causal models with hidden variables. P O Hoyer, S Shimizu, A J Kerminen, M Palviainen, International Journal of Approximate Reasoning. 4922008</p>
<p>Learning linear cyclic causal models with latent variables. A Hyttinen, F Eberhardt, P O Hoyer, The Journal of Machine Learning Research. 1312012</p>
<p>Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces. A Hyvärinen, P Hoyer, Neural computation. 1272000</p>
<p>Independent component analysis: algorithms and applications. A Hyvärinen, E Oja, Neural networks. 200013</p>
<p>A proof of generalized Laplace's expansion theorem. M Janjic, Bull. Soc. Math. Banja Luka. 152008. 2008</p>
<p>Discovering cyclic causal models by independent components analysis. G Lacerda, P Spirtes, J Ramsey, P O Hoyer, Conference on Uncertainty in Artificial Intelligence. 2008</p>
<p>Gene regulatory networks for development. M Levine, E H Davidson, Proceedings of the National Academy of Sciences. 102142005</p>
<p>Using feature selection for local causal structure learning. Z Ling, K Yu, H Wang, L Li, X Wu, IEEE Transactions on Emerging Topics in Computational Intelligence. 542020</p>
<p>High-dimensional learning of linear causal networks via inverse covariance estimation. P.-L Loh, P Bühlmann, Journal of Machine Learning Research. 15882014</p>
<p>Local causal pathway discovery for singlecell rna sequencing count data: a benchmark study. S Ma, J Wang, C Bieganek, R Tourani, C Aliferis, Journal of Translational Genetics and Genomics. 712023</p>
<p>Bayesian network induction via local neighborhoods. D Margaritis, S Thrun, 199912Advances in neural information processing systems</p>
<p>Feedback theory-some properties of signal flow graphs. S J Mason, Proceedings of the IRE. the IRE195341</p>            </div>
        </div>

    </div>
</body>
</html>
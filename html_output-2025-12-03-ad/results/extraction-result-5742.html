<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5742 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5742</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5742</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-38c0543aa72b68d1ded4237e8cc5333b165ea249</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/38c0543aa72b68d1ded4237e8cc5333b165ea249" target="_blank">LogBERT: Log Anomaly Detection via BERT</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Joint Conference on Neural Network</p>
                <p><strong>Paper TL;DR:</strong> LogBERT is proposed, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT), which outperforms state-of-the-art approaches for anomaly detection.</p>
                <p><strong>Paper Abstract:</strong> Detecting anomalous events in online computer systems is crucial to protect the systems from malicious attacks or malfunctions. System logs, which record detailed information of computational events, are widely used for system status analysis. In this paper, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). LogBERT learns the patterns of normal log sequences by two novel self-supervised training tasks, masked log message prediction and volume of hypersphere minimization. After training, LogBERT is able to capture the patterns of normal log sequences and further detect anomalies where the underlying patterns deviate from expected patterns. The experimental results on three log datasets show that LogBERT outperforms state-of-the-art approaches for anomaly detection.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5742.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5742.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT: Log Anomaly Detection via BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-inspired Transformer encoder trained with two self-supervised objectives (masked log-key prediction and a hypersphere-volume minimization) to model normal log-key sequences and detect anomalous log sequences via masked-token prediction likelihoods and distance-from-center scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogBERT (BERT-style Transformer encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Transformer-encoder (BERT-style) adapted to log-key sequences: input is sequence of parsed log keys (with a special DIST token). The implementation used two Transformer layers, input embedding dimension 50, hidden dimension 256. Trained self-supervised with (1) Masked Log Key Prediction (MLKP) analogous to BERT's MLM and (2) Volume of Hypersphere Minimization (VHM), a spherical objective that pulls sequence representations (DIST token embeddings) close to a center.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Self-supervised fine-tuning on normal-only data: (a) MLKP — randomly mask log keys and train to predict them (softmax over log-key vocabulary); (b) VHM — minimize mean squared distance of DIST token embeddings to a center vector. At inference, random masks are applied and for each masked position the model produces top-g candidate keys; a key not in top-g is treated as anomalous; a sequence with > r anomalous keys is labeled anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Ordered sequences of log keys (semi-structured log sequences produced by log parsing); sequence-level and token-level structured sequential data.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequence-level anomalies (anomalous sequences composed of otherwise normal-looking log entries), and anomalous log keys within sequences (deviations from learned sequential patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (parsed log datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision / Recall / F1 reported per dataset. LogBERT results: HDFS P=87.02, R=78.10, F1=82.32; BGL P=89.40, R=92.32, F1=90.83; Thunderbird P=96.75, R=96.52, F1=96.64.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LogBERT outperformed traditional unsupervised baselines (PCA, One-Class SVM, IsolationForest, LogCluster) and deep-learning baselines (DeepLog, LogAnomaly) on F1 across all three evaluated datasets. Example comparisons: DeepLog F1s = {HDFS:77.34, BGL:86.12, Thunderbird:93.08}; LogAnomaly F1s = {HDFS:56.19, BGL:74.08, Thunderbird:92.73}, showing LogBERT gains (notably large on HDFS).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires a log parser to obtain log keys (parsing errors propagate). Needs only-normal training data and tuning of hyperparameters (mask ratio m, candidate set size g, anomalous-key threshold r, VHM weight α). VHM alone performs poorly; MLKP dominates for long sequences (VHM more helpful for short sequences). Detection depends on choice of mask ratio and top-g candidate size (trade-off between precision and recall). Model size/compute requirements and parameter counts not specified. The detection method assumes anomalies manifest as deviations in masked-token predictability or large distance from center; attacks that preserve local masked-token likelihoods might evade detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5742.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5742.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT: Bidirectional Encoder Representations from Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained bidirectional Transformer encoder model using masked-language-model and next-sentence objectives to produce contextual token embeddings; used here as the architectural inspiration for LogBERT's modeling of log-key sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bidirectional Transformer encoder pretrained with masked-language-model (MLM) and next sentence prediction (NSP) objectives; provides contextualized token embeddings by attending to both left and right context. The paper references BERT conceptually and adopts a similar masked-token training objective for logs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Mentioned as architectural and training inspiration (masked-token prediction) rather than directly used; LogBERT adapts BERT-style MLM to log-key prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Textual / sequential data (here adapted to sequences of log keys).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Not directly applied in experiments in this paper; cited as enabling bidirectional context modeling for sequence anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not directly evaluated in the paper; only referenced as the inspiration for masked-token training and Transformer encoder usage. No parameter counts or pretraining details taken from original BERT in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5742.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5742.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based approach that models normal log-key sequences by predicting the next log key; anomalies are detected when the actual next key is not among the top-g predicted candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepLog (LSTM RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent neural network (LSTM) trained to predict the next log-key in a sequence given past keys; detection based on next-key prediction accuracy (top-g candidate approach).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Sequence modeling via next-log prediction (autoregressive). At inference, compare model's top-g predicted next keys with observed key; if observed key not in top-g it's anomalous; sequence-level anomaly decisions are based on counts of anomalous tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Ordered sequences of parsed log keys (log sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequence anomalies caused by unexpected next-events / broken correlations among events in a sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (used as baseline evaluations in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 2: HDFS P=88.44, R=69.49, F1=77.34; BGL P=89.74, R=82.78, F1=86.12; Thunderbird P=87.34, R=99.61, F1=93.08.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>DeepLog significantly outperformed traditional counting/vector-based methods (PCA, OCSVM, iForest) and is a strong deep-learning baseline; LogBERT achieved higher F1 than DeepLog on all three datasets, suggesting the Transformer encoder + MLKP (and VHM) better captures log-sequence patterns than LSTM next-step prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes RNN-based approaches (like DeepLog) can only condition on prior context (no bidirectional context) and are trained for next-key prediction, which may fail to explicitly encode common patterns shared by all normal sequences; can mislabel sequences where bidirectional/contextual cues would be necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5742.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5742.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogAnomaly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning approach for log anomaly detection designed to handle both sequential and quantitative anomalies in unstructured logs; used here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogAnomaly</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep-learning based anomaly detection that models sequence patterns and numeric/count-based anomalies in logs; exact architecture not detailed in this paper (referenced and used as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Unsupervised deep modeling of log sequences to detect sequential (order) anomalies and quantitative (statistical) anomalies; used here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Parsed log-key sequences, with consideration of sequence-level and quantitative features.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequential anomalies and quantitative anomalies in logs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (evaluated as baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 2: HDFS P=94.15, R=40.47, F1=56.19; BGL P=73.12, R=76.09, F1=74.08; Thunderbird P=86.72, R=99.63, F1=92.73.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms many traditional methods on some datasets but underperforms LogBERT in F1 on all three datasets in this paper; LogBERT achieved higher balanced precision/recall trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In HDFS it attains high precision but low recall, indicating possible imbalance in sensitivity to anomalies or thresholds; details of architecture/limitations are not expanded in this paper (refer to original LogAnomaly paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5742.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5742.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep SVDD (inspiration)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep One-Class Classification (Deep SVDD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep one-class classification method that learns representations by minimizing the volume of a hypersphere enclosing the normal data in latent space; cited as inspiration for LogBERT's Volume of Hypersphere Minimization (VHM) objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep One-Class Classification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deep SVDD (conceptual inspiration)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a mapping of inputs into a representation space where the network is trained to minimize the distance of normal samples to a learned center (minimize hypersphere volume), enabling anomaly detection via distance to center.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>One-class training objective that minimizes squared distances of representations to a center (spherical objective); anomalies detected by large distances from center.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>General - images and other vector data in original work; here the concept is applied to DIST token embeddings of log sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Outlier/anomaly detection as distance-based anomalies in embedding space (samples far from center considered anomalies).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as methodological inspiration; the paper reports that VHM alone (the analogous spherical objective) performs poorly for this log application, but combined with MLKP improves performance especially for short sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Spherical objective alone was insufficient for good detection in log sequences (poor performance when used alone); distance-only measures may fail to capture nuanced sequential structure in logs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
                <li>Deep One-Class Classification <em>(Rating: 2)</em></li>
                <li>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs <em>(Rating: 2)</em></li>
                <li>Drain: An Online Log Parsing Approach with Fixed Depth Tree <em>(Rating: 1)</em></li>
                <li>Detecting large-scale system problems by mining console logs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5742",
    "paper_id": "paper-38c0543aa72b68d1ded4237e8cc5333b165ea249",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "LogBERT",
            "name_full": "LogBERT: Log Anomaly Detection via BERT",
            "brief_description": "A BERT-inspired Transformer encoder trained with two self-supervised objectives (masked log-key prediction and a hypersphere-volume minimization) to model normal log-key sequences and detect anomalous log sequences via masked-token prediction likelihoods and distance-from-center scoring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogBERT (BERT-style Transformer encoder)",
            "model_description": "A Transformer-encoder (BERT-style) adapted to log-key sequences: input is sequence of parsed log keys (with a special DIST token). The implementation used two Transformer layers, input embedding dimension 50, hidden dimension 256. Trained self-supervised with (1) Masked Log Key Prediction (MLKP) analogous to BERT's MLM and (2) Volume of Hypersphere Minimization (VHM), a spherical objective that pulls sequence representations (DIST token embeddings) close to a center.",
            "model_size": null,
            "anomaly_detection_method": "Self-supervised fine-tuning on normal-only data: (a) MLKP — randomly mask log keys and train to predict them (softmax over log-key vocabulary); (b) VHM — minimize mean squared distance of DIST token embeddings to a center vector. At inference, random masks are applied and for each masked position the model produces top-g candidate keys; a key not in top-g is treated as anomalous; a sequence with &gt; r anomalous keys is labeled anomalous.",
            "data_type": "Ordered sequences of log keys (semi-structured log sequences produced by log parsing); sequence-level and token-level structured sequential data.",
            "anomaly_type": "Sequence-level anomalies (anomalous sequences composed of otherwise normal-looking log entries), and anomalous log keys within sequences (deviations from learned sequential patterns).",
            "dataset_name": "HDFS, BGL, Thunderbird (parsed log datasets)",
            "performance_metrics": "Precision / Recall / F1 reported per dataset. LogBERT results: HDFS P=87.02, R=78.10, F1=82.32; BGL P=89.40, R=92.32, F1=90.83; Thunderbird P=96.75, R=96.52, F1=96.64.",
            "baseline_comparison": "LogBERT outperformed traditional unsupervised baselines (PCA, One-Class SVM, IsolationForest, LogCluster) and deep-learning baselines (DeepLog, LogAnomaly) on F1 across all three evaluated datasets. Example comparisons: DeepLog F1s = {HDFS:77.34, BGL:86.12, Thunderbird:93.08}; LogAnomaly F1s = {HDFS:56.19, BGL:74.08, Thunderbird:92.73}, showing LogBERT gains (notably large on HDFS).",
            "limitations_or_failure_cases": "Requires a log parser to obtain log keys (parsing errors propagate). Needs only-normal training data and tuning of hyperparameters (mask ratio m, candidate set size g, anomalous-key threshold r, VHM weight α). VHM alone performs poorly; MLKP dominates for long sequences (VHM more helpful for short sequences). Detection depends on choice of mask ratio and top-g candidate size (trade-off between precision and recall). Model size/compute requirements and parameter counts not specified. The detection method assumes anomalies manifest as deviations in masked-token predictability or large distance from center; attacks that preserve local masked-token likelihoods might evade detection.",
            "uuid": "e5742.0",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT: Bidirectional Encoder Representations from Transformers",
            "brief_description": "A pre-trained bidirectional Transformer encoder model using masked-language-model and next-sentence objectives to produce contextual token embeddings; used here as the architectural inspiration for LogBERT's modeling of log-key sequences.",
            "citation_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "mention_or_use": "mention",
            "model_name": "BERT",
            "model_description": "Bidirectional Transformer encoder pretrained with masked-language-model (MLM) and next sentence prediction (NSP) objectives; provides contextualized token embeddings by attending to both left and right context. The paper references BERT conceptually and adopts a similar masked-token training objective for logs.",
            "model_size": null,
            "anomaly_detection_method": "Mentioned as architectural and training inspiration (masked-token prediction) rather than directly used; LogBERT adapts BERT-style MLM to log-key prediction.",
            "data_type": "Textual / sequential data (here adapted to sequences of log keys).",
            "anomaly_type": "Not directly applied in experiments in this paper; cited as enabling bidirectional context modeling for sequence anomalies.",
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": "",
            "limitations_or_failure_cases": "Not directly evaluated in the paper; only referenced as the inspiration for masked-token training and Transformer encoder usage. No parameter counts or pretraining details taken from original BERT in the experiments.",
            "uuid": "e5742.1",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "DeepLog",
            "name_full": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "brief_description": "An LSTM-based approach that models normal log-key sequences by predicting the next log key; anomalies are detected when the actual next key is not among the top-g predicted candidates.",
            "citation_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "mention_or_use": "use",
            "model_name": "DeepLog (LSTM RNN)",
            "model_description": "Recurrent neural network (LSTM) trained to predict the next log-key in a sequence given past keys; detection based on next-key prediction accuracy (top-g candidate approach).",
            "model_size": null,
            "anomaly_detection_method": "Sequence modeling via next-log prediction (autoregressive). At inference, compare model's top-g predicted next keys with observed key; if observed key not in top-g it's anomalous; sequence-level anomaly decisions are based on counts of anomalous tokens.",
            "data_type": "Ordered sequences of parsed log keys (log sequences).",
            "anomaly_type": "Sequence anomalies caused by unexpected next-events / broken correlations among events in a sequence.",
            "dataset_name": "HDFS, BGL, Thunderbird (used as baseline evaluations in this paper)",
            "performance_metrics": "Reported in Table 2: HDFS P=88.44, R=69.49, F1=77.34; BGL P=89.74, R=82.78, F1=86.12; Thunderbird P=87.34, R=99.61, F1=93.08.",
            "baseline_comparison": "DeepLog significantly outperformed traditional counting/vector-based methods (PCA, OCSVM, iForest) and is a strong deep-learning baseline; LogBERT achieved higher F1 than DeepLog on all three datasets, suggesting the Transformer encoder + MLKP (and VHM) better captures log-sequence patterns than LSTM next-step prediction.",
            "limitations_or_failure_cases": "Paper notes RNN-based approaches (like DeepLog) can only condition on prior context (no bidirectional context) and are trained for next-key prediction, which may fail to explicitly encode common patterns shared by all normal sequences; can mislabel sequences where bidirectional/contextual cues would be necessary.",
            "uuid": "e5742.2",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "LogAnomaly",
            "name_full": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "brief_description": "A deep learning approach for log anomaly detection designed to handle both sequential and quantitative anomalies in unstructured logs; used here as a baseline.",
            "citation_title": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "mention_or_use": "use",
            "model_name": "LogAnomaly",
            "model_description": "Deep-learning based anomaly detection that models sequence patterns and numeric/count-based anomalies in logs; exact architecture not detailed in this paper (referenced and used as baseline).",
            "model_size": null,
            "anomaly_detection_method": "Unsupervised deep modeling of log sequences to detect sequential (order) anomalies and quantitative (statistical) anomalies; used here as a baseline.",
            "data_type": "Parsed log-key sequences, with consideration of sequence-level and quantitative features.",
            "anomaly_type": "Sequential anomalies and quantitative anomalies in logs.",
            "dataset_name": "HDFS, BGL, Thunderbird (evaluated as baseline in this paper)",
            "performance_metrics": "Reported in Table 2: HDFS P=94.15, R=40.47, F1=56.19; BGL P=73.12, R=76.09, F1=74.08; Thunderbird P=86.72, R=99.63, F1=92.73.",
            "baseline_comparison": "Outperforms many traditional methods on some datasets but underperforms LogBERT in F1 on all three datasets in this paper; LogBERT achieved higher balanced precision/recall trade-offs.",
            "limitations_or_failure_cases": "In HDFS it attains high precision but low recall, indicating possible imbalance in sensitivity to anomalies or thresholds; details of architecture/limitations are not expanded in this paper (refer to original LogAnomaly paper).",
            "uuid": "e5742.3",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Deep SVDD (inspiration)",
            "name_full": "Deep One-Class Classification (Deep SVDD)",
            "brief_description": "A deep one-class classification method that learns representations by minimizing the volume of a hypersphere enclosing the normal data in latent space; cited as inspiration for LogBERT's Volume of Hypersphere Minimization (VHM) objective.",
            "citation_title": "Deep One-Class Classification",
            "mention_or_use": "mention",
            "model_name": "Deep SVDD (conceptual inspiration)",
            "model_description": "Learns a mapping of inputs into a representation space where the network is trained to minimize the distance of normal samples to a learned center (minimize hypersphere volume), enabling anomaly detection via distance to center.",
            "model_size": null,
            "anomaly_detection_method": "One-class training objective that minimizes squared distances of representations to a center (spherical objective); anomalies detected by large distances from center.",
            "data_type": "General - images and other vector data in original work; here the concept is applied to DIST token embeddings of log sequences.",
            "anomaly_type": "Outlier/anomaly detection as distance-based anomalies in embedding space (samples far from center considered anomalies).",
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": "Used as methodological inspiration; the paper reports that VHM alone (the analogous spherical objective) performs poorly for this log application, but combined with MLKP improves performance especially for short sequences.",
            "limitations_or_failure_cases": "Spherical objective alone was insufficient for good detection in log sequences (poor performance when used alone); distance-only measures may fail to capture nuanced sequential structure in logs.",
            "uuid": "e5742.4",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "Deep One-Class Classification",
            "rating": 2
        },
        {
            "paper_title": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "rating": 2
        },
        {
            "paper_title": "Drain: An Online Log Parsing Approach with Fixed Depth Tree",
            "rating": 1
        },
        {
            "paper_title": "Detecting large-scale system problems by mining console logs",
            "rating": 1
        }
    ],
    "cost": 0.011658,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LogBERT: Log Anomaly Detection via BERT</h1>
<p>Haixuan Guo ${ }^{1}$, Shuhan Yuan ${ }^{1}$, and Xintao $\mathrm{Wu}^{2}$<br>${ }^{1}$ Utah State University, Logan UT<br>ghaixan95@aggiemail.usu.edu, shuhan.yuan@usu.edu<br>${ }^{2}$ University of Arkansas, Fayetteville AR<br>xintaowu@uark.edu</p>
<h4>Abstract</h4>
<p>Detecting anomalous events in online computer systems is crucial to protect the systems from malicious attacks or malfunctions. System logs, which record detailed information of computational events, are widely used for system status analysis. In this paper, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). LogBERT learns the patterns of normal log sequences by two novel selfsupervised training tasks and is able to detect anomalies where the underlying patterns deviate from normal log sequences. The experimental results on three log datasets show that LogBERT outperforms state-of-the-art approaches for anomaly detection.</p>
<p>Keywords: anomaly detection $\cdot$ log sequences $\cdot$ BERT</p>
<h2>1 Introduction</h2>
<p>Online computer systems are vulnerable to various malicious attacks in cyberspace. Detecting anomalous events from online computer systems in a timely manner is the fundamental step to protect the systems. System logs, which record detailed information about computational events generated by computer systems, play an important role in anomaly detection nowadays.</p>
<p>Currently, many traditional machine learning models are proposed for identifying anomalous events from log messages. These approaches extract useful features from log messages and adopt machine learning algorithms to analyze the log data. Due to the data imbalance issue, it is infeasible to train a binary classifier to detect anomalous log sequences. As a result, many unsupervised learning models, such as Principal Component Analysis (PCA) [19], or one class classification models, such as one-class SVM [516], are widely-used to detect anomalies. However, traditional machine learning models, such as one-class SVM, are hard to capture the temporal information of discrete log messages.</p>
<p>Recently, deep learning models, especially recurrent neural networks (RNNs), are widely used for log anomaly detection since they are able to model the sequential data [2|317]. However, there are still some limitations of using RNN for modeling log data. First, although RNN can capture the sequential information by the recurrence formula, it cannot make each $\log$ in a sequence encoding the</p>
<p>context information from both the left and right context. However, it is crucial to observe the complete context information instead of only the information from previous steps when detecting malicious attacks based on log messages. Second, current RNN-based anomaly detection models are trained to capture the patterns of normal sequences by prediction the next log message given previous log messages. This training objective mainly focuses on capturing the correlation among the log messages in normal sequences. When such correlation in a log sequence is violated, the RNN model cannot correctly predict the next log message based on previous ones. Then, we will label the sequence as anomalous. However, only using the prediction of next log message as objective function cannot not explicitly encode the common patterns shared by all normal sequences.</p>
<p>To tackle the existing limitations of RNN-based models, in this work, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). Inspired by the great success of BERT in modeling sequential text data [1], we leverage BERT to capture patterns of normal log sequences. By using the structure of BERT, we expect the contextual embedding of each log entry can capture the information of whole log sequences. To achieve that, we propose two selfsupervised training tasks: 1) masked log key prediction, which aims to correctly predict log keys in normal log sequences that are randomly masked; 2) volume of hypersphere minimization, which aims to make the normal log sequences close to each other in the embedding space. After training, we expect LogBERT encodes the information about normal log sequences. We then derive a criterion to detect anomalous log sequences based on LogBERT. Experimental results on three log datasets show that LogBERT achieves the best performance on log anomaly detection by comparing with various state-of-the-art baselines.</p>
<h1>2 Related Work</h1>
<p>System logs are widely used by large online computer systems for troubleshooting, where each log message is usually a semi-structured text string. The traditional approaches explicitly use the keywords (e.g., "fail") or regular expressions to detect anomalous log entries. However, these approaches cannot detect malicious attacks based on a sequence of operations, where each log entry looks normal, but the whole sequence is anomalous. To tackle this challenge, many rule-based approaches are proposed to identify anomalous events [1120]. Although rule based approaches can achieve high accuracy, they can only identify pre-defined anomalous scenarios and require heavy manual engineering.</p>
<p>As malicious attacks become more complicated, learning-based approaches are proposed. The typical pipeline for these approaches consists of three steps [4]. First, a log parser is adopted to transform log messages to log keys. A feature extraction approach, such as TF-IDF, is then used to build a feature vector to represent a sequence of log keys in a sliding window. Finally, in most cases, an unsupervised approach is applied for detecting the anomalous sequences [189].</p>
<p>Recently, many deep learning-based log anomaly detection approaches are proposed for log anomaly detection [21|2|22|23|8|17]. Most of the existing approaches adopt recurrent neural networks, especially long-short term memory (LSTM) or gated recurrent unit (GRU) to model the normal log key sequences and derive anomalous scores to detect the anomalous log sequences [2|23|17]. In this work, we explore the advanced BERT model to capture the information of log sequences and propose two novel self-supervised tasks to train the model.</p>
<h1>3 LogBERT</h1>
<p>In this section, we introduce our framework, LogBERT, for log sequence anomaly detection. Inspired by BERT [1], LogBERT leverages the Transformer encoder to model log sequences and is trained by novel self-supervised tasks to capture the patterns of normal sequences. Figure 1 shows the whole framework of LogBERT.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: The overview of LogBERT</p>
<h3>3.1 Framework</h3>
<p>Given a sequence of unstructured log messages, we aim to detect whether this sequence is normal or anomalous. In order to represent log messages, following a typical pre-processing approach, we first extract log keys (string templates) from log messages via a log parser (shown in Figure 2). Then, we can define a log sequence as a sequence of ordered log keys $S=\left{k_{1}, \ldots, k_{t}, \ldots, k_{T}\right}$, where $k_{t} \in \mathcal{K}$ indicates the log key in the $t$-th position, and $\mathcal{K}$ indicates a set of log keys extracted from log messages. The goal of this task is to predict whether a new log sequence $S$ is anomalous based on a training dataset $\mathcal{D}=\left{S^{j}\right}_{j=1}^{N}$ that consists of only normal log sequences. To achieve that, LogBERT models the normal sequences and further derive an anomaly detection criterion to identify anomalous sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Log Messages</th>
<th style="text-align: center;">Log Keys</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ CE sym $\mathrm{s}^{\mathrm{s}} \mathrm{s}$, at $\mathrm{s}^{\mathrm{s}} \mathrm{s}$, mask $\mathrm{s}^{\mathrm{s}} \mathrm{s}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ ddr: activating redundant bit steering: rank $1 / 2$ by $\mathrm{rad} / \mathrm{s}^{\mathrm{s}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ rank $1 / 2$ by $\mathrm{rad} / \mathrm{s}^{\mathrm{s}}$</td>
</tr>
</tbody>
</table>
<p>Fig. 2: Log messages in the BGL dataset and the corresponding log keys extracted by a log parser. The message with red underscore indicates the detailed computational event.</p>
<p>Input Representation. Given a normal log sequence $S^{j}$, we first add a special token, DIST, at the beginning of $S^{j}=\left{k_{1}^{j}, \ldots, k_{t}^{j}, \ldots, k_{T}^{j}\right}$ as the first log key, which is used to represent the whole log sequence based on the structure of Transformer encoder. LogBERT then represents each log key $k_{t}^{j}$ as an input representation $\mathbf{x}<em t="t">{t}^{j}$, where the representation $\mathbf{x}</em>}^{j}$ is a summation of a log key embedding and a position embedding. In this work, we randomly generate a matrix $\mathbf{E} \in \mathbb{R}^{|\mathcal{K}| * d}$ as the log key embedding matrix, where $d$ is the dimension of log key embedding, while the position embeddings $\mathbf{T} \in \mathbb{R}^{T * d}$ are generated by using a sinusoid function to encode the position information of log keys in a sequence [1]. Finally, the input representation of the log key $k_{t}$ is defined as: $\mathbf{x<em k__t="k_{t">{t}^{j}=\mathbf{e}</em>}^{j}}+\mathbf{t<em t="t">{k</em>$.
Transformer Encoder. LogBERT adopts Transformer encoder to learn the contextual relations among log keys in a sequence. Transformer encoder consists of multiple transformer layers. Each transformer layer includes a multi-head self-attention and a position-wise feed forward sub-layer in which a residual connection is employed around each of two sub-layers, followed by layer normalization [15]. The multi-head attention employs $H$ parallel self-attentions to jointly capture different aspect information at different positions over the input log sequence. Formally, for the $l$-th head of the attention layer, the scaled dot-product self-attention is defined as:}^{j}</p>
<p>$$
\text { head }<em l="l">{l}=\operatorname{Attention}\left(\mathbf{X}^{j} \mathbf{W}</em>}^{Q}, \mathbf{X}^{j} \mathbf{W<em l="l">{l}^{K}, \mathbf{X}^{j} \mathbf{W}</em>\right)
$$}^{V</p>
<p>where $\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{T}}{\sqrt{d_{v}}}\right) \mathbf{V} ; \mathbf{X}^{j} \in \mathbb{R}^{T * d}$ is the input representation of the log sequence; $\mathbf{W}<em l="l">{l}^{Q}, \mathbf{W}</em>}^{K}$ and $\mathbf{W<em v="v">{l}^{V}$ are linear projection weights with dimensions $\mathbb{R}^{d * d</em>$ is the dimension for one head ot the attention layer. Each self-attention makes each key attend to all the log keys in an input sequence and computes the hidden representation for each log key with an attention distribution over the sequence.}}$ for the $l$-th head, and $d_{v</p>
<p>The multi-head attention employs a parallel of self-attentions to jointly capture different aspect information at different log keys. Formally, the multi-head attention concatenates $H$ parallel heads together as:</p>
<p>$$
f\left(\mathbf{X}^{j}\right)=\operatorname{Concat}\left(\text { head }<em H="H">{1}, \ldots, \text { head }</em>
$$}\right) \mathbf{W}^{O</p>
<p>where $\mathbf{W}^{O} \in \mathbb{R}^{h d_{v} * d_{o}}$ is a projection matrix, and $d_{o}$ is the dimension for the output of multi-head attention sub-layer.</p>
<p>Then, the position-wise feed forward sub-layer with a ReLU activation is applied to the hidden representation of each activity separately. Finally, by combining the position-wise feed forward sub-layer and multi-head attention, a transformer layer is defined as:</p>
<p>$$
\text { transformer_layer }\left(\mathbf{X}^{j}\right)=F F N\left(f\left(\mathbf{X}^{j}\right)\right)=\operatorname{ReLU}\left(f\left(\mathbf{X}^{j}\right) \mathbf{W}<em 2="2">{1}\right) \mathbf{W}</em>
$$</p>
<p>where $\mathbf{W}<em 2="2">{1}$ and $\mathbf{W}</em>$ are trained projection matrices.
The Transformer encoder usually consists of multiple transformer layers. We denote $\mathbf{h}<em t="t">{t}^{j}$ as the contextual embedding vector of the log key $k</em>}^{j}$ produced by the Transformer encoder, i.e., $\mathbf{h<em t="t">{t}^{j}=\operatorname{Transformer}\left(x</em>\right)$.}^{j</p>
<h1>3.2 Objective Function</h1>
<p>In order to train the LogBERT model, we propose two self-supervised training tasks to capture the patterns of normal log sequences.
Task I: Masked Log Key Prediction (MLKP). In order to capture the bidirectional context information of log sequences, we train LogBERT to predict the masked log keys in log sequences. In our scenario, LogBERT takes log sequences with random masks as inputs, where we randomly replace a ratio of log keys in a sequence with a specific MASK token. The training objective is to accurately predict the randomly masked log keys. The purpose is to make LogBERT encode the prior knowledge of normal log sequences.</p>
<p>To achieve that, we feed the contextual embedding vector of the $i$-th MASK token in the $j$-th log sequence $\mathbf{h}<em i="i">{\left[\operatorname{MASK}</em>$ :}\right]}^{j}$ to a softmax function, which will output a probability distribution over the entire set of log keys $\mathcal{K</p>
<p>$$
\hat{\mathbf{y}}<em i="i">{\left[\operatorname{MASK}</em>}\right]}^{j}=\operatorname{Softmax}\left(\mathbf{W<em _left_operatorname_MASK="\left[\operatorname{MASK">{C} \mathbf{h}</em><em C="C">{i}\right]}^{j}+\mathbf{b}</em>\right)
$$</p>
<p>where $\mathbf{W}<em C="C">{C}$ and $\mathbf{b}</em>$ are trainable parameters. Then, we adopt the cross entropy loss as the objective function for masked log key prediction, which is defined as:</p>
<p>$$
\mathcal{L}<em j="1">{M L K P}=-\frac{1}{N} \sum</em>}^{N} \sum_{i=1}^{M} \mathbf{y<em i="i">{\left[\operatorname{MASK}</em>}\right]}^{j} \log \hat{\mathbf{y}<em i="i">{\left[\operatorname{MASK}</em>
$$}\right]}^{j</p>
<p>where $\mathbf{y}<em _left_right.="\left[\right.">{\left[\right.}^{j}{ }</em>$ indicates the real log key for the $i$-th masked token, and $M$ is the total number of masked tokens in the $j$-th log sequence. Since the patterns of normal and anomalous log sequences are different, we expect once LogBERT is able to correctly predict the masked log keys, it can distinguish the normal and anomalous log sequences.
Task II: Volume of Hypersphere Minimization (VHM). Inspired by the Deep SVDD approach [13], where the objective is to minimize the volume of a data-enclosing hypersphere, we propose a spherical objective function to regulate the distribution of normal log sequences. The motivation is that normal log sequences should be concentrated and close to each other in the embedding space, while the anomalous log sequences are far to the center of the sphere. We</p>
<p>first derive the representations of normal log sequences and then compute the center representation based on the mean operation. In particular, we consider the contextual embedding vector of the DIST token $\mathbf{h}<em _mathrm_DIST="\mathrm{DIST">{\text {DIST }}^{j}$, which encodes the information of entire log sequence based on the Transformer encoder, as the representation of a log sequence in the embedding space. To make the representations of normal log sequences close to each other, we further derive the center representation of normal log sequences $\mathbf{c}$ in the training set by a mean operation, i.e., $\mathbf{c}=\operatorname{Mean}\left(\mathbf{h}</em>$ close to the center representation c:}}^{j}\right)$. Then, the objective function is to make the representation of normal log sequence $\mathbf{h}_{\text {DIST }}^{j</p>
<p>$$
\mathcal{L}<em j="1">{V H M}=\frac{1}{N} \sum</em>
$$}^{N}\left|\mathbf{h}_{\mathrm{DIST}}^{j}-\mathbf{c}\right|^{2</p>
<p>By minimizing the Equation 6, we expect all the normal log sequences in the training set are close to the center, while the anomalous log sequences have a larger distance to the center. Meanwhile, another advantage of the spherical objective function is that by making the sequence representations close to the center, the Transformer encoder can also leverage the information from other log sequences via the center representation $\mathbf{c}$, since $\mathbf{c}$ encodes all the information of normal log sequences. As a result, the model should be able to predict the masked log keys with higher accuracy for normal log sequences because the normal log sequences should share similar patterns.</p>
<p>Finally, the objective function for training the LogBERT is defined as below:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em H="H" M="M" V="V">{M L K P}+\alpha \mathcal{L}</em>
$$</p>
<p>where $\alpha$ is a hyper-parameter to balance two training tasks.</p>
<h1>3.3 Anomaly Detection</h1>
<p>After training, we can deploy LogBERT for anomalous log sequence detection. The idea of applying LogBERT for log anomaly detection is that since LogBERT is trained on normal log sequences, it can achieve high prediction accuracy on predicting the masked log keys if a testing log sequence is normal. Hence, we can derive the anomalous score of a log sequence based on the prediction results on the MASK tokens. To this end, given a testing log sequence, similar to the training process, we first randomly replace a ratio log keys with MASK tokens and use the randomly-masked log sequence as an input to LogBERT. Then, given a MASK token, the probability distribution calculated based on Equation 4 indicates the likelihood of a log key appeared in the position of the MASK token. Similar to the strategy in DeepLog [2], we build a candidate set consisting of $g$ normal log keys with the top $g$ highest likelihoods computed by $\hat{\mathbf{y}}<em i="i">{\left[\right.$ MASK $\left.</em>$. If the real log key is in the candidate set, we treat the key as normal. In other words, if the observed log key is not in the top- $g$ candidate set predicted by LogBERT, we consider the log key as an anomalous log key. Then, when a log sequence consists of more than $r$ anomalous log keys, we will label this log sequence as}\right]</p>
<p>anomalous. Both $g$ and $r$ are hyper-parameters and will be tuned based on a validation set.</p>
<h1>4 Experiments</h1>
<h3>4.1 Experimental Setup</h3>
<p>Datasets. We evaluate the proposed LogBERT on three log datasets, HDFS, BGL, and Thunderbird. Table 1 shows the statistics of the datasets. For all datasets, we adopt around 5000 normal log sequences for training. The number in the brackets under the column "# Log Keys" indicates the number of unique log keys in the training dataset.</p>
<p>Table 1: Statistics of evaluation datasets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Log Messages</th>
<th style="text-align: center;"># Anomalies</th>
<th style="text-align: center;"># Log Keys</th>
<th style="text-align: center;"># of Log Sequences in Test Dataset</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">Anomalous</td>
</tr>
<tr>
<td style="text-align: center;">HDFS</td>
<td style="text-align: center;">$11,172,157$</td>
<td style="text-align: center;">284,818</td>
<td style="text-align: center;">46 (15)</td>
<td style="text-align: center;">553,366</td>
<td style="text-align: center;">10,647</td>
</tr>
<tr>
<td style="text-align: center;">BGL</td>
<td style="text-align: center;">$4,747,963$</td>
<td style="text-align: center;">348,460</td>
<td style="text-align: center;">334 (175)</td>
<td style="text-align: center;">10,045</td>
<td style="text-align: center;">2,630</td>
</tr>
<tr>
<td style="text-align: center;">Thunderbird-mimi</td>
<td style="text-align: center;">20,000,000</td>
<td style="text-align: center;">758,562</td>
<td style="text-align: center;">1,165 (866)</td>
<td style="text-align: center;">71,155</td>
<td style="text-align: center;">45,385</td>
</tr>
</tbody>
</table>
<ul>
<li>Hadoop Distributed File System (HDFS) [18]. HDFS dataset is generated by running Hadoop-based map-reduce jobs on Amazon EC2 nodes and manually labeled through handcrafted rules to identify anomalies. HDFS dataset consists of $11,172,157 \log$ messages, of which 284,818 are anomalous. For HDFS, we group log keys into log sequences based on the session ID in each log message. The average length of log sequences is 19 .</li>
<li>BlueGene/L Supercomputer System (BGL) [10]. BGL dataset is collected from a BlueGene/L supercomputer system at Lawrence Livermore National Labs (LLNL). Logs contain alert and non-alert messages identified by alert category tags. The alert messages are considered as anomalous. BGL dataset consists of $4,747,963 \log$ messages, of which 348,460 are anomalous. For BGL, we define a time sliding window as 5 minutes to generate log sequences, where the average length is 562 .</li>
<li>Thunderbird [10]. Thunderbird dataset is another large log dataset collected from a supercomputer system. We select the first $20,000,000 \log$ messages from the original Thunderbird dataset to compose our dataset, of which 758,562 are anomalous. For Thunderbird, we also adopt a time sliding window as 1 minute to generate log sequences, where the average length is 326 .</li>
</ul>
<p>Baselines. We compare our LogBERT model with the following baselines.</p>
<ul>
<li>
<p>Principal Component Analysis (PCA) [19]. PCA builds counting matrix based on the frequency of log keys sequences and then reduces the original counting matrix into a low dimensional space to detect anomalous sequences.</p>
</li>
<li>
<p>One-Class SVM (OCSVM) [14]. One-Class SVM is a well-known one-class classification model and widely used for log anomaly detection [5,16] by only observing the normal data.</p>
</li>
<li>IsolationForest (iForest) [7]. Isolation forest is an unsupervised learning algorithm for anomaly detection by representing features as tree structures.</li>
<li>LogCluster [6]. LogCluster is a clustering based approach, where the anomalous log sequences are detected by comparing with the existing clusters.</li>
<li>DeepLog [2]. DeepLog is a state-of-the-art log anomaly detection approach. DeepLog adopts recurrent neural network to capture patterns of normal log sequences and further identifies the anomalous log sequences based on the performance of log key predictions.</li>
<li>LogAnomaly [23]. Log Anomaly is a deep learning-based anomaly detection approach and able to detect sequential and quantitative log anomalies.</li>
</ul>
<p>Implementation Details. We adopt Drain [3] to parse the log messages into log keys. Regarding baselines, we leverage the package Loglizer [4] to evaluate PCA, OCSVM, iForest as well as LogCluster for anomaly detection and adopt the open source deep learning-based log analysis toolkit to evaluate DeepLog and LogAnomaly ${ }^{3}$. For LogBERT, we construct a Transformer encoder by using two Transformer layers. The dimensions for the input representation and hidden vectors are 50 and 256, respectively. The hyper-parameters, including $\alpha$ in Equation 7, $m$ the ratio of masked log keys for the MKLP task, $r$ the number of predicted anomalous log keys, and $g$ the size of top- $g$ candidate set for anomaly detection are tuned based on a small validation set. In our experiments, both training and detection phases have the same ratio of masked log keys $m$. The code of LogBERT is available online ${ }^{4}$.</p>
<h1>4.2 Experimental Results</h1>
<p>Table 2: Experimental Results on HDFS, BGL, and Thunderbird Datasets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">HDFS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BGL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thunderbird</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
</tr>
<tr>
<td style="text-align: center;">PCA</td>
<td style="text-align: center;">5.89</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">98.23</td>
<td style="text-align: center;">16.61</td>
<td style="text-align: center;">37.35</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">54.39</td>
</tr>
<tr>
<td style="text-align: center;">iForest</td>
<td style="text-align: center;">53.60</td>
<td style="text-align: center;">69.41</td>
<td style="text-align: center;">60.49</td>
<td style="text-align: center;">99.70</td>
<td style="text-align: center;">18.11</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;">34.45</td>
<td style="text-align: center;">1.68</td>
<td style="text-align: center;">3.20</td>
</tr>
<tr>
<td style="text-align: center;">OCSVM</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">12.24</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">18.89</td>
<td style="text-align: center;">39.11</td>
<td style="text-align: center;">25.48</td>
</tr>
<tr>
<td style="text-align: center;">LogCluster</td>
<td style="text-align: center;">99.26</td>
<td style="text-align: center;">37.08</td>
<td style="text-align: center;">53.99</td>
<td style="text-align: center;">95.46</td>
<td style="text-align: center;">64.01</td>
<td style="text-align: center;">76.63</td>
<td style="text-align: center;">98.28</td>
<td style="text-align: center;">42.78</td>
<td style="text-align: center;">59.61</td>
</tr>
<tr>
<td style="text-align: center;">DeepLog</td>
<td style="text-align: center;">88.44</td>
<td style="text-align: center;">69.49</td>
<td style="text-align: center;">77.34</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">82.78</td>
<td style="text-align: center;">86.12</td>
<td style="text-align: center;">87.34</td>
<td style="text-align: center;">99.61</td>
<td style="text-align: center;">93.08</td>
</tr>
<tr>
<td style="text-align: center;">LogAnomaly</td>
<td style="text-align: center;">94.15</td>
<td style="text-align: center;">40.47</td>
<td style="text-align: center;">56.19</td>
<td style="text-align: center;">73.12</td>
<td style="text-align: center;">76.09</td>
<td style="text-align: center;">74.08</td>
<td style="text-align: center;">86.72</td>
<td style="text-align: center;">99.63</td>
<td style="text-align: center;">92.73</td>
</tr>
<tr>
<td style="text-align: center;">LogBERT</td>
<td style="text-align: center;">87.02</td>
<td style="text-align: center;">78.10</td>
<td style="text-align: center;">82.32</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">92.32</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">96.75</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">96.64</td>
</tr>
</tbody>
</table>
<p>Performance on Log Anomaly Detection. Table 2 shows the results of LogBERT as well as baselines on three datasets. We can notice that PCA, Isolation Forest, and OCSVM have poor performance on log anomaly detection.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Although these methods could achieve extremely high precision or recall values, they cannot balance the performance on both precision and recall, which lead to extremely low F1 scores. This could be because using the counting vector to represent a log sequence leads to the loss of temporal information from sequences. LogCluster, which is designed for log anomaly detection, achieves better performance than the PCA, Isolation Forest, and OCSVM. Meanwhile, two deep learning-based baselines, DeepLog and LogAnomaly, significantly outperform the traditional approaches and achieve reasonable F1 scores on three datasets, which show the advantage to adopt deep learning models to capture the patterns of log sequences. Moreover, our proposed LogBERT achieves the highest F1 scores on three datasets with large margins by comparing with all baselines. It indicates that by using self-supervised training tasks, LogBERT can successfully model the normal log sequences and further identify anomalous sequences with high accuracy.</p>
<p>Table 3: Performance of LogBERT base on One Self-supervised Training Task</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">HDFS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BGL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thunderbird</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
</tr>
<tr>
<td style="text-align: left;">MLKP</td>
<td style="text-align: center;">77.54</td>
<td style="text-align: center;">78.65</td>
<td style="text-align: center;">78.09</td>
<td style="text-align: center;">93.16</td>
<td style="text-align: center;">86.46</td>
<td style="text-align: center;">89.69</td>
<td style="text-align: center;">97.07</td>
<td style="text-align: center;">95.90</td>
<td style="text-align: center;">96.48</td>
</tr>
<tr>
<td style="text-align: left;">VHM</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">39.17</td>
<td style="text-align: center;">4.58</td>
<td style="text-align: center;">71.04</td>
<td style="text-align: center;">43.84</td>
<td style="text-align: center;">54.22</td>
<td style="text-align: center;">56.58</td>
<td style="text-align: center;">43.87</td>
<td style="text-align: center;">49.42</td>
</tr>
<tr>
<td style="text-align: left;">Both</td>
<td style="text-align: center;">87.02</td>
<td style="text-align: center;">78.10</td>
<td style="text-align: center;">82.32</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">92.32</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">96.75</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">96.64</td>
</tr>
</tbody>
</table>
<p>Ablation Studies. In order to further understand our proposed LogBERT, we conduct ablation experiments on three log datasets. LogBERT is trained by two self-supervised tasks. We evaluate the performance of LogBERT by only using one training task each time. When the model is only trained by minimizing the volume of hypersphere, we identify anomalous log sequences by computing distances of the log sequence representations to the center of normal log sequences $\mathbf{c}$. If the distance is larger than a threshold, we consider a log sequence is anomalous. Table 3 shows the experimental results. We can notice that when only using the task of masked log key prediction to train the model, we can still get very good performance on log anomaly detection, which shows the effectiveness of training the model by predicting masked log keys. We can also notice that even we do not train the LogBERT with the task of the volume of hypersphere minimization, LogBERT achieves higher F1 scores than DeepLog on all three datasets, which shows that compared with LSTM, Transformer encoder is better at capturing the patterns of log sequences. Meanwhile, we can observe that when only training the model for minimizing the volume of hypersphere, the performance is poor. It indicates that only using distance as a measure to identify anomalous log sequences cannot achieve good performance. However, combining two self-supervised tasks to train LogBERT can achieve better performance than the models only trained by one task. Especially, for the HDFS dataset, LogBERT trained by two self-supervised tasks gains a large margin in terms of F1 score (82.32) compared with the model only trained by MLKP (78.09). For BGL and</p>
<p>Thunderbird, the improvement of LogBERT is not as significant as the model in HDFS. This could be because the average length of log sequences in BGL (562) and Thunderbird (326) datasets are much larger than the log sequences in HDFS (19). For longer sequences, only predicting the masked log keys can capture the most important patterns of log sequences since there are many more mask tokens in longer sequences. On the other hand, for short log sequences, we cannot have many masks tokens. As a result, the task of the volume of hypersphere minimization can help to boost the performance. Hence, based on Table 3, we can conclude that using two self-supervised tasks to train LogBERT can achieve better performance, especially when the log sequences are relatively short.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: Visualization of log sequences by using the contextual embedding of DIST tokens $\mathbf{h}_{\text {DIST }}$. The blue dots indicate the normal log sequences, while the orange ' $x$ ' symbols indicate anomalous log sequences.</p>
<p>Visualization. In order to visualize the log sequences, we adopt locally linear embedding (LLE) algorithm [12] to map the log sequence representations into a two dimensional space, where the contextual embedding of DIST token $\mathbf{h}_{\text {DIST }}$ is used as the representation of a log sequence. We randomly select 1000 normal and 1000 anomalous sequences from the HDFS dataset for visualization. Figure 3 shows the visualization results of log sequences trained by LogBERT with and without the VHM task. We can notice that the normal and anomalous log sequences are mixed together when we trained the model without the VHM task (shown in Figure 3a). On the contrary, as shown in Figure 3b, by incorporating the VHM task, the normal and anomalous log sequences are clearly separated in the latent space, and the normal log sequences group together. Therefore, the visualization presents that the VHM task is effective in regulating the model to split the normal and abnormal data in latent space.
Parameter analysis. We analyze the sensitivity of model performance by tuning various hyper-parameters. Figure 4a shows that the model performance is relatively stable by setting different $\alpha$ values in Equation 7. This is because, for the BGL dataset, the loss from the masked log key prediction dominates the final loss value due to the long log sequences. As a result, the weight for the VHM</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Parameter analysis on the BGL dataset.
task does not have much influence on the performance. Figure 4b shows the performance with different ratios of masked log keys. Note that we use the same ratio in both training and detection phases. We can notice that increasing the ratios of masked log keys in the sequences from 0.1 to 0.5 can slightly increase the F1 scores while keeping increasing the ratios makes the performance worse. This is because while the masked log keys increase in a reasonable range, the model can capture more information about the sequence. However, if a sequence contains too many masked log keys, it loses too much information for making the predictions. Figure 4c shows that when increasing the size of the candidate set as normal log keys, the precision for anomaly detection keeps increasing while the recall is reducing, which meets our expectation. Hence, we need to find the appropriate size of the candidate set to balance the precision and recall for the anomaly detection.</p>
<h1>5 Conclusion</h1>
<p>Log anomaly detection is essential to protect online computer systems from malicious attacks or malfunctions. In this paper, we have developed LogBERT, a novel log anomaly detection model based on BERT. In order to train LogBERT only based on normal log sequences, we have proposed two self-supervised training tasks. One is to predict the masked log keys in log sequences, while the other is to make the normal log sequences close to each other in the embedding space. After training over normal log sequences, LogBERT is able to detect anomalous log sequences. Experimental results on three log datasets have shown that LogBERT outperforms the state-of-the-art approaches for log anomaly detection.</p>
<h2>References</h2>
<ol>
<li>Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] (2018)</li>
<li>
<p>Du, M., Li, F., Zheng, G., Srikumar, V.: DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. (2017).</p>
</li>
<li>
<p>He, P., Zhu, J., Zheng, Z., Lyu, M.R.: Drain: An Online Log Parsing Approach with Fixed Depth Tree. In: 2017 IEEE International Conference on Web Services (ICWS). (2017).</p>
</li>
<li>He, S., Zhu, J., He, P., Lyu, M.R.: Experience Report: System Log Analysis for Anomaly Detection. In: 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE). (2016).</li>
<li>Li, K.L., Huang, H.K., Tian, S.F., Xu, W.: Improving one-class SVM for anomaly detection. In: Proceedings of the 2003 International Conference on Machine Learning and Cybernetics. (2003).</li>
<li>Lin, Q., Zhang, H., Lou, J., Zhang, Y., Chen, X.: Log Clustering Based Problem Identification for Online Service Systems. In: 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (2016)</li>
<li>Liu, F.T., Ting, K.M., Zhou, Z.: Isolation Forest. In: 2008 Eighth IEEE International Conference on Data Mining. pp. 413-422 (2008).</li>
<li>Liu, F., Wen, Y., Zhang, D., Jiang, X., Xing, X., Meng, D.: Log2vec: A Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats within Enterprise. In: Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. (2019).</li>
<li>Lou, J.G., Fu, Q., Yang, S., Xu, Y., Li, J.: Mining invariants from console logs for system problem detection. In: Proceedings of the 2010 USENIX Conference on USENIX Annual Technical Conference. (2010)</li>
<li>Oliner, A., Stearley, J.: What supercomputers say: A study of five system logs. In: 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07). pp. 575-584. IEEE (2007)</li>
<li>Pecchia, A., Cinque, M., Cotroneo, D.: Event logs for the analysis of software failures: A rule-based approach. IEEE Transactions on Software Engineering 39(06), $806-821(2013)$.</li>
<li>Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by locally linear embedding. science 290(5500), 2323-2326 (2000)</li>
<li>Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S.A., Binder, A., Müller, E., Kloft, M.: Deep One-Class Classification. In: International Conference on Machine Learning. pp. 4393-4402. PMLR (2018)</li>
<li>Schölkopf, B., Platt, J.C., Shawe-Taylor, J., Smola, A.J., Williamson, R.C.: Estimating the Support of a High-Dimensional Distribution. Neural Computation 13(7), 1443-1471 (2001).</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention Is All You Need. arXiv:1706.03762 [cs] (2017)</li>
<li>Wang, Y., Wong, J., Miner, A.: Anomaly intrusion detection using one class SVM. In: Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop, 2004. pp. 358-364 (2004).</li>
<li>Wang, Z., Chen, Z., Ni, J., Liu, H., Chen, H., Tang, J.: Multi-Scale One-Class Recurrent Neural Networks for Discrete Event Sequence Anomaly Detection. In: WSDM (2021)</li>
<li>Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.: Online system problem detection by mining patterns of console logs. In: 2009 Ninth IEEE International Conference on Data Mining. pp. 588-597. IEEE (2009)</li>
<li>
<p>Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.I.: Detecting large-scale system problems by mining console logs. In: Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles. (2009).</p>
</li>
<li>
<p>Yen, T.F., Oprea, A., Onarlioglu, K., Leetham, T., Robertson, W., Juels, A., Kirda, E.: Beehive: Large-scale log analysis for detecting suspicious activity in enterprise networks. In: Proceedings of the 29th Annual Computer Security Applications Conference. pp. 199-208 (2013)</p>
</li>
<li>Zhang, K., Xu, J., Min, M.R., Jiang, G., Pelechrinis, K., Zhang, H.: Automated IT system failure prediction: A deep learning approach. In: 2016 IEEE International Conference on Big Data (Big Data). pp. 1291-1300 (2016).</li>
<li>Zhang, X., Xu, Y., Lin, Q., Qiao, B., Zhang, H., Dang, Y., Xie, C., Yang, X., Cheng, Q., Li, Z., Chen, J., He, X., Yao, R., Lou, J.G., Chintalapati, M., Shen, F., Zhang, D.: Robust log-based anomaly detection on unstable log data. In: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. (2019).</li>
<li>Zhou, R., Sun, P., Tao, S., Zhang, R., Meng, W., Liu, Y., Zhu, Y., Liu, Y., Pei, D., Zhang, S., Chen, Y.: LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs. In: IJCAI. pp. 4739-4745 (2019)</li>
<li>He, S., Zhu, J., He, P., Lyu, M.R.: Loghub: A Large Collection of System Log Datasets towards Automated Log Analytics. arXiv:2008.06448 [cs] (Aug 2020)</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/donglee-afar/logdeep
${ }^{4}$ https://github.com/HelenGuohx/logbert&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
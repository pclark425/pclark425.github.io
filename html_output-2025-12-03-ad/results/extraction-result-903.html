<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-903 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-903</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-903</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-793a791fa20fe3552c1ebad0ec90c0336ada1183</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/793a791fa20fe3552c1ebad0ec90c0336ada1183" target="_blank">StreamBench: Towards Benchmarking Continuous Improvement of Language Agents</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work introduces StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence, and proposes several simple yet effective baselines for improving LLMs on StreamBench, and provides a comprehensive analysis to identify critical components that contribute to successful streaming strategies.</p>
                <p><strong>Paper Abstract:</strong> Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios. Source code: https://github.com/stream-bench/stream-bench. Benchmark website: https://stream-bench.github.io.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e903.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e903.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-StreamICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Stream In-Context Learning (Self-StreamICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A streaming baseline introduced in this paper that saves only correct self-generated (input, output) pairs into external memory and uses them as in-context demonstrations for future queries, enabling continuous online improvement without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Self-StreamICL (method applied with gpt-3.5 / gemini / claude endpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM agent pipeline that maintains an external vector-store memory of past self-generated (x_t, ŷ_t) pairs but only inserts those with correctness feedback fb_t=1; at each time step it retrieves k examples and uses them as in-context demonstrations (ICL) in the prompt. No network parameter updates are performed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA (distractor)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>54.80% exact match (averaged across three LLM endpoints; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench; DS-1000; Text-to-SQL (Spider/CoSQL/BIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; code generation (programming); text-to-SQL (structured query generation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ToolBench: 71.33% accuracy; DS-1000: 41.30% pass@1; Spider (Text-to-SQL): 74.63% execution accuracy (all averaged across three LLM endpoints; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>external memory (vector DB), retriever for ICL, prompting template; no parameter fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning using accumulated self-generated examples; no supervised fine-tuning or RL</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy + memory-based streaming</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Collects only correct self-generated (x, ŷ) pairs (fb_t=1) into a shared memory and uses them as in-context demonstrations (k retrieved examples) for future queries; avoids verbalizing correctness flags and excludes incorrect examples from memory.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Averaged HotpotQA EM improved from 48.49% (zero-shot) to 54.80%; ToolBench improved from 61.38% to 71.33%; Spider improved from 67.89% to 74.63% (Table 2, averaged across three LLM endpoints).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper argues that incorrect self-generated examples are distracting and can degrade performance when included in ICL; collecting only correct outputs gives useful demonstrations and reduces noise, helping both QA and interactive tasks—but improvements vary by task indicating differences in difficulty and signal utility across task types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StreamBench: Towards Benchmarking Continuous Improvement of Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e903.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e903.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAM-StreamICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agentic-Memory Stream In-Context Learning (MAM-StreamICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent streaming approach (introduced in this paper) where multiple distinct LLM agents take turns answering and share a common memory of correct self-generated examples, enabling cross-agent experiential transfer while maintaining per-step cost similar to a single agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>MAM-StreamICL (multi-agent shared-memory method using multiple LLM endpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Round-robin multi-agent framework: K different LLM agents (different endpoints) take turns to answer each incoming instance; when an agent's output is correct (fb_t=1) it is appended to a shared external memory; future retrievals draw from the combined experiences of all agents for ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA (distractor)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>55.20% exact match (averaged across three LLM endpoints; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench; DS-1000; Text-to-SQL (Spider/CoSQL/BIRD); DDXPlus (medical diagnosis)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; programming; text-to-SQL; diagnostic classification (sequential decision-style accumulation of cases)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ToolBench: 75.87% accuracy; DS-1000: 43.10% pass@1; Spider: 75.69% execution accuracy; DDXPlus: 83.50% accuracy (all averaged; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>external shared memory, retriever, multi-agent scheduling (round-robin), prompting/ICL</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning with multi-agent shared experience; no network parameter updates</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural (multi-agent shared-memory) + prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Introduces a shared external memory into which correct outputs from multiple distinct agents are stored; agents are scheduled in round-robin so only one agent answers per timestep (cost ~ average of single agent) while benefiting from the diverse correct examples produced by other agents.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Consistent improvements across tasks relative to zero-shot and single-agent streaming: e.g., HotpotQA EM 48.49% → 55.20%; ToolBench 61.38% → 75.87%; DDXPlus 52.85% → 83.50% (Table 2 averaged). Shared memory yielded particularly strong gains on domain tasks (medical) where agents had complementary strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper hypothesizes that agents have complementary strengths and that sharing correct experiences across agents supplies diverse, high-quality demonstrations that improve interactive/procedural performance; cost-effective sharing avoids linear cost scaling of multiple agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StreamBench: Towards Benchmarking Continuous Improvement of Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e903.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e903.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemPrompt (memory-assisted prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed memory-augmented prompting method that stores past (x, ŷ, feedback) triples and retrieves examples at inference time to condition the LLM, enabling improvement via memory retrieval rather than parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memory-assisted prompt editing to improve gpt-3 after deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>MemPrompt (implemented with external memory and retriever in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>External memory stores all past (x_t, ŷ_t, fb_t) triples; a retriever selects k elements from memory and they are verbalized (including correctness flag) and appended to the prompt to influence the model's next output.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA (distractor)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>52.62% exact match (averaged across three LLM endpoints; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench; DS-1000; Text-to-SQL (Spider/CoSQL/BIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; programming; text-to-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ToolBench: 64.31% accuracy; DS-1000: 35.47% pass@1; Spider: 70.78% execution accuracy (Table 2 averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>external memory, retriever, verbalized correctness flags included in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting with retrieved memory examples (no parameter updates)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural (memory) + prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Store all past interactions including correctness flags in a memory and retrieve k items to include (verbalized) in the prompt at inference time to steer future outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Small improvements over zero-shot on many tasks (e.g., HotpotQA 48.49% → 52.62%; Spider 67.89% → 70.78%), but less effective than Self-StreamICL and MAM-StreamICL. Ablations in paper show using only correct retrieved examples yields more consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Including incorrect past outputs (even when verbalized as incorrect) can distract the LLM and hinder learning; the paper suggests that correct examples are much more valuable for improving interactive/procedural performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StreamBench: Towards Benchmarking Continuous Improvement of Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e903.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e903.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GrowPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GrowPrompt (sliding-window prompt memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A streaming prompting baseline that stores recent (x, ŷ, fb) tuples in a sliding window and incorporates them into the prompt as recent examples for in-context conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GrowPrompt (sliding-window streaming prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stores recent k time-step tuples in a sliding window W; at inference, the contents of W (including verbalized correctness flags) are appended to the prompt as demonstrations to influence the model's next prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA (distractor)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>51.38% exact match (averaged across three LLM endpoints; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench; DS-1000; Text-to-SQL (Spider/CoSQL/BIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; programming; text-to-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ToolBench: 65.07% accuracy; DS-1000: 33.77% pass@1; Spider: 69.90% execution accuracy (Table 2 averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>sliding-window memory (textual), prompting with verbalized correctness flags</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context examples from recent history</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (streaming examples via sliding window)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Maintain a sliding window of the most recent k (x, ŷ, fb) and include them as in-context examples (fb verbalized) at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Modest gains over zero-shot on some tasks (e.g., HotpotQA 48.49% → 51.38%; Spider 67.89% → 69.90%), but less effective than policies that only store correct examples or that share memory across agents.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Presence of incorrect examples in the sliding-window can introduce noise; verbalizing incorrectness does not fully mitigate negative impact of incorrect demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StreamBench: Towards Benchmarking Continuous Improvement of Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e903.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e903.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that encourages the LLM to generate intermediate reasoning steps (a rationale) before producing the final answer; evaluated here as a non-streaming baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting (applied to evaluated LLM endpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting with a trigger phrase to elicit step-by-step reasoning (rationale) prior to the final answer; applied zero-shot without memory or streaming.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA (distractor)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>52.47% exact match (averaged; Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench; DS-1000; Text-to-SQL; others</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning; code generation; text-to-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ToolBench: 58.98% accuracy; DS-1000: 25.93% pass@1; Spider: 61.53% execution accuracy (Table 2 averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting (CoT trigger phrase), no parameter updates</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (reasoning elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Append a CoT trigger (e.g., 'Let's think step by step') to the prompt to elicit chain-of-thought style reasoning before producing an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>CoT produced mixed results: improved or similar on some QA tasks (HotpotQA 48.49% → 52.47%) but significantly degraded performance on some procedural tasks like DS-1000 (37.70% zero-shot → 25.93% with CoT), indicating task-dependent effects.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper demonstrates that interventions like CoT can help multi-hop QA reasoning but may hurt tasks requiring precise outputs (e.g., code execution or API calls) where additional extraneous reasoning text or misaligned decoding can reduce exactness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StreamBench: Towards Benchmarking Continuous Improvement of Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memory-assisted prompt editing to improve gpt-3 after deployment. <em>(Rating: 2)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Expel: Llm agents are experiential learners. <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate. <em>(Rating: 1)</em></li>
                <li>RECONCILE: Round-table conference improves reasoning via consensus among diverse llms. <em>(Rating: 1)</em></li>
                <li>Self-ICL: Zero-shot in-context learning with self-generated demonstrations. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-903",
    "paper_id": "paper-793a791fa20fe3552c1ebad0ec90c0336ada1183",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "Self-StreamICL",
            "name_full": "Self-Stream In-Context Learning (Self-StreamICL)",
            "brief_description": "A streaming baseline introduced in this paper that saves only correct self-generated (input, output) pairs into external memory and uses them as in-context demonstrations for future queries, enabling continuous online improvement without parameter updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Self-StreamICL (method applied with gpt-3.5 / gemini / claude endpoints)",
            "model_description": "An LLM agent pipeline that maintains an external vector-store memory of past self-generated (x_t, ŷ_t) pairs but only inserts those with correctness feedback fb_t=1; at each time step it retrieves k examples and uses them as in-context demonstrations (ICL) in the prompt. No network parameter updates are performed.",
            "model_size": null,
            "qa_task_name": "HotpotQA (distractor)",
            "qa_performance": "54.80% exact match (averaged across three LLM endpoints; Table 2)",
            "interactive_task_name": "ToolBench; DS-1000; Text-to-SQL (Spider/CoSQL/BIRD)",
            "interactive_task_type": "tool use; code generation (programming); text-to-SQL (structured query generation)",
            "interactive_performance": "ToolBench: 71.33% accuracy; DS-1000: 41.30% pass@1; Spider (Text-to-SQL): 74.63% execution accuracy (all averaged across three LLM endpoints; Table 2)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "external memory (vector DB), retriever for ICL, prompting template; no parameter fine-tuning",
            "training_method": "prompting / in-context learning using accumulated self-generated examples; no supervised fine-tuning or RL",
            "intervention_type": "prompting strategy + memory-based streaming",
            "intervention_description": "Collects only correct self-generated (x, ŷ) pairs (fb_t=1) into a shared memory and uses them as in-context demonstrations (k retrieved examples) for future queries; avoids verbalizing correctness flags and excludes incorrect examples from memory.",
            "intervention_effect": "Averaged HotpotQA EM improved from 48.49% (zero-shot) to 54.80%; ToolBench improved from 61.38% to 71.33%; Spider improved from 67.89% to 74.63% (Table 2, averaged across three LLM endpoints).",
            "hypothesized_cause_of_gap": "Paper argues that incorrect self-generated examples are distracting and can degrade performance when included in ICL; collecting only correct outputs gives useful demonstrations and reduces noise, helping both QA and interactive tasks—but improvements vary by task indicating differences in difficulty and signal utility across task types.",
            "uuid": "e903.0",
            "source_info": {
                "paper_title": "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MAM-StreamICL",
            "name_full": "Multi-Agentic-Memory Stream In-Context Learning (MAM-StreamICL)",
            "brief_description": "A multi-agent streaming approach (introduced in this paper) where multiple distinct LLM agents take turns answering and share a common memory of correct self-generated examples, enabling cross-agent experiential transfer while maintaining per-step cost similar to a single agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "MAM-StreamICL (multi-agent shared-memory method using multiple LLM endpoints)",
            "model_description": "Round-robin multi-agent framework: K different LLM agents (different endpoints) take turns to answer each incoming instance; when an agent's output is correct (fb_t=1) it is appended to a shared external memory; future retrievals draw from the combined experiences of all agents for ICL.",
            "model_size": null,
            "qa_task_name": "HotpotQA (distractor)",
            "qa_performance": "55.20% exact match (averaged across three LLM endpoints; Table 2)",
            "interactive_task_name": "ToolBench; DS-1000; Text-to-SQL (Spider/CoSQL/BIRD); DDXPlus (medical diagnosis)",
            "interactive_task_type": "tool use; programming; text-to-SQL; diagnostic classification (sequential decision-style accumulation of cases)",
            "interactive_performance": "ToolBench: 75.87% accuracy; DS-1000: 43.10% pass@1; Spider: 75.69% execution accuracy; DDXPlus: 83.50% accuracy (all averaged; Table 2)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "external shared memory, retriever, multi-agent scheduling (round-robin), prompting/ICL",
            "training_method": "prompting / in-context learning with multi-agent shared experience; no network parameter updates",
            "intervention_type": "architectural (multi-agent shared-memory) + prompting strategy",
            "intervention_description": "Introduces a shared external memory into which correct outputs from multiple distinct agents are stored; agents are scheduled in round-robin so only one agent answers per timestep (cost ~ average of single agent) while benefiting from the diverse correct examples produced by other agents.",
            "intervention_effect": "Consistent improvements across tasks relative to zero-shot and single-agent streaming: e.g., HotpotQA EM 48.49% → 55.20%; ToolBench 61.38% → 75.87%; DDXPlus 52.85% → 83.50% (Table 2 averaged). Shared memory yielded particularly strong gains on domain tasks (medical) where agents had complementary strengths.",
            "hypothesized_cause_of_gap": "Paper hypothesizes that agents have complementary strengths and that sharing correct experiences across agents supplies diverse, high-quality demonstrations that improve interactive/procedural performance; cost-effective sharing avoids linear cost scaling of multiple agents.",
            "uuid": "e903.1",
            "source_info": {
                "paper_title": "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MemPrompt",
            "name_full": "MemPrompt (memory-assisted prompting)",
            "brief_description": "A previously proposed memory-augmented prompting method that stores past (x, ŷ, feedback) triples and retrieves examples at inference time to condition the LLM, enabling improvement via memory retrieval rather than parameter updates.",
            "citation_title": "Memory-assisted prompt editing to improve gpt-3 after deployment.",
            "mention_or_use": "use",
            "model_or_agent_name": "MemPrompt (implemented with external memory and retriever in this paper)",
            "model_description": "External memory stores all past (x_t, ŷ_t, fb_t) triples; a retriever selects k elements from memory and they are verbalized (including correctness flag) and appended to the prompt to influence the model's next output.",
            "model_size": null,
            "qa_task_name": "HotpotQA (distractor)",
            "qa_performance": "52.62% exact match (averaged across three LLM endpoints; Table 2)",
            "interactive_task_name": "ToolBench; DS-1000; Text-to-SQL (Spider/CoSQL/BIRD)",
            "interactive_task_type": "tool use; programming; text-to-SQL",
            "interactive_performance": "ToolBench: 64.31% accuracy; DS-1000: 35.47% pass@1; Spider: 70.78% execution accuracy (Table 2 averaged)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "external memory, retriever, verbalized correctness flags included in prompts",
            "training_method": "prompting with retrieved memory examples (no parameter updates)",
            "intervention_type": "architectural (memory) + prompting strategy",
            "intervention_description": "Store all past interactions including correctness flags in a memory and retrieve k items to include (verbalized) in the prompt at inference time to steer future outputs.",
            "intervention_effect": "Small improvements over zero-shot on many tasks (e.g., HotpotQA 48.49% → 52.62%; Spider 67.89% → 70.78%), but less effective than Self-StreamICL and MAM-StreamICL. Ablations in paper show using only correct retrieved examples yields more consistent gains.",
            "hypothesized_cause_of_gap": "Including incorrect past outputs (even when verbalized as incorrect) can distract the LLM and hinder learning; the paper suggests that correct examples are much more valuable for improving interactive/procedural performance.",
            "uuid": "e903.2",
            "source_info": {
                "paper_title": "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GrowPrompt",
            "name_full": "GrowPrompt (sliding-window prompt memory)",
            "brief_description": "A streaming prompting baseline that stores recent (x, ŷ, fb) tuples in a sliding window and incorporates them into the prompt as recent examples for in-context conditioning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GrowPrompt (sliding-window streaming prompting)",
            "model_description": "Stores recent k time-step tuples in a sliding window W; at inference, the contents of W (including verbalized correctness flags) are appended to the prompt as demonstrations to influence the model's next prediction.",
            "model_size": null,
            "qa_task_name": "HotpotQA (distractor)",
            "qa_performance": "51.38% exact match (averaged across three LLM endpoints; Table 2)",
            "interactive_task_name": "ToolBench; DS-1000; Text-to-SQL (Spider/CoSQL/BIRD)",
            "interactive_task_type": "tool use; programming; text-to-SQL",
            "interactive_performance": "ToolBench: 65.07% accuracy; DS-1000: 33.77% pass@1; Spider: 69.90% execution accuracy (Table 2 averaged)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "sliding-window memory (textual), prompting with verbalized correctness flags",
            "training_method": "prompting / in-context examples from recent history",
            "intervention_type": "prompting strategy (streaming examples via sliding window)",
            "intervention_description": "Maintain a sliding window of the most recent k (x, ŷ, fb) and include them as in-context examples (fb verbalized) at inference time.",
            "intervention_effect": "Modest gains over zero-shot on some tasks (e.g., HotpotQA 48.49% → 51.38%; Spider 67.89% → 69.90%), but less effective than policies that only store correct examples or that share memory across agents.",
            "hypothesized_cause_of_gap": "Presence of incorrect examples in the sliding-window can introduce noise; verbalizing incorrectness does not fully mitigate negative impact of incorrect demonstrations.",
            "uuid": "e903.3",
            "source_info": {
                "paper_title": "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that encourages the LLM to generate intermediate reasoning steps (a rationale) before producing the final answer; evaluated here as a non-streaming baseline.",
            "citation_title": "Large language models are zero-shot reasoners.",
            "mention_or_use": "use",
            "model_or_agent_name": "Chain-of-Thought (CoT) prompting (applied to evaluated LLM endpoints)",
            "model_description": "Prompting with a trigger phrase to elicit step-by-step reasoning (rationale) prior to the final answer; applied zero-shot without memory or streaming.",
            "model_size": null,
            "qa_task_name": "HotpotQA (distractor)",
            "qa_performance": "52.47% exact match (averaged; Table 2)",
            "interactive_task_name": "ToolBench; DS-1000; Text-to-SQL; others",
            "interactive_task_type": "multi-step reasoning; code generation; text-to-SQL",
            "interactive_performance": "ToolBench: 58.98% accuracy; DS-1000: 25.93% pass@1; Spider: 61.53% execution accuracy (Table 2 averaged)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "prompting (CoT trigger phrase), no parameter updates",
            "intervention_type": "prompting strategy (reasoning elicitation)",
            "intervention_description": "Append a CoT trigger (e.g., 'Let's think step by step') to the prompt to elicit chain-of-thought style reasoning before producing an answer.",
            "intervention_effect": "CoT produced mixed results: improved or similar on some QA tasks (HotpotQA 48.49% → 52.47%) but significantly degraded performance on some procedural tasks like DS-1000 (37.70% zero-shot → 25.93% with CoT), indicating task-dependent effects.",
            "hypothesized_cause_of_gap": "Paper demonstrates that interventions like CoT can help multi-hop QA reasoning but may hurt tasks requiring precise outputs (e.g., code execution or API calls) where additional extraneous reasoning text or misaligned decoding can reduce exactness.",
            "uuid": "e903.4",
            "source_info": {
                "paper_title": "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memory-assisted prompt editing to improve gpt-3 after deployment.",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners.",
            "rating": 2
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate.",
            "rating": 1
        },
        {
            "paper_title": "RECONCILE: Round-table conference improves reasoning via consensus among diverse llms.",
            "rating": 1
        },
        {
            "paper_title": "Self-ICL: Zero-shot in-context learning with self-generated demonstrations.",
            "rating": 2
        }
    ],
    "cost": 0.015658,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>StreamBench: Towards Benchmarking Continuous Improvement of Language Agents</h1>
<p>Cheng-Kuang Wu ${ }^{1,2}$, Zhi Rui Tam ${ }^{1}$, Chieh-Yen Lin ${ }^{1}$, Yun-Nung Chen ${ }^{1,2}$, Hung-yi Lee ${ }^{2}$<br>${ }^{1}$ Appier AI Research<br>${ }^{2}$ National Taiwan University<br>{brian.wu, ray.tam}@appier.com</p>
<h4>Abstract</h4>
<p>Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios. Source code: https://github.com/stream-bench/stream-bench. Benchmark website: https://stream-bench.github.io.</p>
<h2>1 Introduction</h2>
<p>Recently, large-scale pretraining [1] and instruction fine-tuning [2] have driven paradigm shifts in how we interact with language models. These advancements allow us to use them out-of-the-box to solve problems. Consequently, many benchmarks have emerged to evaluate the general capabilities of these models. Some notable examples include MMLU [3], GSM8K [4], and BIG-Bench-Hard [5]. All these benchmarks aim to assess LLMs' innate capabilities, which we define as the general knowledge or reasoning abilities demonstrated when used out-of-the-box.</p>
<p>In addition to LLMs' strong innate capabilities, recent works have shown that LLM agents, which are LLMs augmented with extra components such as memory, retrievers, or tools, are able to improve themselves from experience. MemPrompt [6] shows that memory-enhanced GPT-3 can improve through time by storing past user feedback and retrieve them in the future. Reflexion [7] demonstrates that LLM agents can perform better in future trials by running repeated trials on the same dataset via self-reflection. ExpeL [8] further shows that LLM agents can learn from cross-task experience and improve performance without executing repeated trials on the target task.</p>
<p>Given LLM agents' self-improvement abilities, there remains a missing piece in the current evaluation landscape. Beyond measuring LLMs' innate capabilities with aforementioned offline benchmarks [3, 4, 5], it is important to assess their capacity to improve over time since we would like our systems to gradually improve after deployment. This gap motivated us to develop a new evaluation scenario-an online setting to measure LLM agents' ability to continuously enhance their performance over time.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (Left) A schematic diagram showing the online evaluation setting of StreamBench, where agents update their components (p, r, M, or θ) from an input-feedback sequence to achieve the highest final accuracy (refer to Section 3.1 for details). (Right) Performance curve on the DDXPlus dataset on StreamBench. Agents are able to gradually improve with our proposed streaming baselines.</p>
<p>This online setting focuses on scenarios where LLM agents attempt to solve a specific downstream task and improve themselves from an input-feedback sequence, with the goal to maximize the accuracy for the whole sequence of the agent's predictions.</p>
<p>Given these rationales, we introduce StreamBench, a benchmark designed to evaluate LLM agents' ability to improve themselves over an input-feedback sequence. StreamBench simulates an environment where LLM agents are exposed to a sequence of users' natural language requirements and feedback. To the best of our knowledge, StreamBench is the first benchmark to evaluate LLM agents in streaming scenarios with a diverse range of tasks. StreamBench aims to inspire further efforts to develop more adaptive LLM agents, thereby enhancing their practical effectiveness. Our contributions can be summarized as follows:</p>
<ul>
<li>We introduce StreamBench, the first benchmark designed to evaluate LLM agents' ability to improve over an input-feedback sequence in an online setting across a wide range of tasks.</li>
<li>We propose several simple yet effective baselines for enhancing LLM agents' performance in streaming scenarios, including a cost-effective multi-agent method that outperforms other baselines while maintaining the average cost of a single agent.</li>
<li>We conduct analysis on the advantages and potential pitfalls of the proposed methods, providing insights into effective streaming strategies of LLMs.</li>
</ul>
<h1>2 Formulation</h1>
<p>Consider a streaming scenario involving an agent, an external environment, and a sequence of inputs:
Agent. We define an agent as an LLM parameterized by $\theta$ and augmented with additional components to enhance the agent's capabilities, such as the external memory $\mathcal{M}$ and a retriever $r(\cdot)$ to store and retrieve useful information. Given an instance $x$ in natural language, a prompting template $p(\cdot)$, and a retrieval function $r(\cdot)$, the agent's output is denoted as $\hat{y}=f(p(x, r(\mathcal{M})) \mid \theta)$.
Environment. The external environment, denoted as $g(\cdot)$, provides feedback to the agent. The nature of $g(\cdot)$ varies depending on the specific downstream task and the type of feedback being collected. Potential roles for $g(\cdot)$ include human users, code execution environments, and API responses.
Input-feedback sequence. Consider a sequence of input stream where each input is denoted by $x_{t}$, with $t$ representing the $t$-th time step. After the agent provides the output $\hat{y}<em t="t">{t}$, the environment provides feedback signal $f b</em>\right)$. Figure 1 shows an overview of the streaming scenario.
Algorithm 1 presents a simple framework for language agents to continuously learn from feedback. Benchmark users can adapt Algorithm 1 or develop their own algorithms to update components of their language agents, with the goal to maximize the accuracy of the entire sequence.}=g\left(x_{t}, \hat{y}_{t</p>
<p>Algorithm 1 Framework for Language Agents to Continuously Learn from Feedback on StreamBench
1: Initialize agent $f(\cdot \mid \theta)$, prompting template $p(\cdot)$, retriever $r(\cdot)$, and external memory $\mathcal{M}$;
2: for $t=1,2, \ldots, T$ do
3: Receive instance $x_{t}$ from the data stream;
4: The agent predicts $\hat{y}<em t="t">{t}=f\left(p\left(x</em>)\right) \mid \theta\right)$;
5: Receive feedback signal $f b_{t}=g\left(x_{t}, \hat{y}}, r(\mathcal{M<em t="t">{t}\right)$;
6: Update $p(\cdot), r(\cdot), \mathcal{M}$, or $\theta$ using $x</em>}, \hat{y<em t="t">{t}$, and $f b</em> ; \triangleright$ Benchmark users can develop their own algorithms for updating their Language Agents to learn continuously
7: end for</p>
<p>Traditionally, updating the agent at each time step $t$ involves updating the model parameters $\theta$. However, as foundation models grow increasingly larger, frequently updating the agent's network parameters has become computationally expensive. Recent advancements offer promising alternatives for iterative improvement by updating other components of the agent. For example, one can adapt existing iterative prompt refinement strategies to refine $p(\cdot)[9,10,11]$, update the weights of the retriever $r(\cdot)[12,13,14]$, expand the agent's memory $\mathcal{M}[6,15]$, or use parameter-efficient fine-tuning techniques for augmenting $\theta$ [16]. These different strategies open new possibilities for continuous adaptation of agents without relying solely on network parameter updates. In this work, we develop several baselines for improving agents over time, with a particular focus on updating $p(\cdot)$ and $\mathcal{M}$. The baselines demonstrate both simplicity and effectiveness. We leave methods for updating $r(\cdot)$ and $\theta$, which require computationally expensive network parameter updates, for future research.</p>
<h1>3 StreamBench</h1>
<h3>3.1 General setup</h3>
<p>Streaming sequence Most public datasets are inherently static, meaning each instance does not have a time-related dependency. To adapt them for our streaming setup, we serialize each selected dataset in Section 3.2 by assigning a time step to each instance. To avoid arbitrary sequence assignment in the original datasets, we randomly shuffle each dataset using a fixed random seed. We release each dataset's assigned sequence obtained by this random seed in the supplementary materials to ensure reproducibility on StreamBench. Additionally, to ensure the robustness of our evaluation, we conduct experiments on different shuffled sequences with five random seeds, as discussed in Section 5.2. We also discuss the effects of distributional shifts in Appendix C.2.</p>
<p>Feedback signals Choosing appropriate type of feedback signal is a crucial consideration in StreamBench. Firstly, cost and practicality play a significant role; in practice, obtaining ground truth $y_{t}$ at each time step can be prohibitively expensive. For example, providing the exact code in programming tasks or the complete schema of each API call in tool use tasks is often impractical. In contrast, partial feedback, such as the helpfulness or correctness of the agent's output, is relatively easy to obtain-such as the "thumbs up" or "thumbs down" buttons commonly found in user interfaces of LLM applications. Given these rationales, we formalize the type of $f b_{t}$ as follows:</p>
<p>$$
f b_{t}=g\left(x_{t}, \hat{y}<em t="t">{t}\right), f b</em> \in{0,1}
$$</p>
<p>where $f b_{t}$ is a scalar serving as a proxy for the correctness of $\hat{y}<em t="t">{t}$ with respect to $x</em>}$, determined by the environment $g(\cdot)$ of the given downstream tasks. The feedback $f b_{t} \in{0,1}$ is binary, indicating whether the agent's output $\hat{y<em t="t">{t}$ is correct. This simplified feedback setting aims to offer a unified evaluation framework for ensuring consistency and practicality across diverse tasks. We leave other designs of $f b</em>$, such as ground truth or natural language feedback, for future works.</p>
<p>Evaluation In practice, an agent's goal is to satisfy as many user requirements as possible over a time sequence. We thus evaluate an agent by its aggregate metric at the final time step $(T)$. For example, the final metric on a given dataset can be calculated as $\frac{\sum_{t=1}^{T} h\left(\hat{y}<em t="t">{t}, y</em>$, where $h$ is the function for calculating the corresponding metric on a given dataset. Table 1 shows metrics for each dataset.}\right)}{T</p>
<p>Table 1: Input, output, evaluation metrics, and number of testing instances of selected datasets.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Text-to-SQL</th>
<th></th>
<th></th>
<th>Python</th>
<th>Tool Use</th>
<th>Medical</th>
<th>QA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Spider</td>
<td>CoSQL</td>
<td>BIRD</td>
<td>DS-1000</td>
<td>ToolBench</td>
<td>DDXPlus</td>
<td>HotpotQA</td>
</tr>
<tr>
<td>Input ( $x_{t}$ )</td>
<td>Data requirements</td>
<td></td>
<td></td>
<td>Question</td>
<td>User query</td>
<td>Symptoms</td>
<td>Question</td>
</tr>
<tr>
<td>Output ( $y_{t}$ )</td>
<td>SQL code</td>
<td></td>
<td></td>
<td>Code</td>
<td>API calls</td>
<td>Diagnosis</td>
<td>Answer</td>
</tr>
<tr>
<td>Metric</td>
<td>Execution accuracy</td>
<td></td>
<td></td>
<td>Pass@1</td>
<td>Accuracy</td>
<td>Accuracy</td>
<td>Exact Match</td>
</tr>
<tr>
<td>Test size $(T)$</td>
<td>2,147</td>
<td>1,007</td>
<td>1,534</td>
<td>1,000</td>
<td>750</td>
<td>1,764</td>
<td>1,500</td>
</tr>
</tbody>
</table>
<h1>3.2 Datasets</h1>
<p>To measure LLM agents' capacity for continuous improvement post-deployment, we select a diverse set of downstream tasks with potential real-world applications. Following the setting in Section 3.1, these tasks share the property that their ground truth output $y_{t}$ is costly to obtain at each time step.</p>
<p>Text-to-SQL For text-to-SQL tasks, the agent has to convert users' natural language queries into SQL code to meet their data requirements. StreamBench integrates three prominent datasets: Spider [17], CoSQL [18], and BIRD [19]. These datasets represent a progressive difficulty curve, allowing for evaluation of how well agents improve when faced with data of varying difficulties.</p>
<p>Python programming To evaluate coding ability improvement, we use the DS-1000 [20] dataset, which consists of real-world Python programming questions from StackOverflow. To successfully solve a given question, the agent must provide a solution and pass the associated test cases.</p>
<p>Tool use The ability to use external tools is a significant milestone in the development of LLMs, as it compensates for certain limitations, such as performing precise arithmetic operations or conducting web searches. For this purpose, we utilize the large-scale tool usage dataset ToolBench [21], and select the subset that includes stable and low-latency tool APIs collected in a previous work [22].</p>
<p>Medical diagnosis To assess LLMs' continuous improvement in applying expert knowledge, we use the DDXPlus [23] dataset, where agents must make a medical diagnosis out of 49 diagnoses based on patient profiles detailing their symptoms. This setup mimics how medical doctors improve their diagnostic skills through accumulated patient encounters. Evaluating LLMs on this dataset helps us understand their potential for continuous improvement in a highly specialized domain.</p>
<p>Question answering Question answering (QA) tasks evaluate an agent's ability to reason over supporting facts to answer users' questions. We adopt the distractor setting in HotpotQA [24], which requires reasoning over multiple supporting or distracting documents to answer questions. This helps measure the agent's improved proficiency in reasoning over grounded knowledge to provide accurate answers. Given the extensive volume of questions, we sampled 1,500 out of the total 7,410 questions.</p>
<p>Detailed information of the aforementioned datasets are provided in Table 1 and Appendix F.</p>
<h2>4 Experiments</h2>
<h3>4.1 Baselines</h3>
<p>A key objective of StreamBench is to compare the performance gains of LLM agents using nonstreaming versus streaming methods. In non-streaming settings, methods focus on optimizing performance at a per-instance level, with improvements made independently for each testing instance. For these non-streaming methods, the overall performance boost on the testing set stems from improvements on individual testing instances. In contrast, streaming methods utilize information from past instances to improve future performance. For streaming methods, we adapt two previously proposed methods and introduce two new approaches to explore effective streaming strategies.</p>
<h1>4.1.1 Non-streaming methods</h1>
<p>Zero-shot It reflects the basic instruction-following abilities of LLMs for solving a given task.</p>
<p>Few-shot It involves providing several ground truth $(x, y)$ pairs in the prompting template $p(\cdot)$. For datasets with training sets, we construct few-shot examples from the training data. For datasets without training sets, we compose few-shot examples and inspect their quality to ensure reliability. We include few-shot examples for each dataset in the supplementary materials for reproducibility.</p>
<p>Chain-of-thought (CoT) Following previous work [25], we employ a trigger phrase (e.g., "Let's think step by step.") to instruct the LLM to generate the reasoning process before providing its final answer. We then extract the answer in the correct format from the generated reasoning text.</p>
<p>Self-Refine Self-Refine [26] is a technique where the LLM iteratively improves its output based on self-feedback. The model generates an initial response and refines it through multiple iterations of refinement. It leverages LLMs' ability to self-evaluate and adjust its responses at a per-instance level.</p>
<h3>4.1.2 Streaming methods</h3>
<p>GrowPrompt We adapt the previously proposed method GrowPrompt [6], where $\left(x_{t}, \hat{y}<em t="t">{t}, f b</em>$.}\right)$ of the latest time steps are stored in a sliding window $\mathcal{W}$. The contents of $\mathcal{W}$ are incorporated into the prompt at inference time to output $y_{t}=f\left(p\left(x_{t}, \mathcal{W}\right) \mid \theta\right)$. This provides the agent with information from the past $k$ instances, where $k$ is a hyperparameter. Since LLM agents take text input, we verbalize $f b_{t} \in{0,1}$ to inform the agent of whether its output $y_{t}$ correctly satisfies the input $x_{t</p>
<p>MemPrompt As an advanced version of GrowPrompt, MemPrompt [6] incorporates an external memory $\mathcal{M}$ to store $\left(x_{t}, \hat{y}<em t="t">{t}, f b</em>$ 's correctness.}\right)$ of all past time points. During inference, a retriever $r(\cdot)$ is used to select $k$ elements from $\mathcal{M}$, and $f b_{t}$ is also verbalized to inform the agent of $\hat{y}_{t</p>
<p>Self-StreamICL Previous works [27, 28] have found that incorrect examples can negatively impact in-context learning (ICL) performance, though the extent varies for different LLMs. Based on these insights, we hypothesize that while GrowPrompt and MemPrompt use verbalized $f b_{t}$ to inform the agent about the correctness of its output, incorrect $\hat{y}<em t="t">{t}$ still introduces distracting signals that can hinder improvement. Therefore, we propose to save $\left(x</em>}, \hat{y<em t="t">{t}\right)$ pairs to memory $\mathcal{M}$ only when $f b</em>$. This method, called Self-StreamICL, operates similarly to regular ICL, except that the labels are now self-generated and gradually accumulate over the data stream, without the need to preconstruct few-shot examples. For more details, refer to Algorithm 2.}=1$, eliminating the need to save verbalized $f b_{t</p>
<p>Multi-Agentic-Memory StreamICL In Self-StreamICL, the agent learns exclusively from its own past experiences. However, we hypothesize that different LLM agents possess distinct strengths and weaknesses, so they can potentially benefit from the experiences of other agents. To explore this idea, we introduce Multi-Agentic-Memory StreamICL (MAM-StreamICL), which employs a multi-agent framework where multiple LLM agents share a common memory. This shared memory incorporates the past outputs of all agents, allowing each agent to learn from the diverse experiences of the others.</p>
<p>We implement a simple round-robin-like scheduling to switch between different LLM agents outlined in Algorithm 2. This ensures that each agent contributes to the shared memory in a balanced manner. Our experiments show that this straightforward strategy can boost performance beyond the average performance of the individual agents. In fact, Self-StreamICL can be seen as a special case of MAM-StreamICL with only one LLM agent.</p>
<p>Note that the high cost associated with scaling is the most critical drawback of multi-agent methods proposed by previous works, and the key advantage of MAM-StreamICL is its cost-effectiveness. Unlike methods such as Multiagent Debate [29] and RECONCILE [30], the cost of MAM-StreamICL does not scale proportionally with the number of agents. Instead, the cost is equivalent to the averaged cost of a single agent, since only one agent is assigned to answer at each time step.</p>
<p>Algorithm 2 Round-Robin Algorithm for MAM-StreamICL
1: Initialize $K$ agents $f_{0}\left(\cdot \mid \theta_{0}\right), f_{1}\left(\cdot \mid \theta_{1}\right), \ldots, f_{K-1}\left(\cdot \mid \theta_{K-1}\right) ; \triangleright \mathrm{K}=1$ in the Self-StreamICL baseline
2: Initialize prompt $p(\cdot)$, retriever $r(\cdot)$, and external memory $\mathcal{M}<em t="t">{0}$, all shared between agents;
3: for $t=1,2, \ldots, T$ do
4: $\quad$ Receive instance $x</em>$ from the data stream;
5: Select the next agent by $k=t \bmod K$;
6: $\quad$ The $k$-th agent predicts $\hat{y}<em k="k">{t}=f</em>}\left(p\left(x_{t}, r\left(\mathcal{M<em k="k">{t-1}\right)\right) \mid \theta</em>\right)$;
7: $\quad$ Receive feedback signal $f b_{t}=g\left(x_{t}, \hat{y}<em t="t">{t}\right) ; \quad \triangleright f b</em> \in{0,1}$ under the StreamBench setting
8: if $f b_{t}=1$ then $\triangleright$ which means the self-output $\hat{y}<em t="t">{t}$ is correct
9: $\mathcal{M}</em>} \leftarrow \mathcal{M<em t="t">{t-1} \cup\left{\left(x</em>}, \hat{y<em t="t">{t}\right)\right}$
10: else
11: $\mathcal{M}</em>$;
12: end if
13: end for} \leftarrow \mathcal{M}_{t-1</p>
<h1>4.2 Implementation details</h1>
<p>We conduct experiments using three LLM families: GPT [31, 32], Gemini [33, 34], and Claude [35]. For our main experiments, we use the endpoints gpt-3.5-turbo-0125, gemini-1.0-pro-001, and claude-3-haiku-20240307. These models represent cost-effective LLMs, balancing performance and affordability. The models initialize the $K=3$ agents in MAM-StreamICL. For methods with $\mathcal{M}$ (MemPrompt, Self-StreamICL, and MAM-StreamICL), we implement $\mathcal{M}$ as a vector database. We use BAAI/bge-base-en-v1.5 to encode $x_{t}$ as key embeddings and save $x_{t}, \hat{y}<em t="t">{t}$ (and $f b</em>$ for MemPrompt) as values. For hyperparameters and prompts, refer to Appendix A and F.</p>
<h3>4.3 Main results</h3>
<p>The main results are shown in Table 2, which lists the averaged performance of the three LLM agents. The only exception is MAM-StreamICL, which only runs once on the streaming sequence where each agent takes turns at each time step. For results of each respective model, refer to Appendix B.</p>
<p>Overall, streaming methods outperform non-streaming methods, though the extent of improvement varies across different datasets. These results demonstrate the value of leveraging input-feedback streams to enhance agent performance on downstream tasks. In addition, as demonstrated by the robust performance of Self-StreamICL compared to GrowPrompt and MemPrompt, leveraging feedback as simple as correctness can enable agents to improve even more through self-generated outputs $\hat{y}_{t}$. This provides an important insight: rather than solely focusing on prompting pipelines to boost per-instance performance, adopting simple yet effective streaming approaches, such as collecting correctness feedback, could potentially lead to notable improvements on LLM agents. Lastly, MAM-StreamICL shows the most notable and consistent performance boost across all datasets.</p>
<p>Table 2: Averaged performance of three LLM agents across different baselines and datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Text-to-SQL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Python</th>
<th style="text-align: center;">Tool Use</th>
<th style="text-align: center;">Medical</th>
<th style="text-align: center;">QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">Spider</td>
<td style="text-align: center;">CoSQL</td>
<td style="text-align: center;">BIRD</td>
<td style="text-align: center;">DS-1000</td>
<td style="text-align: center;">ToolBench</td>
<td style="text-align: center;">DDXPlus</td>
<td style="text-align: center;">HotpotQA</td>
</tr>
<tr>
<td style="text-align: center;">Non-streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">67.89</td>
<td style="text-align: center;">50.55</td>
<td style="text-align: center;">29.60</td>
<td style="text-align: center;">37.70</td>
<td style="text-align: center;">61.38</td>
<td style="text-align: center;">52.85</td>
<td style="text-align: center;">48.49</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">68.55</td>
<td style="text-align: center;">50.61</td>
<td style="text-align: center;">30.40</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">68.58</td>
<td style="text-align: center;">60.98</td>
<td style="text-align: center;">53.11</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">61.53</td>
<td style="text-align: center;">46.01</td>
<td style="text-align: center;">27.23</td>
<td style="text-align: center;">25.93</td>
<td style="text-align: center;">58.98</td>
<td style="text-align: center;">58.20</td>
<td style="text-align: center;">52.47</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine</td>
<td style="text-align: center;">67.75</td>
<td style="text-align: center;">49.49</td>
<td style="text-align: center;">29.62</td>
<td style="text-align: center;">36.30</td>
<td style="text-align: center;">60.67</td>
<td style="text-align: center;">52.89</td>
<td style="text-align: center;">43.53</td>
</tr>
<tr>
<td style="text-align: center;">Streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GrowPrompt</td>
<td style="text-align: center;">69.90</td>
<td style="text-align: center;">51.97</td>
<td style="text-align: center;">30.35</td>
<td style="text-align: center;">33.77</td>
<td style="text-align: center;">65.07</td>
<td style="text-align: center;">55.10</td>
<td style="text-align: center;">51.38</td>
</tr>
<tr>
<td style="text-align: center;">MemPrompt</td>
<td style="text-align: center;">70.78</td>
<td style="text-align: center;">53.29</td>
<td style="text-align: center;">31.99</td>
<td style="text-align: center;">35.47</td>
<td style="text-align: center;">64.31</td>
<td style="text-align: center;">54.02</td>
<td style="text-align: center;">52.62</td>
</tr>
<tr>
<td style="text-align: center;">Self-StreamICL</td>
<td style="text-align: center;">74.63</td>
<td style="text-align: center;">55.05</td>
<td style="text-align: center;">35.31</td>
<td style="text-align: center;">41.30</td>
<td style="text-align: center;">71.33</td>
<td style="text-align: center;">70.56</td>
<td style="text-align: center;">54.80</td>
</tr>
<tr>
<td style="text-align: center;">MAM-StreamICL</td>
<td style="text-align: center;">75.69</td>
<td style="text-align: center;">55.17</td>
<td style="text-align: center;">36.38</td>
<td style="text-align: center;">43.10</td>
<td style="text-align: center;">75.87</td>
<td style="text-align: center;">83.50</td>
<td style="text-align: center;">55.20</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Correctness ablations. The y-axis denotes performance difference from zero-shot. The results are the average of three LLM endpoints. Please refer to Appendix D for results of each LLM.</p>
<h1>5 Discussion</h1>
<h3>5.1 What makes effective streaming strategies?</h3>
<p>This subsection provides insights into the key aspects that contribute to successful streaming strategies. We identify two effective factors for streaming improvements as follows:</p>
<h3>5.1.1 Collecting correct self-output</h3>
<p>To investigate whether incorrect self-output hinders agents' improvement, we conducted ablation studies on GrowPrompt and MemPrompt. In the default setting in Table 2, both methods use all $k$ retrieved $\left(x_{t}, \hat{y}<em t="t">{t}, f b</em>=1$ (only correct).
The ablation results in Figure 2 reveal several findings. First, using incorrect self-output degrades performance, sometimes even worse than the zero-shot baseline. In contrast, using only correct self-output consistently boosts performance over the zero-shot baseline, with particularly consistent improvements observed in the MemPrompt (only correct) method. An important observation is that, even if $f b_{t}$ is verbalized to inform the agent whether its $\hat{y}}\right)$ triples during inference (use all). In contrast, the ablations either use only the triples where $f b_{t}=0$ (only incorrect), or use only the triples where $f b_{t<em t="t">{t}$ correctly satisfies $x</em>$ only when the self-output is correct.}$ in GrowPrompt and MemPrompt, simply informing the agent that its self-output is incorrect does not help it learn from past mistakes. Conversely, telling the LLM agent what it does correctly is very effective in facilitating improvement. These findings underscore the importance of collecting and utilizing correct self-output in streaming. This also explains the intuition and effectiveness behind Self-StreamICL, where input-output pairs are saved to $\mathcal{M</p>
<h3>5.1.2 Sharing memory across multiple agents</h3>
<p>Another key insight is the benefit of sharing memory across multiple agents, as demonstrated in MAM-StreamICL. To analyze why memory sharing works, we use DDXPlus as an example and visualize the confusion matrices for a subset of diagnoses related to upper respiratory tract diseases.
Figure 3 presents the confusion matrices for three different LLM agents: gpt-3.5-turbo-0125, gemini-1.0-pro, and claude-3-haiku-20240307, along with the matrix of MAM-StreamICL. Each matrix illustrates the proficiency of an agent across various medical diagnosis categories. It is evident that each model excels in certain areas while struggling in others. For instance, gpt-3.5-turbo-0125 shows high accuracy in predicting "acute rhinosinusitis" and "allergic sinusitis" but struggles with "chronic rhinosinusitis" and "URTI". In contrast, gemini-1.0-pro performs well in "URTI", and claude-3-haiku could solve "chronic rhinosinusitis".</p>
<p>The diversity in performance across models suggests that their collective past experiences can provide complementary strengths, thereby enhancing overall performance when these experiences are shared. Since each agent takes turn to solve an incoming $x_{t}$ at each time point $t$, the shared memory system allows the agents to benefit from others while maintaining a cost similar to that of a single agent. We also conduct further ablation studies in Appendix D to discuss the importance of sharing memory.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Confusion matrices of the diagnoses subset of upper respiratory tract diseases in DDXPlus.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Averaged performance and standard errors of each method on five shuffled sequences.</p>
<h3>5.2 Robustness to different streaming sequences</h3>
<p>Given the time-variant nature of streaming, evaluating each method's robustness across different data streams is essential. Therefore, we rerun the streaming baselines with 5 random seeds on five tasks. Figure 4 presents the averaged performance and standard errors of Claude-3-haiku across 5 shuffled sequences, with results for gpt-3.5-turbo and gemini-1.0-pro provided in Appendix C. The performance ranking of streaming baselines remains mostly consistent across datasets, with Self-StreamICL and MAM-StreamICL being the top performers. Due to the high cost of running all 5 sequences on StreamBench, we select a fixed sequence for fair comparison among future benchmark users. However, we also release all 5 sequences for those who wish to conduct a thorough evaluation.</p>
<h3>5.3 Would stronger LLMs still benefit from streaming?</h3>
<p>To evaluate whether stronger LLMs still benefit from streaming, we tested two newer models: gpt-4o-2024-08-06 and gemini-1.5-flash-001. Due to the high cost, we only run the methods shown in Table 3. We found that with Self-StreamICL, these stronger models still showed significant performance improvements. This demonstrates that even the most advanced models can leverage the information from streaming data to further enhance their performance across diverse tasks.</p>
<h3>5.4 Cost analysis</h3>
<p>For benchmark users to estimate the cost, the token usage of all baselines is listed in Appendix E.</p>
<p>Table 3: Performance of gpt-4o-2024-08-06 and gemini-1.5-flash-001 on StreamBench.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Text-to-SQL</th>
<th></th>
<th></th>
<th>Python</th>
<th>Tool Use</th>
<th>Medical</th>
<th>QA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Spider</td>
<td>CoSQL</td>
<td>BIRD</td>
<td>DS-1000</td>
<td>ToolBench</td>
<td>DDXPlus</td>
<td>HotpotQA</td>
</tr>
<tr>
<td>gemini-1.5-flash</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Zero-shot</td>
<td>69.63</td>
<td>48.26</td>
<td>33.83</td>
<td>50.20</td>
<td>69.47</td>
<td>58.90</td>
<td>60.60</td>
</tr>
<tr>
<td>Few-shot</td>
<td>71.40</td>
<td>49.35</td>
<td>37.03</td>
<td>50.60</td>
<td>72.13</td>
<td>73.58</td>
<td>64.87</td>
</tr>
<tr>
<td>CoT</td>
<td>72.52</td>
<td>52.73</td>
<td>35.14</td>
<td>44.80</td>
<td>68.00</td>
<td>64.06</td>
<td>63.13</td>
</tr>
<tr>
<td>Self-StreamICL</td>
<td>77.83</td>
<td>56.21</td>
<td>41.20</td>
<td>52.20</td>
<td>75.07</td>
<td>86.34</td>
<td>65.20</td>
</tr>
<tr>
<td>gpt-4o-2024-08-06</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Zero-shot</td>
<td>73.54</td>
<td>53.33</td>
<td>34.42</td>
<td>54.90</td>
<td>72.40</td>
<td>70.64</td>
<td>65.53</td>
</tr>
<tr>
<td>Few-shot</td>
<td>76.85</td>
<td>57.60</td>
<td>36.25</td>
<td>52.30</td>
<td>71.47</td>
<td>83.45</td>
<td>66.87</td>
</tr>
<tr>
<td>CoT</td>
<td>72.52</td>
<td>54.82</td>
<td>31.16</td>
<td>41.90</td>
<td>66.80</td>
<td>73.02</td>
<td>62.80</td>
</tr>
<tr>
<td>Self-StreamICL</td>
<td>80.58</td>
<td>59.19</td>
<td>42.63</td>
<td>59.40</td>
<td>76.27</td>
<td>92.01</td>
<td>67.00</td>
</tr>
</tbody>
</table>
<h1>6 Related work</h1>
<h3>6.1 Online learning</h3>
<p>Online learning [36] explores the incremental updating of models as new data arrives, making it valuable for dynamic improvement in downstream applications. Traditionally, it focuses updating network weights, such as in methods for training recurrent neural networks [37], online representation learning for image classification [38], and adapting language models to learn new world knowledge [39]. Recent advancements have introduced strategies for improving LLM agents by updating prompts [9, 10, 11], memory [6, 15], or retrievers [12, 13, 14]. These new strategies are promising for designing new algorithms to adapt LLMs in the online setting. However, there are no standard testbeds for this setup. Addressing this gap, we propose StreamBench, the first benchmark to pave the way for developing more dynamic adaptation methods for LLM agents.</p>
<h3>6.2 Improvement from feedback with LLMs</h3>
<p>Recent works have shown that LLMs can improve from feedback when augmented with prompting pipelines or memory mechanisms, forming two main research branches. One is instance-level improvement methods, such as ReAct [40], Self-ICL [41], and Self-Refine [26]. These methods focus on boosting performance on each input instance without leveraging information from past instances. The other is time-sequence-level improvement methods. For example, MemPrompt [6] enhances GPT-3 by storing past user feedback and retrieve them in the future. Reflexion [7] shows that LLM agents can perform better in future trials by running repeated trials on the same dataset, but this is not always practical in real-world scenarios. ExpeL [8] shows that LLM agents can benefit from cross-task experience without needing repeated trials on the target task. However, these works use different datasets and lack a standardized evaluation setting. StreamBench bridges this gap by providing a consistent empirical testbed across diverse tasks to evaluate LLM agents' improvement.</p>
<h2>7 Conclusion</h2>
<p>In this work, we introduce a new evaluation setting to measure LLM agents' performance improvement on downstream tasks, and propose StreamBench as an instance of this setting. There are two major findings in our experiments. Firstly, collecting correct self-generated outputs improve performance consistently, while informing agents of their incorrect outputs sometimes degrade performance. Secondly, sharing memory across multiple agents is a promising cost-effective technique, as MAM-StreamICL achieves robust performance while maintaining the average cost of a single agent.
StreamBench serves as a stepping stone towards more adaptive AI systems. Future directions include exploring online active learning where agents could inquire feedback only when necessary, or viewing multi-agent collaboration as multi-arm bandits (MABs) to develop more sophisticated methods for selecting agents and sharing memory at each time point. It is also practical to investigate the utilization of different feedback signals beyond correctness, such as users' natural language feedback. We hope that this work inspires development of adaptive methodology for improving LLM agents.</p>
<h2>8 Limitations</h2>
<p>Tasks and modality coverage The current version of StreamBench includes tasks such as programming, text-to-SQL conversion, medical diagnosis, question-answering, and tool use. While diverse, they do not encompass all possible types of tasks or domains where LLMs can be applied. StreamBench is also limited to text and does not cover other modalities such as image and audio.</p>
<p>Sim2Real gap Although we have attempted to simulate feedback signals as practical as possible, there may still be a gap between the simulated correctness feedback in StreamBench and the feedback encountered in real-world applications. Real-world feedback can be more diverse, noisy, and contextdependent, which may not be fully captured by the current benchmark.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>We would like to express our gratitude to Chih-Han Yu, Wei-Lin Chen, Yi-Lin Tsai, Chao-Chung Wu, Zhen-Ting Liu, and An-Zi Yen for their valuable comments and feedback on this work. Their insights greatly contributed to the improvement of our research. We declare no competing interests related to this work.</p>
<h2>References</h2>
<p>[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[2] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[3] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.
[4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[5] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003-13051, 2023.
[6] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve gpt-3 after deployment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2833-2861, 2022.
[7] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Neural Information Processing Systems, 2023.
[8] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19632-19642, 2024.
[9] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.
[10] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023.
[11] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023.
[12] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for incontext learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655-2671, 2022.
[13] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. In International Conference on Machine Learning, pages 39818-39833. PMLR, 2023.
[14] Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. Unified demonstration retriever for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4644-4668, 2023.</p>
<p>[15] Xiaonan Li and Xipeng Qiu. Mot: Memory-of-thought enables chatgpt to self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6354-6374, 2023.
[16] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220-235, 2023.
[17] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911-3921, 2018.
[18] Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, et al. Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases. arXiv preprint arXiv:1909.05378, 2019.
[19] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sql. Advances in Neural Information Processing Systems, 36, 2024.
[20] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wentau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pages 18319-18345. PMLR, 2023.
[21] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.
[22] Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. Llms in the imaginarium: tool learning through simulated trial and error. arXiv preprint arXiv:2403.04746, 2024.
[23] Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. Ddxplus: A new dataset for automatic medical diagnosis. Advances in Neural Information Processing Systems, 35:31306-31318, 2022.
[24] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.
[25] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.
[26] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.
[27] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, 2022.
[28] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.
[29] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.
[30] Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023.</p>
<p>[31] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.
[32] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[33] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
[34] Gemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv, abs/2403.05530, 2024.
[35] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024.
[36] Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online learning: A comprehensive survey. Neurocomputing, 459:249-289, 2021.
[37] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270-280, 1989.
[38] Yanis Bahroun and Andrea Soltoggio. Online representation learning with single and multi-layer hebbian networks for image classification. arXiv preprint arXiv:1702.06456, 2017.
[39] Nathan Hu, Eric Mitchell, Christopher D Manning, and Chelsea Finn. Meta-learning online adaptation of language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4418-4432, 2023.
[40] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[41] Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, and Hsin-Hsi Chen. Self-icl: Zero-shot in-context learning with self-generated demonstrations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15651-15662, 2023.</p>
<h1>Checklist</h1>
<ol>
<li>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See the experiment results in Section 4.3.
(b) Did you describe the limitations of your work? [Yes] See Section 8.
(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</li>
<li>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]</li>
<li>If you ran experiments (e.g. for benchmarks)...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Please refer to the supplementary materials.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please refer to Appendix A and F.
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See the results in Figure 4.
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.4.</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] See Section 3.2.
(b) Did you mention the license of the assets? [Yes] See the supplementary materials.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We include code in the supplementary materials.
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See the supplementary materials.
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? $[\mathrm{N} / \mathrm{A}]$</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? $[\mathrm{N} / \mathrm{A}]$
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</li>
</ol>
<h1>A Hyperparameters</h1>
<p>For decoding strategies of all model endpoints used in this work, we set temperature to 0 and top-p to 1 . The few-shot baseline and streaming baselines (GrowPrompt, MemPrompt, Self-StreamICL, and MAM-StreamICL) incorporate information from $k$ instances into the prompt $p(\cdot)$ to improve LLM agents. We use the same $k$ across these baselines for fair comparison. We set $k=16$ for Spider, CoSQL, BIRD, ToolBench, and DDXPlus. For DS-1000 and HotpotQA, we set $k=4$ to avoid exceeding the context size of gpt-3.5-turbo-0125.
We also analyze how different text embeddings used in memory correlate with streaming performance in Table 4. We observe that within the same text encoder family (bge), the larger model (109M parameters) generally delivers better performance. However, smaller models ( 22.7 M parameters) can also achieve strong results, indicating that each LLM may benefit from a specific encoder.</p>
<p>Table 4: Performance of Self-StreamICL (implemented with different text encoders and LLMs) on the DDXPlus dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Text encoder / LLMs</th>
<th style="text-align: center;">gpt-3.5-turbo-0125</th>
<th style="text-align: center;">claude-3-haiku</th>
<th style="text-align: center;">gemini-1.5-flash-001</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">all-MiniLM-L6-v2 (22.7M)</td>
<td style="text-align: center;">63.61</td>
<td style="text-align: center;">$\mathbf{7 8 . 9 1}$</td>
<td style="text-align: center;">83.50</td>
</tr>
<tr>
<td style="text-align: left;">bge-small-en-v1.5 (33.4M)</td>
<td style="text-align: center;">63.55</td>
<td style="text-align: center;">75.51</td>
<td style="text-align: center;">83.90</td>
</tr>
<tr>
<td style="text-align: left;">bge-base-en-v1.5 (109M)</td>
<td style="text-align: center;">$\mathbf{6 6 . 1 6}$</td>
<td style="text-align: center;">76.02</td>
<td style="text-align: center;">$\mathbf{8 6 . 3 4}$</td>
</tr>
</tbody>
</table>
<h2>B Main results of each LLM endpoint</h2>
<p>Main experiments results for three different LLM families models are shown below:</p>
<p>Table 5: Performance of different baselines and datasets for gpt-3.5-turbo-0125</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Text-to-SQL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Python</th>
<th style="text-align: center;">Tool Use</th>
<th style="text-align: center;">Medical</th>
<th style="text-align: center;">QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">Spider</td>
<td style="text-align: center;">CoSQL</td>
<td style="text-align: center;">BIRD</td>
<td style="text-align: center;">DS-1000</td>
<td style="text-align: center;">ToolBench</td>
<td style="text-align: center;">DDXPlus</td>
<td style="text-align: center;">HotpotQA</td>
</tr>
<tr>
<td style="text-align: left;">Non-streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: center;">68.89</td>
<td style="text-align: center;">52.83</td>
<td style="text-align: center;">29.75</td>
<td style="text-align: center;">41.50</td>
<td style="text-align: center;">64.13</td>
<td style="text-align: center;">47.56</td>
<td style="text-align: center;">54.53</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: center;">69.54</td>
<td style="text-align: center;">52.73</td>
<td style="text-align: center;">28.94</td>
<td style="text-align: center;">33.30</td>
<td style="text-align: center;">70.13</td>
<td style="text-align: center;">54.31</td>
<td style="text-align: center;">54.93</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">65.53</td>
<td style="text-align: center;">47.96</td>
<td style="text-align: center;">29.21</td>
<td style="text-align: center;">32.80</td>
<td style="text-align: center;">57.20</td>
<td style="text-align: center;">53.18</td>
<td style="text-align: center;">57.13</td>
</tr>
<tr>
<td style="text-align: left;">Self-Refine</td>
<td style="text-align: center;">67.21</td>
<td style="text-align: center;">51.64</td>
<td style="text-align: center;">29.92</td>
<td style="text-align: center;">39.80</td>
<td style="text-align: center;">64.53</td>
<td style="text-align: center;">47.68</td>
<td style="text-align: center;">40.06</td>
</tr>
<tr>
<td style="text-align: left;">Streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GrowPrompt</td>
<td style="text-align: center;">70.89</td>
<td style="text-align: center;">53.43</td>
<td style="text-align: center;">30.31</td>
<td style="text-align: center;">27.20</td>
<td style="text-align: center;">67.60</td>
<td style="text-align: center;">44.62</td>
<td style="text-align: center;">55.80</td>
</tr>
<tr>
<td style="text-align: left;">MemPrompt</td>
<td style="text-align: center;">73.68</td>
<td style="text-align: center;">54.32</td>
<td style="text-align: center;">34.16</td>
<td style="text-align: center;">29.40</td>
<td style="text-align: center;">67.07</td>
<td style="text-align: center;">51.53</td>
<td style="text-align: center;">56.73</td>
</tr>
<tr>
<td style="text-align: left;">StreamICL</td>
<td style="text-align: center;">75.59</td>
<td style="text-align: center;">54.92</td>
<td style="text-align: center;">35.07</td>
<td style="text-align: center;">41.90</td>
<td style="text-align: center;">74.00</td>
<td style="text-align: center;">66.16</td>
<td style="text-align: center;">55.80</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance of different baselines and datasets for gemini-1.0-pro-001</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Text-to-SQL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Python</th>
<th style="text-align: center;">Tool Use</th>
<th style="text-align: center;">Medical</th>
<th style="text-align: center;">QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">Spider</td>
<td style="text-align: center;">CoSQL</td>
<td style="text-align: center;">BIRD</td>
<td style="text-align: center;">DS-1000</td>
<td style="text-align: center;">ToolBench</td>
<td style="text-align: center;">DDXPlus</td>
<td style="text-align: center;">HotpotQA</td>
</tr>
<tr>
<td style="text-align: left;">Non-streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: center;">68.28</td>
<td style="text-align: center;">49.26</td>
<td style="text-align: center;">28.16</td>
<td style="text-align: center;">33.20</td>
<td style="text-align: center;">61.73</td>
<td style="text-align: center;">50.57</td>
<td style="text-align: center;">49.47</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: center;">68.33</td>
<td style="text-align: center;">49.65</td>
<td style="text-align: center;">31.49</td>
<td style="text-align: center;">29.40</td>
<td style="text-align: center;">66.27</td>
<td style="text-align: center;">57.48</td>
<td style="text-align: center;">54.40</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">52.31</td>
<td style="text-align: center;">40.81</td>
<td style="text-align: center;">21.71</td>
<td style="text-align: center;">21.10</td>
<td style="text-align: center;">58.40</td>
<td style="text-align: center;">59.30</td>
<td style="text-align: center;">49.67</td>
</tr>
<tr>
<td style="text-align: left;">Self-Refine</td>
<td style="text-align: center;">69.59</td>
<td style="text-align: center;">46.47</td>
<td style="text-align: center;">28.94</td>
<td style="text-align: center;">32.80</td>
<td style="text-align: center;">61.60</td>
<td style="text-align: center;">50.57</td>
<td style="text-align: center;">49.53</td>
</tr>
<tr>
<td style="text-align: left;">Streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GrowPrompt</td>
<td style="text-align: center;">71.59</td>
<td style="text-align: center;">52.43</td>
<td style="text-align: center;">28.68</td>
<td style="text-align: center;">33.10</td>
<td style="text-align: center;">61.87</td>
<td style="text-align: center;">51.13</td>
<td style="text-align: center;">52.80</td>
</tr>
<tr>
<td style="text-align: left;">MemPrompt</td>
<td style="text-align: center;">70.28</td>
<td style="text-align: center;">54.22</td>
<td style="text-align: center;">30.18</td>
<td style="text-align: center;">35.50</td>
<td style="text-align: center;">58.80</td>
<td style="text-align: center;">43.59</td>
<td style="text-align: center;">55.00</td>
</tr>
<tr>
<td style="text-align: left;">StreamICL</td>
<td style="text-align: center;">76.48</td>
<td style="text-align: center;">55.41</td>
<td style="text-align: center;">33.25</td>
<td style="text-align: center;">35.80</td>
<td style="text-align: center;">68.80</td>
<td style="text-align: center;">69.50</td>
<td style="text-align: center;">57.20</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of different baselines and datasets for claude-3-haiku-20240307</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Text-to-SQL</th>
<th></th>
<th></th>
<th>Python</th>
<th>Tool Use</th>
<th>Medical</th>
<th>QA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Spider</td>
<td>CoSQL</td>
<td>BIRD</td>
<td>DS-1000</td>
<td>ToolBench</td>
<td>DDXPlus</td>
<td>HotpotQA</td>
</tr>
<tr>
<td>Non-streaming</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Zero-Shot</td>
<td>66.51</td>
<td>49.55</td>
<td>30.90</td>
<td>38.40</td>
<td>58.27</td>
<td>60.43</td>
<td>41.47</td>
</tr>
<tr>
<td>Few-Shot</td>
<td>67.77</td>
<td>49.45</td>
<td>30.77</td>
<td>37.30</td>
<td>69.33</td>
<td>71.15</td>
<td>50.00</td>
</tr>
<tr>
<td>CoT</td>
<td>66.74</td>
<td>49.26</td>
<td>30.77</td>
<td>23.90</td>
<td>61.33</td>
<td>62.13</td>
<td>50.60</td>
</tr>
<tr>
<td>Self-Refine</td>
<td>66.46</td>
<td>50.35</td>
<td>29.99</td>
<td>36.30</td>
<td>55.87</td>
<td>60.43</td>
<td>41.00</td>
</tr>
<tr>
<td>Streaming</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GrowPrompt</td>
<td>67.21</td>
<td>50.05</td>
<td>32.07</td>
<td>41.00</td>
<td>65.73</td>
<td>69.56</td>
<td>45.53</td>
</tr>
<tr>
<td>MemPrompt</td>
<td>68.37</td>
<td>51.34</td>
<td>31.62</td>
<td>41.50</td>
<td>67.07</td>
<td>66.95</td>
<td>46.13</td>
</tr>
<tr>
<td>Self-StreamICL</td>
<td>71.82</td>
<td>54.82</td>
<td>37.61</td>
<td>46.20</td>
<td>71.20</td>
<td>76.02</td>
<td>51.40</td>
</tr>
</tbody>
</table>
<h1>C Robustness to different streaming sequences</h1>
<h2>C. 1 Performance on sequences shuffled by five different random seeds</h2>
<p>Results of averaged performance and standard errors of gpt-3.5-turbo-0125, gemini-1.0-pro-001, and claude-3-haiku-20240307 are listed below:
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Averaged performance and standard errors of gpt-3.5-turbo on five shuffled sequences.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Averaged performance and standard errors of gemini-1.0-pro on five shuffled sequences.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Averaged performance and standard errors of claude-3-haiku on five shuffled sequences.</p>
<h1>C. 2 Performance on the sequence with distributional shifts</h1>
<p>Since we randomly assign time steps to each instance, our main results in Table 2 simulates scenarios where each instance in the streaming sequence is drawn from the same distribution. However, it is important to investigate how well streaming methods perform on sequences with distributional shifts. To this end, we conduct experiments on BIRD [19] by arranging instances from the same database consecutively (DB1, DB2, ..., DB11), resulting in 10 distributional shifts during streaming. The results are shown in Table 8. The key finding is that Self-StreamICL still outperforms non-streaming baselines, and the method's performance does not differ drastically when distributional shifts occur.</p>
<p>Table 8: Performance of Self-StreamICL with different LLMs on BIRD.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method / LLM</th>
<th style="text-align: center;">gpt-3.5</th>
<th style="text-align: center;">gemini-1.0-pro</th>
<th style="text-align: center;">claude-3-haiku</th>
<th style="text-align: center;">gemini-1.5-flash</th>
<th style="text-align: center;">gpt-40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: center;">29.75</td>
<td style="text-align: center;">28.16</td>
<td style="text-align: center;">30.90</td>
<td style="text-align: center;">33.83</td>
<td style="text-align: center;">34.42</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: center;">28.94</td>
<td style="text-align: center;">31.49</td>
<td style="text-align: center;">30.77</td>
<td style="text-align: center;">37.03</td>
<td style="text-align: center;">36.25</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">29.21</td>
<td style="text-align: center;">21.71</td>
<td style="text-align: center;">30.77</td>
<td style="text-align: center;">35.14</td>
<td style="text-align: center;">31.16</td>
</tr>
<tr>
<td style="text-align: left;">Self-StreamICL (no shifts)</td>
<td style="text-align: center;">35.07</td>
<td style="text-align: center;">33.25</td>
<td style="text-align: center;">37.61</td>
<td style="text-align: center;">41.20</td>
<td style="text-align: center;">42.63</td>
</tr>
<tr>
<td style="text-align: left;">Self-StreamICL (w/ shifts)</td>
<td style="text-align: center;">36.31</td>
<td style="text-align: center;">32.60</td>
<td style="text-align: center;">36.57</td>
<td style="text-align: center;">40.48</td>
<td style="text-align: center;">43.09</td>
</tr>
</tbody>
</table>
<h2>D Detailed ablation study results</h2>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Correctness ablations of gpt-3.5-turbo-0125.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Correctness ablations of gemini-1.0-pro-001.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Correctness ablations of claude-3-haiku-20240307.</p>
<p>Table 9: Ablation studies with MAM-StreamICL on DDXPlus. The ablated version of MAMStreamICL only uses memory of the single corresponding agent, while still uses round-robin algorithm for multi-agent inference. We can see that both multi-agent memory and inference are beneficial for performance boost. The detailed algorithm for this ablation study can be found in Appendix F.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>GPT</th>
<th>Gemini</th>
<th>Claude</th>
<th>Memory</th>
<th>Inference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero-Shot</td>
<td>47.56</td>
<td>50.57</td>
<td>60.43</td>
<td>x</td>
<td>single agent</td>
</tr>
<tr>
<td>Self-StreamICL</td>
<td>66.16</td>
<td>69.50</td>
<td>76.02</td>
<td>single agent</td>
<td>single agent</td>
</tr>
<tr>
<td>MAM-StreamICL (ablation)</td>
<td>65.31</td>
<td>72.05</td>
<td>81.52</td>
<td>single agent</td>
<td>multi agent</td>
</tr>
<tr>
<td>MAM-StreamICL</td>
<td>$\mathbf{8 3 . 5 0}$</td>
<td>$\mathbf{8 3 . 5 0}$</td>
<td>$\mathbf{8 3 . 5 0}$</td>
<td>multi agent</td>
<td>multi agent</td>
</tr>
</tbody>
</table>
<h1>E Token cost breakdown for each LLM endpoint</h1>
<p>Table 10, 11, 12, and 13 shows how many millions of input, output tokens are used by gpt-3.5-turbo-0125, gemini-1.0-pro-001, claude-3-haiku-20240307, and the latest models. The cost of MAM-StreamICL is simply the averaged cost of the three LLMs due to the round-robin algorithm. As most LLM endpoints use input and output tokens to charge usage fees, we provide this information for benchmark users to estimate the cost for running StreamBench.</p>
<p>Table 10: Cost analysis (using millions of input and output tokens of gpt-3.5-turbo-0125).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Text-to-SQL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Python</th>
<th style="text-align: center;">Tool Use</th>
<th style="text-align: center;">Medical</th>
<th style="text-align: center;">QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spider</td>
<td style="text-align: center;">CoSQL</td>
<td style="text-align: center;">BIRD</td>
<td style="text-align: center;">DS-1000</td>
<td style="text-align: center;">ToolBench</td>
<td style="text-align: center;">DDXPlus</td>
<td style="text-align: center;">HotpotQA</td>
</tr>
<tr>
<td style="text-align: center;">Non-streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">0.734/0.064</td>
<td style="text-align: center;">0.392/0.023</td>
<td style="text-align: center;">1.338/0.070</td>
<td style="text-align: center;">0.522/0.053</td>
<td style="text-align: center;">5.202/0.019</td>
<td style="text-align: center;">1.131/0.013</td>
<td style="text-align: center;">2.182/0.014</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">2.367/0.061</td>
<td style="text-align: center;">0.954/0.021</td>
<td style="text-align: center;">3.206/0.074</td>
<td style="text-align: center;">1.984/0.074</td>
<td style="text-align: center;">5.869/0.023</td>
<td style="text-align: center;">8.138/0.014</td>
<td style="text-align: center;">9.864/0.013</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">0.778/0.181</td>
<td style="text-align: center;">0.407/0.077</td>
<td style="text-align: center;">1.252/0.168</td>
<td style="text-align: center;">0.550/0.134</td>
<td style="text-align: center;">5.239/0.053</td>
<td style="text-align: center;">1.186/0.184</td>
<td style="text-align: center;">2.227/0.082</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine</td>
<td style="text-align: center;">2.317/0.164</td>
<td style="text-align: center;">1.285/0.079</td>
<td style="text-align: center;">4.634/0.245</td>
<td style="text-align: center;">1.802/0.078</td>
<td style="text-align: center;">45.67/0.262</td>
<td style="text-align: center;">2.384/0.023</td>
<td style="text-align: center;">10.43/0.101</td>
</tr>
<tr>
<td style="text-align: center;">Streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GrowPrompt</td>
<td style="text-align: center;">2.819/0.065</td>
<td style="text-align: center;">1.205/0.021</td>
<td style="text-align: center;">3.309/0.069</td>
<td style="text-align: center;">2.103/0.088</td>
<td style="text-align: center;">5.910/0.019</td>
<td style="text-align: center;">7.699/0.012</td>
<td style="text-align: center;">10.56/0.013</td>
</tr>
<tr>
<td style="text-align: center;">MemPrompt</td>
<td style="text-align: center;">2.711/0.064</td>
<td style="text-align: center;">1.193/0.021</td>
<td style="text-align: center;">3.178/0.065</td>
<td style="text-align: center;">2.066/0.090</td>
<td style="text-align: center;">5.890/0.018</td>
<td style="text-align: center;">7.932/0.013</td>
<td style="text-align: center;">10.60/0.013</td>
</tr>
<tr>
<td style="text-align: center;">Self-StreamICL</td>
<td style="text-align: center;">2.156/0.063</td>
<td style="text-align: center;">0.966/0.021</td>
<td style="text-align: center;">2.688/0.066</td>
<td style="text-align: center;">1.765/0.073</td>
<td style="text-align: center;">5.756/0.019</td>
<td style="text-align: center;">7.833/0.013</td>
<td style="text-align: center;">10.46/0.013</td>
</tr>
</tbody>
</table>
<p>Table 11: Cost analysis (using millions of input and output tokens of gemini-1.0-pro-001).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Text-to-SQL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Python</th>
<th style="text-align: center;">Tool Use</th>
<th style="text-align: center;">Medical</th>
<th style="text-align: center;">QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spider</td>
<td style="text-align: center;">CoSQL</td>
<td style="text-align: center;">BIRD</td>
<td style="text-align: center;">DS-1000</td>
<td style="text-align: center;">ToolBench</td>
<td style="text-align: center;">DDXPlus</td>
<td style="text-align: center;">HotpotQA</td>
</tr>
<tr>
<td style="text-align: center;">Non-streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">0.871/0.110</td>
<td style="text-align: center;">0.454/0.045</td>
<td style="text-align: center;">1.538/0.114</td>
<td style="text-align: center;">0.591/0.044</td>
<td style="text-align: center;">5.603/0.020</td>
<td style="text-align: center;">1.131/0.011</td>
<td style="text-align: center;">2.222/0.018</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">2.639/0.108</td>
<td style="text-align: center;">1.062/0.043</td>
<td style="text-align: center;">3.668/0.120</td>
<td style="text-align: center;">2.215/0.157</td>
<td style="text-align: center;">6.301/0.023</td>
<td style="text-align: center;">8.553/0.010</td>
<td style="text-align: center;">9.863/0.015</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">0.930/0.322</td>
<td style="text-align: center;">0.477/0.130</td>
<td style="text-align: center;">1.443/0.319</td>
<td style="text-align: center;">0.626/0.158</td>
<td style="text-align: center;">5.626/0.873</td>
<td style="text-align: center;">1.191/0.310</td>
<td style="text-align: center;">2.273/0.083</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine</td>
<td style="text-align: center;">1.944/0.131</td>
<td style="text-align: center;">0.987/0.056</td>
<td style="text-align: center;">2.977/0.129</td>
<td style="text-align: center;">1.869/0.205</td>
<td style="text-align: center;">11.14/0.038</td>
<td style="text-align: center;">2.350/0.019</td>
<td style="text-align: center;">4.587/0.025</td>
</tr>
<tr>
<td style="text-align: center;">Streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GrowPrompt</td>
<td style="text-align: center;">4.307/0.152</td>
<td style="text-align: center;">1.774/0.053</td>
<td style="text-align: center;">4.103/0.109</td>
<td style="text-align: center;">2.146/0.040</td>
<td style="text-align: center;">6.334/0.020</td>
<td style="text-align: center;">8.077/0.010</td>
<td style="text-align: center;">10.82/0.016</td>
</tr>
<tr>
<td style="text-align: center;">MemPrompt</td>
<td style="text-align: center;">3.405/0.100</td>
<td style="text-align: center;">1.693/0.049</td>
<td style="text-align: center;">4.012/0.109</td>
<td style="text-align: center;">2.073/0.039</td>
<td style="text-align: center;">6.325/0.020</td>
<td style="text-align: center;">8.354/0.011</td>
<td style="text-align: center;">10.88/0.015</td>
</tr>
<tr>
<td style="text-align: center;">Self-StreamICL</td>
<td style="text-align: center;">3.402/0.142</td>
<td style="text-align: center;">1.212/0.034</td>
<td style="text-align: center;">3.299/0.106</td>
<td style="text-align: center;">1.880/0.037</td>
<td style="text-align: center;">6.175/0.019</td>
<td style="text-align: center;">8.250/0.010</td>
<td style="text-align: center;">10.79/0.014</td>
</tr>
</tbody>
</table>
<p>Table 12: Cost analysis (using millions of input and output tokens of claude-3-haiku-20240307).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Text-to-SQL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Python</th>
<th style="text-align: center;">Tool Use</th>
<th style="text-align: center;">Medical</th>
<th style="text-align: center;">QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spider</td>
<td style="text-align: center;">CoSQL</td>
<td style="text-align: center;">BIRD</td>
<td style="text-align: center;">DS-1000</td>
<td style="text-align: center;">ToolBench</td>
<td style="text-align: center;">DDXPlus</td>
<td style="text-align: center;">HotpotQA</td>
</tr>
<tr>
<td style="text-align: center;">Non-streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">0.911/0.096</td>
<td style="text-align: center;">0.474/0.038</td>
<td style="text-align: center;">1.510/0.094</td>
<td style="text-align: center;">0.669/0.204</td>
<td style="text-align: center;">5.971/0.028</td>
<td style="text-align: center;">1.287/0.011</td>
<td style="text-align: center;">2.419/0.025</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">2.790/0.094</td>
<td style="text-align: center;">1.111/0.036</td>
<td style="text-align: center;">4.047/0.098</td>
<td style="text-align: center;">2.347/0.256</td>
<td style="text-align: center;">6.726/0.027</td>
<td style="text-align: center;">9.160/0.024</td>
<td style="text-align: center;">10.98/0.022</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">0.975/0.391</td>
<td style="text-align: center;">0.500/0.148</td>
<td style="text-align: center;">1.571/0.370</td>
<td style="text-align: center;">0.704/0.341</td>
<td style="text-align: center;">5.996/0.074</td>
<td style="text-align: center;">1.350/0.297</td>
<td style="text-align: center;">2.473/0.166</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine</td>
<td style="text-align: center;">2.088/0.194</td>
<td style="text-align: center;">1.109/0.089</td>
<td style="text-align: center;">3.753/0.195</td>
<td style="text-align: center;">1.624/0.225</td>
<td style="text-align: center;">20.75/0.252</td>
<td style="text-align: center;">2.948/0.298</td>
<td style="text-align: center;">6.427/0.076</td>
</tr>
<tr>
<td style="text-align: center;">Streaming</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GrowPrompt</td>
<td style="text-align: center;">3.562/0.099</td>
<td style="text-align: center;">1.551/0.037</td>
<td style="text-align: center;">4.103/0.098</td>
<td style="text-align: center;">2.336/0.222</td>
<td style="text-align: center;">6.750/0.025</td>
<td style="text-align: center;">8.650/0.022</td>
<td style="text-align: center;">11.77/0.024</td>
</tr>
<tr>
<td style="text-align: center;">MemPrompt</td>
<td style="text-align: center;">3.495/0.103</td>
<td style="text-align: center;">1.527/0.038</td>
<td style="text-align: center;">4.067/0.101</td>
<td style="text-align: center;">2.275/0.214</td>
<td style="text-align: center;">6.763/0.025</td>
<td style="text-align: center;">8.924/0.024</td>
<td style="text-align: center;">11.82/0.024</td>
</tr>
<tr>
<td style="text-align: center;">Self-StreamICL</td>
<td style="text-align: center;">2.844/0.100</td>
<td style="text-align: center;">1.219/0.036</td>
<td style="text-align: center;">3.409/0.098</td>
<td style="text-align: center;">2.048/0.216</td>
<td style="text-align: center;">6.577/0.026</td>
<td style="text-align: center;">8.666/0.026</td>
<td style="text-align: center;">11.71/0.021</td>
</tr>
</tbody>
</table>
<p>Table 13: Cost analysis (millions of input and output tokens) on gemini-1.5-flash and gpt-4o.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Text-to-SQL</th>
<th></th>
<th></th>
<th>Python</th>
<th>Tool Use</th>
<th>Medical</th>
<th>QA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Spider</td>
<td>CoSQL</td>
<td>BIRD</td>
<td>DS-1000</td>
<td>ToolBench</td>
<td>DDXPlus</td>
<td>HotpotQA</td>
</tr>
<tr>
<td>gemini-1.5-flash</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Zero-Shot</td>
<td>0.851/0.103</td>
<td>0.439/0.045</td>
<td>1.386/0.110</td>
<td>0.690/0.048</td>
<td>5.603/0.023</td>
<td>1.119/0.019</td>
<td>2.222/0.018</td>
</tr>
<tr>
<td>Self-StreamICL</td>
<td>2.622/0.092</td>
<td>1.274/0.042</td>
<td>3.339/0.109</td>
<td>2.108/0.045</td>
<td>6.201/0.023</td>
<td>8.018/0.016</td>
<td>10.83/0.013</td>
</tr>
<tr>
<td>gpt-4o-2024-05-13</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Zero-Shot</td>
<td>0.717/0.079</td>
<td>0.377/0.031</td>
<td>1.359/0.082</td>
<td>0.586/0.067</td>
<td>5.171/0.021</td>
<td>1.088/0.011</td>
<td>2.145/0.019</td>
</tr>
<tr>
<td>Self-StreamICL</td>
<td>2.233/0.076</td>
<td>0.978/0.026</td>
<td>2.596/0.074</td>
<td>1.846/0.060</td>
<td>5.705/0.019</td>
<td>7.533/0.011</td>
<td>10.42/0.013</td>
</tr>
</tbody>
</table>
<h1>F Supplementary materials</h1>
<p>The supplementary materials include details such as preprocessing of datasets, prompts of each baseline method, and code to reproduce the experiments.</p>
<h2>F. 1 Code repository</h2>
<p>The code for reproducing the experiments can be found in our GitHub repository: https://github. com/stream-bench/stream-bench.</p>
<h2>F. 2 Details for each dataset</h2>
<p>We provide the licenses, preprocessing pipelines, calculation of evaluation metrics, and links to the datasets in StreamBench: https://huggingface.co/datasets/appier-ai-research/ StreamBench. To construct the streaming sequences, one only needs to download the datasets and follow the instructions in our code repository.</p>
<p>Table 14: Licenses of each dataset on StreamBench.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Text-to-SQL</th>
<th></th>
<th></th>
<th>Python</th>
<th>Tool Use</th>
<th>Medical</th>
<th>QA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Spider</td>
<td>CoSQL</td>
<td>BIRD</td>
<td>DS-1000</td>
<td>ToolBench</td>
<td>DDXPlus</td>
<td>HotpotQA</td>
</tr>
<tr>
<td>License</td>
<td>CC BY-SA</td>
<td>CC BY-SA</td>
<td>CC BY-SA</td>
<td>CC BY-SA</td>
<td>Apache-2.0</td>
<td>CC-BY</td>
<td>CC BY-SA</td>
</tr>
</tbody>
</table>
<h2>F.2.1 Spider</h2>
<p>Preprocessing We download the Spider [17] dataset from their project website: https:// yale-lily.github.io/spider, and use the original test set as our test set on StreamBench ${ }^{2}$.</p>
<p>Evaluation metric We adopt the commonly used Execution Accuracy (EA) for all three Text-toSQL datasets (Spider, CoSQL, and BIRD). This metric quantifies the proportion of instances where the execution result of the generated SQL $\hat{y}<em t="t">{t}$ is identical to that of the ground truth SQL $y</em>$ across all instances from time step $t=1$ to $T$, where $T$ is the number of instances in the test set:</p>
<p>$$
\mathrm{EA}=\frac{\sum_{t=1}^{T} \mathbb{1}\left(r_{t}, \hat{r}_{t}\right)}{T}
$$</p>
<p>Here, $r_{t}$ represents the execution results of $y_{t}$, while $\hat{r}<em t="t">{t}$ is the execution results of $\hat{y}</em>(\cdot)$ being the indicator function defined by:}$, with $\mathbb{1</p>
<p>$$
\mathbb{1}\left(r_{t}, \hat{r}<em t="t">{t}\right)= \begin{cases}1, &amp; r</em>}=\hat{r<em t="t">{t} \ 0, &amp; r</em>
$$} \neq \hat{r}_{t}\end{cases</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>F.2.2 CoSQL</h1>
<p>Preprocessing The CoSQL [18] dataset is sourced from its official website: https://yale-lily. github.io/cosql. Due to the unavailability of the official test set, we utilize the original development set as our test set on StreamBench. CoSQL was originally formatted in the multi-turn conversation structure, including a sequence of question-SQL pairs (i.e., $(x, y)$ pairs where $x$ is the user's natural language question and $y$ is the SQL code). To adapt CoSQL into the streaming framework of StreamBench, we extract each $(x, y)$ pair from the conversations to build the test set of size $T=1,007$.</p>
<h2>F.2.3 BIRD</h2>
<p>Preprocessing We download the BIRD [19] dataset from their project website https:// bird-bench.github.io/. Similar to COSQL, we use the full development set as our test set on StreamBench due to the unavailability of the original test set.</p>
<h2>F.2.4 DS-1000</h2>
<p>Preprocessing We use the DS-1000 [20] dataset on Huggingface: https://huggingface.co/ datasets/xlangai/DS-1000. Since this dataset only contains the test set, we manually construct few-shot examples for the few-shot baseline. The few-shot examples are available in our code repository. Please refer to Section F.1.</p>
<p>Evaluation metric DS-1000 adopts pass@1 as the evaluation metric, which denotes the proportion of instances where the agent's code solution $\hat{y}<em t="t">{t}$ pass all test cases of $x</em>$ for $t=1,2, \ldots, T$.</p>
<h2>F.2.5 ToolBench</h2>
<p>Preprocessing Since the original ToolBench [21] contains large-scale real online APIs suffering from instability, we adopt the 50 high-quality APIs curated by STE [22]. Each API has 15 test instances, so there are 750 instances in the test set.</p>
<p>Evaluation metric We following the same evaluation protocol specified in STE [22] to calculate the accuracy. The agent's output $\hat{y}_{t}$ is considered correct if and only if both the API name and API arguments are correct. The API name is checked by exact string matching. For APIs that have deterministic values for the API arguments, exact string matching is performed. For APIs that accept natural language inputs, a judge LLM is used to evaluate the correctness of API arguments. The implementation details can be found in our code repository (Section F.1).</p>
<h2>F.2.6 DDXPlus</h2>
<p>Preprocessing DDXPlus [23] is a large-scale dataset for medical diagnosis, and it contains more than 100,000 instances in the test set originally. Since it would be too expensive to run all test instances on LLMs, we sample equal number of instances from each medical diagnosis to make the test set of size $T=1,764$ on StreamBench. The full test set is available from the link provided in Section F.2, where $x$ is the patient profile and $y$ is the pathology (i.e., diagnosis). The original dataset can be found in the repository of DDXPlus: https://github.com/mila-iqia/ddxplus.</p>
<p>Evaluation metric We use accuracy as the evaluation metric, which is calculated as $\frac{\sum_{t=1}^{T} 1\left(\hat{y}<em t="t">{t}=y</em>$.}\right)}{T</p>
<h2>F.2.7 HotpotQA</h2>
<p>Preprocessing We adopt the distractor setting in HotpotQA [24], where each instance contains both supporting or distracting documents for answering the question. The supporting documents have the information needed to answer the question, while the distracting documents do not. Because the test set is not available, we construct the test set by sampling 1,500 instances randomly from the dev set (distractor) downloaded from the official website: https://hotpotqa.github.io/.</p>
<p>Evaluation metric Following the HotpotQA paper, we adopt exact match (EM) and F1 as two evaluation metrics. We use EM as the primary evaluation metric on StreamBench. However, we also include the calculation of F1 in our code.</p>
<h1>F. 3 Prompt templates</h1>
<p>We use similar prompt templates in all tasks to minimize prompt engineering. To demonstrate, the prompt templates of the Text-to-SQL task (Spider, CoSQL, and BIRD) as well as the medical diagnosis task (DDXPlus) are provided below. Note that the Self-Refine [26] prompting pipeline involves using the zero-shot prompt to generate the initial output, and then use the feedback prompt and refinement prompt alternatingly to arrive at the final answer. Therefore, we provide two prompt templates for the Self-Refine baseline, one for feedback and the other for refinement. The prompt templates for other datasets (DS-1000, ToolBench, and HotpotQA) can be found in our code repository.</p>
<h2>F.3.1 Text-to-SQL</h2>
<p>The prompt templates for Spider, CoSQL, and BIRD are provided below.</p>
<p>Zero-shot In this template, {schema} would be replaced by the database schema, while {question} would be replaced by the user's data requirements.</p>
<div class="codehilite"><pre><span></span><code>{schema}
</code></pre></div>

<ul>
<li>Using valid SQLite, answer the following question for the tables provided above.</li>
<li>Question: {question}</li>
</ul>
<p>Now, generate the correct SQL code directly (Do NOT generate other text except the SQL code):
Figure 11: The prompt template for the zero-shot baseline for Text-to-SQL datasets.</p>
<p>Chain-of-thought (CoT) It is similar to the zero-shot prompt template, except that the trigger phrase "take a deep breath and work on this problem step-by-step to derive the correct SQL code" is appended to the end.</p>
<div class="codehilite"><pre><span></span><code>{schema}
</code></pre></div>

<ul>
<li>Using valid SQLite, answer the following question for the tables provided above.</li>
<li>Question: {question}</li>
</ul>
<p>Now, take a deep breath and work on this problem step-by-step to derive the correct SQL code. Provide your output in the following format:
Rationale: <your_rationale>
Answer: ""sqlln<your_SQL_code>ln""</p>
<p>Figure 12: The prompt template for the CoT baseline for Text-to-SQL datasets.</p>
<p>Self-Refine The feedback prompt and refinement prompt are provided below.
You are performing the text-to-SQL task. Here is the database schema, user's question, and your previously generated SQL code.</p>
<ul>
<li>SQL schema: {schema}</li>
<li>User's question: {question}</li>
<li>Your SQL code: {model_output}</li>
</ul>
<p>First, determine whether you need to refine your SQL code in terms of its correctness.
If you consider that your SQL code is correct, output 'NO NEED TO REFINE' in uppercase. Otherwise, provide a suggestion to correct the SQL code.</p>
<p>Figure 13: The feedback prompt template for the Self-Refine baseline for Text-to-SQL datasets.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The current "Spider" on StreamBench refers to Spider 1.0. Please refer to their project website for details on the upcoming version of Spider.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
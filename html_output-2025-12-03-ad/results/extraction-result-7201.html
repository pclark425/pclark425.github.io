<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7201 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7201</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7201</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-442aa31e210a9582551ec61afcb79dbdfee92efb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/442aa31e210a9582551ec61afcb79dbdfee92efb" target="_blank">SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The fact that multiple SMILES represent the same molecule is explored as a technique for data augmentation of a molecular QSAR dataset modeled by a long short term memory (LSTM) cell based neural network.</p>
                <p><strong>Paper Abstract:</strong> Simplified Molecular Input Line Entry System (SMILES) is a single line text representation of a unique molecule. One molecule can however have multiple SMILES strings, which is a reason that canonical SMILES have been defined, which ensures a one to one correspondence between SMILES string and molecule. Here the fact that multiple SMILES represent the same molecule is explored as a technique for data augmentation of a molecular QSAR dataset modeled by a long short term memory (LSTM) cell based neural network. The augmented dataset was 130 times bigger than the original. The network trained with the augmented dataset shows better performance on a test set when compared to a model built with only one canonical SMILES string per molecule. The correlation coefficient R2 on the test set was improved from 0.56 to 0.66 when using SMILES enumeration, and the root mean square error (RMS) likewise fell from 0.62 to 0.55. The technique also works in the prediction phase. By taking the average per molecule of the predictions for the enumerated SMILES a further improvement to a correlation coefficient of 0.68 and a RMS of 0.52 was found.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7201.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7201.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simplified Molecular Input Line Entry System (SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear, character-based textual encoding of a molecule's graph that represents atoms, bonds, branches (parentheses) and ring closures (digits) as a sequential string; used here as the input sequence for LSTM QSAR models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A single-line sequence of characters where atoms are represented by element symbols, bonds implicitly or explicitly by characters, branching by parentheses and rings by numeric labels; the order of characters arises from a traversal/order of the molecular graph and encodes connectivity implicitly in the linear sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (lossless in principle but non-unique unless canonicalized)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph traversal / atom-order based serialization (SMILES generation depends on atom ordering; typical generators perform a depth-first-like traversal following an atom ordering), with parentheses for branches and digits for ring closures.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td>74</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Dihydrofolate reductase inhibitors dataset (Sutherland et al. 2003)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QSAR regression (predict log IC50 from SMILES sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM‑QSAR (LSTM sequence model feeding a final linear output neuron)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent LSTM network(s) trained on one-hot encoded SMILES character sequences (padded to length 74) with final state fed to a feed‑forward linear output; architectures optimized by Bayesian optimization (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pearson correlation R (reported as R or R^2 in text) and root mean square error (RMS)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Canonical-model on canonical SMILES (test): R = 0.56, RMS = 0.62; Enumerated-model on enumerated SMILES (test): R = 0.66, RMS = 0.55; Averaged enumerated-model predictions per molecule (test): R = 0.68, RMS = 0.52.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Using SMILES sequences as direct input allowed end-to-end learning; SMILES enumeration (see separate entity) increased sample diversity and improved generalization (higher test R and lower RMS) and produced smoother (less noisy) training curves compared with training on one canonical SMILES per molecule.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>SMILES strings are non‑unique without canonicalization (multiple textual variants per graph); the model must infer graph topology from sequence tokens (indirect representation) which may be harder than graph-native methods; requires padding/one‑hot encoding and increased sequence length; sensitivity to atom ordering unless canonicalized or enumerated for augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to CORAL-style fragment descriptors and graph-convolution approaches (which operate on graph topology directly), raw SMILES let the model learn task-specific features from sequence tokens; the paper notes graph-convolution models more directly read topology, whereas SMILES-based LSTM must infer topology via brackets and ring labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7201.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7201.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Canonical SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canonical SMILES (unique SMILES per molecule)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic variant of SMILES produced by applying a canonical atom-ordering algorithm so that each molecule maps to one unique SMILES string; used in the paper as the baseline (one SMILES per molecule).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards a universal smiles representation - a standard method to generate canonical smiles based on the inchi.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Canonical SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>SMILES generated under a deterministic canonical atom-ordering algorithm (e.g., InChI-based canonicalization) so the same molecule always yields the same character sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, canonical (lossless and deterministic by construction)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Deterministic canonical atom ordering followed by standard SMILES serialization (ensures one-to-one mapping molecule→SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td>74</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Dihydrofolate reductase inhibitors dataset (Sutherland et al. 2003)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QSAR regression (predict log IC50)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Canonical-model (LSTM‑QSAR optimized on canonical SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Best hyperparameters for canonical-model found by Bayesian optimization: 1 LSTM layer, 128 units, no dropout, strong L1/L2 regularization on dense layer, learning rate 0.0001 (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pearson correlation R and RMS</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Canonical-model predicting canonical SMILES: Train R = 0.78 (RMS 0.46), Test R = 0.56 (RMS 0.62).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Training on a dataset with only one canonical SMILES per molecule produced noisy training curves and worse generalization than the enumerated data approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Single canonical SMILES per molecule limits data diversity (smaller effective dataset size), leading to noisier training and worse test performance for the LSTM models in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Canonical SMILES provides deterministic input but was outperformed by models trained with SMILES enumeration (augmented non-canonical variants) in terms of test R and RMS in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7201.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7201.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES enumeration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES enumeration (randomized atom-order SMILES augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data augmentation technique that generates multiple different SMILES strings for the same molecular graph by randomizing atom order prior to SMILES serialization, increasing training examples and sampling diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Enumerated SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Multiple textual SMILES variants per molecule created by randomly scrambing atom order (via molfile atom reordering) and regenerating SMILES (non-canonical SMILES), producing diverse sequential encodings that all map to the same graph.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, augmentation technique (lossless per instance but non-unique across enumerations)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Random atom-order perturbation: convert to molfile, randomize atom ordering, convert back and generate non-canonical SMILES with RDKit; repeat to collect unique SMILES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td>74</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Dihydrofolate reductase inhibitors dataset (Sutherland et al. 2003) — augmented to ~130×</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QSAR regression (predict log IC50); also used for prediction-time sampling/averaging</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Enumerated-model (LSTM‑QSAR trained on enumerated SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM model optimized on enumerated dataset found by Bayesian optimization: 1 LSTM layer, 64 units, dropout_W 0.19, L1 0.005, L2 0.01, learning rate 0.005 (see Table 2). Training dataset grew from ~602 canonical train molecules to ~79,143 enumerated SMILES rows (avg ~130 SMILES/molecule).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pearson correlation R and RMS</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Enumerated-model on enumerated SMILES (test): R = 0.66, RMS = 0.55; Averaging enumerated-model predictions per molecule (test): R = 0.68, RMS = 0.52. Training MSE and loss also lower and training curves less noisy than canonical-training.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>SMILES enumeration increased effective dataset size (~130×), produced smoother training curves (less noise), enabled fitting a network with different regularization/hyperparameters and improved test-set generalization; averaging predictions across enumerated SMILES further improved accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Creates large expansion of training examples which increases number of weight updates per epoch and storage/compute needs (though wall-clock training time remained similar in this study); requires consistent aggregation strategy at prediction time (averaging used here); study lacks a separate validation split, so reported gains may be partially optimistic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperformed single-canonical-SMILES training in this study; suggested as complementary to SMILES-based autoencoders (could enlarge unsupervised training) and contrasted with graph-convolution approaches that consume graph structure directly rather than via multiple sequence variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7201.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7201.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORAL SMILES fragments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORAL SMILES-based fragment descriptors (Sk, SSk, SSSk etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that tokenizes/breaks SMILES into single-atom, two-atom and three-atom fragments plus manually coded features (BOND, NOSP, HALO, PAIR) to create descriptor vectors for QSAR modelling (implemented in CORAL software).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Coral software: prediction of carcinogenicity of drugs by means of the monte carlo method</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES fragment descriptors (CORAL)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Extracted SMILES substrings of length one, two and three atoms (Sk, SSk, SSSk) and additional hand-crafted tokens (BOND, NOSP, HALO, PAIR) used as features/descriptors rather than raw sequential SMILES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based, lossy (feature/descriptor extraction rather than full-sequence encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Substring/fragment extraction from canonical SMILES combined with manually coded global features; used to build descriptor vectors for Monte Carlo QSAR in CORAL.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Referenced large-scale QSAR studies (not evaluated in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QSAR modeling / carcinogenicity prediction in CORAL literature</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CORAL (Monte Carlo QSAR software)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Monte Carlo based QSAR approach using SMILES-fragment-derived descriptors and additional hand-coded tokens; not used experimentally in this paper but described for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not evaluated in this paper; mentioned as a different strategy that decomposes SMILES into engineered descriptors rather than end-to-end sequence learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Approach is more engineered and potentially more lossy than sequence models; closer to topological torsions and atom-pair fingerprints and thus may miss sequence-level patterns that LSTM can learn automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Paper contrasts CORAL's manual fragment-based descriptors with the end-to-end LSTM approach on raw SMILES, arguing LSTM may better learn task-specific features from raw sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7201.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7201.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES autoencoder (Gómez‑Bombarelli et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES-based neural network autoencoder for continuous molecular representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence SMILES autoencoder that reconstructs input SMILES through a bottleneck layer to produce continuous vector embeddings of molecules useful for chemical space exploration and property modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chemical design using a data-driven continuous representation of molecules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES (autoencoder input → continuous embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>SMILES sequences fed to an encoder-decoder neural architecture; the encoder compresses sequence information into a low-dimensional continuous bottleneck vector which can be decoded back to SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential → continuous embedding (lossy/abstracted representation)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Sequence-to-sequence autoencoder (neural encoder compresses SMILES into latent continuous vector; decoder reconstructs SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Large unlabeled molecular datasets used in Gómez‑Bombarelli et al.; not the DHFR dataset used in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Unsupervised representation learning / generative modeling (chemical design)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES autoencoder (seq2seq encoder-decoder with bottleneck)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoencoder architecture trained to reconstruct SMILES; bottleneck latent vectors used as continuous molecular representations for property prediction and molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Paper suggests SMILES enumeration could expand datasets for autoencoder training or allow training on smaller, focused labeled sets; enumeration might affect whether different SMILES of same molecule map to the same latent vector.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potential ambiguity whether different SMILES of same molecule are mapped to identical or widely different points in latent space; the current study notes this as an open question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>SMILES autoencoders produce continuous embeddings which differ conceptually from CORAL fragment descriptors and from graph-based convolutional fingerprints that use explicit graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules <em>(Rating: 2)</em></li>
                <li>Towards a universal smiles representation - a standard method to generate canonical smiles based on the inchi. <em>(Rating: 2)</em></li>
                <li>Coral software: prediction of carcinogenicity of drugs by means of the monte carlo method <em>(Rating: 2)</em></li>
                <li>Learning to SMILE(S) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7201",
    "paper_id": "paper-442aa31e210a9582551ec61afcb79dbdfee92efb",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "SMILES",
            "name_full": "Simplified Molecular Input Line Entry System (SMILES)",
            "brief_description": "A linear, character-based textual encoding of a molecule's graph that represents atoms, bonds, branches (parentheses) and ring closures (digits) as a sequential string; used here as the input sequence for LSTM QSAR models.",
            "citation_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules",
            "mention_or_use": "use",
            "representation_name": "SMILES",
            "representation_description": "A single-line sequence of characters where atoms are represented by element symbols, bonds implicitly or explicitly by characters, branching by parentheses and rings by numeric labels; the order of characters arises from a traversal/order of the molecular graph and encodes connectivity implicitly in the linear sequence.",
            "representation_type": "sequential, token-based (lossless in principle but non-unique unless canonicalized)",
            "encoding_method": "Graph traversal / atom-order based serialization (SMILES generation depends on atom ordering; typical generators perform a depth-first-like traversal following an atom ordering), with parentheses for branches and digits for ring closures.",
            "canonicalization": false,
            "average_token_length": 74,
            "dataset_name": "Dihydrofolate reductase inhibitors dataset (Sutherland et al. 2003)",
            "task_name": "QSAR regression (predict log IC50 from SMILES sequences)",
            "model_name": "LSTM‑QSAR (LSTM sequence model feeding a final linear output neuron)",
            "model_description": "Recurrent LSTM network(s) trained on one-hot encoded SMILES character sequences (padded to length 74) with final state fed to a feed‑forward linear output; architectures optimized by Bayesian optimization (see Table 2).",
            "performance_metric": "Pearson correlation R (reported as R or R^2 in text) and root mean square error (RMS)",
            "performance_value": "Canonical-model on canonical SMILES (test): R = 0.56, RMS = 0.62; Enumerated-model on enumerated SMILES (test): R = 0.66, RMS = 0.55; Averaged enumerated-model predictions per molecule (test): R = 0.68, RMS = 0.52.",
            "impact_on_training": "Using SMILES sequences as direct input allowed end-to-end learning; SMILES enumeration (see separate entity) increased sample diversity and improved generalization (higher test R and lower RMS) and produced smoother (less noisy) training curves compared with training on one canonical SMILES per molecule.",
            "limitations": "SMILES strings are non‑unique without canonicalization (multiple textual variants per graph); the model must infer graph topology from sequence tokens (indirect representation) which may be harder than graph-native methods; requires padding/one‑hot encoding and increased sequence length; sensitivity to atom ordering unless canonicalized or enumerated for augmentation.",
            "comparison_with_other": "Compared to CORAL-style fragment descriptors and graph-convolution approaches (which operate on graph topology directly), raw SMILES let the model learn task-specific features from sequence tokens; the paper notes graph-convolution models more directly read topology, whereas SMILES-based LSTM must infer topology via brackets and ring labels.",
            "uuid": "e7201.0",
            "source_info": {
                "paper_title": "SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Canonical SMILES",
            "name_full": "Canonical SMILES (unique SMILES per molecule)",
            "brief_description": "A deterministic variant of SMILES produced by applying a canonical atom-ordering algorithm so that each molecule maps to one unique SMILES string; used in the paper as the baseline (one SMILES per molecule).",
            "citation_title": "Towards a universal smiles representation - a standard method to generate canonical smiles based on the inchi.",
            "mention_or_use": "use",
            "representation_name": "Canonical SMILES",
            "representation_description": "SMILES generated under a deterministic canonical atom-ordering algorithm (e.g., InChI-based canonicalization) so the same molecule always yields the same character sequence.",
            "representation_type": "sequential, token-based, canonical (lossless and deterministic by construction)",
            "encoding_method": "Deterministic canonical atom ordering followed by standard SMILES serialization (ensures one-to-one mapping molecule→SMILES).",
            "canonicalization": true,
            "average_token_length": 74,
            "dataset_name": "Dihydrofolate reductase inhibitors dataset (Sutherland et al. 2003)",
            "task_name": "QSAR regression (predict log IC50)",
            "model_name": "Canonical-model (LSTM‑QSAR optimized on canonical SMILES)",
            "model_description": "Best hyperparameters for canonical-model found by Bayesian optimization: 1 LSTM layer, 128 units, no dropout, strong L1/L2 regularization on dense layer, learning rate 0.0001 (see Table 2).",
            "performance_metric": "Pearson correlation R and RMS",
            "performance_value": "Canonical-model predicting canonical SMILES: Train R = 0.78 (RMS 0.46), Test R = 0.56 (RMS 0.62).",
            "impact_on_training": "Training on a dataset with only one canonical SMILES per molecule produced noisy training curves and worse generalization than the enumerated data approach.",
            "limitations": "Single canonical SMILES per molecule limits data diversity (smaller effective dataset size), leading to noisier training and worse test performance for the LSTM models in this study.",
            "comparison_with_other": "Canonical SMILES provides deterministic input but was outperformed by models trained with SMILES enumeration (augmented non-canonical variants) in terms of test R and RMS in this study.",
            "uuid": "e7201.1",
            "source_info": {
                "paper_title": "SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "SMILES enumeration",
            "name_full": "SMILES enumeration (randomized atom-order SMILES augmentation)",
            "brief_description": "A data augmentation technique that generates multiple different SMILES strings for the same molecular graph by randomizing atom order prior to SMILES serialization, increasing training examples and sampling diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Enumerated SMILES",
            "representation_description": "Multiple textual SMILES variants per molecule created by randomly scrambing atom order (via molfile atom reordering) and regenerating SMILES (non-canonical SMILES), producing diverse sequential encodings that all map to the same graph.",
            "representation_type": "sequential, token-based, augmentation technique (lossless per instance but non-unique across enumerations)",
            "encoding_method": "Random atom-order perturbation: convert to molfile, randomize atom ordering, convert back and generate non-canonical SMILES with RDKit; repeat to collect unique SMILES strings.",
            "canonicalization": false,
            "average_token_length": 74,
            "dataset_name": "Dihydrofolate reductase inhibitors dataset (Sutherland et al. 2003) — augmented to ~130×",
            "task_name": "QSAR regression (predict log IC50); also used for prediction-time sampling/averaging",
            "model_name": "Enumerated-model (LSTM‑QSAR trained on enumerated SMILES)",
            "model_description": "LSTM model optimized on enumerated dataset found by Bayesian optimization: 1 LSTM layer, 64 units, dropout_W 0.19, L1 0.005, L2 0.01, learning rate 0.005 (see Table 2). Training dataset grew from ~602 canonical train molecules to ~79,143 enumerated SMILES rows (avg ~130 SMILES/molecule).",
            "performance_metric": "Pearson correlation R and RMS",
            "performance_value": "Enumerated-model on enumerated SMILES (test): R = 0.66, RMS = 0.55; Averaging enumerated-model predictions per molecule (test): R = 0.68, RMS = 0.52. Training MSE and loss also lower and training curves less noisy than canonical-training.",
            "impact_on_training": "SMILES enumeration increased effective dataset size (~130×), produced smoother training curves (less noise), enabled fitting a network with different regularization/hyperparameters and improved test-set generalization; averaging predictions across enumerated SMILES further improved accuracy.",
            "limitations": "Creates large expansion of training examples which increases number of weight updates per epoch and storage/compute needs (though wall-clock training time remained similar in this study); requires consistent aggregation strategy at prediction time (averaging used here); study lacks a separate validation split, so reported gains may be partially optimistic.",
            "comparison_with_other": "Outperformed single-canonical-SMILES training in this study; suggested as complementary to SMILES-based autoencoders (could enlarge unsupervised training) and contrasted with graph-convolution approaches that consume graph structure directly rather than via multiple sequence variants.",
            "uuid": "e7201.2",
            "source_info": {
                "paper_title": "SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "CORAL SMILES fragments",
            "name_full": "CORAL SMILES-based fragment descriptors (Sk, SSk, SSSk etc.)",
            "brief_description": "An approach that tokenizes/breaks SMILES into single-atom, two-atom and three-atom fragments plus manually coded features (BOND, NOSP, HALO, PAIR) to create descriptor vectors for QSAR modelling (implemented in CORAL software).",
            "citation_title": "Coral software: prediction of carcinogenicity of drugs by means of the monte carlo method",
            "mention_or_use": "mention",
            "representation_name": "SMILES fragment descriptors (CORAL)",
            "representation_description": "Extracted SMILES substrings of length one, two and three atoms (Sk, SSk, SSSk) and additional hand-crafted tokens (BOND, NOSP, HALO, PAIR) used as features/descriptors rather than raw sequential SMILES strings.",
            "representation_type": "token-based, lossy (feature/descriptor extraction rather than full-sequence encoding)",
            "encoding_method": "Substring/fragment extraction from canonical SMILES combined with manually coded global features; used to build descriptor vectors for Monte Carlo QSAR in CORAL.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Referenced large-scale QSAR studies (not evaluated in this paper's experiments)",
            "task_name": "QSAR modeling / carcinogenicity prediction in CORAL literature",
            "model_name": "CORAL (Monte Carlo QSAR software)",
            "model_description": "Monte Carlo based QSAR approach using SMILES-fragment-derived descriptors and additional hand-coded tokens; not used experimentally in this paper but described for comparison.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Not evaluated in this paper; mentioned as a different strategy that decomposes SMILES into engineered descriptors rather than end-to-end sequence learning.",
            "limitations": "Approach is more engineered and potentially more lossy than sequence models; closer to topological torsions and atom-pair fingerprints and thus may miss sequence-level patterns that LSTM can learn automatically.",
            "comparison_with_other": "Paper contrasts CORAL's manual fragment-based descriptors with the end-to-end LSTM approach on raw SMILES, arguing LSTM may better learn task-specific features from raw sequences.",
            "uuid": "e7201.3",
            "source_info": {
                "paper_title": "SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "SMILES autoencoder (Gómez‑Bombarelli et al.)",
            "name_full": "SMILES-based neural network autoencoder for continuous molecular representation",
            "brief_description": "A sequence-to-sequence SMILES autoencoder that reconstructs input SMILES through a bottleneck layer to produce continuous vector embeddings of molecules useful for chemical space exploration and property modeling.",
            "citation_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "mention_or_use": "mention",
            "representation_name": "SMILES (autoencoder input → continuous embedding)",
            "representation_description": "SMILES sequences fed to an encoder-decoder neural architecture; the encoder compresses sequence information into a low-dimensional continuous bottleneck vector which can be decoded back to SMILES.",
            "representation_type": "sequential → continuous embedding (lossy/abstracted representation)",
            "encoding_method": "Sequence-to-sequence autoencoder (neural encoder compresses SMILES into latent continuous vector; decoder reconstructs SMILES).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Large unlabeled molecular datasets used in Gómez‑Bombarelli et al.; not the DHFR dataset used in this paper",
            "task_name": "Unsupervised representation learning / generative modeling (chemical design)",
            "model_name": "SMILES autoencoder (seq2seq encoder-decoder with bottleneck)",
            "model_description": "Autoencoder architecture trained to reconstruct SMILES; bottleneck latent vectors used as continuous molecular representations for property prediction and molecule generation.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Paper suggests SMILES enumeration could expand datasets for autoencoder training or allow training on smaller, focused labeled sets; enumeration might affect whether different SMILES of same molecule map to the same latent vector.",
            "limitations": "Potential ambiguity whether different SMILES of same molecule are mapped to identical or widely different points in latent space; the current study notes this as an open question.",
            "comparison_with_other": "SMILES autoencoders produce continuous embeddings which differ conceptually from CORAL fragment descriptors and from graph-based convolutional fingerprints that use explicit graph structure.",
            "uuid": "e7201.4",
            "source_info": {
                "paper_title": "SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2
        },
        {
            "paper_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules",
            "rating": 2
        },
        {
            "paper_title": "Towards a universal smiles representation - a standard method to generate canonical smiles based on the inchi.",
            "rating": 2
        },
        {
            "paper_title": "Coral software: prediction of carcinogenicity of drugs by means of the monte carlo method",
            "rating": 2
        },
        {
            "paper_title": "Learning to SMILE(S)",
            "rating": 2
        }
    ],
    "cost": 0.013212,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules</h1>
<p>Esben Jannik Bjerrum*</p>
<p>Wildcard Pharmaceutical Consulting, Frødings Allé 41, 2860 Søborg, Denmark
*) esben@wildcardconsulting.dk</p>
<h4>Abstract</h4>
<p>Simplified Molecular Input Line Entry System (SMILES) is a single line text representation of a unique molecule. One molecule can however have multiple SMILES strings, which is a reason that canonical SMILES have been defined, which ensures a one to one correspondence between SMILES string and molecule. Here the fact that multiple SMILES represent the same molecule is explored as a technique for data augmentation of a molecular QSAR dataset modeled by a long short term memory (LSTM) cell based neural network. The augmented dataset was 130 times bigger than the original. The network trained with the augmented dataset shows better performance on a test set when compared to a model built with only one canonical SMILES string per molecule. The correlation coefficient R2 on the test set was improved from 0.56 to 0.66 when using SMILES enumeration, and the root mean square error (RMS) likewise fell from 0.62 to 0.55 . The technique also works in the prediction phase. By taking the average per molecule of the predictions for the enumerated SMILES a further improvement to a correlation coefficient of 0.68 and a RMS of 0.52 was found.</p>
<h2>Introduction</h2>
<p>Neural networks and deep learning has shown interesting application successes, such as image classification[1], and speech recognition[2]. One of the issues that limits their general applicability in the QSAR domain may be the limited sizes of the labeled datasets available, although successes do appear.[3] Limited datasets necessitates harsh regularization or shallow and narrow architectures. Within image analysis and classification, data augmentation techniques has been used with excellent results. $[4,5,6,7]$ As an example, a dataset of labeled images can be enlarged by operations such as mirroring, rotation, morphing and zooming. The afterwards trained network gets more robust towards such variations and the neural network can recognize the same object in different versions.</p>
<p>Neural networks has also been used on molecular data, where the input may be calculated descriptors,[3] neural network interpretation of the molecular graph[8] or also SMILES representations.[9] Simplified Molecular Input Line Entry System (SMILES) is a single line text based molecular notation format.[10] A single molecule has multiple possible SMILES strings, which has led to the definition of a canonical SMILES,[11] which ensures that a molecule corresponds to a single SMILES string. The possibilities for variation in the SMILES strings of simple molecules are limited. Propane has two possibilities CCC and $\mathrm{C}(\mathrm{C}) \mathrm{C}$. But as the molecule gets larger in size and more complex in branching,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SMILES enumeration enables data augmentation. The molecule toluene corresponds to seven different SMILES, the top one is the canonical smile. One data point with toluene in the dataset would thus leads to seven samples in the augmented dataset.
the number of possible SMILES strings grows rapidly. Toluene with seven atoms, has seven possible SMILES strings (Figure 1).</p>
<p>Here data augmentation of molecular structures with SMILES enumeration for QSAR studies will be investigated using long short term memory (LSTM) cell neural networks inspired by networks used for Twitter tweets sentiment analysis.[12]</p>
<h2>Methods</h2>
<h2>SMILES enumeration</h2>
<p>SMILES enumeration was done with a Python script utilizing the cheminformatics library RDKit.[13] The atom ordering of the molecule is scrambled randomly by converting to molfile format[14] and changing the atom order, before converting back to the RDKit mol format. A SMILES is then generated using RDKit with the default option of producing canonical SMILES set to false, where different atom orderings lead to different SMILES. The SMILES strings is then compared and possible added to a growing set of unique SMILES strings. The process is repeated a predefined number of times. The python functions are available on github: https://github.com/Ebjerrum/SMILES-enumeration</p>
<h2>Molecular dataset</h2>
<p>The dataset was obtained from Sutherland et al 2003.[15] It consists of 756 dihydrofolate inhibitors with P. carinii DHFR inhibition data. The dataset was split in test and a training set in a 1:9 ratio. It was expanded with SMILES enumeration and the SMILES strings were padded with spaces to fixed length of 74 , which is one characters longer than the longest SMILES in the dataset. It was subsequently vectorized by one-hot encoding the characters into a bit matrix with one bit set for the corresponding character in each row using a generated char to int dictionary. Molecules where the associated affinity was not a number were removed. The associated IC50 data was converted to $\log$ IC50 and normalized to unit variance and mean zero with utilities from Scikitlearn.[16]</p>
<h2>LSTM neural network</h2>
<p>Two different neural networks were built and trained using Keras version 1.1.2[17] with Theano v. 0.8.0[18] as back end. One or more LSTM layers were used in batch mode, and the final state fed to a feed-forward neural network with a single linear output neuron. The network layout was optimized using Bayesian optimization with Gaussian processes as implemented in the Python package GpyOpt[19] version 1.0.3, varying the hyper parameters listed in Table 1. 10 initial trainings was done before using the GP_MCMC and the EI_MCMC acquisition function to sample new hyper parameter sets.[20] One network was optimized and trained only using a dataset with canonical SMILES, whereas the other were optimized and trained with the dataset that expanded with SMILES enumeration. In the rest of the publication they will be referred to as the canonical model and enumerated model, respectively.</p>
<p>All computations and training were done on a Linux workstation (Ubuntu Mate 16.04) with 4 GB of
ram, i5-2405S CPU @ 2.50GHz and an Nvidia Geforce GTX1060 graphics card with 6 GB of ram.</p>
<h2>Results</h2>
<p>Filtering, splitting and SMILES enumeration resulted in a canonical SMILES dataset with 602 train molecules and 71 test molecules, whereas the enumerated dataset had 79143 and 9412 rows for train and test, respectively. This corresponds to an augmentation factor of approximately 130. Each molecule had on average 130 alternative SMILES representations. Optimization of the architecture yielded two different best configurations of hyper parameters, depending on the dataset used. The best hyper parameters found for each dataset are shown in Table 2.</p>
<p>The train history is shown in Figure 2. The best neural network trained on the canonical dataset had a loss of 0.44 including regularization penalty and a mean square error of 0.22 and 0.41 for train and test set, respectively. The curves for the training using the canonical dataset are very noisy (Figure 2A). The best neural network trained on the enumerated dataset loss of 0.18 including regularization penalty and a mean square error of 0.09 and 0.30 for train and test set, respectively. The training curve is significantly less noisy than for the canonical dataset (Figure 2B).</p>
<p>Both neural networks were used to predict the IC50 values from the canonical and enumerated datasets, and the scatter plots are shown in Figure 3.</p>
<p>The correlation coefficients and root mean square deviation (RMS) are tabulated in Table 3. The combination with the worst performance was predicting the test set molecules is using enumerated SMILES neural network model trained on the canonical dataset. Which has a correlation coefficient of 0.26 and an RMS of 0.84 . The bad correlation is clearly visible from Figure 3 plot C. The best performance predicting the test set, was seen with the combination of the enumerated model and the enumerated SMILES. Here the correlation coefficient is 0.66 and the RMS 0.55 . The two other combinations, canonical model-canonical SMILES and enumerated model, canonical SMILES are close in performance (Table 3). ${ }^{2}$</p>
<p>Figure 4 show a scatter plot of the average prediction for each molecule obtained with the enumerated model. The calculated correlation coefficient is 0.68 for the test set and the RMS is 0.52 .</p>
<h2>Discussion</h2>
<p>The results clearly suggest that SMILES enumeration as a data augmentation technique for molecular data has benefits. The model trained on canonical data is not able to predict many of the alternative SMILES of the train and test set as is evident for</p>
<p>Table 1: Hyper parameter Search Space</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Search Space</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of LSTM layers</td>
<td>$[1,2]$</td>
<td>Discrete</td>
</tr>
<tr>
<td>Number of units in LSTM layers</td>
<td>$[32,64,128,256]$</td>
<td>Discrete</td>
</tr>
<tr>
<td>Dropout for input gates (dropout_W)</td>
<td>$0-0.2$</td>
<td>Continuous</td>
</tr>
<tr>
<td>Dropout for recurrent connection (dropout_U)</td>
<td>$0-0.5$</td>
<td>Continuous</td>
</tr>
<tr>
<td>Number of dense hidden layers</td>
<td>$[0,1]$</td>
<td>Discrete</td>
</tr>
<tr>
<td>Hidden layer size</td>
<td>$[4,8,16,32,64,128]$</td>
<td>Discrete</td>
</tr>
<tr>
<td>Weight regularization on dense layer, L1</td>
<td>$0-0.2$</td>
<td>Continuous</td>
</tr>
<tr>
<td>Weight regularization on dense layer, L2</td>
<td>$0-0.2$</td>
<td>Continuous</td>
</tr>
<tr>
<td>Learning rate</td>
<td>$0.05-0.0001$</td>
<td>Continuous</td>
</tr>
</tbody>
</table>
<p>Table 2: Best Hyperparameters found</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Canonical Model</th>
<th>Enumerated Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of LSTM layers</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>Number of units in LSTM layers</td>
<td>128</td>
<td>64</td>
</tr>
<tr>
<td>Dropout for input gates (dropout_W)</td>
<td>0.0</td>
<td>0.19</td>
</tr>
<tr>
<td>Dropout for recurrent connections (dropout_U)</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>Number of dense hidden layers</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Hidden layer size</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>Weight regularization on dense layer, L1</td>
<td>0.2</td>
<td>0.005</td>
</tr>
<tr>
<td>Weight regularization on dense layer, L2</td>
<td>0.2</td>
<td>0.01</td>
</tr>
<tr>
<td>Learning rate</td>
<td>0.0001</td>
<td>0.005</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Training history for the two datasets and neural networks. A: Neural network trained on canonical SMILES shows a noisy curve where the best model has a test loss of 0.41. B: Neural network trained on enumerated SMILES obtains the best model with a test loss of 0.30. Blue lines are the mean square error without regularization penalty, green is loss including regularization penalty and the red line is mean square error on the test set.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Scatter plots of predicted vs. true values. Left column shows scatter plots obtained with the model trained on canonical SMILES only. Right column shows predictions with the model trained on enumerated data. Top row is scatter plots with only canonical SMILES and bottom row is predictions of the enumerated dataset. The blue line denotes the perfect correlation (y = x).</p>
<p>Table 3: Statistics of predicted values, values are for Train/Test set respectively</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th></th>
<th>Canonical Model</th>
<th></th>
<th>Enumerated Model</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>R</td>
<td>RMS</td>
<td>R</td>
<td>RMS</td>
</tr>
<tr>
<td>Canonical</td>
<td>Train</td>
<td>0.78</td>
<td>0.46</td>
<td>0.85</td>
<td>0.39</td>
</tr>
<tr>
<td></td>
<td>Test</td>
<td>0.56</td>
<td>0.62</td>
<td>0.63</td>
<td>0.56</td>
</tr>
<tr>
<td>Enumerated</td>
<td>Train</td>
<td>0.25</td>
<td>0.88</td>
<td>0.87</td>
<td>0.37</td>
</tr>
<tr>
<td></td>
<td>Test</td>
<td>0.26</td>
<td>0.84</td>
<td>0.66</td>
<td>0.55</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average of predictions from the enumerated model for each molecule. Train set R2 is 0.88 and RMS is 0.38. Test set R2 is 0.68 and RMS is 0.52.</p>
<p>Figure 3 plot C, where the bad generalization to non canonical SMILES strings are evident. Instead the best performance was observed by taking the average for each molecule of the predictions of the enumerated SMILES using the enumerated model (Figure 4), which shows that the SMILES enumeration can also be of value in the sampling phase. The canonical model needed a lot more epochs to train, but here it must be considered that the dataset contained 130 times fewer examples. Thus each epoch in the training was only 3 mini batches leading to 3 updates of the weights, whereas the enumerated dataset had approximately 360 updates of the weights of the neural network per epoch. The curves in Figure 2 thus represent 3000 and 18000 updates of the weights. The higher overhead of running more epochs however led to approximately the same wall clock time in training. The hyper parameters found during the optimization of the network architecture and amount of regularization was not entirely as expected. The expectation was that the canonical dataset would prefer a smaller and simpler network with a larger regularization. Instead the canonical dataset has a larger amount of LSTM cells (128) with no dropout, but a much larger regularization of the final weights to the input neuron (L1 and L2 maxed out at 0.2). The enumerated model had fewer LSTM cells (64) and thus fewer connections, but nevertheless found dropout on the input to the LSTM cells to be beneficial. To test if the differences were due to the Bayesian optimization getting trapped in a local minimum, the network architecture found for the enumerated dataset was test trained with the dataset with the canonical SMILES only. The first try with a learning rate of 0.005 failed (results not shown), but lowering the learning rate to the one found for the canonical SMILES (0.0001), gave a model with a correlation coefficient of 0.5 and RMS of 0.68 on the train set. The predictive performance was even lower with 0.45 and 0.69, for R2 and RMS respectively. The differences in hyper parameters after optimization of using the two different datasets thus seem justified. The study lacks the division into train, test and validation set, where the hyper parameters are tuned on the test set, but the final performance evaluated on the validation set. The observed prediction performance of the LSTM-QSAR models are thus likely overestimated to some degree. However, this study is focused on the gains of using SMILES enumeration and not on producing the optimal DHFR QSAR model. The performance on both the train and test set are lower for the canonical model. If the differences in performance had been due to over-fitting, the smaller dataset would probably have had an advantage.</p>
<p>The use of SMILES as descriptors for QSAR is not new[21, 22, 21] and is as an example implemented in the CORAL software.[23] The approach in the CORAL software is however very different from the one in this study. CORAL software breaks down the SMILES into single atoms, double atoms and triple atoms (Sk, SSk and SSSk) as well as some extra manually coded extracted features such as BOND, NOSP, HALO and PAIR.[23, 21] The approach seems close to using a mixture of topological torsions[24] with one, two and three atoms and atom-pair[25] fingerprints. The LSTM-QSAR used in this approach directly uses the SMILES string and supposedly let the model best extract the features from the SMILES strings that best fit with the task at hand, and similar approaches have been shown to outperform other common machine learning algorithms[22], although the details of optimization of the competing algorithms were not completely clear.</p>
<p>SMILES were also used recently in an application of a neural network based auto-encoder.[9] Here the SMILES are used as input to a neural network with the task of recreating the input sequence. The information is passed through a "bottle-neck" layer in between the encoder and the decoder, which limits the direct transfer of information. The bottle neck layer thus ends up as a more continuous floating point vector representation of the molecule, which can be used to explore the chemical space near an input molecule, interpolate between molecules and link the vector representation to physico-chemical properties. The amount of unlabeled molecules for the study already surpassed the needed amount, but could in principle be expanded even more with the SMILES enumera-</p>
<p>tion technique described here. SMILES enumeration could possible allow the autoencoder to be trained with smaller and more focused datasets of biological interest. Additionally, tt would be interesting to see if different SMILES of the same molecule ends up with the same vector representation or in entirely different areas in the continuous molecular representations.</p>
<p>LSTM networks have also been used in QSAR applications demonstrating learning transfer from large datasets to smaller. [26] Here the input was however not SMILES strings but rather molecular graph convolution layers[27] working directly on the molecular graph representation. The approach thus more directly reads in the topology of the molecular model, rather than indirectly letting the network infer the topology from the SMILES branching and ring closures defined by the brackets and numbering in the SMILES strings.</p>
<h2>Conclusion</h2>
<p>This short investigation has shown promise in using SMILES enumeration as a data augmentation technique for neural network QSAR models based on SMILES data. SMILES enumeration enables the use of more limited sizes of labeled data sets for use in modeling by more complex neural network models. SMILES enumeration gives more robust QSAR models both when predicting single SMILES, but even more when taking the average prediction using enumerated SMILES for the same molecule. The SMILES enumeration code as well as some of the scripts used for generating the LSTM-QSAR models are available on GitHub: https://github.com/Ebjerrum/SMILES-enumeration</p>
<h2>Conflicts of Interest</h2>
<p>E. J. Bjerrum is the owner of Wildcard Pharmaceutical Consulting. The company is usually contracted by biotechnology/pharmaceutical companies to provide third party services</p>
<h2>References</h2>
<p>[1] P. Y. Simard, D. Steinkraus, J. C. Platt, et al., Best practices for convolutional neural networks applied to visual document analysis., in: ICDAR, Vol. 3, Citeseer, 2003, pp. 958-962.
[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups, IEEE Signal Processing Magazine 29 (6) (2012) $82-97$.
[3] A. Mayr, G. Klambauer, T. Unterthiner, S. Hochreiter, Deeptox: toxicity prediction using deep learning, Frontiers in Environmental Science 3 (2016) 80.
[4] A.-D. Almási, S. Woźniak, V. Cristea, Y. Leblebici, T. Engbersen, Review of advances in neural networks: Neural design technology stack, Neurocomputing 174 (2016) $31-41$.
[5] K. Chatfield, K. Simonyan, A. Vedaldi, A. Zisserman, Return of the devil in the details: Delving deep into convolutional nets, arXiv preprint arXiv:1405.3531.
[6] X. Cui, V. Goel, B. Kingsbury, Data augmentation for deep neural network acoustic modeling, IEEE/ACM Trans. Audio, Speech and Lang. Proc. 23 (9) (2015) 1469-1477. doi:10.1109/TASLP.2015.2438544.
URL http://dx.doi.org/10.1109/TASLP. 2015.2438544
[7] J. Schmidhuber, Deep learning in neural networks: An overview, Neural networks 61 (2015) $85-117$.
[8] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, P. Riley, Molecular graph convolutions: moving beyond fingerprints, Journal of computer-aided molecular design 30 (8) (2016) 595-608.
[9] R. Gómez-Bombarelli, D. Duvenaud, J. M. Hernández-Lobato, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, A. Aspuru-Guzik, Automatic chemical design using a data-driven continuous representation of molecules, arXiv preprint arXiv:1610.02415.
[10] D. Weininger, Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules, in: Proc. Edinburgh Math. SOC, Vol. 17, 1970, pp. 1-14.
[11] N. M. O'Boyle, Towards a universal smiles representation - a standard method to generate canonical smiles based on the inchi., Journal of cheminformatics 4 (2012) 22. doi:10.1186/ 1758-2946-4-22.
[12] D. Tang, B. Qin, X. Feng, T. Liu, Targetdependent sentiment classification with long short term memory, CoRR, abs/1512.01100.
[13] G. A. Landrum, Rdkit: Open-source cheminformatics software (2016).
URL http://www.rdkit.org/,https: //github.com/rdkit/rdkit
[14] Ctfile formats, http://accelrys.com/products/informatics/cheminformatics/ctfile-formats/no-fee.php (Dec 2011).</p>
<p>URL http://accelrys.com/products/ informatics/cheminformatics/ ctfile-formats/no-fee.php
[15] J. J. Sutherland, L. A. O'Brien, D. F. Weaver, Spline-fitting with a genetic algorithm: a method for developing classification structure-activity relationships., Journal of chemical information and computer sciences 43 (2003) 1906-1915. doi: 10.1021/ci034143r.
[16] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine Learning Research 12 (2011) 2825-2830.
[17] F. Chollet, keras, https://github.com/fchollet/keras (2015).
[18] R. Al-Rfou, G. Alain, A. Almahairi, C. Angermueller, D. Bahdanau, N. Ballas, F. Bastien, J. Bayer, A. Belikov, A. Belopolsky, Y. Bengio, A. Bergeron, J. Bergstra, V. Bisson, J. Bleecher Snyder, N. Bouchard, N. BoulangerLewandowski, X. Bouthillier, A. de Brébisson, O. Breuleux, P.-L. Carrier, K. Cho, J. Chorowski, P. Christiano, T. Cooijmans, M.-A. Côté, M. Côté, A. Courville, Y. N. Dauphin, O. Delalleau, J. Demouth, G. Desjardins, S. Dieleman, L. Dinh, M. Ducoffe, V. Dumoulin, S. Ebrahimi Kahou, D. Erhan, Z. Fan, O. Firat, M. Germain, X. Glorot, I. Goodfellow, M. Graham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. Heng, B. Hidasi, S. Honari, A. Jain, S. Jean, K. Jia, M. Korobov, V. Kulkarni, A. Lamb, P. Lamblin, E. Larsen, C. Laurent, S. Lee, S. Lefrancois, S. Lemieux, N. Léonard, Z. Lin, J. A. Livezey, C. Lorenz, J. Lowin, Q. Ma, P.-A. Manzagol, O. Mastropietro, R. T. McGibbon, R. Memisevic, B. van Merriënboer, V. Michalski, M. Mirza, A. Orlandi, C. Pal, R. Pascanu, M. Pezeshki, C. Raffel, D. Renshaw, M. Rocklin, A. Romero, M. Roth, P. Sadowski, J. Salvatier, F. Savard, J. Schlüter, J. Schulman, G. Schwartz, I. V. Serban, D. Serdyuk, S. Shabanian, E. Simon, S. Spieckermann, S. R. Subramanyam, J. Sygnowski, J. Tanguay, G. van Tulder, J. Turian, S. Urban, P. Vincent, F. Visin, H. de Vries, D. Warde-Farley, D. J. Webb, M. Willson, K. Xu, L. Xue, L. Yao, S. Zhang, Y. Zhang, Theano: A Python framework for fast computation of mathematical expressions, arXiv e-prints abs/1605.02688.
URL http://arxiv.org/abs/1605.02688
[19] T. G. authors, Gpyopt: A bayesian optimization framework in python,
http://github.com/SheffieldML/GPyOpt (2016).
[20] C. Wang, R. M. Neal, Mcmc methods for gaussian process models using fast approximations for the likelihood, arXiv preprint arXiv:1305.2235.
[21] A. Worachartcheewan, P. Mandi, V. Prachayasittikul, A. P. Toropova, A. A. Toropov, C. Nantasenamat, Large-scale qsar study of aromatase inhibitors using smiles-based descriptors, Chemometrics and Intelligent Laboratory Systems 138 (2014) 120-126.
[22] S. Jastrzebski, D. Lesniak, W. M. Czarnecki, Learning to SMILE(S), CoRR abs/1602.06289. URL http://arxiv.org/abs/1602.06289
[23] A. P. Toropova, A. A. Toropov, Coral software: prediction of carcinogenicity of drugs by means of the monte carlo method, European Journal of Pharmaceutical Sciences 52 (2014) 21-25.
[24] R. Nilakantan, N. Bauman, J. S. Dixon, R. Venkataraghavan, Topological torsion: a new molecular descriptor for sar applications. comparison with other descriptors, Journal of Chemical Information and Computer Sciences 27 (2) (1987) $82-85$.
[25] R. E. Carhart, D. H. Smith, R. Venkataraghavan, Atom pairs as molecular features in structureactivity studies: definition and applications, Journal of Chemical Information and Computer Sciences 25 (2) (1985) 64-73.
[26] H. Altae-Tran, B. Ramsundar, A. S. Pappu, V. Pande, Low data drug discovery with one-shot learning, arXiv preprint arXiv:1611.03199.
[27] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, R. P. Adams, Convolutional networks on graphs for learning molecular fingerprints, in: Advances in neural information processing systems, 2015, pp. $2224-2232$.</p>            </div>
        </div>

    </div>
</body>
</html>
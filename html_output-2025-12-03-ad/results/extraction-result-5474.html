<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5474 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5474</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5474</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-c11810fa8887b678facea62da4607c4898360308</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c11810fa8887b678facea62da4607c4898360308" target="_blank">Training Language Models with Language Feedback at Scale</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback, is introduced and it is shown theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback.</p>
                <p><strong>Paper Abstract:</strong> Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5474.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5474.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ILF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imitation learning from Language Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative refine-and-finetune algorithm that uses human natural-language feedback to generate candidate refinements, selects the best refinement via a scoring function, and finetunes the original LM on the selected refinement; repeatable across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci / text-davinci-001 'FeedME' for refinement; GPT3-175B finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses OpenAI GPT-3 family models. Key models: text-davinci-001 (instruction-finetuned, 'FeedME') for generating refinements and scoring; GPT-3 Davinci (175B params) is finetuned (via OpenAI API) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ILF (iterative refine-and-finetune)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each context c: sample an initial output x0 from the LM, collect human-written feedback f on (c,x0), condition a refinement LM (π_ψ, here FeedME) on (c,x0,f) to generate multiple candidate refinements {x1^i}, score each candidate with a reward/scoring model R (implemented as an instruction-tuned LM answering a binary question or as an RM) and pick the best x1*, then finetune the original LM π_θ to maximize likelihood of x1* given c. Repeat for K iterations (Algorithm 1).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization (TL;DR Reddit dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate short summaries (≤48 tokens) of Reddit posts/titles; human feedback annotators write the most important shortcoming and an ideal summary; used for evaluating improvement from language feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Finetuning on refinements produced by ILF (5K samples) achieved a win rate vs human-written reference summaries of 31.3 ± 1.7% (test set of 698, human evaluation). Combining ILF finetuned model with an OPT reward model for best-of-64 sampling (ILF + OPT-RM best-of-64) reached a win rate of 50.8 ± 1.9% (human-level).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Finetuning on initial summaries (no feedback-refinement) achieved a win rate vs human summaries of 27.3 ± 1.7% (5K). Finetuning on human-written summaries achieved 28.9 ± 1.7%. FeedME (instruction-tuned base sampling) achieved 22.5 ± 1.6%. (All numbers from the paper, 5K regime.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative human-evaluation win rates: ILF finetuning on refinements outperformed finetuning on initial summaries and finetuning on human summaries at the same data sizes (e.g., 31.3% vs 28.9% and 27.3% in the 5K case). Combining ILF with binary-feedback best-of-N ranking further improved quality up to ~50.8% win rate vs human references. Additional evidence: models finetuned on refinements had lower validation loss on refinement validation data and higher (reverse) KL divergence indicating substantive model change correlated with better human preference scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires very large LMs to reliably incorporate feedback (only 175B models showed non-negligible refinement ability in targeted experiments). ILF depends on quality of human feedback and on a reliable scoring function R; the paper assumes q(x1) constant for importance sampling approximations. Multiple-iteration ILF results are preliminary (App. H.3) and need more study. Finetuning on refinements produced larger KL divergence from the pretrained LM, which may raise concerns about distribution shift or undesirable behaviors. Computational cost: Best-of-N sampling and scoring add inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Language Models with Language Feedback at Scale', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5474.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5474.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ILF (targeted word removal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ILF validation on targeted offensive-word removal task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled experiment testing an LM's ability to incorporate language feedback to remove specified offensive words from a sentence; used to determine which models can meaningfully refine outputs given feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (Ada, Babbage, Curie, Davinci) and instruction-finetuned FeedME variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 variants from smallest (Ada) up to Davinci (175B). FeedME denotes instruction-finetuned versions (text-davinci-001).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Refinement conditioned on feedback (single-step refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt models with an initial sentence containing offensive words and explicit instruction to remove specified words (the human feedback is effectively the instruction to remove particular tokens); generate a single refinement (greedy decoding) and measure exact string match to the target.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Targeted word removal (synthetic offensive-word removal)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a sentence containing up to 10 offensive words and a target list of ≤3 words to remove, generate a refined sentence that exactly matches the target sentence (automatically generated ground truth).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Only the largest models performed non-negligibly: GPT-3 Davinci (175B) achieved 38.5 ± 1.3% exact match; FeedME Davinci (instruction-tuned 175B) achieved 35.8 ± 1.3%. Smaller models (Curie 6.7B, Babbage 1B, Ada) performed near random/very low (e.g., Curie ≈ 6–8%, Babbage ≈ 1–2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not applicable in the same way; baseline is generation without conditioning on the feedback/instruction, which would be expected to fail the task. The paper's comparison is across model scales (with feedback instruction supplied).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Clear scaling result: only very large models (175B) can reliably incorporate fine-grained language feedback to perform exact targeted edits; this motivated using 175B models for downstream ILF experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller models fail to incorporate feedback for this controlled editing task; thus ILF and refinement-based approaches may not generalize to smaller architectures or sizes. Experiment used greedy decoding and single-step refinement; multiple samples or better decoding might change results but were not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Language Models with Language Feedback at Scale', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5474.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5474.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructRM Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction-finetuned Reward Model Ensemble (LM-based scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scoring method that uses an instruction-finetuned LM to answer a binary question ('Does this new text incorporate the feedback? Yes/No') across multiple prompts and averages the positive-answer probabilities to score candidate refinements, selecting the highest-scoring candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-001 (FeedME) used as instruction-finetuned scoring LM; OPT-13B used as alternative RM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FeedME is OpenAI's instruction-tuned GPT-3 variant (text-davinci-001). OPT-13B is an open pretrained transformer used for finetuned reward models.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LM self-evaluation ensemble (InstructRM Ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Write several different binary-evaluation prompts; for each candidate refinement, compute p('Yes') that the refinement incorporates the feedback for each prompt using the instruction-tuned LM; average probabilities across prompts and choose the candidate with highest average p('Yes').</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Refinement ranking for summarization (development set evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given five candidate refinements produced by conditioning on (context, initial summary, feedback), select the candidate that better incorporates the feedback according to human rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>InstructRM Ensemble achieved a win rate of 56.0 ± 3.0% vs random selection when evaluated on human rankings of refinements on the development set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Embedding-similarity baseline (contrastive embeddings) achieved 48.3 ± 3.0% (not better than random); individual zero-shot InstructRM prompts varied, but the ensemble outperformed single prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Human evaluation (win rates) shows selecting refinements using the LM-based InstructRM Ensemble significantly improves the chance of picking a refinement that humans judge better than a random candidate (56% vs ~50%). This scoring method was used throughout experiments to produce the best-of-N samples for ILF finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LM scoring is sensitive to prompt formulation; ensemble mitigates but does not eliminate sensitivity. The method requires a strong instruction-tuned LM to be effective. It does not guarantee absolute correctness—win rate only modestly above random, so imperfect selection remains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Language Models with Language Feedback at Scale', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5474.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5474.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Refinement without Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction to improve output without explicit feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant where the LM is instructed to improve its initial output without receiving human-written feedback; used as an ablation to isolate the value of explicit language feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-001 (FeedME)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned GPT-3 (text-davinci-001) used to generate a single refinement when asked to improve the initial summary but not given feedback f.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate-then-refine (no explicit feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the refinement LM with the initial summary and an instruction to improve it (no additional feedback). Generate a refinement and compare quality against initial summary and feedback-conditioned refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization (validation set ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether simply instructing the LM to improve the initial summary (without explicit feedback) improves quality relative to the initial summary and to feedback-conditioned refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Refinement without feedback achieved a win rate of 59.4 ± 2.1% versus the initial (FeedME) summaries on the validation set (human evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial summaries (FeedME samples) are the baseline: this method significantly outperformed that baseline (59.4% wins).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Human evaluations show that simply instructing the LM to improve its output (generate-then-refine without feedback) yields substantial gains over the initial generation. However, adding explicit feedback gave further gains (Refinement with Feedback: 63.9%; Best-of-N + InstructRM: 69.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Refinements generated without feedback addressed the feedback's most important point only ~30.8% of the time (vs 49.6% for single feedback-conditioned refinements and 57.4% for Best-of-N), indicating that they may improve general quality but not reliably address specific, targeted corrections provided by feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Language Models with Language Feedback at Scale', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5474.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5474.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Refinement with Feedback + Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Refinement conditioned on feedback with Best-of-N selection using LM scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate multiple refinements conditioned on (context, initial summary, human feedback), score candidates with InstructRM Ensemble, and select the best-of-N for evaluation and finetuning; used as the main data-generation procedure for ILF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-001 (FeedME) for generation; InstructRM (FeedME) for scoring; GPT-3-175B finetuned on chosen refinements</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FeedME generates N=5 refinements per sample; InstructRM Ensemble scores candidates and picks the best; GPT-3-175B is finetuned on chosen refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate-then-score (Best-of-N) with language feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each sample generate N=5 refinements conditioned on (c,x0,f); compute InstructRM Ensemble score p(Yes) for each refinement; select the refinement with highest average p(Yes) and add (c,x1*) to finetuning dataset; finetune original LM on these selected refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization (validation and test evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce high-quality summaries by incorporating human feedback; evaluate via human rankings and by measuring how often the most important feedback point is incorporated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Refinement with Feedback + Best-of-N achieved a win rate vs initial summaries of 69.1 ± 1.9% (validation set, human evaluation). The method incorporated the most important feedback point 57.4 ± 2.2% of the time (validation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Randomly choosing a refinement (Refinement with Feedback w/o Best-of-N) achieved a lower win rate; single refinement with feedback (no best-of) had win rate 63.9 ± 2.0%; refinement without feedback had 59.4 ± 2.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Best-of-N selection with InstructRM yields the highest win rates and the highest rate of incorporation of the annotator-specified most important feedback point, showing both qualitative and quantitative improvement over ablations (no feedback, feedback but no scoring, random selection).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Best-of-N requires extra model calls and a reliable scoring LM; scoring is imperfect (ensemble win rate only modestly above random). Improvements depend on the strength of the refinement LM and scoring RM; smaller models underperform.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Language Models with Language Feedback at Scale', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5474.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5474.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Saunders et al. (self-critiquing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critiquing models for assisting human evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work cited that shows language models can write high-quality feedback (self-critiquing) on LM outputs, potentially enabling training from LM-written feedback rather than human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-critiquing models for assisting human evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Referenced claim: LMs can generate useful feedback on their outputs (LM-written feedback) which can reduce human annotation costs; details and experiments are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper is cited as evidence that LMs can produce high-quality feedback; this paper (Scheurer et al.) cites Saunders et al. in related work and notes subsequent work used LM-written feedback to train dialog assistants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper; cited as follow-on work demonstrating potential to reduce human feedback cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Language Models with Language Feedback at Scale', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5474.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5474.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bai et al. (Constitutional AI / LM feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constitutional AI: Harmlessness from AI feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited related work where LM-written feedback and synthetic 'constitutional' constraints are used to train assistants without requiring human feedback for each example.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constitutional ai: Harmlessness from ai feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LM-written feedback / ILF-style training using LM feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Referenced approach: use LMs to write feedback or critiques and train models from that feedback (reducing human annotation requirements); details in Bai et al.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as subsequent work showing feasibility of training with LM-written language feedback (concurrent developments after Scheurer et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed here; cited as promising complementary direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Language Models with Language Feedback at Scale', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Constitutional ai: Harmlessness from ai feedback <em>(Rating: 2)</em></li>
                <li>When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels <em>(Rating: 1)</em></li>
                <li>On improving summarization factual consistency from natural language feedback <em>(Rating: 1)</em></li>
                <li>Peer: A collaborative language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5474",
    "paper_id": "paper-c11810fa8887b678facea62da4607c4898360308",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "ILF",
            "name_full": "Imitation learning from Language Feedback",
            "brief_description": "An iterative refine-and-finetune algorithm that uses human natural-language feedback to generate candidate refinements, selects the best refinement via a scoring function, and finetunes the original LM on the selected refinement; repeatable across iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci / text-davinci-001 'FeedME' for refinement; GPT3-175B finetuned)",
            "model_description": "Uses OpenAI GPT-3 family models. Key models: text-davinci-001 (instruction-finetuned, 'FeedME') for generating refinements and scoring; GPT-3 Davinci (175B params) is finetuned (via OpenAI API) in experiments.",
            "reflection_method_name": "ILF (iterative refine-and-finetune)",
            "reflection_method_description": "For each context c: sample an initial output x0 from the LM, collect human-written feedback f on (c,x0), condition a refinement LM (π_ψ, here FeedME) on (c,x0,f) to generate multiple candidate refinements {x1^i}, score each candidate with a reward/scoring model R (implemented as an instruction-tuned LM answering a binary question or as an RM) and pick the best x1*, then finetune the original LM π_θ to maximize likelihood of x1* given c. Repeat for K iterations (Algorithm 1).",
            "num_iterations": 1,
            "task_name": "Summarization (TL;DR Reddit dataset)",
            "task_description": "Generate short summaries (≤48 tokens) of Reddit posts/titles; human feedback annotators write the most important shortcoming and an ideal summary; used for evaluating improvement from language feedback.",
            "performance_with_reflection": "Finetuning on refinements produced by ILF (5K samples) achieved a win rate vs human-written reference summaries of 31.3 ± 1.7% (test set of 698, human evaluation). Combining ILF finetuned model with an OPT reward model for best-of-64 sampling (ILF + OPT-RM best-of-64) reached a win rate of 50.8 ± 1.9% (human-level).",
            "performance_without_reflection": "Finetuning on initial summaries (no feedback-refinement) achieved a win rate vs human summaries of 27.3 ± 1.7% (5K). Finetuning on human-written summaries achieved 28.9 ± 1.7%. FeedME (instruction-tuned base sampling) achieved 22.5 ± 1.6%. (All numbers from the paper, 5K regime.)",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative human-evaluation win rates: ILF finetuning on refinements outperformed finetuning on initial summaries and finetuning on human summaries at the same data sizes (e.g., 31.3% vs 28.9% and 27.3% in the 5K case). Combining ILF with binary-feedback best-of-N ranking further improved quality up to ~50.8% win rate vs human references. Additional evidence: models finetuned on refinements had lower validation loss on refinement validation data and higher (reverse) KL divergence indicating substantive model change correlated with better human preference scores.",
            "limitations_or_failure_cases": "Requires very large LMs to reliably incorporate feedback (only 175B models showed non-negligible refinement ability in targeted experiments). ILF depends on quality of human feedback and on a reliable scoring function R; the paper assumes q(x1) constant for importance sampling approximations. Multiple-iteration ILF results are preliminary (App. H.3) and need more study. Finetuning on refinements produced larger KL divergence from the pretrained LM, which may raise concerns about distribution shift or undesirable behaviors. Computational cost: Best-of-N sampling and scoring add inference cost.",
            "uuid": "e5474.0",
            "source_info": {
                "paper_title": "Training Language Models with Language Feedback at Scale",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ILF (targeted word removal)",
            "name_full": "ILF validation on targeted offensive-word removal task",
            "brief_description": "A controlled experiment testing an LM's ability to incorporate language feedback to remove specified offensive words from a sentence; used to determine which models can meaningfully refine outputs given feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (Ada, Babbage, Curie, Davinci) and instruction-finetuned FeedME variants",
            "model_description": "GPT-3 variants from smallest (Ada) up to Davinci (175B). FeedME denotes instruction-finetuned versions (text-davinci-001).",
            "reflection_method_name": "Refinement conditioned on feedback (single-step refine)",
            "reflection_method_description": "Prompt models with an initial sentence containing offensive words and explicit instruction to remove specified words (the human feedback is effectively the instruction to remove particular tokens); generate a single refinement (greedy decoding) and measure exact string match to the target.",
            "num_iterations": 1,
            "task_name": "Targeted word removal (synthetic offensive-word removal)",
            "task_description": "Given a sentence containing up to 10 offensive words and a target list of ≤3 words to remove, generate a refined sentence that exactly matches the target sentence (automatically generated ground truth).",
            "performance_with_reflection": "Only the largest models performed non-negligibly: GPT-3 Davinci (175B) achieved 38.5 ± 1.3% exact match; FeedME Davinci (instruction-tuned 175B) achieved 35.8 ± 1.3%. Smaller models (Curie 6.7B, Babbage 1B, Ada) performed near random/very low (e.g., Curie ≈ 6–8%, Babbage ≈ 1–2%).",
            "performance_without_reflection": "Not applicable in the same way; baseline is generation without conditioning on the feedback/instruction, which would be expected to fail the task. The paper's comparison is across model scales (with feedback instruction supplied).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Clear scaling result: only very large models (175B) can reliably incorporate fine-grained language feedback to perform exact targeted edits; this motivated using 175B models for downstream ILF experiments.",
            "limitations_or_failure_cases": "Smaller models fail to incorporate feedback for this controlled editing task; thus ILF and refinement-based approaches may not generalize to smaller architectures or sizes. Experiment used greedy decoding and single-step refinement; multiple samples or better decoding might change results but were not reported here.",
            "uuid": "e5474.1",
            "source_info": {
                "paper_title": "Training Language Models with Language Feedback at Scale",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "InstructRM Ensemble",
            "name_full": "Instruction-finetuned Reward Model Ensemble (LM-based scoring)",
            "brief_description": "A scoring method that uses an instruction-finetuned LM to answer a binary question ('Does this new text incorporate the feedback? Yes/No') across multiple prompts and averages the positive-answer probabilities to score candidate refinements, selecting the highest-scoring candidate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-001 (FeedME) used as instruction-finetuned scoring LM; OPT-13B used as alternative RM",
            "model_description": "FeedME is OpenAI's instruction-tuned GPT-3 variant (text-davinci-001). OPT-13B is an open pretrained transformer used for finetuned reward models.",
            "reflection_method_name": "LM self-evaluation ensemble (InstructRM Ensemble)",
            "reflection_method_description": "Write several different binary-evaluation prompts; for each candidate refinement, compute p('Yes') that the refinement incorporates the feedback for each prompt using the instruction-tuned LM; average probabilities across prompts and choose the candidate with highest average p('Yes').",
            "num_iterations": 1,
            "task_name": "Refinement ranking for summarization (development set evaluation)",
            "task_description": "Given five candidate refinements produced by conditioning on (context, initial summary, feedback), select the candidate that better incorporates the feedback according to human rankings.",
            "performance_with_reflection": "InstructRM Ensemble achieved a win rate of 56.0 ± 3.0% vs random selection when evaluated on human rankings of refinements on the development set.",
            "performance_without_reflection": "Embedding-similarity baseline (contrastive embeddings) achieved 48.3 ± 3.0% (not better than random); individual zero-shot InstructRM prompts varied, but the ensemble outperformed single prompts.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Human evaluation (win rates) shows selecting refinements using the LM-based InstructRM Ensemble significantly improves the chance of picking a refinement that humans judge better than a random candidate (56% vs ~50%). This scoring method was used throughout experiments to produce the best-of-N samples for ILF finetuning.",
            "limitations_or_failure_cases": "LM scoring is sensitive to prompt formulation; ensemble mitigates but does not eliminate sensitivity. The method requires a strong instruction-tuned LM to be effective. It does not guarantee absolute correctness—win rate only modestly above random, so imperfect selection remains.",
            "uuid": "e5474.2",
            "source_info": {
                "paper_title": "Training Language Models with Language Feedback at Scale",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Refinement without Feedback",
            "name_full": "Instruction to improve output without explicit feedback",
            "brief_description": "A variant where the LM is instructed to improve its initial output without receiving human-written feedback; used as an ablation to isolate the value of explicit language feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-001 (FeedME)",
            "model_description": "Instruction-finetuned GPT-3 (text-davinci-001) used to generate a single refinement when asked to improve the initial summary but not given feedback f.",
            "reflection_method_name": "Generate-then-refine (no explicit feedback)",
            "reflection_method_description": "Prompt the refinement LM with the initial summary and an instruction to improve it (no additional feedback). Generate a refinement and compare quality against initial summary and feedback-conditioned refinements.",
            "num_iterations": 1,
            "task_name": "Summarization (validation set ablation)",
            "task_description": "Assess whether simply instructing the LM to improve the initial summary (without explicit feedback) improves quality relative to the initial summary and to feedback-conditioned refinements.",
            "performance_with_reflection": "Refinement without feedback achieved a win rate of 59.4 ± 2.1% versus the initial (FeedME) summaries on the validation set (human evaluation).",
            "performance_without_reflection": "Initial summaries (FeedME samples) are the baseline: this method significantly outperformed that baseline (59.4% wins).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Human evaluations show that simply instructing the LM to improve its output (generate-then-refine without feedback) yields substantial gains over the initial generation. However, adding explicit feedback gave further gains (Refinement with Feedback: 63.9%; Best-of-N + InstructRM: 69.1%).",
            "limitations_or_failure_cases": "Refinements generated without feedback addressed the feedback's most important point only ~30.8% of the time (vs 49.6% for single feedback-conditioned refinements and 57.4% for Best-of-N), indicating that they may improve general quality but not reliably address specific, targeted corrections provided by feedback.",
            "uuid": "e5474.3",
            "source_info": {
                "paper_title": "Training Language Models with Language Feedback at Scale",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Refinement with Feedback + Best-of-N",
            "name_full": "Refinement conditioned on feedback with Best-of-N selection using LM scoring",
            "brief_description": "Generate multiple refinements conditioned on (context, initial summary, human feedback), score candidates with InstructRM Ensemble, and select the best-of-N for evaluation and finetuning; used as the main data-generation procedure for ILF.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-001 (FeedME) for generation; InstructRM (FeedME) for scoring; GPT-3-175B finetuned on chosen refinements",
            "model_description": "FeedME generates N=5 refinements per sample; InstructRM Ensemble scores candidates and picks the best; GPT-3-175B is finetuned on chosen refinements.",
            "reflection_method_name": "Generate-then-score (Best-of-N) with language feedback",
            "reflection_method_description": "For each sample generate N=5 refinements conditioned on (c,x0,f); compute InstructRM Ensemble score p(Yes) for each refinement; select the refinement with highest average p(Yes) and add (c,x1*) to finetuning dataset; finetune original LM on these selected refinements.",
            "num_iterations": 1,
            "task_name": "Summarization (validation and test evaluations)",
            "task_description": "Produce high-quality summaries by incorporating human feedback; evaluate via human rankings and by measuring how often the most important feedback point is incorporated.",
            "performance_with_reflection": "Refinement with Feedback + Best-of-N achieved a win rate vs initial summaries of 69.1 ± 1.9% (validation set, human evaluation). The method incorporated the most important feedback point 57.4 ± 2.2% of the time (validation).",
            "performance_without_reflection": "Randomly choosing a refinement (Refinement with Feedback w/o Best-of-N) achieved a lower win rate; single refinement with feedback (no best-of) had win rate 63.9 ± 2.0%; refinement without feedback had 59.4 ± 2.1%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Best-of-N selection with InstructRM yields the highest win rates and the highest rate of incorporation of the annotator-specified most important feedback point, showing both qualitative and quantitative improvement over ablations (no feedback, feedback but no scoring, random selection).",
            "limitations_or_failure_cases": "Best-of-N requires extra model calls and a reliable scoring LM; scoring is imperfect (ensemble win rate only modestly above random). Improvements depend on the strength of the refinement LM and scoring RM; smaller models underperform.",
            "uuid": "e5474.4",
            "source_info": {
                "paper_title": "Training Language Models with Language Feedback at Scale",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Saunders et al. (self-critiquing)",
            "name_full": "Self-critiquing models for assisting human evaluators",
            "brief_description": "Related work cited that shows language models can write high-quality feedback (self-critiquing) on LM outputs, potentially enabling training from LM-written feedback rather than human feedback.",
            "citation_title": "Self-critiquing models for assisting human evaluators",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "self-critiquing",
            "reflection_method_description": "Referenced claim: LMs can generate useful feedback on their outputs (LM-written feedback) which can reduce human annotation costs; details and experiments are in the cited paper.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper is cited as evidence that LMs can produce high-quality feedback; this paper (Scheurer et al.) cites Saunders et al. in related work and notes subsequent work used LM-written feedback to train dialog assistants.",
            "limitations_or_failure_cases": "Not detailed in this paper; cited as follow-on work demonstrating potential to reduce human feedback cost.",
            "uuid": "e5474.5",
            "source_info": {
                "paper_title": "Training Language Models with Language Feedback at Scale",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Bai et al. (Constitutional AI / LM feedback)",
            "name_full": "Constitutional AI: Harmlessness from AI feedback",
            "brief_description": "Cited related work where LM-written feedback and synthetic 'constitutional' constraints are used to train assistants without requiring human feedback for each example.",
            "citation_title": "Constitutional ai: Harmlessness from ai feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "LM-written feedback / ILF-style training using LM feedback",
            "reflection_method_description": "Referenced approach: use LMs to write feedback or critiques and train models from that feedback (reducing human annotation requirements); details in Bai et al.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited as subsequent work showing feasibility of training with LM-written language feedback (concurrent developments after Scheurer et al.).",
            "limitations_or_failure_cases": "Not detailed here; cited as promising complementary direction.",
            "uuid": "e5474.6",
            "source_info": {
                "paper_title": "Training Language Models with Language Feedback at Scale",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2
        },
        {
            "paper_title": "Constitutional ai: Harmlessness from ai feedback",
            "rating": 2
        },
        {
            "paper_title": "When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels",
            "rating": 1
        },
        {
            "paper_title": "On improving summarization factual consistency from natural language feedback",
            "rating": 1
        },
        {
            "paper_title": "Peer: A collaborative language model",
            "rating": 1
        }
    ],
    "cost": 0.0200485,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Training Language Models with Language Feedback at Scale</h1>
<p>Jérémy Scheurer ${ }^{12}$ Jon Ander Campos ${ }^{13}$ Tomasz Korbak ${ }^{124}$ Jun Shern Chan ${ }^{12}$ Angelica Chen ${ }^{1}$ Kyunghyun Cho ${ }^{156}$ Ethan Perez ${ }^{127}$</p>
<h4>Abstract</h4>
<p>Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of modelgenerated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: To learn from language feedback on a language model (LM) output, we have an LM generate multiple refinements of the original output based on the feedback. We use an LM to pick the best refinement and finetune the original LM to maximize the likelihood of the chosen refinement.</p>
<h2>1. Introduction</h2>
<p>Language Models (LMs) achieve strong performance across diverse NLP tasks, from summarization to question answering and dialog (Radford \&amp; Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Rae et al., 2021, inter alia). One of their key limitations, however, is that they generate text that violates human preferences, such as misinformation (Lin et al., 2021), offensive language (Gehman et al., 2020), and factually incorrect summaries (Stiennon et al., 2020). To alleviate such issues, existing methods train LMs to generate text that scores highly according to human preferences or a predictive model thereof (Ziegler et al., 2019; Stiennon et al., 2020; Nakano et al., 2021; Ouyang et al., 2022). These approaches learn from human feedback regarding which of two outputs is better. However, each comparison only conveys limited information about human preferences.</p>
<p>We propose an alternative approach that learns from language feedback, an information-rich and natural form of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Algorithm 1 Imitation Learning from Language Feedback
Input: number of iterations $K$, a sequence of sets of source documents $\mathcal{C}=\left[\mathcal{C}<em K="K">{1}, \ldots, \mathcal{C}</em>$, reward model $R$
for $k$ in $1 \ldots K$ do
Initialize finetuning dataset $\mathcal{D}}\right]$, language model $\pi_{\theta}$, refinement language model $\pi_{\psi<em k="k">{k}={ }$
for document $c$ in $\mathcal{C}</em>$ do
$x_{0} \sim \pi_{\theta}\left(x_{0} \mid c\right)$
Human provides feedback $f$ on $\left(c, x_{0}\right)$
$\left{x_{1}^{1}, \ldots, x_{1}^{N}\right} \sim \pi_{\psi}\left(x_{1} \mid c, x_{0}, f\right)$
$x_{1}=\operatorname{argmax}<em 1="1">{x</em>, f, c\right)$
Add $\left(c, x_{1}\right)$ to $\mathcal{D}}^{c}} R\left(x_{1}^{c} \mid x_{0<em _theta="\theta">{k}$
end for
Update $\pi</em>$ (as in Eq. 4) end for}$ by supervised finetuning on $\mathcal{D}_{k</p>
<p>Figure 2: Top Left: The graphical model of the target distribution $p_{\theta}$ that our algorithm approximates. $c$ is a context and $x_{1}$ is a high-quality LM output. Top Right: Graphical model of the proposal distribution $q$ for importance sampling. $x_{0}$ is an initial LM output and $f$ is language feedback on $x_{0}$. Bottom: Pseudocode for our learning algorithm.
human feedback. We introduce Imitation learning from Language Feedback (ILF), a 3-step algorithm for learning from language feedback (Fig. 1). First, we generate multiple refinements of an LM-generated output, given the input, initial LM-generated output, and human-written feedback on the output. Second, we use an instruction-finetuned LM to choose the refinement that best incorporates the feedback. Third, we finetune the LM that generated the initial output on the chosen refinement given the input. In this way, we finetune an LM using language feedback; with the resulting model, we may then collect more feedback on its outputs and learn with the above refine-and-finetune approach. The algorithm's pseudocode (Algorithm 1) and the corresponding graphical model are shown in Fig 2. ILF departs from prior work, which uses reinforcement learning (RL) (Ziegler et al., 2019; Stiennon et al., 2020, inter alia) or auxiliary losses (Stacey et al., 2021) and cannot be straightforwardly generalized to using free-form language feedback.</p>
<p>We analyze our approach both theoretically and empirically. We show that ILF can be viewed as Bayesian Inference, similar to RL with Human Feedback with KL penalties (Korbak et al., 2022). We then validate our algorithm on a carefully-controlled synthetic task of removing offensive words from a sentence with GPT-3-based models (Brown et al., 2020; Ouyang et al., 2022). We find that only the largest GPT-3-based models (175B parameters) accurately
refine outputs. Using this insight, we use the largest GPT-3 models to test our algorithm on text summarization, following Stiennon et al. (2020). Our work extends our earlier unpublished results (Scheurer et al., 2022), showing that ILF improves LM-generated summaries monotonically with the amount of feedback provided, testing up to 5 k samples. In all data regimes, ILF leads to comparable or better results to finetuning on human-written summaries, suggesting our approach is a strong alternative to supervised learning on human demonstrations. We also introduce an approach for learning from both language and comparison feedback by choosing the best-of-N samples from an ILF-trained model using a model trained with comparison feedback. The hybrid approach outperforms learning from each form of feedback alone, leading to summaries that human evaluators prefer over high-quality human reference summaries $\sim 50.8 \%$ of the time. Our analysis shows that LM-generated refinements typically incorporate the feedback, especially when we use an LM to choose the refinement that best incorporates the feedback. In our concurrent paper (Chen et al., 2023), we show that ILF also achieves strong performance on code generation. Our results suggest that language feedback is a promising avenue for learning human preferences.</p>
<h2>2. Methods</h2>
<p>We now formulate the problem setting and describe our approach. We aim to generate improved outputs $x_{1}$ (e.g., high-quality summaries), according to human preferences, given language feedback $f$ on an initial model-generated output $x_{0}$, and a context $c$ (e.g., a source document). We tackle this problem by updating an LM $\pi_{\theta}$ based on evidence provided by language feedback.</p>
<p>Our goal is to sample a diverse set of high-quality outputs $x_{1}$ given a context $c$ (e.g., a summary of a document), where $c$ is drawn from the context distribution $p(c)$. We do so by fitting an autoregressive LM $\pi_{\theta}$ to approximate the groundtruth distribution $p_{c}^{<em>}\left(x_{1}\right)$ which is proportional to the quality of $x_{1}$, measured by the reward function $R$. Fitting $\pi_{\theta}$ can be written down as minimizing the expected KL-divergence from the true distribution $p_{c}^{</em>}\left(x_{1}\right)$ to $\pi_{\theta}$ over the context distribution $p(c)$ :</p>
<p>$$
\begin{gathered}
\min <em _sim="\sim" c="c" p_c_="p(c)">{\theta} \mathbb{E}</em>^{} \mathrm{KL}\left(p_{c<em>}, \pi_{\theta}\right) \
\text { where } p_{c}^{</em>}\left(x_{1}\right) \propto \exp \left(\beta R\left(x_{1} \mid c\right)\right)
\end{gathered}
$$</p>
<p>Minimizing the objective in Eq. 1 equivalent to minimizing the cross-entropy loss (i.e., supervised learning):</p>
<p>$$
\begin{aligned}
\mathcal{L}(\theta) &amp; =-\mathbb{E}<em _theta="\theta">{c \sim p(c)} \mathcal{L}</em>(c) \
\text { where } \mathcal{L}<em x__1="x_{1">{\theta}(c) &amp; =\sum</em> \mid c\right)
\end{aligned}
$$}} p_{c}^{*}\left(x_{1}\right) \log \pi_{\theta}\left(x_{1</p>
<p>It is intractable to compute this loss exactly for a number of reasons, including the exponential size of the space of $x_{1}$ as well as the intractability of computing the normalization constant of $p_{c}^{<em>}\left(x_{1}\right)$. To avoid the first issue, we use Monte Carlo approximation sampling using a small set of samples drawn from $p_{c}^{</em>}$. Directly sampling from $p_{c}^{*}$ is however still intractable. We thus resort to using importance sampling with a proposal distribution $q_{c}\left(x_{1}\right)$ that is simpler to sample:</p>
<p>$$
\mathcal{L}<em x__1="x_{1">{\theta}(c)=\sum</em> \mid c\right)
$$}} q_{c}\left(x_{1}\right) \frac{p_{c}^{*}\left(x_{1}\right)}{q_{c}\left(x_{1}\right)} \log \pi_{\theta}\left(x_{1</p>
<p>To minimize the variance, we must design $q_{c}$ to be as close as possible to $p_{c}^{*}$. We achieve this goal by defining $q_{c}$ to incorporate human feedback that directly reflects the unknown reward function $R$, in the process of sampling. We do so by first drawing an initial output $x_{0}$ from a suboptimal LM $\pi_{\theta}$ given the context $c$. Second, we ask humans to rate $x_{0}$ and provide language feedback $f$ on the $\left(c, x_{0}\right)$, pair. Third, a refinement $\mathrm{LM} \pi_{\psi}$ generates a refined output $x_{1}$ conditioned on $\left(c, x_{0}, f\right)$. The proposal distribution, corresponding to this sampling procedure, can be written down as:</p>
<p>$$
q_{c}\left(x_{1}\right)=\sum_{f, x_{0}} \pi_{\psi}\left(x_{1} \mid x_{0}, f\right) p\left(f \mid x_{0}\right) \pi_{\theta}\left(x_{0} \mid c\right)
$$</p>
<p>Let $x_{1}^{i}, \ldots, x_{1}^{N}$ be $N$ summaries sampled from $q_{c}\left(x_{1}\right)$. Then, we can approximate the objective in Eq. 2 as:</p>
<p>$$
\mathcal{L}<em i="1">{\theta}(c) \approx \sum</em>}^{N} \underbrace{\frac{p_{c}^{*}\left(x_{1}^{i}\right)}{q_{c}\left(x_{1}^{i}\right)}<em _theta="\theta">{=\omega^{i}} \log \pi</em> \mid c\right)
$$}\left(x_{1}^{i</p>
<p>where $\omega^{i}$ is the importance weight of the $i$-th sample from $q_{c}$. The importance weight $\omega^{i}$ is not computable as it is because we do not have access to $q_{c}$ other than being able to draw samples from it. We avoid this issue by assuming that $q_{c}\left(x_{1}^{i}\right)$ is constant, implying that our samples are all equally good due to the high quality of human feedback. We then replace $R\left(x_{1}^{i} \mid c\right)$ in the definition of $p_{c}^{<em>}$ by $R\left(x_{1}^{i} \mid x_{0}, f, c\right)$, as the quality is not dependent on the intermediate summary and feedback but can be more easily assessed with these quantities. This allows us to compute the unnormalized $p_{c}^{</em>}$, after which we use self-normalization to finally compute the above loss.</p>
<p>We implement $R$ by conditioning an instruction-finetuned LM on a binary question such as Does this new text $\left[x_{1}\right]$ incorporate the feedback $[f]$ provided on the initial text $\left[x_{0}\right]$ ? Answer Yes or No., where the label $y$ is either $y_{\text {good }}$ (" Yes") or $y_{\text {bad }}\left({ }^{\prime \prime}\right.$ No"). We use the probability of the positive answer $y_{\text {good }}$ as $R$, i.e. $R\left(x_{1} \mid x_{0}, f, c\right)=\frac{p\left(y_{\text {good }} \mid \text { prompt }\right)}{p\left(y_{\text {good }} \mid \text { prompt }\right)+p\left(y_{\text {bad }} \mid \text { prompt }\right)}$. Finally, we use an extremely low temperature when computing $p_{c}^{<em>}$, i.e., $\beta \rightarrow \infty$. Due to self-normalization, this is equivalent to using only the best summary $x_{1}^{</em>}$ per context $c$ sampled from $q_{c}$ for computing the loss, resulting in the following, final objective:</p>
<p>$$
\mathcal{L}(\theta) \approx \mathbb{E}<em _theta="\theta">{c \sim p(c)} \log \pi</em> \mid c\right)
$$}\left(x_{1}^{*</p>
<p>Our objective of approximating the ground truth distribution $p_{c}^{*}\left(x_{1}\right)$, which is proportional to the reward $R$ has clear connections to maximizing reward in RL. However, in RL, the goal is to find the best policy that maximizes the reward, whereas our algorithm results in a distribution of highquality outputs $x_{1}$ given a document $c$, which allows us to draw a diverse set of outputs achieving a high reward. The broad diversity of high-quality outputs endows downstream users and systems with more control over which aspects they prefer and want to avoid. In App. A.1, we further provide an alternative derivation of ILF that follows variational inference and shows that ILF can also be understood as Bayesian Inference. This process involves updating an LM based on the evidence provided by language feedback. This different lense highlights the correspondence between ILF and RL with Human Feedback (Ziegler et al., 2019; Stiennon et al., 2020, inter alia), which was previously demonstrated to be equivalent to Bayesian inference (Korbak et al., 2022).</p>
<h2>3. Can Language Models Use Feedback?</h2>
<p>For our algorithm to work, LMs must be able to accurately incorporate feedback to generate refinements. Thus, we first validate the refinement step of our algorithm on a carefully-controlled synthetic task of removing specific offensive words from a given sentence. We examine how effectively various models incorporate feedback to determine what model to use for refining outputs.</p>
<p>Experimental Setup We instruct an LM to refine an automatically-generated sentence with $\leq 10$ offensive words by removing $\leq 3$ specific words (see Appendix D for a detailed explanation and examples). In this experiment, we generate one output per sample with greedy decoding, i.e., we do not sample with best-of- $N$. We evaluate how often the generated refinement exactly matches the target sentence, which we automatically generate. For our LMs, we use differently-sized GPT-3 models (Brown et al., 2020) and text-davinci-001, their instruction-finetuned (Feedback Made Easy or FeedME) counterparts (Ouyang et al., 2022; OpenAI, 2022b). ${ }^{1}$ We report all hyperparameters used in</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Ada <br> (-)</th>
<th style="text-align: center;">$\begin{gathered} \text { Babbage } \ \text { (1B) } \end{gathered}$</th>
<th style="text-align: center;">$\begin{gathered} \text { Curie } \ (6.7 \mathrm{~B}) \end{gathered}$</th>
<th style="text-align: center;">$\begin{gathered} \text { Davinci } \ (175 \mathrm{~B}) \end{gathered}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">$1.2 \pm 0.3$</td>
<td style="text-align: center;">$1.7 \pm 0.4$</td>
<td style="text-align: center;">$8.2 \pm 0.7$</td>
<td style="text-align: center;">$38.5 \pm 1.3$</td>
</tr>
<tr>
<td style="text-align: center;">FeedME</td>
<td style="text-align: center;">$1.6 \pm 0.3$</td>
<td style="text-align: center;">$2.2 \pm 0.4$</td>
<td style="text-align: center;">$6.0 \pm 0.6$</td>
<td style="text-align: center;">$35.8 \pm 1.3$</td>
</tr>
</tbody>
</table>
<p>Table 1: On the task of removing offensive words from a sentence, only large LMs incorporate feedback. We report the percentage of exact string matches with the target.</p>
<p>Appendix G. We report the mean and standard error for all results in our work.</p>
<p>Results Table 1 shows the results. We observe that only the largest GPT-3 and FeedME models (175B parameters) incorporate feedback in a non-negligible amount of time. Using this insight, we only use the 175B parameter models in the rest of our experiments. Specifically, we use FeedME, because it is an instruction-finetuned model.</p>
<h2>4. Summarization from Language Feedback</h2>
<p>Having established that large LMs can leverage language feedback, we now evaluate our algorithm on the real-world task of text summarization. In $\S 4.1$, we introduce a novel summarization dataset that we use to evaluate our algorithm, in $\S 4.2$, we explore different methods for ranking refinements and in $\S 4.3$, we use the best ranking method to learn from language feedback.</p>
<h3>4.1. Summarization with Language Feedback Dataset</h3>
<p>We evaluate the effectiveness of ILF on the task of text summarization using the TL;DR dataset (Völske et al., 2017), which consists of Reddit titles, posts, and their corresponding summaries. Stiennon et al. (2020) adapt this dataset and show that it is a more realistic task for evaluating summarization models compared to the commonly used CNN/DM dataset (Hermann et al., 2015). To ensure the quality of our dataset, we follow the same preprocessing steps as outlined in Stiennon et al. (2020) and extract a train dataset with 5000 samples, a development dataset with 200 samples, a validation dataset with 500 samples, and a test dataset with 698 samples $^{2}$. We then hire experienced annotators through Surge $\mathrm{AI}^{3}$ to create our language feedback dataset, which we open source along with our code ${ }^{4}$. For each sample, we first generate three summaries for each Reddit post using the instruction-finetuned model text-davinci-001 (FeedME) (Ouyang et al., 2022; OpenAI, 2022b). Two of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: We compare various ranking methods for selecting refinements using a human evaluation. InstructRM Ensemble performs best and is used throughout our paper.
these summaries are used for a binary comparison, in which annotators indicate their preference. The third summary serves as the initial output for which we solicit language feedback. This feedback should address the single most important shortcoming of the summary and can be related to coverage (how well the summary covers the important information in the post), accuracy (the factual accuracy of the summary), coherence (the coherence of the summary on its own), or other. We do not impose any restrictions on how the feedback should be written. In addition to providing feedback, annotators are also asked to write an ideal summary that is maximally 48 tokens long. The same crowd worker annotates all three tasks for a given sample. Overall the dataset collection and human evaluations cost 40K\$. On selected samples of the binary comparison task, we achieve an author-annotator agreement of $81.0 \%$ and annotator-annotator agreement of $70.0 \%$. The human summaries we collect are of excellent quality, as demonstrated in a human evaluation, where we compare our human-written summaries to the ones automatically extracted from Reddit (Völske et al., 2017) (also used as baselines in Stiennon et al. (2020); Scheurer et al. (2022)). We find that our humanwritten summaries are preferred $72.0 \pm 3.2 \%$ of the time, making them a much stronger baseline.</p>
<h3>4.2. Comparing Refinement Ranking Methods</h3>
<p>Generating Refinements We condition FeedME on the initial summaries of our train dataset (generated with FeedME) and the human-written feedback and generate 5 refinements $x_{1}^{1}, \ldots, x_{1}^{5}$ using the instructions in App. J.1.</p>
<p>Scoring Refinements with InstructRM We chose a refinement with a scoring function $R$ that scores refinements for how effectively they incorporate feedback. For $R$ we use the instruction-finetuned LM FeedME and ask it whether a refinement is better than the initial summary (see $\S 2$ for more details). We then evaluate the probability that the</p>
<p>refinement incorporates language feedback on the initial summary and is accordingly a high-quality summary, i.e., $p\left(y_{\text {good }} \mid\right.$ prompt $)$. LMs are sensitive to the exact prompt used (Perez et al., 2021; Lu et al., 2021), so we write 5 different prompts (see App. J.2) and select the refinement with the highest average $p\left(y_{\text {good }} \mid\right.$ prompt $)$ and call this method InstructRM Ensemble.</p>
<p>Scoring Refinements with Embedding Similarity Previous work (Scheurer et al., 2022) use a contrastive pretrained text-embedding function (Neelakantan et al., 2022) to embed the feedback $f$ and refinements $x_{1}^{1}, \ldots, x_{1}^{5}$ and select the refinement with the highest cosine similarity to the feedback. They use this scoring function because feedback would often describe what the ideal text should look like. This method is less general because it assumes that good refinements are semantically similar to the feedback, which is not necessarily the case for all tasks or forms of feedback.</p>
<p>Results We now evaluate the above ranking methods on the development dataset by calculating the fraction of times the refinement selected by a method is better than a randomly-selected refinement ("win rate"), according to a ranking given by human evaluators (see App. E for more details). The results, shown in Table 2, show that the embedding similarity selection does not outperform random selection, while most (4/5) InstructRM prompts do. While the embedding similarity worked well in previous work (Scheurer et al., 2022), it does not perform well on our dataset. We believe this is because the feedback we collect, written by many annotators, is much more diverse, while in Scheurer et al. (2022), the authors wrote the feedback themselves. InstructRM Ensemble has a win rate of $56.0 \pm 3.0 \%$ against random selection, demonstrating that an LM can evaluate its own output to some extent. Based on these results, we recommend using the InstructRM Ensemble approach, as it performs well and is less sensitive to the particular prompt. Throughout our paper, we use InstructRM Ensemble as our scoring function to select refinements and refer to our method of generating and selecting refinements as Refinement with Feedback + Best of $N$.</p>
<h3>4.3. Comparing Feedback Learning Algorithms</h3>
<p>In this section, we compare various algorithms for learning from language feedback, binary feedback, and normal supervised finetuning. We present an overview of each method and then provide the results of our evaluations.</p>
<h3>4.3.1. Methods</h3>
<p>Finetuning on Refinements (ILF) For this evaluation, we use a single iteration of ILF to learn from language feedback.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: How often human evaluators prefer summaries from ILF, OPT-RM best-of-64 FeedME, ILF + OPT-RM (best-of-64), finetuning baselines and FeedME to human summaries. ILF + OPT-RM (best-of-64) generates summaries of a similar quality to human summaries.</p>
<p>We finetune GPT3-175B (davinci) (Brown et al., 2020) ${ }^{5}$ to maximize the log-likelihood of the refinement given the input prompt (consisting of the Reddit title, and post), i.e., $\log p\left(x_{1} \mid\right.$ prompt $)$, using the refinements generated with Refinement with Feedback + Best of N. For all our finetuning methods we add $\lambda \log p($ prompt $)$ to the loss (Radford et al., 2018; OpenAI, 2022a), which maximizes the log-probability of the prompt. The prompt-loss weight $\lambda \in[0,1]$ is chosen on our development dataset (see paragraph Finetuning on Human Summaries). The selected hyperparameters are detailed in App. G and the finetuning prompts in App. J.3.</p>
<p>Finetuning on Human Summaries Here we finetune GPT3-175B on the dataset of human-written summaries $x_{\text {human }}$, with the objective of maximizing the log-probability of human summaries given the input prompt (consisting of the Reddit title and post) with the additional loss term, i.e. $\log p\left(x_{\text {human }} \mid\right.$ prompt $)+\lambda \log p($ prompt $)$. To ensure the best performance of our finetuned models, we conduct thorough hyperparameter tuning on the human-written summary datasets of various sizes $(100,1 \mathrm{~K}, 5 \mathrm{~K})$. The hyperparameters optimized include the number of training epochs, the prompt loss weight $\lambda$, and the learning rate multiplier, as detailed in the OpenAI documentation (OpenAI, 2022a). We use the perplexity of the predicted summaries on the development dataset to select the most effective hyperparameters. The selected hyperparameters are applied to all datasets, i.e., finetuning on refinements, initial summaries, and human-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>written summaries, with the same sample size. More details on hyperparameter tuning can be found in Appendix G.</p>
<p>Finetuning on Initial Summaries We finetune GPT3175B on the dataset of initial summaries (generated by FeedME). The objective is to maximize the log probability of the initial summary given the prompt (consisting of the Reddit title and post) with the additional loss term i.e. $\log p\left(x_{0} \mid\right.$ prompt $)+\lambda \log p($ prompt $)$. Details on hyperparameter tuning can be found in the paragraph Finetuning on Human Summaries and Appendix G.</p>
<p>Learning from Binary Feedback: Best-of- $N$ We compare ILF against binary feedback as a baseline, the standard approach for learning from feedback. One way of learning from binary feedback is to train a reward model and use it to do best-of- $N$ sampling. We use best-of-N because it is often competitive with RL from human feedback (Nakano et al., 2021), a highly effective but more sophisticated approach (Stiennon et al., 2020; Ouyang et al., 2022). To train the RM, we finetune OPT-13B (OPT-RM) (Zhang et al., 2022) to classify whether a summary $x_{0}$ is high quality or not. To do so, we use the instruction Is the above an excellent summary of the given text? An excellent summary is coherent, accurate, concise, and detailed. Answer with Yes or No., where the label $y$ is either $y_{\text {good }}$ (" Yes") or $y_{\text {bad }}$ (" No"). Given human labels on which of two summaries is preferred, we label the preferred summary with $y_{\text {good }}$ and the other summary with $y_{\text {bad }}$. We then finetune the LM to maximize $\log p\left(y \mid x_{0}\right)+\lambda \log p\left(x_{0}\right)$, where $\lambda \in[0,1]$, chosen using the development dataset, and $y \in\left{y_{\text {good }}, y_{\text {bad }}\right}$. Using the finetuned LM, we evaluate a given summary by computing $p\left(y_{\text {good }} \mid x_{0}\right)$ and select the summary with the higher probability. We find that this approach leads to more accurate RMs than other RM training methods, such as the commonly used method from Stiennon et al. (2020); see Appendix F for comparisons and Appendix J. 4 for the used prompts. We perform Bayesian hyperparameter optimization for OPT-RM and sweep over the learning rate, batch size, and prompt-loss weight $\lambda$, using classification accuracy on the development dataset as the selection criteria (see Appendix G for more details).</p>
<p>ILF + Learning from Binary Feedback As a final step, we combine ILF and learning from binary feedback, by first finetuning GPT3-175B on the refinements as described in the paragraph finetuning on refinements (ILF). We then train the reward model, OPT-RM, and use it to perform best-of$N$ sampling, as outlined in the paragraph on learning from binary feedback. At test time, we generate 64 summaries with our finetuned model and rank them based on their probability of being a high-quality summary, $p_{\text {norm }}\left(y_{\text {good }} \mid x_{0}\right)$, using OPT-RM. The summary with the highest normalized probability is then selected.</p>
<h3>4.3.2. Evaluation</h3>
<p>We evaluate the effectiveness of our learning algorithm, by comparing it to human written reference summaries, several finetuning baselines, and OPT-RM on the task of text summarization using 100,1 K , and 5 K train samples. Using a test dataset of 698 samples, we generate a summary for each method and evaluate them with human evaluators who rank them based on quality, using a standard ranking scheme that allows for ties between summaries (see App. G for more details). Based on the rankings, we calculate the fraction of times each method's sampled summary outperforms the human-written reference summary, referred to as the "win rate". We sample summaries up to 48 tokens in length (as in Stiennon et al. (2020)) using nucleus sampling (Holtzman et al., 2019) with $p=0.95$ and temperature $t=1.0$ (see App. G for further details on hyperparameters and postprocessing). We use best-of-64 sampling with summaries sampled from FeedME for learning from binary feedback.</p>
<h3>4.3.3. ReSults</h3>
<p>Our results, shown in Fig. 3, demonstrate that finetuning on refinements (ILF) outperforms all other finetuning methods ${ }^{6}$ ), including sampling from FeedME, with a win rate against human summaries of $31.3 \pm 1.7 \%$ (for finetuning on 5 K samples), while the other methods achieve win rates of $27.3 \pm 1.7 \%$ (finetuning on initial summaries), $28.9 \pm 1.7 \%$ (finetuning on human summaries), and $22.5 \pm 1.6 \%$ (FeedME). It is surprising that ILF outperforms finetuning on human summarise across all sample sizes, despite human-written summaries generally being of higher quality (see Fig. 4, top). Further evaluation (see App. Fig. 8) shows that the model finetuned on 1 K refinements (ILF) exhibits significantly lower loss when evaluated on the validation dataset of refinements compared to the model finetuned on human summaries when evaluated on the validation dataset of human summaries, suggesting that the model is more adept at approximating the distribution of refinements. Additionally, when evaluating GPT3-175B on the summaries of 1 K samples from various train datasets, we observe significantly lower loss on the refinement dataset than on the dataset of human summaries (see Table. 6). Overall, these results demonstrate the effectiveness of our proposed ILF approach in accurately incorporating feedback and improving model performance, even outperforming finetuning on human summaries.
(Scheurer et al., 2022) found that ILF with 100 feedback samples outperformed FeedME, while here we find it underperforms FeedME with 100 feedback samples. Prior work uses author-written feedback that often conveys what the refinement should include, while our work includes more</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Top: Human evaluators prefer summaries from all refinement methods to the initial summaries (FeedME). Refine with Feedback + best-of-5 is rated highest. Bottom: Refine with Feedback + best-of-5 generally does incorporate the most important feedback point.
varied, crowdsourced feedback. As a result, we observe that embedding similarity does not properly rank refinements on our human feedback dataset (Table 2), and we believe the difference in feedback may be a significant source of differences in results in this section as well; see Appendix H. 5 for more discussion.</p>
<p>Our results demonstrate that using OPT-RM for best-of-64 sampling on FeedME summaries outperforms all finetuning methods and sampling approaches across all sample sizes. The improved performance of OPT-RM best-of-64 FeedME comes at the cost of added inference time for best-of- $N$ sampling. Combining ILF and learning from binary feedback (ILF + OPT-RM (best-of-64)) achieves human-level summarization performance with a win rate of $50.8 \pm 1.9 \%$ using 5 K samples for training. This suggests that both methods independently learn valuable information about human preferences that can be cumulative when used together. It should be noted that the result for ILF + OPT-RM (best-of64) is obtained through a separate human evaluation with different comparison summaries (see App. Fig. 9), and was added to Fig. 3 for reference. In App. H.3, we present
some initial, promising results for multiple iterations of ILF. These results suggest that the method is effective, but further experimentation is necessary to understand it better.</p>
<h3>4.4. Does Language Feedback Improve Refinements?</h3>
<p>The improvements from ILF suggest that the refinements used for finetuning are high-quality, so here we investigate whether language feedback is responsible for the high quality. To do so, we have human evaluators rank Refinement with Feedback + Best of N summaries against summaries from several other methods, similar to $\S 4.2$. We use the human ranking to compute a win rate between each method and the initial summary. We compare against Refinement with Feedback, which randomly chooses a refinement $\in x_{1}^{1}, \ldots, x_{1}^{5}$. This ablation helps to evaluate the importance of choosing a refinement with our scoring function $R$, i.e., InstructRM Ensemble. We also evaluate Refinement without Feedback, which instructs the LM to refine the initial summary but without feedback. This ablation helps to evaluate the importance of using language feedback. Lastly, we evaluate Human Summaries and Initial Summaries i.e., the initial summary $x_{0}$ generated by FeedME. We evaluate all methods on the validation dataset.</p>
<p>Results. Fig. 4 (top) shows the win rates of summaries from various methods against initial summaries. Surprisingly, instructing a model to improve its output without feedback already leads to a significant improvement (win rate of $59.4 \pm 2.1 \%$ over the initial summaries). Refinements with Feedback achieve an improved win rate of $63.9 \pm 2.0 \%$, showing that language feedback is useful for improving refinement quality. Refinement with Feedback + Best of N achieves an even better win rate of $69.1 \pm 1.9 \%$, highlighting that Best-of-N with the InstructRM Ensemble further improves the refinements. Overall, language feedback is important for high-quality refinements, especially when using Best-of-N sampling.</p>
<h3>4.5. Do Refinements Incorporate the Feedback?</h3>
<p>To determine whether refinements are of higher quality due to incorporating feedback rather than improving the summary in other ways, we conduct a study on the validation dataset in which crowd workers evaluate how often the most important point of the feedback is incorporated in the refinements produced by various methods. As shown in Fig. 4, bottom, our method Refinement with Feedback + Best of N incorporates the most important point in the feedback most frequently ( $57.4 \pm 2.2 \%$ often). Refinement with Feedback incorporates feedback $49.6 \pm 2.2 \%$ of the time, showing that Best-of-N sampling improves how often the feedback is incorporated. For reference, Refinement without Feedback fixes the most important point in the feedback $30.8 \pm 2.1 \%$ of the time, despite the model not receiving the language</p>
<p>feedback. Human Summaries address the most important point in the feedback $74.0 \pm 1.9 \%$ of the time when writing the summary from scratch despite not receiving the feedback explicitly. Our results suggest that refinements are high-quality in part because they incorporate the most important point in the feedback.</p>
<h3>4.6. Which Finetuning Dataset Changes Models Most?</h3>
<p>Here, we aim to understand how the summaries used for finetuning influence how much the model changes after finetuning. Gao et al. (2022) find that models optimized with binary human feedback are more likely to learn undesirable behaviors when their output distribution deviates more from the initial, pretrained LM. It is unclear whether these findings apply to models trained with language feedback, but we take a preliminary step in this direction for understanding language feedback-trained models. In particular, we measure the (reverse) KL divergence (following Gao et al., 2022) between an ILF-finetuned model and the pretrained LM before ILF-training, $D_{\mathrm{KL}}$ (finetuned|GPT3-175B), by unconditionally sampling from the finetuned model and evaluating the log-likelihood of the generated text with GPT3-175B. We also report the forward KL divergence, $D_{\mathrm{KL}}(\mathrm{GPT} 3-175 \mathrm{~B} \mid$ finetuned). For reference, we evaluate both of the above for models finetuned on the initial summaries and on human summaries.</p>
<p>Results. Finetuning on refinements (ILF) shows the largest KL divergence (in both directions), followed by finetuning on human summaries, and then followed by finetuning on initial summaries; see App. Table 6 for the exact numbers. We find it surprising that finetuning on refinements results in higher KL divergences than finetuning on human summaries; we expected the refinements to be closer to the model's initial output distribution, relative to human summaries, therefore causing the finetuned model to undergo less change. The larger KL divergence with ILF may be partly responsible for the larger gains in human evaluations observed in Fig. 3.</p>
<h2>5. Related Work</h2>
<p>Our work builds upon our previous report (Scheurer et al., 2022), which showed that large LMs can refine outputs with language feedback. There, we introduce the same three-step algorithm that ILF builds upon, with the key difference that here we use an LM, i.e., InstructRM Ensemble, to evaluate whether a refinement incorporates feedback, whereas in Scheurer et al. (2022) we use a contrastive pre-trained text-embedding function (Neelakantan et al., 2022). InstructRM Ensemble is more general than this Embedding Similarity since it does not assume semantic similarity of the refinements to the feedback. Another difference is that</p>
<p>ILF is an iterative, refine-and-finetune algorithm, which can be understood as Bayesian Inference corresponding to RL with Human Feedback. In addition, here we conduct different and more extensive experiments than in Scheurer et al. (2022) and use human annotators. In particular, we show that ILF outperforms finetuning on human summaries and that combining ILF with learning from binary feedback achieves roughly human-level summarization performance. For a more detailed comparison to Scheurer et al. (2022) we refer to App. H.5.</p>
<p>Subsequent work to ours suggests several ways to improve upon our approach. Saunders et al. (2022) show that LMs themselves write high-quality feedback on LM outputs. Bai et al. (2022) then train a dialog assistant using ILF to learn from LM-written language feedback, eliminating the cost and effort of collecting human feedback. Liu et al. (2022); Schick et al. (2022) train LMs to refine outputs based on feedback (without finetuning on the refinements), an approach that improves results when incorporated into ILF, as shown in subsequent work to ours (Shi et al., 2022).</p>
<p>Other work aims to use language in other ways than we do. Some work investigates using explanations for gold labeled outputs to classification tasks, while our work addresses the more general text generation setting which classification tasks can be formulated as (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020). Explanations describe why a labeled output is correct, while feedback describes how to improve a candidate's output. Prior work explores ways of using explanations to train text classification models, with mixed results (Camburu et al., 2018; Stacey et al., 2021; Pruthi et al., 2021; Wiegreffe et al., 2021; Hase \&amp; Bansal, 2021; Lampinen et al., 2022, inter alia). A few prior works also learn from language feedback for the purpose of ranking candidate outputs rather than generating outputs (Weston, 2016; Li et al., 2016; Hancock et al., 2019; Li et al., 2022; Xu et al., 2022). Matiana et al. (2021) learn text embeddings of language feedback, where improvements could benefit the refinement-scoring step of our algorithm. Language has also been used for various purposes in RL settings as well, as discussed in App. B.</p>
<p>Several other works draw connections between Bayesian Inference and learning algorithms for LMs. Korbak et al. (2022) show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by a reward function. Dohan et al. (2022) further argues that the process of generating output through multiple rounds of interaction between prompted LMs and other agents (e.g. humans providing language feedback) can be seen as executing probabilistic programs.</p>
<h2>6. Conclusion</h2>
<p>In this work, we propose Imitation learning from Language Feedback (ILF), an iterative algorithm for training LMs to behave in line with human preferences, by learning from language feedback. We validate our approach on a carefullycontrolled word-removal task, showing that only large LMs (175B parameters) accurately incorporate feedback. Using this insight, we then test our algorithm on the real-world task of text summarization. Combining ILF and learning from binary feedback brought a GPT-3 model to roughly humanlevel summarization ability. ILF on its own outperformed finetuning on human summaries, despite human summaries being of higher quality, suggesting that the model is better at approximating the distribution of refinements. Our work opens up many avenues for future work, from improving algorithms for learning from language to tackling settings where it is hard to learn from sparse or binary feedback.</p>
<h2>7. Acknowledgements</h2>
<p>We are grateful to Nat McAleese, Geoffrey Irving, Jeff Wu, Jan Leike, Cathy Yeh, William Saunders, Jonathan Ward, Sam Bowman, Daniel Ziegler, Seraphina Nix, Quintin Pope, Kay Kozaronek, Peter Hase, Asa Cooper Stickland, Jacob Pfau, David Lindner, Lennart Heim, Nitarshan Rajkumar, Kath Lumpante, Pablo Morena, Edwin Chen, Scott Heiner, and David Dohan for helpful conversations and feedback. Jérémy Scheurer and Jun Shern Chan thank Open Philanthropy for funding that enabled this research. Ethan Perez thanks the National Science Foundation and Open Philanthropy for fellowship support. Jon Ander Campos is supported by a doctoral grant from the Spanish MECD. Angelica Chen and Kyunghyun Cho are supported by the NYU Center for Data Science National Science Foundation (Award 1922658). KC was supported by 42dot, Hyundai Motor Company (under the project Uncertainty in Neural Sequence Modeling), Samsung Advanced Institute of Technology (under the project Next Generation Deep Learning: From Pattern Recognition to AI), and NSF Award 1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Science We also thank OpenAI for providing access and credits to their models via the API Academic Access Program.</p>
<h2>References</h2>
<p>Andreas, J., Klein, D., and Levine, S. Modular multitask reinforcement learning with policy sketches. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 166-175. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/v70/ andreas17a.html.</p>
<p>Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V., and Sutton, C. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https: //arxiv.org/abs/2108.07732.</p>
<p>Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020. URL https://arxiv.org/pdf/ 2005.14165.pdf.</p>
<p>Camburu, O.-M., Rocktäschel, T., Lukasiewicz, T., and Blunsom, P. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31, 2018. URL https: //arxiv.org/pdf/1812.01193.pdf.</p>
<p>Chen, A., Scheurer, J., Korbak, T., Campos, J. A., Chan, J. S., Bowman, S. R., Cho, K., and Perez, E. Improving code generation by training with natural language feedback. arXiv preprint arXiv:2303.16749, 2023.</p>
<p>Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.</p>
<p>Elgohary, A., Hosseini, S., and Hassan Awadallah, A. Speak to your parser: Interactive text-to-SQL with natural language feedback. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2065-2077, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main. 187. URL https://aclanthology.org/2020. acl-main. 187.</p>
<p>Fidler, S. et al. Teaching machines to describe images with natural language feedback. Advances in Neural Information Processing Systems, 30, 2017.</p>
<p>Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization, 2022. URL https: //arxiv.org/abs/2210.10760.</p>
<p>Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020. URL https://aclanthology.org/2020. findings-emnlp.301.pdf.</p>
<p>Goyal, P., Niekum, S., and Mooney, R. J. Using Natural Language for Reward Shaping in Reinforcement Learning, 2019.</p>
<p>Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019.</p>
<p>Hase, P. and Bansal, M. When can models learn from explanations? a formal framework for understanding the roles of explanation data. arXiv preprint arXiv:2102.02201, 2021. URL https://arxiv. org/pdf/2102.02201.pdf.</p>
<p>Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015.</p>
<p>Hilton, J. and Gao, L. Measuring goodhart's law. https://openai.com/blog/measuring-goodharts-law/, 2022.</p>
<p>Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. URL https:// arxiv.org/pdf/1904.09751.pdf.</p>
<p>Kaplan, R., Sauer, C., and Sosa, A. Beating Atari with Natural Language Guided Reinforcement Learning, 2017.</p>
<p>Korbak, T., Perez, E., and Buckley, C. L. Rl with kl penalties is better viewed as bayesian inference. arXiv preprint arXiv:2205.11275, 2022.</p>
<p>Lampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson, K., Tessler, M. H., Creswell, A., McClelland, J. L., Wang, J. X., and Hill, F. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022.</p>
<p>Li, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J. Dialogue learning with human-in-the-loop. arXiv preprint arXiv:1611.09823, 2016.</p>
<p>Li, Z., Sharma, P., Lu, X. H., Cheung, J. C., and Reddy, S. Using interactive feedback to improve the accuracy and explainability of question answering systems postdeployment. arXiv preprint arXiv:2204.03025, 2022.</p>
<p>Lin, J., Fried, D., Klein, D., and Dragan, A. Inferring rewards from language in context. arXiv preprint arXiv:2204.02515, 2022.</p>
<p>Lin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring How Models Mimic Human Falsehoods, 2021.</p>
<p>Liu, Y., Deb, B., Teruel, M., Halfaker, A., Radev, D., and Awadallah, A. H. On improving summarization factual consistency from natural language feedback. arXiv preprint arXiv:2212.09968, 2022.</p>
<p>Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.</p>
<p>Luketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas, J., Grefenstette, E., Whiteson, S., and Rocktäschel, T. A survey of reinforcement learning informed by natural language. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 6309-6317. International Joint Conferences on Artificial Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/880. URL https://doi.org/ 10.24963/ijcai.2019/880.</p>
<p>Matiana, S., Smith, J., Teehan, R., Castricato, L., Biderman, S., Gao, L., and Frazier, S. Cut the carp: Fishing for zeroshot story evaluation. arXiv preprint arXiv:2110.03111, 2021.</p>
<p>Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. WebGPT: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https://arxiv. org/pdf/2112.09332.pdf.</p>
<p>Neelakantan, A., Xu, T., Puri, R., Radford, A., Han, J. M., Tworek, J., Yuan, Q., Tezak, N., Kim, J. W., Hallacy, C., Heidecke, J., Shyam, P., Power, B., Nekoul, T. E., Sastry, G., Krueger, G., Schnurr, D., Such, F. P., Hsu, K., Thompson, M., Khan, T., Sherbakov, T., Jang, J., Welinder, P., and Weng, L. Text and Code Embeddings by Contrastive Pre-Training, 2022.</p>
<p>Nguyen, K. X., Misra, D., Schapire, R., Dudík, M., and Shafto, P. Interactive learning from activity description. In International Conference on Machine Learning, pp. 8096-8108. PMLR, 2021.</p>
<p>OpenAI. Openai finetuning documentation. https://beta.openai.com/docs/api-reference/finetunes/create, 2022a.</p>
<p>OpenAI. Model index for researchers. https://beta.openai.com/docs/model-index-forresearchers, 2022b.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Preprint, 2022. URL https://cdn.openai.com/papers/ Training_language_models_to_follow_ instructions_with_human_feedback.pdf.</p>
<p>Perez, E., Kiela, D., and Cho, K. True few-shot learning with language models. Advances in Neural Information Processing Systems, 34:11054-11070, 2021.</p>
<p>Pruthi, D., Bansal, R., Dhingra, B., Soares, L. B., Collins, M., Lipton, Z. C., Neubig, G., and Cohen, W. W. Evaluating Explanations: How much do explanations from the teacher aid students?, 2021.</p>
<p>Qi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2020.</p>
<p>Radford, A. and Narasimhan, K. Improving Language Understanding by Generative Pre-Training, 2018. URL https://openai-assets. s3.amazonaws.com/research-covers/ language-unsupervised/language_ understanding_paper.pdf.</p>
<p>Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training, 2018.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models are Unsupervised Multitask Learners, 2019. URL https://d4mucfpksywv.cloudfront.net/ better-language-models/language_ models_are_unsupervised_multitask_ learners.pdf.</p>
<p>Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. URL https:// arxiv.org/pdf/2112.11446.pdf.</p>
<p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2020.</p>
<p>Rupprecht, C., Laina, I., Navab, N., Hager, G. D., and Tombari, F. Guide me: Interacting with deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8551-8561, 2018.</p>
<p>Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.</p>
<p>Scheurer, J., Campos, J. A., Chan, J. S., Chen, A., Cho, K., and Perez, E. Training language models with language feedback. In The First Workshop on Learning with Natural Language Supervision at ACL, 2022.</p>
<p>Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis, P., Izacard, G., You, Q., Nalmpantis, C., Grave, E., and Riedel, S. Peer: A collaborative language model. arXiv preprint arXiv:2208.11663, 2022.</p>
<p>Shi, W., Dinan, E., Shuster, K., Weston, J., and Xu, J. When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels. arXiv preprint arXiv:2210.15893, 2022.</p>
<p>Stacey, J., Belinkov, Y., and Rei, M. Supervising Model Attention with Human Explanations for Robust Natural Language Inference. arXiv preprint arXiv:2104.08142, 2021. URL https://arxiv.org/pdf/2104. 08142.pdf.</p>
<p>Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020. URL https://arxiv.org/ pdf/2009.01325.pdf.</p>
<p>Sumers, T. R., Ho, M. K., Hawkins, R. D., Narasimhan, K., and Griffiths, T. L. Learning rewards from linguistic feedback. feedback, 1(2):3, 2021.</p>
<p>Tam, A. C., Rabinowitz, N. C., Lampinen, A. K., Roy, N. A., Chan, S. C., Strouse, D., Wang, J. X., Banino, A., and Hill, F. Semantic exploration from language abstractions and pretrained representations. arXiv preprint arXiv:2204.05080, 2022.</p>
<p>Völske, M., Potthast, M., Syed, S., and Stein, B. TL;DR: Mining Reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pp. 59-63, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4508. URL https: //aclanthology.org/W17-4508.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Weston, J. E. Dialog-based language learning. Advances in Neural Information Processing Systems, 29, 2016.</p>
<p>Wiegreffe, S., Marasović, A., and Smith, N. A. Measuring association between labels and free-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10266-10284, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.804. URL https:// aclanthology.org/2021.emnlp-main.804.</p>
<p>Xu, J., Ung, M., Komeili, M., Arora, K., Boureau, Y.-L., and Weston, J. Learning new skills after deployment: Improving open-domain internet-driven dialogue with human feedback. arXiv preprint arXiv:2208.03270, 2022.</p>
<p>Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https: //arxiv.org/pdf/1909.08593.pdf.</p>
<h1>A. Additional derivations</h1>
<h2>A.1. Imitation Learning from Language Feedback as Bayesian Inference</h2>
<p>Language Feedback as Variational Inference Our goal is to produce a high-quality output $x_{1}$ for a context $c \sim p(c)$ (e.g., a summary of a document). We use an LM $\pi_{\theta}$ to generate an output $x_{1}$, by conditioning on the context $c$, i.e., $x_{1} \sim p_{\theta}\left(x_{1} \mid c\right)$. We then introduce the predicate $\mathcal{I}$, a random variable such that $\mathcal{I}=1$ if the output is high quality according to human preferences. We denote this data-generating process, shown in Fig. 5 left, as:</p>
<p>$$
p_{\theta}\left(c, x_{1}, \mathcal{I}\right)=p(c) \pi_{\theta}\left(x_{1} \mid c\right) p\left(\mathcal{I} \mid c, x_{1}\right)
$$</p>
<p>We frame our goal as maximizing the marginal log probability of quality across contexts: $\mathbb{E}<em 1="1">{c \sim p(c)} \log p(\mathcal{I}=1 \mid c)$. For a particular context $c$, we approximate $\log p(\mathcal{I}=1 \mid c)$ by introducing an importance sampling proposal distribution $q\left(x</em> \mid c\right)$ and using the Evidence Lower Bound (ELBo):</p>
<p>$$
\begin{aligned}
\log p(\mathcal{I}=1 \mid c) &amp; =\log \sum_{x_{1}} p_{\theta}\left(x_{1}, \mathcal{I}=1 \mid c\right) \
&amp; \geq \sum_{x_{1}} q\left(x_{1} \mid c\right) \log \frac{p_{\theta}\left(x_{1}, \mathcal{I}=1 \mid c\right)}{q\left(x_{1} \mid c\right)}
\end{aligned}
$$</p>
<p>We maximize the lower bound in Eq. 6, henceforth called $F(\theta, q)$, using an Expectation-Maximization (EM) procedure: alternating between maximizing $F$ w.r.t. the proposal distribution $q$ (E-step) and w.r.t. $\pi_{\theta}$ (M-step) We call this algorithm Imitation learning from Language Feedback.</p>
<p>E-step Maximizing $F(\theta, q)$ w.r.t $q$ corresponds to refining the proposal distribution $q$ to assign higher likelihood to high-quality texts. This is achieved by embedding $x_{1}$ into a data-generating process involving humans, by introducing the initial output $x_{0}$, and human feedback $f$ (via sum rule):</p>
<p>$$
\begin{aligned}
q\left(x_{1} \mid c\right)= &amp; \sum_{x_{0}, f} p_{\theta}\left(x_{0}, f, x_{1} \mid \mathcal{I}=1, c\right) \
&amp; \propto \sum_{x_{0}, f} p_{\theta}\left(x_{0}, f, x_{1} \mid c\right) p_{\theta}\left(\mathcal{I}=1 \mid c, x_{0}, f, x_{1}\right) \
= &amp; \sum_{x_{0}, f} p_{\theta}\left(x_{0} \mid c\right) p\left(f \mid c, x_{0}\right) p_{\theta}\left(x_{1} \mid c, x_{0}, f\right) \
&amp; \quad p_{\theta}\left(\mathcal{I}=1 \mid c, x_{0}, f, x_{1}\right)
\end{aligned}
$$</p>
<p>Eq. 10 gives rise to the following sampling procedure (see also Fig. 5, right): First, an LM is conditioned on the context $c$ and generates an initial output $x_{0}$. Second, a human provides language feedback $f$ on the $\left(c, x_{0}\right)$ pair. Third, the LM generates a refined text $x_{1}$ conditioned on $\left(c, x_{0}, f\right)$. Finally, a binary variable $\mathcal{I}$ indicates whether $x_{1}$ is a high-quality text, given an initial output $x_{0}$, feedback $f$, and a context $c$. We model $p_{\theta}\left(\mathcal{I}=1 \mid c, x_{0}, f, x_{1}\right)$ as a Boltzmann distribution:</p>
<p>$$
p_{\theta}\left(\mathcal{I}=1 \mid c, x_{0}, f, x_{1}\right) \propto \exp \left(R\left(c, x_{0}, f, x_{1}\right) / \beta\right)
$$</p>
<p>which uses a reward function $R$ defined in terms of four variables: $c, x_{0}, f, x_{1} ; \beta$ is a temperature hyperparameter. This Boltzmann distribution makes quality easy to evaluate since it expresses it as a reward function $R$ of a previous output and human language feedback.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Left: The graphical model of the target distribution $p_{\theta}$ that our algorithm approximates. $c$ is a context and $x_{1}$ is a high-quality LM output and $\mathcal{I}$ indicates whether the output is high-quality according to human preferences. Right: The graphical model of the proposal distribution $q$ we use for importance sampling. $x_{0}$ is an initial LM output and $f$ is language feedback on $x_{0}$.</p>
<p>We now argue why the E-step results in a proposal distribution that is better than the original distribution $p_{\theta}\left(x_{1} \mid c\right)$, i.e., why samples from $q\left(x_{1} \mid c\right)$ tend to be of higher quality than samples from $p_{\theta}\left(x_{1} \mid c\right)$. First, we know that $x_{0}$ is already a reasonably good output (since $\pi_{\theta_{\text {std }}} \approx \pi_{\theta}$ ). We can assume that the feedback $f$ is informative and high-quality. Therefore $x_{1} \sim p_{\theta}\left(x_{1} \mid c, x_{0}, f\right)$ is going to be of higher quality than $x_{0} \sim p_{\theta}\left(x_{0} \mid c\right)$ because it leverages useful information from the feedback. Furthermore, let us choose $R$ to assign higher values to refined texts $x_{1}$ that improve upon $x_{0}$ w.r.t to $f$ and $c$. Consequently, Eq. 11 assigns a higher likelihood to high-quality outputs $x_{1}$, allowing us to put additional weight on high-quality outputs and improving the proposal distribution $q$ further.</p>
<p>M-step Maximizing $F(\theta, q)$ w.r.t. the policy $\pi_{\theta}$ is equivalent to supervised learning (minimizing cross-entropy loss) on a distribution defined by $q$. To see that, we drop all the terms from Eq. 7 that do not depend on $\theta$ :</p>
<p>$$
\begin{aligned}
\underset{\theta}{\operatorname{argmax}} F(\theta, q) &amp; =\underset{\theta}{\operatorname{argmax}} \mathbb{E}<em 1="1">{x</em>=1 \mid c\right) \
&amp; =\underset{\theta}{\operatorname{argmin}} \mathbb{E}} \sim q\left(x_{1} \mid c\right)} \log p_{\theta}\left(x_{1}, \mathcal{I<em 1="1">{x</em> \mid c\right)
\end{aligned}
$$} \sim q\left(x_{1} \mid c\right)}-\log \pi_{\theta}\left(x_{1</p>
<p>ILF: Imitation learning from Language Feedback In ILF, we alternate between the E-step and M-step, using the pseudocode in Algorithm 1. In the M-step, we use the model from the previous iteration $\pi_{\theta_{\text {std }}}$ as both $p_{\theta}\left(x_{0} \mid c\right)$ and $p_{\theta}\left(x_{1} \mid c, x_{0}, f\right)$. In practice, we implement $R$ by conditioning an instruction-finetuned LM on a binary question such as Does this new text incorporate the feedback provided? Answer Yes or No. where the label $y$ is either $y_{\text {good }}\left({ }^{\text {w }}\right.$ Yes") or $y_{\text {bad }}\left({ }^{\text {w }}\right.$ No"). We use the probability of the positive answer $y_{\text {good }}$ given the prompt as a reward, i.e. $p\left(y_{\text {good }} \mid\right.$ prompt $)=$ $\frac{p\left(y_{\text {good }} \mid\right.}{\text { prompt })}$ $p\left(y_{\text {good }} \mid\right.$ prompt) $+p\left(y_{\text {bad }} \mid\right.$ prompt) $^{\text {prompt }}$. With these assumptions, $q$ takes the form:</p>
<p>$$
\begin{aligned}
q\left(x_{1} \mid c\right) \propto &amp; \mathbb{E}<em 0="0">{x</em>} \sim \pi_{\theta_{\text {std }}}\left(x_{0} \mid c\right)} \mathbb{E<em 0="0">{f \sim p\left(f \mid c, x</em> \
&amp; \pi_{\theta_{\text {std }}}\left(x_{1} \mid c, x_{0}, f\right) \exp \left(R\left(c, x_{0}, f, x_{1}\right) / \beta\right)
\end{aligned}
$$}\right)</p>
<p>We take advantage of this proposal distribution and perform the M-step, i.e., $\operatorname{argmax}<em 1="1">{\theta} F(\theta, q)$ on optimized data. Finally, we approximate sampling from $q\left(x</em>, f\right)$, and compute} \mid c\right)$ by best-of- $N$ sampling. To obtain a sample $x_{1} \sim q$, we sample $N$ refinements $\left{x_{1}^{1}, \ldots, x_{1}^{N}\right} \sim \pi_{\theta_{\text {std }}}\left(x_{1} \mid c, x_{0</p>
<p>$$
x_{1}=\operatorname{argmax}<em 1="1">{x</em>\right)
$$}^{i}} \exp R\left(c, x_{0}, f, x_{1}^{i</p>
<p>In summary, we show that ILF can be understood as Bayesian inference. This process involves updating an LM based on the evidence provided by language feedback. This lens highlights the correspondence between ILF and RL with Human Feedback (Ziegler et al., 2019; Stiennon et al., 2020, inter alia), which was previously demonstrated to be equivalent to Bayesian inference (Korbak et al., 2022).</p>
<h1>B. Additional Related Work on Language in RL Settings</h1>
<p>Language has been widely used in RL for various purposes (see Luketina et al., 2019, for an overview), such as specifying tasks ("instruction following", inter alia) driving exploration (Tam et al., 2022), inferring reward functions (Lin et al., 2022; Sumers et al., 2021; Fidler et al., 2017, inter alia), and training a model via strong supervision (Andreas et al., 2017; Kaplan et al., 2017), reward shaping (Goyal et al., 2019), or by providing descriptions of trajectories (Nguyen et al., 2021). In contrast, we use language to correct faulty behavior. Other work uses language feedback at test time to correct mistakes in a model's behavior, e.g., image segmentation (Rupprecht et al., 2018) or code generation (Elgohary et al., 2020; Austin et al., 2021). In contrast, we use feedback to train models, and our approach does not require human intervention at test time.</p>
<h2>C. Dataset Collection and Analysis</h2>
<p>Annotation process To ensure the high quality of our human annotations, we employ experienced annotators sourced through the data-labeling company Surge AI. During an onboarding and evaluation process, we calculate author-annotator agreement on the binary comparison task and manually review the quality of the written feedback and ideal summaries to ensure their high quality. Then we select 31 qualified annotators for all annotation tasks, though they can choose which tasks to participate in and for how long. To further ensure the quality of our annotations, we provide detailed instructions, which</p>
<p>we provide to the annotators, and update throughout the process to ensure continuous improvement (these instructions can be found in Appendix I). To measure the agreement rate between the annotators and the authors, we select a sample of 10 Reddit posts from the training dataset as a gold standard and have 17 annotators label them. When comparing the binary comparison annotations with our own ones, this results in an author-annotator agreement rate of $81.0 \%$. We also calculate the average agreement rate between all the possible annotator combinations, yielding an annotator-annotator agreement of $70 \%$. By utilizing these thorough processes and evaluations, we can ensure the accuracy and reliability of our human annotations.</p>
<p>Dataset Analysis The feedback we collect typically addresses the most critical shortcomings of the summaries. In $92.0 \%$ of our train samples, the annotators' feedback was complete and addressed all important shortcomings of the summary, as reported by the annotators. Across our train dataset, we observe that the majority of the feedback pertains to coverage ( $77.0 \%$ ), with smaller percentages relating to accuracy ( $16.0 \%$ ), coherence ( $5.0 \%$ ), and other categories ( $2.0 \%$ ). We also analyze the length of the various summaries and feedback, measured in the average number of tokens. Our human-written summaries have an average length of $41.0 \pm 0.1$ tokens, the extracted human summaries from Reddit had an average length of $32.5 \pm 0.1$ tokens, the initial summaries generated by FeedME have an average length of $29.3 \pm 0.1$ tokens, and the feedback written by annotators on these initial summaries has an average length of $20.4 \pm 0.2$ tokens.</p>
<p>In addition to these analyses, we also measure the time it takes annotators to complete various tasks (i.e., binary comparison, feedback writing, and ideal summary writing) on our development dataset. We ignore outliers and consider only samples with annotation times of at least 20 seconds and at most 420 seconds ( 7 minutes). Annotators take $61.5 \pm 5.3$ seconds on average on the binary comparison task, $182.5 \pm 6.3$ seconds on the feedback task, and $195.5 \pm 6.1$ seconds on the ideal summary task. We plot the annotation times on the development dataset for the tasks of annotating binary comparisons, writing feedback, and writing ideal summaries as histograms in Fig. 6. The annotators are much faster at annotating binary comparisons than feedback or ideal summaries. Writing feedback takes less time than writing ideal summaries, which is expected, as critiquing a task is usually easier than solving it. These comprehensive evaluations demonstrate the high quality and thoroughness of our dataset and annotation processes.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Histogram Plot of annotation times (in seconds) of the binary comparison task, the feedback annotation task and the human summary writing task. The evaluation is conducted on the development dataset. We observe that annotators are much quicker at the binary comparison task, which is expected. The results also show that writing feedback takes less time than writing an ideal summary.</p>
<h1>D. Targeted Word Removal Details</h1>
<p>Below is an example of how we instruct or "prompt" an LM to remove specific, offensive words from a sentence.
"In this text, many toxic and offensive words are used: You are such a jerk, and a nice person, and an idiot. The ideal text should remove the word jerk, but otherwise be unchanged: You are"</p>
<p>Here, the target completion is " such a nice person and an idiot." More formally, we sample offensive sentences by using $k$ offensive words from a fixed set of 25 offensive words drawn uniformly at random (without replacement). Each offensive sentence also includes the words "nice person" in addition to all the offensive words. For each $k \in{1, \ldots, 10}$, we sample</p>
<p>50 offensive sentences. The task is then to remove $l \in[1,2,3]$ offensive words from a given sentence with $k \geq l$. Since we include the words "nice person" in the offensive sentence, we can remove $l=k$ offensive words and still have a target sentence that intuitively makes sense.</p>
<h1>E. Details about Ranking Procedure</h1>
<p>We use a standard ranking scheme where each of $K$ summaries is given a rank between 1 and $K$ (inclusive). Sometimes refinements are exact copies of the initial summaries or are very similar in terms of quality, which is why we allow for summaries to be tied. When calclating the win rate we assign 0.5 wins for tied samples. We assign the rank $r^{\prime}$ to all summaries ranked in a tie, where $r^{\prime}=\frac{r+(r+n-1)}{2}, r$ is the rank of the tied elements, and $n$ is the number of ties at the rank. For example, we map a ranking of $(1,2,2,4,5) \rightarrow(1,2.5,2.5,4,5)$ and a ranking of $(1,2,3,3,3) \rightarrow(1,2,4,4,4)$.</p>
<h2>F. Reward Model</h2>
<p>Here we describe the various RMs that we evaluate in more detail. We evaluate the final RM that we use, which produces a language output (e.g., " Yes" or " No") and a standard reward model that produces a scalar output.</p>
<p>Standard RM. Akin to (Stiennon et al., 2020), we remove the last embedding layer of a language model and train it to output a scalar value. This scalar value predicts which summary, $x \in\left{x_{0}^{0}, x_{0}^{1}\right}$, is better as judged by a human, given a context $c$. We use the OPT 13B LM, introduced in (Zhang et al., 2022), as the base model for our RM and finetune it on the human preference comparisons that we collected. It is worth noting that it is not possible to add linear layers on top of GPT-3 models provided via the API, which is why we use the OPT model.</p>
<p>Reward Model with Language Output. In addition to the classic RM (Stiennon et al., 2020), we train an RM to output language tokens instead of a scalar value. To do so, we finetune an LM to classify whether a summary $x_{0}$ is high quality or not, by training it to predict a label $y \in\left{y_{\text {good }}, y_{b a d}\right}$. We then finetune the LM to maximize $\lambda \log p\left(x_{0}\right)+\log p\left(y \mid x_{0}\right)$, where $\lambda \in[0,1]$, chosen using the development dataset. The complete loss can also be written as:</p>
<p>$$
\mathcal{L}\left(p_{\theta}, x, y\right)=-\lambda \cdot \sum_{t=1}^{|x|} \log p_{\theta}\left(x_{t} \mid x_{&lt;t}\right)-\sum_{t=1}^{|y|} \log p_{\theta}\left(y_{t} \mid x, y_{&lt;t}\right)
$$</p>
<p>where the subscript $t$ indicates the token index. We evaluate the finetuned LM on a given summary $x_{0}$ by computing $p\left(y_{\text {good }} \mid x_{0}\right)$. The best RM overall uses the following instruction Is the above an excellent summary of the given text? An excellent summary is coherent, accurate, concise, and detailed. Answer with Yes or No., which we refer to as the OPT-RM (when finetuning OPT-13B) and GPT-3 Binary (when finetuning GPT-3-175B). We also explore finetuning on another prompt, where we provide both summaries $A$ and $B$ to the LM and instruct it to indicate which summary is preferred, i.e. Question: Which summary is the better one? An excellent summary is coherent, accurate, concise, and detailed. Answer with A or B. We then finetune the LM on the label of the preferred summary (according to binary human feedback), i.e. on $y \in\left{y_{A}, y_{B}\right}$. We evaluate the finetuned LM on a given summary $x_{0}$ by computing $p\left(y_{A} \mid x_{0}\right)$. We refer to this RM as Comparison RM. We explore two RMs, namely, OPT-13B Zhang et al. (2022), and GPT-3-175B and refer to Appendix G for the hyperparameters we use and to Appendix J. 4 for the prompt templates).</p>
<p>Results. We evaluate all RMs on our validation dataset, and calculate the accuracy of predicting the preferred summary out of two, based on human preferences. Table 4 shows the complete results, and here we report on some of the RMs trained on 5 K samples. The OPT model with the standard RM loss achieves an accuracy of $71.8 \pm 2.0 \%$ on the validation dataset. The results further show that both of our methods for training OPT with the LM loss outperform the standard RM loss, with OPT comparison achieving an accuracy of $72.6 \pm 1.9 \%$, and OPT-RM an accuracy of $73.4 \pm 1.9 \%$. We obtain similar results with finetuning GPT-3-175B, achieving an accuracy of $71.2 \pm 2.0 \%$ with the GPT3 Comparison, and an accuracy of $74.2 \pm 2.0 \%$ with GPT-3 Binary, which outperforms the OPT-RM.</p>
<p>Based on these results, we further evaluate the OPT Binary and GPT-3-175B Binary models on the development dataset that we use to evaluate the scoring functions in $\S 4.2$. We calculate the fraction of times the refinement selected by an RM is better than a randomly-selected refinement ("win rate"), according to a ranking given by human evaluators (see App. E for more details). The results can be found in Table 3. OPT-RM achieves a win rate of $63.3 \pm 2.7 \%$, and the GPT-3-175B Binary</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Scoring Function</th>
<th style="text-align: center;">Win Rate vs Random Selection (in \%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task Specific Heuristic</td>
<td style="text-align: center;">Max Length</td>
<td style="text-align: center;">$65.0 \pm 2.7$</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Embedding Similarity</td>
<td style="text-align: center;">$48.3 \pm 3.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">InstructRM Ensemble</td>
<td style="text-align: center;">$\mathbf{5 6 . 0} \pm \mathbf{3 . 0}$</td>
</tr>
<tr>
<td style="text-align: center;">Finetuning on 5K samples</td>
<td style="text-align: center;">OPT Binary</td>
<td style="text-align: center;">$\mathbf{6 3 . 3} \pm \mathbf{2 . 7}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 Binary</td>
<td style="text-align: center;">$61.8 \pm 2.9$</td>
</tr>
</tbody>
</table>
<p>Table 3: In a human evaluation, we compare reward models and ranking methods on the development dataset (in the same way as in Fig 2. Both RMs are trained on 5K samples and outperform the zero-shot methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;"># Params</th>
<th style="text-align: center;">Train Data Size</th>
<th style="text-align: center;">Development Accuracy (in \%)</th>
<th style="text-align: center;">Validation Accuracy (in \%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LM Loss / Our dataset</td>
<td style="text-align: center;">OPT Comparison</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">5 K</td>
<td style="text-align: center;">$66.5 \pm 3.3$</td>
<td style="text-align: center;">$72.6 \pm 1.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT RM</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">5 K</td>
<td style="text-align: center;">$70.0 \pm 3.2$</td>
<td style="text-align: center;">$69.6 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT RM</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$54.5 \pm 3.5$</td>
<td style="text-align: center;">$53.4 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT RM</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">$68.5 \pm 3.2$</td>
<td style="text-align: center;">$67.2 \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT RM</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">5 K</td>
<td style="text-align: center;">$\mathbf{6 9 . 5} \pm \mathbf{3 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 4} \pm \mathbf{1 . 9}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 Comparison</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5 K</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">$71.2 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 Binary</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5 K</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{7 4 . 2} \pm \mathbf{2 . 0}$</td>
</tr>
<tr>
<td style="text-align: center;">RM Loss / Our dataset</td>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">5 K</td>
<td style="text-align: center;">$68.5 \pm 3.2$</td>
<td style="text-align: center;">$71.8 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;">RM Loss / Stiennon et al. (2020) train dataset</td>
<td style="text-align: center;">Stiennon et al. (2020) RM</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">64 K</td>
<td style="text-align: center;">$58.0 \pm 3.4$</td>
<td style="text-align: center;">$63.8 \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;">LM Loss / Stiennon et al. (2020) train dataset</td>
<td style="text-align: center;">OPT Binary</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">90 K</td>
<td style="text-align: center;">$69.0 \pm 3.2$</td>
<td style="text-align: center;">$68.6 \pm 2.0$</td>
</tr>
</tbody>
</table>
<p>Table 4: In a human evaluation, we evaluate various RMs on the development dataset and validation dataset. We also report the results of training on the train dataset of Stiennon et al. (2020) and evaluating on our development and validation datasets. We calculate the accuracy of predicting which of two summaries is preferred by a human.
model achieved a win rate of $61.8 \pm 2.9 \%$. In this evaluation, OPT-RM outperforms GPT-3 Binary. When considering the results from both the validation and development datasets, both OPT-RM and GPT-3-Binary seem to perform similarly. Given that we have more control over the training process of OPT, the possibility of releasing the model, and the cost involved in training using OpenAI's API, we select OPT-RM model as our reward model for comparison with ILF. In Figure 7, we show the validation accuracy of OPT-RM trained on 100, 1K, and 5K samples on a log-log plot. The figure shows scaling when increasing the dataset size.</p>
<p>We further evaluate results for finetuning OPT-RM on the dataset of Stiennon et al. (2020), and also evaluating their model with 1.3B parameters on our dataset. We observe that the binary preference distribution of the training dataset has a significant impact on the performance of the reward model. For example, OPT-RM trained on 5K samples of our own train dataset (i.e., our final reward model) achieves an accuracy of $61.9 \pm 0.2 \%$ on the test set from Stiennon et al. (2020) (not shown in Table 4). When this same model is trained on 90 K samples from the train dataset of Stiennon et al. (2020), it achieves an accuracy of $69.3 \pm 0.2 \%$ on their test set (also not shown in Table 4). In contrast, this same model trained on 90 K samples from their train dataset achieves an accuracy of only $68.6 \pm 2.0 \%$ on our validation dataset, which is significantly lower than the accuracy of $73.4 \pm 1.9 \%$ achieved by the model trained on 5 K samples of our own train dataset. Similar patterns can be observed when comparing the OPT Binary model with 1.3B parameters trained on 5K samples of our own train dataset to the released 1.3B reward model trained by Stiennon et al. (2020) on approx. 64 K samples of their own train dataset. The former model achieves an accuracy of $69.6 \pm 2.0 \%$ on our validation dataset, while the latter only achieves an accuracy of $63.8 \pm 2.1 \%$ (note, though, that the RMs are trained with different loss functions). These results highlight two important considerations: (1) preference distributions can vary significantly and have a strong effect on what a reward model learns, and (2) the sample efficiency of a reward model depends heavily on the train and test distributions. If the test distribution differs from the train distribution, reward models may be very sample inefficient and fail to accurately learn the true distribution, even when given significantly more samples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Samples</th>
<th style="text-align: center;">Epochs</th>
<th style="text-align: center;">Prompt Loss Weight</th>
<th style="text-align: center;">Learning Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.05 *$</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">5 K</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
</tr>
</tbody>
</table>
<p>Table 5: We report the chosen hyperparameters of finetuning on 100, 1K, and 5K samples of Human Summaries. *This hyperparameter is optimal but used only for finetuning on Human Summaries. For finetuning on Refinements and Initial Summaries we inadvertently use the prompt loss weight 0 .</p>
<h1>G. Hyper Parameters</h1>
<h2>G.1. Generating Refinements</h2>
<p>For the targeted word removal experiments (§3), we use greedy decoding until 200 tokens or / $n$ is generated. For all summarization experiments we sample up to 48 tokens (as in Stiennon et al., 2020) with nucleus sampling (Holtzman et al., 2019) with $p=0.95$ and temperature $t=1.0$. We strip non-alphanumeric characters (e.g., newlines) from the beginning of sampled summaries. We further remove empty white spaces in the generated summaries and remove all text that comes after a new line token $/ n$. Due to the maximum token length, sampled summaries sometimes end with incomplete sentences. Thus, we remove ending sentences that do not end in "."', "!", or "?". The described temperature and post-processing are applied to all summary generations, i.e., for generating initial summaries, refinements, and test summaries.</p>
<h2>G.2. Finetuning on Summaries</h2>
<p>We conduct independent hyperparameter optimization sweeps with three dataset sizes of human summaries of 100, 1 K and 5 K samples, and then use the same hyperparameters for finetuning on refinements (ILF) and finetuning on initial summaries. We choose to run the hyperparameter sweep on Human summaries since this will not give an unfair advantage to our algorithm that finetunes on refinements. For the sweep, we utilize the train dataset of human summaries (consisting of 100, 1 K , and 5 K samples) and evaluate on the development dataset. Unfortunately, the OpenAI API only provides validation loss and token accuracy for batches of the development dataset, making it impossible to evaluate the model on the full development dataset during training. As a result, we utilize the model API to evaluate on the full development dataset after finetuning and calculate the perplexity of the generated summaries as a performance measure.</p>
<p>To determine the optimal hyperparameters, we perform a sweep over a range of values for the following parameters: epochs ${1,2,3,4}$, prompt loss weight ${0,0.01,0.05,0.1}$, and learning rates ${0.02,0.05,0.1,0.2}$. We first sweep over epochs and select the best value, then perform a sweep using that value for the prompt loss weight, and so on. Our empirical observations indicate that the number of epochs has the greatest impact on perplexity, with training for more than one epoch resulting in overfitting. The selected hyperparameters can be found in Table 5.</p>
<p>During the finetuning phase for the REFINEMENTS and Initial Summaries datasets with 1K samples each, we made an error in our hyperparameter selection. Instead of using a prompt loss weight of 0.05 , we mistakenly used a value of 0 , when finetuning on human summaries. While this error may have slightly impacted our results, the difference in perplexity between the two settings is minimal, with a value of 6.68 for a prompt loss weight of 0.05 and 6.71 for a prompt loss weight of 0 . Despite this mistake, our method still outperforms finetuning on human summaries for 1 K samples, as well as finetuning on initial summaries using suboptimal hyperparameters.</p>
<h2>G.3. Multiple Iterations of ILF</h2>
<p>To evaluate multiple iterations of ILF, i.e., multiple iterations of refining-and-finetuning, we finetune GPT-3-175B on a refinement dataset with 200 and 300 samples. Thus we conduct a hyperparameter optimization on a train dataset of 200 and 300 refinements and evaluate on a development dataset of 200 refinements (instead of human summaries). To determine the optimal hyperparameters, we perform a sweep over a range of values for the following parameters: epochs ${1,2,3,4}$, prompt loss weight ${0,0.01,0.05,0.1}$, and learning rates ${0.02,0.05,0.1,0.2}$. We first sweep over epochs and select the best value, then perform a sweep using that value for the prompt loss weight, and so on. For finetuning on 200 refinements we select the following hyperparameters: epochs $=1$, prompt loss weight $=0.05$, learning rate multiplier $=0.1$. For finetuning on 300 refinements we select epochs $=1$, prompt loss weight $=0$, and learning rate multiplier $=0.2$.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Here we plot the validation accuracy of OPT-RM trained on 100, 1K, and 5K samples on a log-log plot. The figure shows scaling when increasing the dataset size.</p>
<h1>G.4. Finetuning Reward Models</h1>
<p>OPT Reward Model. For finetuning the OPT Reward Model, we perform bayesian hyperparameter optimization for each of the three different types of reward models: Standard, Comparison and Classification (see section F). We sweep over the learning rate in the range of $\left[1 \mathrm{e}^{-5}, 1 \mathrm{e}^{-6}\right]$ and the batch size ${32,64}$ for all the models. For the reward models using the language loss, we also optimize the prompt-loss weight ${0.0,0.01,0.05,0.1,0.5,1.0}$. We run 10 iterations per model and evaluate all the sweeps with the 200 development examples. We use a linear learning rate scheduler and a weight decay of 0.1 for all the runs. The optimal batch size is 32 for all the models. The best prompt loss weight is 0.01 for both the Comparison and Classification RMs. As for the learning rate, we use $9.3 \mathrm{e}^{-6}$ for the Standard RM, $5.8 \mathrm{e}^{-6}$ for the Classification RM and $1 \mathrm{e}^{-6}$ for the Comparison RM. In the final finetuning, we select the best RM in the validation split over 10 epochs.</p>
<p>GPT-3 Reward Model. In order to finetune GPT-3-175B as an RM, we utilize the OpenAI API. We finetune two types of RMs: the Comparison RM, which learns to predict which of two summaries is superior, and the Classification RM, which predicts whether a given summary is of high quality or not. For cost considerations, we conduct hyperparameter tuning on a training dataset of 1 K samples (instead of 5 K ) and evaluate on a development dataset of 200 samples. We use a dataset with 1 K samples for cost reasons. We then apply the same hyperparameters when finetuning on 5 K samples while implementing early stopping in terms of epochs. Due to the binary nature of the human preference annotations in the classification reward model, the effective train dataset size for this model is doubled to 2 K samples.</p>
<p>In order to determine the optimal hyperparameters, we perform a sweep over a range of values for the number of epochs ${1,2,3,4}$ and the prompt loss weights ${0,0.001,0.005,0.01,0.05,0.1,0.5}$. The OpenAI API provides classification accuracy (for both the comparison and classification tasks) for the full development dataset after each epoch, allowing us to select the appropriate number of epochs and prompt loss weight. When finetuning on 5 K samples, we utilize early stopping to prevent overfitting, using 1 epoch and a prompt loss weight of 0 for the comparison model and 4 epochs and a prompt loss weight of 0.001 for the classification model. We use default values for all other hyperparameters, which may vary depending on the dataset size.</p>
<h2>H. Additional Results</h2>
<h2>H.1. Analyis of Finetuned Models</h2>
<p>In Table 6, we evaluate GPT-3-175B on various finetuning datasets used for finetuning: the refinements, the initial summaries, and the human summaries. We evaluate the log-likelihood of GPT-3-175B on the summaries of 1 K samples from the various train datasets (i.e. initial summaries, refinements, and human summaries). Concretely, we pass the whole prompt to</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Evaluation of models finetuned on 5K initial summaries, refinements, and human summaries on 500 samples from the corresponding validation datasets. For example, the model finetuned on human summaries is evaluated on 500 human summaries from the validation dataset. The model finetuned on refinements has a significantly lower negative log-likelihood than the model finetuned on human summaries.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Neg. Log Likelihood of GPT-3-175B <br> on 1K train samples of respective distribution</th>
<th style="text-align: center;">$D_{K L}($ GPT-3-175B $)$ finetuned $)$ (in nats)</th>
<th style="text-align: center;">$D_{K L}($ finetuned $)$ GPT-3-175B $)$ (in nats)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Finetuned on Initial Summaries</td>
<td style="text-align: center;">$1.19 \pm 0.01$</td>
<td style="text-align: center;">$0.43 \pm 0.11$</td>
<td style="text-align: center;">$0.83 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: center;">Finetuned on Refinements</td>
<td style="text-align: center;">$1.37 \pm 0.01$</td>
<td style="text-align: center;">$0.60 \pm 0.10$</td>
<td style="text-align: center;">$1.10 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;">Finetuned on Human Summaries</td>
<td style="text-align: center;">$1.61 \pm 0.01$</td>
<td style="text-align: center;">$0.12 \pm 0.09$</td>
<td style="text-align: center;">$0.55 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">OPT-RM best-of-64 FeedME</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.17</td>
</tr>
</tbody>
</table>
<p>Table 6: First we evaluate the log-likelihood of GPT-3-175B on the 1K samples of the various data distributions that we finetune on. Then we empirically calculate the KL-divergence by sampling 2000 texts of length 64 tokens from GPT-3-175B and evaluating the log-likelihood of the finetuned models on the samples (for the reverse KL we sample from the finetuned models and evaluate GPT-3-175B on the samples). We report the mean and standard error across 2 runs. For Best of 64 on a specific reward model, we use the analytical formula $K L(N, R M)=\log N-\frac{N-1}{N}$ (see also (Hilton \&amp; Gao, 2022)).</p>
<p>GPT-3-175B, including the Reddit post, but only evaluate the log-likelihood of the completion, i.e. the generated summary. We also measure the (reverse) KL divergence (following Gao et al., 2022) between an ILF-finetuned model and the pretrained LM before ILF-training, $D_{\mathrm{KL}}($ finetuned $)$ GPT-3-175B). We sample unconditionally (i.e. using a beginning of sentence token) from the finetuned models and evaluate the log-likelihood of the generated text with GPT-3-175B. We also report the forward KL divergence, $D_{\mathrm{KL}}($ GPT-3-175B $\mid$ finetuned $)$. We discuss the results in $\S 4.6$.</p>
<h1>H.2. Results: ILF + OPT-RM</h1>
<p>In this section, we present the full results of our best-performing method ILF + OPT-RM and other additional methods (see $\S 4.3 .1$ for a description of ILF + OPT-RM and $\S 4.3 .3$ for a discussion of the results). We conduct the same evaluation as described in $\S 4.3 .2$, i.e. in a human evaluation, annotators rank various test summaries based on quality. We then calculate the win rate against human written summaries, which we use as an evaluation metric. Importantly, all methods evaluated here are trained on datasets with 5 K samples. Note that the methods compared here are not exactly the same as the methods compared in Fig. 3. Concretely, the test summaries generated by the methods finetuning on refinements (ILF), finetuning on human summaries, and OPT-RM best-of-64 FeedME are the same as in Fig. 3, for the test summaries generated by corresponding methods trained on 5 K samples. Here, however, we don't evaluate FeedME and finetuning on initial summaries. However, we evaluate ILF + OPT-RM (best-of-64), our best-performing model, which we also added to Fig. 3 for reference. We also evaluate a new method called Finetuned on Feedback + Refinements, which we describe below.</p>
<p>For finetuning on feedback + refinements, we us a title, post, and summary as input and the model is trained to predict the</p>
<p>corresponding feedback and refinement. Our motivation for this approach is that generating feedback first may improve the quality of the resulting refinements, similar to the findings of previous work on self-prompting methods <em>Saunders et al. (2022); Bai et al. (2022)</em> and the Chain of Thought (CoT) prompting technique <em>Wei et al. (2022)</em>. CoT has been shown to improve the performance of models across various tasks <em>Wei et al. (2022)</em> when allowing the model to reason before answering a question. For finetuning on feedback and refinements, we utilize the initial summaries that were used to gather human feedback, as well as the refinements generated by our method. We use the loss $\log p(x_{1},f|\text{prompt})+\lambda \log p(\text{prompt})$, i.e. we learn to predict the refinement and the feedback. We employ the same hyperparameters as in the finetuning on refinements algorithm (including the prompt loss weight). During testing, we require initial summaries, from which we generate feedback and refinements. As initial summaries, we use the test samples generated by FeedME (as evaluated in Figure 3). To ensure compatibility with the 48-token length restriction of the test summaries, we append the special end token / $n$ ### to the end of the feedback and refinements during training. At test time, we set the maximum number of tokens to generate 300, and terminate generation when the stop-word / $n$ ### appears. We then apply the same postprocessing procedure outlined in Appendix G.1 to shorten the refinements to 48 tokens. We refer to Appendix J.3 for the exact prompt templates we used.</p>
<p>We present all the results in Fig. 9. We find that finetuning on a set of 5K refinements achieves a win rate of $36.0 \pm 1.8 \%$, while ILF + OPT-RM (best-of-64) has a win rate of $50.8 \pm 1.9 \%$, achieving human-level summarization performance (see $\S 4.3 .3$ for a more detailed discussion). OPT-rM best-of-64 FeedMe achieves a win rate of $45.1 \pm 1.9 \%$, finetuning on a set of 5K human-generated summaries achieves a win rate of $35.4 \pm 1.8 \%$, and finetuning on a combination of 5K feedback and refinements has a win rate of $26.1 \pm 1.7 \%$. It is worth noting that the performance of finetuning on feedback and refinements is lower than that of finetuning on refinements alone. We attribute this to the increased difficulty of generating both feedback and refinements and believe that this discrepancy may be due to limitations in our models, dataset size, or hyperparameters. Previous work has demonstrated the feasibility of training models to generate feedback <em>Saunders et al. (2022); Bai et al. (2022)</em>, so we believe that further optimization and experimentation may improve the performance of this method. We further want to note that the results for finetuning on 5K refinements, 5K human summaries, and best-of-64 FeedME deviate from the results in Fig 3. This is because we compare different methods with each other, and human annotations generally contain some amount of noise (given that different people annotate the same samples).
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: How often human evaluators prefer summaries from ILF: Finetuned on Refinements, OPT-RM best-of-64 FeedME, ILF + OPT-RM (best-of-64), finetuning on human summaries, and finetuning on feedback + refinements (all methods finetuned on 5K samples). ILF + OPT-RM (best-of-64) generates summaries of a similar quality to human summaries. Finetuning on feedback + refinements performs worse than finetuning on refinements (ILF).</p>
<h1>H.3. Multiple Iterations of ILF</h1>
<p>Our experiments suggest that ILF is an effective method for leveraging language feedback in the training of LMs. Here we explore ILF in its most general form by doing multiple iterations of refining-and-finetuning.</p>
<p>Dataset Improvement. In this experiment, we evaluate the effectiveness of iterative refinement of the dataset distribution using ILF. To this end, we first finetune GPT-3-175B on 100 refinements from iteration 1 of ILF (i.e. doing one iteration</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Finetuning on 100 refinements is tied with finetuning on 100 initial summaries.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8545 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8545</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8545</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-cea62bcb7b951befe148daa34b6100593c2031f0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cea62bcb7b951befe148daa34b6100593c2031f0" target="_blank">Exploring and Benchmarking the Planning Capabilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work constructs a comprehensive benchmark suite encompassing both classical planning benchmarks and natural language scenarios, and investigates the use of many-shot in-context learning to enhance LLM planning, exploring the relationship between increased context length and improved planning performance.</p>
                <p><strong>Paper Abstract:</strong> Classical and natural language planning tasks remain a difficult domain for modern large language models (LLMs). In this work, we lay the foundations for improving planning capabilities of LLMs. First, we construct a comprehensive benchmark suite encompassing both classical planning benchmarks and natural language scenarios. This suite includes algorithms to methodically generate instances of tasks with varying levels of difficulty, allowing for rigorous and systematic evaluation of LLM performance. Next, we investigate the use of many-shot in-context learning to enhance LLM planning, exploring the relationship between increased context length and improved planning performance. In addition, we demonstrate the positive impact of fine-tuning LLMs on optimal planning paths. We also probe the efficacy of chain-of-thought reasoning methods to improve LLM planning performance. Moreover, we probe the performance of the proposed methods in out-of-distribution scenarios, assessing the ability to generalize to novel and unseen planning challenges. Finally, we investigate model's failure modes and reveal insights that hold true across different benchmarks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8545.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8545.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 1.5 Pro on BlocksWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5 Pro evaluated on BlocksWorld planning benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini 1.5 Pro (a multimodal, long-context-capable family model) was evaluated on the BlocksWorld spatial planning benchmark presented both in PDDL and mapped natural language; evaluation used many-shot in-context learning, chain-of-thought methods, and comparisons to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemini 1.5 Pro, a member of the Gemini family (multimodal, designed to exploit long context), described in the paper as able to make effective use of additional and longer context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BlocksWorld</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Discrete object-manipulation spatial planning (stacking/unstacking blocks on a table; requires 2D spatial relations and sequence of manipulations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Problems provided in PDDL and also mapped to natural-language via slot-filling templates. Prompts used few-shot / many-shot in-context learning (ICL) with example problem+solution 'shots'; verification of generated plans performed with a PDDL verifier (Fast-Downward) for PDDL, and by mapping NL plans back to PDDL via regex for natural language. Output format constrained (e.g., 'Your plan as plain text without formatting').</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Many-shot ICL (up to hundreds of shots), prompt engineering for output format; chain-of-thought style inference methods were evaluated generally (ToT, MCTS, debate) though primary BlocksWorld results reported under ICL and SFT. No external search tool used to solve the planning tasks for Gemini 1.5 Pro in ICL experiments (plans produced directly by LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>1-shot accuracy ~35% (BlocksWorld, Gemini 1.5 Pro). With ~40 shots performance reached ~48% on the BlocksWorld benchmark (paper reports Gemini 1.5 Pro 40-shots ~48% which outperformed GPT-4 at its best many-shot). In Table 5 and other plots Gemini 1.5 Pro on 'Our-BW' shows per-block-size accuracies such as 66%/38.5% for 1-shot on 3/4 blocks (natural language variants) and 100%/44.6% on some PDDL settings (reported in table comparisons) — see paper for full breakdown by shot count and block counts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative successes (non-trivial accuracy increasing with shots) indicate the model can produce valid spatial plans in many instances. The paper includes a detailed failure analysis: (1) constraint violations (e.g., attempting to place a block on a non-clear block) and (2) failure-to-goal cases were characterized, showing where spatial constraints were violated stepwise; the analysis shows the model often fails early on in OOD larger-instances, implying partial or brittle spatial reasoning rather than robust systematic spatial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to GPT-4 Turbo and Gemini 1.5 Flash in the same BlocksWorld tasks: Gemini 1.5 Pro outperformed other evaluated models in many-shot regimes (e.g., 1-shot 35% vs Gemini 1.5 Flash 26% and GPT-4 ~0% in 1-shot). For many-shot (40+ shots) Gemini 1.5 Pro (≈48%) exceeded GPT-4's best many-shot reported (GPT-4 peaks ~43% at 200 shots in the reported comparisons). The paper also contrasts ICL results with SFT (fine-tuning) where smaller models fine-tuned on optimal plans can outperform ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degrades as instance difficulty (number of blocks) increases; failure modes include (i) explicit constraint violations (attempting illegal actions), (ii) generating plans that don't reach the goal, and (iii) generating out-of-vocabulary actions (not observed for BlocksWorld in SFT/ICL in many cases). The model exhibits learned correlations between plan step number and action type (a training bias), causing systematic failures and poor OOD generalization (e.g., training on 3–7 blocks then evaluating on 8–20 blocks yields much lower accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Benchmarking the Planning Capabilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8545.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8545.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo on BlocksWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo evaluated on BlocksWorld planning benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 Turbo (referred to as GPT-4 in the paper) was evaluated on BlocksWorld PDDL and mapped natural-language tasks using few-shot prompting, and compared with Gemini models and fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 Turbo (OpenAI's GPT-4 variant). In the paper GPT-4 refers to GPT-4 Turbo; experiments used few-shot prompting and comparisons to other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BlocksWorld</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Discrete object-manipulation spatial planning (stacking/unstacking blocks; requires handling spatial constraints and action preconditions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same PDDL and natural-language mapping as for other models; few-shot in-context learning with up to many shots, outputs validated via PDDL verifier or mapping back to PDDL.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard few-shot ICL (1-shot, 2-shot up to many-shots), prompts designed with example problem+plan pairs. No external planner used at inference; verification used Fast-Downward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-4 1-shot performance on BlocksWorld was reported as close to zero in one comparison, but GPT-4 achieves higher performance in some prompt variants and datasets (Table 5 shows GPT-4 Turbo 1-shot on Val-BW: 49% for 3-blocks, 32.4% for 4-blocks). In many-shot regimes GPT-4 peaks at ~43% (reported as best 200-shot performance in one comparison) on the authors' BlocksWorld settings, below Gemini 1.5 Pro many-shot in that setting.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Mixed: GPT-4 can produce valid plans in some prompt settings (see Val-BW comparisons), but in some experiments GPT-4 underperforms relative to Gemini 1.5 Pro in many-shot BlocksWorld. The paper's failure analyses (constraint violation, early-step failures) apply across models and indicate that even GPT-4 exhibits brittle spatial planning behavior in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to Gemini 1.5 Pro and Gemini 1.5 Flash. Depending on prompt and dataset, GPT-4 sometimes outperforms Gemini 1.5 Pro (e.g., on Val-BW natural-language prompts GPT-4 did better than Gemini 1.5 Pro in that table), but on the authors' many-shot BlocksWorld dataset Gemini 1.5 Pro often outperformed GPT-4. Fine-tuned smaller models (SFT) can outperform GPT-4 on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-4 shows variability across prompt templates and shot counts; performance can be sensitive to prompt formatting. Like other models, GPT-4 suffers constraint violations and goal-failure cases and has limited OOD generalization as task complexity increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Benchmarking the Planning Capabilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8545.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8545.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 1.5 Pro on Mini-Grid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5 Pro evaluated on Mini-Grid planning/navigation benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini 1.5 Pro was evaluated on Mini-Grid (grid navigation with locked cells, keys and connectivity) both in PDDL and mapped natural language; many-shot ICL and verification with a planner were used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemini 1.5 Pro, a Gemini-family model described as effective at exploiting long context and multimodal capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Mini-Grid</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based navigation and manipulation (2D grid with rooms, locked cells, keys, connectivity graph) requiring spatial navigation and reasoning about connectivity and key/lock interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Mini-Grid instances generated in PDDL and mapped to natural-language; prompts used few-shot/many-shot ICL; verification done via PDDL planner mapping back from NL plans. Example 1-shot prompts included PDDL problem description and expected plan format.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Few-shot/many-shot ICL; models required to produce full plans in one output. No external online simulator used at inference; plans verified post-hoc. Chain-of-thought methods were explored broadly in the paper but Mini-Grid primary results reported under ICL scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>With 400 shots, Gemini 1.5 Pro reached ~77% accuracy on Mini-Grid (reported in the text: 'With 400 shots, Gemini 1.5 Pro reaches 77% accuracy'). Performance increased with number of few-shot examples for all models; GPT-4 slightly better with natural language in some cases per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High many-shot accuracy (77%) on Mini-Grid demonstrates the model can handle spatial navigation tasks when provided many examples. The dataset contains explicit spatial constraints (connectivity, locked cells) and the model's success implies learned handling of such constraints in many cases. However, the generalization and failure analyses remain important—performance improves with many examples, indicating reliance on in-context pattern matching rather than robust algorithmic spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Gemini 1.5 Pro compared with GPT-4 and Gemini 1.5 Flash: Gemini models performed comparably on PDDL and natural-language Mini-Grid tasks; GPT-4 sometimes slightly outperformed on natural-language inputs. CoT/search-based methods (ToT/MCTS) were evaluated elsewhere in paper but not reported as the main enhancers for Mini-Grid.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance depends strongly on number of shots; fewer shots produce much lower accuracy. OOD generalization beyond the distributions trained/prompted on is limited (paper documents similar trends in BlocksWorld). No detailed per-action failure breakdown for Mini-Grid is included in main text, but the general failure categories (constraint violation, failure to reach goal) apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Benchmarking the Planning Capabilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8545.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8545.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 1.0 S (SFT) on BlocksWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.0 S supervised fine-tuned on optimal plans for BlocksWorld</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller Gemini model (Gemini 1.0 S) was supervised-fine-tuned (SFT) using optimal plans generated by a classical planner (Fast-Downward), yielding large accuracy gains on BlocksWorld in-distribution while revealing generalization limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.0 S (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemini 1.0 S — a smaller Gemini-family model fine-tuned in a supervised manner on optimal plans generated by Fast-Downward planner for BlocksWorld and Logistics datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BlocksWorld</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Discrete object-manipulation spatial planning (stacking/unstacking).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Supervised fine-tuning (SFT) on datasets (PDDL problems paired with optimal plans from Fast-Downward). Evaluated with a PDDL verifier to compute accuracy. Also experimented with cross-split generalization (e.g., fine-tune on 3–7 blocks, evaluate on 8–20).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Supervised fine-tuning on optimal plans (Fast-Downward generated) rather than pure ICL; verification via planner; no external search at inference besides what the model learned during finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>High in-domain accuracies: BlocksWorld (3–7 blocks) — 96.26% (Table 1). BlocksWorld (8–9 blocks) — 92.6%; BlocksWorld (8–20 blocks) — 67.0%. Generalization table shows asymmetry: fine-tuning on 8–20 and evaluating on 3–7 yields 98.27%, while fine-tuning on 3–7 and evaluating on 8–20 yields 34.20% (Table 2), showing good within-distribution performance but limited OOD scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>SFT yields near-perfect execution on in-distribution BlocksWorld instances, demonstrating that the model can internalize optimal spatial plans when trained on them. However, OOD evaluations (e.g., larger block counts) show marked degradation, indicating the learned behavior may not reflect fully generalizable spatial reasoning but rather learned mappings correlated with training distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>SFT with Gemini 1.0 S outperforms many-shot ICL (even with larger models) on in-distribution BlocksWorld and Logistics tasks (Table 3/1). For example, Gemini 1.0 S (SFT) achieves 96.26% on BW(3–7) versus much lower ICL numbers for Gemini 1.5 Flash/Pro and GPT-4 in many-shot settings. However, SFT performance drops on harder/longer instances compared to models fine-tuned or trained on broader distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Marked OOD generalization gap: fine-tuning on small/easier instances does not generalize well to much larger instances (e.g., training on 3–7 -> eval on 8–20 yields 34.2%). Failure modes mirror ICL: constraint violations, failing to reach goals, and (in some Logistics SFT cases) generation of illegal actions. Performance declines with instance complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Benchmarking the Planning Capabilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the planning abilities of large language models-a critical investigation <em>(Rating: 2)</em></li>
                <li>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 1)</em></li>
                <li>Travelplanner: A benchmark for real-world planning with language agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8545",
    "paper_id": "paper-cea62bcb7b951befe148daa34b6100593c2031f0",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Gemini 1.5 Pro on BlocksWorld",
            "name_full": "Gemini 1.5 Pro evaluated on BlocksWorld planning benchmark",
            "brief_description": "Gemini 1.5 Pro (a multimodal, long-context-capable family model) was evaluated on the BlocksWorld spatial planning benchmark presented both in PDDL and mapped natural language; evaluation used many-shot in-context learning, chain-of-thought methods, and comparisons to fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5 Pro",
            "model_description": "Gemini 1.5 Pro, a member of the Gemini family (multimodal, designed to exploit long context), described in the paper as able to make effective use of additional and longer context.",
            "model_size": null,
            "puzzle_name": "BlocksWorld",
            "puzzle_type": "Discrete object-manipulation spatial planning (stacking/unstacking blocks on a table; requires 2D spatial relations and sequence of manipulations).",
            "task_setup": "Problems provided in PDDL and also mapped to natural-language via slot-filling templates. Prompts used few-shot / many-shot in-context learning (ICL) with example problem+solution 'shots'; verification of generated plans performed with a PDDL verifier (Fast-Downward) for PDDL, and by mapping NL plans back to PDDL via regex for natural language. Output format constrained (e.g., 'Your plan as plain text without formatting').",
            "mechanisms_or_strategies": "Many-shot ICL (up to hundreds of shots), prompt engineering for output format; chain-of-thought style inference methods were evaluated generally (ToT, MCTS, debate) though primary BlocksWorld results reported under ICL and SFT. No external search tool used to solve the planning tasks for Gemini 1.5 Pro in ICL experiments (plans produced directly by LLM).",
            "performance_metrics": "1-shot accuracy ~35% (BlocksWorld, Gemini 1.5 Pro). With ~40 shots performance reached ~48% on the BlocksWorld benchmark (paper reports Gemini 1.5 Pro 40-shots ~48% which outperformed GPT-4 at its best many-shot). In Table 5 and other plots Gemini 1.5 Pro on 'Our-BW' shows per-block-size accuracies such as 66%/38.5% for 1-shot on 3/4 blocks (natural language variants) and 100%/44.6% on some PDDL settings (reported in table comparisons) — see paper for full breakdown by shot count and block counts.",
            "evidence_of_spatial_reasoning": "Quantitative successes (non-trivial accuracy increasing with shots) indicate the model can produce valid spatial plans in many instances. The paper includes a detailed failure analysis: (1) constraint violations (e.g., attempting to place a block on a non-clear block) and (2) failure-to-goal cases were characterized, showing where spatial constraints were violated stepwise; the analysis shows the model often fails early on in OOD larger-instances, implying partial or brittle spatial reasoning rather than robust systematic spatial planning.",
            "comparisons": "Compared directly to GPT-4 Turbo and Gemini 1.5 Flash in the same BlocksWorld tasks: Gemini 1.5 Pro outperformed other evaluated models in many-shot regimes (e.g., 1-shot 35% vs Gemini 1.5 Flash 26% and GPT-4 ~0% in 1-shot). For many-shot (40+ shots) Gemini 1.5 Pro (≈48%) exceeded GPT-4's best many-shot reported (GPT-4 peaks ~43% at 200 shots in the reported comparisons). The paper also contrasts ICL results with SFT (fine-tuning) where smaller models fine-tuned on optimal plans can outperform ICL.",
            "limitations_or_failure_cases": "Performance degrades as instance difficulty (number of blocks) increases; failure modes include (i) explicit constraint violations (attempting illegal actions), (ii) generating plans that don't reach the goal, and (iii) generating out-of-vocabulary actions (not observed for BlocksWorld in SFT/ICL in many cases). The model exhibits learned correlations between plan step number and action type (a training bias), causing systematic failures and poor OOD generalization (e.g., training on 3–7 blocks then evaluating on 8–20 blocks yields much lower accuracy).",
            "uuid": "e8545.0",
            "source_info": {
                "paper_title": "Exploring and Benchmarking the Planning Capabilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 Turbo on BlocksWorld",
            "name_full": "GPT-4 Turbo evaluated on BlocksWorld planning benchmark",
            "brief_description": "GPT-4 Turbo (referred to as GPT-4 in the paper) was evaluated on BlocksWorld PDDL and mapped natural-language tasks using few-shot prompting, and compared with Gemini models and fine-tuned models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_description": "GPT-4 Turbo (OpenAI's GPT-4 variant). In the paper GPT-4 refers to GPT-4 Turbo; experiments used few-shot prompting and comparisons to other LLMs.",
            "model_size": null,
            "puzzle_name": "BlocksWorld",
            "puzzle_type": "Discrete object-manipulation spatial planning (stacking/unstacking blocks; requires handling spatial constraints and action preconditions).",
            "task_setup": "Same PDDL and natural-language mapping as for other models; few-shot in-context learning with up to many shots, outputs validated via PDDL verifier or mapping back to PDDL.",
            "mechanisms_or_strategies": "Standard few-shot ICL (1-shot, 2-shot up to many-shots), prompts designed with example problem+plan pairs. No external planner used at inference; verification used Fast-Downward.",
            "performance_metrics": "GPT-4 1-shot performance on BlocksWorld was reported as close to zero in one comparison, but GPT-4 achieves higher performance in some prompt variants and datasets (Table 5 shows GPT-4 Turbo 1-shot on Val-BW: 49% for 3-blocks, 32.4% for 4-blocks). In many-shot regimes GPT-4 peaks at ~43% (reported as best 200-shot performance in one comparison) on the authors' BlocksWorld settings, below Gemini 1.5 Pro many-shot in that setting.",
            "evidence_of_spatial_reasoning": "Mixed: GPT-4 can produce valid plans in some prompt settings (see Val-BW comparisons), but in some experiments GPT-4 underperforms relative to Gemini 1.5 Pro in many-shot BlocksWorld. The paper's failure analyses (constraint violation, early-step failures) apply across models and indicate that even GPT-4 exhibits brittle spatial planning behavior in some settings.",
            "comparisons": "Directly compared to Gemini 1.5 Pro and Gemini 1.5 Flash. Depending on prompt and dataset, GPT-4 sometimes outperforms Gemini 1.5 Pro (e.g., on Val-BW natural-language prompts GPT-4 did better than Gemini 1.5 Pro in that table), but on the authors' many-shot BlocksWorld dataset Gemini 1.5 Pro often outperformed GPT-4. Fine-tuned smaller models (SFT) can outperform GPT-4 on these tasks.",
            "limitations_or_failure_cases": "GPT-4 shows variability across prompt templates and shot counts; performance can be sensitive to prompt formatting. Like other models, GPT-4 suffers constraint violations and goal-failure cases and has limited OOD generalization as task complexity increases.",
            "uuid": "e8545.1",
            "source_info": {
                "paper_title": "Exploring and Benchmarking the Planning Capabilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gemini 1.5 Pro on Mini-Grid",
            "name_full": "Gemini 1.5 Pro evaluated on Mini-Grid planning/navigation benchmark",
            "brief_description": "Gemini 1.5 Pro was evaluated on Mini-Grid (grid navigation with locked cells, keys and connectivity) both in PDDL and mapped natural language; many-shot ICL and verification with a planner were used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5 Pro",
            "model_description": "Gemini 1.5 Pro, a Gemini-family model described as effective at exploiting long context and multimodal capabilities.",
            "model_size": null,
            "puzzle_name": "Mini-Grid",
            "puzzle_type": "Grid-based navigation and manipulation (2D grid with rooms, locked cells, keys, connectivity graph) requiring spatial navigation and reasoning about connectivity and key/lock interactions.",
            "task_setup": "Mini-Grid instances generated in PDDL and mapped to natural-language; prompts used few-shot/many-shot ICL; verification done via PDDL planner mapping back from NL plans. Example 1-shot prompts included PDDL problem description and expected plan format.",
            "mechanisms_or_strategies": "Few-shot/many-shot ICL; models required to produce full plans in one output. No external online simulator used at inference; plans verified post-hoc. Chain-of-thought methods were explored broadly in the paper but Mini-Grid primary results reported under ICL scaling.",
            "performance_metrics": "With 400 shots, Gemini 1.5 Pro reached ~77% accuracy on Mini-Grid (reported in the text: 'With 400 shots, Gemini 1.5 Pro reaches 77% accuracy'). Performance increased with number of few-shot examples for all models; GPT-4 slightly better with natural language in some cases per the paper.",
            "evidence_of_spatial_reasoning": "High many-shot accuracy (77%) on Mini-Grid demonstrates the model can handle spatial navigation tasks when provided many examples. The dataset contains explicit spatial constraints (connectivity, locked cells) and the model's success implies learned handling of such constraints in many cases. However, the generalization and failure analyses remain important—performance improves with many examples, indicating reliance on in-context pattern matching rather than robust algorithmic spatial reasoning.",
            "comparisons": "Gemini 1.5 Pro compared with GPT-4 and Gemini 1.5 Flash: Gemini models performed comparably on PDDL and natural-language Mini-Grid tasks; GPT-4 sometimes slightly outperformed on natural-language inputs. CoT/search-based methods (ToT/MCTS) were evaluated elsewhere in paper but not reported as the main enhancers for Mini-Grid.",
            "limitations_or_failure_cases": "Performance depends strongly on number of shots; fewer shots produce much lower accuracy. OOD generalization beyond the distributions trained/prompted on is limited (paper documents similar trends in BlocksWorld). No detailed per-action failure breakdown for Mini-Grid is included in main text, but the general failure categories (constraint violation, failure to reach goal) apply.",
            "uuid": "e8545.2",
            "source_info": {
                "paper_title": "Exploring and Benchmarking the Planning Capabilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gemini 1.0 S (SFT) on BlocksWorld",
            "name_full": "Gemini 1.0 S supervised fine-tuned on optimal plans for BlocksWorld",
            "brief_description": "A smaller Gemini model (Gemini 1.0 S) was supervised-fine-tuned (SFT) using optimal plans generated by a classical planner (Fast-Downward), yielding large accuracy gains on BlocksWorld in-distribution while revealing generalization limits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini 1.0 S (SFT)",
            "model_description": "Gemini 1.0 S — a smaller Gemini-family model fine-tuned in a supervised manner on optimal plans generated by Fast-Downward planner for BlocksWorld and Logistics datasets.",
            "model_size": null,
            "puzzle_name": "BlocksWorld",
            "puzzle_type": "Discrete object-manipulation spatial planning (stacking/unstacking).",
            "task_setup": "Supervised fine-tuning (SFT) on datasets (PDDL problems paired with optimal plans from Fast-Downward). Evaluated with a PDDL verifier to compute accuracy. Also experimented with cross-split generalization (e.g., fine-tune on 3–7 blocks, evaluate on 8–20).",
            "mechanisms_or_strategies": "Supervised fine-tuning on optimal plans (Fast-Downward generated) rather than pure ICL; verification via planner; no external search at inference besides what the model learned during finetuning.",
            "performance_metrics": "High in-domain accuracies: BlocksWorld (3–7 blocks) — 96.26% (Table 1). BlocksWorld (8–9 blocks) — 92.6%; BlocksWorld (8–20 blocks) — 67.0%. Generalization table shows asymmetry: fine-tuning on 8–20 and evaluating on 3–7 yields 98.27%, while fine-tuning on 3–7 and evaluating on 8–20 yields 34.20% (Table 2), showing good within-distribution performance but limited OOD scaling.",
            "evidence_of_spatial_reasoning": "SFT yields near-perfect execution on in-distribution BlocksWorld instances, demonstrating that the model can internalize optimal spatial plans when trained on them. However, OOD evaluations (e.g., larger block counts) show marked degradation, indicating the learned behavior may not reflect fully generalizable spatial reasoning but rather learned mappings correlated with training distributions.",
            "comparisons": "SFT with Gemini 1.0 S outperforms many-shot ICL (even with larger models) on in-distribution BlocksWorld and Logistics tasks (Table 3/1). For example, Gemini 1.0 S (SFT) achieves 96.26% on BW(3–7) versus much lower ICL numbers for Gemini 1.5 Flash/Pro and GPT-4 in many-shot settings. However, SFT performance drops on harder/longer instances compared to models fine-tuned or trained on broader distributions.",
            "limitations_or_failure_cases": "Marked OOD generalization gap: fine-tuning on small/easier instances does not generalize well to much larger instances (e.g., training on 3–7 -&gt; eval on 8–20 yields 34.2%). Failure modes mirror ICL: constraint violations, failing to reach goals, and (in some Logistics SFT cases) generation of illegal actions. Performance declines with instance complexity.",
            "uuid": "e8545.3",
            "source_info": {
                "paper_title": "Exploring and Benchmarking the Planning Capabilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the planning abilities of large language models-a critical investigation",
            "rating": 2
        },
        {
            "paper_title": "Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 1
        },
        {
            "paper_title": "Travelplanner: A benchmark for real-world planning with language agents",
            "rating": 1
        }
    ],
    "cost": 0.01585625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring and Benchmarking Planning CapaBILITIES OF LARGE LANGUAGE MODELS</h1>
<p>Bernd Bohnet<em> Azade Nova</em> Aaron T Parisi Kevin Swersky Katayoon Goshvadi<br>Hanjun Dai Dale Schuurmans Noah Fiedel Hanie Sedghi<br>Google DeepMind</p>
<h4>Abstract</h4>
<p>Classical and natural language planning tasks remain a difficult domain for modern large language models (LLMs). In this work, we lay the foundations for improving planning capabilities of LLMs. First, we construct a comprehensive benchmark suite encompassing both classical planning benchmarks and natural language scenarios. This suite includes algorithms to methodically generate instances of tasks with varying levels of difficulty, allowing for rigorous and systematic evaluation of LLM performance. Next, we investigate the use of many-shot in-context learning to enhance LLM planning, exploring the relationship between increased context length and improved planning performance. In addition, we demonstrate the positive impact of fine-tuning LLMs on optimal planning paths. We also probe the efficacy of chain-of-thought reasoning methods to improve LLM planning performance. Moreover, we probe the performance of the proposed methods in out-of-distribution scenarios, assessing the ability to generalize to novel and unseen planning challenges. Finally, we investigate model's failure modes and reveal insights that hold true across different benchmarks.</p>
<h2>1 INTRODUCTION</h2>
<p>Intelligent agents require the ability to plan to proactively chart a course of action to achieve their objectives. This capacity for strategic foresight is considered fundamental to intelligent behavior (Russell \&amp; Norvig, 2016). While classical search algorithms have long been the cornerstone of planning studies, machine learning techniques, particularly Monte-Carlo Tree Search (MCTS) and reinforcement learning, have emerged as useful additions, significantly expanding the capabilities of modern planning systems.</p>
<p>With the advent of powerful large language models (LLMs), there are new opportunities to both revisit classical planning problems, and to further explore new problems through natural language specification that reflects the ambiguity and uncertainty of real-world domains. Planning capability is important for many tasks such as game playing, meeting scheduling and trip planning. Research is already underway to leverage the commonsense knowledge of LLMs in real-world tasks (Huang et al., 2022; Singh et al., 2023; Ding et al., 2023) and to generate plans (Valmeekam et al., 2023; Hao et al., 2023; Guan et al., 2024). The research has shed some light on LLMs' struggle with planning tasks. Even state-of-the-art LLMs may produce ineffective or even incorrect plans, even in straightforward scenarios. Our paper focuses on analyzing and improving the planning capability of LLM systems.</p>
<p>We provide a scalable benchmark suite in both PDDL and natural language to measure planning capability of LLMs. Specifically, we explore two distinct planning representations: the formal Planning Domain Definition Language (PDDL) (McDermott et al., 1998), which provides a standardized representation for classical planning problems and allows for rigorous plan validation; and natural language, which offers a more flexible and intuitive representation better reflecting real-world scenarios. For both scenarios, we provide a code for generating as many instances with a degree of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A simple instance of BlocksWorld planning benchmark: visual, PDDL and natural language descriptions side by side. Problem definition includes list of objects, initial and goal states. We have also included LLM's output plan in the in-context learning scenario of Figure 2.
difficulty of choice. We also provide a mapping method for translating PDDL benchmarks to natural language and measure the performance of the generated benchmarks. The generated planning tasks are scalable and can grow to examine and assist stronger models. Figure 1 depicts a simple sample of BlocksWorld benchmark with description both in PDDL and natural language. Each instance in BlocksWorld consists of a set of blocks, a table and a robot hand, where the goal is to move from one block configuration to another. The problem definition includes list of objects, initial and goal states. The figure also includes the LLM's output plan from the in-context learning scenario, which we extensively analyze using various LLM models and benchmarks in Figure 2.</p>
<p>We explore the planning capability of LLMs using both In Context Learning (ICL) through the many-shot paradigm as well and through chain-of-thought (CoT) (Wei et al., 2022) inference time techniques; we also explore fine-tuning strategies. We observe that carefully instructing the model using ICL leads to a significant boost in planning performance, which can be further improved by using a many-shot approach with long context. Moreover, CoT reasoning strategies (MCTS, Tree-ofthought, debate-as-reasoning) allow smaller models to perform closer to SoTA frontier models in the natural language task domain. Our results show that fine-tuning with the optimal plan can lead to near-perfect accuracy, even when using relatively small models.</p>
<p>Next, we investigate both in-domain and out-of-domain generalization and note that the proposed plans demonstrate excellent in-domain generalization: instances with similar complexity are solved with similar levels of accuracy. For ICL, prompting with easier instances leads to better performance on hard instances compared to prompting with hard instances. Fine-tuning leads to better performance for both in-domain and out-of-domain scenarios, even with a much smaller model, compared to ICL with long-context.</p>
<p>Lastly, we probe the failure modes of the model. We categorize the failure modes into three categories: failure to satisfy environmental constraints, failure to meet the goal and failure to generate legal actions in a given state. We note that not all of these failure modes are present across all benchmarks and methods. Moreover, as the instance complexity increases the model success rate decreases. Additionally, we pinpoint failure modes that are result of biases in training data emphasizing the importance of data curation during training.</p>
<h1>1.1 Related Work</h1>
<p>Prior works investigations the planning capabilities of LLMs, have found that these models struggle to solve planning tasks (Hao et al., 2023; Valmeekam et al., 2023; 2024; Kambhampati et al., 2024), In contrast, we show that LLMs can be capable of solving such tasks and one can reach near-perfect accuracy for some scenarios, with certain methods. We run experiments on the same BlocksWorld benchmark as these prior works, and generate additional difficult cases (i.e., adding more blocks). Similar to these works, we also use PDDL verifiers to compute accuracy. See Section 3.4 for details.</p>
<p>Xie et al. (2024) proposed a TravelPlanner benchmark and showed GPT4-Turbo can solve some of the benchmark tasks with a success rate of $0.6 \%$. The TripPlanner benchmark used in our work has</p>
<p>two main differences: it is not an agent-based environment and rather a natural language benchmark, and it has unique answers due to carefully designed constraints.</p>
<p>Stechly et al. (2024) suggests that LLMs are not capable of generalizing their plans on BlocksWorld if one uses chain-of-thought (CoT) (Wei et al., 2022). In this work we show positive results in terms of generalization performance.</p>
<p>There is another line of work that uses a hybrid approach, meaning that they either use an external tool to solve the planning tasks (Kambhampati et al., 2024; Hirsch et al., 2024), or reformulate the problem as another task such as SMT (Hao et al., 2024) and use an external tool to solve it. Lehnert et al. (2024) use $A^{*}$ as search mechanism and a specific transformer architecture to achieve planning capability for that specific architecture. We differ from this line of work in that we focus on teaching LLM itself to perform the planning task.</p>
<p>In addition to standard few-shot prompting, we also make use of inference time techniques that construct a chain-of-thought (Wei et al., 2022), namely tree-of-thought (ToT) (Yao et al., 2023) and Monte-Carlo Tree Search (MCTS) (Hao et al., 2023) and debate-as-reasoning (Du et al., 2023). We demonstrate that these methods can considerably improve an LLMs planning capabilities such that smaller open-source models that use these approaches can outperform larger foundational models.</p>
<h1>2 BENCHMARK</h1>
<p>To evaluate the planning capabilities of LLMs, we assemble a benchmark suite that appropriately represents various classical and natural-language planning tasks. On these benchmarks, we assess LLM performance using In-Context Learning (ICL), Supervised Fine-Tuning (SFT), and chain-ofthought (CoT) methods for planning.
PDDL (Planning Domain Definition Language): PDDL McDermott et al. (1998) is a standardized language used in artificial intelligence for representing planning problems. PDDL provides a formal way to describe the initial state of an environment, a goals, the space of valid actions, and the statetransition properties of actions in the environment. PDDL has two main components (1) Domain: Describes the general characteristics of the planning problem, including the types of objects, actions, and predicates (conditions that can be true or false). (2) Problem: Defines a specific instance of the planning problem within the domain, including the initial state of the world and the goals to be achieved.</p>
<p>We select three datasets that use PDDL, generated additional subsets of them, and additionally map these datasets to natural language for an additional evaluation task. Additionally, we select two native natural language datasets, containing Trip Planning and Calendar Scheduling tasks (Zheng et al., 2024). For the PDDL-based datasets, we select BlocksWorld, Logistics, and Mini-Grid. We then translated the PDDL problem descriptions from these datasets into natural language to compare performance when using formal and informal representations.</p>
<h3>2.1 PDDL BENCHMARKS</h3>
<p>The creation of all PDDL datasets follows a three-step procedure. (1) Initially, the process involves the creation of an initial state and a goal (target state). (2) Subsequently, the initial state and goal are utilized to formulate a problem in PDDL. (3) Finally, the problem is solved using a classic planner Fast-Downward ${ }^{1}$.</p>
<p>This procedure is repeated with increasingly difficult configurations for a select number of problems. The result of this procedure are additional datasets that comprise a set of PDDL problems and solutions of various difficulty. Importantly, this procedure enables us to create datasets with increasingly difficult problems and any number of samples, which are appropriate for assessing the ability to plan using different methods, such as in-context learning versus Supervised Fine-Tuning. Moreover, we can scale the dataset generation and create as many instances as needed for different investigations. We provide the details of our benchmark suite below.</p>
<p>We perform planning for BlocksWorld, Logistics and Minigrid both for PDDL and Natural Language. For the mapping to natural language, we use a slot filling technique which maps each predicate of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the initialization and goal as well the action to sentences (Appendix B.2). For the verification of the plans, we use regular expressions to map the plan in Natural Language back to PDDL.
BlocksWorld: BlocksWorld is a standard planning problem from International Planning Conference (IPC) $2000^{2}$. This domain consists of a set of blocks, a table and a robot hand, where the goal is to move from one block configuration to another. We generate a dataset for 3 to 7 blocks. As detailed in the Appendix B.1, we produced 28 k unique BlocksWorld samples. From these, 25.5 k were randomly selected for the training set and 2,500 for the validation set.
Logistics: Logistics is an AI planning problem from IPC-1998 ${ }^{3}$ expressed in PDDL that involves arranging the delivery of packages to their destinations using trucks within cities and airplanes between cities. The aim is to optimize transportation modes under constraints such as vehicle capacities and locations, showcasing model's ability to manage multi-step logistics efficiently.
Mini-Grid: Mini-Grid is a task from Artificial Intelligence Planning Systems (AIPS)-1998 ${ }^{4}$, also expressed in PDDL. We create various floorplans with rooms containing random configurations of key shapes. The goal then is for a robot to navigate from an initial position to a designated goal cell.</p>
<h1>2.2 Native Natural Language Planning Benchmarks</h1>
<p>Trip Planning: Trip Planning is a task from NaturalPlan (Zheng et al., 2024) benchmark focusing on planning a trip itinerary under given constraints. The goal of the task is to find an itinerary satisfying constraints such as the order of visiting N cities. It includes enough constraints for each instance such that there is only one solution to the task, which makes the evaluation of the predictions straightforward.
Calendar Scheduling: Calendar Scheduling from the NaturalPlan (Zheng et al., 2024) benchmark represents the task of scheduling a meeting of either 30 minutes or an hour among up to 7 attendees. The attendees may have a busy schedule or a light schedule with less than half of the working hours spent in meetings.</p>
<h2>3 EXPERIMENTS</h2>
<p>Previous works demonstrated that, without intervention, LLMs often struggle with even simple planning tasks (Hao et al., 2023; Valmeekam et al., 2023; 2024; Kambhampati et al., 2024). LLMs often lack the information on how to structure their plan constructively, and struggle to plan around the enumerated constraints. In this section we present our experimental results for the interventions we investigate, demonstrating that they lead to significant improvements to the LLMs planning capability. We also investigate plan generalization (i.e., the ability to generalize to unseen instances) in several scenarios.</p>
<p>For PDDL experiments, we measure accuracy of the generated plan with a verifier (Howey et al., 2004). For natural language experiments we rely on either recasting the task to PDDL and verifying with a verifier, or extracting the answer and comparing it to expected results (Zheng et al., 2024). In all experiments, GPT-4 refers to GPT-4 Turbo and we may omit "Turbo" for space constraints, also if we do not mention Pro or Flash Gemini 1.5 refers to Gemini 1.5 Pro.</p>
<h3>3.1 In-CONTEXT LEARNING</h3>
<p>For in-context learning (Brown et al., 2020), we adhere to the standard procedure, employing a prompt containing several examples for the task. Each example comprises a planning problem statement and its corresponding solution, referred to as a shot. Following the examples, the test problem is added without the corresponding solution. Subsequently, an LLM receives this prompt and is expected to generate a plan following the format and logic of the examples in the prompt. See Appendix A for examples of prompts.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: PDDL Planning and Natural Language Planning with few-shots for different LLMs. Natural language text are generated from formal PDDL problem definitions.</p>
<h1>3.1.1 MANY-SHOT IN-CONTEXT LEARNING</h1>
<p>We evaluate the planning capability of the model as we add more examples ("shots") into the context, inspired by the success of many-shot learning across a large number of tasks (Agarwal et al., 2024). The challenge of "in-context planning" involves understanding a specific task and problem through a limited number of examples. Additionally, it requires the models to produce a solution without checking each planning step to confirm if a proposed move is correct. The model has to create a plan in a single inference step, keeping 'in mind' all the constraints the task imposes.</p>
<p>Figure 2 shows the in-context learning performance on classical planning and natural language benchmarks as we vary the number of shots. We consider Gemini 1.5 Pro (GeminiTeam et al., 2024b), GPT-4 Turbo (OpenAI et al., 2024), Gemini 1.5 Flash (GeminiTeam et al., 2024b) and Gemma 2 27b (Team et al., 2024) models. Overall, we notice that for natural language and PDDL scenarios, models have similar trends in terms of planning accuracy as we increase the number of shots. Moreover, different models are impacted differently as we provide additional number of shots, e.g., Gemini 1.5 Pro outperforms other models both in one shot scenario and as we increase the number of shots; indicating that the model not only can plan better with a fewer number of examples/shots, it can also make effective use of additional and longer context. Gemini 1.5 Flash -a smaller, faster and more efficient model than Gemini 1.5 Pro is generally outperformed by Gemini 1.5 Pro but occasionally matches GPT-4 performance.</p>
<p>BlOCKSWORLD: Figure 2a, 2d show the performance of Gemini 1.5 models on this benchmark as we increase the number of few-shot examples. We note that as we increase the number of shots GPT-4 performance increases while Gemini 1.5 Pro's performance saturates or degrades as we go beyond 40 shots. The 1-shot planning capability of Gemini 1.5 Pro and Gemini 1.5 Flash reaches reaches $35 \%$ and $26 \%$, while GPT-4 performance is close to zero. Moreover the 40-shots planning capability of Gemini 1.5 Pro reaches $48 \%$ range which performs better than the best (200-shots) performance of GPT-4, which peaks at $43 \%$.</p>
<p>Logistics: The planning capabilities of GPT-4 and Gemini 1.5 models on the Logistics benchmark are shown in Figure 2e for PDDL and in Figure 2b for Natural Language. The 1-shot planning capability of Gemini 1.5 Pro reaches $43 \%$ for PDDL and for Natural Language $48 \%$. Moreover for Gemini 1.5 Pro increasing the context consistently lead to better results, indicating that the model can</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Natural Language Planning with few-shots using native natural language datasets.
make effective use of additional contexts. For Gemini 1.5 Flash and GPT-4, the performance drops slight for PDDL and Natural Language.
Mini-Grid: Figure 2f and Figure 2c show the performance of GPT-4 and Gemini models as we increase the number of few-shot examples for PDDL and Natural Language, respectively. The Gemini models perform comparably for both PDDL and Natural Language, although GPT-4 appears to perform slightly better with Natural Language. Increasing the number of few-shot examples leads to better performance for all models. With 400 shots, Gemini 1.5 Pro reaches $77 \%$ accuracy.
Trip Planning and Calendar Scheduling: Figure 3a shows the performance on the Trip Planning and Calendar Scheduling natural language tasks as we increase the number of few-shot examples, respectively. We observe that, in both benchmarks, for both GPT-4 and Gemini 1.5 Flash the performance first increases with the number of shots and after a certain point, having more shots leads to worse model performance. However, for Gemini 1.5 Pro performance improves as the number of shot increases. Therefore, Gemini 1.5 Pro seems to be making more efficient use of additional shots compared to the other two. On the other hand GPT-4 performs better in the 1-shot scenario compared to the other two models for Trip Planning, while Gemini 1.5 Pro has a higher accuracy in the 1-shot setting.</p>
<p>Overall, we observe that the trend of accuracy vs number of shots depends both on the model and on the benchmark.</p>
<h1>3.1.2 EFFECT OF INFERENCE TIME TECHNIQUES</h1>
<p>In addition to standard ICL, we consider inference time ICL methods that are based on constructing a chain-of-thought (Wei et al., 2022): Tree-of-Thought (ToT) (Yao et al., 2023), Monte Carlo Tree Search (MCTS) (Hao et al., 2023), and Debate-as-reasoning (Du et al., 2023).</p>
<p>In Figure 4 we provide experimental evidence that methods such as Debate-as-reasoning, MCTS, and ToT can augment Gemma2 27B (considerably smaller than Gemini 1.5 and GPT-4 models), to be competitive with GPT-4, Gemini 1.5 Pro and Gemini 1.5 Flash at smaller few-shot context lengths. Additional details and parameters for these search methods are included in Appendix B.3. These results demonstrate how inference-time reasoning procedures using smaller open-source models can perform competitively with larger foundational models in the natural language planning domain.</p>
<p>However, the ability of these methods to scale to few-shot examples seems to be less significant than larger foundational models. Specifically, for the Travel Scheduling task, the larger models all outperform the CoT methods after 4 few-shot examples. It is unclear if this scaling trend is a result of the underlying model (Gemma2 27B) or the method itself.</p>
<p>Most interesting to note is the performance of debate-as-reasoning, which, despite not being an explicit search or planning strategy, works comparatively to MCTS in both tasks. This indicates that, for some planning tasks, even unstructured CoT style inference methods are competitive with explicit search-based CoT methods, and larger foundational models with ICL. This may indicate that, for natural language planning tasks, allowing the model to construct a CoT and consider multiple solutions may be more valuable then the specific method behind constructing the CoT.</p>
<p>Table 1: Impact of SFT on accuracy, for BlocksWorld: instance of 3-7, 8-9 and 8-20 blocks, and for Logistics: instances of 1-2 and 3-5 packets.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Gemini 1.0 S</th>
</tr>
</thead>
<tbody>
<tr>
<td>BlocksWorld(3-7)</td>
<td>96.26</td>
</tr>
<tr>
<td>BlocksWorld(8-9)</td>
<td>92.6</td>
</tr>
<tr>
<td>BlocksWorld(8-20)</td>
<td>67.00</td>
</tr>
<tr>
<td>Logistics(1-2)</td>
<td>99.8</td>
</tr>
<tr>
<td>Logistics(3-5)</td>
<td>63.4</td>
</tr>
</tbody>
</table>
<h1>3.2 Supervised Fine Tuning (SFT)</h1>
<p>Supervised fine tuning (SFT) (Ouyang et al., 2022) has proved to be an effective method for teaching LLMs various capabilities. In this section, we investigate the effect of Supervised Fine Tuning (SFT) with optimal plan on planning capability of LLMs. We use the Fast-Downward classical planner to generate the optimal plan. We specifically ran experiments on</p>
<p>Gemini 1.0 S (GeminiTeam et al., 2024a) and investigate planning performance for two different benchmarks with different levels of difficulty, namely, we look into 5 scenarios: BlocksWorld with 3-7 blocks, 8-9 blocks or 8-20 blocks, and Logistics with 1-2 packets, or with 3-5 packets. The data size and splits are documented in Appendix B.4.</p>
<p>The results are shown in Table 1. We observe that SFT leads to high accuracy for some instances of both datasets and outperforms many-shot ICL. The performance appears to drop as the planning problem becomes more difficult.</p>
<p>Table 2: Plan generalization analysis for instances of BlocksWorld of different number of blocks in SFT scenarios, for Gemini 1.0 S. Accuracy is in $\%$. Accuracy is measured by a verifier.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Finetune data</th>
<th style="text-align: left;">Eval data</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{BW}(3-7)$</td>
<td style="text-align: left;">$\mathrm{BW}(3-7)$</td>
<td style="text-align: left;">96.26</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{BW}(3-7)$</td>
<td style="text-align: left;">$\mathrm{BW}(8-20)$</td>
<td style="text-align: left;">34.20</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{BW}(8-20)$</td>
<td style="text-align: left;">$\mathrm{BW}(3-7)$</td>
<td style="text-align: left;">98.27</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{BW}(8-20)$</td>
<td style="text-align: left;">$\mathrm{BW}(8-20)$</td>
<td style="text-align: left;">67.00</td>
</tr>
</tbody>
</table>
<h3>3.3 Plan GENERALIZATION</h3>
<p>For any LLM application, the question of how well the method generalizes to out-of-trainingdistribution (OOD) inputs is always present. Here, we investigate how LLM planning generalizes. Plan generalization has three main categories: (1) Generalize to unseen instances of the same environment (2) Generalize to renaming of actions and objects (3) Generalize to unseen plan environments. We focus on the first category which sits at the core of the desired capability.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Calendar Scheduling (left) and Travel Scheduling (right) tasks with reasoning procedures (ToT, MCTS, and Debate). We use Gemma2 27B to perform these procedures, and compare with GPT-4 and Gemini 1.5 models.</p>
<p>For these generalization experiments, we consider BlocksWorld and Logistics benchmarks with various difficulty levels (as described in Section 3.2). We look into performance of both SFT and ICL approaches. Tables 2 shows generalization performance for BlocksWorld in SFT setting for BlocksWorld 3-7, 8-20 split and Table 3 considers generalization between BlocksWorld 3-7 and 8-9 blocks split for both SFT and ICL with different models and number of shots. Table 4 depicts plan generalization for Logistics benchmark for splits of 1-2 and 3-5 packets.</p>
<p>Our analysis reveals several key findings: (1) Superiority of SFT: SFT consistently outperforms ICL across both benchmarks, even when utilizing a smaller model for SFT. This suggests that SFT's explicit training process, focused on the specific task, leads to more effective learning and better generalization. (2) In most ICL scenarios, training the model on easier instances first results in improved performance on harder examples, e.g., see Table 3, rows 1-4 and 6. (3) Limitations of hard example training: Contrary to some expectations, training the model exclusively on hard examples does not always translate to better performance on easier ones (for example, see Table 3 rows 2, 4-6). This suggests that a balanced approach, incorporating both easy and hard examples, might be optimal for achieving well-rounded performance.</p>
<p>Table 3: OOD accuracy for BlocksWorld splits 3-7 and 8-9 blocks.</p>
<table>
<thead>
<tr>
<th>Model Name</th>
<th>Train data</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$3-7$</td>
<td>$8-9$</td>
</tr>
<tr>
<td></td>
<td>$\frac{\text { Eval data }}{3-7 / 8-9}$</td>
<td>$3-7 / 8-9$</td>
</tr>
<tr>
<td>Gemini 1.5 Flash (1 Shot)</td>
<td>$26.4 / 13.7$</td>
<td>$32.9 / 12.3$</td>
</tr>
<tr>
<td>Gemini 1.5 Flash (70 Shot)</td>
<td>$35.7 / 23.3$</td>
<td>$27.6 / 11.0$</td>
</tr>
<tr>
<td>Gemini 1.5 Pro (1 Shot)</td>
<td>$40.0 / 25.6$</td>
<td>$39.0 / 20.3$</td>
</tr>
<tr>
<td>Gemini 1.5 Pro (70 Shot)</td>
<td>$46.3 / 36.3$</td>
<td>$38.3 / 18.4$</td>
</tr>
<tr>
<td>Gemini 1.0 S (1 Shot)</td>
<td>$3.68 / 0.331$</td>
<td>$2.99 / 1.35$</td>
</tr>
<tr>
<td>Gemini 1.0 S (70 Shot)</td>
<td>$12.4 / 2.33$</td>
<td>$3.99 / 1.68$</td>
</tr>
<tr>
<td>Gemini 1.0 S (SFT)</td>
<td>$96.3 / 81.6$</td>
<td>$96.0 / 92.6$</td>
</tr>
</tbody>
</table>
<p>Table 4: OOD accuracy for Logistics tasks splits of 1-2, 3-5 packets.</p>
<table>
<thead>
<tr>
<th>Model Name</th>
<th>Train data</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$1-2$</td>
<td>$3-5$</td>
</tr>
<tr>
<td></td>
<td>$\frac{\text { Eval data }}{1-2 / 3-5}$</td>
<td>$1-2 / 3-5$</td>
</tr>
<tr>
<td>Gemini 1.5 Flash (1 Shot)</td>
<td>$18.3 / 1.35$</td>
<td>$26.7 / 1.00$</td>
</tr>
<tr>
<td>Gemini 1.5 Flash (30 Shot)</td>
<td>$12.7 / 1.67$</td>
<td>$19.7 / 1.33$</td>
</tr>
<tr>
<td>Gemini 1.5 Pro (1 Shot)</td>
<td>$35.3 / 9.03$</td>
<td>$57.6 / 7.01$</td>
</tr>
<tr>
<td>Gemini 1.5 Pro (30 Shot)</td>
<td>$56.4 / 11.3$</td>
<td>$62.7 / 8.04$</td>
</tr>
<tr>
<td>Gemini 1.0 S (1 Shot)</td>
<td>$7.0 / 0.0$</td>
<td>$5.33 / 0.0$</td>
</tr>
<tr>
<td>Gemini 1.0 S (30 Shot)</td>
<td>$9.99 / 0.336$</td>
<td>$8.00 / 0.662$</td>
</tr>
<tr>
<td>Gemini 1.0 S (SFT)</td>
<td>$99.8 / 10.8$</td>
<td>$98.0 / 63.4$</td>
</tr>
</tbody>
</table>
<h1>3.4 COMPARISON WITH PLANBENCH</h1>
<p>Valmeekam et al. (2023) proposed a benchmark for planning that maps domain definitions to instructions and problem statements into natural language using zero-shot and one-shot techniques. We utilize their dataset on BlocksWorld, as the problems are comparable. Unlike their approach, which limits problems to configurations of 3,4 , and 5 blocks using only zero-shot and one-shot prompting, our work extends this using for ICL up to 7 blocks and by employing many-shot prompting.</p>
<p>Table 5 compares results using the natural language prompts from Valmeekam et al. (2023)(their dataset is referred to as Val-BW) and, novel to this work, presents results on PDDL for their datasets using both 1-shot and 2-shot techniques.</p>
<p>We utilize the natural language prompts from (Valmeekam et al., 2023) test them on Gemini 1.5 Pro. We observe that GPT-4 performs better with these prompts. For our dataset, no such difference is observed. Manual inspection reveals that especially 1-shot prompts need to be crafted carefully while</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: In-domain failure analysis: Distribution of number of blocks in successful and failed cases and different failure reasons for BlocksWorld 8-20 blocks. As the number of blocks increases the number of successful cases decreases.
few-shot or many-shot prompts are more robust. For instance, results improve when the prompts are more specific about the output format, changing from 'My plan is as follows' to 'Your plan as plain text without formatting', which enhances results for Gemini 1.5 Pro. Further, our prompts do not include explanations of the actions and we do not use color coding for the blocks but rather keep the names (e.g., blue block vs block b3).</p>
<h1>4 Investigating Failure Cases</h1>
<p>In this section, we analyze the outcome of various approaches to improving LLM planning performance, and dive into a more detailed categorization of failure modes.</p>
<p>The LLM planning failure modes we observed can be classified into the following three categories:
(1) Constraint Violation: The model fails to satisfy one of the explicitly declared environment constraints. i.e., it ignores one of the conditions required to be able to do an action. Even if this happen only for one instance of an action and not for all instances of the same action. For example model wants to put block b1 on block b2 while block b2 is not empty.
(2) Failure to meet the goal: The model makes a plan that meets the environment constraints but does not reach the goal state at the end of the trajectory.
(3) Out-of-vocabulary actions: The model proposes actions which are not in the environment's action space.</p>
<p>PDDL benchmarks Here we examine the failure modes for both the Blocksworld and Logistics domains and SFT, ICL setting. Due to space constraints we report Blocksworld SFT settings here and refer the reader to Appendix C for the rest of plots and analysis. First we look at the effect of distribution of number of blocks in the planning instance. We compare Figure 5a that includes successful eval cases of BlocksWorld 8-20 blocks, to all its eval data in Figure 5b respectively. We note that the model failure cases increases as the number of blocks in the problem increases. We also separate the reasons of failure in BlocksWorld 8-20 instances as seen in Figure 5c and Figure 10d respectively. Note that failure mode (3) does not happen for BlocksWorld benchmark in SFT or ICL settings and for Logistics in ICL setting, but for Logistics benchmark in SFT settings all three categories are present (see Appendix C for details).</p>
<p>Table 5: Accuracy (in \%) comparing the state of the art for different datasets and systems. Val-BW denotes BlocksWorld dataset as open-source by Valmeekam et al. (2023).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Shots</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">No. of Blocks</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{3}$</td>
<td style="text-align: center;">$\mathbf{4}$</td>
</tr>
<tr>
<td style="text-align: left;">Val-BW</td>
<td style="text-align: center;">GPT-4 Turbo</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: left;">Val-BW</td>
<td style="text-align: center;">Gemini 1.5 Pro</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">18.4</td>
</tr>
<tr>
<td style="text-align: left;">Val-BW</td>
<td style="text-align: center;">Gemini 1.5 Pro</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">PDDL</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: left;">Val-BW</td>
<td style="text-align: center;">Gemini 1.5 Pro</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">PDDL</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">46.2</td>
</tr>
<tr>
<td style="text-align: left;">Our-BW</td>
<td style="text-align: center;">Gemini 1.5 Pro</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">38.5</td>
</tr>
<tr>
<td style="text-align: left;">Our-BW</td>
<td style="text-align: center;">Gemini 1.5 Pro</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">NL</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">58.5</td>
</tr>
<tr>
<td style="text-align: left;">Our-BW</td>
<td style="text-align: center;">Gemini 1.5 Pro</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">PDDL</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">44.6</td>
</tr>
<tr>
<td style="text-align: left;">Our-BW</td>
<td style="text-align: center;">Gemini 1.5 Pro</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">PDDL</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">44.6</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy (in \%) comparing the state of the art for different datasets and systems. Val-BW denotes BlocksWorld dataset as open-source by Valmeekam et al. (2023).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: BlocksWorld 3-7 to 8-20 OOD scenario (a) Success per number of blocks (b) investigation of failures in constraint violation per plan step, color coded by action where the constraint violation happens (c) distribution of all cases per plan steps.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Error Analysis for various CoT reasoning strategies in the TravelPlan and CalendarPlan natural language tasks</p>
<p>Next, We study the distribution of number of blocks in out-of-domain cases focusing on BlocksWorld 3-7 to 8-20 blocks generalization. Comparing the successful cases in Figure 6a to all the eval samples in Figure 5b, we observe that the generalization performance of the model that is trained on 3-7 blocks drops as the number of eval blocks increases from 8 to 20 .</p>
<p>To probe the models even further, we look into the category where the generated plan fails to satisfy one of the environment constraints. We study the step at which the plan fails to meet the constraint. Considering Figure 6b and comparing it to Figure 6c, the model seems to have trouble generalizing from the beginning, having the majority of its failures concentrated on earlier steps. This trend stays true for Logistics benchmark as well and is due to the fact that models go deep before they go wide when they want to produce the plan output. In other terms, this gives the intuition that the model seems to get overwhelmed when encountering difficult unseen examples, leading to failure from the beginning by not satisfying the constraints.</p>
<p>In addition, looking at the granular level of which actions that cause the failure in Figure 6b, we note that the model has learned a correlation between the step number and the action it chooses, leading to having stack and un-stack not having any mutual failure step. This is also observed in failure modes of BlocksWorld 3-7 both for ICL and SFT scenarios (see Figure 9 in Appendix C). Further investigation of training data confirms this observation.</p>
<p>Natural Language benchmarks Since TravelPlan and CalendarPlan benchmarks require plans that do not involve sequential execution of actions, the only failure mode we consider is failure mode (2) failure to reach a goal state. All errors discussed in this subsection are in terms of whether or not the method under study succeeded in reaching the goal. Figure 7 shows the average error by CoT method as task complexity increases in the TravelPlan and CalendarPlan benchmarks. Figure 12 in Appendix C histograms of how many examples belong to each task length. Both domains have a higher distribution of "smaller" or shorter tasks, which impacts the distribution of error in Figure 7. It is interesting to note how the error is disproportionate between debate-as-reasoning and MCTS, ToTs, aligning with our findings in Section 3. As discussed in Section 3, methods such as MCTS and ToT seem to perform inconsistently at less-structured planning tasks. We can see this in how they perform relative to debate, as well as each other. Specifically, for the TravelPlan task, they fail at inconsistent rates relative to one another.</p>
<h1>5 CONCLUSION</h1>
<p>We examined planning capabilities of LLMs through benchmark development, generalization assessment and analysis of failure modes. Our observations for ICL setting has implications for the future development and training of LLMs, potentially informing strategies to enhance their capacity to process and leverage extended contextual information. It also points to the opportunity for followup work investigating ways to further improve the inference time reasoning procedures, and their scaling properties. Our investigation of plan generalization reveals three key findings: superiority of SFT, curriculum learning effectiveness and limitations of hard example training; suggesting that a balanced approach, incorporating both easy and hard examples, might be optimal for achieving well-rounded performance. Our analysis of failure modes point to future work on dataset design and reasoning procedures to directly address the discussed failure modes. These enhancements can unlock new levels of versatility and robustness in LLM-based planning systems, paving the way for their broader adoption in real-world applications.</p>
<h2>REFERENCES</h2>
<p>Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning with large language models for object rearrangement. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2086-2092. IEEE, 2023.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023.</p>
<p>GeminiTeam, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, et. al, Jeffrey Dean, and Oriol Vinyals. Gemini: A family of highly capable multimodal models, 2024a.</p>
<p>GeminiTeam, Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, et al, Jeffrey Dean, and Oriol Vinyals. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024b.</p>
<p>Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.</p>
<p>Yilun Hao, Yongchao Chen, Yang Zhang, and Chuchu Fan. Large language models can plan your travels rigorously with formal verification tools. arXiv preprint arXiv:2404.11891, 2024.</p>
<p>Eran Hirsch, Guy Uziel, and Ateret Anaby-Tavor. What's the plan? evaluating and developing planning-aware techniques for llms. arXiv preprint arXiv:2402.11489, 2024.</p>
<p>Richard Howey, Derek Long, and Maria Fox. Val: Automatic plan validation, continuous effects and mixed initiative planning using pddl. In Proceedings 1 of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004). IEEE, 2004.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.</p>
<p>Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Llms can't plan, but can help planning in llm-modulo frameworks. arXiv preprint arXiv:2402.01817, 2024.</p>
<p>Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.</p>
<p>Drew McDermott, Malik Ghallab, Adele E. Howe, Craig A. Knoblock, Ashwin Ram, Manuela M. Veloso, Daniel S. Weld, and David E. Wilkins. Pddl-the planning domain definition language. In APIS-1998, 1998. URL https://api.semanticscholar.org/CorpusID:59656859.</p>
<p>OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, et al., , William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.</p>
<p>Stuart J Russell and Peter Norvig. Artificial intelligence: a modern approach. Pearson, 2016.
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 11523-11530. IEEE, 2023.</p>
<p>Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. Chain of thoughtlessness: An analysis of cot in planning. arXiv preprint arXiv:2405.04776, 2024.</p>
<p>Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leïcia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at a practical size, 2024. URL https://arxiv.org/abs/2408.00118.</p>
<p>Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:75993-76005, 2023.</p>
<p>Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.</p>
<p>Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622, 2024.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.</p>
<p>Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, and Denny Zhou. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520, 2024.</p>
<h1>Appendix</h1>
<h2>A Prompts</h2>
<h2>Bellow is the 1-shot prompt for the BlocksWorld task.</h2>
<div class="codehilite"><pre><span></span><code>Please solve the problem:
(define (problem BW-rand-4)
(:domain blocksworld-4ops)
(:objects b4 b1 b3 b2)
(:init
(on b3 b1)
(on b1 b4)
(clear b3)
(handempty)
(ontable b2)
(ontable b4)
(clear b2)
)
(:goal (and
(on b2 b4)
(on b3 b1)
))
)
Your plan as plain text without formatting:
(unstack b3 b1)
(put-down b3)
(unstack b1 b4)
(put-down b1)
(pick-up b2)
(stack b2 b4)
(pick-up b3)
(stack b3 b1)
done.
Please solve the problem:
(define (problem BW-rand-6)
(:domain blocksworld-4ops)
(:objects b5 b1 b4 b2 b3 b6)
(:init
(on b4 b1)
(handempty)
(ontable b6)
(on b2 b4)
(clear b3)
(ontable b5)
(on b3 b2)
(clear b6)
(on b1 b5)
)
(:goal (and
(on b4 b2)
(on b1 b4)
(on b5 b1)
(on b3 b5)
))
)
</code></pre></div>

<p>Your plan as plain text without formatting:</p>
<h1>Bellow is the 1-shot prompt for the Logistics task.</h1>
<div class="codehilite"><pre><span></span><code>Please solve the problem:
(define (problem logistics -c4-s2-p3-a4)
(:domain logistics -strips)
(: objects
a0 a1 a2 a3
c0 c1 c2 c3
t0 t1 t2 t3
10-0 10-1 11-0 11-1 12-0 12-1 13-0 13-1
p0 p1 p2
)
(: init
    (AIRPLANE a0) (AIRPLANE a1)(AIRPLANE a2)(AIRPLANE a3)
    (CITY c0)(CITY c1)(CITY c2)(CITY c3)
    (TRUCK t0)(TRUCK t1)(TRUCK t2)(TRUCK t3)
    (LOCATION 10-0)(in-city 10-0 c0)
    (LOCATION 10-1)(in-city 10-1 c0)
    (LOCATION 11-0)(in-city 11-0 c1)
    (LOCATION 11-1)(in-city 11-1 c1)
    (LOCATION 12-0)(in-city 12-0 c2)
    (LOCATION 12-1)(in-city 12-1 c2)
    (LOCATION 13-0)(in-city 13-0 c3)
    (LOCATION 13-1)(in-city 13-1 c3)
    (AIRPORT 10-0)(AIRPORT 11-0)(AIRPORT 12-0)(AIRPORT 13-0)
    (OBJ p0)(OBJ p1)(OBJ p2)
    (at t0 10-0)(at t1 11-1)(at t2 12-0)(at t3 13-0)
    (at p0 11-1)(at p1 10-1)(at p2 10-0)
    (at a0 11-0)
    (at a1 11-0)
    (at a2 12-0)
    (at a3 13-0)
)
(: goal
    (and
        (at p0 12-0)
        (at p1 12-0)
        (at p2 11-1)
    )
)
)
</code></pre></div>

<p>Your plan as plain text without formatting:
(load-truck p0 t1 11-1)
(drive-truck t1 11-1 11-0 c1)
(unload-truck p0 t1 11-0)
(load-airplane p0 a1 11-0)
(fly-airplane a1 11-0 12-0)
(unload-airplane p0 a1 12-0)
(drive-truck t0 10-0 10-1 c0)
(load-truck p1 t0 10-1)
(drive-truck t0 10-1 10-0 c0)
(unload-truck p1 t0 10-0)
(fly-airplane a3 13-0 10-0)
(load-airplane p2 a3 10-0)
(fly-airplane a3 10-0 11-0)
(unload-airplane p2 a3 11-0)
(load-truck p2 t1 11-0)
(drive-truck t1 11-0 11-1 c1)
(unload-truck p2 t1 11-1)
(fly-airplane a1 12-0 10-0)
(load-airplane p1 a1 10-0)
(fly-airplane a1 10-0 12-0)
(unload-airplane p1 a1 12-0)</p>
<p>done.</p>
<p>Please solve the problem:
(define (problem logistics -c2-s2-p3-a2)
(:domain logistics -strips)
(:objects
a0 a1
c0 c1
t0 t1
$10-0 \quad 10-1 \quad 11-0 \quad 11-1$
p0 p1 p2
)
(:init
(AIRPLANE a0)(AIRPLANE a1)
(CITY c0)(CITY c1)
(TRUCK t0) (TRUCK t1)
(LOCATION $10-0$ )(in-city $\quad 10-0 \quad \mathrm{c} 0$ )
(LOCATION $10-1$ )(in-city $\quad 10-1 \quad \mathrm{c} 0$ )
(LOCATION $11-0$ )(in-city $\quad 11-0 \quad \mathrm{c} 1$ )
(LOCATION $11-1$ )(in-city $\quad 11-1 \quad \mathrm{c} 1$ )
(AIRPORT $10-0$ ) (AIRPORT $11-0$ )
(OBJ p0)(OBJ p1)(OBJ p2)
(at t0 $10-1$ )(at t1 $11-0$ )
(at p0 $10-1$ )(at p1 $11-0$ )(at p2 $11-1$ )
(at a0 $10-0$ )(at a1 $10-0$ )
)
(:goal
(and
(at p0 $10-1$ )
(at p1 $11-0$ )
(at p2 $10-0$ )
)
)</p>
<p>Your plan as plain text without formatting:</p>
<h1>Bellow is the 1-shot prompt for the Mini-Grid task.</h1>
<p>Please solve the problem:
(define (problem grid_2Vroom2)
(:domain grid)
(: objects
p0 p1 p2 p3 p4 p5 p6 p7 p8
shape0
key0
)
(:init
; Object types
(place p0) (place p1) (place p2) (place p3) (place p4) (place p5) (
place p6) (place p7) (place p8)
(shape shape0)
(key key0)
; Open/locked cells
(open p0) (open p1) (open p2) (open p3) (open p5) (open p6) (open p7)
(open p8)
(locked p4)
; Connected cells
(conn p0 p1)
(conn p0 p2)
(conn p1 p0)
(conn p1 p3)</p>
<div class="codehilite"><pre><span></span><code>    (conn p2 p0)
    (conn p2 p3)
    (conn p2 p4)
    (conn p3 p2)
    (conn p3 p1)
    (conn p4 p2)
    (conn p4 p5)
    (conn p5 p4)
    (conn p5 p6)
    (conn p5 p7)
    (conn p6 p5)
    (conn p6 p8)
    (conn p7 p5)
    (conn p7 p8)
    (conn p8 p7)
    (conn p8 p6)
    ; Lock and key shapes
    (lock-shape p4 shape0)
    (key-shape key0 shape0)
    ; Key placement
    (at key0 p0)
    ; Robot placement
    (at-robot p3)
    (arm-empty)
    )
    (:goal (at-robot p7))
)
</code></pre></div>

<p>Your plan as plain text without formatting:
(move p3 p2)
(move p2 p0)
(pickup p0 key0)
(move p0 p2)
(unlock p2 p4 key0 shape0)
(move p2 p4)
(move p4 p5)
(move p5 p7)
done.
Please solve the problem:
(define (problem grid_3Vroom3)
(:domain grid)
(:objects
p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16 p17 p18 p19 p20 p21 p22 p23 p24 p25 p26 p27 p28
shape0
key0
)
$(:$ init
; Object types
(place p0) (place p1) (place p2) (place p3) (place p4) (place p5) ( place p6) (place p7) (place p8) (place p9) (place p10) (place p11 ) (place p12) (place p13) (place p14) (place p15) (place p16) ( place p17) (place p18) (place p19) (place p20) (place p21) (place p22) (place p23) (place p24) (place p25) (place p26) (place p27) (place p28)
(shape shape0)
(key key0)
; Open/locked cells
(open p0) (open p1) (open p2) (open p3) (open p4) (open p5) (open p6) (open p7) (open p8) (open p10) (open p11) (open p12) (open p13) (open p14) (open p15) (open p16) (open p17) (open p18) (open p20) (open p21) (open p22) (open p23) (open p24) (open p25) (open p26 ) (open p27) (open p28)
(locked p9) (locked p19)</p>
<p>; Connected cells
( conn p0 p1)
( conn p0 p3)
( conn p1 p0)
( conn p1 p2)
( conn p1 p4)
( conn p2 p1)
( conn p2 p5)
( conn p3 p0)
( conn p3 p4)
( conn p3 p6)
( conn p4 p3)
( conn p4 p1)
( conn p4 p5)
( conn p4 p7)
( conn p5 p4)
( conn p5 p2)
( conn p5 p8)
( conn p6 p3)
( conn p6 p7)
( conn p6 p9)
( conn p7 p6)
( conn p7 p4)
( conn p7 p8)
( conn p8 p7)
( conn p8 p5)
( conn p9 p6)
( conn p9 p10)
( conn p10 p9)
( conn p10 p11)
( conn p10 p13)
( conn p11 p10)
( conn p11 p12)
( conn p11 p14)
( conn p12 p11)
( conn p12 p15)
( conn p13 p10)
( conn p13 p14)
( conn p13 p16)
( conn p14 p13)
( conn p14 p11)
( conn p14 p15)
( conn p14 p17)
( conn p15 p14)
( conn p15 p12)
( conn p15 p18)
( conn p16 p13)
( conn p16 p17)
( conn p17 p16)
( conn p17 p14)
( conn p17 p18)
( conn p18 p17)
( conn p18 p15)
( conn p18 p19)
( conn p19 p18)
( conn p19 p22)
( conn p20 p21)
( conn p20 p23)
( conn p21 p20)
( conn p21 p22)
( conn p21 p24)
( conn p22 p21)
( conn p22 p19)
( conn p22 p25)
( conn p23 p20)</p>
<div class="codehilite"><pre><span></span><code>    (conn p23 p24)
    (conn p23 p26)
    (conn p24 p23)
    (conn p24 p21)
    (conn p24 p25)
    (conn p24 p27)
    (conn p25 p24)
    (conn p25 p22)
    (conn p25 p28)
    (conn p26 p23)
    (conn p26 p27)
    (conn p27 p26)
    (conn p27 p24)
    (conn p27 p28)
    (conn p28 p27)
    (conn p28 p25)
    ; Lock and key shapes
    (lock-shape p9 shape0)
    (lock-shape p19 shape0)
    (key-shape key0 shape0)
    ; Key placement
    (at key0 p12)
    ; Robot placement
    (at-robot p16)
    (arm-empty)
    }
    (:goal (at-robot p4))
)
</code></pre></div>

<p>Your plan as plain text without formatting:</p>
<h1>Bellow is the 1-shot prompt for the Trip Planning task.</h1>
<p>Please solve the problem:
You plan to visit 6 European cities for 13 days in total. You only take direct flights to commute between cities. You want to spend 3 days in Dublin. You would like to meet your friends at Dublin between day 7 and day 9 to tour together. You would like to visit Madrid for 2 days . You plan to visit relatives in Madrid between day 2 and day 3. You plan to stay in Oslo for 3 days. You would like to visit London for 2 days. You want to spend 3 days in Vilnius. You plan to stay in Berlin for 5 days. You are going to attend a wedding in Berlin between day 3 and day 7 .</p>
<p>Here are the cities that have direct flights:
London and Madrid, Oslo and Vilnius, Berlin and Vilnius, Madrid and Oslo, Madrid and Dublin, London and Oslo, Madrid and Berlin, Berlin and Oslo, Dublin and Oslo, London and Dublin, London and Berlin, Berlin and Dublin.</p>
<p>Find a trip plan of visiting the cities for 13 days by taking direct flights to commute between them.</p>
<p>Here is the trip plan for visiting the 6 European cities for 13 days:
<strong>Day 1-2:</strong> Arriving in London and visit London for 2 days.
<strong>Day 2:</strong> Fly from London to Madrid.
<strong>Day 2-3:</strong> Visit Madrid for 2 days.
<strong>Day 3:</strong> Fly from Madrid to Berlin.
<strong>Day 3-7:</strong> Visit Berlin for 5 days.
<strong>Day 7:</strong> Fly from Berlin to Dublin.
<strong>Day 7-9:</strong> Visit Dublin for 3 days.
<strong>Day 9:</strong> Fly from Dublin to Oslo.</p>
<p><strong>Day 9-11:</strong> Visit Oslo for 3 days.
<strong>Day 11:</strong> Fly from Oslo to Vilnius.
<strong>Day 11-13:</strong> Visit Vilnius for 3 days.
done.
Please solve the problem:
You plan to visit 6 European cities for 17 days in total. You only take direct flights to commute between cities. You want to spend 4 days in Manchester. You plan to stay in Florence for 5 days. You want to spend 3 days in Geneva. You are going to attend a wedding in Geneva between day 1 and day 3. You want to spend 3 days in Seville. During day 7 and day 9 , you have to attend a conference in Seville. You would like to visit Prague for 2 days. You plan to stay in Valencia for 5 days. From day 3 to day 7 , there is a annual show you want to attend in Valencia.</p>
<p>Here are the cities that have direct flights:
Manchester and Prague, Seville and Manchester, Geneva and Manchester, Valencia and Seville, Geneva and Valencia, Valencia and Prague, Prague and Florence, Geneva and Prague.</p>
<p>Find a trip plan of visiting the cities for 17 days by taking direct flights to commute between them.</p>
<h1>Bellow is the 1-shot prompt for the Calendar Scheduling task.</h1>
<p>Please solve the problem:
You need to schedule a meeting for Samuel, Evelyn, Ruth and Amanda for half an hour between the work hours of 9:00 to 17:00 on Monday.</p>
<p>Here are the existing schedules for everyone during the day:
Samuel is free the entire day.
Evelyn has meetings on Monday during 9:00 to 10:00, 11:00 to 12:00, 12:30 to $13: 00,15: 30$ to $16: 00$
Ruth has meetings on Monday during 9:30 to 11:00, 11:30 to 12:30, 13:00 to $13: 30,14: 00$ to $14: 30,15: 00$ to $16: 00,16: 30$ to $17: 00$
Amanda has meetings on Monday during 10:00 to 10:30, 11:00 to 12:30, 13:00 to $13: 30,14: 00$ to $15: 00,15: 30$ to $16: 00$</p>
<p>Amanda can not meet on Monday before 16:00. Find a time that works for everyone's schedule and constraints.</p>
<p>Here is the proposed time: Monday, 16:00 - 16:30
done.
Please solve the problem:
You need to schedule a meeting for Walter, Jacob, Jennifer and Joan for one hour between the work hours of 9:00 to 17:00 on Monday.</p>
<p>Here are the existing schedules for everyone during the day:
Walter is busy on Monday during 9:30 to 10:00, 13:00 to 13:30;
Jacob has meetings on Monday during 11:00 to 11:30, 13:00 to 13:30;
Jennifer is busy on Monday during 9:30 to 10:30, 11:30 to 12:00, 12:30 to $15: 00$
Joan has blocked their calendar on Monday during 9:30 to 10:00, 10:30 to $11: 30,12: 00$ to $12: 30,13: 00$ to $14: 00,14: 30$ to $15: 30$</p>
<p>Find a time that works for everyone's schedule and constraints.</p>
<h1>B EXPERIMENTAL DETAILS</h1>
<h2>B. 1 DATASET CREATION</h2>
<p>In these experiments BlocksWorld dataset for 3 to 7 blocks consists of 40000 samples.
In the creation of the BlocksWorld dataset as outlined in Algorithm 1, the key parameters include the maximum number of blocks num_blocks and the quantity of examples $n$ to be generated for each block count. Here, the maximum number of blocks is a number greater than 3. As we use uniform sampling, this results in a linear increase in the number of more complex examples. However, it's important to note that as the number of blocks increases, the simpler combinations are exhausted since all possible combinations might be included. The methods CreateStacks generates random stacks of blocks, iteratively sampling from the available blocks to determine stack heights until all blocks are utilized. The method CreatePro denotes a simple method to translate the block configuration into PDDL which is python reimplementation of functionality in 4ops-Blockworld code $]^{5}$.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Create BlocksWorld Dataset
    function \(\operatorname{CreateDatasetBW}(\) num_blocks, \(n\) )
        dataset \(\leftarrow[]_{\triangleright}\) Initialize an empty list
        for problem_id \(\leftarrow 1\) to \(n\) do
            \(b \leftarrow\) RANDOMUNIFORM(3, num_blocks)
            initStacks \(\leftarrow\) CreateStacks \((b)\)
            goalStacks \(\leftarrow\) CreateStacks \((b)\)
            if initStacks \(==\) goalStacks then
                continue \(\triangleright\) Skip equal stacks.
            end if
            problem \(\leftarrow\) CreatePro(initStacks, goalStacks)
            plan \(\leftarrow\) FASTDOWNWARD(problem, domain)
            dataset \(\leftarrow\) dataset + [(problem, plan)]
        end for
        return dataset
    end function
</code></pre></div>

<p>Algorithm 1, we generate 28 k unique samples. From these, we randomly select 25500 of the for training set and 2500 for validation set. This procedure yields a problem distribution as shown in Figure 8.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Distribution with number of blocks and average plan length.</p>
<h2>B. 2 MAPPINGS PDDL TO NATURAL LANGUAGE</h2>
<p>Here we present the templates to map PDDL problems to Natural Language. Details are shown in Table 6.</p>
<h2>B. 3 SEARCH Procedure PARAMETERS</h2>
<p>The two search procedures deployed and compared alongside ICL and SFT methods, (ToT) (Yao et al., 2023) and monte-carlo tree search (MCTS) (Hao et al., 2023), were implemented as specified in their original papers. The only deviations are listed below.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/AI-Planning/pddl-generators/tree/main/blocksworld/4ops&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
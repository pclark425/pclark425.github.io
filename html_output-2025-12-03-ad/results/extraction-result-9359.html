<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9359 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9359</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9359</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-53a3e2c9f7151da00280677d89cc42e1762404aa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/53a3e2c9f7151da00280677d89cc42e1762404aa" target="_blank">In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes In-Context Symbolic Regression (ICSR), an SR method which iteratively refines a functional form with an LLM and determines its coefficients with an external optimizer and leverages LLMs' strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors.</p>
                <p><strong>Paper Abstract:</strong> State of the art Symbolic Regression (SR) methods currently build specialized models, while the application of Large Language Models (LLMs) remains largely unexplored. In this work, we introduce the first comprehensive framework that utilizes LLMs for the task of SR. We propose In-Context Symbolic Regression (ICSR), an SR method which iteratively refines a functional form with an LLM and determines its coefficients with an external optimizer. ICSR leverages LLMs' strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors. Our findings reveal that LLMs are able to successfully find symbolic equations that fit the given data, matching or outperforming the overall performance of the best SR baselines on four popular benchmarks, while yielding simpler equations with better out of distribution generalization.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9359.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9359.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICSR (Llama 3 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Symbolic Regression (ICSR) using Llama 3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that leverages a large language model via optimization-by-prompting to propose and iteratively refine symbolic function 'skeletons' for symbolic regression; coefficients are fitted with an external non-linear least squares optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama 3 8B backbone used as the foundation LLM in experiments (approx. 8B parameters, 8k token context window in this work as reported), employed to generate functional forms via in-context learning and optimization-by-prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical modeling / Symbolic regression (function discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based generation of symbolic mathematical expressions (function skeletons) that fit given (X,Y) observations; iterative refinement via meta-prompt containing previous attempts and scores; external numeric optimizer fits coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Coefficient of determination R^2 on unseen test points (in-domain and out-of-distribution); objective during optimization uses Normalized MSE (NMSE) combined with a complexity penalty r(f|D) and error = 1 / r; also report expression complexity (# nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Table 2 overall average R^2 = 0.993 ± 0.002 across four SR benchmarks (Nguyen: 0.996 ± 0.002; Constant: 0.9991 ± 0.0004; R: 0.996 ± 0.001; Keijzer: 0.981 ± 0.004). Complexity (avg # nodes) overall = 6.7 ± 0.4. Out-of-distribution performance degrades with extended input ranges but ICSR outperforms most baselines at 175% and 200% domain increases.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>- Complexity penalty (λ): trades fit vs simplicity; λ=0 leads to overfitting and high complexity, λ≈0.05 used as default. - Number of seed generations n_s (diversity of initial seeds): n_s=10 used; fewer seeds hurt robustness. - Optimization loop (iterative refinement) improves final performance vs seeds-only in many cases. - External coefficient fitting (NLS restarts) affects parameter recovery. - Prompt design and inclusion of previous attempts (meta-prompt) leverage in-context learning. - Sampling parameters (temperature, number of samples per prompt) affect exploration/exploitation. - Context window size limits number of explicit datapoints and previous attempts that can be included. - Model pretraining bias / vocabulary (LLM can propose functions beyond fixed SR vocabularies).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against gplearn (GP), DSR, uDSR, NeSymReS, E2E, and TPSR. ICSR matches or outperforms most baselines on average R^2 while producing lower-complexity expressions; uDSR can match R^2 but with much higher complexity. ICSR shows better extrapolation than uDSR owing to simpler recovered functions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>- Limited context window (Llama 3 used with 8k tokens) restricts number of datapoints and previous attempts that can be supplied; too many points cause generation issues. - Dimensionality scaling: higher input dimensionality exacerbates prompt-size and clarity issues. - Some runs failed to produce valid seed functions (seed generation failure rates reported). - Overfitting when complexity penalty λ is too small; underfitting when λ too large. - Out-of-distribution extrapolation performance drops quickly for all methods; negative R^2 occurrences reported and treated as failures. - Vision-Language extension (ICSR-V) did not consistently improve results. - Some generated expressions include tiny-coefficient terms (O(1e-3)) that could be pruned.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>- Use a complexity penalty (λ) to trade off accuracy and generalization; authors found λ≈0.05 a good default. - Generate multiple diverse seeds (n_s) to ensure valid starting points. - Fit coefficients numerically (NLS) rather than relying on LLMs for numeric values. - Increase context window or provide compressed/most-informative datapoints if possible. - Supply domain-specific natural language information in prompts when available (cited as useful in related work). - Consider pruning tiny coefficients to reduce complexity and improve extrapolation. - Potential future improvements: chain-of-thought style prompts, tree-based search combined with LLM, multimodal inputs (plots) with better VLM encoders, and simply using stronger LLM backbones as they become available.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>ICSR is introduced in this paper; all numerical results above are reported in the paper's tables (Tables 2–4) and figures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9359.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9359.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICSR-V (LLaVa-NeXT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Symbolic Regression with Vision-Language Model (ICSR-V) using LLaVa-NeXT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of ICSR that augments the meta-prompt with plots (images) of the datapoints and prior function predictions and uses a Vision-Language Model (LLaVa-NeXT) as the backbone; evaluated to test whether visual inputs help SR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVa-NeXT (vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language model (VLM) built on top of an LLM backbone that accepts an image plus text input; used here with the same LLM backbone as the text-only experiments but including image encoder; specifics beyond the name are from references cited in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical modeling / Symbolic regression (multimodal attempt)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same symbolic regression task as ICSR, with additional image inputs: scatter plot of observations (seed generation) and overlay plot of best previous function (optimization loop).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>R^2 on test sets; expression complexity (# nodes). Comparison to text-only ICSR.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Table 4: ICSR-V overall avg R^2 = 0.989 ± 0.003, complexity avg = 5.7 ± 0.5; text-only ICSR overall avg R^2 = 0.990 ± 0.003, complexity = 5.5 ± 0.5. Per-benchmark differences were minor and not consistently in favor of ICSR-V.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>- Out-of-distribution of visual encoder: VLM vision encoder likely not trained on plots, so visual inputs may be out-of-distribution. - Single-image limitation (model supports only one input image in experiments). - Quality and representation of plots (e.g., resolution, overlay) potentially affect usefulness. - Same prompt and LLM-related factors as text-only ICSR (prompt design, context window, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to text-only ICSR using same backbone; results were similar with no consistent improvement from images.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>- No consistent benefit from visual inputs observed; VLM did not reliably extract additional helpful information from plots. - Visual encoder domain mismatch (natural images vs. plots) suspected. - Only a single image could be provided during optimization loop, limiting expressivity for multidimensional data.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>- Future VLMs trained on plot-like images or dedicated encoders for mathematical plots might be more effective. - Multimodal presentation of data remains promising but current VLMs tested here did not improve SR performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9359.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9359.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR (Shojaee et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR: Scientific equation discovery via programming with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contemporary work that applies LLMs to symbolic regression with a focus on including scientific natural-language descriptions of input/output variables to leverage domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-SR: Scientific equation discovery via programming with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Scientific equation discovery / domain-specific symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Use LLMs to discover scientific equations by including domain-specific natural language descriptions about variables in the prompt, thereby combining language knowledge with regression.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>- Inclusion of natural-language descriptions of variables and domain knowledge in prompts (authors focus on this as an enabling factor).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Paper is contrasted in this work: Shojaee et al. emphasize domain-informed prompting, whereas the current paper focuses on domain-agnostic SR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>- According to the present paper, Shojaee et al. focus exclusively on the domain-informed use case, so it is not directly comparable to domain-agnostic SR performance.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>- Including domain descriptions in prompts can help LLMs perform better on scientific equation discovery when natural-language semantic cues are informative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9359.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9359.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPRO (Yang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Optimization by Prompting (OPRO) / Large language models as optimizers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses LLMs with a 'meta-prompt' containing task description and previous attempts + scores to iteratively propose improved solutions to optimization problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as optimizers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Optimization / algorithmic solving (classical optimization problems)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Use LLMs as optimizers via meta-prompts for tasks such as linear regression and the travelling salesman problem (authors present experiments on these classical optimization tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>- Use of meta-prompt with prior attempts and their numerical scores; in-context learning capability of LLMs is the key enabling property.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>- The OPRO idea is applied and adapted in this paper (ICSR) for symbolic regression; meta-prompt design and example selection are important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9359.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9359.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot Time Series Forecasters (Gruver et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models are zero-shot time series forecasters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study showing LLMs can extrapolate patterns from time-series examples in-context and perform zero-shot forecasting, though not necessarily producing explicit functional forms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot time series forecasters</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Time-series forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Zero-shot forecasting of time series using in-context examples; LLMs extrapolate patterns without explicit function extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>- Ability to extrapolate patterns from in-context examples; dataset/task-specific pattern regularity likely matters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>- Paper notes LLMs do not extract symbolic functional representations in that study (they forecast but don't return formulas).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9359.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9359.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pattern Extrapolation (Mirchandani et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models as general pattern machines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work demonstrating that LLMs can recognize and extrapolate patterns given in-context examples, motivating their use for tasks that require pattern induction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as general pattern machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Pattern recognition / general in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Recognize patterns from examples and extrapolate them to complete related tasks provided in the input prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>- Quality and representativeness of in-context examples; prompt structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9359.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9359.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer optimization study (Fu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformers learn higher-order optimization methods for in-context learning: A study with linear models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study that finds Transformer models can internally implement higher-order optimization dynamics (e.g., Newton-like methods) in the course of in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers learn higher-order optimization methods for in-context learning: A study with linear models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Optimization theory / mechanistic interpretability of Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Analyze whether transformers can emulate higher-order optimization algorithms through in-context learning behavior (experiments on linear models).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>- Model architecture and training lead to emergent in-context optimization dynamics; task simplicity affects interpretability of learned dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM-SR: Scientific equation discovery via programming with large language models <em>(Rating: 2)</em></li>
                <li>Large language models as optimizers <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot time series forecasters <em>(Rating: 2)</em></li>
                <li>Large language models as general pattern machines <em>(Rating: 2)</em></li>
                <li>Transformers learn higher-order optimization methods for in-context learning: A study with linear models <em>(Rating: 2)</em></li>
                <li>Transformer-based planning for symbolic regression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9359",
    "paper_id": "paper-53a3e2c9f7151da00280677d89cc42e1762404aa",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "ICSR (Llama 3 8B)",
            "name_full": "In-Context Symbolic Regression (ICSR) using Llama 3 8B",
            "brief_description": "A method that leverages a large language model via optimization-by-prompting to propose and iteratively refine symbolic function 'skeletons' for symbolic regression; coefficients are fitted with an external non-linear least squares optimizer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3 8B",
            "model_description": "Meta's Llama 3 8B backbone used as the foundation LLM in experiments (approx. 8B parameters, 8k token context window in this work as reported), employed to generate functional forms via in-context learning and optimization-by-prompting.",
            "scientific_subdomain": "Mathematical modeling / Symbolic regression (function discovery)",
            "simulation_task": "Text-based generation of symbolic mathematical expressions (function skeletons) that fit given (X,Y) observations; iterative refinement via meta-prompt containing previous attempts and scores; external numeric optimizer fits coefficients.",
            "evaluation_metric": "Coefficient of determination R^2 on unseen test points (in-domain and out-of-distribution); objective during optimization uses Normalized MSE (NMSE) combined with a complexity penalty r(f|D) and error = 1 / r; also report expression complexity (# nodes).",
            "simulation_accuracy": "Table 2 overall average R^2 = 0.993 ± 0.002 across four SR benchmarks (Nguyen: 0.996 ± 0.002; Constant: 0.9991 ± 0.0004; R: 0.996 ± 0.001; Keijzer: 0.981 ± 0.004). Complexity (avg # nodes) overall = 6.7 ± 0.4. Out-of-distribution performance degrades with extended input ranges but ICSR outperforms most baselines at 175% and 200% domain increases.",
            "factors_affecting_accuracy": "- Complexity penalty (λ): trades fit vs simplicity; λ=0 leads to overfitting and high complexity, λ≈0.05 used as default. - Number of seed generations n_s (diversity of initial seeds): n_s=10 used; fewer seeds hurt robustness. - Optimization loop (iterative refinement) improves final performance vs seeds-only in many cases. - External coefficient fitting (NLS restarts) affects parameter recovery. - Prompt design and inclusion of previous attempts (meta-prompt) leverage in-context learning. - Sampling parameters (temperature, number of samples per prompt) affect exploration/exploitation. - Context window size limits number of explicit datapoints and previous attempts that can be included. - Model pretraining bias / vocabulary (LLM can propose functions beyond fixed SR vocabularies).",
            "comparison_baseline": "Compared against gplearn (GP), DSR, uDSR, NeSymReS, E2E, and TPSR. ICSR matches or outperforms most baselines on average R^2 while producing lower-complexity expressions; uDSR can match R^2 but with much higher complexity. ICSR shows better extrapolation than uDSR owing to simpler recovered functions.",
            "limitations_or_failure_cases": "- Limited context window (Llama 3 used with 8k tokens) restricts number of datapoints and previous attempts that can be supplied; too many points cause generation issues. - Dimensionality scaling: higher input dimensionality exacerbates prompt-size and clarity issues. - Some runs failed to produce valid seed functions (seed generation failure rates reported). - Overfitting when complexity penalty λ is too small; underfitting when λ too large. - Out-of-distribution extrapolation performance drops quickly for all methods; negative R^2 occurrences reported and treated as failures. - Vision-Language extension (ICSR-V) did not consistently improve results. - Some generated expressions include tiny-coefficient terms (O(1e-3)) that could be pruned.",
            "author_recommendations_or_insights": "- Use a complexity penalty (λ) to trade off accuracy and generalization; authors found λ≈0.05 a good default. - Generate multiple diverse seeds (n_s) to ensure valid starting points. - Fit coefficients numerically (NLS) rather than relying on LLMs for numeric values. - Increase context window or provide compressed/most-informative datapoints if possible. - Supply domain-specific natural language information in prompts when available (cited as useful in related work). - Consider pruning tiny coefficients to reduce complexity and improve extrapolation. - Potential future improvements: chain-of-thought style prompts, tree-based search combined with LLM, multimodal inputs (plots) with better VLM encoders, and simply using stronger LLM backbones as they become available.",
            "additional_notes": "ICSR is introduced in this paper; all numerical results above are reported in the paper's tables (Tables 2–4) and figures.",
            "uuid": "e9359.0",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ICSR-V (LLaVa-NeXT)",
            "name_full": "In-Context Symbolic Regression with Vision-Language Model (ICSR-V) using LLaVa-NeXT",
            "brief_description": "A variant of ICSR that augments the meta-prompt with plots (images) of the datapoints and prior function predictions and uses a Vision-Language Model (LLaVa-NeXT) as the backbone; evaluated to test whether visual inputs help SR.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVa-NeXT (vision-language model)",
            "model_description": "A vision-language model (VLM) built on top of an LLM backbone that accepts an image plus text input; used here with the same LLM backbone as the text-only experiments but including image encoder; specifics beyond the name are from references cited in the paper.",
            "scientific_subdomain": "Mathematical modeling / Symbolic regression (multimodal attempt)",
            "simulation_task": "Same symbolic regression task as ICSR, with additional image inputs: scatter plot of observations (seed generation) and overlay plot of best previous function (optimization loop).",
            "evaluation_metric": "R^2 on test sets; expression complexity (# nodes). Comparison to text-only ICSR.",
            "simulation_accuracy": "Table 4: ICSR-V overall avg R^2 = 0.989 ± 0.003, complexity avg = 5.7 ± 0.5; text-only ICSR overall avg R^2 = 0.990 ± 0.003, complexity = 5.5 ± 0.5. Per-benchmark differences were minor and not consistently in favor of ICSR-V.",
            "factors_affecting_accuracy": "- Out-of-distribution of visual encoder: VLM vision encoder likely not trained on plots, so visual inputs may be out-of-distribution. - Single-image limitation (model supports only one input image in experiments). - Quality and representation of plots (e.g., resolution, overlay) potentially affect usefulness. - Same prompt and LLM-related factors as text-only ICSR (prompt design, context window, etc.).",
            "comparison_baseline": "Compared directly to text-only ICSR using same backbone; results were similar with no consistent improvement from images.",
            "limitations_or_failure_cases": "- No consistent benefit from visual inputs observed; VLM did not reliably extract additional helpful information from plots. - Visual encoder domain mismatch (natural images vs. plots) suspected. - Only a single image could be provided during optimization loop, limiting expressivity for multidimensional data.",
            "author_recommendations_or_insights": "- Future VLMs trained on plot-like images or dedicated encoders for mathematical plots might be more effective. - Multimodal presentation of data remains promising but current VLMs tested here did not improve SR performance.",
            "uuid": "e9359.1",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-SR (Shojaee et al., 2024)",
            "name_full": "LLM-SR: Scientific equation discovery via programming with large language models",
            "brief_description": "A contemporary work that applies LLMs to symbolic regression with a focus on including scientific natural-language descriptions of input/output variables to leverage domain knowledge.",
            "citation_title": "LLM-SR: Scientific equation discovery via programming with large language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Scientific equation discovery / domain-specific symbolic regression",
            "simulation_task": "Use LLMs to discover scientific equations by including domain-specific natural language descriptions about variables in the prompt, thereby combining language knowledge with regression.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "- Inclusion of natural-language descriptions of variables and domain knowledge in prompts (authors focus on this as an enabling factor).",
            "comparison_baseline": "Paper is contrasted in this work: Shojaee et al. emphasize domain-informed prompting, whereas the current paper focuses on domain-agnostic SR.",
            "limitations_or_failure_cases": "- According to the present paper, Shojaee et al. focus exclusively on the domain-informed use case, so it is not directly comparable to domain-agnostic SR performance.",
            "author_recommendations_or_insights": "- Including domain descriptions in prompts can help LLMs perform better on scientific equation discovery when natural-language semantic cues are informative.",
            "uuid": "e9359.2",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "OPRO (Yang et al., 2023)",
            "name_full": "Optimization by Prompting (OPRO) / Large language models as optimizers",
            "brief_description": "A framework that uses LLMs with a 'meta-prompt' containing task description and previous attempts + scores to iteratively propose improved solutions to optimization problems.",
            "citation_title": "Large language models as optimizers",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Optimization / algorithmic solving (classical optimization problems)",
            "simulation_task": "Use LLMs as optimizers via meta-prompts for tasks such as linear regression and the travelling salesman problem (authors present experiments on these classical optimization tasks).",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "- Use of meta-prompt with prior attempts and their numerical scores; in-context learning capability of LLMs is the key enabling property.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": "- The OPRO idea is applied and adapted in this paper (ICSR) for symbolic regression; meta-prompt design and example selection are important.",
            "uuid": "e9359.3",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Zero-shot Time Series Forecasters (Gruver et al., 2023)",
            "name_full": "Large language models are zero-shot time series forecasters",
            "brief_description": "A study showing LLMs can extrapolate patterns from time-series examples in-context and perform zero-shot forecasting, though not necessarily producing explicit functional forms.",
            "citation_title": "Large language models are zero-shot time series forecasters",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Time-series forecasting",
            "simulation_task": "Zero-shot forecasting of time series using in-context examples; LLMs extrapolate patterns without explicit function extraction.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "- Ability to extrapolate patterns from in-context examples; dataset/task-specific pattern regularity likely matters.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": "- Paper notes LLMs do not extract symbolic functional representations in that study (they forecast but don't return formulas).",
            "author_recommendations_or_insights": null,
            "uuid": "e9359.4",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Pattern Extrapolation (Mirchandani et al., 2023)",
            "name_full": "Large language models as general pattern machines",
            "brief_description": "Work demonstrating that LLMs can recognize and extrapolate patterns given in-context examples, motivating their use for tasks that require pattern induction.",
            "citation_title": "Large language models as general pattern machines",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Pattern recognition / general in-context learning",
            "simulation_task": "Recognize patterns from examples and extrapolate them to complete related tasks provided in the input prompt.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "- Quality and representativeness of in-context examples; prompt structure.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9359.5",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Transformer optimization study (Fu et al., 2023)",
            "name_full": "Transformers learn higher-order optimization methods for in-context learning: A study with linear models",
            "brief_description": "A study that finds Transformer models can internally implement higher-order optimization dynamics (e.g., Newton-like methods) in the course of in-context learning.",
            "citation_title": "Transformers learn higher-order optimization methods for in-context learning: A study with linear models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Optimization theory / mechanistic interpretability of Transformers",
            "simulation_task": "Analyze whether transformers can emulate higher-order optimization algorithms through in-context learning behavior (experiments on linear models).",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "- Model architecture and training lead to emergent in-context optimization dynamics; task simplicity affects interpretability of learned dynamics.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9359.6",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM-SR: Scientific equation discovery via programming with large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models as optimizers",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot time series forecasters",
            "rating": 2
        },
        {
            "paper_title": "Large language models as general pattern machines",
            "rating": 2
        },
        {
            "paper_title": "Transformers learn higher-order optimization methods for in-context learning: A study with linear models",
            "rating": 2
        },
        {
            "paper_title": "Transformer-based planning for symbolic regression",
            "rating": 1
        }
    ],
    "cost": 0.01726425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery</h1>
<p>Matteo Merler ${ }^{\star}$, Katsiaryna Haitsiukevich<em>, Nicola Dainese</em> and Pekka Marttinen<br>Department of Computer Science<br>Aalto University<br>{firstname.lastname}@aalto.fi</p>
<h4>Abstract</h4>
<p>State of the art Symbolic Regression (SR) methods currently build specialized models, while the application of Large Language Models (LLMs) remains largely unexplored. In this work, we introduce the first comprehensive framework that utilizes LLMs for the task of SR. We propose In-Context Symbolic Regression (ICSR), an SR method which iteratively refines a functional form with an LLM and determines its coefficients with an external optimizer. ICSR leverages LLMs' strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors. Our findings reveal that LLMs are able to successfully find symbolic equations that fit the given data, matching or outperforming the overall performance of the best SR baselines on four popular benchmarks, while yielding simpler equations with better out of distribution generalization.</p>
<h2>1 Introduction</h2>
<p>Classical Machine Learning regression methods can be divided into two broad categories: statistical methods, which learn an implicit statistical (black-box) model of the relationship between the observations, and rule-based methods, which instead attempt to extract an explainable set of rules that explicitly model the transformation between the inputs and outputs (Lample and Charton, 2019). Symbolic Regression (SR) is a particular subset of the latter category, which searches the set of all possible explicit mathematical expressions to find the equation that best fits the given set of observations. This has the clear advantage of explainability, as well as a potential for better generalization, if the trend holds outside of the observed data.</p>
<p>The traditional approach for SR algorithms is Genetic Programming (Willis et al., 1997) (GP),</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>which combines fundamental blocks for mathematical expressions (e.g., basic operators, trigonometric functions, etc.) into more complex formulas using strategies borrowed from evolutionary biology, such as mutations and fitness. The recent success of Transformer models, first introduced by Vaswani et al. (2017), has revolutionized various fields of Artificial Intelligence, notably Natural Language Processing (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023; Anil et al., 2023) and Computer Vision (Dosovitskiy et al., 2021). Transformer-based methods have also been proposed for SR (Biggio et al., 2021; Kamienny et al., 2022), typically by employing a model pre-trained on a large amount of synthetic SR datasets.</p>
<p>Large Language Models (LLMs), also based on the Transformer, have proven to possess unprecedented reasoning and generalization abilities, based on their capability for In-Context Learning (ICL) (Brown et al., 2020). This refers to the ability to perform tasks based on the context provided in the input text without any additional fine-tuning. With the help of ICL, these models can be leveraged for a wide range of different tasks, suggesting a potential use case for Symbolic Regression.</p>
<p>In this paper, we examine the integration of LLMs into the SR pipeline, with the aim of using them to search for new equations that could fit the data. Inspired by the Optimization by Prompting (OPRO) approach presented by Yang et al. (2023), we propose In-Context Symbolic Regression (ICSR) ${ }^{1}$. This approach leverages pre-trained language models by providing a number of previously tested equations and their fitness scores in the prompt, tasking them to generate a new candidate that could be a better fit. The method is repeated until convergence is reached or the computational budget is exhausted. To the best of our knowledge,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: High level overview of the ICSR approach. Given an initial set of observations, we prompt the LLM to generate multiple initial guesses (seeds) of the true function that generated the observations. We then iteratively refine our guesses within an optimization loop where we propose new functions (based on a set of the previous best attempts), fit their coefficients and evaluate their fitness. The model only produces the functional form of a function, while the unknown coefficients are fitted using non-linear least squares optimization.</p>
<p>Only a contemporary work by Shojaee et al. (2024) has ever explored the use of LLMs for SR. However, they focus on working with equations from a scientific domain where natural language knowledge can be directly incorporated, while this work aims to generally explore the capabilities of LLMs for SR without any additional information, in order to lay a foundation that can be expanded later. We discuss in depth the differences between the two works in Section 2.</p>
<p>Our approach presents several advantages compared to models specifically trained for SR: as the LLM is not fine-tuned for this task, improvements in the underlying base model can improve ICSR without any changes to the method itself. Further, LLMs provide a natural language interface that can be leveraged to include additional information about the problem, like the domain of the equation and the interpretation of the observation values. The models could also be asked to explain the reasoning behind the proposed functions, potentially leading to a more interpretable process.</p>
<p>In summary, we make the following contributions: 1) We propose ICSR, the first general framework to leverage LLMs for the SR task. 2) We compare the method with a range of competitive SR baselines, matching or outperforming state of the art results on four popular SR benchmarks: Nguyen (Nguyen et al., 2011), Constant (Li et al., 2023d), R (Krawiec and Pawlak, 2013) and Keijzer (Keijzer, 2003). 3) We show that the equations generated with our method tend to exhibit lower complexity, which correlates with stronger out of distribution performance.</p>
<h2>2 Related Work</h2>
<p><strong>Symbolic Regression.</strong> GP has traditionally formed the backbone for SR methods (Smits and Kotanchek, 2005; Schmidt and Lipson, 2011; Virgolin et al., 2021). Typically, from an initial population, an iterative tournament is played where functions with the highest fitness are selected to 'reproduce' with some random mutation, as in Koza and Poli (2005).</p>
<p>More recently, Deep Learning methods have been applied to enhance the available toolkit for SR. Udrescu and Tegmark (2020) proposed an iterative simplification of the problem relying on insights from physics and outsourcing the function approximation part to a neural network. Petersen et al. (2021) used a Recurrent Neural Network (RNN) with a risk-seeking policy to perform a hierarchical search over the space of user-defined operators and mathematical functions. The main drawback</p>
<p>of these methods, including GP, is the fact that the algorithms start from scratch for every new expression, with very limited abilities of knowledge preservation between tasks.</p>
<p>To address this limitation, numerous Transformer-based methods inspired by language modelling have been developed. SymbolicGPT by Valipour et al. (2021), NeSymReS by Biggio et al. (2021) and CL-SR by Li et al. (2023d) proposed different generative Transformer models specifically trained for SR. These models generate a functional form ('skeleton') of the equation with a special token for coefficients which are fitted via an external numerical optimizer. Subsequently, Kamienny et al. (2022) presented E2E, a Transformer model able to produce the full expression including the coefficient values. While retaining knowledge between tasks, Transformer-based methods are quite limited in refining their solutions for the given set of points. To this end, Shojaee et al. (2023) presented a method integrating a pre-trained Transformer with Monte Carlo Tree Search to guide the equation generation merging the strength of the search and model pre-training. The proposed framework can be also viewed as a combination of a pre-trained model and an iterative refinement process. However, none of the prior methods employ a foundation model (Bommasani et al., 2021), such as an LLM, in order to leverage mathematical knowledge, but either pre-train an SR model (Biggio et al., 2021; Kamienny et al., 2022), or learn from scratch for every new function (Petersen et al., 2021).</p>
<p>Mathematical Reasoning with LLMs. As LLMs form the backbone of the method presented in this work, we rely entirely on their mathematical reasoning capabilities, such as ICL (Brown et al., 2020), to explore the solution space. Mirchandani et al. (2023) show that LLMs are able to recognize patterns from in-context examples and can extrapolate them to complete related tasks in the input. Similarly, Gruver et al. (2023) find that LLMs can extrapolate zero-shot the pattern from a timeseries (although they do not extract any functional representation). Furthermore, Fu et al. (2023) present a study in which they find that Transformer models can learn higher-order optimization methods (similar to Newton's method).</p>
<p>Contemporary to our work, Shojaee et al. (2024) also propose to perform SR with an LLM aided by an external coefficient optimizer. However, they
focus exclusively on the case where LLMs can leverage scientific knowledge for SR, by including a description of the input and output variables in the LLM prompt. In contrast, we focus on the general case where no extra knowledge is given and test on standard benchmarks within the SR community and include a wider range of established baselines, with the aim to directly evaluate the capability of LLMs on the task of SR. Furthermore, we propose advancements in the structure of the prompt, including the coordinates of the points to be regressed, the score of previous attempts, and more in-context examples. Finally, rather than asking the LLM to optimize a Mean Squared Error (MSE) objective, we employ a more advanced loss function, presented in Section 4, which jointly optimizes for the accuracy and complexity of the function for improved generalization properties.</p>
<h2>3 Background</h2>
<p>The Optimization by Prompting (OPRO) framework was introduced by Yang et al. (2023) for prompt optimization, i.e., for increasing the performance of models (such as LLMs) that receive a textual prompt in the input and have to perform a specific task (such as mathematical reasoning). Closer to our interest, the authors also present experiments on classical optimization problems (Linear Regression and Travelling Salesman Problem), suggesting that OPRO can solve such tasks.</p>
<p>The key idea of the method is the use of a socalled meta-prompt, a higher level prompt which contains a description of the task to be optimized and previous attempts (examples) in solving it with their corresponding scores. An example of such task can be querying the model to find a linear function that fits a set of points. In this case, the prompt is augmented by the functions that have been tried out and the mean squared error on the data, obtained with an external evaluation procedure. The assumption behind it is that LLMs have the ability to extrapolate the pattern formed by the examples, thanks to ICL, and propose a better alternative. The meta-prompt is given as input to the LLM and the model's output is then evaluated and added back to the meta-prompt if the score is good enough. This approach can then be iterated until a satisfying result is achieved or a certain computational budget is exhausted.</p>
<h2>4 Method</h2>
<p>We consider a regression dataset $\mathcal{D}$ with $N$ observations and target variables $\left{x_{i}, y_{i}\right}<em 0="0">{i=1}^{N}$, also denoted with $(X, Y)$ more compactly, to be used for producing a function $\hat{f}$ that well approximates the data. To leverage the OPRO approach for SR, we need to design a meta-prompt suitable for the task and fill it with the available observations $(X, Y)$, an initial set of $k$ functions $\hat{F}</em>}=\left{\hat{f<em 0="0">{0}^{(1)}, \hat{f}</em>}^{(2)}, \ldots, \hat{f<em i="i">{0}^{(k)}\right}$ (either hand-written or model-generated) and a measure of their fitness (score) on $\mathcal{D}$. For our purposes, we frame the refinement process as a minimization problem over an objective function, also called the error function, such that generated equations with the lowest error have the highest fitness. The goal is then to iteratively refine the set of functions $\hat{F}</em>$, where $k$ is a design choice ( $k=5$ in this study). The meta-prompt used in the experiment can be found in Appendix C.}, i \in[1, \ldots, n]$ for each iteration $i$, until a sufficiently low error is obtained by one of them or a maximum number of iterations is reached; we denote this process as the optimization loop. Due to the finite size of the LLM context window, we only keep the $k$ best performing previous attempts in the set $\hat{F}_{i</p>
<p>Seed Functions. At the first iteration, $\hat{F}<em s="s">{0}$ is empty as there are no previous guesses from the model. Thus, an initial population of seed functions is required to kickstart the optimization loop. Instead of relying on a fixed set of initial functions, which could be restrictive in general, we ask the model to generate the initial seed functions (with the prompt provided in Appendix C). This results in a complex and diverse set of functions, from which the LLM can refine its future predictions with the optimization loop. In our implementation we repeat this initial process $n</em>=10$ for this work and explore its impact in Section 5.5 through an ablation study.}$ times, as some of the generated functions can be undefined for certain input points (e.g., $\log (x)$ for negative numbers). We set $n_{s</p>
<p>Error Function. The immediate choice for the objective function would be an MSE, or a similar error metric, over the regression dataset $\mathcal{D}$. However, simply minimizing this error can result in overfitting on the training points in $\mathcal{D}$. As overfitting in SR often occurs due to a growing number of terms in the generated equation, we adapt from Shojaee et al. (2023) a fitness function $r(\hat{f} \mid \mathcal{D})$ with an extra penalty term for the complexity $\mathcal{C}$ of the generated expression, defined as the following:</p>
<p>$$
r(\hat{f} \mid \mathcal{D})=\frac{1}{1+\operatorname{NMSE}(\hat{f} \mid \mathcal{D})}+\lambda e^{\left(-\frac{\mathcal{C}(\hat{f})}{L}\right)}
$$</p>
<p>where $\hat{f}$ is the predicted function, $\mathcal{C}$ is the complexity defined as the number of nodes in the expression tree, $L$ is the maximum sequence length (set to 30), and $\lambda$ is a hyperparameter to trade-off between the fit to the data and the complexity. The Normalized Mean Square Error (NMSE) is calculated as</p>
<p>$$
\operatorname{NMSE}(\hat{f} \mid \mathcal{D})=\frac{\sum_{i=1}^{N}\left(y_{i}-\hat{f}\left(x_{i}\right)\right)^{2}}{\sum_{i=1}^{N} y_{i}^{2}+\epsilon}
$$</p>
<p>where $\epsilon$ is a small regularizing constant. Finally, we use $\operatorname{err}(\hat{f} \mid \mathcal{D})=r(\hat{f} \mid \mathcal{D})^{-1}$ as our error function to frame ICSR as a minimization problem. We explore the choice of the $\lambda$ parameter in Section 5.5 with a sensitivity analysis.</p>
<p>Parameter Fitting. We utilize the LLM only to generate functional forms (skeletons), while the unknown coefficients associated to the predicted functional form are optimized by Non-linear Least Squares (NLS) (Kelley, 1999) available from SciPy's (Virtanen et al., 2020). This not only yields better coefficient values, due to the superior optimization performance of NLS over LLMs, but also allows for more efficient exploration of the space of functions, by grouping them in equivalence classes of unique functional forms. In our implementation, we optimize the function's coefficients five times starting from different random initial values, to avoid local minima, similarly to Li et al. (2023d).</p>
<p>For other details about the OPRO implementation, we follow the original work. Specifically, we also sample multiple functions for every iteration (asking the model to generate 5 functions for every call) in an attempt to improve the stability of the loop and we experiment with a decreasing temperature parameter to balance exploration/exploitation (with a higher initial temperature encouraging the exploration of the underlying functional space, and a lower temperature at the later stages forcing smaller tweaks to the trajectory). To avoid saturating the model's context window, we limit the amount of training points that are included in written form to a certain threshold, empirically set to 40 . We discuss this in more detail in the Limitations Section 6.1.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>SR training</th>
<th>Evaluated</th>
<th>Model</th>
<th>Pre-trained</th>
<th>Flexible</th>
<th>Problem specific</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>examples</td>
<td>expressions</td>
<td>size</td>
<td>model</td>
<td>vocabulary</td>
<td>refinement</td>
<td>penalty</td>
</tr>
<tr>
<td>ICSR (Ours)</td>
<td>0</td>
<td>$\mathcal{O}((50 \cdot 5+10) \cdot 5)$</td>
<td>8B</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>gplearn</td>
<td>0</td>
<td>$\mathcal{O}(1000 \cdot 20)$</td>
<td>-</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>DSR</td>
<td>0</td>
<td>$\mathcal{O}(200 \mathrm{~K})$</td>
<td>8 K</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>uDSR*</td>
<td>0</td>
<td>$\mathcal{O}(200 \mathrm{~K})$</td>
<td>8 K</td>
<td>$\checkmark / \boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>NeSymReS</td>
<td>100 M</td>
<td>$\mathcal{O}(10 \cdot 10)$</td>
<td>26 M</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>E2E</td>
<td>3 M</td>
<td>$\mathcal{O}(100)$</td>
<td>86 M</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>TPSR</td>
<td>3 M</td>
<td>$\mathcal{O}(200 \cdot 3)$</td>
<td>86 M</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
</tbody>
</table>
<ul>
<li>The uDSR method potentially allows using a pre-trained model as a prior. However, as reported in the original paper, while this is useful in a low-budget search it has tendencies to worsen the performance.</li>
</ul>
<p>Table 1: Qualitative comparison across baselines. We compare different properties for all baselines. Evaluated expressions is the total number of equations a method considers for modeling a given training set. Pre-trained model refers to the use of an underlying model as opposed to training from scratch for each problem. Problem specific refinement refers to the use of a search algorithm on the space of possible skeletons.</p>
<h2>5 Experiments</h2>
<p>We empirically evaluate ICSR and compare it against a set of competitive baselines, checking both in-domain and out of distribution performance of the proposed approach.</p>
<h3>5.1 Benchmarks</h3>
<p>For our experiments, we choose four popular SR benchmarks containing functions with one or two input dimensions: Nguyen <em>Nguyen et al. (2011)</em>, Constant (a modified version of some of the Nguyen equations with different numerical values for the coefficients <em>Li et al. (2023d)</em>), R <em>Krawiec and Pawlak (2013)</em> and Keijzer <em>Keijzer (2003)</em>. The symbolic equations and ranges for both the training and testing points are reported in Appendix D. We leave for future work the evaluation of ICSR on higher dimensionality benchmarks.</p>
<h3>5.1.1 Metrics</h3>
<p>While we use the error function $\operatorname{err}(\hat{f} \mid \mathcal{D})$ during the optimization loop (see Section 4), we follow the literature in reporting the coefficient of determination $R^{2}$ <em>Glantz et al. (2017)</em> to evaluate the quality of our method. This staple metric in SR can be interpreted as follows: a function will get a positive score if it is more accurate than the average prediction and will get a score of 1 for a perfect prediction. The coefficient is computed as:</p>
<p>$$
R^{2}=1-\frac{\sum_{i=1}^{n}\left(y_{i}-\hat{y}<em i="1">{i}\right)^{2}}{\sum</em>
$$}^{n}\left(y_{i}-\bar{y}\right)^{2}</p>
<p>where $y_{i}$ is the ground truth value, $\hat{y}<em i="i">{i}$ is the predicted value and $\bar{y}$ is the average of all $y</em>$.</p>
<p>We report the $R^{2}$ metric computed on a set of unseen testing points, obtained from a dense grid within the same range of input point values as during training (for in-domain performance) or its extended version (for out of distribution performance). We follow <em>Li et al. (2023d)</em> and <em>Biggio et al. (2021)</em> in removing the 5\% worst predictions in all methods to ensure robustness against outliers. We further report the complexity $\mathcal{C}$ of the generated equations, calculated as the number of nodes in its expression tree. For all methods, we repeat all experiments across five different random seeds and report the average values together with the standard error of the mean. For ICSR, we allow up to 50 iterations in the optimization loop and end it earlier if the $R^{2}$ score on the training set exceeds 0.99999 .</p>
<h3>5.2 Baselines</h3>
<p>To evaluate the performance of the proposed method we opted for the following list of competitive baselines: gplearn <em>Stephens (2022)</em>, a classical GP approach; DSR <em>Glatt et al. (2022)</em> and uDSR <em>Landajuela et al. (2022)</em>, two searchbased methods; NeSymReS <em>Biggio et al. (2021)</em> and E2E <em>Kamienny et al. (2022)</em>, selected as representatives for Transformer-based model pre-trained over a large-scale SR dataset; and TPSR <em>Shojaee et al. (2023)</em>, which augments E2E with a decoding strategy guided by Monte-Carlo Tree Search, as an efficient combination of pre-training and search. The details of the baseline model and the hyperparameters can be found in Appendix B.</p>
<p>We compare various properties of the considered methods in Table 1. Thanks to the use of LLMs, ICSR is able to leverage a much larger model size without the need for SR-specific training examples, as opposed to the other Transformer based methods. Furthermore, our method is far more sample</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Nguyen ( $\mathcal{C}=5.2$ )</th>
<th></th>
<th>Constant ( $\mathcal{C}=4.3$ )</th>
<th></th>
<th>R ( $\mathcal{C}=8.3$ )</th>
<th></th>
<th>Keijzer ( $\mathcal{C}=5.0$ )</th>
<th></th>
<th>Overall avg.</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$R^{2}(\uparrow)$</td>
<td>$\mathcal{C}(\downarrow)$</td>
<td>$R^{2}(\uparrow)$</td>
<td>$\mathcal{C}(\downarrow)$</td>
<td>$R^{2}(\uparrow)$</td>
<td>$\mathcal{C}(\downarrow)$</td>
<td>$R^{2}(\uparrow)$</td>
<td>$\mathcal{C}(\downarrow)$</td>
<td>$R^{2}(\uparrow)$</td>
<td>$\mathcal{C}(\downarrow)$</td>
</tr>
<tr>
<td>ICSR (Ours)</td>
<td>$0.996 \pm 0.002$</td>
<td>$6.4 \pm 0.5$</td>
<td>$0.9991 \pm 0.0004$</td>
<td>$\mathbf{4 . 6} \pm \mathbf{0 . 4}$</td>
<td>$\mathbf{0 . 9 9 6} \pm \mathbf{0 . 0 0 1}$</td>
<td>$7.5 \pm 0.3$</td>
<td>$\mathbf{0 . 9 8 1} \pm \mathbf{0 . 0 0 4}$</td>
<td>$8.2 \pm 0.7$</td>
<td>$\mathbf{0 . 9 9 3} \pm \mathbf{0 . 0 0 2}$</td>
<td>$6.7 \pm 0.4$</td>
</tr>
<tr>
<td>gplearn</td>
<td>$0.75 \pm 0.15$</td>
<td>$7.2 \pm 0.8$</td>
<td>$0.74 \pm 0.22$</td>
<td>$5.6 \pm 0.7$</td>
<td>$0.97 \pm 0.01$</td>
<td>$7.6 \pm 1.3$</td>
<td>$0.09 \pm 0.43$</td>
<td>$10.5 \pm 1.4$</td>
<td>$0.6 \pm 0.2$</td>
<td>$7.7 \pm 1.0$</td>
</tr>
<tr>
<td>DSR</td>
<td>$0.983 \pm 0.005$</td>
<td>$\mathbf{5 . 8} \pm \mathbf{0 . 3}$</td>
<td>$0.96 \pm 0.01$</td>
<td>$6.9 \pm 0.7$</td>
<td>$0.95 \pm 0.03$</td>
<td>$\mathbf{5 . 7} \pm \mathbf{0 . 5}$</td>
<td>$0.84 \pm 0.03$</td>
<td>$6.3 \pm 0.3$</td>
<td>$0.93 \pm 0.02$</td>
<td>$\mathbf{6 . 2} \pm \mathbf{0 . 5}$</td>
</tr>
<tr>
<td>uDSR</td>
<td>$\mathbf{0 . 9 9 9 8} \pm \mathbf{0 . 0 0 0 1}$</td>
<td>$20.4 \pm 1.1$</td>
<td>$\mathbf{0 . 9 9 9 7} \pm \mathbf{0 . 0 0 0 1}$</td>
<td>$21.9 \pm 1.5$</td>
<td>$0.993 \pm 0.004$</td>
<td>$15.3 \pm 0.5$</td>
<td>$0.980 \pm 0.005$</td>
<td>$22.4 \pm 1.5$</td>
<td>$\mathbf{0 . 9 9 3} \pm \mathbf{0 . 0 0 2}$</td>
<td>$20.0 \pm 1.2$</td>
</tr>
<tr>
<td>NeSymReS</td>
<td>$0.976 \pm 0.007$</td>
<td>$6.3 \pm 0.2$</td>
<td>$0.97 \pm 0.01$</td>
<td>$5.9 \pm 0.2$</td>
<td>$0.92 \pm 0.02$</td>
<td>$6.2 \pm 0.5$</td>
<td>$0.87 \pm 0.02$</td>
<td>$\mathbf{6 . 2} \pm \mathbf{0 . 2}$</td>
<td>$0.93 \pm 0.01$</td>
<td>$\mathbf{6 . 2} \pm \mathbf{0 . 3}$</td>
</tr>
<tr>
<td>E2E</td>
<td>$0.9976 \pm 0.0005$</td>
<td>$18.1 \pm 1.1$</td>
<td>$0.996 \pm 0.002$</td>
<td>$16.8 \pm 1.2$</td>
<td>$0.68 \pm 0.20$</td>
<td>$22.3 \pm 1.3$</td>
<td>$0.82 \pm 0.05$</td>
<td>$20.2 \pm 0.9$</td>
<td>$0.87 \pm 0.06$</td>
<td>$19.4 \pm 1.1$</td>
</tr>
<tr>
<td>TPSR</td>
<td>$\mathbf{0 . 9 9 9 8} \pm \mathbf{0 . 0 0 0 1}$</td>
<td>$13.7 \pm 0.6$</td>
<td>$0.9993 \pm 0.0001$</td>
<td>$11.5 \pm 0.7$</td>
<td>$\mathbf{0 . 9 9 6} \pm \mathbf{0 . 0 0 1}$</td>
<td>$13.3 \pm 0.7$</td>
<td>$0.92 \pm 0.03$</td>
<td>$17.2 \pm 0.8$</td>
<td>$0.979 \pm 0.008$</td>
<td>$14.0 \pm 0.7$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison across baselines. We evaluate each method on all benchmarks with five random seeds, reporting the averages for the coefficient of determination $R^{2}$ and the function complexity $\mathcal{C}$ with the error of the mean. We further report the average ground truth complexity for each benchmark, indicated with $\mathcal{C}$.
efficient when compared to search-based methods like DSR and uDSR. The LLM is slower in generating a single expression, but is able to produce more meaningful equations, thanks to the large pretraining bias, as opposed to methods like gplearn and DSR which have to be trained from scratch on each problem. ICSR is also the only method with a natural language interface and a flexible vocabulary, which we discuss further in Section 6.</p>
<h3>5.3 Comparison across Baselines</h3>
<p>For comparison of ICSR with the baselines, we choose Llama 3 8B (Meta, 2024) as the underlying LLM. The results (see Table 2) show that the ICSR approach is very robust, consistently achieving very high scores across all benchmarks while producing expressions with a lower average complexity. The overall average columns show that ICSR outperforms all baselines, with only uDSR matching its $R^{2}$ score at the cost of significantly higher complexity. In general, it is important to consider both metrics simultaneously, as simpler functions can lead to a sightly lower $R^{2}$ value while bringing other advantages, such as better out of distribution generalization, which we explore in Section 5.4. As seen in the headers in Table 2, the complexity values of the ground truth equations align much more closely to the ones recovered by ICSR as opposed to the ones for other high-performing baselines, such as uDSR or TPSR. The improvement in complexity compared to TPSR is particularly noteworthy, as both methods are using the same objective function: this could be a sign that LLMs tend to produce more human-readable expressions thanks to their pre-training bias. It is also worth noting that ICSR can potentially improve over time by simply increasing the performance of the underlying LLM backbone without any additional training, while that is not the case for the other methods.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison across baselines on out of distribution data. We compared the proposed method with the baselines by increasing the input domain for the generated functions. Whenever the $R^{2}$ becomes negative, we fix it to 0 when computing the average for the figure on the left and report the fraction of negative values in the figure on the right.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Out of distribution examples. Qualitative examples demonstrating the generalization capabilities of ICSR and uDSR on two experiments. The higher complexity from the uDSR examples introduces unnecessary terms that harm the out of distribution performance (area shaded in red).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Nguyen</th>
<th></th>
<th>Constant</th>
<th></th>
<th>R</th>
<th></th>
<th>Keijzer</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$R^{2}$ (?)</td>
<td>$\mathcal{C}$ ( $\downarrow$ )</td>
<td>$R^{2}$ (?)</td>
<td>$\mathcal{C}$ ( $\downarrow$ )</td>
<td>$R^{2}$ (?)</td>
<td>$\mathcal{C}$ ( $\downarrow$ )</td>
<td>$R^{2}$ (?)</td>
<td>$\mathcal{C}$ ( $\downarrow$ )</td>
</tr>
<tr>
<td>ICSR $(\lambda=0.05)$</td>
<td>$0.996 \pm 0.002$</td>
<td>$6.4 \pm 0.5$</td>
<td>$0.9991 \pm 0.0004$</td>
<td>$4.6 \pm 0.4$</td>
<td>$\mathbf{0 . 9 9 6} \pm \mathbf{0 . 0 0 1}$</td>
<td>$7.5 \pm 0.3$</td>
<td>$\mathbf{0 . 9 8 1} \pm \mathbf{0 . 0 0 4}$</td>
<td>$8.2 \pm 0.7$</td>
</tr>
<tr>
<td>ICSR $(\lambda=0)$</td>
<td>$\mathbf{0 . 9 9 9 0} \pm \mathbf{0 . 0 0 0 4}$</td>
<td>$18.6 \pm 0.5$</td>
<td>$\mathbf{0 . 9 9 9 4} \pm \mathbf{0 . 0 0 0 1}$</td>
<td>$17.6 \pm 0.6$</td>
<td>$0.988 \pm 0.005$</td>
<td>$19.9 \pm 0.8$</td>
<td>$0.92 \pm 0.05$</td>
<td>$16.6 \pm 0.6$</td>
</tr>
<tr>
<td>ICSR $(\lambda=0.1)$</td>
<td>$0.992 \pm 0.003$</td>
<td>$6.1 \pm 0.5$</td>
<td>$0.9978 \pm 0.0006$</td>
<td>$4.3 \pm 0.3$</td>
<td>$0.989 \pm 0.004$</td>
<td>$6.5 \pm 0.4$</td>
<td>$0.980 \pm 0.005$</td>
<td>$7.8 \pm 0.6$</td>
</tr>
<tr>
<td>ICSR $(\lambda=0.5)$</td>
<td>$0.94 \pm 0.03$</td>
<td>$4.4 \pm 0.4$</td>
<td>$0.983 \pm 0.003$</td>
<td>$3.0 \pm 0.2$</td>
<td>$0.972 \pm 0.005$</td>
<td>$4.07 \pm 0.07$</td>
<td>$0.95 \pm 0.02$</td>
<td>$6.4 \pm 0.6$</td>
</tr>
<tr>
<td>ICSR $(\lambda=1)$</td>
<td>$0.92 \pm 0.03$</td>
<td>$\mathbf{3 . 7} \pm \mathbf{0 . 3}$</td>
<td>$0.89 \pm 0.04$</td>
<td>$\mathbf{2 . 5} \pm \mathbf{0 . 2}$</td>
<td>$0.95 \pm 0.01$</td>
<td>$\mathbf{3 . 4} \pm \mathbf{0 . 2}$</td>
<td>$0.77 \pm 0.05$</td>
<td>$\mathbf{4 . 8} \pm \mathbf{0 . 5}$</td>
</tr>
<tr>
<td>Seed only $\left(n_{s}=10\right)$</td>
<td>$0.95 \pm 0.03$</td>
<td>$11.4 \pm 0.8$</td>
<td>$0.982 \pm 0.003$</td>
<td>$8.0 \pm 0.7$</td>
<td>$0.990 \pm 0.005$</td>
<td>$10.3 \pm 0.5$</td>
<td>$0.93 \pm 0.03$</td>
<td>$10.8 \pm 0.8$</td>
</tr>
<tr>
<td>Seed only $\left(n_{s}=5\right)$</td>
<td>$0.986 \pm 0.003$</td>
<td>$12.4 \pm 0.7$</td>
<td>$0.986 \pm 0.003$</td>
<td>$10.0 \pm 0.7$</td>
<td>$0.995 \pm 0.001$</td>
<td>$10.7 \pm 0.6$</td>
<td>$0.88 \pm 0.04$</td>
<td>$12.7 \pm 0.8$</td>
</tr>
<tr>
<td>Seed only $\left(n_{s}=1\right)$</td>
<td>$0.91^{*} \pm 0.04$</td>
<td>$14.5^{*} \pm 0.8$</td>
<td>$0.95^{*} \pm 0.03$</td>
<td>$13.5^{*} \pm 0.9$</td>
<td>$0.97^{*} \pm 0.02$</td>
<td>$12.8^{*} \pm 0.7$</td>
<td>$0.66^{*} \pm 0.07$</td>
<td>$16.0^{*} \pm 0.7$</td>
</tr>
<tr>
<td>Random Guessing</td>
<td>$0.960 \pm 0.006$</td>
<td>$3.9 \pm 0.2$</td>
<td>$0.971 \pm 0.005$</td>
<td>$4.4 \pm 0.2$</td>
<td>$0.91 \pm 0.03$</td>
<td>$4.7 \pm 0.2$</td>
<td>$0.77 \pm 0.04$</td>
<td>$4.0 \pm 0.2$</td>
</tr>
</tbody>
</table>
<ul>
<li>Some runs failed to generate valid seed functions. Only 88\% of the experiments for nguyen, 93\% for constant, 87\% for R and 95\% for keijzer finished with at least one valid function.</li>
</ul>
<p>Table 3: Sensitivity analysis and ablation studies. We perform sensitivity analysis on the values of the complexity penalty parameter $\lambda$ and two ablation studies: one using only $n_{s}$ initial seed functions without improving them and the other one using random guessing, rather than ICSR, for proposing new functions. All ablations on $n_{s}$ are performed without the optimization loop, only keeping the best generated seed function. We report the averages for the coefficient of determination $R^{2}$ and the function complexity $\mathcal{C}$ with the error of the mean for all experiments. We highlight in bold the best performance across different values of $\lambda$.</p>
<h3>5.4 Out of Distribution Performance</h3>
<p>We further explore the advantage of producing functions with a lower complexity value by testing the out of distribution capabilities of the expressions recovered by ICSR and the other baselines. We exclude gplearn from these experiments, as we observe its performance to be significantly lower compared to the rest of the methods. To include out-ofdomain test points, we extend the input range by 100\% to all directions (in which the function is defined). We compute the $R^{2}$ value on the extended range, reporting the results in Figure 2. Note that the $R^{2}$ value can quickly become increasingly negative when the functions diverge significantly. In order to keep the results stable, we treat all negative values as 0 when computing the average and report the fraction of experiments with a negative $R^{2}$.</p>
<p>In general, we observe a sharp decline in performance for all methods, with the fraction of negative $R^{2}$ values quickly increasing towards the further extensions of the range. Specifically, ICSR is the highest performing method in the 175\% and 200\% domain increases, with the second lowest and lowest number of failures respectively. Generally, methods with lower complexity such as NeSymReS, E2E and TPSR tend to perform better than uDSR, with the exception of DSR which exhibits the poorest out of distribution performance even with a low average complexity. The comparison between ICSR and uDSR is particularly meaningful: as reported in Table 2, the two methods are tied for the best overall average performance, but ICSR outperforms uDSR when extrapolating further outside of the training range thanks to the lower complexity of the recovered expressions. We present some qualitative examples that demonstrate the difference between the methods in Figure 3.</p>
<h3>5.5 Sensitivity Analysis and Ablation Studies</h3>
<p>In this section we first investigate the impact of the $\lambda$ parameter and then test the importance of the iterative refinement of the equations with the optimization loop. Finally, we compare ICSR with a baseline where the LLM was not given any information about the observations. All results are reported in Table 3.</p>
<p>Lambda Parameter. In our sensitivity analysis we considered the complexity penalty parameter $\lambda=[0,0.05,0.1,0.5,1]$. We noticed that the smallest penalty $\lambda=0.05$ was already sufficient to considerably reduce the complexity of the selected functions and increasing the penalty further had a relatively smaller impact on complexity. Therefore we used $\lambda=0.05$ for our experiments. With $\lambda=0$ the complexity is not considered and the equations overfit on the observations: the $R^{2}$ score tends to improve slightly at the cost of a large increase in complexity, with expressions composed of many different terms attempting to fit perfectly the training set. As the value for the parameter $\lambda$ increases, both the $R^{2}$ score and the complexity tend to decrease, resulting in equations that underfit the data, as they do not have enough terms to properly capture all the observed dependencies. These results align with Shojaee et al. (2023), who introduced the fitness function we use. They chose 0.1 as the</p>
<p>final parameter value, which we find performing similarly to 0.05 , but slightly underfitting on some benchmarks, particularly R.</p>
<p>Optimization Loop. The results suggest that the seed functions generation step plays a key role in our approach, as with $n_{s}=10$ the results already show a high fitness on the test set, although they still underperform the full method in terms of both $R^{2}$ and complexity. We notice that using the best seed functions without refinement can outperform the results with ICSR for some values of $\lambda$ (e.g. $\lambda=0,0.1)$ in the most complex benchmarks ( R and Keijzer). This is because the performance is reported on the set of test points and can decrease when refining, due to overfitting on the training points. It's also worth noting that some of the experiments with only a single initial call did not result in any valid seed functions, showing the need for repeating the generation process multiple times. In the prompt used to generate the seed functions (reported in Appendix C) we specifically ask for a diverse and complex set of functions that can be optimized, which is likely why the complexity on the seed functions is much higher, as it will be lowered later in the optimization loop. Overall, both parts of the method are necessary for the best possible performance; repeating the seed function generation step multiple times allows the model to generate a large number of potential initial expressions, resulting in a solid set of initial candidates for the optimization loop to build upon.</p>
<p>Random Guessing. As some of the benchmarks contain common equations such as simple polynomials, the LLM could simply be randomly generating functions that fit the data points, instead of actually making use of the information provided in the prompt. To ensure that this is not the case, we compare ICSR with a 'Random Guessing' baseline, where the LLM was prompted for 60 times (matching the budget used for ICSR, which uses 10 prompts to generate the seed functions and 50 prompts for the optimization loop) to generate five random functions, without any information about the observations or previous guesses (the prompt is reported in Appendix C). The results show that this baseline underperforms ICSR on all four benchmarks, especially on Keijzer, the hardest one. Empirically, we observe that the functions generated by the LLM in this way are all extremely simple, mostly constrained to basic polynomials. This confirms that LLMs are able to extract patterns from
the prompt and are not simply randomly generating the solutions.</p>
<h2>6 Discussion</h2>
<p>Optimizing for out of distribution. A general framework for optimizing the out of distribution performance of a predictive model (such as a symbolic equation) is to regularise its complexity, following the Occam's Razor principle that simpler explanations are preferable to more complex ones, all other things being equal. In our work we use the working definition of complexity as the number of nodes in the expression tree of an equation. However, more optimal choices could be available: for instance, equations containing expressions not defined on all the real domain (such as logarithms and square roots) could be penalised more, as they could be undefined when extrapolating to larger domains. Knowing in advance the full domain in which an equation is supposed to hold could also greatly improve out of distribution performance by filtering out invalid candidate functions. In the case of ICSR, it could also be leveraged as extra information by the LLM. Furthermore, we observe that numerous equations that we derive with ICSR have extra terms with very small coefficients (e.g. $\mathcal{O}\left(10^{-3}\right)$ ) that do not contribute significantly to the shape of the equation and could be safely suppressed, resulting in expressions with a lower complexity. This could be done by modifying the optimization procedure of the coefficients, to eliminate coefficients under a certain threshold, which would be a hyperparameter of the method.</p>
<p>Vocabulary. In general, most SR methods are limited to a predefined vocabulary of operators and tokens, while LLMs can virtually explore any possible function and combination. An example of this is with the $x_{1}^{x_{2}}$ function in the Nguyen benchmark: in Biggio et al. (2021), the authors mention that it is not included in the set of equations that their model can fit, while our approach can recover the exact expression. We also observe a similar trend with the other baselines for this specific expression. In our prompts (see Appendix C) we include a vocabulary for the LLM, but this is meant more to guide the LLM into the correct search space and is by no means a hard restriction: for example, we observe that ICSR can produce the erf function even if it wasn't reported in this list. Furthermore, any function that can be found in the model's pre-training corpus (fundamentally the In-</p>
<p>ternet) can be potentially added to the prompt at any time if desired, which is impossible for other fixed-vocabulary methods.</p>
<h3>6.1 Limitations</h3>
<p>Although promising, the approach presented in this work still suffers from some key limitations that hold back its potential as a full Symbolic Regression method.</p>
<p>Size of the context window. LLMs are provided with a context window, which represents the maximum number of tokens they can process as input at the same time. For instance, Llama3, used for ICSR, has an 8 k token context window. This limits the amount of information that we can include in the prompt, in terms of training datapoints and previously attempted functions with their errors. However, with context-window size increasing, commercially available LLMs like GPT-4 Turbo (Achiam et al., 2023) and Claude 3 (Anthropic, 2024), which process over 100k tokens, this issue is likely to be alleviated or lifted completely.</p>
<p>What to include in the prompt? Including all needed information in the prompt might not be enough, as some research suggests LLMs cannot fully utilize extended contexts (Liu et al., 2024c). In practice, we observe that when too many points are included, the model often continues generating points, especially with two-dimensional functions. Limiting training points in the prompt to 40 (chosen empirically) helps, while all input points are still used for coefficient optimization. Some directions to help the model leveraging the information in the data could be to sample the most informative subset of points to fit in the prompt, or present the LLM with higher-level descriptions of the points, rather than feeding them directly to the model. Finally, we hypothesize that presenting the data in different modalities, such as images of the points and plots of the functions, by using multimodal foundation models, might be helpful to incorporate all information available. We experimented with Vision-Language Models, but our attempts in that direction, reported in Section A of the Appendix, were not fruitful so far.</p>
<p>Dimensionality. Using an LLM for higher dimensional inputs is possible, but dimensionality exacerbates the issues presented above. As the number of variables grows, so does the space dedicated to the input points in the prompt, which
will naturally confuse the model and obfuscate the structure in the datapoints even further. Specifically fine-tuning an LLM on this kind of examples might show some improvement, but scaling this approach for higher dimensional problems remains a challenge.</p>
<h2>7 Conclusion</h2>
<p>We show that LLMs paired with the ICSR approach are able to perform Symbolic Regression tasks on classical SR benchmarks. The proposed method matches or outperforms a variety of established SR baselines, while producing simpler expressions that more closely resemble the complexity of the ground truth equations and result in better out of distribution performance. This work exposes yet another task that LLMs can be leveraged for, thanks to specialized techniques such as ICSR, and shows promise for integrating these models with mathematical reasoning methods.</p>
<h3>7.1 Future Work</h3>
<p>As this is one of the first works published on this topic, much work remains to be done. LLMs allow the inclusion of domain-specific natural language information into the prompt, as explored by Shojaee et al. (2024). The natural language interface could be further exploited by employing explicit Chain of Thought-like (Wei et al., 2022; Kojima et al., 2022) techniques, allowing the model to output even more well-informed guesses at every step and resulting in an interpretable method. Another interesting direction would be to consider tree-based search algorithms on top of the LLM, analogously to the TPSR (Shojaee et al., 2023) approach. As our work proves the intrinsic ability of LLMs to perform SR without taking into consideration any additional inputs, we have hope that future work can build upon ICSR to further leverage foundation models for SR.</p>
<h2>Acknowledgements</h2>
<p>We are grateful to Alexander Ilin and Alberto Zabeo for the fruitful discussions. We thank AaltoIT (IT Services of Aalto University, Finland) for provided support with computational resources. This work was supported by the Research Council of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI, and grants 352986, 358246) and EU (H2020 grant 101016775 and NextGenerationEU).</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems, volume 35, pages 2371623736. Curran Associates, Inc.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Anthropic. 2024. Introducing the next generation of Claude. URL: https://www.anthropic.com/ news/claude-3-family.</p>
<p>Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar. 2023. Fuyu-8b: A multimodal architecture for ai agents. URL: https://www. adept. ai/blog/fuyu-8b.</p>
<p>Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. 2021. Neural symbolic regression that scales. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 936-945. PMLR.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations.</p>
<p>Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. 2023. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086.</p>
<p>Stanton A. Glantz, Bryan K. Slinker, and Torsten B. Neilands. 2017. Dedication. McGraw-Hill Education, New York, NY.</p>
<p>Ruben Glatt, Felipe Leno da Silva, VAN HAI BUI, Can Huang, Lingxiao Xue, Mengqi Wang, Fangyuan Chang, Yi Murphey, and Wencong Su. 2022. Deep symbolic optimization for electric component sizing in fixed topology power converters. In AAAI 2022 Workshop on AI for Design and Manufacturing (ADAM).</p>
<p>Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. 2023. Large language models are zero-shot time series forecasters. In Advances in Neural Information Processing Systems, volume 36, pages 19622-19635. Curran Associates, Inc.</p>
<p>Pierre-alexandre Kamienny, Stéphane d'Ascoli, Guillaume Lample, and Francois Charton. 2022. End-to-end symbolic regression with transformers. In Advances in Neural Information Processing Systems, volume 35, pages 10269-10281. Curran Associates, Inc.</p>
<p>Maarten Keijzer. 2003. Improving Symbolic Regression with Interval Arithmetic and Linear Scaling. In Genetic Programming, Lecture Notes in Computer Science, pages 70-82, Berlin, Heidelberg. Springer.
C. T. Kelley. 1999. Iterative Methods for Optimization, pages 22-25. Society for Industrial and Applied Mathematics.</p>
<p>Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. 2023. Generating images with multimodal language models. In Advances in Neural Information Processing Systems, volume 36, pages 21487-21506. Curran Associates, Inc.</p>
<p>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213. Curran Associates, Inc.</p>
<p>John R. Koza and Riccardo Poli. 2005. Genetic programming. In Edmund K. Burke and Graham Kendall, editors, Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques, pages 127-164. Springer US, Boston, MA.</p>
<p>Krzysztof Krawiec and Tomasz Pawlak. 2013. Approximating geometric crossover by semantic backpropagation. In Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation, GECCO '13, page 941-948, New York, NY, USA. Association for Computing Machinery.</p>
<p>Guillaume Lample and François Charton. 2019. Deep learning for symbolic mathematics. In International Conference on Learning Representations.</p>
<p>Mikel Landajuela, Chak Shing Lee, Jiachen Yang, Ruben Glatt, Claudio P Santiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, and Brenden K Petersen. 2022. A unified framework for deep symbolic regression. In Advances in Neural Information Processing Systems, volume 35, pages 33985-33998. Curran Associates, Inc.</p>
<p>Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. 2023a. Otterhd: A highresolution multi-modality model. arXiv preprint arXiv:2311.04219.</p>
<p>Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023b. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726.</p>
<p>Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c. BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 19730-19742. PMLR.</p>
<p>Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 12888-12900. PMLR.</p>
<p>Wenqiang Li, Weijun Li, Linjun Sun, Min Wu, Lina Yu, Jingyi Liu, Yanjie Li, and Songsong Tian. 2023d. Transformer-based model for symbolic regression via joint supervised learning. In International Conference on Learning Representations.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296-26306.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024b. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. URL: https://llava-v1.github.io/ blog/2024-01-30-llava-next/.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Advances in Neural Information Processing Systems, volume 36, pages 34892-34916. Curran Associates, Inc.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024c. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Meta. 2024. Meta Llama 3. URL: https://llama. meta.com/llama3.</p>
<p>Suvir Mirchandani, Fei Xia, Pete Florence, brian ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large language models as general pattern machines. In 7th Annual Conference on Robot Learning.</p>
<p>Quang Uy Nguyen, Nguyen Hoai, Michael O’Neill, Robert McKay, and Edgar Galván-López. 2011. Semantically-based crossover in genetic programming: Application to real-valued symbolic regression. Genetic Programming and Evolvable Machines, 12:91-119.</p>
<p>Brenden K Petersen, Mikel Landajuela Larma, Terrell N. Mundhenk, Claudio Prata Santiago, Soo Kyung Kim, and Joanne Taery Kim. 2021. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In International Conference on Learning Representations.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748-8763. PMLR.</p>
<p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical textconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125.</p>
<p>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8821-8831. PMLR.</p>
<p>Michael Schmidt and Hod Lipson. 2011. Age-fitness pareto optimization. In Rick Riolo, Trent McConaghy, and Ekaterina Vladislavleva, editors, Genetic Programming Theory and Practice VIII, pages 129-146. Springer New York, New York, NY.</p>
<p>Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, and Chandan Reddy. 2023. Transformer-based planning for symbolic regression. In Advances in Neural Information Processing Systems, volume 36, pages 45907-45919. Curran Associates, Inc.</p>
<p>Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. 2024.</p>
<p>LLM-SR: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400.</p>
<p>Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. FLAVA: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1563815650 .</p>
<p>Guido F. Smits and Mark Kotanchek. 2005. Paretofront exploitation in symbolic regression. In UnaMay O'Reilly, Tina Yu, Rick Riolo, and Bill Worzel, editors, Genetic Programming Theory and Practice II, pages 283-299. Springer US, Boston, MA.</p>
<p>Trevor Stephens. 2022. gplearn: Genetic programming in python. URL: https://gplearn. readthedocs.io/en/stable/. https://github. com/trevorstephens/gplearn.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Silviu-Marian Udrescu and Max Tegmark. 2020. AI Feynman: A physics-inspired method for symbolic regression. Science Advances, 6(16):eaay2631.</p>
<p>Mojtaba Valipour, Bowen You, Maysum Panju, and Ali Ghodsi. 2021. Symbolicgpt: A generative transformer model for symbolic regression. arXiv preprint arXiv:2106.14131.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.
M. Virgolin, T. Alderliesten, C. Witteveen, and P. A. N. Bosman. 2021. Improving model-based genetic programming for symbolic regression of small expressions. Evolutionary Computation, 29(2):211-237.</p>
<p>Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, SciPy 1.0 Contributors, et al. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272.</p>
<p>Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. 2024. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.</p>
<p>M-J Willis, Hugo G Hiden, Peter Marenbach, Ben McKay, and Gary A Montague. 1997. Genetic programming: An introduction and survey of applications. In Second international conference on genetic algorithms in engineering systems: innovations and applications, pages 314-319. IET.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. In International Conference on Learning Representations.</p>
<p>Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2024. Vision-language models for vision tasks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence.</p>
<h1>A Vision-Language Models</h1>
<p>In this section we report our findings on extending ICSR to Vision-Language Models (VLMs), which we considered a promising direction, but was not successful experimentally, at least with the VLMs that we considered.</p>
<h2>A. 1 Vision-Language Extension</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Example of plots used with the VLM. (a) Scatter plot of the observations used when generating the seed functions. (b) Plot of the best function from a previous iteration used in the optimization loop.</p>
<p>Reasoning on the observations and the previously attempted functions to come up with better function candidates is a challenging task. Visualising the data and the functions, when possible, can be of great help for humans and, we hypothesize, for SR models too. We thus explore the use of visual information in ICSR by considering VLMs in place of LLMs and adding to the optimization meta-prompt a scatter plot containing the observations (Figure 4a), as well as plots superimposing the best previously generated function (Figure 4b). We dub this variant ICSR-V and present results for it in Section A.3. However, the use of both vision and language as input comes with the restriction of dimensionality, as it is impossible to visualize inputs with more than two inputs in a single image. A solution could be to include projections into each dimension as the input, but this can quickly grow out of control as the number of variables increases, and then the additional information would probably provide diminishing returns.</p>
<h2>A. 2 Related Work</h2>
<p>VLMs have gained traction after Radford et al. (2021) introduced CLIP, which aligns text and image representations using a contrastive objective. Various foundation models have been proposed, such as FLAVA (Singh et al., 2022), LLaVa (Liu
et al., 2023, 2024a,b), Flamingo (Alayrac et al., 2022), OTTER (Li et al., 2023b,a), Fuyu (Bavishi et al., 2023) and more recently OpenAI's GPT4's vision extension. A thorough survey of VLM techniques and tasks was performed recently by Zhang et al. (2024). Typically, a VLM can be built on top of a pre-trained LLM, which is then paired with an image embedding network that can transfer the image into the same token space used by the model, attempting to keep semantic similarity. This approach is employed, for instance, by BLIP (Li et al., 2022) and its successor BLIP2 (Li et al., 2023c). Moreover, these models typically can only consume images as input, but are unable to generate them as an answer, but the general framework can be enhanced with methods for text-to-image generation, such as DALL-E (Ramesh et al., 2021, 2022) and GILL (Koh et al., 2023).</p>
<h2>A. 3 Comparison of Text-Only and Vision-Language Models</h2>
<p>To evaluate the effectiveness of the additional plots, we compare our method with a variant using the LLaVa-NeXT (Liu et al., 2024b) VLM. To ensure a fair comparison, we use the same backbone model and repeat the experiments with and without the inclusion of visual information. This consists of a scatter plot of the observations for the seed functions generation step with the overlay of the best previous function (as the model only supports one input image at the time of writing) during the optimization loop. An example of the input plots can be found in Figure 4. We repeat both experiments across five different random seeds and report the results in Table 4. Surprisingly, the performance of the method seems to be unaffected by the presence of the images. This might be due to several factors, among which the fact that the vision encoder of the VLM has not been trained on plots of functions, but rather on natural images, thus, the visual inputs might be out of distribution for the model. We also experimented asking the model facts about the plots in input (such as range of points, maximum and minimum values of the function, shape, first and second derivatives), with no consistent success. It might be that future models will be more amenable to this sort of visual mathematical reasoning, but this is not the case for current VLMs, as was also suggested by recent work (Wang et al., 2024).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">ICSR-V</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ICSR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$R^{2}(\uparrow)$</td>
<td style="text-align: center;">$\mathcal{C}(\downarrow)$</td>
<td style="text-align: center;">$R^{2}(\uparrow)$</td>
<td style="text-align: center;">$\mathcal{C}(\downarrow)$</td>
</tr>
<tr>
<td style="text-align: left;">Nguyen</td>
<td style="text-align: center;">$0.991 \pm 0.003$</td>
<td style="text-align: center;">$5.1 \pm 0.3$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 4} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: center;">$\mathbf{5 . 0} \pm \mathbf{0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Constant</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 5} \pm \mathbf{0 . 0 0 1}$</td>
<td style="text-align: center;">$4.3 \pm 0.3$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 5} \pm \mathbf{0 . 0 0 1}$</td>
<td style="text-align: center;">$\mathbf{3 . 9} \pm \mathbf{0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">R</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 8} \pm \mathbf{0 . 0 0 3}$</td>
<td style="text-align: center;">$\mathbf{5 . 7} \pm \mathbf{0 . 5}$</td>
<td style="text-align: center;">$0.986 \pm 0.003$</td>
<td style="text-align: center;">$\mathbf{5 . 7} \pm \mathbf{0 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Keijzer</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 3} \pm \mathbf{0 . 0 0 6}$</td>
<td style="text-align: center;">$7.6 \pm 0.8$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 4} \pm \mathbf{0 . 0 0 4}$</td>
<td style="text-align: center;">$\mathbf{7 . 4} \pm \mathbf{0 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Overall avg.</td>
<td style="text-align: center;">$0.989 \pm 0.003$</td>
<td style="text-align: center;">$5.7 \pm 0.5$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 0} \pm \mathbf{0 . 0 0 3}$</td>
<td style="text-align: center;">$\mathbf{5 . 5} \pm \mathbf{0 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison on the impact of additional visual input. All experiments are performed with LLaVaNeXT as the underlying model, either providing or excluding a plot of the best previous function in the prompts (respectively ICSR-V and ICSR columns). We report the averages with their errors.</p>
<h2>B Hyperparameters</h2>
<p>We report the hyperparameters used with LLMs (Table 5). As reported in the main text, for ICSR we sample $n_{s}=10$ initial seed functions and repeat the optimization loop for 50 iterations, using an acceptance threshold of 0.99999 and repeating the coefficient fitting for 5 times with different initializations. For DSR and uDSR we set the computation budget for the number of expressions to evaluate to 200 K and extend the vocabulary as {add, sub, mul, div, sin, cos, exp, log, sqrt, n2, abs, n3, n4 } and {add, sub, mul, div, sin, cos, exp, log, sqrt, abs, poly} correspondingly. For the NeSymRes model we evaluate the model checkpoint that has been obtained with the training set of 100 M expression skeletons. The actual number of the equations in the training set is even larger since the values for the coefficients are resampled on each training batch. The beam size in NeSymRes is set to 10 and the number of restarts for the external coefficient optimizer is 10 , while for E2E model the beam size is 100 but the coefficient optimizer is applied just once. E2E doesn't benefit from restarting the external coefficient optimizer as much since E2E predicts the whole equation including the values of the coefficients. The predicted coefficients can be further improved by numerical optimizer but they serve as good initial values. For all other implementation details, we follow the default hyperparameters provided in the following repositories: gplearn ${ }^{2}$, DSR/uDSR ${ }^{3}$, NeSymReS ${ }^{4}$ and E2E/TPSR ${ }^{5}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 5: Sampling parameters for the LLMs.</p>
<h2>C Prompts</h2>
<p>The prompt used to generate the seed functions is reported in Figure 5, while the prompt used during the optimization loop is reported in Figure 6 and the one used for the random guessing baseline is reported in Figure 7. For the ICSR-V extension presented in Appendix A we add a brief description of the provided plots as well as the image.</p>
<h2>D Benchmark functions</h2>
<p>The list of functions and point ranges for all the benchmarks can be found in Table 6. The range for training and testing points was taken from the original source where available. Nguyen and Constant do not include a range for the testing points, so we used the same range as the training points but with more sample points. $\mathcal{U}[$ min, max, num] indicates points randomly sampled from a uniform distribution between the min and max values, while [min, max, num] indicates a range of equispaced points from min to max. The training points are sampled from $\mathcal{U}[$ min, max, num] once and then kept fixed across the random seeds and all tested methods to ensure consistency.</p>
<h2>E Sample results</h2>
<p>We present a sample of one solution for each function in the benchmarks found by our method, to qualitatively investigate the generated expressions. The observations are seen in blue, the true function is seen in red and the model's guess is seen in green (Figures 8, 9, and 10 and 11). Some of the failures of the models are apparent: in areas where there is a low density of training points the model sometimes makes guesses that ignore the overall trend, as seen, for example, in the R3 equation (Figure 10). The Keijzer benchmark is also much harder in the last 5 equations, with only 20 randomly sampled points to cover a complex 2D space, which can lead to some failures (e.g., in Keijzer 14).</p>
<p>I want you to act as a mathematical function generator. Given a set of points below, you are to come up with 5 potential functions that would fit the points. Don't worry too much about accuracy: your task is to generate a set of functions that are as diverse as possible, so that they can serve as starting points for further optimization.
To generate the functions, you will start from a set of basic operators and expressions, and combine them into something more complex.
Your options are:</p>
<ul>
<li>An independent variable symbol: x.</li>
<li>A coefficient symbol: c (there is no need to write a number - write this generic coefficient instead).</li>
<li>Basic operators: +, -, *, /, 3 sqrt, exp, log, abs</li>
<li>Trigonometric expressions: sin, cos, tan, sinh, cosh, tanh</li>
</ul>
<p>Make sure there are no numbers in the functions, use the coefficient token 'c' instead. Analyze the points carefully: if there are any negative points in the input, sqrt and log can not be used unless the input is combined with abs.
The functions should all begin with the indicators "f1(x) = ", "f2(x) = "... Your task is to combine an arbitrary number of these basic blocks to create a complex expression. Don't be afraid to be creative and experiment! The functions should be as complex as possible, combining many different operations. Variety is key!
Points: {points}
Functions:</p>
<p>Figure 5: Prompt used to generate the seed functions.</p>
<p>I want you to act as a mathematical function generator. You are given a set of points with ( $\mathrm{x}, \mathrm{y}$ ) coordinates below: {points}
Below are some previous functions and the error they make on the points above. The errors are arranged in order of their fit values, with the highest values coming first, and lower is better.
Your task is to give me a list of five new potential functions that are different from all the ones reported below, and have a lower error value than all of the functions below. Only output the new functions and nothing else.
Remember that the functions you generate should always have at most {num_variables} variables {variables_list}. The functions should have parametric form, using 'c' in place of any constant or coefficient. The coefficients will be optimized to fit the data. Make absolutely sure that the functions you generate are completely different from the ones already given to you.
The functions should all begin with the indicators "f1(x) = ", "f2(x) = "...
Remember that you can combine the simple building blocks (operations, constants, variables) in any way you want to generate more complex functions. Don't be afraid to experiment!
{previous_trajectory}</p>
<p>Figure 6: Prompt used during the optimization loop.</p>
<p>Generate five random functions of the form Function: $f(x)$. The functions you generate should always have at most {num_variables} variables {variables_list}. Only output the functions and nothing else.</p>
<p>Figure 7: Prompt used for the random guessing baseline.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">Function</th>
<th style="text-align: center;">Train Points</th>
<th style="text-align: center;">Test Points</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">nguyen1</td>
<td style="text-align: center;">$x^{3}+x^{2}+x$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,200]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen2</td>
<td style="text-align: center;">$x^{4}+x^{3}+x^{2}+x$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,200]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen3</td>
<td style="text-align: center;">$x^{5}+x^{4}+x^{3}+x^{2}+x$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,200]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen4</td>
<td style="text-align: center;">$x^{6}+x^{5}+x^{4}+x^{3}+x^{2}+x$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,200]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen5</td>
<td style="text-align: center;">$\sin \left(x^{2}\right) \cdot \cos (x)-1$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,200]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen6</td>
<td style="text-align: center;">$\sin (x)+\sin \left(x+x^{2}\right)$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,200]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen7</td>
<td style="text-align: center;">$\log (x+1)+\log \left(x^{2}+1\right)$</td>
<td style="text-align: center;">$\mathcal{U}[0,2,20]$</td>
<td style="text-align: center;">$[0,2,200]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen8</td>
<td style="text-align: center;">$\sqrt{x}$</td>
<td style="text-align: center;">$\mathcal{U}[0,4,20]$</td>
<td style="text-align: center;">$[0,4,200]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen9</td>
<td style="text-align: center;">$\sin \left(x_{1}\right)+\sin \left(x_{2}^{2}\right)$</td>
<td style="text-align: center;">$\mathcal{U}[[-1,-1],[1,1], 100]$</td>
<td style="text-align: center;">$[[-1,-1],[1,1], 500]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen10</td>
<td style="text-align: center;">$2 \cdot \sin \left(x_{1}\right) \cdot \cos \left(x_{2}\right)$</td>
<td style="text-align: center;">$\mathcal{U}[[-1,-1],[1,1], 100]$</td>
<td style="text-align: center;">$[[-1,-1],[1,1], 500]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen11</td>
<td style="text-align: center;">$x_{1}^{x_{2}}$</td>
<td style="text-align: center;">$\mathcal{U}[[0,0],[1,1], 100]$</td>
<td style="text-align: center;">$[[0,0],[1,1], 500]$</td>
</tr>
<tr>
<td style="text-align: center;">nguyen12</td>
<td style="text-align: center;">$x_{1}^{4}-x_{1}^{3}+\frac{1}{2} \cdot x_{2}^{2}-x_{2}$</td>
<td style="text-align: center;">$\mathcal{U}[[-1,-1],[1,1], 100]$</td>
<td style="text-align: center;">$[[-1,-1],[1,1], 500]$</td>
</tr>
<tr>
<td style="text-align: center;">constant1</td>
<td style="text-align: center;">$3.39 x^{3}+2.12 x^{2}+1.78 x$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,200]$</td>
</tr>
<tr>
<td style="text-align: center;">constant2</td>
<td style="text-align: center;">$\sin \left(x^{2}\right) \cdot \cos (x)-0.75$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,200]$</td>
</tr>
<tr>
<td style="text-align: center;">constant3</td>
<td style="text-align: center;">$\sin \left(1.5 x_{1}\right) \cdot \cos \left(0.5 x_{2}\right)$</td>
<td style="text-align: center;">$\mathcal{U}[[-1,-1],[1,1], 100]$</td>
<td style="text-align: center;">$[[-1,-1],[1,1], 500]$</td>
</tr>
<tr>
<td style="text-align: center;">constant4</td>
<td style="text-align: center;">$2.7 x_{1}^{x_{2}}$</td>
<td style="text-align: center;">$\mathcal{U}[[0,0],[1,1], 100]$</td>
<td style="text-align: center;">$[[0,0],[1,1], 500]$</td>
</tr>
<tr>
<td style="text-align: center;">constant5</td>
<td style="text-align: center;">$\sqrt{1.23 x}$</td>
<td style="text-align: center;">$\mathcal{U}[0,4,20]$</td>
<td style="text-align: center;">$[0,4,200]$</td>
</tr>
<tr>
<td style="text-align: center;">constant6</td>
<td style="text-align: center;">$x^{0.426}$</td>
<td style="text-align: center;">$\mathcal{U}[0,4,20]$</td>
<td style="text-align: center;">$[0,4,200]$</td>
</tr>
<tr>
<td style="text-align: center;">constant7</td>
<td style="text-align: center;">$2 \sin \left(1.3 x_{1}\right)+\cos \left(x_{2}\right)$</td>
<td style="text-align: center;">$\mathcal{U}[[-1,-1],[1,1], 100]$</td>
<td style="text-align: center;">$[[-1,-1],[1,1], 500]$</td>
</tr>
<tr>
<td style="text-align: center;">constant8</td>
<td style="text-align: center;">$\ln (x+1.4)+\ln \left(x^{2}+1.3\right)$</td>
<td style="text-align: center;">$\mathcal{U}[0,2,20]$</td>
<td style="text-align: center;">$[0,2,200]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer3</td>
<td style="text-align: center;">$0.3 x \cdot \sin (2 \pi x)$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,100]$</td>
<td style="text-align: center;">$[-1,1,10000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer4</td>
<td style="text-align: center;">$x^{3} \cdot \exp (-x) \cdot \cos (x) \sin (x)$</td>
<td style="text-align: center;">$[0,10,200]$</td>
<td style="text-align: center;">$[0.05,10.05,200]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$(\sin (x)^{2} \cdot \cos (x)-1)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">keijzer6</td>
<td style="text-align: center;">$(x \cdot(x+1)) / 2$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,50]$</td>
<td style="text-align: center;">$[-1,1,100]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer7</td>
<td style="text-align: center;">$\ln (x)$</td>
<td style="text-align: center;">$\mathcal{U}[1,100,100]$</td>
<td style="text-align: center;">$[1,100,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer8</td>
<td style="text-align: center;">$\sqrt{x}$</td>
<td style="text-align: center;">$\mathcal{U}[0,100,100]$</td>
<td style="text-align: center;">$[0,100,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer9</td>
<td style="text-align: center;">$\ln \left(x+\sqrt{x^{2}+1}\right)$</td>
<td style="text-align: center;">$\mathcal{U}[0,100,100]$</td>
<td style="text-align: center;">$[0,100,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer10</td>
<td style="text-align: center;">$x_{1}^{x_{2}}$</td>
<td style="text-align: center;">$\mathcal{U}[0,1,100]$</td>
<td style="text-align: center;">$[0,1,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer11</td>
<td style="text-align: center;">$x_{1} \cdot x_{2}+\sin \left(\left(x_{1}-1\right) \cdot\left(x_{2}-1\right)\right)$</td>
<td style="text-align: center;">$\mathcal{U}[-3,3,20]$</td>
<td style="text-align: center;">$[-3,3,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer12</td>
<td style="text-align: center;">$x_{1}^{4}-x_{1}^{3}+\frac{\left(x_{2}^{2}\right)}{2}-x_{2}$</td>
<td style="text-align: center;">$\mathcal{U}[-3,3,20]$</td>
<td style="text-align: center;">$[-3,3,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer13</td>
<td style="text-align: center;">$6 \cdot \sin \left(x_{1}\right) \cdot \cos \left(x_{2}\right)$</td>
<td style="text-align: center;">$\mathcal{U}[-3,3,20]$</td>
<td style="text-align: center;">$[-3,3,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer14</td>
<td style="text-align: center;">$8 /\left(2+x_{1}^{2}+x_{2}^{2}\right)$</td>
<td style="text-align: center;">$\mathcal{U}[-3,3,20]$</td>
<td style="text-align: center;">$[-3,3,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">keijzer15</td>
<td style="text-align: center;">$\frac{x_{1}^{3}}{5}+\frac{x_{2}^{3}}{2}-x_{2}-x_{1}$</td>
<td style="text-align: center;">$\mathcal{U}[-3,3,20]$</td>
<td style="text-align: center;">$[-3,3,1000]$</td>
</tr>
<tr>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">$(x+1)^{3} /\left(x^{2}-x+1\right)$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,20]$</td>
</tr>
<tr>
<td style="text-align: center;">R2</td>
<td style="text-align: center;">$\left(x^{5}-3 \cdot x^{3}+1\right) /\left(x^{2}+1\right)$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,20]$</td>
</tr>
<tr>
<td style="text-align: center;">R3</td>
<td style="text-align: center;">$\left(x^{6}+x^{5}\right) /\left(x^{4}+x^{3}+x^{2}+x+1\right)$</td>
<td style="text-align: center;">$\mathcal{U}[-1,1,20]$</td>
<td style="text-align: center;">$[-1,1,20]$</td>
</tr>
</tbody>
</table>
<p>Table 6: Functions and point ranges for all benchmarks.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: ICSR Results for the Nguyen benchmark for the random seed 1.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: ICSR Results for the Constant benchmark for the random seed 1.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: ICSR Results for the R benchmark for the random seed 1.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 11: ICSR Results for the Keijzer benchmark for the random seed 1.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/trevorstephens/gplearn
${ }^{3}$ https://github.com/dso-org/
deep-symbolic-optimization
${ }^{4}$ https://github.com/SymposiumOrganization/ NeuralSymbolicRegressionThatScales
${ }^{5}$ https://github.com/deep-symbolic-mathematics/ TPSR&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ We release the code at: https://github.com/merlerm/ In-Context-Symbolic-Regression.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
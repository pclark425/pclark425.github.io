<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1795 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1795</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1795</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-dc4b654b1a9688ead7ae67f02afe9d22be4a43d3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dc4b654b1a9688ead7ae67f02afe9d22be4a43d3" target="_blank">Grounded Action Transformation for Robot Learning in Simulation</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A new algorithm for learning in simulation — Grounded Action Transformation — is proposed and applied to learning of humanoid bipedal locomotion and results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk.</p>
                <p><strong>Paper Abstract:</strong> 
 
 Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. This paper proposes a new algorithm for learning in simulation — Grounded Action Transformation — and applies it to learning of humanoid bipedal locomotion. Our approach results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk.
 
</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1795.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1795.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded Action Transformation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that grounds a simulator by learning an action-transformation g(s,a)=α f_sim^{-1}(s,f(s,a))+(1−α)a where f is a learned forward model of the real robot dynamics and f_sim^{-1} is a learned inverse dynamics model of the simulator; used to make simulated actions produce the same next-state distribution as on the physical robot so policy search in sim yields policies that transfer to reality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>SoftBank NAO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 25 degree-of-freedom humanoid robot used for bipedal locomotion experiments; here used with an existing UNSW open-source walk engine parameterized by 15 walk parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>humanoid bipedal locomotion / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>SimSpark and Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics-based rigid-body simulators: SimSpark (fast, lower-fidelity, uses the Open Dynamics Engine) and Gazebo (higher-fidelity, more realistic joint/actuator/reactivity modeling); both simulate joint kinematics, dynamics, IMU and foot sensors and environment contacts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>two levels: SimSpark = lower-fidelity physics (very reactive / near-instantaneous joint commands), Gazebo = higher-fidelity physics (more realistic joint reactivity at higher computational cost)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body physics, joint dynamics/reactivity, IMU and foot sensors, contact with ground, actuator responses (modeled but with differing reactivity), joint accelerations (used in model prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>exact actuator delays and command timing (SimSpark models instantaneous achievement of target joint angles), contact dynamics/detail of external forces may be mismodeled, differences in joint reactivity vs real robot, simulator parameter values (friction/coefficient specifics) not guaranteed to match real world, mismatch in action frequency (simulators at 50 Hz vs NAO at 100 Hz) requiring downsampling of real data.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical SoftBank NAO robot executing forward walk trials toward a target until 4 meters or a fall; actions on the robot are issued at 100 Hz, IMU and foot sensors available, trajectories ≈20.5s for baseline policy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Optimized bipedal forward walking policy (tuning parameters of a walk engine) to maximize average forward walk velocity while maintaining stability (avoid falling).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Policy search (CMA-ES evolutionary optimization) performed entirely in grounded simulation; forward and inverse dynamics models (f and f_sim^{-1}) trained by supervised regression with neural networks from robot and simulator rollouts respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Average forward walk velocity (cm/s) and stability (fall penalty; unstable if falls in any of the 5 evaluation trajectories). Percentage improvement over baseline walk velocity reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Baseline θ₀: 19.52 cm/s; After two iterations of GAT (SimSpark-grounded then re-grounded): 27.97 cm/s (43.27% improvement over baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>mismatch in actuator dynamics and command timing (SimSpark's near-instantaneous actions vs real robot delays), different joint reactivity between simulators and hardware, action frequency mismatch (sim 50 Hz vs robot 100 Hz), imperfect contact dynamics modeling, limited observability (partial state), distribution shift as policy changes leading to model error accumulation over trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Collecting real-robot transition data (≈15 trajectories) to train a forward model f, collecting simulator rollouts to train f_sim^{-1}, applying the learned action-transformation g during simulation; using a smoothing parameter α chosen to preserve stability; performing heavy sample-based optimization in sim (CMA-ES with ~30,000 simulated trajectories per iteration) so only few (≈65) robot trials are needed; iterative re-grounding (collecting new robot data from improved policy) to account for distributional change.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate modeling of action effects (inverse dynamics) is critical; contact dynamics and external forces are recognized as likely failure modes for the action-transformation assumption; grounding need only be accurate in the local state-action distribution visited by the current policy (local grounding), and shorter trajectory lengths reduce compounding model error.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Real-world data was used only to train the forward model (15 trajectories) and to evaluate candidate policies (5 trajectories per candidate); no further policy optimization was performed on the physical robot beyond evaluation and re-grounding the simulator with collected transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>GAT was evaluated transferring from lower-fidelity SimSpark to higher-fidelity Gazebo (SimSpark->Gazebo) showing GAT outperformed no grounding and noise-envelope baselines (average improvement SimSpark->Gazebo: GAT 22.48% vs Noise-Envelope 18.93% vs No Ground 11.09%); when applied SimSpark->physical NAO, GAT produced a 43.27% improvement in walk velocity; Gazebo also used as a more realistic simulator and GAT with Gazebo improved walk velocity by over 30%, demonstrating generality across simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning an action-transformation using a learned forward model of the real robot and an inverse dynamics model of the simulator (GAT) significantly improves sim-to-real transfer of locomotion skills compared to unmodified simulation or simple action-noise envelopes; GAT reduces required real-world samples to a small number by moving expensive learning to simulation, but its success depends on the existence of simulator actions that can reproduce real transitions (assumption may break under complex contact dynamics) and grounding is local to the state distribution of the training policy so re-grounding is useful as policies change.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Action Transformation for Robot Learning in Simulation', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1795.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1795.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded Simulation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework where a modifiable simulator is iteratively grounded using transition data collected on the physical robot so that policy improvement performed in the modified simulator yields policies that transfer better to the real system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Humanoid robots learning to walk faster: From the real world to simulation and back</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>SoftBank NAO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 25 degree-of-freedom humanoid robot used for bipedal locomotion experiments; here used with an existing UNSW open-source walk engine parameterized by 15 walk parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>humanoid bipedal locomotion / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Modifiable physics simulator (parameterized P_φ), implemented with SimSpark and Gazebo in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A simulator whose dynamics are parameterized and can be altered (grounded) to better match the real environment; in practice implemented using SimSpark (fast, lower-fidelity) and Gazebo (higher-fidelity), both simulating rigid-body physics, joints, sensors, and contacts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>framework-agnostic; experiments use lower-fidelity (SimSpark) and higher-fidelity (Gazebo) simulators</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>physics/dynamics, sensors (IMU, foot), joint actuation and kinematics, contact with ground</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>simulator parameter mismatches can exist (friction, contact response, actuator delays), and original GSL approaches assumed instantaneous action effects (an assumption that may not hold on real robots).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical SoftBank NAO robot executing walking trials; used to collect transition data to ground the simulator and to evaluate candidate policies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Bipedal walking gait optimization (increasing forward walk velocity while maintaining stability).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Iterative cycle: collect data on robot, ground simulator by fitting simulator parameters (or with GAT, an action transform), perform policy improvement in grounded simulator (here CMA-ES), evaluate candidates on robot, repeat.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Average forward walk velocity and stability (fall count), and percent improvement over baseline policy.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Prior GSL work (Farchy et al.) reported ~26.7% improvement on a slow stable walk; this paper (using GAT within GSL) achieved up to 43.27% improvement on the NAO.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>simulator parameter mismatch, assumption of instantaneous action effects (in some prior methods), distributional shift as policy changes, model capacity and supervised-learning approximation errors when fitting dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Simulator must be modifiable (parameterized), ability to record trajectories on the physical robot, an optimization routine that returns candidate policies to evaluate on the robot, and methods for grounding either via parameter fitting or action/sensor transformations (GAT is one such method).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Grounding can focus on minimizing one-step dynamics error as a surrogate for trajectory-level match; the paper emphasizes the need for simulator modifications that match the state-action distribution of the current policy (local grounding) and recognizes that contact dynamics and inverse mapping existence are critical fidelity aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>GSL uses robot data to ground the simulator and uses robot evaluations to select among candidate policies; it does not require extensive policy optimization directly on the robot in the studied workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The GSL framework is compatible with different fidelity simulators; in experiments using SimSpark (low-fidelity) grounded to Gazebo (used as surrogate real) and then to the physical NAO, grounded simulation (especially with GAT) outperformed baselines, demonstrating that grounding is beneficial across fidelity levels though grounding must be local to the policy's state distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Grounding a modifiable simulator using real-robot transition data enables effective sim-to-real transfer; making the simulator more faithful in the local distribution visited by the policy improves transfer, but grounding must account for distribution shift as policies change and some simulator biases (e.g., contact dynamics, actuator delays) are hard to eliminate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Action Transformation for Robot Learning in Simulation', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Humanoid robots learning to walk faster: From the real world to simulation and back <em>(Rating: 2)</em></li>
                <li>Transfer from simulation to real world through learning deep inverse dynamics model <em>(Rating: 2)</em></li>
                <li>Noise and the reality gap: The use of simulation in evolutionary robotics <em>(Rating: 2)</em></li>
                <li>Epopt: Learning robust neural network policies using model ensembles <em>(Rating: 1)</em></li>
                <li>Robots that can adapt like animals <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1795",
    "paper_id": "paper-dc4b654b1a9688ead7ae67f02afe9d22be4a43d3",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "GAT",
            "name_full": "Grounded Action Transformation",
            "brief_description": "An algorithm that grounds a simulator by learning an action-transformation g(s,a)=α f_sim^{-1}(s,f(s,a))+(1−α)a where f is a learned forward model of the real robot dynamics and f_sim^{-1} is a learned inverse dynamics model of the simulator; used to make simulated actions produce the same next-state distribution as on the physical robot so policy search in sim yields policies that transfer to reality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "SoftBank NAO",
            "agent_system_description": "A 25 degree-of-freedom humanoid robot used for bipedal locomotion experiments; here used with an existing UNSW open-source walk engine parameterized by 15 walk parameters.",
            "domain": "humanoid bipedal locomotion / robotics",
            "virtual_environment_name": "SimSpark and Gazebo",
            "virtual_environment_description": "Physics-based rigid-body simulators: SimSpark (fast, lower-fidelity, uses the Open Dynamics Engine) and Gazebo (higher-fidelity, more realistic joint/actuator/reactivity modeling); both simulate joint kinematics, dynamics, IMU and foot sensors and environment contacts.",
            "simulation_fidelity_level": "two levels: SimSpark = lower-fidelity physics (very reactive / near-instantaneous joint commands), Gazebo = higher-fidelity physics (more realistic joint reactivity at higher computational cost)",
            "fidelity_aspects_modeled": "rigid-body physics, joint dynamics/reactivity, IMU and foot sensors, contact with ground, actuator responses (modeled but with differing reactivity), joint accelerations (used in model prediction)",
            "fidelity_aspects_simplified": "exact actuator delays and command timing (SimSpark models instantaneous achievement of target joint angles), contact dynamics/detail of external forces may be mismodeled, differences in joint reactivity vs real robot, simulator parameter values (friction/coefficient specifics) not guaranteed to match real world, mismatch in action frequency (simulators at 50 Hz vs NAO at 100 Hz) requiring downsampling of real data.",
            "real_environment_description": "Physical SoftBank NAO robot executing forward walk trials toward a target until 4 meters or a fall; actions on the robot are issued at 100 Hz, IMU and foot sensors available, trajectories ≈20.5s for baseline policy.",
            "task_or_skill_transferred": "Optimized bipedal forward walking policy (tuning parameters of a walk engine) to maximize average forward walk velocity while maintaining stability (avoid falling).",
            "training_method": "Policy search (CMA-ES evolutionary optimization) performed entirely in grounded simulation; forward and inverse dynamics models (f and f_sim^{-1}) trained by supervised regression with neural networks from robot and simulator rollouts respectively.",
            "transfer_success_metric": "Average forward walk velocity (cm/s) and stability (fall penalty; unstable if falls in any of the 5 evaluation trajectories). Percentage improvement over baseline walk velocity reported.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Baseline θ₀: 19.52 cm/s; After two iterations of GAT (SimSpark-grounded then re-grounded): 27.97 cm/s (43.27% improvement over baseline).",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "mismatch in actuator dynamics and command timing (SimSpark's near-instantaneous actions vs real robot delays), different joint reactivity between simulators and hardware, action frequency mismatch (sim 50 Hz vs robot 100 Hz), imperfect contact dynamics modeling, limited observability (partial state), distribution shift as policy changes leading to model error accumulation over trajectories.",
            "transfer_enabling_conditions": "Collecting real-robot transition data (≈15 trajectories) to train a forward model f, collecting simulator rollouts to train f_sim^{-1}, applying the learned action-transformation g during simulation; using a smoothing parameter α chosen to preserve stability; performing heavy sample-based optimization in sim (CMA-ES with ~30,000 simulated trajectories per iteration) so only few (≈65) robot trials are needed; iterative re-grounding (collecting new robot data from improved policy) to account for distributional change.",
            "fidelity_requirements_identified": "Accurate modeling of action effects (inverse dynamics) is critical; contact dynamics and external forces are recognized as likely failure modes for the action-transformation assumption; grounding need only be accurate in the local state-action distribution visited by the current policy (local grounding), and shorter trajectory lengths reduce compounding model error.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Real-world data was used only to train the forward model (15 trajectories) and to evaluate candidate policies (5 trajectories per candidate); no further policy optimization was performed on the physical robot beyond evaluation and re-grounding the simulator with collected transitions.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "GAT was evaluated transferring from lower-fidelity SimSpark to higher-fidelity Gazebo (SimSpark-&gt;Gazebo) showing GAT outperformed no grounding and noise-envelope baselines (average improvement SimSpark-&gt;Gazebo: GAT 22.48% vs Noise-Envelope 18.93% vs No Ground 11.09%); when applied SimSpark-&gt;physical NAO, GAT produced a 43.27% improvement in walk velocity; Gazebo also used as a more realistic simulator and GAT with Gazebo improved walk velocity by over 30%, demonstrating generality across simulators.",
            "key_findings": "Learning an action-transformation using a learned forward model of the real robot and an inverse dynamics model of the simulator (GAT) significantly improves sim-to-real transfer of locomotion skills compared to unmodified simulation or simple action-noise envelopes; GAT reduces required real-world samples to a small number by moving expensive learning to simulation, but its success depends on the existence of simulator actions that can reproduce real transitions (assumption may break under complex contact dynamics) and grounding is local to the state distribution of the training policy so re-grounding is useful as policies change.",
            "uuid": "e1795.0",
            "source_info": {
                "paper_title": "Grounded Action Transformation for Robot Learning in Simulation",
                "publication_date_yy_mm": "2017-02"
            }
        },
        {
            "name_short": "GSL",
            "name_full": "Grounded Simulation Learning",
            "brief_description": "A framework where a modifiable simulator is iteratively grounded using transition data collected on the physical robot so that policy improvement performed in the modified simulator yields policies that transfer better to the real system.",
            "citation_title": "Humanoid robots learning to walk faster: From the real world to simulation and back",
            "mention_or_use": "use",
            "agent_system_name": "SoftBank NAO",
            "agent_system_description": "A 25 degree-of-freedom humanoid robot used for bipedal locomotion experiments; here used with an existing UNSW open-source walk engine parameterized by 15 walk parameters.",
            "domain": "humanoid bipedal locomotion / robotics",
            "virtual_environment_name": "Modifiable physics simulator (parameterized P_φ), implemented with SimSpark and Gazebo in experiments",
            "virtual_environment_description": "A simulator whose dynamics are parameterized and can be altered (grounded) to better match the real environment; in practice implemented using SimSpark (fast, lower-fidelity) and Gazebo (higher-fidelity), both simulating rigid-body physics, joints, sensors, and contacts.",
            "simulation_fidelity_level": "framework-agnostic; experiments use lower-fidelity (SimSpark) and higher-fidelity (Gazebo) simulators",
            "fidelity_aspects_modeled": "physics/dynamics, sensors (IMU, foot), joint actuation and kinematics, contact with ground",
            "fidelity_aspects_simplified": "simulator parameter mismatches can exist (friction, contact response, actuator delays), and original GSL approaches assumed instantaneous action effects (an assumption that may not hold on real robots).",
            "real_environment_description": "Physical SoftBank NAO robot executing walking trials; used to collect transition data to ground the simulator and to evaluate candidate policies.",
            "task_or_skill_transferred": "Bipedal walking gait optimization (increasing forward walk velocity while maintaining stability).",
            "training_method": "Iterative cycle: collect data on robot, ground simulator by fitting simulator parameters (or with GAT, an action transform), perform policy improvement in grounded simulator (here CMA-ES), evaluate candidates on robot, repeat.",
            "transfer_success_metric": "Average forward walk velocity and stability (fall count), and percent improvement over baseline policy.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Prior GSL work (Farchy et al.) reported ~26.7% improvement on a slow stable walk; this paper (using GAT within GSL) achieved up to 43.27% improvement on the NAO.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "simulator parameter mismatch, assumption of instantaneous action effects (in some prior methods), distributional shift as policy changes, model capacity and supervised-learning approximation errors when fitting dynamics.",
            "transfer_enabling_conditions": "Simulator must be modifiable (parameterized), ability to record trajectories on the physical robot, an optimization routine that returns candidate policies to evaluate on the robot, and methods for grounding either via parameter fitting or action/sensor transformations (GAT is one such method).",
            "fidelity_requirements_identified": "Grounding can focus on minimizing one-step dynamics error as a surrogate for trajectory-level match; the paper emphasizes the need for simulator modifications that match the state-action distribution of the current policy (local grounding) and recognizes that contact dynamics and inverse mapping existence are critical fidelity aspects.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "GSL uses robot data to ground the simulator and uses robot evaluations to select among candidate policies; it does not require extensive policy optimization directly on the robot in the studied workflows.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "The GSL framework is compatible with different fidelity simulators; in experiments using SimSpark (low-fidelity) grounded to Gazebo (used as surrogate real) and then to the physical NAO, grounded simulation (especially with GAT) outperformed baselines, demonstrating that grounding is beneficial across fidelity levels though grounding must be local to the policy's state distribution.",
            "key_findings": "Grounding a modifiable simulator using real-robot transition data enables effective sim-to-real transfer; making the simulator more faithful in the local distribution visited by the policy improves transfer, but grounding must account for distribution shift as policies change and some simulator biases (e.g., contact dynamics, actuator delays) are hard to eliminate.",
            "uuid": "e1795.1",
            "source_info": {
                "paper_title": "Grounded Action Transformation for Robot Learning in Simulation",
                "publication_date_yy_mm": "2017-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Humanoid robots learning to walk faster: From the real world to simulation and back",
            "rating": 2
        },
        {
            "paper_title": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "rating": 2
        },
        {
            "paper_title": "Noise and the reality gap: The use of simulation in evolutionary robotics",
            "rating": 2
        },
        {
            "paper_title": "Epopt: Learning robust neural network policies using model ensembles",
            "rating": 1
        },
        {
            "paper_title": "Robots that can adapt like animals",
            "rating": 1
        }
    ],
    "cost": 0.012355749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Grounded Action Transformation for Robot Learning in Simulation</h1>
<p>Josiah P. Hanna, Peter Stone<br>Dept. of Computer Science<br>The University of Texas at Austin<br>Austin, TX 78712 USA<br>{jphanna,pstone}@cs.utexas.edu</p>
<h4>Abstract</h4>
<p>Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. Grounded simulation learning (GSL) promises to address this issue by altering the simulator to better match the real world. This paper proposes a new algorithm for GSL - Grounded Action Transformation - and applies it to learning of humanoid bipedal locomotion. Our approach results in a $43.27 \%$ improvement in forward walk velocity compared to a state-of-the art hand-coded walk. We further evaluate our methodology in controlled experiments using a second, higher-fidelity simulator in place of the real world. Our results contribute to a deeper understanding of grounded simulation learning and demonstrate its effectiveness for learning robot control policies.</p>
<h2>Introduction</h2>
<p>Manually designing control policies for every possible situation a robot could encounter is impractical. Reinforcement learning (RL) provides a promising alternative to handcoding skills. Recent applications of RL to high dimensional control tasks have seen impressive successes within simulation (Schulman et al. 2015; Lillicrap et al. 2015). Unfortunately, a large gap exists between what is possible in simulation and the reality of robot learning. State-of-the-art learning methods require thousands of episodes of experience which is impractical for a physical robot. Aside from the time it would take, collecting the required training data may lead to substantial wear on the robot. Furthermore, as the robot explores different policies it may execute unsafe actions which could damage the robot.</p>
<p>An alternative to learning directly on the robot is learning in simulation (Cutler and How 2015; Koos, Mouret, and Doncieux 2010). Simulation is a valuable tool for robotics research as execution of a robotic skill in simulation is comparatively easier than real world execution. However, the value of simulation learning is limited by the inherent inaccuracy of simulators in modeling the dynamics of the physical world (Kober, Bagnell, and Peters 2013). As a result, learning that takes place in a simulator is unlikely to improve real world performance.</p>
<p>Copyright (C) 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p>
<p>Grounded Simulation Learning (GSL) is a framework for learning with a simulator in which the simulator is modified with data from the physical robot, learning takes place in simulation, the new policy is evaluated on the robot, and data from the new policy is used to further modify the simulator (Farchy et al. 2013). The paper introducing GSL demonstrates the effectiveness of the method in a single, limited experiment, by increasing the forward walking velocity of a slow, stable bipedal walk by $26.7 \%$.</p>
<p>This paper introduces a new algorithm - Grounded Action Transformation (GAT) - for simulator grounding within the GSL framework. The algorithm is fully implemented and evaluated using a high-fidelity simulator as a surrogate for the real world. The results of this study contribute to a deeper understanding of transfer from simulation methods and the effectiveness of GAT. In contrast to prior work, our real-world evaluation of GAT starts from a state-of-the-art walk engine as the base policy, and nonetheless is able to improve the walk velocity by over $43 \%$, leading to what may be the fastest known walk on the SoftBank NAO robot.</p>
<h2>Background</h2>
<h2>Preliminaries</h2>
<p>We formalize robot skill learning as a reinforcement learning (RL) problem (Sutton and Barto 1998). At discrete time-step $t$ the robot takes action $A_{t} \sim \pi\left(\cdot \mid S_{t}\right)$ according to a policy, $\pi$, which is a distribution over actions, $A_{t} \in \mathcal{A}$, conditioned on the current state, $S_{t} \in \mathcal{S}$. The environment, $E$, responds with $S_{t+1} \sim P\left(\cdot \mid S_{t}, A_{t}\right)$ according to the dynamics, $P$ : $\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}<em 0="0">{\geq 0}$ which is a probability density function over next states conditioned on the current state and action. A trajectory of length $L$ is a sequence of states and actions, $\tau:=S</em>}, A_{0}, \ldots, S_{L}, A_{L}$. We also define a cost function, $c$, which assigns a scalar cost to each $(s, a)$ pair. We denote the value of policy, $\pi$, as $J(\pi):=\mathbb{E<em t="0">{\tau \sim \operatorname{Pr}(\cdot \mid \pi)}\left[\sum</em>(\tau \mid \pi)$ is the probability of $\tau$ when selecting actions according to $\pi$.}^{L} c\left(S_{t}, A_{t}\right)\right]$ where $\operatorname{Pr</p>
<p>We assume $\pi$ is parameterized by a vector $\boldsymbol{\theta}$ and denote the parameterized policy as $\pi_{\boldsymbol{\theta}}$. Since $\boldsymbol{\theta}$ determines the policy distribution we overload notation by referring to $\pi_{\boldsymbol{\theta}}$ as $\boldsymbol{\theta}$. Given initial parameters $\boldsymbol{\theta}<em 0="0">{0}$, the goal of policy improvement is to find $\boldsymbol{\theta}^{\prime}$ such that $J\left(\boldsymbol{\theta}^{\prime}\right)&lt;J\left(\boldsymbol{\theta}</em>$ is a deterministic policy that maps state observations to an action}\right)$. In this work, $\boldsymbol{\theta</p>
<p>vector $\mathbf{a}<em t="t">{t}$. Each component of $\mathbf{a}</em>}, \mathbf{a<em t="t">{t}^{i}$, is the desired joint angle for degree-of-freedom $i$. At each time-step, $t, \boldsymbol{\theta}$ selects $\mathbf{a}</em>}$ according to the robot's configuration in joint space, $\mathbf{x<em t="t">{t}$, high level intention commands (e.g., walk forward at $75 \%$ of maximum velocity), $\boldsymbol{\omega}</em>}$, and a sensor vector, $\boldsymbol{\psi<em t="t">{t}$, of inertial measurement (IMU) and foot sensor readings. The state of the robot can be fully described as $\mathbf{s}</em>}=\left\langle\mathbf{x<em t="t">{t}, \dot{\mathbf{x}}</em>}, \ddot{\mathbf{x}<em t="t">{t}, \boldsymbol{\omega}</em>}, \boldsymbol{\psi<em t="t">{t}\right\rangle$ where $\dot{\mathbf{x}}</em>}$ and $\ddot{\mathbf{x}<em t="t">{t}$ are the time derivatives of $\mathbf{x}</em>}$. Since the robot only observes $\mathbf{x<em t="t">{t}, \boldsymbol{\omega}</em>$, these directional commands are state features.}$, and $\boldsymbol{\psi}_{t}, E$ is partially observable. Note that while the high level intention commands are determined by the robot, from the point-of-view of $\boldsymbol{\theta</p>
<p>In this paper, learning takes place in a simulator which is an environment, $E_{\text {sim }}$, that models $E$. Specifically $E_{\text {sim }}$ has the same state-action space as $E$ but inevitably a different dynamics distribution, $P_{\text {sim }}$. In many robotics settings, $c$ is user defined and thus is identical for $E$ and $E_{\text {sim }}$. However, the different dynamics distribution mean that for any policy, $\boldsymbol{\theta}$, $J(\boldsymbol{\theta}) \neq J_{\text {sim }}(\boldsymbol{\theta})$ since $\boldsymbol{\theta}$ induces a different trajectory distribution in $E$ than in $E_{\text {sim }}$. Thus $\boldsymbol{\theta}^{\prime}$ with $J_{\text {sim }}\left(\boldsymbol{\theta}^{\prime}\right)&lt;J_{\text {sim }}\left(\boldsymbol{\theta}<em 0="0">{0}\right)$ does not imply $J\left(\boldsymbol{\theta}^{\prime}\right)&lt;J\left(\boldsymbol{\theta}</em>}\right)$ - in fact $J\left(\boldsymbol{\theta}^{\prime}\right)$ could even be worse than $J\left(\boldsymbol{\theta<em _sim="{sim" _text="\text">{0}\right)$. In practice and in the literature, learning in a simulator frequently leads to catastrophic degradation of $J$. This paper explores methods for learning in $E</em>$ that result in lower policy cost.}</p>
<h2>Related Work</h2>
<p>This section surveys previous research in simulation-transfer methods. Our work also relates to model-based reinforcement learning, however, we restrict our attention here to methods directly concerned with learning in simulation.</p>
<p>One approach to simulation-transfer is using experience in simulation to reduce learning time on the physical robot. Cutler et al. (2014) use lower fidelity simulators to narrow the action search space for faster learning in higher fidelity simulators or the real world. Cully et al. (2015) use a simulator to estimate fitness values for low-dimensional robot behaviors which gives the robot prior knowledge of how to adapt its behavior if it becomes damaged during real world operation. Cutler et al. (2015) use experience in simulation to estimate a prior for a Gaussian process model to be used with the PILCO learning algorithm (Deisenroth and Rasmussen 2011). Rusu et al. (2016a; 2016b) introduce progressive neural network policies which are initially trained in simulation before a final period of learning in the true environment. In contrast to these methods, our method performs all learning in a grounded simulator and only uses the physical robot for policy evaluation.</p>
<p>Another class of simulation-transfer methods optimize an objective besides $J_{\text {sim }}$ in simulation. Rajeswaran et al. (2016) use a set of different simulators to learn robust policies that can perform well in a variety of environments. Koos et al. (2010) use multi-objective optimization to find policies that trade off between optimizing $J_{\text {sim }}(\pi)$ and transferability. Iocchi et al. (2007) compute a correction function from $J(\pi)$ such that $J_{\text {sim }}(\pi)=J(\pi)$ and then optimize the corrected objective. In contrast, we directly optimize $J_{\text {sim }}$ within a modified simulator.</p>
<p>Christiano et al. (2016) turn simulation policies into real
world policies by transforming policy actions so that they produce the same effect that they did in simulation. This approach is complementary to ours although it also requires a simulator which can be reset to an arbitrary state during policy execution.</p>
<h2>Grounded Simulation Learning</h2>
<p>In this section we introduce the Grounded Simulation Learning (GSL) framework as presented by Farchy et al. (2013). GSL improves robot learning in simulation by using state transition data from the physical system to modify $E_{\text {sim }}$ such that the modified $E_{\text {sim }}$ is a higher fidelity model of $E$. The process of making the simulator more like the real world is referred to as grounding.</p>
<p>The GSL framework assumes the following:</p>
<ol>
<li>There is an imperfect simulator, $E_{\text {sim }}=\left\langle\mathcal{S}, \mathcal{A}, P_{\text {sim }}, c\right\rangle$, that models the environment of interest, $E$. Furthermore, $E_{\text {sim }}$ must be modifiable. A modifiable simulator has parameterized transition probabilities $P_{\phi}(\cdot \mid s, a):=$ $P_{\text {sim }}(\cdot \mid s, a ; \phi)$ where the vector $\phi$ can be changed to produce, in effect, a different simulator.</li>
<li>Additionally, GSL assumes the physical robot can record a data set, $\mathcal{D}$ of trajectories when executing any policy $\boldsymbol{\theta}$.</li>
<li>Finally, GSL requires a policy improvement routine, optimize, that searches for $\boldsymbol{\theta}$ which decreases $J_{\text {sim }}(\boldsymbol{\theta})$. The optimize routine returns a set of candidate policies, $\Pi_{c}$ to evaluate on the physical robot.
Let $d(p, q)$ be a measure of similarity between probabilities $p$ and $q$. GSL grounds $E_{\text {sim }}$ by finding $\phi^{*}$ such that:</li>
</ol>
<p>$$
\phi^{*}=\underset{\phi}{\operatorname{argmin}} \sum_{\tau \in \mathcal{D}} d\left(\operatorname{Pr}(\tau \mid \boldsymbol{\theta}), \operatorname{Pr}_{\text {sim }}(\tau \mid \boldsymbol{\theta}, \boldsymbol{\phi})\right)
$$</p>
<p>where $\operatorname{Pr}(\tau \mid \boldsymbol{\theta})$ is the probability of observing trajectory $\tau$ on the physical robot and $\operatorname{Pr}_{\text {sim }}(\tau \mid \boldsymbol{\theta}, \boldsymbol{\phi})$ is the probability of $\tau$ in the simulator with dynamics parameterized by $\phi$. For instance, if $d(p, q):=\log p-\log q$ for two probabilities $p$ and $q$ then $\phi^{<em>}$ minimizes the Kullback-Leibler divergence between $\operatorname{Pr}(\cdot \mid \boldsymbol{\theta})$ and $\operatorname{Pr}_{\text {sim }}\left(\cdot \mid \boldsymbol{\theta}, \boldsymbol{\phi}^{</em>}\right)$.</p>
<p>Assuming GSL can solve (1), the framework is as follows:</p>
<ol>
<li>Execute policy $\boldsymbol{\theta}_{0}$ on the physical robot to collect a data set of trajectories, $\mathcal{D}$.</li>
<li>Use $\mathcal{D}$ to find $\phi^{*}$ that satisfies Equation 1.</li>
<li>Use optimize with $J_{\text {sim }}$ and $P_{\phi^{*}}$ to learn a set of candidate policies $\Pi_{c}$ in simulation which are expected to perform well on the physical robot.</li>
<li>Evaluate each proposed $\boldsymbol{\theta}<em c="c">{c} \in \Pi</em>}$ on the physical robot and return the policy, $\boldsymbol{\theta<em 1="1">{1}$, with minimal $J$.
GSL can be applied iteratively with $\boldsymbol{\theta}</em>}$ being used to collect more trajectories to ground the simulator again before learning $\boldsymbol{\theta<em 0="0">{2}$. The re-grounding step is necessary since changes to $\boldsymbol{\theta}$ result in changes to the distribution of states that the robot observes. When this happens, a simulator that has been modified with data from the state distribution of $\boldsymbol{\theta}</em>$.}$ may be a poor model under the state distribution of $\boldsymbol{\theta}_{1</li>
</ol>
<p>Farchy et al. proposed a GSL algorithm, which we refer to as GUIDED GSL. GUIDED GSL blends simulator modification with human guided policy optimization. This algorithm demonstrated the efficacy of GSL in optimizing forward walk velocity of a bipedal robot. The robot in that research began learning with a slow but stable walk policy. Four iterations of GUIDED GSL led to an improvement of over $26 \%$ on this baseline walk. Two limitations of GUIDED GSL are that:</p>
<ol>
<li>It makes the assumption that actions in simulation achieve their desired effect instantaneously.</li>
<li>It required a human expert to manually select which components of $\boldsymbol{\theta}$ were allowed to change between iterations of the optimize routine.
In this work, we introduce an enhanced GSL methodology that eliminates both assumptions and optimizes one of the fastest available NAO walk engines.</li>
</ol>
<h2>Robot Platform and Task</h2>
<p>Before presenting GAT, our new GSL algorithm, we describe the robot platform, initial walk policy, and simulators which we use for evaluation. While our method is applicable to other robots, tasks, and simulators we describe these components first in order to ground the algorithm's presentation.</p>
<p>Robot Platform: Our target task is bipedal walking using the SoftBank NAO. ${ }^{1}$ The NAO is a humanoid robot with 25 degrees of freedom (See Figure 2a). For walking, our NAO uses an open source walk engine developed at the University of New South Wales (UNSW) (Ashar et al. 2015; Hall et al. 2016). This walk engine has been used by at least one team in each of the past three RoboCup Standard Platform League (SPL) championship games in which teams of five NAOs compete in soccer matches. To the best of our knowledge, it is one of the fastest open source walks available. ${ }^{2}$ The walk is a zero moment point walk based on an inverted pendulum model. The walk is closed loop, using the inertial measurement (IMU) sensors and a learned ankle control policy to improve stability. The UNSW walk engine has 15 parameters that determine features of the walk (e.g., step height, pendulum model height). The values of the parameters from the open source release constitute $\boldsymbol{\theta}_{0}$. For full information on the UNSW walk see (Hengst 2014).</p>
<p>Simulators: We use two different simulators in this work. The first is the open source SimSpark simulator. ${ }^{3}$ The simulator simulates realistic physics with the Open Dynamics Engine. ${ }^{4}$ This simulator was also used in the work introducing GSL (Farchy et al. 2013). We also use the Gazebo simulator ${ }^{5}$ provided by the Open Source Robotics Foundation. ${ }^{6}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Gazebo is an open source simulator that is commonly used in robotics research. SimSpark enables fast simulation but is a lower fidelity model of the real world. Gazebo enables high fidelity simulation with an additional computational cost. In one of our experiments we consider the more realistic Gazebo environment as $E$ and SimSpark as $E_{\text {sin }}$. The main difference between these simulators and the physical robot is how actions change the robot's configuration. In SimSpark, actions achieve the desired command angle almost instantaneously. On the physical robot there is a delay. Gazebo also models joints as more reactive than the real world although less reactive than in SimSpark.</p>
<h2>Grounded Action Transformation</h2>
<p>We now introduce our principle algorithmic contribution, GSL with Grounded Action Transformation (GAT) which improves the grounding step (step 2) of the GSL framework. GAT improves grounding by correcting differences in the effects of actions between $E$ and $E_{\text {sin }}$. GSL with GAT is presented in Algorithm 1.</p>
<p>The GSL framework grounds the simulator by finding $\phi^{*}$ that satisfies (1). Since it is often easier to minimize error in the one step dynamics distribution than error in the trajectory distributions, GAT uses:</p>
<p>$$
\phi^{*}=\underset{\phi}{\operatorname{argmin}} \sum_{\tau_{i} \in \mathcal{D}} \sum_{t=0}^{L} d\left(P\left(\mathbf{s}<em t="t">{t+1}^{i} \mid \mathbf{s}</em>}^{i}, \mathbf{a<em _phi="\phi">{t}^{i}\right), P</em>}\left(\mathbf{s<em t="t">{t+1}^{i} \mid \mathbf{s}</em>\right)\right)
$$}^{i}, \mathbf{a}_{t}^{i</p>
<p>as a surrogate loss function which can be minimized with transitions observed in $\mathcal{D}$. GSL with GAT begins by collecting the dataset $\mathcal{D}$ (Algorithm 1, line 4).</p>
<p>Physics-based simulators (such as SimSpark and Gazebo) have a large number of parameters determining the physics of the simulated environment (e.g., friction coefficients). However, using these parameters as $\phi$ is not amenable to numerical optimization of (2). To find $\phi^{*}$ efficiently, GAT uses a parameterized action transformation function which takes the agent's state and action as input and outputs a new action which - when taken in simulation - will result in the robot transitioning to the same next state it would have in $E$. We denote this function, $g_{\phi}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{A}$; the parameters of $g$ serve as $\phi$ under the GSL framework. GAT constructs $g$ with parameterized models of the robot's dynamics and the simulator's inverse dynamics. Assuming the simulated robot can record a dataset $\mathcal{D}_{\text {sin }}$ of experience like the physical robot, GAT reduces (2) to a supervised learning problem.</p>
<p>GAT defines $g:=g_{\phi}$ by a deterministic forward model of the robot's dynamics, $f$ and a deterministic model of the simulator's inverse dynamics, $f_{\text {sin }}^{-1}$. The function, $f$ maps $\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)$ to the maximum likelihood estimate of $\mathbf{x<em _sin="{sin" _text="\text">{t+1}$ under $P$. The function $f</em>}}^{-1}$ maps $\left(\mathbf{s<em t_1="t+1">{t}, \mathbf{s}</em>}\right)$ to the action that is most likely to produce this transition in simulation. When executing $\boldsymbol{\theta}$ in simulation, the robot selects $\mathbf{a<em _boldsymbol_theta="\boldsymbol{\theta">{t} \sim \pi</em>}}\left(\cdot \mid \mathbf{s<em t_1="t+1">{t}\right)$ and then uses $f$ to predict what the resulting configuration, $\mathbf{x}</em>}$, would be in $E$. Then $\mathbf{a<em t="t">{t}$ is replaced with $\hat{\mathbf{a}}</em>}:=f_{\text {sin }}^{-1}\left(\mathbf{s<em t="t">{t}, f\left(\mathbf{s}</em>}, \mathbf{a<em t_1="t+1">{t}\right)\right)$. The result is that the robot achieves the exact $\mathbf{x}</em>$}$ it would have on the physical robot. ${ }^{7</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Algorithm 1 Grounded Action Transformation (GAT) Pseudo code. Input: An initial policy, $\boldsymbol{\theta}$, the environment, $E$, a simulator, $E_{\text {sim }}$, smoothing parameter $\alpha$, and a policy improvement method, optimize. The function rolloutN $(\boldsymbol{\theta}, \mathrm{N})$ executes $N$ trajectories with $\boldsymbol{\theta}$ and returns the observed state transition data. The functions trainForwardModel and trainInverseModel estimate models of the forward and inverse dynamics respectively.</p>
<div class="codehilite"><pre><span></span><code><span class="k">function</span><span class="w"> </span><span class="nf">GAT</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">robot</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">RolloutN</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">E</span><span class="p">,</span><span class="w"> </span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="n">N</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">RolloutN</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">E_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="n">N</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="n">f</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">trainForwardModel</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">robot</span><span class="w"> </span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="n">f_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">trainInverseModel</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="nb">alpha</span><span class="w"> </span><span class="n">f_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">}(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="p">))</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-\</span><span class="nb">alpha</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="n">cdot</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">Pi</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">optimize</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">E_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">},</span><span class="w"> </span><span class="n">g</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">argmin</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="w"> </span><span class="o">\</span><span class="n">in</span><span class="w"> </span><span class="o">\</span><span class="n">Pi</span><span class="p">}</span><span class="w"> </span><span class="n">J</span><span class="p">(</span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">})</span><span class="o">\</span><span class="p">)</span>
<span class="k">end</span><span class="w"> </span><span class="k">function</span>
</code></pre></div>

<p>In practice $f$ and $f_{\text {sim }}^{-1}$ are represented with supervised regression models and learned from $\mathcal{D}$ and $\mathcal{D}_{\text {sim }}$ respectively (Algorithm 1 lines 5-6). The approximation of $g$ introduces noise into the robot's motion. To ensure stable motions, GAT uses a smoothing parameter $\alpha$. The action transformation function (Algorithm 1 line 8) is then defined as:</p>
<p>$$
g\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right):=\alpha f_{\text {sim }}^{-1}\left(\mathbf{s<em t="t">{t}, f\left(\mathbf{s}</em>}, \mathbf{a<em t="t">{t}\right)\right)+(1-\alpha) \mathbf{a}</em>
$$</p>
<p>In our experiments, we set $\alpha$ as high as possible subject to the walk remaining stable. Figure 1 illustrates the GAT-modified $E_{\text {sim }}$. GAT then proceeds to improve $\boldsymbol{\theta}$ with optimize within the grounded simulator (lines 8-9).</p>
<h2>GAT Implementation</h2>
<p>In this work, GAT uses two neural networks to approximate $f$ and $f_{\text {sim }}^{-1}$. Each function is a three layer network with 200 hidden units in the first layer and 180 hidden units in the second. During simulator modification, the $f$ network receives $\mathbf{s}<em t="t">{t}$ and $\mathbf{a}</em>}$ as input and the $f_{\text {sim }}^{-1}$ network receives $\mathbf{s<em _sim="{sim" _text="\text">{t}$ and the output of $f$ as input. The final output from $f</em>}}^{-1}$ is the replacement action $\tilde{\mathbf{a}<em t="t">{t}$. While $\mathbf{a}</em>}$ is a vector of desired joint angles, the action input to $f$ and action output of $f_{\text {sim }}^{-1}$ is encoded as a desired change in $\mathbf{x<em t="t">{t}$ which was found to improve prediction. We follow Fu et al. (2015) by having $f$ predict the joint acceleration caused by executing $\mathbf{a}</em>}$ in $\mathbf{s<em t_1="t+1">{t}$ instead of directly predicting $\mathbf{s}</em>}$. The accelerations can then be integrated and added to $\mathbf{s<em _sim="{sim" _text="\text">{t+1}$ to produce the resulting next state. Additionally, $f$ and $f</em>$ regress to the sine and cosine of the target angular accelerations and then pass the}}^{-1</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of the modifiable simulator GAT induces. At each time-step the robot takes an action, $\mathbf{a}<em t="t">{t}$, and passes $\mathbf{a}</em>}$ to a modification function, $g$. The modification function uses a deterministic model of the real robot's dynamics, $f$, to predict the effect of executing $\mathbf{a<em t="t">{t}$ on the physical robot. Then, a model of the simulated robot's inverse dynamics uses the prediction, $\tilde{\mathbf{x}}</em>}$, to predict the action $\tilde{\mathbf{a}<em t="t">{t}$ which will achieve $\tilde{\mathbf{x}}</em>}$ in simulation. Finally, $\tilde{\mathbf{a}<em _sim="{sim" _text="\text">{t}$ is passed to the environment, $E</em>$ and the resulting state transition will be similar to the transition that would have occurred in $E$.
outputs through the $\operatorname{arctan}$ function to produce the final angular acceleration. Passing the network outputs through the $\arctan$ function ensures $f$ and $f_{\text {sim }}^{-1}$ produce valid joint angles. The true state, $s_{t}$, is estimated by concatenating joint configurations $\mathbf{x}}<em t-1="t-1">{t}, \mathbf{x}</em>}, . . \mathbf{x<em t-1="t-1">{t-4}$ and past actions $\mathbf{a}</em>}, \ldots, \mathbf{a<em t="t">{t-4}$. The state estimate $\left\langle\mathbf{x}</em>}, \ldots, \mathbf{x<em t-1="t-1">{t-4}, \mathbf{a}</em>}, \ldots, \mathbf{a<em _sim="{sim" _text="\text">{t-4}\right\rangle$ improves the predictions of $f$ and $f</em>}}^{-1}$ because it implicitly captures the unobserved $\tilde{\mathbf{x}<em t="t">{t}$ and $\tilde{\mathbf{x}}</em>$ state variables. The length of the configuration history was chosen to balance predictive accuracy with keeping the number of inputs to the networks small. Both networks are trained with backpropagation.</p>
<h2>Empirical Results</h2>
<h2>Experimental Set-up</h2>
<p>We evaluate GAT on the task of bipedal robot walking. The walk takes a target forward velocity parameter in the range $[0,1]$. We set this parameter to 0.75 which we found led to the fastest walk that was reliably stable. The robot walks forward towards a target at this velocity. If the robot's angle to the target becomes greater than five degrees it turns back towards the target while continuing to walk forward. In all environments, $J(\boldsymbol{\theta})$ is the negative of the average forward walk velocity while executing $\boldsymbol{\theta}$. On the physical robot a trajectory terminates once the robot has walked four meters or falls. A trajectory generated with $\boldsymbol{\theta}<em t="t">{0}$ lasts $\approx 20.5$ seconds on the robot. In simulation a trajectory terminates after a fixed time interval ( 7.5 seconds in SimSpark and 10 seconds in Gazebo) or when the robot falls. Previous work has shown that when using a model estimated from data it is better to use shorter action trajectories to avoid overfitting to an inaccurate model (Jiang et al. 2015). Even for identical $s</em>$. This observation motivates using shorter trajectory lengths as simulator fidelity decreases.}$ and $a_{t}, s_{t+1}$ in $E_{\text {sim }}$ will most likely be different than $s_{t+1}$ in $E$. This error compounds over the course of a trajectory since state error at $s_{t}$ is propagated forward into $s_{t+1</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The three robotic environments used in this work. The Softbank NAO is our target physical robot. The NAO is simulated in the Gazebo and SimSpark simulators.</p>
<p>Optimizing $\boldsymbol{\theta}$ We use the Covariance Matrix AdaptionEvolutionary Strategy (CMA-ES) algorithm (Hansen 2006) as the optimize routine. CMA-ES is a policy search method which samples a population of candidate $\boldsymbol{\theta}$ from a Gaussian distribution over parameters. The top $k$ parameter vectors are used to update the sampling distribution so that the mean is closer to an optimal policy. We modify $J_{\text {sim }}(\boldsymbol{\theta})$ for the optimization by adding a cost of 15 for any trajectory in which the robot falls. The added penalty encourages CMA-ES to strongly favor stable policies over faster, less stable ones. To clarify terminology, a generation refers to a single update of CMA-ES; an iteration refers to a complete cycle of GAT.</p>
<p>SimSpark to Gazebo: Since a large number of trials are difficult to obtain on a physical robot, we present a study of GAT using Gazebo as a surrogate for the real world. In this setting we evaluate the effectiveness of GAT compared to learning with no grounding and grounding $E_{\text {sim }}$ by injecting noise into the robot's actions. Adding an "envelope" of noise has been used before to minimize simulation bias by preventing the policy improvement algorithm from overfitting to the simulator's dynamics (Jakobi, Husbands, and Harvey 1995). We refer to this baseline as Noise-Envelope. Since GAT with function approximation introduces noise into the robot's actions we wish to verify that GAT offers benefits over such methods. Noise-Envelope adds standard Gaussian noise to the robot's actions within the ungrounded simulator. We also attempted to evaluate GUIDED GSL but preliminary experiments showed that the assumption that actions achieve their desired effect instantaneously did not hold for this setting.</p>
<p>We run 10 trials of each method. For GAT we collect 50 trajectories of robot experience to train $f$ and 50 trajectories of simulated experience to train $f_{\text {sim }}^{-1}$. For each method, we optimize $\boldsymbol{\theta}$ for 10 generations of the CMA-ES algorithm. In each generation, 150 policies are sampled and evaluated in simulation. CMA-ES estimates $J_{\text {sim }}$ with 20 trajectories from each policy. Overall, the CMA-ES optimization requires 30,000 simulated trajectories for each trial.</p>
<p>Table 1 gives the average improvement in stable walk policies for each method and the number of trials in which a method failed to produce a stable improvement. This table only considers policies found after the first generation of CMA-ES. The reason for this is that the policies in the first generation were randomly sampled from an initial search
distribution. We consider learning to begin once CMA-ES has used the $J_{\text {sim }}$ values of the first generation to update the search distribution. Using $J_{\text {sim }}$ to identify $\boldsymbol{\theta}$ which improve $J$ in the first generation is a policy evaluation problem while improvement afterwards is a policy improvement problem.</p>
<p>Simulation to NAO Set-up: We evaluate GAT for transferring policies learned in Simspark or Gazebo to the physical NAO. The data set $\mathcal{D}$ consists of 15 trajectories collected with $\boldsymbol{\theta}_{0}$ on the physical NAO. For each iteration, we optimize $\boldsymbol{\theta}$ for 10 generations of the CMA-ES algorithm. We evaluate the best policy from each generation with 5 trajectories on the physical robot. If the robot falls in any of the 5 trajectories the policy is considered unstable. While the number of evaluations is small, in practice stable policies had small variance in walk velocity and unstable policies fell in almost all trajectories.</p>
<p>One challenge in this setting is the simulators receive robot actions at 50 Hz while the physical NAO receives robot actions at 100 Hz . The discrepancy in action frequency poses a problem for using real world data to modify how joints move in simulation. By skipping every other measurement to get an effectively 50 Hz data trace, we are able to model how the physical robot's joints move at the simulator's frequency.</p>
<h2>Experimental Results</h2>
<p>SimSpark to Gazebo Results: Table 1 shows that GAT maximizes improvement in $J$ while minimizing iterations without improvement. NOISE-ENVELOPE improves upon no grounding in both improvement and number of iterations without improvement. Adding noise to the simulator encourages CMA-ES to propose robust policies which are more likely to be stable. However, GAT further improves over NOISEENVELOPE. This result demonstrates that action transformations are grounding the simulator in a more effective way than simply injecting noise.</p>
<p>Table 1 also shows that on average GAT finds an improved policy within the first few policy updates after grounding. When learning with no grounding finds an improvement it is also usually in an early generation of CMA-ES. As policy improvement progresses, the best policies in each generation begin to overfit to the dynamics of $E_{\text {sim }}$. Without grounding overfitting happens almost immediately. NOISE-ENVELOPE is shown to be more robust to overfitting since any policy it</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">\% Improve</th>
<th style="text-align: center;">Failures</th>
<th style="text-align: center;">Best Gen.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No Ground</td>
<td style="text-align: center;">11.094</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1.33</td>
</tr>
<tr>
<td style="text-align: center;">Noise-Envelope</td>
<td style="text-align: center;">18.93</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6.6</td>
</tr>
<tr>
<td style="text-align: center;">GAT</td>
<td style="text-align: center;">$\mathbf{2 2 . 4 8}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">2.67</td>
</tr>
</tbody>
</table>
<p>Table 1: This table compares Grounded Action Transformation (GAT) with baseline approaches for transferring learning between SimSpark and Gazebo. The first column displays the average maximum improvement found by each method after the first policy update made by CMA-ES. The second column is the number of times a method failed to find a stable walk. The third column gives the average generation of CMA-ES when the best policy was found. No Ground refers to learning done in the unmodified simulator.
proposes achieved good cost in a noisy $E_{\text {sim }}$. The grounding done by GAT is inherently local to the trajectory distribution of $\boldsymbol{\theta}_{0}$. Thus as $\boldsymbol{\theta}$ changes in policy improvement, the action transformation function fails to produce a more realistic simulator. Noise modification methods can mitigate overfitting by emphasizing robust policies although it is also limited in finding as strong of an improvement as GAT.</p>
<p>Simulator to Physical NAO Results: Our final empirical evaluation applies GAT to learning bipedal walks on a physical NAO. Table 2 gives the walk velocity of policies learned in simulation with GAT. The physical robot walks at a velocity of $19.52 \mathrm{~cm} / \mathrm{s}$ with $\boldsymbol{\theta}<em 0="0">{0}$. Two iterations of GAT with SimSpark increased the walk velocity of the NAO to 27.97 $\mathrm{cm} / \mathrm{s}$ - an improvement of $43.27 \%$ compared to $\boldsymbol{\theta}</em>$ GAT with SimSpark and GAT with Gazebo both improved walk velocity by over $30 \%$. This result demonstrates generality of our approach across different simulators.} .{ }^{8</p>
<p>As in the previous experiment, policy improvement with CMA-ES required 30,000 trajectories per iteration to find the 10 policies that were evaluated on the robot. In contrast the total number of trajectories executed on the physical robot is 65 ( 15 trajectories in $\mathcal{D}$ and 5 evaluations per $\boldsymbol{\theta}<em c="c">{c} \in \Pi</em>$ ). This result demonstrates GAT can use sample-intensive simulation learning to optimize real world skills with a low number of trajectories on the physical robot.</p>
<p>Farchy et al. demonstrated the benefits of re-grounding and further optimizing $\boldsymbol{\theta}$. We reground using the 15 trajectories collected with the best policy found by GAT with SimSpark and optimize for a further 10 generations in simulation. The second iteration results in a walk, $\boldsymbol{\theta}<em 0="0">{2}$, which averages 27.97 $\mathrm{cm} / \mathrm{s}$ for a total improvement of $43.27 \%$ over $\boldsymbol{\theta}</em>$.</p>
<p>Finally, we evaluate $\boldsymbol{\theta}_{0}$ with $100 \%$ of maximum velocity requested (i.e., the forward velocity request parameter set to 1.0). The average velocity of this walk across five stable trajectories is $24.3 \mathrm{~cm} / \mathrm{s}$. This result shows that GAT can learn walk policies that outperform one of the best hand coded walks available.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: This table gives the maximum learned velocity and percent improvement for each method starting from $\boldsymbol{\theta}_{0}$ (top row).</p>
<h2>Discussion and Future Work</h2>
<p>Our proposed algorithm, GAT, has some limitations that we discuss here. The decision to learn an action modification function, $g$, makes the assumption that $\forall\left(s_{t}, a_{t}, s_{t+1}\right)$ that could be observed on the physical robot there exists an action $\tilde{a}<em t="t">{t}$ that will produce the same transition when used in place of $a</em>$ can usually be executed with more or less force in simulation to achieve the desired response. However, this assumption is likely to break down under contact dynamics where external forces resist the robot's actions. Other tasks may introduce other forms of simulator bias that GAT is currently limited in handling. An important direction for future work is to characterize the settings where this approach is limited and to identify alternatives.}$. We posit that this assumption is often true since $\tilde{a}_{t</p>
<p>We specify simulator grounding as a supervised learning problem however the distribution of inputs to the learned models, $f$ and $f_{\text {sim }}^{-1}$, during policy execution differ from the training distributions. The distribution change is likely to weaken the quality of action modification under $g$. To see why the change happens consider that $f$ is trained under $\left(s_{t}, a_{t}, s_{t+1}\right)$ sampled from executing $\boldsymbol{\theta}$ on the real robot while $f_{\text {sim }}^{-1}$ is trained on $\left(s_{t}, a_{t}, s_{t+1}\right)$ sampled in simulation. During simulator modification, both models are evaluated on data sampled from the distribution of $\boldsymbol{\theta}$ in the grounded simulator. We have started to explore methods similar to the Dataset Aggregation (DAgger) algorithm (Ross and Bagnell 2012) which collects new data to retrain $f_{\text {sim }}^{-1}$ as the state distribution of $\boldsymbol{\theta}$ in $E_{\text {sim }}$ changes with grounding.</p>
<p>Finally, this paper considers action modification but a complementary approach could consider sensor modification. Sensor modification would add another layer to the modified simulator such that the environment returns a state and the sensor modification function predicts what the robot would observe in that state. The two methods have an interesting interaction since sensor modification changes the action $\boldsymbol{\theta}$ selects while action modification changes the next state which changes the observed sensor values as well.</p>
<h2>Conclusion</h2>
<p>This paper proposed and evaluated the Grounded Action Transformation (GAT) algorithm for grounded simulation learning. Our method led to a $43.27 \%$ improvement in the walk velocity of a state-of-the-art bipedal robot. We conducted an empirical study that analyzed the benefits of GAT compared to a pair of baseline simulation-transfer methods. This experiment demonstrates GAT enhances learning in simulation in comparison to other methods.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Matthew Hausknecht, Patrick MacAlpine, and Garrett Warnell for insightful discussions and the anonymous reviewers for helpful comments. This work has taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (CNS-1330072, CNS-1305287, IIS-1637736, IIS1651089), ONR (21C184-01), and AFOSR (FA9550-14-10087). Josiah Hanna is supported by an NSF Graduate Research Fellowship. Peter Stone serves on the Board of Directors of, Cogitai, Inc. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.</p>
<h2>References</h2>
<p>Ashar, J.; Ashmore, J.; Hall, B.; and Harris, S. e. a. 2015. Robocup spl 2014 champion team paper. In RoboCup 2014: Robot World Cup XVIII, volume 8992 of Lecture Notes in Computer Science. Springer International Publishing. 70-81.
Christiano, P.; Shah, Z.; Mordatch, I.; Schneider, J.; Blackwell, T.; Tobin, J.; Abbeel, P.; and Zaremba, W. 2016. Transfer from simulation to real world through learning deep inverse dynamics model. arXiv preprint arXiv:1610.03518.
Cully, A.; Clune, J.; Tarapore, D.; and Mouret, J.-B. 2015. Robots that can adapt like animals. Nature.
Cutler, M., and How, J. P. 2015. Efficient reinforcement learnng for robots using informative simulated priors. In IEEE International Conference on Robotics and Automation, ICRA.
Cutler, M.; Walsh, T. J.; and How, J. P. 2014. Reinforcement learning with multi-fidelity simulators. In IEEE Conference on Robotics and Automation, ICRA.
Deisenroth, M. P., and Rasmussen, C. E. 2011. Pilco: A model-based and data-efficient approach to policy search. In International Conference on Machine Learning, ICML.
Farchy, A.; Barrett, S.; MacAlpine, P.; and Stone, P. 2013. Humanoid robots learning to walk faster: From the real world to simulation and back. In Twelth International Conference on Autonomous Agents and Multiagent Systems, AAMAS.
Fu, J.; Levine, S.; and Abbeel, P. 2015. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. In IEEE/RSJ International Conference on Intelligent Robots and Systems.
Hall, B.; Harris, S.; Hengst, B.; Liu, R.; Ng, K.; Pagnucco, M.; Pearson, L.; Sammut, C.; and Schmidt, P. 2016. Robocup spl 2015 champion team paper.
Hansen, N. 2006. The CMA evolution strategy: a comparing review. In Lozano, J.; Larranaga, P.; Inza, I.; and Bengoetxea, E., eds., Towards a new evolutionary computation. Advances on estimation of distribution algorithms. Springer. 75-102.
Hengst, B. 2014. runswift walk2014 report robocup standard platform league. Technical report, The University of New South Wales.
Iocchi, L.; Libera, F. D.; and Menegatti, E. 2007. Learning humanoid soccer actions interleaving simulated and real data. In Second Workshop on Humanoid Soccer Robots.</p>
<p>Jakobi, N.; Husbands, P.; and Harvey, I. 1995. Noise and the reality gap: The use of simulation in evolutionary robotics. In European Conference on Artificial Life, 704-720. Springer.
Jiang, N.; Kulesza, A.; Singh, S.; and Lewis, R. 2015. The dependence of effective planning horizon on model accuracy. In International Conference on Autonomous Agents and Multiagent Systems, AAMAS.
Kober, J.; Bagnell, J. A.; and Peters, J. 2013. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research.
Koos, S.; Mouret, J.-B.; and Doncieux, S. 2010. Crossing the reality gap in evolutionary robotics by promoting transferable controllers. In Proceedings of the 12th annual conference on Genetic and evolutionary computation, 119-126. ACM.
Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.; Silver, D.; and Wierstra, D. 2015. Continuous control with deep reinforcement learning. CoRR abs/1509.02971.
Rajeswaran, A.; Ghotra, S.; Levine, S.; and Ravindran, B. 2016. Epopt: Learning robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283.
Ross, S., and Bagnell, J. A. 2012. Agnostic system identification for model-based reinforcement learning. In 29th International Conference on Machine Learning, ICML.
Rusu, A. A.; Rabinowitz, N. C.; Desjardins, G.; Soyer, H.; Kirkpatrick, J.; Kavukcuoglu, K.; Pascanu, R.; and Hadsell, R. 2016a. Progressive neural networks. arXiv preprint arXiv:1606.04671.
Rusu, A. A.; Vecerik, M.; Rothörl, T.; Heess, N.; Pascanu, R.; and Hadsell, R. 2016b. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286.
Schulman, J.; Moritz, P.; Levine, S.; Jordan, M. I.; and Abbeel, P. 2015. High-dimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations.
Sutton, R. S., and Barto, A. G. 1998. Reinforcement Learning: An Introduction. MIT Press.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8} \mathrm{~A}$ video of the learned walk policies is available at https://www.cs.utexas.edu/users/AustinVilla/?p=research/ real_and_sim_walk_learning.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ GAT subsumes GUIDED GSL which makes the additional as-&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
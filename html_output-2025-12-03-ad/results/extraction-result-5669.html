<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5669 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5669</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5669</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-259108959</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.05179v2.pdf" target="_blank">M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5669.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5669.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT_prompting_strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT prompting strategies: Monolingual, EN-Instruct, EN-Translation, Few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of how different prompt formulations (monolingual prompt, English instruction, translating questions into English, and few-shot in-context examples) affect ChatGPT (gpt-3.5-turbo) accuracy on the M3Exam multilingual multiple-choice benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>M3Exam (multilingual multiple-choice exam questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice exam questions drawn from real official exams across 9 languages and three educational levels (primary/middle/high); evaluation metric = accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Default zero-shot monolingual prompt: start with 'The following is a multiple choice question about {subject type}.', include instruction 'Please only give the correct option, without any other details or explanations.', present question and options each on a new line, end with 'Answer:'. Alternative formats tested: (1) EN-Instruct — same prompt but instruction line in English instead of target language; (2) EN-Translation — translate the target-language question into English and present prompt+question in English; (3) Few-shot — append 3 held-out in-context examples (same format with labeled answers) before test sample.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Monolingual (default) vs EN-Instruct vs EN-Translation vs Few-shot (in-context examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-format accuracies (ChatGPT) by language from Table 3 — Monolingual: en 75.98, zh 61.00, it 67.94, pt 62.43, vi 57.18, th 34.09, sw 53.04, af 68.99, jv 37.47. EN-Instruct: en null, zh 60.56, it 69.30, pt 61.42, vi 57.57, th 32.70, sw 49.30, af 70.16, jv 38.27. EN-Translation: en null, zh 57.92, it 62.76, pt 59.62, vi 56.40, th 46.49, sw 48.13, af 70.16, jv 50.94. Few-shot: en 75.46, zh 60.26, it 64.36, pt 62.99, vi 58.64, th 37.41, sw 51.87, af 67.05, jv 33.42.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Comparing Monolingual vs alternatives: EN-Instruct mixed (some languages up, some down); EN-Translation substantially improved Thai (th) and Javanese (jv) relative to Monolingual (th: 34.09 -> 46.49; jv: 37.47 -> 50.94); Few-shot produced negligible changes overall (small increases or decreases depending on language).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example effect sizes observed (ChatGPT): Thai: +12.40 percentage points accuracy with EN-Translation vs Monolingual (34.09 -> 46.49); Javanese: +13.47 percentage points with EN-Translation (37.47 -> 50.94). Few-shot vs Monolingual: changes within ±2 percentage points for several languages (e.g., en −0.52, zh −0.74), but varied directionally across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied by language: EN-Translation improved performance for some low-resource / difficult languages (improved); EN-Instruct produced inconsistent changes (no consistent effect); Few-shot produced no consistent improvement on average (no effect / mixed).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize: (1) Using English instructions (EN-Instruct) does not consistently help because M3Exam contains native-language-specific content not better elicited via English instructions; (2) EN-Translation can help when the model struggles to understand the target language (translation removes language barrier), but can hurt when translation removes culturally-specific cues or information; (3) Few-shot demonstrations do not reliably improve performance because instruction-tuned LLMs are already familiar with exam-style formats and because few-shot effectiveness depends on language complexity, inherent model knowledge, and choice of examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Few-shot prompting produced decreases for some languages (e.g., Chinese and Swahili) rather than improvements; EN-Instruct did not yield consistent gains and in some cases reduced accuracy relative to Monolingual.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5669.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5669.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal_input_formats</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal input handling: single-image vs multi-image vs text-only baselines and constraint decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of how different multimodal input presentations (feeding single image, multiple images, or only text) and decoding constraints affect multimodal LLM performance on M3Exam image-requiring questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLIP-2, InstructBLIP, Fromage, OpenFlamingo, Flan-T5 (text-only), ChatGPT (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>M3Exam multimodal multiple-choice questions (English subset used for multimodal evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice exam questions that require image(s) for correct answering; evaluation reports per-question accuracy for single-image and multi-image questions and overall accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single-image multimodal models (BLIP-2, InstructBLIP): provided only the first image when model supports single-image input. Multi-image-capable models (Fromage, OpenFlamingo): provided multiple images where present. Text-only baselines (Flan-T5, ChatGPT): provided only the text portion of the question (images omitted). Constraint decoding applied for models to force generation of valid multiple-choice options.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single-image input (first image only) vs multi-image input vs text-only (images omitted) with constraint decoding on multimodal outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative and reported numeric bounds: overall multimodal performance was low; authors note BLIP-2 attains less than 50% accuracy on M3Exam multimodal questions (abstract/main text). Flan-T5 (text-only) achieved similar parameter-sized baseline performance and in some cases matched or outperformed multimodal models. Fromage and OpenFlamingo (multi-image models) performed worse than BLIP-2/InstructBLIP despite multi-image capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>BLIP-2 marginally outperformed Flan-T5 in some settings but still fell below 50% accuracy. Fromage/OpenFlamingo (multi-image) performed notably worse than BLIP-2 and InstructBLIP (single-image models given first image). ChatGPT and Flan-T5 (text-only) sometimes matched or exceeded multimodal models when image understanding failed.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Overall, providing images (in the tested multimodal setups) did not produce the expected performance gains; in many cases multimodal models underperformed or only marginally improved relative to text-only baselines (reduced or no effect).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue multimodal models struggle because M3Exam images often require fine-grained visual understanding (axis labels, angle marks, map details) and cross-image reasoning which current models do not reliably perform. Pretraining on multiple images does not guarantee cross-image or fine-detail understanding. When image understanding fails, text-only models that ignore images can match or outperform multimodal models. Constraint decoding and feeding only the first image for single-image models are practical workarounds but limit model access to multi-image information.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Counterintuitively, the text-only Flan-T5 baseline sometimes performed as well as or better than multimodal models by ignoring images (i.e., inclusion of images via current multimodal models did not always help).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5669.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5669.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt_output_constraint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concise-output instruction for automatic evaluation ('Please only give the correct option...')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of an explicit instruction in prompts to force LLMs to output only the option letter/value to enable automatic, consistent evaluation of multiple-choice answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, ChatGPT, Claude, Vicuna, BLOOM, and multimodal models in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>M3Exam multiple-choice evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice answer generation constrained to a single option token/line to allow parsing and accuracy calculation by taking the first alphabetic letter of model output.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts include a strict instruction 'Please only give the correct option, without any other details or explanations.' followed by the question and options and 'Answer:'; for multimodal experiments, format constraint omitted in prompt and constraint decoding enforced externally for valid option generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that in most cases models adhere to the instruction and produce only the option, enabling automatic scoring by extracting the first alphabetic letter of the output. No direct accuracy delta from this constraint is reported, but it is used as the evaluation standard.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>enables reliable automatic evaluation (no direct effect on predictive accuracy reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Constraint improves evaluation robustness: instruction-tuned LLMs readily follow format constraints, so enforcing a concise answer reduces output parsing errors and invalid responses (e.g., BLOOM sometimes generated invalid options when unconstrained). For multimodal models, constraint decoding was necessary because prompt-based instruction omission alone was insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some open-source models (e.g., BLOOM) still produced invalid outputs despite the format; thus instruction alone does not guarantee valid option-only outputs for all models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models <em>(Rating: 2)</em></li>
                <li>Instructblip: Towards general-purpose vision-language models with instruction tuning <em>(Rating: 2)</em></li>
                <li>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity <em>(Rating: 1)</em></li>
                <li>Making the V in VQA matter: Elevating the role of image understanding in visual question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5669",
    "paper_id": "paper-259108959",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "ChatGPT_prompting_strategies",
            "name_full": "ChatGPT prompting strategies: Monolingual, EN-Instruct, EN-Translation, Few-shot",
            "brief_description": "Evaluation of how different prompt formulations (monolingual prompt, English instruction, translating questions into English, and few-shot in-context examples) affect ChatGPT (gpt-3.5-turbo) accuracy on the M3Exam multilingual multiple-choice benchmark.",
            "citation_title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_size": null,
            "task_name": "M3Exam (multilingual multiple-choice exam questions)",
            "task_description": "Multiple-choice exam questions drawn from real official exams across 9 languages and three educational levels (primary/middle/high); evaluation metric = accuracy.",
            "problem_format": "Default zero-shot monolingual prompt: start with 'The following is a multiple choice question about {subject type}.', include instruction 'Please only give the correct option, without any other details or explanations.', present question and options each on a new line, end with 'Answer:'. Alternative formats tested: (1) EN-Instruct — same prompt but instruction line in English instead of target language; (2) EN-Translation — translate the target-language question into English and present prompt+question in English; (3) Few-shot — append 3 held-out in-context examples (same format with labeled answers) before test sample.",
            "comparison_format": "Monolingual (default) vs EN-Instruct vs EN-Translation vs Few-shot (in-context examples).",
            "performance": "Per-format accuracies (ChatGPT) by language from Table 3 — Monolingual: en 75.98, zh 61.00, it 67.94, pt 62.43, vi 57.18, th 34.09, sw 53.04, af 68.99, jv 37.47. EN-Instruct: en null, zh 60.56, it 69.30, pt 61.42, vi 57.57, th 32.70, sw 49.30, af 70.16, jv 38.27. EN-Translation: en null, zh 57.92, it 62.76, pt 59.62, vi 56.40, th 46.49, sw 48.13, af 70.16, jv 50.94. Few-shot: en 75.46, zh 60.26, it 64.36, pt 62.99, vi 58.64, th 37.41, sw 51.87, af 67.05, jv 33.42.",
            "performance_comparison": "Comparing Monolingual vs alternatives: EN-Instruct mixed (some languages up, some down); EN-Translation substantially improved Thai (th) and Javanese (jv) relative to Monolingual (th: 34.09 -&gt; 46.49; jv: 37.47 -&gt; 50.94); Few-shot produced negligible changes overall (small increases or decreases depending on language).",
            "format_effect_size": "Example effect sizes observed (ChatGPT): Thai: +12.40 percentage points accuracy with EN-Translation vs Monolingual (34.09 -&gt; 46.49); Javanese: +13.47 percentage points with EN-Translation (37.47 -&gt; 50.94). Few-shot vs Monolingual: changes within ±2 percentage points for several languages (e.g., en −0.52, zh −0.74), but varied directionally across languages.",
            "format_effect_direction": "varied by language: EN-Translation improved performance for some low-resource / difficult languages (improved); EN-Instruct produced inconsistent changes (no consistent effect); Few-shot produced no consistent improvement on average (no effect / mixed).",
            "explanation_or_hypothesis": "Authors hypothesize: (1) Using English instructions (EN-Instruct) does not consistently help because M3Exam contains native-language-specific content not better elicited via English instructions; (2) EN-Translation can help when the model struggles to understand the target language (translation removes language barrier), but can hurt when translation removes culturally-specific cues or information; (3) Few-shot demonstrations do not reliably improve performance because instruction-tuned LLMs are already familiar with exam-style formats and because few-shot effectiveness depends on language complexity, inherent model knowledge, and choice of examples.",
            "counterexample_or_null_result": "Few-shot prompting produced decreases for some languages (e.g., Chinese and Swahili) rather than improvements; EN-Instruct did not yield consistent gains and in some cases reduced accuracy relative to Monolingual.",
            "uuid": "e5669.0",
            "source_info": {
                "paper_title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Multimodal_input_formats",
            "name_full": "Multimodal input handling: single-image vs multi-image vs text-only baselines and constraint decoding",
            "brief_description": "Evaluation of how different multimodal input presentations (feeding single image, multiple images, or only text) and decoding constraints affect multimodal LLM performance on M3Exam image-requiring questions.",
            "citation_title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
            "mention_or_use": "use",
            "model_name": "BLIP-2, InstructBLIP, Fromage, OpenFlamingo, Flan-T5 (text-only), ChatGPT (text-only)",
            "model_size": null,
            "task_name": "M3Exam multimodal multiple-choice questions (English subset used for multimodal evaluation)",
            "task_description": "Multiple-choice exam questions that require image(s) for correct answering; evaluation reports per-question accuracy for single-image and multi-image questions and overall accuracy.",
            "problem_format": "Single-image multimodal models (BLIP-2, InstructBLIP): provided only the first image when model supports single-image input. Multi-image-capable models (Fromage, OpenFlamingo): provided multiple images where present. Text-only baselines (Flan-T5, ChatGPT): provided only the text portion of the question (images omitted). Constraint decoding applied for models to force generation of valid multiple-choice options.",
            "comparison_format": "Single-image input (first image only) vs multi-image input vs text-only (images omitted) with constraint decoding on multimodal outputs.",
            "performance": "Qualitative and reported numeric bounds: overall multimodal performance was low; authors note BLIP-2 attains less than 50% accuracy on M3Exam multimodal questions (abstract/main text). Flan-T5 (text-only) achieved similar parameter-sized baseline performance and in some cases matched or outperformed multimodal models. Fromage and OpenFlamingo (multi-image models) performed worse than BLIP-2/InstructBLIP despite multi-image capability.",
            "performance_comparison": "BLIP-2 marginally outperformed Flan-T5 in some settings but still fell below 50% accuracy. Fromage/OpenFlamingo (multi-image) performed notably worse than BLIP-2 and InstructBLIP (single-image models given first image). ChatGPT and Flan-T5 (text-only) sometimes matched or exceeded multimodal models when image understanding failed.",
            "format_effect_size": null,
            "format_effect_direction": "Overall, providing images (in the tested multimodal setups) did not produce the expected performance gains; in many cases multimodal models underperformed or only marginally improved relative to text-only baselines (reduced or no effect).",
            "explanation_or_hypothesis": "Authors argue multimodal models struggle because M3Exam images often require fine-grained visual understanding (axis labels, angle marks, map details) and cross-image reasoning which current models do not reliably perform. Pretraining on multiple images does not guarantee cross-image or fine-detail understanding. When image understanding fails, text-only models that ignore images can match or outperform multimodal models. Constraint decoding and feeding only the first image for single-image models are practical workarounds but limit model access to multi-image information.",
            "counterexample_or_null_result": "Counterintuitively, the text-only Flan-T5 baseline sometimes performed as well as or better than multimodal models by ignoring images (i.e., inclusion of images via current multimodal models did not always help).",
            "uuid": "e5669.1",
            "source_info": {
                "paper_title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Prompt_output_constraint",
            "name_full": "Concise-output instruction for automatic evaluation ('Please only give the correct option...')",
            "brief_description": "Use of an explicit instruction in prompts to force LLMs to output only the option letter/value to enable automatic, consistent evaluation of multiple-choice answers.",
            "citation_title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4, ChatGPT, Claude, Vicuna, BLOOM, and multimodal models in experiments",
            "model_size": null,
            "task_name": "M3Exam multiple-choice evaluation",
            "task_description": "Multiple-choice answer generation constrained to a single option token/line to allow parsing and accuracy calculation by taking the first alphabetic letter of model output.",
            "problem_format": "Prompts include a strict instruction 'Please only give the correct option, without any other details or explanations.' followed by the question and options and 'Answer:'; for multimodal experiments, format constraint omitted in prompt and constraint decoding enforced externally for valid option generation.",
            "comparison_format": null,
            "performance": "Authors report that in most cases models adhere to the instruction and produce only the option, enabling automatic scoring by extracting the first alphabetic letter of the output. No direct accuracy delta from this constraint is reported, but it is used as the evaluation standard.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "enables reliable automatic evaluation (no direct effect on predictive accuracy reported).",
            "explanation_or_hypothesis": "Constraint improves evaluation robustness: instruction-tuned LLMs readily follow format constraints, so enforcing a concise answer reduces output parsing errors and invalid responses (e.g., BLOOM sometimes generated invalid options when unconstrained). For multimodal models, constraint decoding was necessary because prompt-based instruction omission alone was insufficient.",
            "counterexample_or_null_result": "Some open-source models (e.g., BLOOM) still produced invalid outputs despite the format; thus instruction alone does not guarantee valid option-only outputs for all models.",
            "uuid": "e5669.2",
            "source_info": {
                "paper_title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "rating": 2,
            "sanitized_title": "blip2_bootstrapping_languageimage_pretraining_with_frozen_image_encoders_and_large_language_models"
        },
        {
            "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "rating": 2,
            "sanitized_title": "instructblip_towards_generalpurpose_visionlanguage_models_with_instruction_tuning"
        },
        {
            "paper_title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "rating": 1,
            "sanitized_title": "a_multitask_multilingual_multimodal_evaluation_of_chatgpt_on_reasoning_hallucination_and_interactivity"
        },
        {
            "paper_title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
            "rating": 1,
            "sanitized_title": "making_the_v_in_vqa_matter_elevating_the_role_of_image_understanding_in_visual_question_answering"
        }
    ],
    "cost": 0.014637999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models
10 Nov 2023</p>
<p>Wenxuan Zhang 
DAMO Academy
Alibaba Group</p>
<p>Sharifah Mahani Aljunied 
DAMO Academy
Alibaba Group</p>
<p>Chang Gao gaochang.gao@alibaba-inc.com 
DAMO Academy
Alibaba Group</p>
<p>The Chinese University of Hong Kong</p>
<p>YewKen Chia yewken.chia@alibaba-inc.com 
DAMO Academy
Alibaba Group</p>
<p>Singapore University of Technology</p>
<p>Bing Lidong 
DAMO Academy
Alibaba Group</p>
<p>M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models
10 Nov 202394A2253F543CA0AF01B436599D851AC3arXiv:2306.05179v2[cs.CL]
Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills.To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context.M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels.In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23% of the questions require processing images for successful solving.We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages.Multimodal LLMs also perform poorly with complex multimodal questions.We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development.Data and evaluation code is available at https://github.com/DAMO-NLP-SG/M3Exam. * Chang Gao is a research intern at Alibaba.Yew Ken Chia is under the Joint Ph.D. Program between Alibaba and SUTD.37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.</p>
<p>Introduction</p>
<p>In recent years, large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing (NLP) tasks [9,31,3,4].For instance, ChatGPT [30] shows an impressive ability to effectively respond to a wide range of questions and provide high-quality answers [7].Their applications even extend beyond traditional NLP domains, as they have been integrated to address real-world challenges in diverse areas [22,27,40].Given the increasing reliance on LLMs, the need for appropriate and comprehensive evaluations has become ever more critical.Such assessments should not only examine whether the models exhibit strong language understanding, but also evaluate their capabilities to handle complex problems requiring different kinds of skills [31].</p>
<p>Typically, NLP models are evaluated using well-designed benchmarks for specific tasks, such as SQuAD [34] for question-answering or WMT [8] for machine translation.Although useful, these taskspecific benchmarks often emphasize on certain aspects, and thus do not adequately assess the breadth of abilities possessed by powerful modern LLMs.In comparison, tackling human exam questions often require diverse skills such as language understanding, complex reasoning, etc.Therefore, a new trend has emerged to utilize tests originally designed for humans to assess the performance of LLMs.For example, MMLU [17] contains exam questions covering 57 tasks across a diverse set of subjects for assessing models.AGIEval [43] collects questions from standardized exams such as law school admission tests and math competitions.GPT-4 [31] also uses a variety of human exams to test its ability in complex scenarios.These human-centric evaluations, approximating real-world applications and expectations, have been demonstrated to be valuable testbeds for gauging the artificial general intelligence (AGI) capabilities of LLMs.</p>
<p>Despite the advantages of evaluations based on human exams, current benchmarks exhibit several key limitations.Firstly, the majority of these benchmarks focus on questions in English [17,43], neglecting the evaluation of a model's performance in a multilingual context.As many LLMs exhibit multilingual ability and are widely used across different countries and languages [30,3,4], it is important to evaluate their multilingual capabilities.Moreover, some existing multilingual benchmarks of traditional NLP tasks [14,33,31] that were created by translating original English datasets have been found to introduce an English-centric bias.This bias arises because the translation process, while making the benchmarks available in multiple languages, does not always capture culturally specific or unique concepts present in the target languages.This shows the importance of sourcing real data from various languages to represent their native cultural background [28].Secondly, most benchmarks consider solely text-based questions, ignoring a significant portion of real-world exam questions include images.Considering this type of question is essential to test a model's multimodal understanding abilities in a wide range of practical applications [15].Lastly, existing exam-type benchmarks usually draw from mixed exams such as college final exams or professional certificate tests [17,43], the constructed resources are thus also comprised of questions from mixed levels.Gathering exam questions from varying educational levels is critical to assess and understand the level of intelligence that LLMs have developed.[10].</p>
<p>In this paper, we present M3Exam, a novel benchmark dataset designed for evaluating the artificial general intelligence of large language models.M3Exam has several unique characteristics: (1) Multilingualism -by gathering questions from official exams across multiple countries, the benchmark contains natural multilingual questions in different languages, which retain the social-cultural diversity of knowledge that may be essential for problem-solving; (2) Multimodality -we incorporate all types of questions including those that require images and carefully process these images to facilitate convenient model evaluation.We show that a significant proportion (approximately 23%) of questions demand information from images for solving; (3) Multilevel structure -we adopt a top-down approach for data collection where we first select three critical educational periods (primary, middle, and high school) and source official exams from the culmination of each period, resulting in a benchmark with varying levels.In total, M3Exam comprises 12,317 questions in 9 diverse languages, with 2,816 questions involving one or more images.Each question includes the question text, candidate answer options, ground-truth answer, and rich meta-information consisting of language, education level, subject, and whether images are involved.Some examples are shown in Figure 1.</p>
<p>We utilize a wide range of top-performing LLMs in both multilingual and multimodal settings to assess their performance on the newly introduced M3Exam dataset.Our findings indicate that the majority of existing models have difficulties in processing multilingual text, with GPT-4 [31] being the only model to achieve over 60% accuracy.Nevertheless, it still faces challenges with low-resource languages such as Javanese, and non-Latin script languages like Thai.Current multimodal models also underperform on M3Exam, with state-of-the-art models such as BLIP-2 [26] attaining less than 50% accuracy.A detailed examination further reveals that comprehending complex images and reasoning across images remain quite challenging for current models.Moreover, we surprisingly find that LLMs' performances do not show a monotonic decrease with the educational level increases which is quite different from human behavior, implying that the development of intelligence in LLMs may not necessarily align with that of human intelligence.Overall, we believe M3Exam can serve as a valuable resource for examining LLMs, both tracking their improvements in terms of multilingual and multimodal settings and providing insights into the development of model intelligence with different education levels.</p>
<p>2 M3Exam Benchmark Dataset</p>
<p>Design Principle</p>
<p>Exams are widely used to assess human intelligence at various educational stages, as they draw on the integration of diverse skills, including language understanding, world knowledge, cultural awareness, and logical reasoning, etc.Consequently, exam questions offer an ideal testbed for evaluating the general intelligence of LLMs.We propose three crucial design principles for constructing the M3Exam benchmark dataset with the exam questions:</p>
<p>• Multilingual Evaluation: While most existing datasets primarily focus on English, assessing LLMs' abilities in multiple languages with different cultural backgrounds, especially those low-resource languages, is crucial to apply LLMs in broad-range scenarios.To achieve this, collecting real-world natural data of different languages instead of translating from English data is of great importance, as culture and world knowledge are deeply rooted in authentic data [28].</p>
<p>• Multimodal Evaluation: In real-world scenarios, humans often encounter problems with different modalities such as images or audio.Multimodal evaluation is thus essential for testing an LLM's ability to jointly process information from multiple modalities, which reflects a key part of cognition capability.Therefore, to facilitate such evaluation, we include questions requiring images for successful solving.</p>
<p>• Multilevel Evaluation: Although education systems vary across countries, they typically organize learning into several stages (e.g., from primary school to middle school and then to high school), with examinations to assess students' readiness to advance to the next stage.The exams at the end of each period effectively reveal the general intelligence expectations within each country.Thus, evaluating LLMs with questions from these critical educational stages offers a comprehensive assessment of their capacity with different levels of intelligence requirements.</p>
<p>Language Selection and Data Collection</p>
<p>Following the design principle outlined above, we adopt a top-down approach to construct our dataset.</p>
<p>To comprehensively evaluate the model, we select 9 languages including English (from the US), Chinese (from China), Italian (from Italy), Portuguese (from Brazil), Vietnamese (from Vietnam), Thai (from Thailand), Swahili (from Kenya), Afrikaans (from South Africa), and Javanese (from Indonesia).This selection is mainly driven by language and cultural diversity, with the aim of covering different language families, languages with varying levels of resources, written scripts, and their major spoken countries.</p>
<p>We then engage native speakers from each of the selected countries to collect official exam papers along with their answers at the end of each educational level, which are typically the graduation exams of primary school, middle school, and high school.We encourage them to 1) choose exams with the largest possible participation (e.g., if a period has two exams, one nationwide and one statewide, the nationwide exam should be collected); 2) collect all available subjects and up to five papers across different years for each subject to ensure a diverse range of questions.In the end, we collect in total 435 exam papers from nine countries.The details of those exam papers are in Appendix A.4.</p>
<p>Data Processing and Annotation</p>
<p>Given the diverse languages we consider, many collected exam papers are only available as images or in scanned versions.Therefore, we first conduct OCR to convert these papers into editable text versions.The original papers, editable text versions, and corresponding answer sets are then passed on to annotators of each specific language, who transform the data into a unified format.In terms of question scope, we focus on multiple-choice questions, as they allow for a standard automatic evaluation of the correctness of model outputs.We exclude subjective questions with free-response answers but include questions that can be easily adapted into the multiple-choice format, such as judging true-false statements.</p>
<p>Specifically, the annotators are asked to check the text content and fix potential errors due to OCR transformations.Then they need to separate the question text from a list of candidate answer options, and input the correct answer.We also address a limitation observed in previous benchmarks: inadequate or limited context information.Many questions require rich contextual information to answer, such as reading comprehension questions with passages or chemistry problems featuring brief introductions to new chemical phenomena.We specifically ask annotators to include such contextual background information.Furthermore, we also convert special formats into pure text, such as converting all equations into LaTeX format or using <br> to include a text span for representing a bold font.All these format adaptations aim to make the constructed benchmark mimic the real exam scenario.Multiple rounds of quality checks are conducted to ensure the data quality.</p>
<p>Data Statistics</p>
<p>At the end of the annotation and quality check, our newly introduced M3Exam dataset contains a total of 12,317 questions.Each question includes context information, the main question text, candidate options, the correct answer, and meta information, such as its language, level, subject, and whether images are needed to solve the question.Figure 1 shows examples of some questions.</p>
<p>Table 1 provides detailed statistics of M3Exam, broken down by language and level.The number of questions that involve only pure text, or require images are separately listed.We rank languages by their ratio in the CommonCrawl corpus, which is a widely-used data source for training LLMs.It can be observed that our selected languages span a wide range, from high-resource languages like English and Chinese, to extremely low-resource languages such as Javanese.Therefore, the diversity of selected languages makes it well-suited for comprehensively assessing the multilingual capabilities of the model.The ratio of questions requiring images also varies across countries, from over 60% questions with images for Chinese, to languages with very few image-type questions.After obtaining the data, we group the questions for each language into four subject categories, namely language, math, social science, and natural science.We then randomly select three questions for each subject category of each level in each language and separate these as held-out development data, which can be used as in-context examples.The remaining questions are used as test data during the experiment.</p>
<p>3 Experiment Setups</p>
<p>Models</p>
<p>To evaluate the performance of various LLMs on our newly introduced M3Exam dataset, we select a range of top-performing models in either multilingual or multimodal settings.</p>
<p>Text-only LLMs To process multilingual texts, we first take ChatGPT (gpt-3.5-turbo)[30] and GPT-4 (gpt-4) [31] from OpenAI, both of which have demonstrated strong multilingual abilities in preliminary studies [2,7,24,31].Additionally, we also adopt Claude (Claude-instant) from Anthropic, a model which is often considered to be comparable to ChatGPT [19].We obtain the results of those close-source models via API call with the corresponding model type.Furthermore, we utilize two open-source models, namely BLOOM (176B) [36] and Vicuna (13B) [12].BLOOM stands out as one of the largest open-source LLMs specializing in multilingual ability, having been trained with data encompassing 46 languages and 13 programming languages.Vicuna, on the other hand, was developed by fine-tuning the LLaMA model [37] on user-shared conversations.Although not specifically designed as a multilingual model, recent leaderboards have identified Vicuna as the top-performing open-source model on both English-only and non-English leaderboards [29].</p>
<p>Multimodal LLMs To evaluate LLMs on multimodal questions, we consider a range of state-ofthe-art open-source models since closed-source models such as GPT-4 do not have official multimodal versions available currently.Specifically, we employ BLIP-2 [26] and InstructBLIP [15], which have demonstrated leading performance in various multimodal question-answering tasks.However, these models are limited to processing a single image per question.Since our M3Exam data may contain multiple images in the background description or as answer options, we additionally utilize Fromage [23] and OpenFlamingo [6], both of which are capable of handling multi-image inputs.We use their pre-trained model weights to directly conduct inference on our test data, and further impose a constraint decoding to generate only valid multiple-choice options for those models.</p>
<p>Settings</p>
<p>The following is a multiple choice question about Social Science.Please only give the correct option, without any other details or explanations.</p>
<p>What is a 100-year period of time called?</p>
<p>Zero-shot Evaluation</p>
<p>We primarily evaluate various LLMs in zero-shot settings.There are three considerations for this setup decision: First, using a zero-shot approach to prompt the model mimics the natural process in real-world applications and the problem-solving process in exams.Secondly, most LLMs have limited context lengths (especially for multilingual models handling diverse languages) or cannot accept multiple images as input (for multimodal models), rendering them unsuitable for evaluation using multiple few-shot demonstrations.Third, since the majority of existing LLMs have undergone instruction tuning [32], they are readily capable of following instructions to output in the desired format.Nonetheless, we also compare zero-shot and few-shot settings from an empirical standpoint with ChatGPT in Section 4.1.</p>
<p>Prompt Following the convention of previous studies [17,31], we clearly specify the subject type of each question by starting with "The following is a multiple choice question about {subject type}.".Subsequently, we include an instruction "Please only give the correct option, without any other details or explanations."to constrain the model output for automatic evaluations.A question is then presented, along with its corresponding options each in a new line.Finally, the prompt ends with "Answer:" for the model to generate its output.It is important to note that all prompts are language-specific [31].We translate the prompt for each language to ensure that the entire prompt presented to the model is monolingual.This prompt design is the same in both multilingual and multimodal settings, except we omit the format constraint for multimodal experiments as constraint decoding is applied.Two example prompts are shown in Figure 2. Detailed prompts as well as examples of different prompting strategies are provided in Appendix A.2.</p>
<p>Evaluations As all the questions are multiple-choice questions, we utilize accuracy as the evaluation metric.In most cases, the models can adhere to the instructions and produce only the option.Consequently, we take the first alphabetic letter of the model's output as the prediction and compare it with the ground truth answer to calculate the accuracy scores.</p>
<p>4 Results and Discussions</p>
<p>Multilingual Evaluation</p>
<p>Main multilingual results</p>
<p>We present the results of various LLMs on different language data in Table 2.We also show the scores of random guesses ("random") and the conventional scores that are considered as passing the exam ("passing"). 2Overall, we observe that most models can only achieve less than 60% accuracy, with GPT-4 being a notable exception, achieving 72.92% and consistently outperforming all other models across different languages.BLOOM, although a multilingual model, gives unsatisfactory performance and is even worse than the random guess since it may generate invalid options.ChatGPT, Claude, and Vicuna show varying degrees of performance depending on the language.Vicuna, despite having a much smaller model size, gives reasonable performance for Latin-script languages.While ChatGPT and Claude have relatively similar performance in English (75.98% v.s.74.25%), ChatGPT demonstrates better results in other languages, suggesting a more robust multilingual ability.When comparing performance across different languages, we observe that existing models generally perform worse for non-Latin languages, such as Chinese (despite being relatively high-resource), as well as low-resource languages like Javanese (even though it mostly uses the Latin script).In summary, the results on our newly introduced M3Exam dataset highlight the challenges and limitations faced by current LLMs in handling non-Latin and low-resource languages, suggesting that there is still a large room for improvement in their multilingual capabilities.</p>
<p>Handling non-English questions with different prompting strategies In multilingual settings, some pilot studies have discovered that using English task instructions [24] or employing a translatetest approach (i.e., translating target language data to English) [2] can lead to improved performance compared to using monolingual prompts in a specific language.To analyze the impact of different prompting strategies, we follow such two settings to create another two types of prompts for ChatGPT, denoted as "EN-Instruct" and "EN-Translation" 3 , respectively.Detailed examples of these two types of prompts are given in Figure 6 in the Appendix, and the results are presented in Table 3, where we also show the performance of the original prompt ("Monolingual").We can note that using English instructions does not consistently improve performance, potentially because our data originates from actual language data rather than merely translated English data.Consequently, using English prompts may not better elicit the knowledge required to solve the questions.The impact of using translated data ("EN-Translation") varies across different languages.On one hand, many questions are closely tied to each specific language, translated data may lose essential information in such cases, leading to poorer performance.On the other hand, translations could eliminate some barriers to understanding particular languages, especially those that the ChatGPT model struggles with, such as Thai and Javanese.Therefore, using the English translations of the questions greatly improves their performance.</p>
<p>Zero-shot v.s.few-shot setting To empirically investigate the impact of few-shot demonstrations, we run experiments on both zero-shot and few-shot settings with ChatGPT.Specifically, we use the held-out few-shot samples for each language, and append the few-shot samples after the instruction but before the final testing sample (see Figure 6 for detailed examples).The format for the few-shot samples is the same as the final test sample, except that the correct option is given after "Answer:" for those samples.We present the results of few-shot samples in Table 3, denoted as "Few-shot".It can be noticed that introducing few-shot examples does not necessarily lead to performance improvement on average.While for languages such as Portuguese and Vietnamese, prompting with few-shot examples result in an improvement, the model's performance in other languages such as Chinese and Swahili slightly decreases with few-shot demonstrations.The reason might be that existing LLMs are already familiar with the question format of human exams.Thus using in-context demonstrations does not provide any additional advantages.Moreover, the effectiveness of few-shot learning depends on many factors such as language complexity, the model's knowledge, the selection of few-shot examples, etc.</p>
<p>Multimodal Evaluation</p>
<p>Table 4: Results on questions with images.We report the performance on both questions with a single image ("Single"), multiple images ("Multi"), as well as the overall scores ("Overall").In Table 4, we present the performance of various models on English questions, as there are no existing LLMs handling both multilingual and multimodal settings.In addition to multimodal models, we provide random guess baselines, the performance of the Flan-T5 model (XXL version) [13], and the performance of ChatGPT.Although Flan-T5 is a text-only model, it has a similar parameter size to the selected multimodal models and serves as the text encoder for both BLIP-2 and InstructBLIP, making it a suitable comparison baseline.Similarly, we also use ChatGPT to understand the extent to which it can perform using only text-based inputs.We only feed the text part for each question to these two text-only models.For BLIP-2 and InstructBLIP models, we only take the first image as the input as they can only process a single image.</p>
<p>Model</p>
<p>The following are multiple-choice questions about natural-science.</p>
<p>The diagram represents a light ray reflecting from a plane mirror.The angle of reflection for the light ray is ( )  We observe that most models do not yield satisfactory performance in general.When compared to Flan-T5, only the BLIP-2 model marginally surpasses its performance.This outcome is unexpected, as Flan-T5 can only process text as input and ignore the images, which intuitively suggests that it may lose crucial information.Upon closer examination, we discover that all existing multimodal models struggle to comprehend complex image details in exam questions (e.g., axis details in math questions, map details in geography questions), which are vital for various subjects.We present an example question and the corresponding outputs from different models in Figure 3.To further assess the extent to which models understand the image used in this question, we construct a new prompt: "What does the image show?Describe all details, including figures, texts, and numbers.Answer:" to gauge the models' behavior.As demonstrated in the right portion of the figure, only BLIP-2 captures relatively more accurate information about the image.However, none of the models can accurately discern details such as the marked angle 65 • , making it impossible for them to solve this question.
(A) 25°(
For questions involving multiple images, the difficulty increases as cross-image reasoning becomes necessary.However, Fromage and OpenFlamingo, models specifically designed for handling multiple images, do not demonstrate clear improvements.Instead, they perform notably worse than BLIP-2 and InstructBLIP, which are only capable of handling single images.We find that they often struggle to comprehend even individual image details (as shown in the example in Figure 3).This finding suggests that pre-training on multiple images does not necessarily guarantee better multimodal understanding abilities.Overall, in comparison to existing multimodal datasets consisting of relatively simple visual question-answering tasks [5,16], our M3Exam dataset presents a significant challenge to understanding image details and reasoning under cross-image and cross-modal settings.We provide more examples and discussions in Appendix A.3.One advantage of the M3Exam dataset is that it encompasses questions from three critical educational periods, namely low, mid, and high, which represent varying levels of difficulty.We here examine LLMs on questions from these three levels.The results are summarized in Figure 4. Comparing the performance on three different levels, the high level indeed generally has the lowest performance, showing its difficulty.Surprisingly, for almost all LLMs, whether text-only or multimodal models, there is no clear decreasing trend as the level increases.This observation contrasts with conventional human behaviors.For example, a high-school student who can achieve reasonable scores in graduation exams should achieve much better results in exams of lower-level schools.Consequently, we expect that human performance will exhibit a monotonic decrease as the level increases.</p>
<p>Multilevel Evaluation</p>
<p>This result suggests that although LLMs show impressive results on many tasks and are even said to spark artificial general intelligence [10], the emergence and development of intelligence in LLMs have significant differences from that of human intelligence and require further investigations.This is also reasonable since the "learning process" of LLMs is different from humans.They are typically trained on massive data first, making their knowledge heavily biased towards the data that are more common, while humans often learn from easy principles and knowledge to more complex reasoning and thinking skills.Moreover, this finding indicates that creating more challenging datasets might not be efficient for improving the models [42,25,21].Instead, it might be more crucial to investigate the underlying reasons for LLM failures, even at primary school-level questions, and devise strategies to address these shortcomings.Performance across various subjects In an effort to better understand the proficiency of models across different subject types, we evaluated ChatGPT's performance in four languages with diverse levels of resources, including English (en), Chinese (zh), Vietnamese (vi), and Thai (th).The results, as displayed in Figure 5, reveal some intriguing patterns.Notably, across all languages, the model tends to underperform in the math category.This suggests that the reasoning skills required in these questions present a great challenge for the model.Conversely, the model exhibits relatively stronger performance in the natural science and social science subjects across all languages, indicating a more effective handling of structured and factual information in these areas.</p>
<p>Discussions</p>
<p>Related Work</p>
<p>Large language models (LLMs) have witnessed remarkable advancements in recent years, enabling them to generate human-like text, answer complex questions, and perform a wide range of NLP tasks.These models, such as GPT-3 [9], Claude [4], GPT-4 [31], and PaLM2 [3] have demonstrated exceptional performance on various benchmarks and have been widely adopted in academia and industry.However, the evaluation of these models is a critical aspect that requires careful consideration to ensure reliable and comprehensive assessments.</p>
<p>For the evaluation of NLP models, traditional approaches primarily rely on established NLP benchmark datasets.Popular benchmarks such as GLUE [39], SuperGLUE [38], and SQuAD [34] focus on specific NLP tasks, such as question answering, sentiment analysis, and text classification.To facilitate multilingual evaluation, researchers have also developed multilingual benchmarks such as XTREME [18] and XTREME-R [35].These benchmarks provide standardized evaluation settings, diverse language coverage, and task-specific evaluation metrics to assess models' performance in a multilingual setting [2,7,24].In the multimodal context, the evaluation often involves assessing the model's ability to understand and generate content that combines multiple modalities, such as text, images, and videos.Some typical evaluation tasks include image captioning [11,1], image question answering [5,16], visual reasoning [20], video question answering [41] etc.</p>
<p>Although performance on typical benchmark datasets provides valuable insights into the capabilities of LLMs, it may not be sufficient to evaluate their general intelligence in real-world scenarios.To bridge this gap, there has been a growing trend of utilizing exams originally designed for humans to evaluate LLMs in recent times.An early work is the MMLU [17] dataset, which collects questions covering 57 tasks to test the model's world knowledge and multitask accuracy.More recently, similar benchmark datasets have been proposed following this direction, such as AGIEval [43] with various types of exams, C-Eval [19] and GAOKAO [42] benchmarks using exam questions in Chinese to evaluate Chinese LLMs, and IgakuQA [22] that evaluates ChatGPT on Japanese Medical Licensing Exams.However, these datasets suffer from several limitations, including limited language diversity, the absence of multimodal evaluation, and the lack of multi-level evaluation.These limitations restrict the comprehensive assessment of LLMs in real-world scenarios.</p>
<p>Conclusions</p>
<p>We introduce M3Exam in this work, a novel benchmark dataset for evaluating LLMs by offering a multilingual, multimodal, and multi-level assessment.Our analysis of top-performing LLMs on M3Exam reveals that current models face challenges in processing multilingual text, especially in low-resource and non-Latin script languages.Additionally, state-of-the-art multimodal models struggle to achieve reasonable accuracy on M3Exam.Overall, it provides a valuable resource for tracking the progress of LLMs in multilingual and multimodal settings and offers insights into the development of model intelligence across various education levels.However, M3Exam only considers multiple-choice questions for now, making it unsuitable to evaluate LLMs for questions requiring creative writing.We will consider such questions in our future work.</p>
<p>A Appendix A.1 Dataset Documentation</p>
<p>We provide additional information on the introduced M3Exam dataset in this section.</p>
<p>A.1.1 Motivation</p>
<p>M3Exam is created to test models' multilingual and multimodal abilities through questions that are from real and official human exams.Current benchmarks on multilingual and/or multimodal evaluation still mainly focus on traditional NLP tasks, which have several limitations as described in the paper, we aim to bridge this gap through M3Exam.Moreover, we aim to provide insights into the development of machine intelligence through questions from different education levels.</p>
<p>A.1.2 Composition</p>
<p>• M3Exam contains textual questions, part of them need images to solve.</p>
<p>• There are 12,317 questions in total.</p>
<p>• Questions are from exam papers across multiple years for each language, thus they are representative of the expected knowledge of certain languages.• M3Exam is self-contained.Part of the questions requiring images are released with the corresponding images and clearly identified.• The dataset does not involve any specific person and does not contain any information that might be offensive, insulting, or threatening.</p>
<p>A.1.3 Usage and Distribution</p>
<p>• The dataset is released at https://github.com/DAMO-NLP-SG/M3Exam.</p>
<p>• The data is saved in JSON format, where an example is shown in the README.mdfile.An example code snippet is also provided showing how to read and process the data.• License: M3Exam is under CC BY-NC-SA License.</p>
<p>A.2 Examples of different prompting strategies</p>
<p>We show some detailed examples of different prompting strategies in Figure 6.It can be noticed from the example of the Thai question, that even when translating the data into English, the question might still be difficult to answer since it may involve background knowledge of each specific language.</p>
<p>A.3 More examples on multimodal questions</p>
<p>We present two additional examples of questions involving images in Figure 7.In the first question, even though an image is required to answer, the keyword "Gandhi" is already mentioned in the question text.As a result, a text-only model like Flan-T5 might be capable of providing the correct answer.Upon further examination of the models' descriptions, we observe that BLIP-2 and Instruct-BLIP offer relatively more accurate descriptions.BLIP-2 refers to nonviolent resistance or civil resistance, demonstrating its ability to capture the semantic or conceptual meaning of the images.Instruct-BLIP, on the other hand, provides a detailed description of the physical activity depicted in the figure.However, neither Fromage nor Flamingo delivers relevant descriptions of the photograph, or gives hallucination descriptions.</p>
<p>In the second example, we select a question with images present in the options.To obtain the descriptions, we separately feed each image to the model and show models' generations of two option images for simplicity.It is worth noting that for this relatively easy image understanding task, all models seem to provide more accurate descriptions.They can recognize the physical objects in the images and even identify the specific number of cubes in some cases.However, it is still difficult for them to consistently give accurate descriptions for all options.Overall, we can see that multimodal questions in M3Exam post a great challenge for existing multimodal models compared to previous multimodal tasks since they require a more accurate understanding of the involved images and may even need to reason across multiple images.</p>
<p>The following are multiple-choice questions about social-science.</p>
<p>Mahatma Gandhi Spinning Yarn in the Late 1920s</p>
<p>Source: Carnegie Council for Ethics in International Affairs, January 15, 1929" This photograph is most often associated with the term</p>
<p>"Figure 1 :
1
Figure 1: Example questions from M3Exam dataset.Correct answers are indicated in bold.Metainformation is provided with blue boxes attached to each question.</p>
<p>Figure 2 :
2
Figure 2: Illustrations of the prompt with two short questions.Model outputs are marked in green.Each option will take a new line in practice.</p>
<p>What does the image show?Describe all details including the figures, texts and numbers.Fromage: Here's what it's supposed to look like: Lines under the mirrors are.Flamingo: Draw a horizontal line so it will fall on the right side of the small object.Draw a line so that it will fall on the other side of the image.Together, draw a perpendicular line from the top of the image from the cross line between the two lines formed with the last two steps.BLIP-2: a diagram showing the light ray path and path length through a plane mirror Instruct-BLIP: plane mirror</p>
<p>Figure 3 :
3
Figure 3: An example question with different model outputs (in green) on the left-hand side, as well as model outputs when asked to describe the image in detail on the right-hand side.</p>
<p>Figure 4 :
4
Figure 4: Performance of different LLMs broken down along different levels.</p>
<p>Figure 5 :
5
Figure 5: Performance of ChatGPT across different subject categories.</p>
<p>What does the image show?Describe all details including the figures, texts and numbers.</p>
<p>Table 1 :
1
Data statistics of M3Exam dataset.We rank languages by their ratio in the CommonCrawl corpus ("CC Size"), and report the detailed number of questions at each level: X/Y denotes there are X questions involving only pure text, and Y questions requiring images to solve.
LanguageCodeCountryCC SizeLowMidHighTotalEnglishenUS46.175306 / 106505 / 2041132 / 4851943 / 795ChinesezhChina4.63283 / 14347 / 335281 / 104711 / 453ItalianitItaly2.726220 / 107291 / 140318 / 160829 / 407PortugueseptBrazil1.13186 / 96182 / 98645 / 278913 / 472VietnameseviVietnam1.056170 / 16361 / 121286 / 881817 / 116ThaithThailand0.440472 / 113568 / 1741154 / 1142194 / 401SwahiliswKenya0.008186 / 4248 / 0-434 / 4AfrikaansafSouth Africa0.00791 / 36138 / 6354 / 64283 / 163JavanesejvIndonesia0.004205 / 4172 / 1-377 / 5Total1819 / 496 2812 / 1027 4870 / 1293 9501 / 2816</p>
<p>Table 2 :
2
Results on questions of different languages.Accuracy scores are reported.
enzhitptvithswafjvavgrandom25.01 25.93 33.77 21.41 25.21 22.89 25.00 25.05 25.00 25.47passing60.00 60.00 60.00 60.00 50.00 50.00 40.00 50.00 60.00 54.44BLOOM 28.62 29.47 33.17 7.20 23.81 9.09 27.10 23.26 26.95 23.19Vicuna56.99 29.18 35.39 41.73 27.33 15.08 24.07 33.33 27.49 32.29Claude74.25 51.61 61.90 62.54 51.65 31.27 38.32 63.95 30.73 51.80ChatGPT 75.98 61.00 67.94 62.43 57.18 34.09 53.04 68.99 37.47 57.57GPT-487.55 79.47 83.23 74.24 70.49 56.04 65.89 84.11 55.26 72.92</p>
<p>Table 3 :
3
Results on different prompting strategies based on ChatGPT.
PromptenzhitptvithswafjvMonolingual75.98 61.00 67.94 62.43 57.18 34.09 53.04 68.99 37.47EN-Instruct-60.56 69.30 61.42 57.57 32.70 49.30 70.16 38.27EN-Translation-57.92 62.76 59.62 56.40 46.49 48.13 70.16 50.94Few-shot75.46 60.26 64.36 62.99 58.64 37.41 51.87 67.05 33.42
Note that the exact passing line depends on each specific exam. Here we provide the scores conventionally used in the corresponding countries, which can indicate the relative difficulty of the questions.
We use Google Translation API (https://translate.google.com/) for translating the data into English.
AcknowledgementsWe extend our gratitude to the data management and annotation team at Alibaba DAMO Academy, particularly Yanyan Zheng, Tantong Champaiboon, Nguyen Ngoc Yen Nhi, and Andila Putri Susanti.Their kind assistance in coordinating the annotators, processing the data, and conducting quality checks has been important to our work.We would also like to thank the annotators from various linguistic and cultural backgrounds who collaborated to contribute to this unique multilingual dataset.Fromage: This is a textile, and not a photograph.Flamingo: The picture shows famous Bharat Ratna(India\u2019s premiere civilian award) awardee, Rashtrapati Dr. A.P.J. Abdul Kalam, better known as Missile Man of India, preparing tea for his students!! How is this a question?No description is given.No mention is made of details such as the figures, texts, and numbers are showing.The important details in this question are only the figures and texts, and a hint concerning the nature of the numbers.TheThe following is a multiple choice question about Social Science.Please only give the correct option, without any other details or explanations.What is a 100-year period of time called?Few-Shot PromptingThe following is a multiple choice question about Math.Please only give the correct option, without any other details or explanations.将3克药放入100克水中，药与药水的比是English-TranslationFigure6: Detailed examples of different prompting strategies."Zero-shot Prompting" refers to our default prompting strategy, where prompts are in each specific language."Few-shot Prompting" involves few-shot in-context examples, which appear before the final test sample."English-instruction" uses English instructions instead of instructions in the target language."English-Translation" uses the translated English data of other languages.A.4 Details on exam papersWe list the specific exams we collected for constructing the datasets in Table5.A.5 Examples questions of each language in M3ExamWe list example questions of each language from Figure8to Figure16.Na tira da série For better or for worse, a comunicação entre as personagens fica comprometida em um determinado momento porque (A) as duas amigas divergem de opinião sobre futebol (B) uma das amigas desconsidera as preferências da outra.(C) uma das amigas ignora que o outono é temporada de futebol.(D) uma das amigas desconhece a razão pela qual a outra a maltrata.(E) as duas amigas atribuem sentidos diferentes à palavra season.Translated to English:In the series strip For better or for worse, the communication between the characters is compromised at a certain moment because  In the human digestive system, as an effect of digestive enzymes, which of the following substances is broken down into glycerol and fatty acids?
nocaps: novel object captioning at scale. Harsh Agrawal, Peter Anderson, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019. IEEE2019</p>
<p>MEGA: multilingual evaluation of generative AI. Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, Sunayana Sitaram, CoRR, abs/2303.125282023</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, 2023Palm 2 technical report. CoRR, abs/2305.10403</p>
<p>. Anthropic. Introducing claude. 2023</p>
<p>VQA: visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, 2015 IEEE International Conference on Computer Vision, ICCV 2015. 2015</p>
<p>. Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Jenia Gadre, Simon Jitsev, Pang Wei Kornblith, Gabriel Koh, Mitchell Ilharco, Ludwig Wortsman, Schmidt, Openflamingo, March 2023</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, CoRR, abs/2302.040232023</p>
<p>Findings of the 2014 workshop on statistical machine translation. Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, Ales Tamchyna, Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL. the Ninth Workshop on Statistical Machine Translation, WMT@ACL2014. 2014</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020. 2020. 2020</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, Yi Zhang, CoRR, abs/2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. 2023</p>
<p>Microsoft COCO captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, C Lawrence Zitnick, CoRR, abs/1504.003252015</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Y Yu, Yanping Zhao, Andrew M Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, CoRR, abs/2210.114162022</p>
<p>XNLI: evaluating cross-lingual sentence representations. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R Bowman, Holger Schwenk, Veselin Stoyanov, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2018</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Making the V in VQA matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, 2017 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society2017. 2017</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. OpenReview.net, 2021</p>
<p>XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, Proceedings of the 37th International Conference on Machine Learning, ICML 2020. the 37th International Conference on Machine Learning, ICML 20202020119</p>
<p>Maosong Sun, and Junxian He. Ceval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, abs/2305.083222023CoRR</p>
<p>GQA: A new dataset for real-world visual reasoning and compositional question answering. Drew A Hudson, Christopher D Manning, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Computer Vision Foundation / IEEE2019</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, Guillaume Lample, CoRR, abs/2210.122832022</p>
<p>Evaluating GPT-4 and chatgpt on japanese medical licensing examinations. Jungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro Yamada, Dragomir Radev, CoRR, abs/2303.180272023</p>
<p>Grounding language models to images for multimodal inputs and outputs. Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, 2023</p>
<p>Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. Dac Viet, Lai, Trung Nghia, Amir Ngo, Ben Pouran, Hieu Veyseh, Franck Man, Trung Dernoncourt, Thien Huu Bui, Nguyen, CoRR, abs/2304.056132023</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, V Vinay, Ambrose Ramasesh, Slone, NeurIPS. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, 2022</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, 2023</p>
<p>Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, You Zhang, CoRR, abs/2303.140702023</p>
<p>Visually grounded reasoning across languages and cultures. Fangyu Liu, Emanuele Bugliarello, Maria Edoardo, Siva Ponti, Nigel Reddy, Desmond Collier, Elliott, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 20212021</p>
<p>. LMSYS Org. Chatbot arena leaderboard. May 2023</p>
<p>. OpenAI. Introducing chatgpt. 2022</p>
<p>GPT-4 technical report. CoRR, abs/2303.087742023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, NeurIPS. 2022</p>
<p>XCOPA: A multilingual dataset for causal commonsense reasoning. Maria Edoardo, Goran Ponti, Olga Glavas, Qianchu Majewska, Ivan Liu, Anna Vulic, Korhonen, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2020. 2020</p>
<p>Squad: 100, 000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016. 2016</p>
<p>XTREME-R: towards more challenging and nuanced multilingual evaluation. Sebastian Ruder, Noah Constant, Jan A Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, Melvin Johnson, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 20212021</p>
<p>Alham Fikri Aji. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova Del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina Mcmillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa ; Canwen, Chenghao Xu, Chris Mou, Christopher Emezue, Colin Klamm, Leong, David Ifeoluwa Daniel Van Strien, Adelani, Ariel Kreisberg Nitzav. 2022Amit Alfassy. A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100</p>
<p>Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Joulin, CoRR, abs/2302.139712023</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. 2019. 2019</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018. Tal Linzen, Grzegorz Chrupala, Afra Alishahi, the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 20182018</p>
<p>Bloomberggpt: A large language model for finance. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David S Rosenberg, Gideon Mann, CoRR, abs/2303.175642023</p>
<p>Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Proceedings of the 2017 ACM on Multimedia Conference, MM 2017. the 2017 ACM on Multimedia Conference, MM 2017ACM2017</p>
<p>Evaluating the performance of large language models on GAOKAO benchmark. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng Qiu, CoRR, abs/2305.124742023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, abs/2304.063642023CoRR</p>            </div>
        </div>

    </div>
</body>
</html>
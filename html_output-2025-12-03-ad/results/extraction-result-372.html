<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-372 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-372</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-372</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-270878452</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.01863v1.pdf" target="_blank">VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</a></p>
                <p><strong>Paper Abstract:</strong> Vision language models (VLMs) are an exciting emerging class of language models (LMs) that have merged classic LM capabilities with those of image processing systems. However, the ways that these capabilities combine are not always intuitive and warrant direct investigation. One understudied capability in VLMs is visual spatial planning -- the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates the spatial planning capability in these models in general, and 2) breaks down the visual planning task into finer-grained sub-tasks, including perception and reasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation shows that both open-source and private VLMs fail to generate effective plans for even simple spatial planning tasks. Evaluations on the fine-grained analytical tasks further reveal fundamental deficiencies in the models' visual perception and bottlenecks in reasoning abilities, explaining their worse performance in the general spatial planning tasks. Our work illuminates future directions for improving VLMs' abilities in spatial planning. Our benchmark is publicly available at https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e372.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e372.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VSP-Maze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Spatial Planning — Maze Navigation Scenario (VSP benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully-observable grid-world maze planning task in the VSP benchmark that tests VLMs' visual perception and multi-step spatial planning (navigation) abilities using images and alternative textual/table encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>benchmark (evaluated across many VLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model — this is the maze navigation task used to probe how VLMs interpret layouts, localize player/goal/hazards, and produce action sequences. Input modalities included images, explicit textual descriptions, and tabular encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Maze Navigation (VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fully-observable grid maps (sizes 3x3 to 8x8) containing a player, goal, and holes. The model must perceive the map and output a safe action plan (sequence of L/R/U/D) that moves the player from start to goal without stepping into holes. Subtasks probe single-cell perception, pairwise spatial relations (player vs goal), environment-to-text mapping, and verifying consequences of an action sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural (spatial layouts, local adjacency + multi-step action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>visual input (image) and/or explicit textual/table descriptions constructed from the image; pretraining/fine-tuning of the VLMs; optionally in-context examples or task fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (primary), few-shot / in-context prompting (examined), and fine-tuning on task data (open-source models experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>When visual: image embeddings (projected into LLM token/embedding space, e.g., CLIP-style encoders or frozen visual encoders). When visual replaced by text: explicit natural-language descriptions or tabular text representing coordinates. Plans are represented as natural-language action sequences (strings of moves).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate for full planning task (path reaches goal without holes); accuracy for subtasks (perception/relation/reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Across models, zero-shot success rates fall rapidly with map size; for example, best private VLMs achieve >50% on 3x3 maps but drop to ~10% on 8x8 maps (paper highlights GPT-Vision: >50% on 3x3 then ~10% on 8x8). Subtask accuracies (Table 3) show private models range ~0.45–0.74 across perception and reasoning subtasks, while open-source models often approach random.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Models can often identify simple local properties (single-cell safety, short relative relations) and produce short 1-step plans; private VLMs do well on simpler, low-step planning and single-object perception subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Degraded visual perception on images as map size increases (mislocalization of player/holes/goal), failure to maintain multi-step (long) action-state bookkeeping (leading to invalid or unsafe plans), context-window and multi-image input handling limits for some open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Random-guess baselines for subtasks provided in paper (e.g., Maze T1 random 0.50; T2 0.25; T3 0.25; T4 0.50). VLMs often only modestly exceed these baselines except on simplest settings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Crucial ablation: replacing visual input with perfect textual/table descriptions (i.e., removing visual perception requirement) substantially improves performance for all tested models, demonstrating perception is a dominant bottleneck; however, some reasoning deficits remain (e.g., Gemini still struggled on reasoning-heavy instances even with text). In-context examples provided only small or inconsistent gains; fine-tuning (for open-source models) produced clear improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This task demonstrates that (1) spatial knowledge encoded from images is mediated by image embeddings which can be lossy for detailed localization; (2) supplying explicit textual state eliminates perception errors and reveals residual reasoning limitations; (3) VLMs’ representation of procedural sequences is natural-language action strings, but maintaining correct multi-step state requires stronger explicit state-tracking than current models offer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e372.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e372.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VSP-Blocks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Spatial Planning — Blocks World Scenario (VSP benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A block-stacking rearrangement planning task in VSP: from image pairs (start/goal) the model must generate a shortest move sequence under the constraint that only top blocks can be moved.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>benchmark (evaluated across many VLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Blocks-world planning scenario used to probe object-relational perception (colors, stacks, levels) and procedural planning (valid action sequences under stack/top constraints). Inputs: photo-realistic images (BIRD dataset) or pure textual descriptions of stack contents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Blocks World (VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an initial and a goal block configuration (3–5 blocks, stacks), generate a shortest valid sequence of moves (only top block moves allowed; destination = table or top of another stack) to transform initial to goal. Subtasks evaluate single-object perception (color at a stack/level), spatial relation perception (above/below/same stack), environment text mapping, and reasoning about validity/consequence of a given move plan.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-manipulation / multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational+procedural (object identities/colors, stack relations, affordance constraint that only top blocks movable, and multi-step action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>visual input (images from BIRD), textual descriptions derived from images (in ablation), pretraining and any fine-tuning; in-context examples and supervised fine-tuning tested for open-source models</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (primary), few-shot/in-context prompting (examined), and supervised fine-tuning (10k examples for open-source models)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Object relations are represented as natural-language descriptions (e.g., 'Stack 1: purple'; 'Stack 3: bottom orange, top red') when text used; visually, objects are encoded via visual encoder embeddings. Procedural plans are expressed as action commands move(SOURCE, TARGET) in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy for subtasks; success rate / shortest-plan correctness for full planning task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Table 3 subtasks: private models show high performance on BlocksWorld perception subtasks (e.g., GPT-4o T1 0.95, T2 0.90, T3 0.90, T4 0.76; GPT-Vision T1 0.73–0.80 across subtasks). Open-source models perform near random on perception subtasks without fine-tuning. Fine-tuning (10k) produced substantial gains for open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Models succeed at recognizing block colors and simple stack relations when perceptual input is clear, and private VLMs often produce correct short move sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When images are visually challenging or multiple images/context tokens exceed model context, models mis-identify block positions or colors; reasoning over longer multi-step rearrangements leads to errors (invalid moves like attempting to move covered blocks).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Random-guess baselines for BlocksWorld subtasks provided in paper (e.g., T1 0.17; T2 0.25; T3 0.25; T4 0.50). Private VLMs substantially exceed these baselines on many subtasks; open-source models often do not without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Replacing images with pure textual descriptions (perfect perception) raises performance markedly. Fine-tuning open-source models on 10k VSP examples yields large accuracy improvements; different model architectures benefited differently (LLaVA improved more than InternLM in the paper's experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Object-relational knowledge can be successfully elicited when perception is accurate or when models are fine-tuned on paired image-text-state data; however, visual encoding losses and context-size limits prevent reliable perception-to-plan pipelines in zero-shot settings for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e372.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e372.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.0-Pro-Vision (evaluated in VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A private multimodal VLM evaluated on VSP; provided decent perception and mixed reasoning in zero-shot across subtasks but showed reasoning shortfalls even when perception was supplied as text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.0-Pro-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A private multimodal LLM (Gemini family) with visual capabilities used by the authors; used zero-shot evaluation on VSP tasks and also tested with in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Maze Navigation & Blocks World (VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See VSP Maze and Blocks World descriptions; Gemini was evaluated zero-shot on all main tasks and subtasks, and in few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation + object-manipulation / multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on multimodal corpora (implicit), zero-shot prompts, optional in-context examples; in experiments also provided textual encodings instead of images to simulate perfect perception</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (primary), few-shot (1–4 examples) experiments conducted</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in model weights plus image embeddings; procedural outputs as natural-language action sequences; when images replaced by text, state encoded as natural-language or table strings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on subtasks (Table 3) and overall success rate on main tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Table 3 subtasks: Maze T1 0.58, T2 0.56, T3 0.33, T4 0.49; Blocks T1 0.86, T2 0.51, T3 0.54, T4 0.55. Overall, Gemini performs moderately well on perception but shows reasoning limits. With textual input (perfect perception) performance improved but reasoning deficits remained.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good at single-object perception in blocks, some spatial relation tasks; handles short actions and simple environment descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Struggles on image-based environment perception for larger maps and on long multi-step reasoning even when perception is provided as text (indicating non-perceptual reasoning limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to random baselines in Table 3, Gemini substantially exceeds random on many tasks but lags behind best private models (e.g., GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Textual replacement (removing vision) led to clear gains, but Gemini still failed certain reasoning tasks — indicating both perception and reasoning are bottlenecks. In-context examples provided limited additional gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gemini's spatial knowledge appears encoded partly via visual embeddings and implicit world-model priors, but its multi-step procedural reasoning is brittle; providing explicit symbolic/textual state reveals latent reasoning limitations beyond perception.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e372.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e372.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Vision / GPT-4 Turbo with vision (evaluated as GPT-Vision in VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong private multimodal model (GPT family with vision) evaluated in VSP; achieves relatively high perception and reasoning on short tasks but performance decays with environment complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Vision (turbo-2024-04-09)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4-derived multimodal variant with integrated vision encoder used for zero-shot and few-shot evaluation on VSP.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Maze Navigation & Blocks World (VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See VSP Maze and Blocks World; evaluated in zero-shot, few-shot (1/2/4 examples), and ablation with textual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation + object-manipulation / multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on multimodal corpora; zero-shot prompting and small in-context examples used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (primary), few-shot prompting tested</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Image inputs converted to embeddings consumed by the language model; when vision removed, textual/tabular encodings used; plans expressed as natural-language action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>subtask accuracy and main task success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Table 3 subtasks: Maze T1 0.56, T2 0.27, T3 0.46, T4 0.56; Blocks T1 0.73, T2 0.80, T3 0.70, T4 0.71. Paper notes >50% success on 3x3 maze but steep drop to ~10% on 8x8.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong at BlocksWorld object-relational perception and many short reasoning chains; better than most open-source models across subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Performance drops quickly as spatial complexity or required plan length increases; weak handling of multiple-image interleaved input in some open-source analogs (context-length issues noted).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms random baselines substantially on many subtasks, particularly BlocksWorld perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Providing textual descriptions instead of images yielded consistent performance improvements, indicating visual perception is a limiting factor for some classes of errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-Vision encodes spatial/object-relational information via its visual encoder and LLM fusion, allowing decent short-horizon planning; however, the model’s implicit state-tracking for long procedural sequences is insufficient under growing complexity, and perception errors from visual embeddings are a major failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e372.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e372.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (multimodal variant evaluated in VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A leading multimodal LLM in the VSP experiments that attains the best overall subtasks performance among tested models, but still exhibits degradation as task difficulty increases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (gpt-4o-2024-05-13)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal, recently released OpenAI model capable of processing images and text; used zero-shot in VSP and attained the strongest performance across private models in the paper's reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Maze Navigation & Blocks World (VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See VSP Maze and Blocks World descriptions; GPT-4o evaluated zero-shot on subtasks and main tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation + object-manipulation / multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on multimodal corpora; zero-shot prompts and few-shot variants tested</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (primary)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Visual encoder embeddings integrated with large language model; actionable plans produced as natural-language sequences; when images replaced by text, state represented in NL description.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>subtask accuracies and main task success rates</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Table 3 subtasks: Maze T1 0.58, T2 0.67, T3 0.58, T4 0.74; Blocks T1 0.95, T2 0.90, T3 0.90, T4 0.76 — best overall among evaluated models on these subtasks. Nevertheless performance decays with larger maps and longer required plan length.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>High accuracy on object-relational perception and pairwise spatial relations, and stronger multi-step planning than peers on short- to medium-length plans.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still fails on larger, longer-horizon planning tasks; perceptual errors and reasoning bottlenecks appear when environment complexity grows.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Strongly outperforms random baselines and generally outperforms other private and open-source models in VSP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Textual replacement of visual input boosts performance further, indicating perceptual encoding still contributes to residual errors; in-context examples produced marginal gains beyond zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o's internal representation effectively fuses visual embeddings and language for spatial and object-relational reasoning at short-to-medium horizons, but like other LLMs its implicit procedural/world-model capabilities are not robust across increasing plan length or visual ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e372.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e372.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA (LLAVA-V1.6-VICUNA-7B) evaluated in VSP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal model built by projecting image embeddings into the LLM space (CLIP encoder + LLaMA family) that performs poorly zero-shot on VSP tasks but shows substantial gains when fine-tuned on VSP data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA (LLAVA-V1.6-VICUNA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source VLM using a CLIP visual encoder to project image features into the LLM token space (LLaMA-derived Vicuna), evaluated zero-shot and after LoRA fine-tuning on 10k VSP examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Maze Navigation & Blocks World (VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See VSP Maze and Blocks World; LLaVA evaluated zero-shot and after supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation + object-manipulation / multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained visually aligned encoder (CLIP) + LLM weights; fine-tuning on VSP supervised examples (10k) improves performance</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting and supervised fine-tuning (LoRA on 10k examples)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Visual features from CLIP mapped into LLM embedding space (implicit representation), plans as NL action sequences; after fine-tuning, model internal representations adapted to VSP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>subtask accuracy; main-task success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Zero-shot subtasks (Table 3): Maze T1 0.49, T2 0.27, T3 0.21, T4 0.54; Blocks T1 0.22, T2 0.21, T3 0.24, T4 0.55 (near random). After fine-tuning on 10k examples, LLaVA showed large improvements (paper reports substantial gains; LLaVA improved more than InternLM in their fine-tuning experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>After task-specific fine-tuning, the model learned mapping from visual embeddings to structured state descriptions and produced more correct procedural plans.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Zero-shot performance is generally poor (close to random) due to limited context window, single-image training bias, and weaker multimodal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Pre-fine-tuning performance near random baseline; post-fine-tuning significantly better, demonstrating supervised adaptation importance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Fine-tuning (10k examples) is an effective intervention to overcome initial perception/format limitations; context-length and multi-image handling remain architecture constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Open-source visual encoders projecting into LLM space (CLIP->LLaMA) store spatial/object-relational information in image embeddings, but without task-specific alignment/fine-tuning these representations are insufficient for reliable embodied planning; supervised fine-tuning can adapt these embeddings to produce correct procedural outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e372.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e372.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Textual-replacement ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation: Replacing visual input with explicit textual or tabular state descriptions (VSP experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where images are replaced by perfect textual descriptions or tables of the environment to separate perception and reasoning bottlenecks; this yields significant performance gains and reveals residual reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to evaluated VLMs (Gemini, GPT-Vision, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Experimentally created textual or tabular encodings of the same information present in images (player/holes/goal coordinates for maze; explicit stack contents for BlocksWorld) used as input in place of images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ablation: Visual -> Textual Input (VSP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For every image input, a corresponding textual description (or table for maze) is provided as input in lieu of the image to test model performance given perfect perception.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>diagnostic (perception vs reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational (encoded explicitly as NL or table)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit textual encoding generated from ground-truth environment state</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (with explicit textual state in input) and evaluation of performance gains relative to vision-input experiments</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit natural-language or tabular descriptions encoding exact positions, stacks, and relations; model uses its language-only capacities to reason about state transitions and produce plans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>delta in success rate / accuracy when using text/table input versus image input</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Paper reports clear, consistent performance improvement across every task when textual input replaces images (Figure 5); indicates perception is a leading cause of failures. Exact quantitative deltas vary by model and task; authors note Gemini's reasoning still inadequate on some tasks even with textual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When provided explicit symbolic state, models are better able to produce correct procedural plans and answer relational queries — perception errors vanish.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Even with perfect perception, some models still fail multi-step reasoning tasks, indicating separate limitations in implicit procedural/world-modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Vision-input zero-shot performance vs text-input zero-shot performance (text consistently higher); demonstrates that perception (vision→embedding) is a major bottleneck in current VLM planning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Textual replacement is the core ablation; it improves performance markedly, isolating reasoning as a residual bottleneck. In-context examples provide limited further benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Replacing vision with explicit symbolic state shows that VLMs often possess the language-level machinery to express plans but fail primarily due to imperfect visual encoding and, secondarily, due to brittle multi-step procedural reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning <em>(Rating: 2)</em></li>
                <li>Lgmcts: Language-guided monte-carlo tree search for executable semantic object rearrangement <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Saycanpay: Heuristic planning with large language models using learnable domain knowledge <em>(Rating: 1)</em></li>
                <li>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning <em>(Rating: 2)</em></li>
                <li>Layoutgpt: Compositional visual planning and generation with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-372",
    "paper_id": "paper-270878452",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "VSP-Maze",
            "name_full": "Visual Spatial Planning — Maze Navigation Scenario (VSP benchmark)",
            "brief_description": "A fully-observable grid-world maze planning task in the VSP benchmark that tests VLMs' visual perception and multi-step spatial planning (navigation) abilities using images and alternative textual/table encodings.",
            "citation_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
            "mention_or_use": "use",
            "model_name": "benchmark (evaluated across many VLMs)",
            "model_size": null,
            "model_description": "Not a single model — this is the maze navigation task used to probe how VLMs interpret layouts, localize player/goal/hazards, and produce action sequences. Input modalities included images, explicit textual descriptions, and tabular encodings.",
            "task_name": "Maze Navigation (VSP)",
            "task_description": "Fully-observable grid maps (sizes 3x3 to 8x8) containing a player, goal, and holes. The model must perceive the map and output a safe action plan (sequence of L/R/U/D) that moves the player from start to goal without stepping into holes. Subtasks probe single-cell perception, pairwise spatial relations (player vs goal), environment-to-text mapping, and verifying consequences of an action sequence.",
            "task_type": "navigation",
            "knowledge_type": "spatial+procedural (spatial layouts, local adjacency + multi-step action sequences)",
            "knowledge_source": "visual input (image) and/or explicit textual/table descriptions constructed from the image; pretraining/fine-tuning of the VLMs; optionally in-context examples or task fine-tuning",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting (primary), few-shot / in-context prompting (examined), and fine-tuning on task data (open-source models experiment)",
            "knowledge_representation": "When visual: image embeddings (projected into LLM token/embedding space, e.g., CLIP-style encoders or frozen visual encoders). When visual replaced by text: explicit natural-language descriptions or tabular text representing coordinates. Plans are represented as natural-language action sequences (strings of moves).",
            "performance_metric": "success rate for full planning task (path reaches goal without holes); accuracy for subtasks (perception/relation/reasoning)",
            "performance_result": "Across models, zero-shot success rates fall rapidly with map size; for example, best private VLMs achieve &gt;50% on 3x3 maps but drop to ~10% on 8x8 maps (paper highlights GPT-Vision: &gt;50% on 3x3 then ~10% on 8x8). Subtask accuracies (Table 3) show private models range ~0.45–0.74 across perception and reasoning subtasks, while open-source models often approach random.",
            "success_patterns": "Models can often identify simple local properties (single-cell safety, short relative relations) and produce short 1-step plans; private VLMs do well on simpler, low-step planning and single-object perception subtasks.",
            "failure_patterns": "Degraded visual perception on images as map size increases (mislocalization of player/holes/goal), failure to maintain multi-step (long) action-state bookkeeping (leading to invalid or unsafe plans), context-window and multi-image input handling limits for some open-source models.",
            "baseline_comparison": "Random-guess baselines for subtasks provided in paper (e.g., Maze T1 random 0.50; T2 0.25; T3 0.25; T4 0.50). VLMs often only modestly exceed these baselines except on simplest settings.",
            "ablation_results": "Crucial ablation: replacing visual input with perfect textual/table descriptions (i.e., removing visual perception requirement) substantially improves performance for all tested models, demonstrating perception is a dominant bottleneck; however, some reasoning deficits remain (e.g., Gemini still struggled on reasoning-heavy instances even with text). In-context examples provided only small or inconsistent gains; fine-tuning (for open-source models) produced clear improvements.",
            "key_findings": "This task demonstrates that (1) spatial knowledge encoded from images is mediated by image embeddings which can be lossy for detailed localization; (2) supplying explicit textual state eliminates perception errors and reveals residual reasoning limitations; (3) VLMs’ representation of procedural sequences is natural-language action strings, but maintaining correct multi-step state requires stronger explicit state-tracking than current models offer.",
            "uuid": "e372.0",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "VSP-Blocks",
            "name_full": "Visual Spatial Planning — Blocks World Scenario (VSP benchmark)",
            "brief_description": "A block-stacking rearrangement planning task in VSP: from image pairs (start/goal) the model must generate a shortest move sequence under the constraint that only top blocks can be moved.",
            "citation_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
            "mention_or_use": "use",
            "model_name": "benchmark (evaluated across many VLMs)",
            "model_size": null,
            "model_description": "Blocks-world planning scenario used to probe object-relational perception (colors, stacks, levels) and procedural planning (valid action sequences under stack/top constraints). Inputs: photo-realistic images (BIRD dataset) or pure textual descriptions of stack contents.",
            "task_name": "Blocks World (VSP)",
            "task_description": "Given an initial and a goal block configuration (3–5 blocks, stacks), generate a shortest valid sequence of moves (only top block moves allowed; destination = table or top of another stack) to transform initial to goal. Subtasks evaluate single-object perception (color at a stack/level), spatial relation perception (above/below/same stack), environment text mapping, and reasoning about validity/consequence of a given move plan.",
            "task_type": "object-manipulation / multi-step planning",
            "knowledge_type": "object-relational+procedural (object identities/colors, stack relations, affordance constraint that only top blocks movable, and multi-step action sequences)",
            "knowledge_source": "visual input (images from BIRD), textual descriptions derived from images (in ablation), pretraining and any fine-tuning; in-context examples and supervised fine-tuning tested for open-source models",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting (primary), few-shot/in-context prompting (examined), and supervised fine-tuning (10k examples for open-source models)",
            "knowledge_representation": "Object relations are represented as natural-language descriptions (e.g., 'Stack 1: purple'; 'Stack 3: bottom orange, top red') when text used; visually, objects are encoded via visual encoder embeddings. Procedural plans are expressed as action commands move(SOURCE, TARGET) in natural language.",
            "performance_metric": "accuracy for subtasks; success rate / shortest-plan correctness for full planning task",
            "performance_result": "Table 3 subtasks: private models show high performance on BlocksWorld perception subtasks (e.g., GPT-4o T1 0.95, T2 0.90, T3 0.90, T4 0.76; GPT-Vision T1 0.73–0.80 across subtasks). Open-source models perform near random on perception subtasks without fine-tuning. Fine-tuning (10k) produced substantial gains for open-source models.",
            "success_patterns": "Models succeed at recognizing block colors and simple stack relations when perceptual input is clear, and private VLMs often produce correct short move sequences.",
            "failure_patterns": "When images are visually challenging or multiple images/context tokens exceed model context, models mis-identify block positions or colors; reasoning over longer multi-step rearrangements leads to errors (invalid moves like attempting to move covered blocks).",
            "baseline_comparison": "Random-guess baselines for BlocksWorld subtasks provided in paper (e.g., T1 0.17; T2 0.25; T3 0.25; T4 0.50). Private VLMs substantially exceed these baselines on many subtasks; open-source models often do not without fine-tuning.",
            "ablation_results": "Replacing images with pure textual descriptions (perfect perception) raises performance markedly. Fine-tuning open-source models on 10k VSP examples yields large accuracy improvements; different model architectures benefited differently (LLaVA improved more than InternLM in the paper's experiments).",
            "key_findings": "Object-relational knowledge can be successfully elicited when perception is accurate or when models are fine-tuned on paired image-text-state data; however, visual encoding losses and context-size limits prevent reliable perception-to-plan pipelines in zero-shot settings for many models.",
            "uuid": "e372.1",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Gemini (evaluated)",
            "name_full": "Gemini-1.0-Pro-Vision (evaluated in VSP)",
            "brief_description": "A private multimodal VLM evaluated on VSP; provided decent perception and mixed reasoning in zero-shot across subtasks but showed reasoning shortfalls even when perception was supplied as text.",
            "citation_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
            "mention_or_use": "use",
            "model_name": "Gemini-1.0-Pro-Vision",
            "model_size": null,
            "model_description": "A private multimodal LLM (Gemini family) with visual capabilities used by the authors; used zero-shot evaluation on VSP tasks and also tested with in-context examples.",
            "task_name": "Maze Navigation & Blocks World (VSP)",
            "task_description": "See VSP Maze and Blocks World descriptions; Gemini was evaluated zero-shot on all main tasks and subtasks, and in few-shot settings.",
            "task_type": "navigation + object-manipulation / multi-step planning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pretraining on multimodal corpora (implicit), zero-shot prompts, optional in-context examples; in experiments also provided textual encodings instead of images to simulate perfect perception",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting (primary), few-shot (1–4 examples) experiments conducted",
            "knowledge_representation": "Implicit in model weights plus image embeddings; procedural outputs as natural-language action sequences; when images replaced by text, state encoded as natural-language or table strings.",
            "performance_metric": "accuracy on subtasks (Table 3) and overall success rate on main tasks",
            "performance_result": "Table 3 subtasks: Maze T1 0.58, T2 0.56, T3 0.33, T4 0.49; Blocks T1 0.86, T2 0.51, T3 0.54, T4 0.55. Overall, Gemini performs moderately well on perception but shows reasoning limits. With textual input (perfect perception) performance improved but reasoning deficits remained.",
            "success_patterns": "Good at single-object perception in blocks, some spatial relation tasks; handles short actions and simple environment descriptions.",
            "failure_patterns": "Struggles on image-based environment perception for larger maps and on long multi-step reasoning even when perception is provided as text (indicating non-perceptual reasoning limitation).",
            "baseline_comparison": "Compared to random baselines in Table 3, Gemini substantially exceeds random on many tasks but lags behind best private models (e.g., GPT-4o).",
            "ablation_results": "Textual replacement (removing vision) led to clear gains, but Gemini still failed certain reasoning tasks — indicating both perception and reasoning are bottlenecks. In-context examples provided limited additional gains.",
            "key_findings": "Gemini's spatial knowledge appears encoded partly via visual embeddings and implicit world-model priors, but its multi-step procedural reasoning is brittle; providing explicit symbolic/textual state reveals latent reasoning limitations beyond perception.",
            "uuid": "e372.2",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-Vision",
            "name_full": "GPT-Vision / GPT-4 Turbo with vision (evaluated as GPT-Vision in VSP)",
            "brief_description": "A strong private multimodal model (GPT family with vision) evaluated in VSP; achieves relatively high perception and reasoning on short tasks but performance decays with environment complexity.",
            "citation_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
            "mention_or_use": "use",
            "model_name": "GPT-Vision (turbo-2024-04-09)",
            "model_size": null,
            "model_description": "OpenAI's GPT-4-derived multimodal variant with integrated vision encoder used for zero-shot and few-shot evaluation on VSP.",
            "task_name": "Maze Navigation & Blocks World (VSP)",
            "task_description": "See VSP Maze and Blocks World; evaluated in zero-shot, few-shot (1/2/4 examples), and ablation with textual inputs.",
            "task_type": "navigation + object-manipulation / multi-step planning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pretraining on multimodal corpora; zero-shot prompting and small in-context examples used in experiments",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting (primary), few-shot prompting tested",
            "knowledge_representation": "Image inputs converted to embeddings consumed by the language model; when vision removed, textual/tabular encodings used; plans expressed as natural-language action sequences.",
            "performance_metric": "subtask accuracy and main task success rate",
            "performance_result": "Table 3 subtasks: Maze T1 0.56, T2 0.27, T3 0.46, T4 0.56; Blocks T1 0.73, T2 0.80, T3 0.70, T4 0.71. Paper notes &gt;50% success on 3x3 maze but steep drop to ~10% on 8x8.",
            "success_patterns": "Strong at BlocksWorld object-relational perception and many short reasoning chains; better than most open-source models across subtasks.",
            "failure_patterns": "Performance drops quickly as spatial complexity or required plan length increases; weak handling of multiple-image interleaved input in some open-source analogs (context-length issues noted).",
            "baseline_comparison": "Outperforms random baselines substantially on many subtasks, particularly BlocksWorld perception tasks.",
            "ablation_results": "Providing textual descriptions instead of images yielded consistent performance improvements, indicating visual perception is a limiting factor for some classes of errors.",
            "key_findings": "GPT-Vision encodes spatial/object-relational information via its visual encoder and LLM fusion, allowing decent short-horizon planning; however, the model’s implicit state-tracking for long procedural sequences is insufficient under growing complexity, and perception errors from visual embeddings are a major failure mode.",
            "uuid": "e372.3",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4o (evaluated)",
            "name_full": "GPT-4o (multimodal variant evaluated in VSP)",
            "brief_description": "A leading multimodal LLM in the VSP experiments that attains the best overall subtasks performance among tested models, but still exhibits degradation as task difficulty increases.",
            "citation_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
            "mention_or_use": "use",
            "model_name": "GPT-4o (gpt-4o-2024-05-13)",
            "model_size": null,
            "model_description": "A multimodal, recently released OpenAI model capable of processing images and text; used zero-shot in VSP and attained the strongest performance across private models in the paper's reported results.",
            "task_name": "Maze Navigation & Blocks World (VSP)",
            "task_description": "See VSP Maze and Blocks World descriptions; GPT-4o evaluated zero-shot on subtasks and main tasks.",
            "task_type": "navigation + object-manipulation / multi-step planning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pretraining on multimodal corpora; zero-shot prompts and few-shot variants tested",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting (primary)",
            "knowledge_representation": "Visual encoder embeddings integrated with large language model; actionable plans produced as natural-language sequences; when images replaced by text, state represented in NL description.",
            "performance_metric": "subtask accuracies and main task success rates",
            "performance_result": "Table 3 subtasks: Maze T1 0.58, T2 0.67, T3 0.58, T4 0.74; Blocks T1 0.95, T2 0.90, T3 0.90, T4 0.76 — best overall among evaluated models on these subtasks. Nevertheless performance decays with larger maps and longer required plan length.",
            "success_patterns": "High accuracy on object-relational perception and pairwise spatial relations, and stronger multi-step planning than peers on short- to medium-length plans.",
            "failure_patterns": "Still fails on larger, longer-horizon planning tasks; perceptual errors and reasoning bottlenecks appear when environment complexity grows.",
            "baseline_comparison": "Strongly outperforms random baselines and generally outperforms other private and open-source models in VSP experiments.",
            "ablation_results": "Textual replacement of visual input boosts performance further, indicating perceptual encoding still contributes to residual errors; in-context examples produced marginal gains beyond zero-shot.",
            "key_findings": "GPT-4o's internal representation effectively fuses visual embeddings and language for spatial and object-relational reasoning at short-to-medium horizons, but like other LLMs its implicit procedural/world-model capabilities are not robust across increasing plan length or visual ambiguity.",
            "uuid": "e372.4",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLaVA (evaluated)",
            "name_full": "LLaVA (LLAVA-V1.6-VICUNA-7B) evaluated in VSP",
            "brief_description": "An open-source multimodal model built by projecting image embeddings into the LLM space (CLIP encoder + LLaMA family) that performs poorly zero-shot on VSP tasks but shows substantial gains when fine-tuned on VSP data.",
            "citation_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
            "mention_or_use": "use",
            "model_name": "LLaVA (LLAVA-V1.6-VICUNA-7B)",
            "model_size": "7B",
            "model_description": "Open-source VLM using a CLIP visual encoder to project image features into the LLM token space (LLaMA-derived Vicuna), evaluated zero-shot and after LoRA fine-tuning on 10k VSP examples.",
            "task_name": "Maze Navigation & Blocks World (VSP)",
            "task_description": "See VSP Maze and Blocks World; LLaVA evaluated zero-shot and after supervised fine-tuning.",
            "task_type": "navigation + object-manipulation / multi-step planning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pretrained visually aligned encoder (CLIP) + LLM weights; fine-tuning on VSP supervised examples (10k) improves performance",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting and supervised fine-tuning (LoRA on 10k examples)",
            "knowledge_representation": "Visual features from CLIP mapped into LLM embedding space (implicit representation), plans as NL action sequences; after fine-tuning, model internal representations adapted to VSP tasks.",
            "performance_metric": "subtask accuracy; main-task success rate",
            "performance_result": "Zero-shot subtasks (Table 3): Maze T1 0.49, T2 0.27, T3 0.21, T4 0.54; Blocks T1 0.22, T2 0.21, T3 0.24, T4 0.55 (near random). After fine-tuning on 10k examples, LLaVA showed large improvements (paper reports substantial gains; LLaVA improved more than InternLM in their fine-tuning experiments).",
            "success_patterns": "After task-specific fine-tuning, the model learned mapping from visual embeddings to structured state descriptions and produced more correct procedural plans.",
            "failure_patterns": "Zero-shot performance is generally poor (close to random) due to limited context window, single-image training bias, and weaker multimodal alignment.",
            "baseline_comparison": "Pre-fine-tuning performance near random baseline; post-fine-tuning significantly better, demonstrating supervised adaptation importance.",
            "ablation_results": "Fine-tuning (10k examples) is an effective intervention to overcome initial perception/format limitations; context-length and multi-image handling remain architecture constraints.",
            "key_findings": "Open-source visual encoders projecting into LLM space (CLIP-&gt;LLaMA) store spatial/object-relational information in image embeddings, but without task-specific alignment/fine-tuning these representations are insufficient for reliable embodied planning; supervised fine-tuning can adapt these embeddings to produce correct procedural outputs.",
            "uuid": "e372.5",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Textual-replacement ablation",
            "name_full": "Ablation: Replacing visual input with explicit textual or tabular state descriptions (VSP experiments)",
            "brief_description": "An ablation where images are replaced by perfect textual descriptions or tables of the environment to separate perception and reasoning bottlenecks; this yields significant performance gains and reveals residual reasoning errors.",
            "citation_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
            "mention_or_use": "use",
            "model_name": "applies to evaluated VLMs (Gemini, GPT-Vision, etc.)",
            "model_size": null,
            "model_description": "Experimentally created textual or tabular encodings of the same information present in images (player/holes/goal coordinates for maze; explicit stack contents for BlocksWorld) used as input in place of images.",
            "task_name": "Ablation: Visual -&gt; Textual Input (VSP)",
            "task_description": "For every image input, a corresponding textual description (or table for maze) is provided as input in lieu of the image to test model performance given perfect perception.",
            "task_type": "diagnostic (perception vs reasoning)",
            "knowledge_type": "spatial+procedural+object-relational (encoded explicitly as NL or table)",
            "knowledge_source": "explicit textual encoding generated from ground-truth environment state",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting (with explicit textual state in input) and evaluation of performance gains relative to vision-input experiments",
            "knowledge_representation": "Explicit natural-language or tabular descriptions encoding exact positions, stacks, and relations; model uses its language-only capacities to reason about state transitions and produce plans.",
            "performance_metric": "delta in success rate / accuracy when using text/table input versus image input",
            "performance_result": "Paper reports clear, consistent performance improvement across every task when textual input replaces images (Figure 5); indicates perception is a leading cause of failures. Exact quantitative deltas vary by model and task; authors note Gemini's reasoning still inadequate on some tasks even with textual inputs.",
            "success_patterns": "When provided explicit symbolic state, models are better able to produce correct procedural plans and answer relational queries — perception errors vanish.",
            "failure_patterns": "Even with perfect perception, some models still fail multi-step reasoning tasks, indicating separate limitations in implicit procedural/world-modeling.",
            "baseline_comparison": "Vision-input zero-shot performance vs text-input zero-shot performance (text consistently higher); demonstrates that perception (vision→embedding) is a major bottleneck in current VLM planning pipelines.",
            "ablation_results": "Textual replacement is the core ablation; it improves performance markedly, isolating reasoning as a residual bottleneck. In-context examples provide limited further benefit.",
            "key_findings": "Replacing vision with explicit symbolic state shows that VLMs often possess the language-level machinery to express plans but fail primarily due to imperfect visual encoding and, secondarily, due to brittle multi-step procedural reasoning.",
            "uuid": "e372.6",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning",
            "rating": 2,
            "sanitized_title": "look_before_you_leap_unveiling_the_power_of_gpt4v_in_robotic_visionlanguage_planning"
        },
        {
            "paper_title": "Lgmcts: Language-guided monte-carlo tree search for executable semantic object rearrangement",
            "rating": 2,
            "sanitized_title": "lgmcts_languageguided_montecarlo_tree_search_for_executable_semantic_object_rearrangement"
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "Saycanpay: Heuristic planning with large language models using learnable domain knowledge",
            "rating": 1,
            "sanitized_title": "saycanpay_heuristic_planning_with_large_language_models_using_learnable_domain_knowledge"
        },
        {
            "paper_title": "Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_good_path_planners_a_benchmark_and_investigation_on_spatialtemporal_reasoning"
        },
        {
            "paper_title": "Layoutgpt: Compositional visual planning and generation with large language models",
            "rating": 1,
            "sanitized_title": "layoutgpt_compositional_visual_planning_and_generation_with_large_language_models"
        }
    ],
    "cost": 0.020954,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs
2 Jul 2024</p>
<p>Qiucheng Wu qiucheng@ucsb.edu 
Handong Zhao 
Adobe Research</p>
<p>Michael Saxon 
Trung Bui 
Adobe Research</p>
<p>William Yang Wang 
Yang Zhang 
MIT-IBM Watson AI Lab</p>
<p>Shiyu Chang 
Uc Santa Barbara 
VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs
2 Jul 20244F4B059B9FCFF061BF9CA5582A269EC4arXiv:2407.01863v1[cs.CL]
Vision language models (VLMs) are an exciting emerging class of language models (LMs) that have merged classic LM capabilities with those of image processing systems.However, the ways that these capabilities combine are not always intuitive and warrant direct investigation.One understudied capability in VLMs is visual spatial planning-the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes.In our study, we introduce VSP, a benchmark that 1) evaluates the spatial planning capability in these models in general, and 2) breaks down the visual planning task into finer-grained sub-tasks, including perception and reasoning, and measure the LMs capabilities in these sub-tasks.Our evaluation shows that both open-source and private VLMs fail to generate effective plans for even simple spatial planning tasks.Evaluations on the fine-grained analytical tasks further reveal fundamental deficiencies in the models' visual perception and bottlenecks in reasoning abilities, explaining their worse performance in the general spatial planning tasks.Our work illuminates future directions for improving VLMs' abilities in spatial planning.Our benchmark is publicly available at https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.Preprint.Under review.</p>
<p>Introduction</p>
<p>The rapid advancement of large language models has driven considerable growth in their capabilities to produce fluent text in many domains, generating outputs exhibiting potential "reasoning" and "understanding" abilities [1,2,3,4].Recently, vision language models (VLMs) have advanced on LMs through additional training on native image inputs, to achieve impressive performance generating text describing and relating to input images [5,6,7,8,9], with applications in image captioning, visual question answering, visual reasoning, and others [10,11,12,13].The swift evolution of VLMs has enabled them to tackle increasingly sophisticated tasks that require multiple emerging abilities in complex scenarios.However, as model capabilities and deployment needs advance, the challenges in usefully evaluating them grow in kind.</p>
<p>Planning is a fundamental capability in intelligent systems that is particularly contested in LMs [14], and is understudied in VLMs.Visual spatial planning refers to the task of comprehending the spatial arrangement of objects in a scene and designing action plans to achieve a desired outcome.For example, the classical maze problem can be considered a visual planning task, where an agent is given an input image describing the maze environment and is asked to produce a viable path to navigate the player from the starting position to the goal.This task requires two capabilities: image perception, which enables the agent to understand the objects, environment and spatial relations present in the image, and reasoning, which enables the agent to perform strategic decision-making.</p>
<p>Visual spatial planning is an important capability in many potential applications for VLMs, such as navigating in complex environments with autonomous driving [15,16] or manipulating objects with robotic hands [17,18].Though there have been increasingly more benchmarks to evaluate the vision processing capabilities of VLMs, few current benchmarks systematically evaluate their capability to perform visual spatial planning tasks.As shown in Table 1, existing benchmarks mostly focus on VLMs' ability to understand image content and perform visual logic reasoning [19,20,21]; however, they often overlook the ability to comprehend the spatial arrangements of entities within images and to devise spatial action plans based on practical restrictions in the visual environment.As a result, two research questions are left unanswered: ❶ How performant are VLMs in performing visual planning tasks?❷ What are the bottleneck capabilities, e.g., perception or reasoning, that limit the performance of VLMs in the visual planning tasks?</p>
<p>To this end, we introduce Visual Spatial Planning (VSP), a benchmark specifically designed to evaluate the spatial planning capabilities of VLMs.As illustrated in Figure 1 and Figure 2, the VSP benchmark is developed from classical maze navigation and block-moving games, where the entire environment is fully observable in the input images.In this benchmark, the VLMs are required to interpret the visual inputs, deduce the consequences of each action, and execute the designated tasks accordingly.To comprehensively evaluate the fine-grained capabilities needed for the visual spatial planning, our VSP includes 4.4K questions in 10 meticulously designed tasks that feature both simulated and photo-realistic visual environments.In addition to testing end-to-end spatial planning performance, these tasks further evaluate essential individual capabilities needed for performing visual planning, such as image perception and reasoning.</p>
<p>We apply the VSP benchmark to evaluate existing state-of-the-art VLMs, including both opensource and private models.Surprisingly, we find that even the most competitive VLMs sometimes struggle in performing the simplest visual planning tasks, such as a 3x3 maze problem or an one-step block-moving task.Our fine-grained capability analysis further reveals that existing VLMs have flaws in reasoning and bigger bottlenecks in perception.We believe the VSP benchmark highlights critical weaknesses in current VLMs and sheds light on future directions for enhancing their spatial understanding and planning capabilities.</p>
<p>Related Work 2.1 General planning in LMs</p>
<p>Planning has been a central focus of research in AI.Traditional work in AI planning includes using formal languages to represent and solve planning problems [24], and developing algorithms like dynamic programming and reinforcement learning to explore environments and formulate viable plans [25,26].While these works mostly focus on predefined and restricted environments, recently, with the advancement of LMs, it has become intriguing to study whether LMs, with the potential to be general intelligent agents, can perform planning in different settings and environments [27,28,29].Many works explore the best ways to activate the planning capabilities of LMs, including divide and conquer [30,31,32,33], grounding outputs in admissible actions [34,35], retrospecting and refining [36,37], and leveraging external tools [38,39].Meanwhile, with the increasing capabilities of LMs, growing research efforts are now dedicated to benchmark their planning capabilities in various complex environments [40,41,42].</p>
<p>Main Task</p>
<p>Sub-Tasks
Spatial</p>
<p>Spatial and visual planning in LMs</p>
<p>Many general planning tasks in LMs involve understanding visual environments and comprehending spatial information.In robotics and embodied agent studies, LMs play a crucial role in grounding visual entities with references in open-domain instructions and formulating plans based on spatial constraints.Consequently, they are increasingly used in physically grounded scenarios such as object rearrangement [17,18], cooking [43,44], and navigation [35,34].LMs are also used in AIGC to propose spatial arrangements of entities following instructions [45].While realistic planning tasks align with real needs, their complexity and expansive action spaces limit the analysis of LMs' detailed planning capabilities.Therefore, research also focuses on LMs' planning in simulated environments and games.For example, mystery blocksworld is a dynamically generated set of blocksworld tasks to test generalization in LMs [14].Additionally, many text games have been introduced to test LMs' abilities in spatial understanding and imagination [46,40,47,48].However, most of these studies transform visual information into text inputs, thus not directly measuring LMs' visual abilities.</p>
<p>Benchmarks for VLMs</p>
<p>VLMs have inherited and advanced many intriguing features from text-only LMs [47,49].Benchmarks for VLMs have rapidly emerged to evaluate performance in areas such as image content understanding [19,50], perception [51,52], knowledge [20,21,53], and reasoning [19,20,54].Recently, there are also emerging benchmarks focusing on the capability of understanding multiple images in long context and complex realistic environments [55,56].While these benchmarks successfully quantify VLMs' abilities in many fields, their capabilities in spatial understanding and reaction are relatively under-explored.Some benchmarks cover spatial relations understanding [22,23], but often overlook the ability to devise complex spatial action plans based on visual environment constraints.We focus on visual spatial planning -the ability to comprehend spatial arrangements of objects and devise action plans to achieve specific outcomes.We fill the gap in benchmarking VLM abilities for visual spatial planning and highlight future directions for improving VLMs towards models with general intelligence.</p>
<p>3 The Visual Spatial Planning Benchmark</p>
<p>Overview of the Benchmark</p>
<p>In this benchmark, our objectives are two-fold: ❶ quantify the visual spatial planning capabilities of current VLMs; and ❷ uncover current capability bottlenecks that limit the effectiveness of VLMs in visual spatial planning tasks.While the first objective can be achieved through direct measurements on corresponding tasks, the second objective requires more careful benchmark design.Specifically, performing spatial planning in visual environments requires a series of cohesive steps.For example, to generate an accurate path to navigate a player to a goal, an agent needs to be able to correctly view and understand the visual map, reason to find which actions are safe or dangerous, and come up with a detailed plan to achieve the goal.Each of these steps could be challenging for a developing VLM, and understanding which of these subtasks challenge them most will drive future improvement.To this end, we propose the Visual Spatial Planning (VSP) benchmark, with the objective of measuring and diagnosing the capabilities of VLMs in producing accurate spatial plans in visual environments.The VSP benchmark consists of two scenarios: ❶ the simulated Maze Navigation scenario, whose main task is to move a game character through a maze, and ❷ the photo-realistic Blocks World scenario, whose main task is to move blocks from a starting configuration to a goal configuration.In each scenario, in addition to the main task, VSP introduces four sub-tasks that focus on the individual capabilities needed for the main task:</p>
<p>Main Task</p>
<p>Sub-Tasks</p>
<p>• T1.Single Object Perception -Determine the characteristics of a single object;</p>
<p>• T2.Spatial Relation Perception -Determine the relative positions of two objects;</p>
<p>• T3.Environment Perception -Find textual descriptions that describe the visual environment;</p>
<p>• T4.Spatial Reasoning -Determine the consequence of a series of actions or moves.</p>
<p>The sub-task details are designed specific to each scenario.Furthermore, to demonstrate the model's performance under different levels of environmental complexity, we establish progressive difficulty settings for each task, which are measured by parameters such as map size, minimum required number of actions, etc.We provide the details of task statistics, i.e., total number of problems, in appendix A. In what follows, we will introduce each scenario in detail, as well as the data curation and the task creation processes.</p>
<p>The Maze Navigation Scenario</p>
<p>The maze navigation scenario is inspired by the popular implementation [57] of a fully observable path-finding problem.As depicted in Figure 1 left, it simulates a classical grid world environment with a designated start and goal position, where part of the grids contain obstacles (the "holes") and cannot be passed through.</p>
<p>The main spatial planning task and the four sub-tasks are defined as follows:</p>
<p>• Main Task (Spatial Planning) -Generate a safe path to navigate from the start grid to the goal;</p>
<p>• T1 (Single Object Perception) -Determine if a specified grid is safe;</p>
<p>• T2 (Spatial Relation Perception) -Find spatial relations between the player and the goal;</p>
<p>• T3 (Environment Perception) -Find the textual description that fits the visual environment;</p>
<p>• T4 (Spatial Reasoning) -Determine the consequence of a given action series.</p>
<p>An example of input image and questions is demonstrated in Figure 1.Each task is equipped with progressive adjusted difficulty settings to evaluate the model's capability under various circumstances.For Main Task and T1-T3, the difficulties are measured by the size of the map, ranging from 3x3 to 8x8, where a larger map introduces more challenges in correctly perceiving objects and planning accordingly.For task T4, since a longer path naturally introduces more challenges for reasoning, we adopt path length ranging from 1 to 9 as the difficulty measure.Please refer to Appendix A for the complete example of the question and answer in each task.</p>
<p>The Blocksword Scenario</p>
<p>Blocksworld is a widely-adopted planning problem [42,58,59].As depicted in Figure 2 left, in this scenario, the agent is given images containing sets of blocks in unique colors.These blocks are stacked vertically, forming multiple stacks on the table.The agent is asked to turn the blocks from initial state to target state through a series of moving actions.For each action, the agent can only move the top block of any stack, providing it is moved to either the table or the top of another stack.</p>
<p>Similarly, the main spatial planning task and the four sub-tasks are defined as follows:</p>
<p>• Main Task (Spatial Planning) -Form a moving plan to achieve the target state of block arrangement;</p>
<p>• T1 (Single Object Perception) -Determine the color of the block at a specific position;</p>
<p>• T2 (Spatial Relation Perception) -Determine the spatial relation between two blocks;</p>
<p>• T3 (Environment Perception) -Find the text representation that fits the visual environment;</p>
<p>• T4 (Spatial Reasoning) -Determine the consequence of a given moving plan.</p>
<p>An example of input image and questions is demonstrated in Figure 2. Similar to the maze navigation scenario, each task is equipped with progressive adjusted difficulty.Specifically, in Main Task and T4, the difficulties are measured by the number of actions involved, ranging from 1 to 7, which quantifies the complexity of the action plan.On the other hand, for tasks T1-T3, which focus on perception, the difficulty is measured by the number of blocks presented in the image, ranging from 3 to 5. Please refer to Appendix A for the complete example of the questions in each task.First, in the left panel of Figure 3, we prepare the input images used for each task and scenario.In the maze navigation scenario, we generate input maps using the OpenAI Gym package [57], with modifications to ensure that the positions of the player, the goal, and the holes are all randomly generated.In the blocksworld scenario, we sample pairs of images from the BIRD dataset [59], ensuring there is at least one viable plan to move the blocks from the initial state to the target state.The images are prepared conditional on different levels of difficulty.</p>
<p>Benchmark Creation Process</p>
<p>Second, in the center panel of Figure 3, we formulate input prompts for each task.The prompt consists of interleaved text and images to provide sufficient information.For example, for maze navigation, we include images to show the appearance of elements in the map and provide example maps to better illustrate how the models should interpret the map.We invite native speakers to refine Gemini [7] 0.31 0.26 0.15 0.06 0.14 0.10 0.10 0.14 0.00 0.01 0.13 GPT-Vision [5] 0.55 0.36 0.27 0.13 0.17 0.10 0.50 0.17 0.03 0.00 0.23 Claude-3 [60] 0.52 0.33 0.16 0.15 0.16 0.09 0.12 0.03 0.00 0.00 0.16 GPT-4o [61] 0.68 0.58 0.35 0.24 0.18 0.23 0.71 0.33 0.12 0.03 0.35</p>
<p>LLaVA [6] 0.03 0.03 0.02 0.08 0.09 0.04 0.04 0.01 0.00 0.00 0.04 InternLM [62] 0.27 0.16 0.06 0.05 0.04 0.07 0.10 0.03 0.00 0.00 0.08 InternLM-VL [62] 0.15 0.14 0.08 0.04 0.02 0.05 0.02 0.00 0.00 0.00 0.05 InstructBLIP [63] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 SPHINX [64] 0.11 0.08 0.05 0.02 0.04 0.03 0.07 0.06 0.01 0.00 0.05 the prompts so that they accurately describe the task requirements.These prompts are demonstrated in Appendix A.</p>
<p>Finally, in the right panel of Figure 3, we evaluate the performance of VLMs under each task.It is worth noting that the answer for each task is often not unique.For example, in the blocksworld scenario, there can be many ways to move the blocks to reach the target state.As such, we develop scripts to automatically evaluate the answers for each task.</p>
<p>In addition to the steps above, some tasks require extra steps to construct meaningful questions, candidates, and answers, such as prompt design or example filtering.For example, in task 5 of the blocksworld scenario, the input actions must cover various valid/invalid movements, requiring filtering and balancing.The detailed steps we followed to create each task set are provided in Appendix A. We release all images, texts, and scripts to facilitate replication and scaling.</p>
<p>Experiments</p>
<p>In this section, we present evaluation results of state-of-the-art VLMs under our main tasks and sub-tasks.Our goal is to answer the following research questions: ❶ How well can state-of-the-art VLMs perform in the visual spatial planning tasks?❷ What are the bottleneck capabilities that limit the VLMs in visual spatial planning tasks?</p>
<p>Baselines</p>
<p>We evaluate various representative VLMs including both private and open-source models.</p>
<p>We cover the following private models: ① Gemini [7] has demonstrated remarkable capabilities in image understanding and reasoning.We adopt Gemini-1.0-Pro-Vision in our experiments 1 .② GPT-4 Turbo with vision [5] inherent strong text understanding capabilities from GPT-4 and is equipped with vision capabilities.We use turbo-2024-04-09 for evaluation.③ Claude-3 [60] is a family of VLMs strong at advanced reasoning and vision analysis.We adopt claude-3-sonnet-20240229, the default model used in chat interface and has comparable speed &amp; cost with GPT Vision.④ GPT-4o [61] is a recently released multimodal LM with one of the most advanced abilities in processing combination of text, audio, and image outputs.We adopt gpt-4o-2024-05-13 in experiments.</p>
<p>We cover the following open-source models: ⑤ LLaVA [6] performs instruction tuning on LLaMA and projects input image into text embedding space through pre-trained CLIP visual encoder [65].We adopt LLAVA-V1.6-VICUNA-7Bfor evaluation.⑥ InternLM-XComposer2 [62] enhances ability to understand free-form text-image composition and surpasses GPT-4V in several tasks.The latest released checkpoints include internlm-xcomposer2-7b and internlm-xcomposer2-vl-7b, with the former focusing on general text-image composition and the latter focusing on VL bench-This is a 4x4 map.</p>
<p>The player is at: row 1, column 1; The hole(s) are at: row 1, column 2; row 4, column 1;</p>
<p>The goal is at: row 4, column 4.  marks.We adopt both for evaluation.⑦ InstructBLIP [63] is a popular VLM based on pre-trained BLIP-2 [66] model.We adopt blip2-t5-instruct-flant5xxl for evaluation.⑧ SPHINX [64] unfreezes the LLM during pre-training to enhance cross-model alignment.We adopt SPHINX-v2-1k for evaluation.Additionally, we attempt to perform measurements on the latest CogVLM2 [67] fine-tuned on LLaMA-3 [68].However, since its current codebase only supports single-image input, we do not include its results.For all open-source models, we use their public released checkpoints, codes, and hyperparameter choices.
| | Col 1 | Col 2 | Col 3 | Col 4 | | Row 1 | @ | # | _ | _ | | Row 2 | _ | _ | _ | _ | | Row 3 | _ | _ | _ | _ | | Row 4 | # | _ | _ | * | -Stack</p>
<p>Main Task (Spatial Planning) Evaluation</p>
<p>First, we present the main task evaluation results for both the maze and the Blocksword scenarios, which reflect the general spatial planning capabilities of existing VLMs.All the evaluation in this section is conducted under zero-shot setting without any fine-tuning or in-context learning.</p>
<p>Evaluations with in-context learning and fine-tuning are presented in Sections 4.5 and 4.6.</p>
<p>The performance is demonstrated in Table 2.Each column represents a difficulty level, which is measured by the size of the map (3 represents 3x3 maps) in Maze Navigation and by the minimum number of steps in Blocksword.From the table, we summarize our findings as follows:</p>
<p>VLMs have considerable room for improvement in spatial planning tasks.We observe that both private and open-source models exhibit sub-optimal performance in various scenarios.In particular, open-source models face significant challenges and rarely succeed in these tasks.Besides, even the most capable private models could frequently make mistakes on relatively simple tasks, such as those involving a 3x3 size map or a single-step block moving task.Considering that these tasks would be simple for humans, the VSP benchmark poses a substantial challenge to VLMs, illustrating that current VLMs have considerable potential for improvement in spatial planning tasks.</p>
<p>Quick performance decay as difficulty increases.We observe a significant drop in the success rates of VLMs as task difficulty escalates.For example, GPT-Vision may achieve a success rate of over 50% on 3x3 size maps, but this plummets to just 10% on 8x8 maps.Analyzing the impact of increased difficulty, we identify two major challenges for the models: First, increasing size of the map in maze navigation scenario could make it difficult for the model to accurately perceive the positions of elements within the map.Second, the increase in both map size and the number of steps required for moving blocks heightens the challenge for the model to reason deeply through the entire path and devise a complete, viable solution.In the following experiments, we focus on these two factors and provide in-depth analysis with subsequent tasks.</p>
<p>Challenges in open-source models.Finally, we note that open-source models often face challenges when evaluating on these tasks.We identify two main factors.① Context length: Open-source models typically have significantly shorter context windows compared to private models.Besides, image embeddings can occupy many tokens.Thus, these models may not have enough capacity to understand the complete inputs.For example, LLAVA-V1.6-VICUNA-7B is trained with a maximum context window of 2048 tokens, while each image consumes 576 tokens.Consequently, when fed with multiple images and relatively long texts in our tasks, the total token length may surpass training, resulting in poor performance.② Multiple image input: Our tasks require the model to understand multiple images interleaved with text inputs, whereas many open-source models are only trained with single-image inputs, with the image positioned at the start of the input.To further explore their potential in our tasks, we assess their performance after training on our inputs in Section 4.6.</p>
<p>Meanwhile, we suggest that future open-source models could consider increasing their context length and reducing restrictions on input formats to address complex and realistic tasks effectively.From the previous observation, we identify that spatial perception and reasoning could be two important capabilities for an agent to successfully perform visual spatial planning.Next, we evaluate the perception and reasoning abilities through the remaining tasks T1-T4.Similar to previous setting, all the evaluation is conducted under zero-shot settings.</p>
<p>The performance results are presented in Table 3.We observe that the recent GPT-4o and GPT-Vision achieve good performance across a series of tasks, demonstrating a decent capability in perception and reasoning.However, the overall performance of private models hovers around 50% accuracy, which is far from satisfactory for agents requiring spatial intelligence.Furthermore, the performance of open-source models is mostly close to random guessing on these tasks, indicating a significant gap compared to private models.Besides, we note that tasks T1-T3 focus on perception abilities, and task T4 involves the ability of both understanding input images and perform reasoning.We perform further analysis to disentangle these two abilities in Section 4.4.</p>
<p>The Effects of Visual Input Perception and Reasoning</p>
<p>Previous analysis shows that even current state-of-the-art models have clear deficiencies in various aspects of visual spatial planning.In this study, we focus on disentangling the effects of perception and reasoning by exploring the performance gain assuming the model had perfect perception.</p>
<p>The key strategy of this study is to create a scenario where the model has already acquired all the necessary information that would typically be obtained through visual perception.To this end, for every input image, we produce the corresponding textual inputs and replace those images, as demonstrated in Figure 4.For the maze navigation scenario, we use either pure text descriptions or tables to depict the image.For the blocks world scenario, we use pure text descriptions.We do not use tables for the blocks world scenario because the number of blocks in each horizontal stack is usually unequal, making it difficult to form a complete table.Please refer to Appendix B for a complete example with pure text or table input.</p>
<p>The results are shown in Figure 5.We observe a clear performance improvement when using textual input across every task.This suggests image perception presents significant challenges for VLMs, and poor perception ability is a key factor in the inferior performance observed in previous tasks.Meanwhile, we observe that even with textual input, Gemini still cannot achieve decent performance on tasks that require reasoning.This indicates deficiencies in its reasoning capabilities as well.</p>
<p>In-context Learning in Visual Spatial Planning</p>
<p>In-context learning is a widely-adopted method to enhance LM's reasoning ability [4].In this analysis, we study if it boosts the visual spatial planning capabilities.We included varying numbers 0 .97 0 .90 0 .9 1 0 .7 7 0 .9 3 0 .9 5 0 .9 2 0 .7 0
GPT-V T1 Gemini GPT-V T2 Gemini GPT-V T4 Gemini GPT-V</p>
<p>Maze Navigation Scenario</p>
<p>Image Text Table
GPT-V T1 Gemini GPT-V T2 Gemini GPT-V T4 Gemini GPT-V</p>
<p>Fine-tuning in VSP Tasks</p>
<p>Finally, we assess the capabilities of the open-source model through dedicated training for each task.</p>
<p>In these tasks, the model is trained on 10k data points.We use the default hyperparameters provided in the official repo.The results, shown in Table 5, demonstrate clear performance improvements for both models across a series of tasks, highlighting their potential in spatial planning.Additionally, we observe that LLaVA shows greater improvement compared to InternLM, suggesting that different model architectures may exhibit varying levels of efficacy in spatial planning capabilities.</p>
<p>Conclusion</p>
<p>We present VSP, a benchmark measuring and diagnosing the visual spatial planning capabilities in VLMs.The VSP quantifies the model's performance through a series of carefully designed tasks, with main tasks focusing on the general spatial planning abilities and sub-tasks focusing on the individual capabilities needed for the main task.Experiments on current models show that both private models and open-source models fail to generate effective plans for even simple spatial planning tasks, and further analyses expose their bottlenecks in spatial perception and reasoning abilities.Our work illuminates future directions for improving VLMs' abilities in spatial planning.</p>
<p>A Details of Benchmarks A.1 Additional Task Implementation Process and Statistics</p>
<p>In Section 3.4, we described our general benchmark creation process.Our task creation process can be divided into three stages: (a) preparing the input image; (b) formulating task prompts; (c) developing scripts for auto evaluation.Additionally, some specific tasks require extra steps to implement, such as question and answer generation.In what follows, we describe these steps in detail.</p>
<p>Maze Navigation, Main Task In this task, agents need to find a safe path to navigate from the start grid to the goal.We adjust the map generation mechanisms to ensure the positions of the start grid, the goal, and the holes are all randomly generated, while ensuring there is at least one viable safe path from the start grid to the goal.For each grid, the probability that it contains the hole is 20%.</p>
<p>Maze Navigation, T1 In this task, agents need to identify whether a specific grid is safe (i.e., whether it contains a hole or not).We randomly sample a row number and a column number and ask the safety question for this randomly chosen grid in each problem of this task.Additionally, to prevent the model from patterned guessing and achieving falsely high ratings (e.g., answering "not safe" for all images and obtaining high accuracy scores), we regenerate the map for this task to ensure that the safe and unsafe grids each comprise around 50% of the total grids in a single map.</p>
<p>Maze Navigation, T3 In this task, agents need to find the correct textual description that fits the visual environment.For each problem, we prepare four textual description candidates.One candidate is the correct answer, one candidate has the correct size but an incorrect map arrangement, and the other two candidates have the wrong size.The candidates are shuffled to prevent the model from making random guesses.</p>
<p>Maze Navigation, T4 In this task, agents need to determine if the given action series is safe or not.Similarly, to prevent the models from achieving falsely high ratings through guessing, we generate the action series to ensure that around 50% of them are safe and the other 50% are not.For the unsafe paths, the particular step in which the player steps into a hole is also randomly chosen.</p>
<p>Blocks World, T2 In this task, agents need to determine the spatial relation between two designated blocks.In addition to the directional relation ("above" and "below"), we note that it is important for agents to recognize if two blocks are at the same stack or not.Therefore, we design the following four candidates for this question: (A) The first block is directly above the second block, and they are in the same stack; (B) The first block is directly below the second block, and they are in the same stack; (C) The two blocks are at different stacks; (D) At least one of the mentioned blocks do not exist in the presented image.</p>
<p>Blocks World, T4 In this task, agents need to determine the consequence of a given moving plan.Specifically, some invalid moving plans contain actions that cannot be executed in the given scenario, such as trying to move a block that is covered by another block.To prevent guessing in this task, similar to the maze navigation scenario, we generate the action plans to ensure that half of the plan candidates are executable.For the plans that cannot be executed, there can be two types of errors: first, the plan may include steps that involve moving a block from or to an invalid position; second, the plan may try to move a block that does not exist.We randomly generate these errors in inputs.</p>
<p>Statistics</p>
<p>The VSP benchmark consists of 10 tasks in two scenarios.For each task, the problems are designed with different difficulty levels.Specifically, each difficulty level consists of 100 problems.In total, the VSP benchmark includes 4.4k questions.</p>
<p>A.2 Complete Prompt</p>
<p>In this subsection, we provide the complete prompts for each task.Generally, the prompts consist of a general task description at the beginning and a specific question at the end.The prompts interleave text and images in a pattern similar to a human-readable manual with reference figures.</p>
<p>Prompt for Maze Navigation scenario, Main task (Spatial Planning):</p>
<p>As a professional maze solver, your task is to analyze a grid-based map and devise an action plan that enables a player to reach the goal from the starting point without falling into any holes, using the fewest possible moves.Since coding is not within your skill set, your approach relies on logical reasoning of the map.</p>
<h2>Game Setup -The game presents a fully observable grid-based map.</h2>
<p>-The player starts at a specified grid square, with the goal located elsewhere on the map.</p>
<p>-Each grid square is either safe or contains a hole.</p>
<p>-Your goal is to guide the player to the goal while avoiding holes.</p>
<p>The following figure shows how the player, the holes (non-safe grid), the lands (safe grids), and the goals look like.</p>
<h2>Moving Rules -The action plan involves a series of moves: 'L' (left), 'R' (right), 'U' (up), or 'D' (down).</h2>
<p>-Each move transfers the player to the adjacent square in that direction, provided it is a safe square.The player cannot move more than one square at a time.</p>
<p>-Moving off the edge of the map has no effect.The player will remain at the same square.</p>
<p>-DO NOT MOVE INTO A HOLE! Falling into a hole results in defeat.</p>
<p>-Locating at the grid containing the goal results in victory.</p>
<p>We provide an example to further illustrate the rules.</p>
<p>In this provided example: -The player is at Row 1, Column 1; -The goal is at Row 4, Column 4; -There are two holes: one at Row 1, Column 2, and another at Row 4, Column 1.</p>
<p>-The player can move DOWN.This is because moving down brings them to Row 2, Column 1, and this cell is safe (without holes).</p>
<p>-Moving UP has no effects.This is because the player is already in the topmost row.</p>
<p>-Similarly, moving LEFT has no effects because the player is already in the left-most column.</p>
<p>-Moving RIGHT places the player at Row 1, Column 2. Since there is a hole at this grid, this move results in a loss.</p>
<h2>Procedure and Output Now you will solve the given maze.To solve it, please generate text exactly follow the following steps: 1.First, interpret map.List where the player is at now, where is the goal, and where are the holes.4. If succeed, output an aggregated plan using "Action plan: <PLAN>", where <PLAN> is a string concatenated action in each step.For example, "Action plan: L,L,R,U,D" meaning an action plan of left, left, right, up, and down.Double check the final action plan is consistent with the previous analysis.Do not output any extra content after the above aggregated output.</h2>
<p>Please generate the action plan for the following maze:</p>
<p>Prompt for Maze Navigation scenario, Task 1 (Single Object Perception):</p>
<p>In this task, you will analyze a maze to determine if there is a hole in a specific position.</p>
<p>The following figure illustrates the appearances of the player, holes, lands, and the goal within the maze.You will need to focus on the appearance of the hole.</p>
<p>is an example to illustrate how to analyze and answer the question:</p>
<p>Example question: Is there a hole in row 3, column 3?</p>
<p>In this example:</p>
<p>-We check the position in row 3, column 3.</p>
<p>-According to the image, it is a land square.It does not contain a hole.</p>
<p>-Therefore, you will output "<Output> No".</p>
<p>Your output should be: "<Output> No" or "<Output> Yes", depending on whether there is a hole at the specified position.Now you will analyze the following maze and answer the question: Is there a hole in row 2, column 1?</p>
<p>Prompt for Maze Navigation scenario, Task 3 (Environment Perception):</p>
<p>In this task, you will analyze a maze presented in an image.Later, you will be presented with four choices, each offering a textual representation of a candidate maze.You will need to choose the representation that exactly reflects the contents of the given image.</p>
<p>The following figure illustrates the appearances of the player, holes, lands, and the goal within the maze in the image.</p>
<p>This is how the player, the holes (non-safe grid), the lands (safe grids), and the goals look like in a map:</p>
<p>-The player is represented as "@" -The hole is represented as "#" -The safe grid is represented as "_" -The goal is represented as "*" -If the player is at the goal (at this case the game is solved), that grid is represented as "%"</p>
<p>We provide an example to illustrate how to interpret the input, candidates, and answer the question.Here is the image input:</p>
<p>Here are the textual candidates:
(A) | | Col 1 | Col 2 | Col 3 | | Row 1 | # | _ | _ | | Row 2 | # | @ | # | | Row 3 | _ | * | _ | (B) | | Col 1 | Col 2 | Col 3 | Col 4 | Col 5 | | Row 1 | _ | _ | _ | _ | _ | | Row 2 | _ | # | _ | _ | _ | | Row 3 | _ | # | * | _ | # | | Row 4 | _ | @ | _ | _ | _ | | Row 5 | _ | _ | _ | # | _ | (C) | | Col 1 | Col 2 | Col 3 | Col 4 | | Row 1 | @ | # | _ | _ | | Row 2 | _ | _ | _ | _ | | Row 3 | _ | _ | _ | _ | | Row 4 | # | _ | _ | * | (D) | | Col 1 | Col 2 | Col 3 | Col 4 | | Row 1 | _ | _ | _ | _ | | Row 2 | * | _ | _ | _ | | Row 3 | @ | _ | # | _ | | Row 4 | _ | _ | _ | # |
Here is an example of how to analyze and answer the question:
(continue in next page) (Continued)
-First, we focus on the difference of the maze shape between the candidates and the input image.</p>
<p>-We begin by examining the input image.It is a 4-by-4 maze.We then review the candidates.Candidate A is a 3-by-3 maze.Therefore, it is not the correct answer.Similarly, Candidate B is a 5-by-5 maze, which also cannot be correct.Both Candidate C and Candidate D are 4-by-4 mazes.Now we only need to choose from them.</p>
<p>-For the remaining candidates, we compare the positions of the players, goals, and the holes in the maze.</p>
<p>-We first check the input image.What is the position of the player in the image?The player is in row 1, column 1.We then check the remaining candidates.For Candidate C, the textual representation indicates the player is also at row 1, column 1, matching the input image.For Candidate D, the player is located at row 3, column 1.Hence, Candidate D is not the correct answer.</p>
<p>-We double check the remaining Candidate C, and it correctly shows the position of the player, holes, and the goal.It is therefore the correct answer.<Answer> C</p>
<p>Your output should consist of two parts: 1.First, analysis the input image and candidates similar to the reasoning process above.</p>
<ol>
<li>Following the reasoning process, output answer as "<Answer> <Choice>", where "<Choice>" is one of A,B,C,D.Important: Note that there will be only one correct answer.If you find no answer or multiple answers, you must go back and recheck your reasoning process.You are not allowed to provide 0 or more than 1 answer.Now answer the question below.Here is the image input:</li>
</ol>
<p>Here are the textual candidates: <CANDIDATES> Prompt for Maze Navigation scenario, Task 4 (Spatial Reasoning):</p>
<p>You are a maze-solving agent playing a pixelated maze video game.Mazes are presented on grid maps, where each tile can be empty land, or contain a player, hole, or goal.Each of the above tile types are represented as square pixel art images.</p>
<p>In this task, you will analyze a grid-based map and determine if a provided action plan is safe.A safe action plan avoids stepping into holes in the map.</p>
<p>The following figure illustrates the appearances of the player, holes, lands, and the goal within the maze.</p>
<h2>Moving Rules -The action plan involves a series of moves: 'L' (left), 'R' (right), 'U' (up), or 'D' (down).</h2>
<p>-Each move transfers the player to the adjacent square in that direction, provided it is a safe square.The player cannot move more than one square at a time.</p>
<p>-Moving off the edge of the map has no effect.The player will remain at the same square.</p>
<p>-DO NOT MOVE INTO A HOLE! Falling into a hole results in defeat.</p>
<p>-Locating at the grid containing the goal results in victory.</p>
<p>We provide an example to further illustrate the rules.</p>
<p>(continue in next page)</p>
<p>Prompt for Block World scenario, Main Task (Spatial Planning):</p>
<p>You are a robot that sorts and organizes colored blocks by adding and removing them to stacks.You can move them between stacks to produce a desired end state.</p>
<p>In this task, you will see two photos of blocks.These photos show the beginning and end state of the blocks.Your task is to find a shortest movement plan to transit from the beginning state to the end state.Since coding is not within your skill set, your approach relies on logical reasoning of the map.</p>
<h2>Game Setup -The stacks of blocks are presented in images.You must view and interpret the image in order to determine which blocks are in which stack and determine how to move them.</h2>
<p>-Each block has a unique color (blue, yellow, purple, orange, red, green).</p>
<p>-Blocks are stacked vertically in a stack, forming multiple stacks.All stacks are on the table.</p>
<p>-In a single move, you can only move the top block of any pile.Attempting to move lower blocks is considered an invalid move.</p>
<p>-You can either (a) move the top block to the top of another stack, or (b) place the top block on the table, creating a new stack with just one block.</p>
<p>We provide an example to further illustrate the rules:</p>
<p>This example features four blocks arranged in three stacks:</p>
<ul>
<li><em>Important Note</em>*: The order of the stacks doesn't matter in this game.Two images are considered equivalent as long as the stacks contain the same blocks, regardless of the order in which the stacks appear.For example, an image with stack A on the left and stack B on the right is equivalent to an image with stack B on the left and stack A on the right.</li>
</ul>
<h2>Procedure and Output</h2>
<p>Your output should follow this format: 1.First, analyze the starting and ending configurations, including the number of stacks and the blocks in each stack (similar to the example above).2.Then, list the moves in a step-by-step manner using the format move(SOURCE, TARGET).Remember, "SOURCE" refers to the block being moved (always the top block of a stack), and "TARGET" refers to the destination (another stack or the table The end state is:</p>
<p>Prompt for Block World scenario, Task 1 (Single Object Perception):</p>
<p>In this task, you will see a photo of blocks.You will analyze the block configuration and then answer a question regarding the color of blocks in a specific place.</p>
<h2>Game Setup -Each block has a unique color (blue, yellow, purple, orange, red, green).</h2>
<p>-Blocks are stacked vertically in a stack, forming multiple stacks.</p>
<p>-In the questions, the position of the blocks is represented as "Stack s, Level l".The stack number is counted from left to right, and the level number is counted from bottom to top.</p>
<p>We provide an example to further illustrate the setting:</p>
<p>In this example, there are four blocks in three stacks.From left to right:</p>
<p>-Stack 1 has one level.Level 1 contains a purple block.</p>
<p>-Stack 2 has one level.Level 1 contains a blue block.</p>
<p>-Stack 3 has one level.From bottom to top: level 1 has an orange block, and level 2 has a red block.As such, for the question "What is the color of the block at stack 3, level 1?", the correct answer is "<Output> orange".</p>
<h2>Procedure and Output Your output should follow this format: 1.First, analyze the block configuration; 2.Then, answer the question with the format <Output> <Color>, where <Color> is one of (blue, yellow, purple, orange, red, green).For example, "<Output> red".</h2>
<p>Now please answer the following question based on the given image below:</p>
<p>What is the color of the block at stack 1, level 2?</p>
<p>Prompt for Block World scenario, Task 3 (Environment Perception):</p>
<p>In this task, you will analyze an image containing several stacks of blocks.Later, you will be presented with four choices, each offering a textual representation of a block configuration.You will need to choose the configuration that exactly reflects the contents of the given image.## Game Setup -Each block has a unique color (blue, yellow, purple, orange, red, green).</p>
<p>-Blocks are stacked vertically in a stack, forming multiple stacks.This is an image input example:</p>
<p>This example features four blocks arranged in three stacks:</p>
<ul>
<li>We can analyze which text representation exactly reflects the configurations in the image accordingly.In this example:</li>
</ul>
<p>-The input image has 3 stacks, while Candidate A only has 2 stacks.Therefore, Candidate A is not the correct answer.</p>
<p>-Similarly, Candidate C has 4 stacks, which also cannot be correct.</p>
<p>-For Candidate B, the blocks in each stack match what's shown in the image.This is the correct answer.</p>
<p>-For Candidate D, the blocks in each stack do not match the image.For example, stack 1 in the image has a purple block, and there is no any purple block in Candidate D. So this is incorrect.</p>
<p>-Therefore, the final answer is B. ## Procedure and Output Your output should follow this format: 1.First, analyze the block configuration in the image and candidates as shown above; 2.Then, answer the question with the format <Output> <Choice>, where <Choice> is one of A,B,C,D.For example, "<Output> A".</p>
<p>Now please choose the correct textual representation based on the given image below:</p>
<p>Prompt for Maze Navigation scenario, Task 4 (Spatial Reasoning):</p>
<p>You are a robot that sorts and organizes colored blocks by adding and removing them to stacks.You can move them between stacks to produce a desired end state.In this task, you will see a photo of blocks.This photo shows the beginning state of the blocks.You will see a photo of blocks.This photo shows the beginning state of the blocks.Meanwhile, you will be provided an action sequence about moving blocks.Your task is to determine if the provided action plan can be successfully executed.## Game Setup -The block configuration is presented in the image.You must view and interpret the image in order to determine which blocks are in which stack and determine the consequence of moving.</p>
<p>-Each block has a unique color (blue, yellow, purple, orange, red, green).</p>
<p>-Blocks are stacked vertically in a stack, forming multiple stacks.</p>
<p>-A valid action can only move the top block of any stacks.Attempting to move lower blocks is considered an invalid move.</p>
<p>-For the destination, a valid move can either (a) move the top block to the top of another stack, or (b) place the top block on the table, creating a new stack with just one block.</p>
<p>We provide an example to further illustrate the rules:</p>
<p>The sequence of actions provided is:</p>
<p>B Prompt for textual input</p>
<p>In Section 4.4, we described the procedure of using textual representation instead of visual input.Below, we use the main task in the maze navigation scenario as an example to show the complete prompt after making the replacement.</p>
<p>Prompt for Maze Navigation scenario, Main task (Spatial Planning, Textual Input):</p>
<p>As a professional maze solver, your task is to analyze a grid-based map and devise an action plan that enables a player to reach the goal from the starting point without falling into any holes, using the fewest possible moves.Since coding is not within your skill set, your approach relies on logical reasoning of the map.</p>
<h2>Game Setup -The game presents a fully observable grid-based map.</h2>
<p>-The player starts at a specified grid square, with the goal located elsewhere on the map.</p>
<p>-Each grid square is either safe or contains a hole.</p>
<p>-Your goal is to guide the player to the goal while avoiding holes.</p>
<h2>Moving Rules -The action plan involves a series of moves: 'L' (left), 'R' (right), 'U' (up), or 'D' (down).</h2>
<p>-Each move transfers the player to the adjacent square in that direction, provided it is a safe square.The player cannot move more than one square at a time.</p>
<p>-Moving off the edge of the map has no effect.The player will remain at the same square.</p>
<p>-DO NOT MOVE INTO A HOLE! Falling into a hole results in defeat.</p>
<p>-Locating at the grid containing the goal results in victory.</p>
<p>We provide an example to further illustrate the rules.</p>
<p>Example Input: This is a 4x4 map.</p>
<p>The player is at: row 1, column 1; The hole(s) are at: row 1, column 2; row 4, column 1;</p>
<p>The goal is at: row 4, column 4.</p>
<p>In this provided example: -The player is at Row Please generate the action plan for the following maze: This is a 3x3 map.</p>
<p>The player is at: row 3, column 2; There is no holes in this map;</p>
<p>The goal is at: Row 1, Column 2.</p>
<p>C Prompt with in-context examples</p>
<p>In Section 4.4, we described the procedure of including in-context example in the test.Below, we use the main task in the maze navigation scenario as an example to show the complete prompt after adding in-context examples.</p>
<p>Prompt for Maze Navigation scenario, Main task (Spatial Planning, in-context examples):</p>
<p>As a professional maze solver, your task is to analyze a grid-based map and devise an action plan that enables a player to reach the goal from the starting point without falling into any holes, using the fewest possible moves.Since coding is not within your skill set, your approach relies on logical reasoning of the map.</p>
<h2>Game Setup -The game presents a fully observable grid-based map.</h2>
<p>-The player starts at a specified grid square, with the goal located elsewhere on the map.</p>
<p>-Each grid square is either safe or contains a hole.</p>
<p>-Your goal is to guide the player to the while avoiding holes.</p>
<p>The following figure shows how the player, the holes (non-safe grid), the lands (safe grids), and the goals look like.</p>
<h2>Moving Rules -The action plan involves a series of moves: 'L' (left), 'R' (right), 'U' (up), or 'D' (down).</h2>
<p>-Each move transfers the player to the adjacent square in that direction, provided it is a safe square.The player cannot move more than one square at a time.</p>
<p>-Moving off the edge of the map has no effect.The player will remain at the same square.</p>
<p>-DO NOT MOVE INTO A HOLE! Falling into a hole results in defeat.</p>
<p>-Locating at the grid containing the goal results in victory.</p>
<p>We provide an example to further illustrate the rules.</p>
<p>In this provided example: -The player is at Row 1, Column 1; -The goal is at Row 4, Column 4; -There are two holes: one at Row 1, Column 2, and another at Row 4, Column 1.</p>
<p>-The player can move DOWN.This is because moving down brings them to Row 2, Column 1, and this cell is safe (without holes).</p>
<p>-Moving UP has no effects.This is because the player is already in the topmost row.</p>
<p>-Similarly, moving LEFT has no effects because the player is already in the left-most column.</p>
<p>-Moving RIGHT places the player at Row 1, Column 2. Since there is a hole at this grid, this move results in a loss.</p>
<p>(continue in next page)</p>
<p>D Training Details</p>
<p>In this section, we describe the training details when we fine-tune LLaVA and InternLM-XComposer for our designed tasks.We perform LoRA fine-tuning, and we stick with the default hyperparameter settings in their official repo.The detailed hyperparameter choices are shown in Table 6.</p>
<p>E Complete Task Performance Results with different difficulty levels</p>
<p>In this section, we present the experimental results of models across different difficulty levels.The results are shown in Table 7.As expected, we observe that as difficulty increases, all models perform progressively worse, with some performing close to random guessing at higher difficulty levels (e.g., Task 1 in Maze Navigation scenario).We also observe that GPT-4o, the most recently released model, performs the best across different tasks, although it still frequently makes mistakes under different difficulty levels.This suggests a current bottleneck in state-of-the-art VLMs.</p>
<p>F Benchmark Documentation and Intended Users</p>
<p>The VSP benchmark is designed to evaluate VLM's capability in visual spatial planning.Visual spatial planning refers to the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes.The VSP benchmark consists of two scenarios: ❶ the simulated Maze Navigation scenario, whose main task is to move a game character through a maze, and ❷ the photo-realistic Blocks World scenario, whose main task is to move blocks from a starting configuration to a goal configuration.In each scenario, in addition to the main task, VSP introduces four sub-tasks that focus on the individual capabilities needed for the main task:</p>
<p>• T1.Single Object Perception -Determine the characteristics of a single object; • T2.Spatial Relation Perception -Determine the relative positions of two objects; • T3.Environment Perception -Find textual descriptions that describe the visual environment; • T4.Spatial Reasoning -Determine the consequence of a series of actions or moves.</p>
<p>The VSP benchmark consists of 10 tasks in two scenarios.For each task, the problems are designed with different difficulty levels.Specifically, each difficulty level consists of 100 problems.In total, the VSP benchmark includes 4.4k questions.To implement the two scenarios, we utilize and enhance existing resources from two aspects.For the maze navigation scenario, we leverage OpenAI's gym [57] engine to generate input images.For the blocks world scenario, we sample input images from the BIRD dataset [59].The BIRD dataset is originally built to test an RL model's capability in understanding visual block configurations and performing sequential actions to reach the target state.We enhance it by designing auxiliary tasks (T1-T4), corresponding textual descriptions, and text prompts necessary for the input of VLMs.Additionally, we implement auto-evaluation scripts</p>
<p>Figure 1 :
1
Figure 1: Overview of the Maze Navigation scenario.</p>
<p>Figure 2 :
2
Figure 2: Overview of the Blocks World scenario.</p>
<p>Figure 3 :
3
Figure 3: Benchmark creation process.Left: We prepare input images that fulfill the task requirements with different difficulties.Mid: We formulate input prompts for each task.The input prompts consists of interleaved texts and images.Right: We develop automatic evaluation process for each task.</p>
<p>Figure 3
3
Figure 3 demonstrates our 3-stage general process for benchmark creation.</p>
<p>Figure 4 :
4
Figure 4: The visual and corresponding textual inputs.</p>
<p>Figure 5 :
5
Figure 5: Performance comparison with the visual/textual input.When the environment is described by text instead of image, the performance increases significantly.</p>
<p>Stack 1 :
1
Purple block (alone) -Stack 2: Blue block (alone) -Stack 3: From bottom to top: Orange block, Red block You can only move the top block of each stack: the purple block, the blue block, and the red block.The orange block is stuck underneath the red block and cannot be moved directly.Each move can place the block on another stack or on the table (creating a new stack of one).For instance, you could move the red block to either the blue stack or the table.</p>
<p>Spatial Planning <Question> Please find a moving plan to transit transit from the beginning state to the end state. <Answer> move(purple, green) move(red, table) move(blue, table) move(yellow, red) move(blue, yellow) Object Perception Please indicate the color of the block at stack 2, level 3? (Stack is counted from left to right; level of blocks is counted from bottom to top) Spatial Relation Perception Please indicate the spatial relation between the yellow and the red block. (A) The red block is above the yellow block (B) ... Environment Perception Please select the best text representation of the given initial state: (A) <Textual Description> (B) … Reasoning Please determine whether the given moving plan can be executed: move(red, table) move(purple, green) Initial State Target State</p>
<p>Table 2 :
2
Zero-shot success rates for the spatial planning task, at various difficulty levels.Maze navigation difficulty levels represent the maze's square grid length.Blocksworld difficulty levels correspond to the minimum number of steps to a solution.Results better than 30% are bolded.
MAZE NAVIGATIONBLOCKSWORDOVERALLDifficulty level3456781357</p>
<p>Text Input Vision Input Table Input Vision Input Pure Text Input Maze Navigation Scenario Blocks World Scenario</p>
<p>with purple block -Stack with blue block -Stack with orange block, red block, from bottom to top Pure</p>
<p>Table 3 :
3
Decomposed Capability Analysis.Similar to the spatial planning task, each task consists of test with different difficulties.Results better than 70% are bolded.Please refer to Appendix E for the complete evaluation results for different difficulties.
MAZE NAVIGATIONBLOCKSWORDTaskT1T2T3T4T1T2T3T4Random Guess0.50.25 0.250.50.17 0.25 0.250.5Gemini [7]0.58 0.56 0.33 0.49 0.86 0.51 0.54 0.55GPT-Vision [5]0.56 0.27 0.46 0.56 0.73 0.80 0.70 0.71Claude-3 [60]0.45 0.67 0.32 0.61 0.43 0.53 0.49 0.66GPT-4o [61]0.58 0.67 0.58 0.74 0.95 0.90 0.90 0.76LLaVA [6]0.49 0.27 0.21 0.54 0.22 0.21 0.24 0.55InternLM [62]0.48 0.27 0.29 0.58 0.25 0.32 0.26 0.53InternLM-VL [62]0.41 0.20 0.17 0.47 0.22 0.20 0.20 0.53InstructBLIP [63]0.44 0.23 0.21 0.37 0.21 0.16 0.22 0.47SPHINX [64]0.56 0.28 0.32 0.59 0.24 0.33 0.27 0.584.3 The Perception and Reasoning Sub-tasks Evaluation</p>
<p>Table 4 :
4
Effects of providing in-context examples.
MAZE NAVIGATIONBLOCKSWORDTaskT1T2T3T4MainT1T2T3T4MainGemini, 0-shot0.58 0.56 0.33 0.490.170.86 0.51 0.54 0.550.03Gemini, 1-shot0.50 0.66 0.31 0.480.200.91 0.68 0.71 0.590.03Gemini, 2-shot0.53 0.68 0.31 0.510.210.90 0.76 0.70 0.610.03Gemini, 4-shot0.53 0.67 0.35 0.530.190.91 0.64 0.69 0.620.06GPT-Vision, 0-shot0.56 0.27 0.46 0.560.260.73 0.80 0.70 0.710.10GPT-Vision, 1-shot0.55 0.50 0.47 0.570.280.89 0.84 0.94 0.730.11GPT-Vision, 2-shot0.55 0.63 0.50 0.560.300.90 0.83 0.95 0.710.16GPT-Vision, 4-shot0.54 0.69 0.54 0.560.290.90 0.79 0.96 0.73-</p>
<p>Table 5 :
5
Fine-tuning results for open-source models.
MAZE NAVIGATIONBLOCKS OF WORLDModelSettingT1T2T3T4MainT1T2T3T4MainLLaVAzero-shot 0.49 0.27 0.21 0.54 fine-tune 0.53 0.99 0.51 0.930.05 0.600.22 0.21 0.24 0.55 1.00 1.00 1.00 1.000.01 0.97InternLMzero-shot 0.48 0.27 0.29 0.58 fine-tune 0.52 0.59 0.91 0.590.11 0.170.25 0.32 0.26 0.53 0.29 0.44 0.69 0.620.00 0.09
of examples for Gemini and GPT-Vision (refer to Appendix C for the input examples).The result is shown in Table4.There are two key observations: First, in-context examples make some potential contributions, but they are not significant.Introducing examples only benefits in several sparse cases, such as T2 in maze navigation and T3 in blocksworld.Second, scaling in-context examples generally does not help, as illustrated by the saturated performance in each task.</p>
<ol>
<li>Then, generate an action plan to navigate to the goal step by step.At each step, you should check: (a) Where the current move leads the player to (the row and column); (b) What is in that grid.Is it a hole?Is it the goal?Is it an empty space?(c) Determine if that is a safe action.If not, correct it and re-generate the action plan.3. Next, verify if the steps successfully navigate the player to the goal without falling into the hole.If not, restart from step 2 and re-generate this step.</li>
</ol>
<p>(continue in next page) (Continued)</p>
<p>Now please generate moving plan.The beginning state is:
(Continue)).## Example Output<Analysis>Starting state: there are three stacks:-Stack 1: Purple block (alone)-Stack 2: Blue block (alone)-Stack 3: From bottom to top: Orange block, Red blockEnding state: there are three stacks:-Stack 1: Purple block (alone)-Stack 2: From bottom to top: Orange block, Blue block-Stack 3: Red block (alone)<Output>1. move(red,table)2. move(blue,orange)(continue in next page)</p>
<p>Stack 1: Purple block (alone) -Stack 2: Blue block (alone) -Stack 3: From bottom to top: Orange block, Red block Here are examples of textual representations: (A) -Stack with red block, yellow block, from bottom to top -Stack with orange block, purple block, green block, from bottom to top
(B)-Stack with purple block-Stack with blue block-Stack with orange block, red block, from bottom to top(C)-Stack with orange block-Stack with purple block-Stack with blue block-Stack with green block, yellow block, from bottom to top(D)-Stack with green block-Stack with yellow block, blue block, from bottom to top-Stack with red block, orange block, from bottom to top</p>
<p>The player can move DOWN.This is because moving down brings them to Row 2, Column 1, and this cell is safe (without holes).-MovingUPhasno effects.This is because the player is already in the topmost row.-Similarly,movingLEFT has no effects because the player is already in the left-most column.-MovingRIGHT places the player at Row 1, Column 2. Since there is a hole at this grid, this move results in a loss.## Procedure and Output Now you will solve the given maze.To solve it, please generate text exactly follow the following steps: 1.First, interpret map.List where the player is at now, where is the goal, and where are the holes.2.Then, generate an action plan to navigate to the goal step by step.At each step, you should check: (a) Where the current move leads the player to (the row and column); (b) What is in that grid.Is it a hole?Is it the goal?Is it an empty space?(c) Determine if that is a safe action.If not, correct it and re-generate the action plan.3. Next, verify if the steps successfully navigate the player to the goal without falling into the hole.If not, restart from step 2 and re-generate this step.4. If succeed, output an aggregated plan using "Action plan: <PLAN>", where <PLAN> is a string concatenated action in each step.For example, "Action plan: L,L,R,U,D" meaning an action plan of left, left, right, up, and down.Double check the final action plan is consistent with the previous analysis.Do not output any extra content after the above aggregated output.</p>
<p>1, Column 1; -The goal is at Row 4, Column 4; -There are two holes: one at Row 1, Column 2, and another at Row 4, Column 1. -</p>
<p>Table 6 :
6
Training details on LLaVA and InternLM-XComposer.Value
Learning rate2e-4SchedulerCosineEpoch1LLaVATraining data10kBatch size32Pretrained Checkpoint llava-v1.6-vicuna-7bLearning rate5e-5SchedulerCosineEpoch1InternLMTraining data10kBatch size8Pretrained Checkpoint internlm-xcomposer2-7b
The latest Gemini-1.5-Pro-Vision currently has a daily request limits of 50. Therefore, we did not include its evaluation.
Prompt for Maze Navigation scenario, Task 2 (Spatial Relation Perception):In this task, you will analyze a maze to determine the relative positions of the player and the goal.The following figure illustrates the appearances of the player, holes, lands, and the goal within the maze.You will need to focus on the player and the goal.To describe their relative positions, use the directional indicators from "Above", "Below", "Left", "Right".We provide an example to illustrate how to interpret and describe these positions:In this example: -We focus on the position of the player and the goal.-Rows: The player is at row 1, and the goal is at row 4. Here, the row number is from top to bottom.Comparing player (row=1) with goal (row=4), player is counted first.Therefore, the player is positioned above the target.-Columns: The player is at column 1, and the goal is at column 4. Here, the column number is from left to right.Comparing player (column=1) with goal (column=4).Therefore, the player is to the left of the target.-Remember that we should answer the player's position with respect to the goal, not the opposite.Therefore, we answer "Above,Left".Your output should be two parts:1. Analyze the rows and columns of the player and the goal like shown above.2. Following your analysis, output answer as "<Output> <Position>".For example, "<Output> Above,Left" means the player is above and to the left of the goal, and "<Output> Below" means the player is below the goal.Note that you should not output "Left" or "Right" if the player and the goal are at the same column, and similarly, you should not output "Above" or "Below" if the player and the goal are at the same row.Now you will analyze the following maze and determine the relative position of the player in relation to the goal.-The player can move DOWN.This is because moving down brings them to Row 2, Column 1, and this cell is safe (without holes).-Moving UP has no effects.This is because the player is already in the topmost row.-Similarly, moving LEFT has no effects because the player is already in the left-most column.-Moving RIGHT places the player at Row 1, Column 2. Since there is a hole at this grid, this move results in a loss.## Example: <Interpret>The player is at row 1, column 1, and the goal is at row 2, column 2. There are 2 holes.They are at: row 3, column 2; row 3, column 3. <Action Plan> -Moving Right (R).The player is now at row 1, column 2. This grid is safe.-Moving Down (D).The player is now at row 2, column 2. This grid is the goal, so we stop here.for each task, aiming to provide a convenient testbed for current VLMs.All the images and texts in this benchmark do not contain any personally identifiable information or offensive content.All the content in this benchmark can be accessed, reviewed, and downloaded via https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.As the authors of this benchmark, we assume full responsibility for any rights violations related to this benchmark.The benchmark is licensed under the MIT license.We will consistently monitor issues and pull requests for better maintenance.Additionally, we also release the test scripts to replicate our results.G LimitationIt is important to note that our proposed VSP benchmark also has limitations.First, as a VLM benchmark specifically tailored for visual spatial planning capability, the VSP does not measure a VLM's abilities in other important aspects, such as semantic understanding, factual knowledge, etc.We emphasize that VSP is not a comprehensive benchmark for VLMs, but rather a benchmark focusing on an important capability that has been mostly overlooked by existing benchmarks.Second, we also note that the appearance of objects in the image may influence models' performance.Specifically, a model might find it easier to recognize objects against a darker background in the blocks world scenario, and vice versa.The current measure is based on a single kind of object appearance, which might favor some particular models trained on similar images.An ideal measurement would assess the average performance on images with the same content but a variety of different appearances.That said, with the detailed prompt description and sufficient information provided in the image, we believe the current version of the VSP benchmark already demonstrates the deficiencies of current state-ofthe-art models in visual spatial planning.In future work, we plan to incorporate appearance/style variations in the input images for a more thorough model ability quantification.
Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, abs/2302.139712023ArXiv preprint</p>
<p>Deepseek llm: Scaling open-source language models with longtermism. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, abs/2401.029542024ArXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, abs/2401.040882024Mixtral of experts. ArXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, abs/2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023831ArXiv preprint</p>
<p>Advances in neural information processing systems. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, 2024. 1, 6, 836Visual instruction tuning</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, abs/2312.118052023831ArXiv preprint</p>
<p>Openflamingo: An opensource framework for training large autoregressive vision-language models. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Shiori Gadre, Sagawa, abs/2308.013902023ArXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, abs/2404.160062024ArXiv preprint</p>
<p>Exploring diverse in-context configurations for image captioning. Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, Xin Geng, Advances in Neural Information Processing Systems. 3612024</p>
<p>Prompting large language models with answer heuristics for knowledge-based visual question answering. Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>On the planning abilities of large language models -a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc2023363</p>
<p>Drivevlm: The convergence of autonomous driving and large vision-language models. Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, Hang Zhao, abs/2402.122892024ArXiv preprint</p>
<p>Dolphins: Multimodal language model for driving. Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, Chaowei Xiao, preprint, abs/2312.004382023</p>
<p>Lgmcts: Language-guided monte-carlo tree search for executable semantic object rearrangement. Haonan Chang, Kai Gao, Kowndinya Boyalakuntla, Alex Lee, Baichuan Huang, Udhaya Harish, Jinjin Kumar, Abdeslam Yu, Boularias, abs/2309.1582120233ArXiv preprint</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, abs/2311.1784220233ArXiv preprint</p>
<p>Mme: A comprehensive evaluation benchmark for multimodal large language models. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji, abs/2306.1339420233ArXiv preprint</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, Proceedings of CVPR. CVPR202423</p>
<p>Measuring multimodal mathematical reasoning with math-vision dataset. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li, abs/2402.1480420243ArXiv preprint</p>
<p>Seedbench: Benchmarking multimodal llms with generative comprehension. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, abs/2307.1612520233ArXiv preprint</p>
<p>Mm-vet: Evaluating large multimodal models for integrated capabilities. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, abs/2308.0249020233ArXiv preprint</p>
<p>Pddl| the planning domain definition language. Constructions Aeronautiques, Adele Howe, Craig Knoblock, Drew Isi, Ashwin Mcdermott, Manuela Ram, Daniel Veloso, David Wilkins Weld, Anthony Sri, Dave Barrett, Christianson, Tech. Rep. 21998Technical Report</p>
<p>Deep learning for real-time atari game play using offline monte-carlo tree search planning. Xiaoxiao Guo, P Satinder, Honglak Singh, Richard L Lee, Xiaoshi Lewis, Wang, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D Lawrence, Kilian Q Weinberger, Montreal, Quebec, Canada2014. December 8-13 2014. 2014</p>
<p>Planning by incremental dynamic programming. Richard S Sutton, Machine learning proceedings 1991. Elsevier1991</p>
<p>Can large language models reason and plan?. Subbarao Kambhampati, Annals of the New York Academy of Sciences. 153412024</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, abs/2402.018172024ArXiv preprint</p>
<p>On the self-verification limitations of large language models on reasoning and planning tasks. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, abs/2402.081152024ArXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 3622024</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, abs/2210.036292022ArXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, abs/2204.01691ArXiv preprint. 32022</p>
<p>Saycanpay: Heuristic planning with large language models using learnable domain knowledge. Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De, Raedt , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024383</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Tptu: Task planning and tool usage of large language model-based ai agents. Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, abs/2308.034272023ArXiv preprint</p>
<p>Smartplay: A benchmark for llms as intelligent agents. Yue Wu, Xuan Tang, Tom M Mitchell, Yuanzhi Li, abs/2310.0155720233ArXiv preprint</p>
<p>Travelplanner: A benchmark for real-world planning with language agents. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, abs/2402.01622ArXiv preprint. 2024</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, abs/2206.104982022ArXiv preprint</p>
<p>Copal: Corrective planning of robot actions with large language models. Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg, Michael Gienger, abs/2310.072632023ArXiv preprint</p>
<p>From cooking recipes to robot task trees-improving planning correctness and task efficiency by leveraging llms with a knowledge network. Md Sadman, Sakib , Yu Sun, abs/2309.09181ArXiv preprint. 2023</p>
<p>Layoutgpt: Compositional visual planning and generation with large language models. Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin , Eric Wang, William Yang, Wang , Advances in Neural Information Processing Systems. 202436</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew J Hausknecht, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. 2021OpenReview.net</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v (ision). Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, ArXiv preprint, abs/2309.174212023</p>
<p>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. Mohamed Aghzal, Erion Plaku, Ziyu Yao, abs/2310.032492023ArXiv preprint</p>
<p>Gemini vs gpt-4v: A preliminary comparison and combination of vision-language models through qualitative cases. Zhangyang Qi, Ye Fang, Mengchen Zhang, Zeyi Sun, Tong Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, Hengshuang Zhao, abs/2312.150112023ArXiv preprint</p>
<p>Visually dehallucinative instruction generation: Know what you don't know. Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang, abs/2402.097172024ArXiv preprint</p>
<p>Mllm-bench, evaluating multi-modal llms using gpt-4v. Wentao Ge, Shunian Chen, Guiming Chen, Junying Chen, Zhihong Chen, Shuo Yan, Chenghao Zhu, Ziyue Lin, Wenya Xie, Xidong Wang, abs/2311.139512023ArXiv preprint</p>
<p>Eyes wide shut? exploring the visual shortcomings of multimodal llms. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann Lecun, Saining Xie, abs/2401.062092024ArXiv preprint</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, Mathvista, arXiv-2310Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. arXiv e-prints. 2023</p>
<p>Hallusionbench: You see what you think? or you think what you see? an imagecontext reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou, abs/2310.145662023ArXiv preprint</p>
<p>Milebench: Benchmarking mllms in long context. Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, Benyou Wang, ArXiv preprint, abs/2404.18532, 2024</p>
<p>Multimodal needle in a haystack: Benchmarking longcontext capability of multimodal large language models. Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang, abs/2406.112302024ArXiv preprint</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, abs/1606.015402016530Openai gym. ArXiv preprint</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, abs/2305.14992ArXiv preprint. 2023</p>
<p>Blocksworld revisited: Learning and reasoning to generate event-sequences from image pairs. Tejas Gokhale, Shailaja Sampat, Zhiyuan Fang, Yezhou Yang, Chitta Baral, 1905.12042. 201930ArXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Ai Anthropic, 2024831Claude-3 Model Card</p>
<p>. Gpt-4o, 831</p>
<p>Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, abs/2401.164202024ArXiv preprint</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, Steven Hoi, Advances in Neural Information Processing Systems. 3662024</p>
<p>Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, abs/2311.0757520237ArXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Cogvlm: Visual expert for pretrained language models. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, abs/2311.030792023ArXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
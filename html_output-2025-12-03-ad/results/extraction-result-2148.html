<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2148 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2148</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2148</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-276575610</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.16069v1.pdf" target="_blank">Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents</a></p>
                <p><strong>Paper Abstract:</strong> Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4$\times$ improvement in correctly answering experimental questions. Curie is open-sourced at https://github.com/Just-Curieous/Curie.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2148.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2148.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curie: AI agent framework for rigorous automated scientific experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based multi-agent system that automates end-to-end scientific experimentation while embedding rigor through intra-agent validators, inter-agent coordination, and a structured experiment knowledge store; evaluated on a 46-task experimentation benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM-based framework (architect + technician agents) with a dedicated Experimental Rigor Engine</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific experimentation with focus on computer science domains (LLM reasoning, Vector DB, Cloud Computing, ML training)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates hypotheses, high-level experimental plans, executable experiment code/setups, experimental partitions, and derived conclusions from produced results</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Integrated multi-stage validation: (1) Intra-ARM validators (Experimental Setup Validator checks alignment with plan, variable handling, placeholders; Execution Validator runs setups in a clean environment, logs errors, and performs reproducibility checks by running workflows multiple times); (2) Inter-ARM control-flow enforcement (prevents invalid transitions and enforces stateful validation steps); (3) Experiment Knowledge Module (time-machine history, tiered write access, structured reads/writes) to ensure provenance and interpretability; final results are evaluated by an LLM judge for straightforward checks and by manual expert review for implementation alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Tasks derived from published results and open-source projects are treated as 'familiar' when matching known benchmarks; novelty operationalized via complexity and out-of-distribution dimensions (Design, Setup, Relationship, Goal complexity) and by deviation from ground-truth published outcomes; novelty also inferred from partitions that are not covered by starter code/benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Curie attains substantially higher generation performance than baselines: reported aggregated improvement includes a 3.4× increase in correctly answering experimental questions versus the strongest baseline; Curie achieves much higher execution/setup reliability (execution-setup performance reported up to ~66.7%–92.7% in text for various settings) and higher conclusion correctness (overall conclusion correctness reported at 36.1%, with domain peaks such as 44.9% on LLM reasoning tasks). Curie degrades with increasing task complexity, but less sharply than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is handled by automated validators and hybrid evaluation: automated checks (Intra-ARM) detect setup errors, runtime errors and reproducibility issues; an LLM judge is used for design, setup, and conclusion verifications where ground truth exists, while implementation alignment is manually audited. Exact numerical accuracy/precision/recall of validators is not reported; empirical evidence shows higher validated execution/setup success rates for Curie versus baselines (text reports much higher execution/setup and alignment scores for Curie).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation reliability decreases as task novelty/complexity increases: all agents' validation and conclusion scores decline with higher complexity; Curie's automated validators and knowledge module mitigate but do not eliminate this decline, leading to relatively better validation on novel/harder tasks compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>The paper documents an asymmetry in baseline systems: many prior or baseline agents can generate creative plans/code but lack systematic validation, causing hallucinations and cascading errors; Curie's integrated validators reduce this gap but do not fully close it. Empirically, generation (design) remains relatively good across agents while correct validated conclusions lag, especially for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Performance on out-of-distribution or high-complexity tasks degrades for all systems; Curie shows better robustness (smaller relative drop) while some baselines (e.g., Magentic) often drop to near-zero conclusion scores on hardest tasks. Specific numeric OOD metrics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported quantitatively. The paper notes the use of structured provenance and validators to reduce hallucination rather than reporting calibrated confidence estimates; no explicit calibration or uncertainty scores are provided and calibration is likely to degrade with novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not numerically quantified. Validation is described as computationally heavier than single-shot generation because Execution Validator runs experiments in a clean environment, performs multiple runs for reproducibility, and the scheduler handles partitions (parallelization mitigates cost). The system explicitly trades extra computation/time for higher reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Intra-ARM validators (setup & execution), Inter-ARM control-flow and partition scheduling, Experiment Knowledge Module (structured history, tiered write access), hybrid automated + manual evaluation (LLM judge + expert checks), iterative refine-and-re-execute workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curie's integrated validation architecture materially reduces generation-to-validation failures compared to state-of-the-art baselines: Curie achieves a 3.4× improvement in correct experimental answers and higher execution/setup reliability and conclusion accuracy, especially on complex and long-running experiments, demonstrating that built-in validation modules mitigate but do not fully remove the generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2148.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2148.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intra-ARM Setup Validator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curie Intra-Agent Rigor Module — Experimental Setup Validator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular validator within Curie that checks that technician-proposed experiment setups align with the high-level plan, handle inputs/outputs correctly, and document intermediate steps and expected results to prevent setup-level errors before execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Intra-ARM Experimental Setup Validator</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>automated rule-based and LLM-assisted validator module</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>experimental workflow validation across computer science experimentation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not generate scientific hypotheses itself; validates generated experiment setups and flags/requests corrections</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>semantic and syntactic checks against the experimental plan: alignment checks for research question and variable definitions, I/O argument checks, detection of placeholders/hardcoded values, and requirement of documented intermediate steps and expected results.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly a novelty detector; novelty effects inferred by increased mismatch rates when setups are divergent from plan or from provided starter code/benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (validator rather than generator).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Qualitative reporting indicates it catches setup issues that baselines miss, improving subsequent execution success; no numeric true/false positive rates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Likely more false alarms and missed catches when setups are novel or when domain-specific knowledge is required, but explicit measurements are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>This validator is designed to reduce an observed asymmetry where generated setups are not rigorously checked; qualitative results show fewer cascading errors after setup validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; intended to improve robustness on unfamiliar setups by enforcing alignment checks, but effectiveness on highly novel domains not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Low computational cost compared to execution (rule/LLM checks), but relative cost not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Explicit pre-execution semantic/syntactic checks, modular validator extensibility.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The setup validator catches common setup errors (placeholders, hardcoded values, missing I/O), reducing execution failures and cascading errors, thereby improving reproducibility and reliability of generated experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2148.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2148.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intra-ARM Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curie Intra-Agent Rigor Module — Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A validator that runs experiment setups in a clean environment to detect runtime errors and verify reproducibility by running workflows multiple times, logging errors for debugging and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Intra-ARM Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>automated execution-and-checking module (sandboxed runtime + reproducibility checks)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>experimental workflow validation across computer science experimentation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not generate scientific outputs; executes generated experiment code to produce empirical results for validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>executes setups in a clean environment, checks for error-free runs, performs repeated runs to assess reproducibility and detect hidden dependencies or anomalies, and validates that outputs align with the experimental plan and quality standards.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Reproducibility checks reveal when outputs diverge across runs (higher divergence signals novelty or instability).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Qualitative improvement reported: Curie shows notably higher execution/setup success rates compared to baselines thanks to this validator; no numerical per-run validation accuracy provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Novel or out-of-distribution experiments are more likely to reveal execution failures and reproducibility problems; repeated runs help surface these issues but do not provide numerical sensitivity/specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>By executing experiments (rather than just reasoning about them), the execution validator narrows the gap between generated claims and ground-truth results; nevertheless, manual alignment checks are still required for semantic correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Execution failures and lower reproducibility expected for OOD tasks; Curie mitigates some of this through iterative debugging but quantitative OOD metrics are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High relative to generation since it requires running experiments (sometimes multiple times) in controlled environments; exact cost/time not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Sandboxed execution, repeated runs for reproducibility, detailed logging for debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated execution and reproducibility checks substantially reduce runtime failures and hidden-dependency issues, enabling more reliable mapping from generated experimental plans to validated results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2148.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2148.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experiment Knowledge Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curie Experiment Knowledge Module (structured experiment datastore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured knowledge bank that stores enriched experimental plans with metadata, maintains a DAG-like time-machine history of changes, and enforces tiered write access to preserve provenance and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Experiment Knowledge Module</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>structured knowledge store / provenance management subsystem</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>experimental reproducibility and interpretability across scientific experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not directly generate discoveries; records generated plans, results, and rationales to enable reconstruction and interpretation of experiments</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>supports validation by providing consistent reads/writes, provenance (time-machine), and enforced write permissions so that validators and agents can verify state transitions and provenance of results; prevents LLM hallucinated writes and inconsistent recall.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Tracks changes and the evolution of hypotheses/partitions over time; novelty inferred from new branches or partitions not previously seen in the knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improves post-hoc validation and diagnosis by enabling traceability and rollback; no numeric measures reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Provides improved interpretability for novel findings by capturing their derivation, though it does not itself adjudicate correctness; effectiveness depends on quality of logged metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Reduces asymmetry by preserving provenance so that generated outputs can be audited and re-run, but does not eliminate the need to empirically validate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Helps diagnose OOD failures by retaining historical context; quantitative impact not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Storage and metadata bookkeeping cost; not quantitatively reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Tiered write access, time-machine history, structured metadata, DAG of changes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured knowledge management reduces the risk of hallucinated or inconsistent records and enables reliable auditing and diagnosis, improving the trustworthiness of automated experimentation outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2148.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2148.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Judge (Zheng et al. style LLM-based experiment verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based automated judge used to verify experiment design, execution setup, and conclusion correctness when ground truth is available; complemented by manual expert checks for implementation alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM judge (Zheng et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model used as an automated verifier</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated evaluation of experimental artifacts and conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not generate experiments; generates verdicts/evaluations about whether an experiment meets design, execution, and conclusion criteria</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Checks experiment logs against ground-truth conclusions and the experiment question; outputs structured JSON-style verdicts (Experiment Design / Execution Setup / Implementation Alignment / Conclusion Correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Works when ground truth is provided; performance on novel outputs lacking clear ground truth is limited and requires manual checking.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used for automated scoring of tasks with ground truth; the authors cross-checked a subset of LLM judge assessments against expert annotations to measure agreement and refine prompts, but no aggregate inter-rater metrics are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Effective for 'straightforward verification' tasks where ground truth exists; cannot reliably detect subtle semantic misalignment in implementation (hence manual audits are used for implementation alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Judge is less reliable for novel or out-of-distribution conclusions where ground truth is absent or ambiguous; the paper reports reliance on manual checks in such cases.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>LLM judge helps automate parts of validation but can miss semantic implementation errors, creating residual asymmetry that requires human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades for novel outputs lacking ground-truth references; no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; authors refined system prompts and validated judge outputs against experts but did not report calibration statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Relatively low compared to execution validation, but exact costs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Automated verdicts for items with ground truth combined with expert cross-checks for implementation alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based judges can scale straightforward verification tasks when ground truth exists, but they miss nuanced semantic alignment errors and require human oversight for implementation-level validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2148.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2148.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenHands</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenHands (coding agent baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art coding-focused agent used as a baseline in Curie's evaluation; it performs well on coding-specific tasks but struggles with rigorous multi-step experimental workflows and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenHands</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenHands</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>specialized coding LLM agent / multi-agent tool for software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>software engineering / experiment setup and code generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates code and implementation artifacts for experiments</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In the benchmark it was given a system prompt to act as a professional experimenter, but lacks Curie's integrated validators; validation is primarily via generated tests/usage and the external LLM judge/manual checks used by the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Performance measured against benchmark tasks and ground truth; tends to perform better on familiar coding tasks with clear starter code/benchmarks and worse on novel complex experimentation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Per-benchmark, OpenHands achieves moderate-to-good design scores but lower execution/setup and conclusion correctness compared to Curie; text reports it outperforms Magentic on some coding aspects but still has execution failures (examples: syntax errors, unresolved dependencies).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Limited automated validation capability; relies on generated code and external evaluation; showed markedly lower validated conclusions relative to Curie (e.g., Curie reported to have much higher conclusion accuracy; Curie achieved 3.4× improvement versus strongest baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance declines substantially on novel/complex tasks; OpenHands benefits from familiar, short execution runs and well-established benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Generates competent code but lacks systematic validation leading to cascading failures that undermine final validated conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades with complexity and OOD tasks; specific quantitative degradation not provided but empirical results show large gaps versus Curie.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>In the benchmark, any gap was addressed externally via LLM judge and manual checks; OpenHands itself lacks integrated rigor modules.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>OpenHands produces strong coding artifacts for familiar tasks but lacks built-in, continuous validation and therefore underperforms on multi-step, novel experimentation workflows, producing more execution failures and lower validated conclusions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2148.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2148.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Magentic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Microsoft Magentic (generalist multi-agent baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalist multi-agent system used as a baseline; capable of broad task coverage but performs poorly on rigorous experiment workflows and file/location handling in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Magentic-one: A generalist multi-agent system for solving complex tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Magentic</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generalist multi-agent LLM system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general problem solving and multi-agent coordination applied to experimentation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates plans, coordination steps, and code, but without Curie's dedicated rigor engine</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In the benchmark it received a system prompt as experimenter but lacks Curie's Intra/Inter-ARM validators; validation performed externally by the benchmark's LLM judge and manual checks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measured via benchmark tasks and complexity; exhibits very poor performance on high-complexity and novel tasks in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to underperform across most complexity levels and domains; often fails to find correct files or manage I/O in starter code, leading to execution failures and near-zero conclusion scores on hardest tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Very low validated conclusion accuracy in the benchmark (often 0% on hard settings); lacks automated reproducibility checks and thus suffers cascading failures.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Strong negative effect: Magentic's validation/conclusion correctness falls to near-zero for many high-complexity or novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Significant asymmetry: generated plans/code frequently fail when not tightly scaffolded, and there is no integrated validation to catch/repair errors leading to poor final validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Performs poorly OOD and on complex tasks; specific metrics not provided but qualitative descriptions indicate catastrophic drops.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>None integrated in base Magentic as evaluated; benchmark uses external evaluation and manual checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Magentic's generalist multi-agent approach lacks systematic validation mechanisms and performs poorly on rigorous experimental tasks, especially for novel or complex experiments, highlighting the need for integrated validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2148.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2148.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (underlying LLM used in agents and judges)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The underlying large language model used as the base model for Curie and baselines in the benchmark; used to power Architect and Technician agents as well as the LLM judge evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general language understanding and generation applied to scientific experiment planning, code generation, and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates experimental plans, code, natural-language justifications, and evaluation verdicts (when used as judge)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>When used for validation (LLM judge), it compares logs to ground truth and outputs structured verdicts; when used as a generating agent it relies on Curie's validation modules for downstream verification.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>No intrinsic novelty quantification reported; benchmark treats novelty via task complexity and ground-truth comparisons rather than distance-from-training-data metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Serves as the base LLM; observed system-level performance differences are primarily attributed to architectural/rigor augmentations (Curie) rather than the base model variant itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>As judge, effective on straightforward checks but limited on semantic implementation alignment; no standalone numeric validation metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>LLM-based judgments and generations are more error-prone on novel or complex tasks; Curie's infrastructure reduces downstream impact.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Base LLMs can generate plausible but unverified outputs; without external validators they contribute to the fabrication-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Declines on novel/complex tasks; benchmark results show agent architectures matter more than base-LM for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported specifically for GPT-4o calls; overall system cost depends on repeated runs and multi-agent orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Use of structured validation modules around the base LLM (as in Curie) and hybrid LLM-judge + human review.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a capable base LLM like GPT-4o is necessary but insufficient for rigorous automated experimentation; the architectural validation layers determine the ultimate reliability on novel tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language monkeys: Scaling inference compute with repeated sampling <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Toward rigorous assessment of language agents for data-driven scientific discovery <em>(Rating: 2)</em></li>
                <li>Magentic-one: A generalist multi-agent system for solving complex tasks <em>(Rating: 2)</em></li>
                <li>OpenHands <em>(Rating: 2)</em></li>
                <li>Judging llm-as-ajudge with mt-bench and chatbot arena <em>(Rating: 1)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants <em>(Rating: 1)</em></li>
                <li>Automating scientific discovery through multi-agent intelligent graph reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2148",
    "paper_id": "paper-276575610",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "Curie",
            "name_full": "Curie: AI agent framework for rigorous automated scientific experimentation",
            "brief_description": "An LLM-based multi-agent system that automates end-to-end scientific experimentation while embedding rigor through intra-agent validators, inter-agent coordination, and a structured experiment knowledge store; evaluated on a 46-task experimentation benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Curie",
            "system_type": "multi-agent LLM-based framework (architect + technician agents) with a dedicated Experimental Rigor Engine",
            "domain": "general scientific experimentation with focus on computer science domains (LLM reasoning, Vector DB, Cloud Computing, ML training)",
            "generation_capability": "generates hypotheses, high-level experimental plans, executable experiment code/setups, experimental partitions, and derived conclusions from produced results",
            "validation_method": "Integrated multi-stage validation: (1) Intra-ARM validators (Experimental Setup Validator checks alignment with plan, variable handling, placeholders; Execution Validator runs setups in a clean environment, logs errors, and performs reproducibility checks by running workflows multiple times); (2) Inter-ARM control-flow enforcement (prevents invalid transitions and enforces stateful validation steps); (3) Experiment Knowledge Module (time-machine history, tiered write access, structured reads/writes) to ensure provenance and interpretability; final results are evaluated by an LLM judge for straightforward checks and by manual expert review for implementation alignment.",
            "novelty_measure": "Tasks derived from published results and open-source projects are treated as 'familiar' when matching known benchmarks; novelty operationalized via complexity and out-of-distribution dimensions (Design, Setup, Relationship, Goal complexity) and by deviation from ground-truth published outcomes; novelty also inferred from partitions that are not covered by starter code/benchmarks.",
            "generation_performance": "Curie attains substantially higher generation performance than baselines: reported aggregated improvement includes a 3.4× increase in correctly answering experimental questions versus the strongest baseline; Curie achieves much higher execution/setup reliability (execution-setup performance reported up to ~66.7%–92.7% in text for various settings) and higher conclusion correctness (overall conclusion correctness reported at 36.1%, with domain peaks such as 44.9% on LLM reasoning tasks). Curie degrades with increasing task complexity, but less sharply than baselines.",
            "validation_performance": "Validation is handled by automated validators and hybrid evaluation: automated checks (Intra-ARM) detect setup errors, runtime errors and reproducibility issues; an LLM judge is used for design, setup, and conclusion verifications where ground truth exists, while implementation alignment is manually audited. Exact numerical accuracy/precision/recall of validators is not reported; empirical evidence shows higher validated execution/setup success rates for Curie versus baselines (text reports much higher execution/setup and alignment scores for Curie).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Validation reliability decreases as task novelty/complexity increases: all agents' validation and conclusion scores decline with higher complexity; Curie's automated validators and knowledge module mitigate but do not eliminate this decline, leading to relatively better validation on novel/harder tasks compared to baselines.",
            "generation_validation_asymmetry": "The paper documents an asymmetry in baseline systems: many prior or baseline agents can generate creative plans/code but lack systematic validation, causing hallucinations and cascading errors; Curie's integrated validators reduce this gap but do not fully close it. Empirically, generation (design) remains relatively good across agents while correct validated conclusions lag, especially for baselines.",
            "out_of_distribution_performance": "Performance on out-of-distribution or high-complexity tasks degrades for all systems; Curie shows better robustness (smaller relative drop) while some baselines (e.g., Magentic) often drop to near-zero conclusion scores on hardest tasks. Specific numeric OOD metrics are not provided.",
            "calibration_quality": "Not reported quantitatively. The paper notes the use of structured provenance and validators to reduce hallucination rather than reporting calibrated confidence estimates; no explicit calibration or uncertainty scores are provided and calibration is likely to degrade with novelty.",
            "validation_computational_cost": "Not numerically quantified. Validation is described as computationally heavier than single-shot generation because Execution Validator runs experiments in a clean environment, performs multiple runs for reproducibility, and the scheduler handles partitions (parallelization mitigates cost). The system explicitly trades extra computation/time for higher reliability.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Intra-ARM validators (setup & execution), Inter-ARM control-flow and partition scheduling, Experiment Knowledge Module (structured history, tiered write access), hybrid automated + manual evaluation (LLM judge + expert checks), iterative refine-and-re-execute workflow.",
            "evidence_type": "supports",
            "key_findings": "Curie's integrated validation architecture materially reduces generation-to-validation failures compared to state-of-the-art baselines: Curie achieves a 3.4× improvement in correct experimental answers and higher execution/setup reliability and conclusion accuracy, especially on complex and long-running experiments, demonstrating that built-in validation modules mitigate but do not fully remove the generation-validation gap.",
            "uuid": "e2148.0"
        },
        {
            "name_short": "Intra-ARM Setup Validator",
            "name_full": "Curie Intra-Agent Rigor Module — Experimental Setup Validator",
            "brief_description": "A modular validator within Curie that checks that technician-proposed experiment setups align with the high-level plan, handle inputs/outputs correctly, and document intermediate steps and expected results to prevent setup-level errors before execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Intra-ARM Experimental Setup Validator",
            "system_type": "automated rule-based and LLM-assisted validator module",
            "domain": "experimental workflow validation across computer science experimentation tasks",
            "generation_capability": "does not generate scientific hypotheses itself; validates generated experiment setups and flags/requests corrections",
            "validation_method": "semantic and syntactic checks against the experimental plan: alignment checks for research question and variable definitions, I/O argument checks, detection of placeholders/hardcoded values, and requirement of documented intermediate steps and expected results.",
            "novelty_measure": "Not explicitly a novelty detector; novelty effects inferred by increased mismatch rates when setups are divergent from plan or from provided starter code/benchmarks.",
            "generation_performance": "Not applicable (validator rather than generator).",
            "validation_performance": "Qualitative reporting indicates it catches setup issues that baselines miss, improving subsequent execution success; no numeric true/false positive rates reported.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Likely more false alarms and missed catches when setups are novel or when domain-specific knowledge is required, but explicit measurements are not provided.",
            "generation_validation_asymmetry": "This validator is designed to reduce an observed asymmetry where generated setups are not rigorously checked; qualitative results show fewer cascading errors after setup validation.",
            "out_of_distribution_performance": "Not quantified; intended to improve robustness on unfamiliar setups by enforcing alignment checks, but effectiveness on highly novel domains not reported.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Low computational cost compared to execution (rule/LLM checks), but relative cost not quantified.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Explicit pre-execution semantic/syntactic checks, modular validator extensibility.",
            "evidence_type": "supports",
            "key_findings": "The setup validator catches common setup errors (placeholders, hardcoded values, missing I/O), reducing execution failures and cascading errors, thereby improving reproducibility and reliability of generated experiments.",
            "uuid": "e2148.1"
        },
        {
            "name_short": "Intra-ARM Execution Validator",
            "name_full": "Curie Intra-Agent Rigor Module — Execution Validator",
            "brief_description": "A validator that runs experiment setups in a clean environment to detect runtime errors and verify reproducibility by running workflows multiple times, logging errors for debugging and validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Intra-ARM Execution Validator",
            "system_type": "automated execution-and-checking module (sandboxed runtime + reproducibility checks)",
            "domain": "experimental workflow validation across computer science experimentation tasks",
            "generation_capability": "does not generate scientific outputs; executes generated experiment code to produce empirical results for validation",
            "validation_method": "executes setups in a clean environment, checks for error-free runs, performs repeated runs to assess reproducibility and detect hidden dependencies or anomalies, and validates that outputs align with the experimental plan and quality standards.",
            "novelty_measure": "Reproducibility checks reveal when outputs diverge across runs (higher divergence signals novelty or instability).",
            "generation_performance": "Not applicable.",
            "validation_performance": "Qualitative improvement reported: Curie shows notably higher execution/setup success rates compared to baselines thanks to this validator; no numerical per-run validation accuracy provided.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Novel or out-of-distribution experiments are more likely to reveal execution failures and reproducibility problems; repeated runs help surface these issues but do not provide numerical sensitivity/specificity.",
            "generation_validation_asymmetry": "By executing experiments (rather than just reasoning about them), the execution validator narrows the gap between generated claims and ground-truth results; nevertheless, manual alignment checks are still required for semantic correctness.",
            "out_of_distribution_performance": "Execution failures and lower reproducibility expected for OOD tasks; Curie mitigates some of this through iterative debugging but quantitative OOD metrics are not given.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "High relative to generation since it requires running experiments (sometimes multiple times) in controlled environments; exact cost/time not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Sandboxed execution, repeated runs for reproducibility, detailed logging for debugging.",
            "evidence_type": "supports",
            "key_findings": "Automated execution and reproducibility checks substantially reduce runtime failures and hidden-dependency issues, enabling more reliable mapping from generated experimental plans to validated results.",
            "uuid": "e2148.2"
        },
        {
            "name_short": "Experiment Knowledge Module",
            "name_full": "Curie Experiment Knowledge Module (structured experiment datastore)",
            "brief_description": "A structured knowledge bank that stores enriched experimental plans with metadata, maintains a DAG-like time-machine history of changes, and enforces tiered write access to preserve provenance and interpretability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Experiment Knowledge Module",
            "system_type": "structured knowledge store / provenance management subsystem",
            "domain": "experimental reproducibility and interpretability across scientific experiments",
            "generation_capability": "does not directly generate discoveries; records generated plans, results, and rationales to enable reconstruction and interpretation of experiments",
            "validation_method": "supports validation by providing consistent reads/writes, provenance (time-machine), and enforced write permissions so that validators and agents can verify state transitions and provenance of results; prevents LLM hallucinated writes and inconsistent recall.",
            "novelty_measure": "Tracks changes and the evolution of hypotheses/partitions over time; novelty inferred from new branches or partitions not previously seen in the knowledge graph.",
            "generation_performance": "Not applicable.",
            "validation_performance": "Improves post-hoc validation and diagnosis by enabling traceability and rollback; no numeric measures reported.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Provides improved interpretability for novel findings by capturing their derivation, though it does not itself adjudicate correctness; effectiveness depends on quality of logged metadata.",
            "generation_validation_asymmetry": "Reduces asymmetry by preserving provenance so that generated outputs can be audited and re-run, but does not eliminate the need to empirically validate outputs.",
            "out_of_distribution_performance": "Helps diagnose OOD failures by retaining historical context; quantitative impact not provided.",
            "calibration_quality": "Not applicable.",
            "validation_computational_cost": "Storage and metadata bookkeeping cost; not quantitatively reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Tiered write access, time-machine history, structured metadata, DAG of changes.",
            "evidence_type": "supports",
            "key_findings": "Structured knowledge management reduces the risk of hallucinated or inconsistent records and enables reliable auditing and diagnosis, improving the trustworthiness of automated experimentation outputs.",
            "uuid": "e2148.3"
        },
        {
            "name_short": "LLM Judge",
            "name_full": "LLM Judge (Zheng et al. style LLM-based experiment verifier)",
            "brief_description": "An LLM-based automated judge used to verify experiment design, execution setup, and conclusion correctness when ground truth is available; complemented by manual expert checks for implementation alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM judge (Zheng et al. style)",
            "system_type": "large language model used as an automated verifier",
            "domain": "automated evaluation of experimental artifacts and conclusions",
            "generation_capability": "does not generate experiments; generates verdicts/evaluations about whether an experiment meets design, execution, and conclusion criteria",
            "validation_method": "Checks experiment logs against ground-truth conclusions and the experiment question; outputs structured JSON-style verdicts (Experiment Design / Execution Setup / Implementation Alignment / Conclusion Correctness).",
            "novelty_measure": "Works when ground truth is provided; performance on novel outputs lacking clear ground truth is limited and requires manual checking.",
            "generation_performance": "Used for automated scoring of tasks with ground truth; the authors cross-checked a subset of LLM judge assessments against expert annotations to measure agreement and refine prompts, but no aggregate inter-rater metrics are reported.",
            "validation_performance": "Effective for 'straightforward verification' tasks where ground truth exists; cannot reliably detect subtle semantic misalignment in implementation (hence manual audits are used for implementation alignment).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Judge is less reliable for novel or out-of-distribution conclusions where ground truth is absent or ambiguous; the paper reports reliance on manual checks in such cases.",
            "generation_validation_asymmetry": "LLM judge helps automate parts of validation but can miss semantic implementation errors, creating residual asymmetry that requires human experts.",
            "out_of_distribution_performance": "Degrades for novel outputs lacking ground-truth references; no numeric values provided.",
            "calibration_quality": "Not reported; authors refined system prompts and validated judge outputs against experts but did not report calibration statistics.",
            "validation_computational_cost": "Relatively low compared to execution validation, but exact costs not provided.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Automated verdicts for items with ground truth combined with expert cross-checks for implementation alignment.",
            "evidence_type": "mixed",
            "key_findings": "LLM-based judges can scale straightforward verification tasks when ground truth exists, but they miss nuanced semantic alignment errors and require human oversight for implementation-level validation.",
            "uuid": "e2148.4"
        },
        {
            "name_short": "OpenHands",
            "name_full": "OpenHands (coding agent baseline)",
            "brief_description": "A state-of-the-art coding-focused agent used as a baseline in Curie's evaluation; it performs well on coding-specific tasks but struggles with rigorous multi-step experimental workflows and validation.",
            "citation_title": "OpenHands",
            "mention_or_use": "use",
            "system_name": "OpenHands",
            "system_type": "specialized coding LLM agent / multi-agent tool for software engineering",
            "domain": "software engineering / experiment setup and code generation",
            "generation_capability": "generates code and implementation artifacts for experiments",
            "validation_method": "In the benchmark it was given a system prompt to act as a professional experimenter, but lacks Curie's integrated validators; validation is primarily via generated tests/usage and the external LLM judge/manual checks used by the benchmark.",
            "novelty_measure": "Performance measured against benchmark tasks and ground truth; tends to perform better on familiar coding tasks with clear starter code/benchmarks and worse on novel complex experimentation tasks.",
            "generation_performance": "Per-benchmark, OpenHands achieves moderate-to-good design scores but lower execution/setup and conclusion correctness compared to Curie; text reports it outperforms Magentic on some coding aspects but still has execution failures (examples: syntax errors, unresolved dependencies).",
            "validation_performance": "Limited automated validation capability; relies on generated code and external evaluation; showed markedly lower validated conclusions relative to Curie (e.g., Curie reported to have much higher conclusion accuracy; Curie achieved 3.4× improvement versus strongest baseline).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance declines substantially on novel/complex tasks; OpenHands benefits from familiar, short execution runs and well-established benchmarks.",
            "generation_validation_asymmetry": "Generates competent code but lacks systematic validation leading to cascading failures that undermine final validated conclusions.",
            "out_of_distribution_performance": "Degrades with complexity and OOD tasks; specific quantitative degradation not provided but empirical results show large gaps versus Curie.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "In the benchmark, any gap was addressed externally via LLM judge and manual checks; OpenHands itself lacks integrated rigor modules.",
            "evidence_type": "supports",
            "key_findings": "OpenHands produces strong coding artifacts for familiar tasks but lacks built-in, continuous validation and therefore underperforms on multi-step, novel experimentation workflows, producing more execution failures and lower validated conclusions.",
            "uuid": "e2148.5"
        },
        {
            "name_short": "Magentic",
            "name_full": "Microsoft Magentic (generalist multi-agent baseline)",
            "brief_description": "A generalist multi-agent system used as a baseline; capable of broad task coverage but performs poorly on rigorous experiment workflows and file/location handling in the benchmark.",
            "citation_title": "Magentic-one: A generalist multi-agent system for solving complex tasks",
            "mention_or_use": "use",
            "system_name": "Magentic",
            "system_type": "generalist multi-agent LLM system",
            "domain": "general problem solving and multi-agent coordination applied to experimentation tasks",
            "generation_capability": "generates plans, coordination steps, and code, but without Curie's dedicated rigor engine",
            "validation_method": "In the benchmark it received a system prompt as experimenter but lacks Curie's Intra/Inter-ARM validators; validation performed externally by the benchmark's LLM judge and manual checks.",
            "novelty_measure": "Measured via benchmark tasks and complexity; exhibits very poor performance on high-complexity and novel tasks in the study.",
            "generation_performance": "Reported to underperform across most complexity levels and domains; often fails to find correct files or manage I/O in starter code, leading to execution failures and near-zero conclusion scores on hardest tasks.",
            "validation_performance": "Very low validated conclusion accuracy in the benchmark (often 0% on hard settings); lacks automated reproducibility checks and thus suffers cascading failures.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Strong negative effect: Magentic's validation/conclusion correctness falls to near-zero for many high-complexity or novel tasks.",
            "generation_validation_asymmetry": "Significant asymmetry: generated plans/code frequently fail when not tightly scaffolded, and there is no integrated validation to catch/repair errors leading to poor final validation.",
            "out_of_distribution_performance": "Performs poorly OOD and on complex tasks; specific metrics not provided but qualitative descriptions indicate catastrophic drops.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "None integrated in base Magentic as evaluated; benchmark uses external evaluation and manual checks.",
            "evidence_type": "supports",
            "key_findings": "Magentic's generalist multi-agent approach lacks systematic validation mechanisms and performs poorly on rigorous experimental tasks, especially for novel or complex experiments, highlighting the need for integrated validation.",
            "uuid": "e2148.6"
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (underlying LLM used in agents and judges)",
            "brief_description": "The underlying large language model used as the base model for Curie and baselines in the benchmark; used to power Architect and Technician agents as well as the LLM judge evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o",
            "system_type": "large language model",
            "domain": "general language understanding and generation applied to scientific experiment planning, code generation, and evaluation",
            "generation_capability": "generates experimental plans, code, natural-language justifications, and evaluation verdicts (when used as judge)",
            "validation_method": "When used for validation (LLM judge), it compares logs to ground truth and outputs structured verdicts; when used as a generating agent it relies on Curie's validation modules for downstream verification.",
            "novelty_measure": "No intrinsic novelty quantification reported; benchmark treats novelty via task complexity and ground-truth comparisons rather than distance-from-training-data metrics.",
            "generation_performance": "Serves as the base LLM; observed system-level performance differences are primarily attributed to architectural/rigor augmentations (Curie) rather than the base model variant itself.",
            "validation_performance": "As judge, effective on straightforward checks but limited on semantic implementation alignment; no standalone numeric validation metrics provided.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "LLM-based judgments and generations are more error-prone on novel or complex tasks; Curie's infrastructure reduces downstream impact.",
            "generation_validation_asymmetry": "Base LLMs can generate plausible but unverified outputs; without external validators they contribute to the fabrication-validation gap.",
            "out_of_distribution_performance": "Declines on novel/complex tasks; benchmark results show agent architectures matter more than base-LM for robustness.",
            "calibration_quality": "Not reported in the paper.",
            "validation_computational_cost": "Not reported specifically for GPT-4o calls; overall system cost depends on repeated runs and multi-agent orchestration.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Use of structured validation modules around the base LLM (as in Curie) and hybrid LLM-judge + human review.",
            "evidence_type": "mixed",
            "key_findings": "Using a capable base LLM like GPT-4o is necessary but insufficient for rigorous automated experimentation; the architectural validation layers determine the ultimate reliability on novel tasks.",
            "uuid": "e2148.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language monkeys: Scaling inference compute with repeated sampling",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Toward rigorous assessment of language agents for data-driven scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Magentic-one: A generalist multi-agent system for solving complex tasks",
            "rating": 2
        },
        {
            "paper_title": "OpenHands",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-ajudge with mt-bench and chatbot arena",
            "rating": 1
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants",
            "rating": 1
        },
        {
            "paper_title": "Automating scientific discovery through multi-agent intelligent graph reasoning",
            "rating": 1
        }
    ],
    "cost": 0.0171785,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents
26 Feb 2025</p>
<p>Patrick Tser 
Jern Kon 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Jiachen Liu 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Qiuyi Ding 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Yiming Qiu 
Zhenning Yang 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Yibo Huang 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Jayanth Srinivasa 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Cisco Systems</p>
<p>Myungjin Lee 
Cisco Systems</p>
<p>Mosharaf Chowdhury 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Ang Chen 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents
26 Feb 2025B6A409D7B1A744479206BB7AA7280F28arXiv:2502.16069v2[cs.AI]
Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results.Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge.To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability.To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects.Compared to the strongest baseline tested, we achieve a 3.4× improvement in correctly answering experimental questions.Curie is open-sourced at https: //github.com/Just-Curieous/Curie.</p>
<p>Introduction</p>
<p>Scientific research drives human progress, advancing medicine, technology, and our understanding of the universe.At the heart of this endeavor lies experimentation-a disciplined intellectual pursuit that transforms human curiosity, expressed through bold hypotheses, into verifiable knowledge.Experimentation thrives on creativity, as new ideas fuel discovery.Yet it also depends on rigor-ensuring that research is methodologically sound and its findings are trustworthy (Armour et al., 2009;Gill &amp; Gill, 2020).If science isn't rigorous, it's reckless (Hofseth, 2018).</p>
<p>In recent years, numerous works (Zhang et al., 2024b;Kramer et al., 2023;Lu et al., 2024) leveraging large language models (LLMs) to automate scientific research have emerged ( §2.3).These solutions typically rely on ad-hoc prompt-based methods to mimic scientific workflows, which are prone to hallucination.While effective for creative tasks such as literature review and brainstorming, these approaches remain limited in their ability to support rigorous experimentation, a largely unexplored capability.</p>
<p>More specifically, rigorous experimentation ( §2.2) involves a methodical procedure that includes formulating hypotheses, designing experiments, executing controlled trials, and analyzing results.Achieving reliability at every step is essential to ensure that the results are accurate, reproducible, and scientifically meaningful.Finally, all procedures and results must be documented in a well-structured and interpretable manner, facilitating verification, reproducibility, and collaboration across the scientific community.</p>
<p>To meet these requirements, we propose Curie, an AI agent framework representing the first step toward rigorous and automated experimentation ( §3).As shown in Fig. 1, Curie takes an experimental question and relevant context (e.g., domain-specific knowledge or starter code) as input.The Architect Agent generates high-level experimental plans, coordinates the process, and reflects on findings to guide subsequent steps.Working in unison, our Technician Agents focus on carefully implementing and executing Curie can help researchers validate, expand, and critique existing research on the benefits of repeated sampling in LLM reasoning (Brown et al., 2024).The first panel (Original Finding) presents a result from the original paper.The second panel (Reproduce) has Curie confirming this finding through rigorous experimentation.The third panel (Extend) has Curie exploring the impact of sampling temperature on repeated sampling.The final panel (Challenge) shows Curie identifying a limitation in the original methodology, suggesting an avenue for future research.controlled experiments following these plans.</p>
<p>At the core of Curie, the Experimental Rigor Engine preserves agent creativity while embedding rigor seamlessly throughout the experimentation process.This is achieved via three key modules: (1) The Intra-Agent Rigor Module safeguards reliability within individual agents by enforcing a set of extensible rigor policies (e.g., validating that experiment plans align with objectives and setups are reproducible).(2) The Inter-Agent Rigor Module maintains methodical control over agent coordination, ensuring correct task transitions and efficient task scheduling.(3) Finally, the Experiment Knowledge Module enhances interpretability by maintaining well-structured documentation, enabling seamless collaboration in large-scale experiments.</p>
<p>Though our architecture suggests applications across various disciplines, this paper focuses on addressing research problems in computer science by leveraging existing LLMfriendly interfaces for computer access (Anthropic, 2024;Yang et al., 2024).To evaluate Curie, we introduce an Experimentation Benchmark comprising 46 tasks of varying complexity across multiple domains within computer science ( §4).We derive these questions directly from influential research papers and widely adopted open-source projects, in order to reflect real-world challenges and practical significance.As shown in Fig. 2, Curie enables researchers to reproduce, extend, and challenge existing research through rigorous experimentation.</p>
<p>We benchmarked Curie ( §5) against several state-ofthe-art agents: OpenHands (Wang et al., 2024c) (a topperforming coding agent on SWE-Bench (Jimenez et al., 2023)), and Microsoft Magentic (Fourney et al., 2024) (a state-of-the-art generalist multi-agent system).Our empirical findings show that Curie achieves a 3.4× improvement in correctly answering experimental questions, compared to the strongest baseline tested, among other aspects.These results underscore Curie's ability to automate complex and rigorous experimentation tasks, making it a promising step toward accelerating scientific research.</p>
<p>Background</p>
<p>Science Experimentation</p>
<p>Scientific experimentation often starts with researchers posing testable hypotheses based on their past results, domain knowledge, and intuition.This experimentation process then unfolds across three key stages: (1) Experimental Design, where researchers plan the controlled experiment by identifying variables, selecting methodologies, and outlining procedures to enhance reproducibility and validity.(2) Experiment Execution, where researchers set up the complex experiment environments and iteratively explore vast search spaces, and (3) Data Documentation and Analysis, where researchers systematically gather data, apply analytical techniques, and extract insights to validate or refine their hypotheses.This process is iterative, as insights gained from data analysis often lead to the refinement of hypotheses, leading to subsequent rounds of these three steps.</p>
<p>Rigor in Experimentation</p>
<p>Rigor is essential in scientific research, ensuring systematic, precise, and reliable findings (Armour et al., 2009).If science isn't rigorous, it's reckless.(Hofseth, 2018).More precisely, experimental rigor is grounded in three core principles (Gill &amp; Gill, 2020):</p>
<p>Methodical Procedure: Experimentation must adhere to a principled and systematic methodology throughout all aforementioned stages, from hypothesis formulation to data documentation.Such a structured procedure ensures that no critical procedures are overlooked or performed incompletely, thereby preserving the integrity of the research.</p>
<p>Reliability: Every stage in the experimental pipeline-such as experiment design and environment setup-needs to be reliable and reproducible so that any final findings rest on solid ground.For instance, it encompasses correct variable identification, controlled experimental design, and rigorous code verification.By meticulously verifying each stage, reliability minimizes the risk of cascading errors, thereby ensuring that the results are trustworthy.</p>
<p>Interpretability: All processes and outcomes need to be clearly documented in a consistent manner.This makes it easier for researchers or agents to replicate experiments, understand results, and extend research.</p>
<p>Related Work</p>
<p>AI Agents for Science.Prior work has leveraged AI to accelerate scientific discovery (Berens et al., 2023;Kitano, 2021), focusing on various stages of the research lifecycle, including literature reviews (Agarwal et al., 2024;Tyser et al., 2024), brainstorming ideas (Gu &amp; Krenn, 2024;Bran et al., 2024), hypothesis generation (Sourati &amp; Evans, 2023;Zhou et al., 2024;Wang et al., 2024a;Qi et al., 2024) and data analysis (Hong et al., 2024a;Chen et al., 2024).While these efforts works on various aspects of the scientific lifecycle, experimentation-a critical, rigor-intensive step-remains underexplored.</p>
<p>Existing agents for end-to-end scientific research (Schmidgall et al., 2025;Lu et al., 2024;Yuan et al., 2025;Ghafarollahi &amp; Buehler, 2024) rely on ad-hoc prompts to guide predefined workflows, from idea generation to paper writing.Their open-sourced frameworks often require ex-perimental code to follow constrained, framework-specific formats, adding overhead and hindering their usability.These solutions mimic experimentation processes using multi-agent systems but lack systematic enforcement of a methodical procedure, reliability, and interpretability.Without these core principles, such agents struggle to deliver meaningful and reproducible results, limiting their practical utility in real-world scientific research.</p>
<p>AI Agent Task Benchmarks.A wide range of benchmarks have been developed to assess the capabilities of AI agents across diverse domains.Existing benchmarks primarily focus on logical reasoning (Cobbe et al., 2021;Hendrycks et al., 2021a;Bang et al., 2023), problem-solving (Hendrycks et al., 2021b;Frieder et al., 2023;Wang et al., 2024b;Sun et al., 2024a;Chevalier et al., 2024), knowledge retrieval tasks (Sun et al., 2024b) and machine learning training (Huang et al., 2024;Zhang et al., 2023;2024a).These benchmarks evaluate agents on well-defined tasks that typically have clear, deterministic solutions.</p>
<p>In contrast, our benchmark focuses on experimentation, which requires a more rigorous and systematic approach beyond problem-solving.Experimental tasks require iterative hypothesis refinement, complex experiment setup and execution, and robust result interpretation.Our benchmark captures these challenges by evaluating AI systems on real-world experimentation tasks derived from influential research papers and widely adopted open-source projects.</p>
<p>Curie: Rigorous Experimentation</p>
<p>Architectural Overview</p>
<p>As shown in Fig. 3, Curie is composed of two types of LLM-based agents (an Architect Agent and a host of Tech- nician Agents), sandwiched between them is our main innovation, the Experimental Rigor Engine that injects rigor throughout the experimental process.</p>
<p>High-level workflow.Given an experimental question, our Architect will 1 designs high-level experimental plans (e.g., defining hypotheses, variables), completing its turn.Our Inter-Agent Rigor Module (Inter-ARM ) will A intercept and enforce methodical procedure.Since the plan is new, it is broken into smaller partitions for finer-grained execution.Inter-ARM applies control flow policies to determine the next step for each partition.In this case, it decides go through the B the Intra-Agent Rigor Module (Intra-ARM ) validation, which enhances reliability by verifying partition integrity (e.g., assessing relevance to the experimental question).Similarly, Inter-ARM repeats this process based on the validation results, eventually C forwarding the partition to a Technician to 2 set up the controlled experiment.The remaining steps are omitted for brevity, but at a high level, every agent action follows the same structured workflow: A interception by Inter-ARM, B validation by Intra-ARM, and C forwarding to the next appropriate agent.Finally, all of the above components will make use of our Experiment Knowledge Module for storing and tracking experimental progress, providing interpretability.For example, the Architect stores refined experimental plans in a structured, metadata-enriched format, making them easier to analyze, track, and validate over time.</p>
<p>Intra-Agent Rigor Module -Reliability</p>
<p>Large-scale and long-running experiments involve complex, interdependent steps where early-stage errors can propagate and compromise final results.This is especially critical to LLM-based experimentation since: (1) LLM-based agents are prone to hallucination, and (2) experimental processes are inherently exploratory, requiring iterative refinements to hypotheses, setups, and designs in response to new or unexpected findings.Despite this, existing works (Lu et al., 2024;Schmidgall et al., 2025) largely overlook the need for  continuous validation throughout the experimental process.A naive approach is to perform end-to-end validation only after an experiment concludes.However, this lacks the ability to backtrack to intermediate stages, preventing error isolation and correction, and forcing researchers to either discard progress or rerun the entire experiment-an inefficient and costly approach.To address this, we introduce Intra-ARM, a validation module that verifies the assigned tasks of our Architect and Technicians step by step, improving reliability and reproducibility to align with the overarching experimental objectives.Inspired by process supervision (Lightman et al., 2023), Intra-ARM utilizes modular validation, where a suite of validators continuously verifies each stage of the experiment (Fig. 3), so that errors can be proactively detected and addressed early.Moreover, Intra-ARM 's validators are extensible, allowing new ones to be incorporated as needed.We focus on two key validators here for brevity:</p>
<p>Experimental Setup Validator.This component (Fig. 4) verifies that the experimental setup by our technicians aligns with the plan before execution, ensuring methodological soundness and logical consistency.Each enforced policy checks alignment within a specific part of the experiment setup.This includes (Fig. 5a): (1) confirming the setup aligns with the experimental plan, including the research question and all specified variables (independent, dependent, and constant).( 2) Analyzing all procedures for correct handling of input/output arguments; and detecting placeholders, hardcoded values, or incomplete variables to ensure meaningful results.(3) Checking that the setup documents all intermediate steps and expected results, including any identified issues for future analysis.</p>
<p>Execution Validator.Once the setup passes the experimental setup validator, this validator enhances reproducibility by executing it in a controlled and clean environment to detect and resolve potential errors, a sample of which is illustrated in Fig. 5b.(1) Error-Free Execution: The setup is executed in a clean environment, verifying that it operates without errors.Any encountered errors are logged in detail, providing actionable feedback for debugging and iterative refinement.(2) Reproducibility Checks: The workflow is also run multiple times to enhance consistency in outputs and detect anomalies or hidden dependencies.Finally, the results are validated to ensure alignment with the experimental plan and compliance with predefined quality standards.</p>
<p>Inter-Agent Rigor Module -Methodical Control</p>
<p>Experimental processes must follow a methodical precedure ( §2.2) while balancing resource constraints (e.g., GPU availability), and experiment priorities.Traditional agentic conversational patterns (AutoGen, 2024)-such as naive LLM-based coordination, sequential, or round-robin execution-are thus ill-suited for such a workflow.To ensure task coordination and optimize resource efficiency, Inter-ARM enables seamless collaboration between our Architect, Technicians and Intra-ARM through three key functions (illustrated in Fig. 6).We discuss each in turn.</p>
<p>Fine-grained Plan Partitioning.Inter-ARM first breaks down new complex experimental plans generated by the Architect into smaller, independent partitions: defined as a distinct subset of independent variable values within the plan.By creating smaller, self-contained tasks, this facilitates modular execution and enables parallelization, making experimentation more scalable.In addition, this enables our Architect to track intermediate progress and results, making real-time decisions as new insights emerge (e.g., reprioritizing partitions by updating their execution priority).</p>
<p>Control Flow Enforcement.This component ensures that transitions between our Architect, Technicians, and Intra-ARM follow a logical sequence aligned with the experimentation lifecycle.This is critical to maintaining consistent, error-free progress.Without structured coordination, tasks may be executed out of order or without necessary dependencies, leading to wasted effort and erroneous conclusions.For instance, it prevents Technicians from directly executing experiment setups before validation by Intra-ARM 's setup validator, to reduce the risk of erroneous data propagation.This is done in two steps: (1) State Evaluation: First, it evaluates the current state of each partition (within an experimental plan) that has been modified by any given agent, e.g., a Technician who produced experimental results and recorded its progress via the Experiment Knowledge Module.(2) Permissible State Transitions: Based on the current state of the partition(s), this component produces a set of allowed state transitions for the given partition, e.g., newly produced experimental results for a given partition need to be validated by Intra-ARM first.It also gathers relevant context that would be useful if the transition were to be executed.This state transition information will be consumed by our scheduler (defined below).Partition Scheduling.Executing large-scale experiments can be resource-intensive and time-consuming, requiring careful scheduling and prioritization of tasks to improve efficiency.Our scheduler currently utilizes three key parameters for partition scheduling: (1) partition execution priorities set by our Architect, (2) allowed partition state transitions, and (3) the availability of our agents (that may be busy handling other partitions).Overall, this adaptive scheduling strategy enables large-scale experimentation by improving resource efficiency while adhering to methodical experimental procedures.</p>
<p>Experiment Knowledge Module -Interpretability</p>
<p>Interpretability is fundamental to experimentation-not only for scientific accountability but also for effective experiment management.Specifically, all other components within Curie require this for real-time visibility, enabling informed decision-making, efficient troubleshooting, and adaptability as new insights emerge.A naive approach would be to delegate experimental knowledge management entirely to LLM-based agents.However, LLMs alone are ill-suited for this task for two reasons: (1) Inconsistent Reads: LLMs have inconsistent recall and are prone to forgetting (Xu et al., 2024).Without a structured and verifiable record of experimental progress, they may retrieve outdated, irrelevant, or hallucinated information, leading to misinterpretations, flawed conclusions, and compounding errors over time.(2) Inconsistent Writes: LLMs tend to hallucinate, particularly when managing large-scale experimental data.This lack of structured control risks corrupting experimental records, propagating inaccuracies, and ultimately compromising the integrity of the experimentation process.Unlike databases, LLMs do not inherently track provenance (Hoque et al., 2024), making it difficult to reconstruct how conclusions were reached.We address these two challenges in turn:</p>
<p>Structured Knowledge Reads.This mechanism organizes experimental progress in a structured format.The process begins by restructuring new experimental plans that were written by our Architect into an enriched format with critical metadata-such as setups, execution status, and results.Subsequent modifications to any part of the plan are recorded as a time machine (Fig. 7) for experimental progression, maintaining a structured, DAG-like history of changes.This historical record captures hypotheses tested, variable changes, and the reasoning behind key decisions.By preserving this evolution, Curie can reconstruct past states, trace decision rationales, and diagnose issues with greater precision.Tiered Write Access.To maintain experimental integrity and minimize the risk of errors, the interface enforces a tiered write access policy that restricts and validates updates made to the experimental plan.This ensures that our other components can only modify the portions of the plan they are responsible for, while all changes undergo rigorous validation.Our LLM-based Architect and Technicians are granted fine-grained write permissions tailored to their roles.For example, Technicians are permitted to append experimental results to their assigned partitions but cannot modify unrelated sections of the plan.Similarly, architects have broader write access, including the ability to create or remove entire partitions, but their modifications are still constrained to specific attributes, such as updating variable values or marking partitions for re-execution.Every write operation is validated before being committed to the knowledge bank.This process ensures proper structuring of inputs and enforces semantic integrity (e.g., that result file paths are valid).If errors are detected, the system returns concise error messages, enabling agents to quickly identify and resolve issues.Through this, Curie enhances robustness and error resistance in collaboration.</p>
<p>Experimentation Benchmark</p>
<p>We design a novel benchmark to stress test Curie's ability to automate experiments while enforcing rigor in the face Investigates strategies for scaling test-time computation in LLMs, focusing on balancing accuracy, latency, and cost.</p>
<p>Research papers: (Brown et al., 2024), (Jin et al., 2024).</p>
<p>Vector Indexing 6 6 3</p>
<p>Examines efficient vector indexing methods for similarity search, analyzing its trade-offs in retrieval recall, memory, and latency.</p>
<p>Open-source project: Faiss (Douze et al.,</p>
<p>Experiment-Centric Task Design</p>
<p>Instead of treating tasks as isolated problems with fixed solutions, we structure each task as a full experimental process.This means that tasks require hypothesis formation, iterative refinement, and rigorous validation, mirroring real-world experiment workflows rather than one-shot problem-solving.</p>
<p>The process begins with distilling high-level contributions from research papers (e.g., theoretical insights or empirical findings), or core system behaviors from open-source projects (e.g., the interplay between configuration parameters and performance).These insights are then translated into testable questions framed with explicit configurations, metrics, and expected outcomes.Ground truth data is derived from published results or official benchmarks provided by open-source projects.We use these findings to design tasks with three key components:</p>
<ol>
<li>Experiment Formulation: Each task specifies the (a) Experiment Question (e.g., optimizing performance, identifying relationships); (b) Practical constraints (e.g., resource budgets); (c) High-level Setup Requirements -Contextual details such as datasets, and experimental environments.This framing ensures that tasks are open-ended, requiring iterative exploration rather than one-shot solutions.3. Ground Truth: This is defined in two key areas: (a) Experimental Design: Does the agent correctly formulate the experiment, identifying relevant variables and methodologies?(b) Result Analysis: Does the agent correctly interpret findings, and justify its conclusions?We outline the expected outcomes or acceptable solution ranges.</li>
</ol>
<p>Experimental Context</p>
<p>Experimental Complexity</p>
<p>Experimental research varies in complexity across different dimensions.Our benchmark reflects this by structuring tasks into a hierarchical framework, assessing an agent's ability to handle increasingly sophisticated experimentation tasks.Unlike standard benchmarks that classify tasks by a single difficulty metric (e.g., easy, medium, hard), ours structures complexity along experiment-driven dimensions (detailed definitions in App.A):</p>
<p>1).Design Complexity: The complexity of structuring an experiment (e.g., requiring hypothesis refinement), including defining the scope of exploration, selecting key variables, and structuring parameter spaces-ranging from discrete to continuous and from sparse to dense configurations.</p>
<p>2). Experiment Setup Complexity:</p>
<p>The difficulty of initializing and configuring the experimental environment, from simple predefined setups to intricate dependencies requiring multi-step configuration.</p>
<p>3).Relationship Complexity: The interactions between variables and outcomes, from simple linear dependencies to complex non-monotonic relationships.</p>
<p>4). Experiment Goal Complexity:</p>
<p>The number of compet-</p>
<p>Evaluation</p>
<p>We evaluate Curie using our experimentation benchmark, which consists of 46 research tasks spanning varying complexity levels across four key domains ( §4).To enhance statistical robustness, each task is executed independently for five trials for each of our baselines (below) and Curie, and we report the average performance across these trials.Apart from our main results described in §5.1, our evaluation includes our case studies (Fig. 2 and App.B), and additional results (App.C).</p>
<p>Baselines.We compare Curie with two state-of-theart AI agents as our baselines: OpenHands (Wang et al., 2024c), a top-performing coding agent, and Microsoft Magentic (Fourney et al., 2024), a generalist multi-agent system.These baselines were selected because our benchmark primarily focuses on coding-related tasks within computer science, where both models demonstrate strong performance, with the expectation that Magentic, as a generalist multiagent system, may be able to generalize to experimental tasks too.To ensure fairness, each baseline is provided with a detailed system prompt instructing them to act as a professional experimenter (see App. E.1).All baselines and Curie utilize GPT-4o as the underlying LLM.</p>
<p>Performance Metrics.We assess performance using four key metrics, each evaluated as a binary score per task, ensuring rigor at every stage of the experimentation process:</p>
<ol>
<li>
<p>Experiment Design -Ability to structure the high-level experiment plan to address the research question.</p>
</li>
<li>
<p>Execution Setup -Ensuring that the generated code (experiment setup) is executable and produces consistent results across multiple runs.</p>
</li>
<li>
<p>Implementation Alignment -Faithfulness of the experimental setup with the proposed plan.</p>
</li>
</ol>
<p>Conclusion Correctness -Accuracy in reflecting the</p>
<p>ground truth answer to the experimental question.</p>
<p>Evaluator.We employ an LLM judge (Zheng et al., 2023) for straightforward verification such as checking design, setup and conclusion, where the ground truth is provided.However, we manually assess the implementation alignment, as detecting semantic discrepancies between the intended methodology and code is non-trivial.To ensure accuracy, we also verify the LLM judge's assessments by cross-checking a subset of its evaluations against expert annotations, measuring agreement rates, and refining the judge system prompt.Details of the evaluation prompts are provided in App.E.2.This hybrid evaluation approach enables reliable and scalable assessment of experimentation performance.</p>
<p>Benchmark Performance</p>
<p>Table 2 shows aggregated success rates across all performance metrics and benchmark task domains.</p>
<p>Performance Breakdown By Metric.Across all four metrics, Curie consistently outperforms the baselines, demonstrating the benefits of our Experimental Rigor Engine in improving experimentation performance.(i) For experiment design correctness, all frameworks perform well since the current tasks are relatively straightforward and do not require iterative refinement.However, for more complex research tasks, Curie holds an advantage by dynamically refining hypotheses based on intermediate observations, whereas baselines rely on static planning.Our experimental knowledge module further enhances performance by improving recall and adaptation.(ii) For execution setup and implementation alignment, Curie demonstrates higher reliability, as Intra-ARM proactively validates and corrects execution steps, while Inter-ARM guarantees that we follow methodical task transitions.This results in particularly strong execution setup performance, from 66.7% to 92.7%.Open-Hands (with 32.4% and 40.2%), as a coding-specialized agent, outperforms Magentic in this aspect.However, it still struggles with incomplete or erroneous setups, including getting stuck in loops, syntax errors, logic mistakes, and unresolved dependencies-leading to execution failures in Curie outperforms the others consistently, with performance generally dropping as complexity increases.</p>
<p>complex environments.Magentic, in particular, performs poorly in locating the correct files in the task starter file and handling script input/output.(iii) Finally, for conclusion correctness, its accuracy is largely constrained by earlier errors, as conclusions rely on the correctness of experimental results.However, Curie maintains a strong lead due to its Experiment Knowledge Module, which systematically documents experimental results for structured data analysis.This enables Curie to achieve a significantly higher conclusion score of 36.1%, compared to 10.5% for OpenHands and 2.3% for Magentic.While Magentic demonstrates relatively decent alignment, it struggles to translate this into meaningful conclusions because of previous cascading errors.</p>
<p>Performance Breakdown By Domain.Across all four task domains, Curie consistently outperforms the baselines, demonstrating Curie's ability to adapt to different research domains.(i) First, for LLM reasoning tasks, Curie performed exceptionally well, achieving the highest conclusion accuracy at 44.9%.OpenHands had its best performance in this category (14.2%), while Magentic attained its only non-zero score of 6.7%.We attribute this to the inherent intuitiveness of conclusions for our tasks in this domain.(ii) For Vector DB tasks, both OpenHands and Magentic achieved their highest alignment scores-52.3%and 63.6%, respectively-likely due to the familiarity of the task.Alignment was also easier given the availability of well-established open-source benchmarks and shorter execution runs, which provided faster feedback.(iii) For Cloud Computing tasks, Curie outperformed OpenHands significantly in all aspects (e.g., 6.5× the conclusion accuracy).This is because these tasks often involve long-running experiments, which requires robust execution tracking and dynamical experimentation workflows adjustment based on partial results.(iv) Finally, for ML Training tasks, all agents underperformed in alignment and execution as the detailed environment setup instructions are not provided for these tasks.Despite this, Curie can figure out the correct setup by reflection and refinement, achieving a 7.3× higher conclusion accuracy than OpenHands.</p>
<p>Performance Breakdown by Complexity.Next, we analyze how each framework performs as we increase difficulty within each complexity dimension.Fig. 8 reports the aggregated performance score, computed as the average across all four evaluation metrics.We observe that increasing complexity difficulties across all dimensions correlates with a decline in performance across all agents.However, the rate of degradation varies across complexity types and agent architectures.Notably, Magentic consistently underperforms across all complexity levels, highlighting the robustness of our complexity-based difficulty scaling in distinguishing agent capabilities.Further, we observe a sublinear decline in performance as task complexity increases, suggesting that our hardest tasks could be made even more challenging.Despite this, our current results demonstrate Curie's capabilities, supported by our case studies.Exploring the limit of experimentation difficulty and its impact on model performance remains an open direction for future work.</p>
<p>In summary, our findings underscore the importance of rigorous evaluation across all stages of the experimentation process, shedding light on each framework's strengths and limitations under varying complexity conditions.</p>
<p>Conclusion and Future Work</p>
<p>We introduced Curie, an AI agent framework designed to automate and enhance the rigor of scientific experimentation.Central to its design is the Experimental Rigor Engine, which enforces methodical control, reliability, and interpretability.To assess Curie's effectiveness, we developed a new Experimentation Benchmark featuring real-world research-level challenges.Our empirical evaluation, comparing Curie against state-of-the-art AI agents, demonstrated its capability to automate rigorous experimentation.</p>
<p>We hope Curie inspires further advancements toward fully autonomous and rigorous experimentation in the era of AI agent-driven scientific research.Several open research challenges remain: For instance, adapting Curie for interdisciplinary research requires accommodating domainspecific methodologies, uncertainty control, and extended time scales, such as long-term biological studies (Hilty et al., 2021).Moreover, enabling knowledge reuse (Wang et al., 2024d) across experiments could enhance efficiency and further accelerate discovery.</p>
<p>Impact Statement</p>
<p>We introduce Curie, an AI agent framework designed to ensure methodical control, execution reliability, and structured knowledge management throughout the experimentation lifecycle.We introduce a novel experimentation benchmark, spanning four key domains in computer science, to evaluate the reliability and effectiveness of AI agents in conducting scientific research.Our empirical results demonstrate that Curie achieves higher conclusion accuracy and execution reliability, significantly outperforming state-ofthe-art AI agents.</p>
<p>Curie has broad implications across multiple scientific disciplines, including machine learning, cloud computing, and database systems, where rigorous experimentation is essential.Beyond computer science, our framework has the potential to accelerate research in materials science, physics, and biomedical research, where complex experimental setups and iterative hypothesis testing are critical for discovery.By automating experimental workflows with built-in validation, Curie can enhance research productivity, reduce human error, and facilitate large-scale scientific exploration.</p>
<p>Ensuring transparency, fairness, and reproducibility in AIdriven scientific research is paramount.Curie explicitly enforces structured documentation and interpretability, making experimental processes auditable and traceable.However, over-reliance on AI for scientific discovery raises concerns regarding bias in automated decision-making and the need for human oversight.We advocate for hybrid human-AI collaboration, where AI assists researchers rather than replacing critical scientific judgment.</p>
<p>Curie lays the foundation for trustworthy AI-driven scientific experimentation, opening avenues for self-improving agents that refine methodologies through continual learning.Future research could explore domain-specific adaptations, enabling AI to automate rigorous experimentation in disciplines such as drug discovery, materials engineering, and high-energy physics.By bridging AI and the scientific method, Curie has the potential to shape the next generation of AI-powered research methodologies, driving scientific discovery at an unprecedented scale.In Fig. 9b, the objective of this experiment is to examine the relationship between task complexity and the optimal length of reasoning chains in large language models (LLMs).The experiment maintains constant variables, including the model (gpt-4o-mini), the method (auto cot), and the environment setup (OpenAI credentials and a Conda environment).The independent variable is the number of reasoning steps, controlled through different demo files, while the dependent variable is the model's accuracy, as reported in the log files.The experiment consists of a control group and experimental groups.The control group uses the gsm8k 1 demo file with a single reasoning step to establish a baseline accuracy.The experimental groups involve testing gsm8k with reasoning steps from gsm8k 2 and gsm8k 3, and last letters with reasoning steps ranging from last letters 1 to last letters 10.The results will help determine whether task complexity influences the optimal number of reasoning steps required for maximizing accuracy in LLMs.</p>
<p>Curie extends the scope by analyzing how task complexity relates to the optimal length of reasoning chains.This study differentiates between problem types (e.g., logical inference and mathematical operations) and systematically evaluates the effect of reasoning step count within different datasets (gsm8k and last letters).By introducing controlled experimental conditions, Curie enables a more detailed exploration of how task complexity interacts with reasoning steps to optimize model performance.</p>
<p>C. Extended Evaluation: Fine-grained Performance Breakdown by Individual Metrics</p>
<p>We detail fine-grained breakdowns for each of our performance metrics mentioned in §5.Here we observe the general trend that increasing complexity across all dimensions causes reductions in average metric scores, as shown in Fig. 10, Fig. 11 and Fig. 12, respectively.In particular, we observe that conclusion scores are most heavily affected as complexity increases across dimensions, reaching 0% on many occasions for Magentic in particular.For design complexity on the other hand, we observe that we're able to maintain a relatively high average score across all baselines and Curie, but this tapers down as the difficulty increases across dimensions.-Did you identify a clear, correct hypothesis?-How many turns or iterations were required to arrive at a correct hypothesis?</p>
<ol>
<li>Experimental Setup:</li>
</ol>
<p>-Is the experimental setup reproducible, usable, and interpretable?-Does it meet the rigor required by the scientific method?</p>
<ol>
<li>Results Generation:</li>
</ol>
<p>-Are the results actually produced through experimentation?-Are the results accurate and sufficient to justify your conclusions?</p>
<p>Conclusion Derivation:</p>
<p>-Are the conclusions correct and logically derived from the results?-Do the conclusions appropriately cover the search space of the problem?</p>
<p>Workflow Design:</p>
<p>-Is the experimental workflow cohesive and callable as a single program?-Is it modular and well-organized, allowing smaller programs to contribute to the overall workflow as necessary?</p>
<p>Expectations for Your Behavior:</p>
<p>-Think like a scientist.Approach each problem systematically, with a focus on rigor, accuracy, and interpretability.</p>
<p>-Produce experiments and results that can be scrutinized, reproduced, and used by others.</p>
<p>-Justify your steps and decisions clearly, and ensure your results align with the problem's requirements.</p>
<p>-Your success depends on delivering usable, rigorous, and interpretable experimental workflows that solve the given questions effectively.</p>
<p>-</p>
<p>Figure 1 .
1
Figure 1.Curie overview.</p>
<p>Figure 2 .
2
Figure 2. Case Study.Curie can help researchers validate, expand, and critique existing research on the benefits of repeated sampling in LLM reasoning (Brown et al., 2024).The first panel (Original Finding) presents a result from the original paper.The second panel (Reproduce) has Curie confirming this finding through rigorous experimentation.The third panel (Extend) has Curie exploring the impact of sampling temperature on repeated sampling.The final panel (Challenge) shows Curie identifying a limitation in the original methodology, suggesting an avenue for future research.</p>
<p>Figure 3 .
3
Figure3.Curie workflow with an example task in LLM reasoning.The Architect is responsible for designing high-level plans and reflects on the new findings.The Technician is responsible for implementing and executing the experiments based on the plans.Whenever an agent completes its action (step 1 , 2 , 3 , 4 , 5 ), the Experimental Rigor Engine (steps A ⇀ B ⇀ C ) validates the action, determines next steps, assigns tasks and maintains interpretable experimental progress, ensuring rigor throughout the entire process.</p>
<p>Figure 4 .
4
Figure 4. Intra-ARM setup validation high-level workflow.</p>
<p>(a) Example errors that can be captured by the setup validator.(b) Example errors that can be captured by the execution validator.</p>
<p>Figure 5 .
5
Figure 5. Errors detected by two of Intra-ARM 's many validators.</p>
<p>Figure 6 .
6
Figure 6.Simplified Inter-ARM workflow with a partition state snapshot.Partition, control flow, and scheduling policies are customizable.</p>
<p>Figure 7 .
7
Figure 7. Simplified partial snapshot of an example Time Machine.</p>
<p>:</p>
<p>To ensure agents correctly interpret and execute tasks, the benchmark provides detailed context for each question.This includes: (a) Domain Knowl-edge -Background information essential for interpreting the problem.(b) Starter Code &amp; Tools -Predefined scaffolding to simulate real-world research workflows.</p>
<p>Figure 9 .
9
Figure 9. Case studies on LLM reasoning tasks.</p>
<p>Figure 12 .
12
Figure12.Average design scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.</p>
<p>Table 1 .
1
Experimentation benchmark overview.
DomainComplexity Dist. Easy Med. HardDescriptionSourcesLLM Reasoning457</p>
<p>Table 2 .
2
Main benchmark results in terms of four metrics introduced in §5.We aggregate and average the success rate among all tasks within each domain.The final row presents the weighted average, computed based on the number of tasks in each domain.One Des.Exec.Align.Con.Des.Exec.Align.Con.Des.Exec.Align.Con.
Curie Microsoft Magentic-LLM Reason. OpenHands 98.3 83.3 76.7 44.9 86.7 24.6 36.7 14.2 72.0 9.3 146.7Vector DB97.871.777.225.6 85.0 48.352.311.7 85.06.463.60.0Cloud Comp. 100.0 92.796.932.3 96.9 25.249.25.095.06.333.80.0ML Training95.266.739.341.7 63.1 24.316.75.790.02.925.70.0Weighted Avg. 97.978.173.436.1 83.6 32.440.210.5 82.96.835.22.3ing objectives and trade-offs involved, from single-metricoptimization to multi-objective balancing under constraints.</p>
<p>Average scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.
90Average score (%)25 50 75CurieOpenHandsMagentic0Easy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardFigure 8.</p>
<p>Average alignment scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.Average conclusion scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.
100Average score (%)25 50 75CurieOpenHandsMagentic0Easy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardEasy Medium Hard0 100 Figure 10. Easy Medium Hard 25 50 75 Average score (%)Easy Medium Hard CurieEasy Medium Hard OpenHandsEasy Medium Hard MagenticEasy Medium Hard0 100 Figure 11. Easy Medium Hard 25 50 75 Average score (%)Easy Medium Hard CurieEasy Medium Hard OpenHandsEasy Medium Hard MagenticEasy Medium Hard</p>
<p>Make sure you provide a reproducible experimental workflow (i.e., verify that it is runnable multiple times to produce acceptable results) that can be callable through a single program; name it experimental_workflow.shReminder:Yourrole is to conduct actual experiments and generate real results, no simulations, placeholders, or unverified assumptions are allowed.E.2.LLM Judge System Prompt[System Prompt] You are an strict Experimentation Agent Verifier, responsible for evaluating whether an experimentation agent correctly conducted an experiment based on the experimentation question.You are provided with an experiment log chunk, the original experimentation question, and the ground truth (only contains the conclusion).Your assessment should focus on: 1. Experiment Design -Did the agent structure the correct high-level plan to address the experimentation question?It does not need to write implementation code or execute the plan.2.Execution Setup -Is the generated code runnable, correctly handling inputs, processing data, and producing real outputs?Is the whole experimental workflow generated for reproducibility?3.Implementation Alignment-Is the code properly aligned with the experimentation design and accurately implementing the intended methodology?Ensure: Legitimate handling of inputs and outputs.No hardcoded or mock data.4.Conclusion Correctness -Is the conclusion acceptable by the ground truth?Analyze the provided chunked Log File, and provide a structured evaluation based on the criteria below: Analyze this log chunk and provide your evaluation in the specified JSON format.
Response Format<em> Overall Verdict: Correct / Incorrect</em> Detailed Assessment:<em> Experiment Design: [Pass/Fail]</em> Execution Setup: [Pass/Fail]<em> Implementation Alignment : [Pass/Fail]</em> Conclusion Correctness: [Pass/Fail]* Explanation: [Concisely explanation about the failure reasons, no reasonneeded if the step is missing]"""user_prompt = f"""&gt; Original Experimentation Question:{question}&gt; Ground Truth:{ground_truth}&gt; Log Chunk:{log_chunk}
A. Curie Benchmark Complexity ExplanationWe describe in detail our complexity level definitions in Table.3.B. Case Studies for CurieWe provide two example case studies for LLM reasoning tasks that Curie was able to extend from the paper The Impact of Reasoning Step Length on Large Language Models(Jin et al., 2024).In Fig.9a, the objective of this experiment is to examine whether different models exhibit varying accuracy levels based on the number of reasoning steps.The experiment maintains constant variables, including the dataset (last letters), the method (auto cot), and the evaluation metric (accuracy).The independent variables include the model type (gpt-4o-mini vs. gpt-4o) and the number of reasoning steps(1,2,3,4,5,6,10), while the dependent variable is the model's accuracy.The experiment consists of a control group and experimental groups.The control group uses gpt-4o-mini with a single reasoning step to establish a baseline accuracy.The experimental groups involve testing gpt-4o-mini with reasoning steps ranging from 2 to 10 and gpt-4o with reasoning steps from 1 to 10.The results will help determine whether reasoning step variations impact accuracy differently across models.Curie extends the original investigation by examining whether different LLMs exhibit varying accuracy using GPT-4o and GPT-4o-mini.What is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function?Do not terminate until you identify the best instance type concretely.Easy Medium Easy Medium MediumWhat is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.Easy Easy Medium Hard MediumWhat is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.Easy Medium Medium Medium MediumWhat is the best AWS EC2 instance type within the c5 and t3 families (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.
S Agarwal, I H Laradji, L Charlin, C Pal, Litllm, arXiv:2402.01788A toolkit for scientific literature review. 2024arXiv preprinta new claude 3.5 sonnet, and claude 3.5 haiku. 2024.</p>
<p>Using context to build rigor: Application to two hermeneutic phenomenological studies. M Armour, S L Rivaux, H Bell, 10.1177/1473325008100424Qualitative Social Work. 1473-325081Mar 2009</p>
<p>Conversation patterns. Autogen , 2024</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, 2023</p>
<p>Ai for science: An emerging agenda. P Berens, K Cranmer, N D Lawrence, U Von Luxburg, J Montgomery, 2023</p>
<p>Knowledge graph extraction from total synthesis documents. A M Bran, Z Jončev, P Schwaller, Proceedings of the 1st Workshop on Language+ Molecules (L+ M 2024). the 1st Workshop on Language+ Molecules (L+ M 2024)2024</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. B Brown, J Juravsky, R Ehrlich, R Clark, Q V Le, C Ré, A Mirhoseini, arXiv:2407.217872024arXiv preprint</p>
<p>Z Chen, S Chen, Y Ning, Q Zhang, B Wang, B Yu, Y Li, Z Liao, C Wei, Z Lu, arXiv:2410.05080Toward rigorous assessment of language agents for data-driven scientific discovery. 2024arXiv preprint</p>
<p>Language models as science tutors. A Chevalier, J Geng, A Wettig, H Chen, S Mizera, T Annala, M J Aragon, A R Fanlo, S Frieder, S Machado, A Prabhakar, E Thieu, J T Wang, Z Wang, X Wu, M Xia, W Xia, J Yu, J.-J Zhu, Z J Ren, S Arora, D Chen, 2024</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>The faiss library. M Douze, A Guzhva, C Deng, J Johnson, G Szilvasy, P.-E Mazaré, M Lomeli, L Hosseini, H Jégou, 2024</p>
<p>Magentic-one: A generalist multi-agent system for solving complex tasks. A Fourney, G Bansal, H Mozannar, C Tan, E Salinas, F Niedtner, G Proebsting, G Bassman, J Gerrits, J Alber, arXiv:2411.044682024arXiv preprint</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Automating scientific discovery through multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, Sciagents, 2024</p>
<p>Informing Science: The International Journal of an Emerging Transdiscipline. T Gill, T Gill, 10.28945/4528232020What is research rigor? lessons for a transdiscipline</p>
<p>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models. X Gu, M Krenn, arXiv:2405.170442024arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021a</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021b</p>
<p>Plant growth: the what, the how, and the why. J Hilty, B Muller, F Pantin, S Leuzinger, 10.1111/nph.17610New Phytologist. 23212021</p>
<p>Getting rigorous with scientific rigor. L J Hofseth, Carcinogenesis. 391January 2018</p>
<p>Data interpreter: An llm agent for data science. S Hong, Y Lin, B Liu, B Liu, B Wu, C Zhang, C Wei, D Li, J Chen, J Zhang, arXiv:2402.186792024aarXiv preprint</p>
<p>Meta programming for a multi-agent collaborative framework. S Hong, M Zhuge, J Chen, X Zheng, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, J Schmidhuber, Metagpt, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>The hallmark effect: Supporting provenance and transparent use of large language models in writing with interactive visualization. M N Hoque, T Mashiat, B Ghai, C D Shelton, F Chevalier, K Kraus, N Elmqvist, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, 2024</p>
<p>C E Jimenez, J Yang, A Wettig, S Yao, K Pei, O Press, K Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>Nobel turing challenge: creating the engine for scientific discovery. M Jin, Q Yu, D Shu, H Zhao, W Hua, Y Meng, Y Zhang, M Du, 10.1038/s41540-021-00189-3arXiv:2401.04925Systems Biology and Applications. 2056-718971292024. Jun 2021arXiv preprintThe impact of reasoning step length on large language models</p>
<p>Automated scientific discovery: From equation discovery to autonomous discovery systems. S Kramer, M Cerrato, S Džeroski, R King, 2023</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.200502023arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. B Qi, K Zhang, K Tian, H Li, Z.-R Chen, S Zeng, E Hua, H Jinfang, B Zhou, 2024</p>
<p>S Schmidgall, Y Su, Z Wang, X Sun, J Wu, X Yu, J Liu, Z Liu, E Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Accelerating science with humanaware artificial intelligence. J Sourati, J A Evans, Nature human behaviour. 7102023</p>
<p>A multi-level large language model evaluation benchmark for scientific research. L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, Scieval, 10.1609/aaai.v38i17.29872Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 2024a38</p>
<p>W Sun, L Yan, X Ma, S Wang, P Ren, Z Chen, D Yin, Z Ren, Is chatgpt good at search? investigating large language models as re-ranking agents. 2024b</p>
<p>Ai-driven review systems: Evaluating llms in scalable and bias-aware academic reviews. K Tyser, B Segev, G Longhitano, X.-Y Zhang, Z Meeks, J Lee, U Garg, N Belsten, A Shporer, M Udell, D Te'eni, I Drori, 2024</p>
<p>R Wang, E Zelikman, G Poesia, Y Pu, N Haber, N D Goodman, Hypothesis search: Inductive reasoning with language models. 2024a</p>
<p>Evaluating college-level scientific problem-solving abilities of large language models. X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, Scibench, 2024b</p>
<p>X Wang, B Li, Y Song, F F Xu, X Tang, M Zhuge, J Pan, Y Song, B Li, J Singh, arXiv:2407.16741An open platform for ai software developers as generalist agents. 2024carXiv preprint</p>
<p>Agent workflow memory. Z Z Wang, J Mao, D Fried, G Neubig, 2024d</p>
<p>R Xu, Z Qi, Z Guo, C Wang, H Wang, Y Zhang, W Xu, arXiv:2403.08319Knowledge conflicts for llms: A survey. 2024arXiv preprint</p>
<p>J Yang, C E Jimenez, A Wettig, K Lieret, S Yao, K Narasimhan, O Press, arXiv:2405.15793Swe-agent: Agent-computer interfaces enable automated software engineering. 2024arXiv preprint</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. J Yuan, X Yan, B Shi, T Chen, W Ouyang, B Zhang, L Bai, Y Qiao, B Zhou, 2025</p>
<p>Unleashing the power of large language models in solving machine learning tasks. L Zhang, Y Zhang, K Ren, D Li, Y Yang, Mlcopilot, 2024a</p>
<p>Automl-gpt: Automatic machine learning with gpt. S Zhang, C Gong, L Wu, X Liu, M Zhou, 2023</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, arXiv:2406.108332024barXiv preprint</p>
<p>Judging llm-as-ajudge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Hypothesis generation with large language models. Y Zhou, H Liu, T Srivastava, H Mei, C Tan, 10.18653/v1/2024.nlp4science-1.10Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)Association for Computational Linguistics2024</p>            </div>
        </div>

    </div>
</body>
</html>
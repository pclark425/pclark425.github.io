<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1883 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1883</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1883</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-37.html">extraction-schema-37</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-277313413</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.18865v3.pdf" target="_blank">Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models offers new possibilities for structured exploration of scientific knowledge. Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights. Specifically, we investigate how knowledge unit--especially those tied to methodological design--can be modeled and recombined to yield research breakthroughs. Our proposed framework addresses two key challenges. First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problem-driven contexts. Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential. This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1883.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1883.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disruption Index</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A citation-based metric that quantifies whether a publication displaces prior work (disruptive) or reinforces it (incremental) using downstream citation patterns; values range continuously and are computed from counts of papers that cite the focal paper exclusively, both the focal paper and its references, or only the references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A dynamic network measure of technological change</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>citation metrics (Disruption Index)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Disruption Index (DI) — fraction-based measure using ni, nj, nk</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Explicit numeric example given: Watson & Crick (1953) DI = 0.62; no general scalar 'bias' against novelty reported for DI itself in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>continuous/continuum (DI is a continuous index defined as (n_i - n_j) / (n_i + n_j + n_k) capturing degree of displacement vs reinforcement)</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>DI is inherently based on downstream citation behavior and thus reflects temporal recognition patterns; the paper notes DI captures later paradigm shifts but does not give a fixed time window for convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>cross-disciplinary concept; applied in this paper to computer science (DBLP), biomedicine (PubMed), and patents (PatSnap).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not reported as a single DI value comparison across fields here; DI is used as target label across three datasets (AI, depression/biomedicine, medical robotics) but no cross-field DI-effect magnitude is given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>traditional citation counts and related bibliometrics (h-index, impact factor) contrasted with DI</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>DI itself is used as the target/ground-truth proxy for transformativness in this work and is validated in prior literature against expert assessments and landmark (e.g., Nobel) cases.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>The paper argues qualitatively that raw citation counts measure adoption rather than disruptiveness and therefore mismatch transformative value, but it does not provide a single numeric gap (reports example DI for Watson & Crick only).</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Conceptually compared: DI distinguishes transformational (high DI) vs incremental (low/negative DI). The paper uses DI thresholds (e.g., DI > 0.5 used in hit-rate experiments) but does not report population-level comparative percentages for acceptance or citation delays.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Paper argues multiple bibliometric failures (ignoring negative citations, reinforcing conservative citation behavior) exist, but does not quantify combined effects numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not applicable to DI as a metric; DI is the target label for models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not applicable to DI itself; however, the paper notes that disruptive (high-DI) papers are scarce in historical data, which biases downstream ML models toward low-DI predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>This paper implements DI-aware learning (entropy-weighted loss, secondary learning, deviation-aware alignment) to improve prediction of DI, but those are model-level interventions rather than changes to DI itself.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>The Watson & Crick example (DI = 0.62) is cited as a prototypical early-recognized transformational discovery validated by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Paper references literature showing team size, atypical combinations, and historical context moderate disruptiveness; specifics (e.g., small teams more disruptive) are cited from literature but not re-measured quantitatively here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>DI is computed/used as labels over three datasets in this study: 14,533 DBLP publications (2011–2021), 96,612 PubMed articles (2015–2025), and 6,677 PatSnap patents (2020–2025). The paper trains ML models to predict DI from problem-method summaries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1883.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1883.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation counts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traditional citation-based metrics (citation counts, h-index, journal impact factor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard bibliometric proxies for scientific impact widely used to evaluate publications, but described in this paper as measuring adoption/visibility rather than transformative potential and subject to biases (e.g., ignoring negative citations, favoring incremental work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>citation metrics / bibliometrics (citation counts, h-index, impact factor)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not a novelty measure; treated as an imperfect proxy for impact/adoption</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Described qualitatively as biased toward incremental research and reinforcing conservative citation behavior; the paper provides no aggregate numeric bias estimate for citation counts themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>tends to correlate with adoption/accumulation of attention rather than disruptiveness; described qualitatively (no explicit functional form given).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Implicitly short-to-medium term (citation accumulation) — paper notes citation counts may not capture later-emerging disruptiveness and can be insensitive to negative citations; no explicit time scales quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>general (discussed across science and technology literature), contrasted implicitly with DI in multiple domains used in experiments (AI, biomedicine, patents).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Paper cites literature indicating inequalities and field differences (e.g., Global citation inequality) but does not quantify per-field bias magnitudes here.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>short-term and cumulative citation counts, h-index, journal impact factor</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Paper argues DI or expert retrospective assessments better capture true transformative value; citation counts are treated as imperfect proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not numerically quantified in this paper; described qualitatively as conflating adoption with transformative potential and thus misaligning with DI/expert assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Paper states traditional citation metrics bias toward incremental work (qualitative claim); no numerical comparison provided for acceptance/citation lag between incremental vs transformational in terms of citation counts.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Paper lists multiple issues with citation metrics (bias toward incremental work, ignoring negative citations, reinforcing conservative behavior) but does not quantify how these failures compound.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not directly applicable; citation metrics are passive proxies rather than automated ML evaluators in this context.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Paper notes that ML/evaluation systems trained on historical citation patterns will inherit bias that favors incremental/majority patterns, but no numeric estimate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Paper proposes using DI as an alternative evaluation objective and integrating DI into model training; specific numeric effectiveness of switching proxies is shown indirectly via model prediction improvements but not as a direct correction to citation-based decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Paper references work showing disruptive papers exist and can be identified via DI and expert validation (e.g., Nobel and Watson & Crick examples).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Negative citations, team size, field norms, and historical adoption dynamics are mentioned as moderators of how citation metrics evaluate novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Not an empirical study of citation counts per se in this paper; rather, the work uses DI as target and critiques citation counts referencing prior literature.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1883.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1883.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Our Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Problem-method combination framework with Disruptive Index prediction and Dynamic Method Optimization (the authors' automated system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end system that (1) extracts problem-method pairs from literature, (2) generates summaries via a fine-tuned LLM, (3) predicts Disruption Index using an entropy-weighted and deviation-aware predictor, and (4) optimizes method combinations via Greedy with Probabilistic Perturbation (GPP) to find high-disruptiveness candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>automated ML evaluation system for predicting disruptiveness and proposing method combinations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Uses Disruption Index as the target measure of novelty/transformational impact</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Paper notes scarcity of high-DI samples biases models toward low-DI predictions; ablation quantifies model degradation when bias-mitigation components are removed (e.g., DBLP MSE rises from 0.0093 to 0.0187 without secondary learning).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>Not a direct functional relationship between novelty and external evaluation documented; the framework models DI from problem-method features (learned non-linear mapping via fine-tuned LLMs and predictors).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>The framework uses downstream citation-derived DI which implicitly requires temporal citation accumulation; the paper does not specify a fixed citation lag but notes scarcity of disruptive cases and iterative feedback over K=5 training/testing iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Applied and evaluated on three domains: artificial intelligence (DBLP CCF-A conferences), biomedicine (PubMed depression subset), and medical robotics patents (PatSnap).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Model predictive performance and hit rates vary by dataset: hit rates (DI > 0.5) for Greedy+GPP are 26.3% (DBLP), 28.1% (PubMed), and 24.6% (PatSnap); baseline GPT-4o hit rates were 16.4%, 18.2%, and 14.9% respectively — showing domain differences in absolute performance and gains.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Model-predicted Disruption Index (as proxy for transformative potential); also compared to general LLM outputs and standard greedy search.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Historical DI computed from citation patterns is used as ground-truth labels to supervise models.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Quantified indirectly via prediction errors: Our Framework obtains much lower error (e.g., DBLP MSE = 0.0093) compared to unguided LLMs (e.g., GPT-4o MSE = 0.1191 in Table 3), indicating prior methods poorly approximate DI.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Framework explicitly optimizes for transformational (high DI) combinations; experimental measures show hit-rate improvements for finding DI > 0.5 items: Greedy+GPP 26.3% vs best LLM ~17.2% (example improvement +9.1 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>The system addresses multiple issues simultaneously (sparsity of disruptive labels, relevance extraction, secondary learning, deviation-aware alignment); ablation shows compounded negative effects when modules are removed (e.g., MSE increases across datasets when relevance/extraction or fine-tuning omitted).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Quantitative results: In Table 3 Our Framework MSEs = {DBLP:0.0093, PubMed:0.0154, Patent:0.0218}; Table 5 hit rates (DI>0.5) for Greedy+GPP = {DBLP:26.3%, PubMed:28.1%, PatSnap:24.6%}; improvement over best LLM = +9.1, +9.1, +9.0 percentage points respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>The paper documents that training on historical data (where high-DI samples are rare) biases predictors toward low DI; mitigation includes an entropy-weighted loss (w_i = -log p(ŷ_i)) and secondary learning on top 20% highest-error samples with KL regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Interventions tested include: summarization fine-tuning, relevance assessment + extraction, secondary learning (top 20% high-error reinforcement), deviation-aware alignment, and GPP for search. Effectiveness: ablation (DBLP) MSE rises from 0.0093 to 0.0274 without summarization fine-tuning, to 0.0351 without relevance+extraction, and to 0.0187 without secondary learning — demonstrating substantial performance loss when interventions are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Not directly provided as counter-examples to framework behavior; paper cites canonical disruptive successes (e.g., Watson & Crick) as targets the framework aims to identify.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Data sparsity in emerging fields (lack of historical examples) and dataset/domain differences moderate system performance; adaptive weighting and GPP mitigate some domain-specific search issues.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Empirical evaluation using three datasets: DBLP (14,533 papers 2011–2021), PubMed (96,612 articles 2015–2025), PatSnap (6,677 patents 2020–2025); methods include LoRA fine-tuning of LLMs, semantic retrieval, frozen models for relevance, entropy-weighted losses, secondary learning with KL regularization, and greedy+GPP optimization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1883.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1883.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General-purpose LLM baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General-purpose large language models (GPT-4o, GPT-4 Turbo, Claude 3.5/3.7, Qwen, LLaMA etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf LLMs and PLMs used as baselines to generate/problem-summarize and predict disruptiveness; they generally perform worse than the authors' fine-tuned, DI-aware pipeline in predicting DI and finding high-disruptiveness combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>automated ML systems / LLM-based evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>These models are used to predict DI or generate candidate method combinations; novelty is assessed via downstream DI labels.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Empirically, baseline LLMs show lower hit rates for identifying high-DI combinations (example: GPT-4o hit rates 16.4% DBLP, 18.2% PubMed, 14.9% PatSnap) compared to the authors' GPP-enhanced system (26.3%, 28.1%, 24.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>No explicit mathematical relationship reported; baseline performance is compared empirically to the DI labels.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Not directly analyzed for temporal recognition of novelty; results reflect model performance on datasets compiled over different timespans but no explicit time-to-recognition metrics given for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Same three domains as main experiments (AI/DBLP, PubMed depression, PatSnap patents).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Baseline hit rates vary by field (see bias_magnitude); e.g., GPT-4o performs slightly better on PubMed (18.2%) than on PatSnap (14.9%) in hit-rate for DI>0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Model-predicted DI and their ability to identify DI>0.5 items (hit rate)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Historical DI values computed from citation data</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Baseline models have much larger prediction errors compared to the authors' framework (e.g., GPT-4o MSE = 0.1191 vs Our Framework MSE = 0.0093 on DBLP in Table 3), indicating a substantial mismatch between baseline outputs and DI ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Baselines tend to identify fewer transformational (high-DI) cases (see hit rates above) compared to the DI-aware framework.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not explicitly decomposed, but baseline failures include poor summarization alignment, lack of relevance extraction, and inability to handle label sparsity—components the authors ablate and show harm when removed.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Quantified in tables: example Table 3 (MSE/MAE/WMSE/WMAE) and Table 5 hit rates; baselines consistently have higher MSE and lower hit rates than the proposed system.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Baselines were not adapted with the entropy-weighted or secondary-learning interventions and therefore inherit bias from training corpora; the paper demonstrates improved performance when such interventions are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Not for baselines themselves beyond fine-tuning variants; the paper fine-tunes some PLMs and LLMs and reports improved results for fine-tuned models relative to pre-trained versions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>None provided where baseline LLMs outperform the proposed system on DI prediction; all reported comparisons favor the DI-aware framework.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Model fine-tuning, problem-method summarization quality, and structured information extraction substantially moderate baseline performance (shown by ablation/fine-tuning effects).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Baselines evaluated on the same three datasets (DBLP 14,533; PubMed 96,612; PatSnap 6,677) with consistent training/testing splits and reported metrics (MSE/MAE/WMSE/WMAE, hit rate of DI>0.5).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1883.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1883.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporal decline in disruptiveness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Papers and patents are becoming less disruptive over time (literature-cited trend)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work (Park et al., 2023) reporting a long-term decline in the average disruptiveness of scientific papers and patents; the current paper references this as background motivation for measuring disruptiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Papers and patents are becoming less disruptive over time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>bibliometric temporal trend analysis</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Disruption Index (as used in the cited literature) applied longitudinally to show decline</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>The paper cites this phenomenon qualitatively; no numeric trend statistics from Park et al. are reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>reported as a temporal decline trend (directional), not a specific functional fit provided in the present text.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Cited as a long-term decline in disruptiveness of papers and patents over time (Park et al., 2023), but this paper does not provide the time constants or slopes.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Cross-domain (papers and patents) according to the cited study; current paper uses cross-domain datasets but does not re-estimate the temporal decline.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not quantified here; cited study likely reports cross-field analyses but the present paper does not extract numeric inter-field differences.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Disruption Index measured over historical citation records</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>DI computed across time windows in the cited literature</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not discussed numerically here.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Cited claim implies increasing predominance of incremental work over time, but no numeric comparison provided in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not applicable directly in this mention; used as contextual background.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not explicitly discussed in this citation mention beyond motivating scarcity of high-DI samples for ML training.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Not applicable in this mention.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Paper references literature discussing moderating factors (e.g., team size, burden of knowledge) but does not quantify them here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>The citation refers to an external large-scale longitudinal analysis; this paper does not re-run that analysis but uses its finding as motivation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A dynamic network measure of technological change <em>(Rating: 2)</em></li>
                <li>Papers and patents are becoming less disruptive over time <em>(Rating: 2)</em></li>
                <li>Bias against novelty in science: A cautionary tale for users of bibliometric indicators <em>(Rating: 2)</em></li>
                <li>Atypical combinations and scientific impact <em>(Rating: 2)</em></li>
                <li>Large teams develop and small teams disrupt science and technology <em>(Rating: 2)</em></li>
                <li>The incidence and role of negative citations in science <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1883",
    "paper_id": "paper-277313413",
    "extraction_schema_id": "extraction-schema-37",
    "extracted_data": [
        {
            "name_short": "DI",
            "name_full": "Disruption Index",
            "brief_description": "A citation-based metric that quantifies whether a publication displaces prior work (disruptive) or reinforces it (incremental) using downstream citation patterns; values range continuously and are computed from counts of papers that cite the focal paper exclusively, both the focal paper and its references, or only the references.",
            "citation_title": "A dynamic network measure of technological change",
            "mention_or_use": "use",
            "evaluation_system_type": "citation metrics (Disruption Index)",
            "novelty_measure": "Disruption Index (DI) — fraction-based measure using ni, nj, nk",
            "bias_magnitude": "Explicit numeric example given: Watson & Crick (1953) DI = 0.62; no general scalar 'bias' against novelty reported for DI itself in this paper.",
            "relationship_type": "continuous/continuum (DI is a continuous index defined as (n_i - n_j) / (n_i + n_j + n_k) capturing degree of displacement vs reinforcement)",
            "temporal_pattern": "DI is inherently based on downstream citation behavior and thus reflects temporal recognition patterns; the paper notes DI captures later paradigm shifts but does not give a fixed time window for convergence.",
            "field_studied": "cross-disciplinary concept; applied in this paper to computer science (DBLP), biomedicine (PubMed), and patents (PatSnap).",
            "field_differences": "Not reported as a single DI value comparison across fields here; DI is used as target label across three datasets (AI, depression/biomedicine, medical robotics) but no cross-field DI-effect magnitude is given in the paper.",
            "proxy_metric_studied": "traditional citation counts and related bibliometrics (h-index, impact factor) contrasted with DI",
            "ground_truth_measure": "DI itself is used as the target/ground-truth proxy for transformativness in this work and is validated in prior literature against expert assessments and landmark (e.g., Nobel) cases.",
            "proxy_truth_gap": "The paper argues qualitatively that raw citation counts measure adoption rather than disruptiveness and therefore mismatch transformative value, but it does not provide a single numeric gap (reports example DI for Watson & Crick only).",
            "incremental_vs_transformational": "Conceptually compared: DI distinguishes transformational (high DI) vs incremental (low/negative DI). The paper uses DI thresholds (e.g., DI &gt; 0.5 used in hit-rate experiments) but does not report population-level comparative percentages for acceptance or citation delays.",
            "multiple_proxy_failures": "Paper argues multiple bibliometric failures (ignoring negative citations, reinforcing conservative citation behavior) exist, but does not quantify combined effects numerically.",
            "automated_system_performance": "Not applicable to DI as a metric; DI is the target label for models in this paper.",
            "training_data_bias": "Not applicable to DI itself; however, the paper notes that disruptive (high-DI) papers are scarce in historical data, which biases downstream ML models toward low-DI predictions.",
            "intervention_tested": "This paper implements DI-aware learning (entropy-weighted loss, secondary learning, deviation-aware alignment) to improve prediction of DI, but those are model-level interventions rather than changes to DI itself.",
            "counter_examples": "The Watson & Crick example (DI = 0.62) is cited as a prototypical early-recognized transformational discovery validated by experts.",
            "moderating_factors": "Paper references literature showing team size, atypical combinations, and historical context moderate disruptiveness; specifics (e.g., small teams more disruptive) are cited from literature but not re-measured quantitatively here.",
            "sample_size_and_methods": "DI is computed/used as labels over three datasets in this study: 14,533 DBLP publications (2011–2021), 96,612 PubMed articles (2015–2025), and 6,677 PatSnap patents (2020–2025). The paper trains ML models to predict DI from problem-method summaries.",
            "uuid": "e1883.0"
        },
        {
            "name_short": "Citation counts",
            "name_full": "Traditional citation-based metrics (citation counts, h-index, journal impact factor)",
            "brief_description": "Standard bibliometric proxies for scientific impact widely used to evaluate publications, but described in this paper as measuring adoption/visibility rather than transformative potential and subject to biases (e.g., ignoring negative citations, favoring incremental work).",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_system_type": "citation metrics / bibliometrics (citation counts, h-index, impact factor)",
            "novelty_measure": "Not a novelty measure; treated as an imperfect proxy for impact/adoption",
            "bias_magnitude": "Described qualitatively as biased toward incremental research and reinforcing conservative citation behavior; the paper provides no aggregate numeric bias estimate for citation counts themselves.",
            "relationship_type": "tends to correlate with adoption/accumulation of attention rather than disruptiveness; described qualitatively (no explicit functional form given).",
            "temporal_pattern": "Implicitly short-to-medium term (citation accumulation) — paper notes citation counts may not capture later-emerging disruptiveness and can be insensitive to negative citations; no explicit time scales quantified.",
            "field_studied": "general (discussed across science and technology literature), contrasted implicitly with DI in multiple domains used in experiments (AI, biomedicine, patents).",
            "field_differences": "Paper cites literature indicating inequalities and field differences (e.g., Global citation inequality) but does not quantify per-field bias magnitudes here.",
            "proxy_metric_studied": "short-term and cumulative citation counts, h-index, journal impact factor",
            "ground_truth_measure": "Paper argues DI or expert retrospective assessments better capture true transformative value; citation counts are treated as imperfect proxies.",
            "proxy_truth_gap": "Not numerically quantified in this paper; described qualitatively as conflating adoption with transformative potential and thus misaligning with DI/expert assessments.",
            "incremental_vs_transformational": "Paper states traditional citation metrics bias toward incremental work (qualitative claim); no numerical comparison provided for acceptance/citation lag between incremental vs transformational in terms of citation counts.",
            "multiple_proxy_failures": "Paper lists multiple issues with citation metrics (bias toward incremental work, ignoring negative citations, reinforcing conservative behavior) but does not quantify how these failures compound.",
            "automated_system_performance": "Not directly applicable; citation metrics are passive proxies rather than automated ML evaluators in this context.",
            "training_data_bias": "Paper notes that ML/evaluation systems trained on historical citation patterns will inherit bias that favors incremental/majority patterns, but no numeric estimate provided.",
            "intervention_tested": "Paper proposes using DI as an alternative evaluation objective and integrating DI into model training; specific numeric effectiveness of switching proxies is shown indirectly via model prediction improvements but not as a direct correction to citation-based decisions.",
            "counter_examples": "Paper references work showing disruptive papers exist and can be identified via DI and expert validation (e.g., Nobel and Watson & Crick examples).",
            "moderating_factors": "Negative citations, team size, field norms, and historical adoption dynamics are mentioned as moderators of how citation metrics evaluate novelty.",
            "sample_size_and_methods": "Not an empirical study of citation counts per se in this paper; rather, the work uses DI as target and critiques citation counts referencing prior literature.",
            "uuid": "e1883.1"
        },
        {
            "name_short": "Our Framework",
            "name_full": "Problem-method combination framework with Disruptive Index prediction and Dynamic Method Optimization (the authors' automated system)",
            "brief_description": "An end-to-end system that (1) extracts problem-method pairs from literature, (2) generates summaries via a fine-tuned LLM, (3) predicts Disruption Index using an entropy-weighted and deviation-aware predictor, and (4) optimizes method combinations via Greedy with Probabilistic Perturbation (GPP) to find high-disruptiveness candidates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "automated ML evaluation system for predicting disruptiveness and proposing method combinations",
            "novelty_measure": "Uses Disruption Index as the target measure of novelty/transformational impact",
            "bias_magnitude": "Paper notes scarcity of high-DI samples biases models toward low-DI predictions; ablation quantifies model degradation when bias-mitigation components are removed (e.g., DBLP MSE rises from 0.0093 to 0.0187 without secondary learning).",
            "relationship_type": "Not a direct functional relationship between novelty and external evaluation documented; the framework models DI from problem-method features (learned non-linear mapping via fine-tuned LLMs and predictors).",
            "temporal_pattern": "The framework uses downstream citation-derived DI which implicitly requires temporal citation accumulation; the paper does not specify a fixed citation lag but notes scarcity of disruptive cases and iterative feedback over K=5 training/testing iterations.",
            "field_studied": "Applied and evaluated on three domains: artificial intelligence (DBLP CCF-A conferences), biomedicine (PubMed depression subset), and medical robotics patents (PatSnap).",
            "field_differences": "Model predictive performance and hit rates vary by dataset: hit rates (DI &gt; 0.5) for Greedy+GPP are 26.3% (DBLP), 28.1% (PubMed), and 24.6% (PatSnap); baseline GPT-4o hit rates were 16.4%, 18.2%, and 14.9% respectively — showing domain differences in absolute performance and gains.",
            "proxy_metric_studied": "Model-predicted Disruption Index (as proxy for transformative potential); also compared to general LLM outputs and standard greedy search.",
            "ground_truth_measure": "Historical DI computed from citation patterns is used as ground-truth labels to supervise models.",
            "proxy_truth_gap": "Quantified indirectly via prediction errors: Our Framework obtains much lower error (e.g., DBLP MSE = 0.0093) compared to unguided LLMs (e.g., GPT-4o MSE = 0.1191 in Table 3), indicating prior methods poorly approximate DI.",
            "incremental_vs_transformational": "Framework explicitly optimizes for transformational (high DI) combinations; experimental measures show hit-rate improvements for finding DI &gt; 0.5 items: Greedy+GPP 26.3% vs best LLM ~17.2% (example improvement +9.1 percentage points).",
            "multiple_proxy_failures": "The system addresses multiple issues simultaneously (sparsity of disruptive labels, relevance extraction, secondary learning, deviation-aware alignment); ablation shows compounded negative effects when modules are removed (e.g., MSE increases across datasets when relevance/extraction or fine-tuning omitted).",
            "automated_system_performance": "Quantitative results: In Table 3 Our Framework MSEs = {DBLP:0.0093, PubMed:0.0154, Patent:0.0218}; Table 5 hit rates (DI&gt;0.5) for Greedy+GPP = {DBLP:26.3%, PubMed:28.1%, PatSnap:24.6%}; improvement over best LLM = +9.1, +9.1, +9.0 percentage points respectively.",
            "training_data_bias": "The paper documents that training on historical data (where high-DI samples are rare) biases predictors toward low DI; mitigation includes an entropy-weighted loss (w_i = -log p(ŷ_i)) and secondary learning on top 20% highest-error samples with KL regularization.",
            "intervention_tested": "Interventions tested include: summarization fine-tuning, relevance assessment + extraction, secondary learning (top 20% high-error reinforcement), deviation-aware alignment, and GPP for search. Effectiveness: ablation (DBLP) MSE rises from 0.0093 to 0.0274 without summarization fine-tuning, to 0.0351 without relevance+extraction, and to 0.0187 without secondary learning — demonstrating substantial performance loss when interventions are removed.",
            "counter_examples": "Not directly provided as counter-examples to framework behavior; paper cites canonical disruptive successes (e.g., Watson & Crick) as targets the framework aims to identify.",
            "moderating_factors": "Data sparsity in emerging fields (lack of historical examples) and dataset/domain differences moderate system performance; adaptive weighting and GPP mitigate some domain-specific search issues.",
            "sample_size_and_methods": "Empirical evaluation using three datasets: DBLP (14,533 papers 2011–2021), PubMed (96,612 articles 2015–2025), PatSnap (6,677 patents 2020–2025); methods include LoRA fine-tuning of LLMs, semantic retrieval, frozen models for relevance, entropy-weighted losses, secondary learning with KL regularization, and greedy+GPP optimization.",
            "uuid": "e1883.2"
        },
        {
            "name_short": "General-purpose LLM baselines",
            "name_full": "General-purpose large language models (GPT-4o, GPT-4 Turbo, Claude 3.5/3.7, Qwen, LLaMA etc.)",
            "brief_description": "Off-the-shelf LLMs and PLMs used as baselines to generate/problem-summarize and predict disruptiveness; they generally perform worse than the authors' fine-tuned, DI-aware pipeline in predicting DI and finding high-disruptiveness combinations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_system_type": "automated ML systems / LLM-based evaluators",
            "novelty_measure": "These models are used to predict DI or generate candidate method combinations; novelty is assessed via downstream DI labels.",
            "bias_magnitude": "Empirically, baseline LLMs show lower hit rates for identifying high-DI combinations (example: GPT-4o hit rates 16.4% DBLP, 18.2% PubMed, 14.9% PatSnap) compared to the authors' GPP-enhanced system (26.3%, 28.1%, 24.6%).",
            "relationship_type": "No explicit mathematical relationship reported; baseline performance is compared empirically to the DI labels.",
            "temporal_pattern": "Not directly analyzed for temporal recognition of novelty; results reflect model performance on datasets compiled over different timespans but no explicit time-to-recognition metrics given for LLMs.",
            "field_studied": "Same three domains as main experiments (AI/DBLP, PubMed depression, PatSnap patents).",
            "field_differences": "Baseline hit rates vary by field (see bias_magnitude); e.g., GPT-4o performs slightly better on PubMed (18.2%) than on PatSnap (14.9%) in hit-rate for DI&gt;0.5.",
            "proxy_metric_studied": "Model-predicted DI and their ability to identify DI&gt;0.5 items (hit rate)",
            "ground_truth_measure": "Historical DI values computed from citation data",
            "proxy_truth_gap": "Baseline models have much larger prediction errors compared to the authors' framework (e.g., GPT-4o MSE = 0.1191 vs Our Framework MSE = 0.0093 on DBLP in Table 3), indicating a substantial mismatch between baseline outputs and DI ground truth.",
            "incremental_vs_transformational": "Baselines tend to identify fewer transformational (high-DI) cases (see hit rates above) compared to the DI-aware framework.",
            "multiple_proxy_failures": "Not explicitly decomposed, but baseline failures include poor summarization alignment, lack of relevance extraction, and inability to handle label sparsity—components the authors ablate and show harm when removed.",
            "automated_system_performance": "Quantified in tables: example Table 3 (MSE/MAE/WMSE/WMAE) and Table 5 hit rates; baselines consistently have higher MSE and lower hit rates than the proposed system.",
            "training_data_bias": "Baselines were not adapted with the entropy-weighted or secondary-learning interventions and therefore inherit bias from training corpora; the paper demonstrates improved performance when such interventions are applied.",
            "intervention_tested": "Not for baselines themselves beyond fine-tuning variants; the paper fine-tunes some PLMs and LLMs and reports improved results for fine-tuned models relative to pre-trained versions.",
            "counter_examples": "None provided where baseline LLMs outperform the proposed system on DI prediction; all reported comparisons favor the DI-aware framework.",
            "moderating_factors": "Model fine-tuning, problem-method summarization quality, and structured information extraction substantially moderate baseline performance (shown by ablation/fine-tuning effects).",
            "sample_size_and_methods": "Baselines evaluated on the same three datasets (DBLP 14,533; PubMed 96,612; PatSnap 6,677) with consistent training/testing splits and reported metrics (MSE/MAE/WMSE/WMAE, hit rate of DI&gt;0.5).",
            "uuid": "e1883.3"
        },
        {
            "name_short": "Temporal decline in disruptiveness",
            "name_full": "Papers and patents are becoming less disruptive over time (literature-cited trend)",
            "brief_description": "Cited prior work (Park et al., 2023) reporting a long-term decline in the average disruptiveness of scientific papers and patents; the current paper references this as background motivation for measuring disruptiveness.",
            "citation_title": "Papers and patents are becoming less disruptive over time",
            "mention_or_use": "mention",
            "evaluation_system_type": "bibliometric temporal trend analysis",
            "novelty_measure": "Disruption Index (as used in the cited literature) applied longitudinally to show decline",
            "bias_magnitude": "The paper cites this phenomenon qualitatively; no numeric trend statistics from Park et al. are reproduced in this paper.",
            "relationship_type": "reported as a temporal decline trend (directional), not a specific functional fit provided in the present text.",
            "temporal_pattern": "Cited as a long-term decline in disruptiveness of papers and patents over time (Park et al., 2023), but this paper does not provide the time constants or slopes.",
            "field_studied": "Cross-domain (papers and patents) according to the cited study; current paper uses cross-domain datasets but does not re-estimate the temporal decline.",
            "field_differences": "Not quantified here; cited study likely reports cross-field analyses but the present paper does not extract numeric inter-field differences.",
            "proxy_metric_studied": "Disruption Index measured over historical citation records",
            "ground_truth_measure": "DI computed across time windows in the cited literature",
            "proxy_truth_gap": "Not discussed numerically here.",
            "incremental_vs_transformational": "Cited claim implies increasing predominance of incremental work over time, but no numeric comparison provided in this text.",
            "multiple_proxy_failures": "Not applicable directly in this mention; used as contextual background.",
            "automated_system_performance": "Not applicable here.",
            "training_data_bias": "Not explicitly discussed in this citation mention beyond motivating scarcity of high-DI samples for ML training.",
            "intervention_tested": "Not applicable in this mention.",
            "counter_examples": "Not provided here.",
            "moderating_factors": "Paper references literature discussing moderating factors (e.g., team size, burden of knowledge) but does not quantify them here.",
            "sample_size_and_methods": "The citation refers to an external large-scale longitudinal analysis; this paper does not re-run that analysis but uses its finding as motivation.",
            "uuid": "e1883.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A dynamic network measure of technological change",
            "rating": 2
        },
        {
            "paper_title": "Papers and patents are becoming less disruptive over time",
            "rating": 2
        },
        {
            "paper_title": "Bias against novelty in science: A cautionary tale for users of bibliometric indicators",
            "rating": 2
        },
        {
            "paper_title": "Atypical combinations and scientific impact",
            "rating": 2
        },
        {
            "paper_title": "Large teams develop and small teams disrupt science and technology",
            "rating": 2
        },
        {
            "paper_title": "The incidence and role of negative citations in science",
            "rating": 1
        }
    ],
    "cost": 0.01800475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations
14 Apr 2025</p>
<p>Junlan Chen 
Sun Yat-sen University
GuangzhouChina</p>
<p>Kexin Zhang 
Sun Yat-sen University
GuangzhouChina</p>
<p>Daifeng Li lidaifeng@mail.sysu.edu.cn 
Sun Yat-sen University
GuangzhouChina</p>
<p>Yangyang Feng 
Sun Yat-sen University
GuangzhouChina</p>
<p>Yuxuan Zhang 
Sun Yat-sen University
GuangzhouChina</p>
<p>Bowen Deng 
Sun Yat-sen University
GuangzhouChina</p>
<p>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations
14 Apr 2025567B202A2AAC0C2C15CCA7E343CA733EarXiv:2503.18865v3[cs.AI]Scientific InnovationKnowledge RecombinationLLM ReasoningProblem-Method Structure
The emergence of large language models (LLMs) offers new possibilities for structured exploration of scientific knowledge.Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights.Specifically, we investigate how knowledge units-especially those tied to methodological design-can be modeled and recombined to yield research breakthroughs.Our proposed framework addresses two key challenges.First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problemdriven contexts.Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential.This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</p>
<p>Introduction</p>
<p>Recent advances in Large Language Models (LLMs) have significantly enhanced text understanding and generation capabilities [27,33,2], demonstrating expertlevel performance across various domains [3].In scientific discovery, LLMs have been applied to generate research ideas and synthesize existing knowledge [3,39,8,17], confirming their potential in assisting scientific exploration.However, despite these advancements, existing approaches still exhibit several limitations: (1) the inability to systematically identify and integrate fine-grained knowledge components, resulting in scientific discovery that remains at a macro-level of idea generation rather than precise matching of research problems and methods; (2) the hallucination phenomenon in LLMs, where models generate problem-solving approaches that lack actual literature support, potentially leading research efforts astray; and (3) the absence of objective metrics to assess the transformative impact of newly proposed discoveries, as current methods predominantly rely on subjective expert alignment rather than quantitative evaluations of scientific breakthroughs.</p>
<p>Research has shown that the most influential scientific discoveries primarily stem from the combination-particularly atypical combinations-of traditional ideas from prior work [32,34,36].Innovation emerges when prior innovations or their components are assembled into an original design [16,24,35].Schumpeter [31], a pioneer in innovation theory, posited that innovation is fundamentally "the recombination of elements of production", meaning a novel combination of production elements or conditions.Similarly, Nelson and Winter [25] argued that "the creation of novelty in art, science, and technology largely depends on the recombination of pre-existing conceptual and physical materials."In scientific research, research questions and methods serve as fundamental building blocks, and their combination determines the scientific novelty of a publication [22].Despite this, existing studies largely focus on LLM-driven idea generation rather than systematically identifying, filtering, and combining problem-method pairs to enhance the effectiveness of scientific discovery.</p>
<p>To address this gap, we introduce the Disruptive Index (DI) to quantify whether a scientific discovery drives a paradigm shift.Disruptive innovation represents fundamental transformations in scientific and technological progress, distinct from incremental improvements that merely refine existing paradigms.Traditional impact metrics, such as citation counts, primarily measure the extent of a technology's adoption rather than its transformative potential.The Disruptive Index (DI), proposed by Funk and Owen-Smith [11], captures whether a scientific discovery supersedes previous approaches rather than merely reinforcing the status quo.A prominent example is Watson and Crick's (1953) discovery of the DNA double-helix structure, which superseded previous approaches, such as Pauling's triple-helix model, and fundamentally altered the field of molecular biology.Their study, with a DI score of 0.62 [28], exemplifies a highly disruptive scientific breakthrough, validated extensively through expert assessments [11,37].Therefore, beyond relying on LLM-generated research ideas, it is essential to construct a framework that integrates DI-based evaluations to systematically assess the transformative impact of problem-method combinations.</p>
<p>Building upon these insights, we propose a problem-method combination framework for scientific discovery, inspired by how scientific breakthroughs emerge from the recombination of existing knowledge.Given a research question, our framework first retrieves and synthesizes relevant papers, then employs an LLM assistant to determine whether specific papers can serve as sources for new scientific discoveries related to the question and extracts a candidate set of methods.Subsequently, we introduce an innovative disruptive index evaluation framework to quantify the disruptiveness of problem-method combinations.Specifically, through model fine-tuning, our assistant generates combination strategies based on research questions and candidate methods.To evaluate the disruptiveness of these strategies, we identify potential source literature in our database, analyze differences between source strategies and current strategies, and propose an adaptive bias-aware alignment model to predict disruptive indices based on these differences.Finally, we iteratively explore candidate method sets to identify the most disruptive problem-method combinations.</p>
<p>We conduct extensive experiments on publication databases across three scientific domains.Our results demonstrate that the proposed framework outperforms state-of-the-art methods in predicting the disruptiveness of problemmethod combinations.Furthermore, validation on real-world high-disruptiveness publications confirms the framework's ability to identify highly disruptive scientific discoveries.</p>
<p>The primary contributions of this research are as follows:</p>
<p>1.A novel framework for scientific discovery that systematically identifies and integrates problem-method combinations rather than relying solely on LLM-generated research ideas.2. A disruptive index evaluation framework that quantitatively assesses the potential disruptiveness of new scientific discoveries, improving upon traditional impact metrics.3. Extensive experimental validation demonstrating the effectiveness of our approach in identifying high-disruptiveness discoveries across multiple scientific domains.</p>
<p>2 Related Work</p>
<p>LLMs in Scientific Discovery and Research Ideation</p>
<p>Large Language Models (LLMs) have demonstrated significant potential in scientific discovery, particularly in generating novel research ideas.Various benchmarks and frameworks have been developed to evaluate the quality of LLMgenerated research hypotheses.One approach involves the establishment of Ide-aBench, a benchmark designed to standardize the assessment of research ideas produced by LLMs [18].Another method introduces the Creativity Index, which incorporates supervised fine-tuning and Direct Preference Optimization (DPO) to evaluate originality, feasibility, impact, and reliability in idea generation [9].Additionally, the MOOSE-Chem multi-agent framework has been implemented, utilizing problem decomposition strategies to improve the quality of LLM-generated research hypotheses in chemistry [39].These efforts reflect a broader trend toward leveraging LLMs for structured scientific ideation.However, despite advancements in benchmark development and evaluation methodologies, existing approaches remain largely reliant on textual synthesis rather than structured reasoning or methodological integration.</p>
<p>The need for systematic frameworks that enhance the logical progression of research ideation and ensure scientific rigor continues to be a critical challenge.</p>
<p>Beyond direct idea generation, LLMs have been incorporated into structured frameworks to enhance the logical progression of research ideation.One such approach is the Chain-of-Ideas (CoI) Agent, an LLM-based system that organizes relevant literature into a structured chain, simulating the progressive development of a research domain and strengthening ideation capabilities [17].To systematically evaluate generated ideas, the Idea Arena protocol has been designed, ensuring that evaluation criteria align with human research preferences.Experimental results indicate that the CoI Agent outperforms conventional methods and produces research ideas of comparable quality to those generated by human researchers.</p>
<p>Additionally, ResearchAgent has been introduced as an iterative framework that refines research ideas through the integration of an academic graph and knowledge retrieval mechanisms.Multiple LLM-powered reviewing agents are employed to provide structured feedback, aligning evaluations with human-defined criteria.This structured review process enhances the clarity, novelty, and validity of generated ideas, demonstrating effectiveness across multiple disciplines [3].</p>
<p>Despite the substantial advancements of Large Language Models (LLMs) in scientific discovery, existing methods still exhibit several critical limitations.First, current research primarily focuses on generating research hypotheses or ideas but fails to systematically explore the finer-grained composition of methodological elements tailored to specific research questions.This limitation results in a lack of effective knowledge recombination mechanisms, rendering LLMs incapable of constructing genuinely innovative research pathways that align with established scientific methodologies.</p>
<p>Second, current LLM-driven research ideation methods heavily rely on providing extensive background information [17], where LLMs are exposed to a large volume of related literature to enhance their inferential capabilities.While this strategy enriches the contextual breadth of generated content, it compromises the traceability of research ideas, making it difficult to directly associate LLM-generated hypotheses with specific prior studies.Consequently, researchers often face challenges in verifying the theoretical foundations and scientific validity of these generated ideas.</p>
<p>Finally, when evaluating LLM-generated research hypotheses, existing approaches predominantly rely on human preference alignment, where assessments are based on subjective ratings or semantic similarity measures.However, there is a notable lack of objective metrics to rigorously quantify the scientific impact of these ideas.In particular, current research lacks systematic methods to evaluate the transformative potential or disruptive impact of newly generated methodologies, thereby making it challenging to distinguish LLM-generated research ideas from truly groundbreaking scientific discoveries.</p>
<p>These limitations underscore the necessity of developing a more systematic and intelligent scientific discovery framework based on problem-method matching, ensuring traceability, scientific rigor, and quantitative evaluation of research ideas.Such a framework would enable a more structured integration of methodological elements while incorporating quantitative analysis to assess the potential impact of newly generated methodologies.</p>
<p>Knowledge Combination and Recombinant Innovation in Scientific Discovery</p>
<p>Innovation fundamentally arises from the combination and recombination of knowledge [38,30].The concept of knowledge recombination has gained increasing attention in the literature, with over 1,000 articles in top management journals leveraging this framework to analyze scientific innovation [38].Recombinant innovation is regarded as a major driver of new idea generation, and its frequent occurrence in scientific research underscores the necessity of understanding how scientific knowledge is integrated and combined in academic publications [7].</p>
<p>Within the domain of scientific discovery, research questions and research methods serve as the fundamental building blocks that determine the novelty and impact of scientific contributions [22].Existing studies have examined scientific novelty through various combination-based perspectives, but have yet to fully address the temporal evolution and semantic complexity of research questions and methods.To bridge this gap, recent work has proposed a life-index novelty measurement, incorporating the frequency and age of research questions and methods, alongside semantic novelty assessment using deep learning and representation learning techniques [22].These advancements highlight the importance of systematically integrating research questions and methods to characterize scientific novelty.Despite these insights, current methodologies predominantly focus on evaluating novelty at the level of individual concepts, rather than systematically modeling how methodological elements are combined to address specific research questions.While research questions and methods both constitute integral knowledge elements in scientific articles, existing studies rarely explore how their structured integration contributes to groundbreaking discoveries.This limitation suggests the need for a systematic framework that formalizes the combination of research questions and methods to assess their transformative potential.</p>
<p>Moreover, the lack of structured mechanisms for problem-method matching has hindered the ability to predict which methodological innovations lead to significant scientific breakthroughs.Although LLM-based approaches have been employed to generate research ideas, they primarily rely on retrieving or synthesizing prior knowledge, rather than systematically aligning methods with research questions to facilitate novel knowledge recombination.This gap underscores the necessity of developing an intelligent framework that systematically integrates research methods as core knowledge elements and models their structured composition for scientific discovery.</p>
<p>The Necessity of the Disruption Index (DI) in Evaluating Scientific Breakthroughs</p>
<p>Traditional metrics for assessing the impact of scientific research, such as citation counts, h-index, and journal impact factor, have long been used as standard measures of scientific influence.However, these indicators primarily capture the magnitude of a study's dissemination rather than its ability to challenge existing paradigms [13? , 40].While citation counts are intuitive and widely adopted, they suffer from inherent limitations, including bias toward incremental research, ignoring negative citations, and reinforcing conservative citation behavior [6,26,10].Consequently, traditional bibliometric indicators often fail to distinguish between studies that reinforce the status quo and those that disrupt established knowledge structures.</p>
<p>To address these limitations, Funk and Owen-Smith (2017)introduced the Disruption Index (DI) as a metric to quantify the extent to which new technological advancements displace or reinforce existing knowledge [11].Inspired by prior literature on technological shifts, they argued that the dichotomy between competence-enhancing and competence-destroying innovations was insufficient for characterizing real-world technological evolution.Instead, they proposed that disruptiveness exists on a continuum, where some innovations incrementally improve existing knowledge, while others render previous technologies obsolete [1].</p>
<p>Originally developed to measure technological innovation using vast patent databases such as the U.S. Patent Citations Data File, the Disruption Index was later extended to scientific research by Wu et al. (2019), who applied the metric to bibliometrics [37].They demonstrated that DI could effectively differentiate groundbreaking discoveries from incremental advancements by analyzing its values in Nobel Prize-winning papers and comparing disruption levels between review papers and their original research articles.</p>
<p>The DI quantifies disruptiveness using the following formulation:
D = n i − n j n i + n j + n k ,(1)
where n i is the number of papers that cite the focal paper exclusively, n j represents papers citing both the focal paper and its references, and n k denotes papers that cite only the references of the focal paper [37].This formulation allows DI to capture the extent to which a research contribution redefines its field, rather than merely accumulating citations.</p>
<p>The necessity of DI stems from its ability to quantitatively evaluate scientific breakthroughs, offering a more precise alternative to traditional citationbased measures.As disruptive innovation is characterized by a paradigm shift that redirects collective attentio*, DI provides a robust framework for distinguishing transformative research from incremental progress [20].Given that existing LLM-based research ideation models primarily focus on generating novel ideas without evaluating their potential to challenge existing scientific conventions, integrating DI into scientific discovery frameworks can significantly enhance the assessment of research novelty and impact.These considerations highlight the limitations of existing citation-based indicators in capturing scientific breakthroughs and underscore the importance of incorporating DI into intelligent scientific discovery frameworks.A disruptionaware approach could enable more systematic evaluations of research impact, ensuring that novel problem-method combinations are assessed not just for their feasibility but also for their potential to drive substantial scientific advancements.</p>
<p>Our research proposes a novel scientific discovery paradigm that not only provides methodological support for evaluating scientific innovation but also establishes a new technological paradigm for intelligent research tools.Through question-methodology combinatorial logic, we have constructed a framework that achieves objective quantitative assessment of the disruptive potential of research ideas and generates corresponding reasoning chains.This study introduces an improved framework with three core modules to enhance disruptive knowledge prediction and method combination exploration.</p>
<p>Methodology</p>
<p>The Problem-Driven Method Exploration Module identifies potential method combinations based on specific research questions, providing innovative and targeted strategies.</p>
<p>The Disruptive Knowledge Prediction Module predicts the disruptive potential of given problems and methods using a deviation-awareness mechanism and a secondary learning approach to ensure accuracy and reliability.</p>
<p>The Dynamic Method Optimization Module iteratively refines method combinations based on disruptive index feedback, enhancing their disruptive potential.</p>
<p>This framework offers a systematic approach for researchers to uncover disruptive knowledge and drive scientific innovation.</p>
<p>Problem-Driven Method Exploration Module</p>
<p>To enhance the efficiency of method exploration and reduce resource consumption, this study designs a Problem-Driven Method Exploration Module that constructs a paper database indexed by problems and methods.Traditional approaches often require repeated evaluation of the relationship between new problems and paper abstracts, which not only increases computational costs but also reduces retrieval efficiency.To address this, we propose an efficient retrieval mechanism that rapidly identifies potential method candidates relevant to specific research problems.</p>
<p>The paper database is built upon a large-scale collection of academic literature.First, we preprocess textual information such as paper titles, abstracts, and keywords to remove redundant and noisy data, ensuring data accuracy and consistency.Subsequently, natural language processing (NLP) techniques are employed to extract the research problem and research method from each paper.These two elements are then indexed as key entries to facilitate efficient retrieval through a problem-driven approach.</p>
<p>During the method exploration process, when a new research problem P new is proposed, the system embeds it into a semantic vector space:
v Pnew = Embed(P new )
Then, a similarity function is applied to retrieve the top-k similar problems:
P sim = {P i | sim(v Pnew , v Pi ) ≥ δ}
The associated methods M sim = {M i | P i ∈ P sim } are collected, and a heuristic filtering mechanism H is applied:
M final = H(M sim , sim(•), rule(•))
Finally, the system constructs candidate problem-method pairs:
C = {(P new , M ) | M ∈ M final }
These selected candidates support downstream disruptive knowledge prediction and method optimization.</p>
<p>Disruptive Index Prediction Model Methodology</p>
<p>This study introduces a disruptive index prediction model consisting of interconnected sub-modules designed to precisely evaluate the innovative potential of specific problem-method combinations.Specifically, the overall module comprises three sub-modules: problem-method summary generation, identification, extraction, and refinement of key reference information, and final prediction of the disruptive index.</p>
<p>The first sub-module aims to automatically generate highly concise summaries for given problem-method pairs.To enhance the accuracy and logical coherence of generated summaries, we employ a summary generation model fine-tuned using Low-Rank Adaptation (LoRA) [15].This model effectively learns from existing real-world literature summaries within a targeted downstream task context.Additionally, a set of meticulously crafted prompts guide the model stepby-step in describing the logical relationships and details involved in combining problems and methods (detailed prompt design is provided in the Appendix).</p>
<p>Formally, given a problem-method pair (P, M ) and a task-specific prompt Prompt, the summary generation model G θ produces a concise summary S:
S = G θ (P, M, Prompt)
Through this approach, our trained model significantly outperforms current state-of-the-art methods, demonstrating superior logical consistency and completeness of information.</p>
<p>The second sub-module integrates both the identification of key reference documents and the extraction of essential information.Given the computationally intensive nature of precise matching across large literature databases, we initially filter candidate references based on the semantic similarity between the given problem-method combination and the problem-method indices from a structured literature database.</p>
<p>Let D be the structured database and (P, M ) the input pair.We first compute the similarity and select the top-k references:
R top = {R i ∈ D | sim((P, M ), (P i , M i )) ≥ δ}
This approach ensures that only references with highly relevant problemmethod associations are considered, constraining the selection to 100 references.Subsequently, we employ a frozen pre-trained model to perform a fine-grained semantic comparison between the generated summaries and candidate reference summaries, thereby accurately identifying the key source references relevant to the problem-method combination.</p>
<p>Let F denote the frozen model and S the generated summary.We obtain the extracted information I:
I = F (S, R top )
Once the key references are identified, we further refine the extracted information to reduce computational burden and minimize noise introduced by large-scale raw textual inputs.Inspired by the RAHA approach [19], we leverage a frozen pre-trained model to compare the generated summary with the identified reference summaries and extract only the most critical, relevant information required for the model input.This integrated method effectively captures complex citation relationships while ensuring the quality of the extracted input information, ultimately improving the efficiency and accuracy of subsequent predictions.</p>
<p>The third sub-module utilizes the generated problem-method summaries and extracted critical information to predict the disruptive index for problem-method combinations.This sub-module employs a prediction model fine-tuned via LoRA.During fine-tuning, supervision is provided using real summaries and extracted reference information.Formally, the prediction model D ϕ outputs a disruption score y based on S and I: y = D ϕ (S, I)</p>
<p>Due to the scarcity of highly disruptive papers in real-world datasets, potentially biasing predictions towards low-disruptive outcomes, we introduce an entropy-based weighted evaluation metric [4].</p>
<p>The weighted loss for entropy-aware learning is given by:
L entropy = N i=1 w i • ℓ(y i , ŷi ), w i = − log(p(ŷ i ))
This metric assigns higher weights to rare but highly disruptive samples, enhancing the model's capability to identify high-disruptiveness cases.</p>
<p>Furthermore, we design a secondary learning mechanism for challenging-toclassify samples, selecting the top 20% of instances with the highest prediction errors for reinforcement training.To prevent prediction drift caused by secondary training, we introduce a KL-divergence-based balancing mechanism to stabilize training outcomes.The KL divergence loss between primary and secondary distributions is defined as:
L KL = D KL (D primary ϕ ∥ D secondary ϕ )
To further stimulate the cognitive and evaluative capabilities of the model, we propose an iterative deviation-awareness mechanism.Specifically, after each prediction iteration, the results are fed back to the model, prompting self-awareness and evaluation of prediction deviations.Let θ t be the model parameters at iteration t, then parameter update is guided by the deviation gradient:
θ t+1 = θ t − η • ∇Deviation(ŷ t , y t )
Experimental results demonstrate that, compared to existing state-of-the-art methods, our proposed framework significantly and comprehensively improves performance in predicting the disruptive index of problem-method combinations.</p>
<p>Dynamic Method Optimization Module</p>
<p>The dynamic method optimization module is designed to iteratively refine method combinations based on feedback from the disruptive index, enhancing their disruptive potential over multiple optimization cycles.This process ensures that method selection and adjustments remain continuously optimized within the problem-method space to maximize impact.</p>
<p>At the core of this module lies a feedback-driven iterative optimization mechanism.Specifically, the system evaluates the disruptive index of a given problemmethod combination at each iteration and utilizes this information to guide subsequent modifications.</p>
<p>Formally, let (P, M t ) represent the problem and current method configuration at iteration t, and let y t be the corresponding disruptive index:
y t = D ϕ (S t , I t )
The optimization process follows a greedy algorithm, selecting adjustments in each iteration that locally maximize the disruptive index:
M t+1 = arg max M ′ ∈N (Mt) D ϕ (S ′ , I ′ )
where N (M t ) denotes the neighborhood of candidate method variants generated by replacing, augmenting, or modifying components of M t .By progressively favoring configurations that yield higher indices, the framework systematically converges towards method combinations with enhanced disruptive impact.</p>
<p>However, traditional greedy algorithms are prone to getting trapped in local optima, leading to stagnation in suboptimal solutions.To address this limitation, we incorporate a Greedy with Probabilistic Perturbation (GPP) approach [14], enhancing the global search capability.</p>
<p>In GPP, with a small probability ϵ, a non-optimal method M rand ∈ N (M t ) is accepted:
M t+1 = arg max M ′ D ϕ (S ′ , I ′ ), with probability 1 − ϵ M rand , with probability ϵ
This probabilistic mechanism allows the optimization process to escape local optima and explore method configurations with long-term disruptive potential.By leveraging this stochastic perturbation strategy, the optimization process effectively balances local search with global exploration.</p>
<p>Furthermore, to prevent stagnation in local optima and ensure comprehensive exploration of the method space, the optimization process integrates adaptive constraints.</p>
<p>Let C(M t ) denote constraint-based penalty terms for overfitting, we define the total objective with regularization as:
L opt = −D ϕ (S t , I t ) + λ • C(M t )
This ensures that method updates remain meaningful and generalizable across problem contexts.</p>
<p>The optimization module also includes an adaptive learning component that dynamically adjusts the weight of disruptive index feedback based on observed trends over multiple iterations.Let w t denote the weight at time t, updated based on temporal smoothing:
w t+1 = α • w t + (1 − α) • y t
This mechanism enables the system to prioritize consistently improving adjustments while attenuating noise introduced by short-term evaluation anomalies.</p>
<p>Empirical evaluations indicate that the incorporation of the GPP mechanism effectively enhances the global search capability, enabling the algorithm to escape local optima while maintaining efficient optimization performance.Overall, this dynamic optimization strategy significantly improves the identification and refinement of disruptive method combinations.By integrating iterative feedback, local optimization, adaptive constraints, and stochastic perturbation, this module provides a more efficient and systematic solution for method selection, ultimately fostering the discovery of high-impact scientific innovations.</p>
<p>Experiments</p>
<p>Data Sources</p>
<p>To evaluate the effectiveness of our proposed framework, we conduct experiments on three citation-based datasets: DBLP, PubMed, and PatSnap.Given the broad scope of these datasets, we focus on specific domains to ensure targeted analysis.</p>
<p>For DBLP, we extract records from 2011 to 2021 covering 14,533 publications from CCF-A conferences in the field of artificial intelligence.From PubMed, we select 96,612 research articles related to depression, published between 2015 and 2025.Lastly, for PatSnap, we use 6,677 patent records on medical robotics, with legal status marked as active, covering the period from 2020 to 2025.Further details on dataset characteristics and preprocessing are provided in Appendix A.</p>
<p>Baselines</p>
<p>To comprehensively assess the performance of our framework, we compare it against a set of established baselines, including both general-purpose large language models and specialized pre-trained models for scientific and technical domains.</p>
<p>We consider the following baselines:</p>
<p>(1) General-purpose LLMs: GPT and Claude, widely used for natural language understanding and text generation tasks.</p>
<p>(2) SciBERT [29], a pre-trained language model designed specifically for scientific text processing, which has demonstrated strong performance in scientific literature comprehension and reasoning tasks.</p>
<p>(3) RoBERTa [21], an optimized variant of BERT that enhances training robustness and performance across multiple NLP tasks.</p>
<p>(4) LLaMA 3 [12], the latest iteration in the LLaMA series of large-scale language models, which offers improved efficiency and reasoning capabilities.</p>
<p>(5) Qwen-7B [5], an autoregressive generative language model based on masked language modeling, optimized for diverse text generation and completion tasks.</p>
<p>All of these models are publicly accessible, allowing for reproducible benchmarking and comparative evaluation of our proposed framework.</p>
<p>Experimental Setup</p>
<p>Our experiments are conducted using PyTorch on four NVIDIA A800 GPUs.The ablation study is performed based on Qwen-7B.The model optimization is implemented using the Adam optimizer, with a learning rate set to 1e-5 and a gradient clipping threshold fixed at 0.2.</p>
<p>For model configurations, the problem-method summary generation model is set to handle a maximum input length of 1000 tokens, while the disruptive index prediction model is configured with a maximum input length of 7000 tokens.The batch size is consistently set to 4 across all experiments.The adapter used in the second LLM is configured with a low-rank dimension of 64.</p>
<p>We employ the PEFT (Parameter-Efficient Fine-Tuning) library to insert adapters into the last attention or feedforward layers of the LLM [23].This analysis is performed based on a principled examination of the forward components.</p>
<p>Both training and testing iterations are set to K = 5.For other baseline models, the number of training epochs is fixed at 5, and the optimal model checkpoint is selected based on validation set performance metrics.</p>
<p>Main Results</p>
<p>We present the main results on the DBLP, PubMed, and PatSnap datasets in Table 1.Our framework, which integrates two models along with the overall system, consistently outperforms existing state-of-the-art LLMs and PLMs across multiple evaluation metrics.Table 1 reports the cosine similarity and ROUGE scores between problem-method summaries generated by our framework and their corresponding ground-truth summaries.We use Qwen-32B as an example for evaluating the summarization performance.The fine-tuned model not only surpasses existing general-purpose LLMs such as GPT and Claude but also demonstrates superior performance over non-fine-tuned pre-trained models.</p>
<p>This confirms the effectiveness of our approach in refining the alignment between problem-method pairs and their corresponding textual representations.As shown in Table 2, we evaluate the effectiveness of our disruptive index prediction model based on four key metrics: MSE, MAE, weighted MSE (WMSE), and weighted MAE (WMAE).Our model, incorporating adaptive bias awareness and secondary sample learning, achieves lower error rates across all metrics, demonstrating its superiority over general-purpose LLMs, pre-trained language models (PLMs), and large language models (LLMs) fine-tuned on scientific tasks.The improvements in these evaluation metrics indicate that our framework effectively captures the disruptive potential of problem-method combinations with higher accuracy and robustness.As shown in Table 3, our full framework, designed for disruptive index prediction, outperforms general LLMs across all evaluation metrics.The consistent improvements in MSE, MAE, WMSE, and WMAE confirm the effectiveness of integrating problem-method pairs and disruptive index prediction to enhance scientific discovery.</p>
<p>Ablation Study</p>
<p>To analyze the contributions of individual components within our framework, we conduct an ablation study, as shown in Table 4. (1) Framework w/o fine-tuning the problem-method summarization model: Removing fine-tuning from the problem-method summarization model results in a noticeable performance degradation across all datasets.This demonstrates the importance of task-specific adaptation in improving the alignment between problem-method pairs and their textual representations.Without fine-tuning, the generated summaries exhibit lower quality, impacting the overall framework's ability to capture meaningful research insights.</p>
<p>(2) Framework w/o relevance assessment and information extraction: Excluding the step of relevance judgment and structured information extraction significantly reduces the effectiveness of disruptive index prediction.The removal of this module leads to an increase in MSE and MAE, as the model is unable to accurately capture contextual knowledge essential for evaluating problem-method disruptiveness.This highlights the necessity of refining input information before disruptive potential estimation.</p>
<p>(3) Framework w/o secondary learning: The absence of secondary learning-where high-error samples undergo further training-results in higher prediction errors across all four key evaluation metrics: MSE, MAE, WMSE, and WMAE.This confirms that the secondary learning mechanism enhances model robustness by mitigating the impact of hard-to-classify instances, ensuring better generalization.</p>
<p>(4) Framework w/o deviation-aware alignment: Removing deviationaware alignment leads to a decline in performance by increasing error rates and reducing model consistency.Without this module, the framework struggles to adjust predictions based on previously observed discrepancies, limiting its ability to refine predictions dynamically.</p>
<p>Overall, the ablation study confirms that each component plays a crucial role in improving the framework's predictive performance.The degradation in results when removing any of these modules underscores their importance in systematically enhancing problem-method integration and disruptive potential assessment.</p>
<p>Greedy Algorithm Optimization Results</p>
<p>To evaluate the effectiveness of our dynamic method optimization module, we conduct experiments measuring its ability to identify high-disruptiveness method combinations.Table 5 reports the hit rate (Disruptive Index &gt; 0.5) on three datasets.</p>
<p>Our framework leverages a Greedy with Probabilistic Perturbation (GPP) approach to iteratively optimize problem-method combinations based on the disruptive index.Compared to a standard greedy algorithm, GPP significantly improves the ability to escape local optima, resulting in superior method selection.Specifically, the model incorporating GPP achieves a higher disruptive index score across all datasets, demonstrating improved long-term optimization capabilities.The observed performance gains are attributed to two key advantages of GPP: (1) Balancing exploration and exploitation, where the probabilistic acceptance of non-optimal choices prevents premature convergence, and (2) Adaptive weighting of disruptive index feedback, which enables a more refined and responsive optimization trajectory.These mechanisms collectively enhance the ability of our framework to discover novel, high-impact problem-method combinations.</p>
<p>Conclusion</p>
<p>In this study, we propose a novel framework for scientific discovery that systematically integrates problem-method combinations with disruptive index prediction.Our approach leverages fine-tuned LLMs for problem-method summarization, an adaptive bias-aware alignment model for disruptive index estimation, and a dynamic optimization strategy incorporating Greedy with Probabilistic Perturbation (GPP) to iteratively refine method selection.</p>
<p>Empirical results on DBLP, PubMed, and PatSnap confirm the effectiveness of our framework.Compared to existing general-purpose LLMs, pre-trained language models, and baseline methods, our approach consistently achieves higher accuracy in problem-method summarization, disruptive index prediction, and high-impact method identification.The introduction of GPP significantly enhances search efficiency by balancing exploitation and exploration, ensuring the discovery of truly novel and disruptive scientific insights.</p>
<p>Our findings contribute to the advancement of AI-driven scientific discovery by demonstrating the value of structured problem-method integration and adaptive learning strategies.Future research may explore expanding the framework to broader domains and improving interpretability to further assist researchers in generating groundbreaking discoveries.</p>
<p>Limitations</p>
<p>While our proposed framework demonstrates strong performance in integrating problem-method combinations with disruptive index prediction, it has two primary limitations.</p>
<p>First, for entirely emerging scientific fields with minimal prior work, our framework may encounter challenges due to a lack of sufficient historical data.The effectiveness of the problem-method integration and disruptive index prediction relies on existing structured research literature.In domains with scarce prior knowledge, the search space for potential method combinations becomes significantly larger, reducing search efficiency and increasing the likelihood of suboptimal results.</p>
<p>Second, our framework involves a multi-step process that includes problemmethod summarization, source validation, information extraction, secondary learning, and deviation-aware alignment.While each step enhances accuracy, it also increases computational complexity and execution time.The sequential nature of these processes results in higher processing overhead, which may limit the scalability of our approach when applied to large-scale real-time applications.</p>
<p>Future research should explore ways to mitigate these limitations, including optimizing search strategies for data-scarce fields and improving computational efficiency through parallelization and adaptive learning techniques.</p>
<p>Fig. 1 .
1
Fig. 1.Enter Caption</p>
<p>Table 1 .
1
Comparison of Question-Method Pair Summarization Performance Across Different Datasets Using Cosine Similarity and ROUGE
ModelDBLPPubMedPatentSimilarity ROUGE Similarity ROUGE Similarity ROUGEGPT-4o0.5030.2060.4320.2250.4470.133GPT-4 Turbo0.5200.2050.3450.1870.4360.133Claude 3.50.4690.2040.3810.1540.3430.095Claude 3.70.4600.2140.4910.1390.2990.085Qwen-32B (Pre-trained) 0.5520.3150.4230.2420.4560.158Qwen-32B (Fine-tuned)0.5580.3250.5000.3240.6120.404</p>
<p>Table 2 .
2
Disruptive Index Prediction Using Summarization and Information Across Different Datasets Using MSE, MAE, WMSE, and WMAE
ModelDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEGPT-4o0.3607 0.5728 0.5821 0.7175 0.3255 0.5369 0.2321 0.4026 0.3018 0.5162 0.0884 0.1770GPT-4 Turbo0.3607 0.5728 0.5821 0.7175 0.3549 0.5621 0.1742 0.2919 0.3455 0.5594 0.6520 0.7707Claude 3.50.4346 0.6115 0.3162 0.4318 0.4269 0.6077 0.3029 0.4219 0.4242 0.5887 0.9875 0.9295Claude 3.70.4699 0.6580 0.2653 0.4131 0.4751 0.6593 0.2728 0.3773 0.4242 0.5887 0.9875 0.9295SciBERT0.3125 0.4153 0.3641 0.3817 0.4218 0.1642 0.3486 0.4357 0.4871 0.5092 0.4217 0.4561RoBERTa0.4351 0.5715 0.3105 0.3751 0.4017 0.4521 0.3465 0.4213 0.4154 0.5184 0.4156 0.4364LLaMA 30.5612 0.6143 0.3155 0.4182 0.4832 0.5961 0.5942 0.4118 0.5624 0.7334 0.3284 0.3912Qwen-7B0.6845 0.8223 0.4558 0.4526 0.4839 0.6587 0.1587 0.2908 0.6843 0.8151 0.5172 0.5241SciBERT(Fine-tuned) 0.0142 0.0247 0.5472 0.6781 0.0093 0.0145 0.3148 0.4207 0.0135 0.0241 0.4151 0.5124RoBERTa(Fine-tuned) 0.0091 0.0154 0.6245 0.6578 0.0075 0.0921 0.2947 0.4814 0.0183 0.0214 0.4521 0.4873LLaMA 3(Fine-tuned) 0.0124 0.0325 0.5175 0.5412 0.0091 0.0124 0.3541 0.4168 0.0265 0.3457 0.5142 0.5321Qwen-7B (Fine-tuned) 0.0052 0.0121 0.6172 0.6739 0.0020 0.0072 0.2533 0.4604 0.0144 0.0181 0.4325 0.4512</p>
<p>Table 3 .
3
Utilizing Question-Method Pairs to Predict Disruptive Index Results with MSE, MAE, WMSE, and WMAE Metrics
ModelDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEGPT-4o0.1191 0.3062 0.5887 0.7399 0.2053 0.4182 0.6545 0.7036 0.1216 0.3044 1.0043 0.9041GPT-4 Turbo 0.1597 0.3628 0.4771 0.6453 0.3289 0.5196 0.3690 0.5558 0.1523 0.3556 1.0803 0.9312Claude 3.50.2291 0.4268 0.9919 0.8767 0.1731 0.3742 1.0523 0.9006 0.2176 0.4201 1.2142 0.9428Claude 3.70.4319 0.6143 0.1355 0.3382 0.1146 0.3021 0.6827 0.7224 0.4594 0.6559 0.1437 0.3431Our Framework 0.0093 0.0111 0.0941 0.1364 0.0154 0.0342 0.1147 0.2142 0.0218 0.0412 0.1241 0.2962</p>
<p>Table 4 .
4
Ablation study of our framework using Qwen-7B across DBLP, PubMed, and Patent datasets with MSE, MAE, WMSE, and WMAE metrics.
Model VariantDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEFull Framework (Ours)0.0093 0.0111 0.0941 0.1364 0.0154 0.0342 0.1147 0.2142 0.0218 0.0412 0.1241 0.2962w/o Summarization Fine-tuning 0.0274 0.0461 0.1403 0.2093 0.0412 0.0623 0.1889 0.2563 0.0586 0.0721 0.2034 0.3121w/o Relevance + Extraction0.0351 0.0592 0.1881 0.2672 0.0526 0.0783 0.2102 0.2907 0.0735 0.0897 0.2345 0.3374w/o Secondary Learning0.0187 0.0329 0.1187 0.1766 0.0291 0.0503 0.1506 0.2384 0.0375 0.0594 0.1741 0.3022w/o Deviation-Aware Alignment 0.0216 0.0382 0.1279 0.1917 0.0318 0.0562 0.1663 0.2496 0.0414 0.0638 0.1862 0.3195</p>
<p>Table 5 .
5
Hit Rate (%) of High-Disruptiveness Method Combinations (Disruptive Index &gt; 0.5) Identified by Various Methods
MethodDBLP PubMed PatSnapGPT-4o (ChatGPT)16.4% 18.2% 14.9%Claude 3.515.7% 17.1% 14.3%Claude 3.717.2% 19.0% 15.6%Standard Greedy19.3% 21.5% 18.2%Greedy + GPP (Ours)26.3% 28.1% 24.6%Improvement over best LLM +9.1% +9.1% +9.0%
J. Chen et al.</p>
<p>Extra credit for disruption: Trend of disruption in radiology academic journals. A Abu-Omar, P Kennedy, M Yakub, J Robbins, A Yassin, N Verma, M Scaglione, F Khosa, Clinical Radiology. 77122022</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>A characterization of entropy in terms of information loss. J C Baez, T Fritz, T Leinster, Entropy. 13112011</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, B Hui, L Ji, M Li, J Lin, R Lin, D Liu, G Liu, C Lu, K Lu, J Ma, R Men, X Ren, X Ren, C Tan, S Tan, J Tu, P Wang, S Wang, W Wang, S Wu, B Xu, J Xu, A Yang, H Yang, J Yang, S Yang, Y Yao, B Yu, H Yuan, Z Yuan, J Zhang, X Zhang, Y Zhang, Z Zhang, C Zhou, J Zhou, X Zhou, T Zhu, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>The incidence and role of negative citations in science. C Catalini, N Lacetera, A Oettl, Proceedings of the National Academy of Sciences. 112452015</p>
<p>Scientific knowledge combination in networks: New perspectives on analyzing knowledge absorption and integration. H Chen, J Liu, Z Liu, EEKE/AII@ JCDL. 2023</p>
<p>Empowering ai as autonomous researchers: Evaluating llms in generating novel research ideas through automated metrics. D Dasgupta, A Mondal, P P Chakrabarti, 2nd AI4Research Workshop: Towards a Knowledgegrounded Scientific Research Lifecycle. </p>
<p>Empowering AI as autonomous researchers: Evaluating LLMs in generating novel research ideas through automated metrics. D Dasgupta, A Mondal, P P Chakrabarti, 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle. 2024</p>
<p>Tradition and innovation in scientists' research strategies. J G Foster, A Rzhetsky, J A Evans, American sociological review. 8052015</p>
<p>A dynamic network measure of technological change. R J Funk, J Owen-Smith, Management science. 6332017</p>
<p>A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>The impact of collaboration and knowledge networks on citations. J Guan, Y Yan, J J Zhang, Journal of Informetrics. 1122017</p>
<p>A probabilistic greedy search algorithm for combinatorial optimisation with application to the set covering problem. M Haouari, J Chaouachi, Journal of the Operational Research Society. 5372002</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, ICLR. 1232022</p>
<p>The burden of knowledge and the "death of the renaissance man": Is innovation getting harder?. B F Jones, The Review of Economic Studies. 7612009</p>
<p>L Li, W Xu, J Guo, R Zhao, X Li, Y Yuan, B Zhang, Y Jiang, Y Xin, R Dang, arXiv:2410.13185Chain of ideas: Revolutionizing research via novel idea development with llm agents. 2024arXiv preprint</p>
<p>C Liang, L Huang, J Fang, H Dou, W Wang, Z F Wu, Y Shi, J Zhang, X Zhao, Y Liu, arXiv:2412.11767Idea-bench: How far are generative models from professional designing?. 2024arXiv preprint</p>
<p>C Lin, J Ren, G He, Z Jiang, H Yu, X Zhu, arXiv:2402.08874Recurrent alignment with hard attention for hierarchical text rating. 2024arXiv preprint</p>
<p>New directions in science emerge from disconnection and discord. Y Lin, J A Evans, L Wu, Journal of Informetrics. 1611012342022</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Combination of research questions and methods: A new measurement of scientific novelty. Z Luo, W Lu, J He, Y Wang, Journal of Informetrics. 1621012822022</p>
<p>Peft: State-of-the-art parameter-efficient fine-tuning methods. S Mangrulkar, S Gugger, L Debut, Y Belkada, S Paul, B Bossan, Peft: State-of-the-art parameter-efficient fine-tuning methods. 2022</p>
<p>A new method for identifying recombinations of existing knowledge associated with high-impact innovation. S Mukherjee, B Uzzi, B Jones, M Stringer, Journal of Product Innovation Management. 3322016</p>
<p>An evolutionary theory of economic change. R R Nelson, S G Winter, 1985harvard university press</p>
<p>Global citation inequality is on the rise. M W Nielsen, J P Andersen, Proceedings of the National Academy of Sciences. 1187e20122081182021</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 352022</p>
<p>Papers and patents are becoming less disruptive over time. M Park, E Leahey, R J Funk, Nature. 61379422023</p>
<p>N Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Search and recombination process to innovate: a review of the empirical evidence and a research agenda. T Savino, A Messeni Petruzzelli, V Albino, International Journal of Management Reviews. 1912017</p>
<p>Business cycles: A theoretical, historical and statistical analysis of the capitalist process. J A Schumpeter, Acessado em. 41939. 1964</p>
<p>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references. I Tahamtan, L Bornmann, Journal of informetrics. 1232018</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Atypical combinations and scientific impact. B Uzzi, S Mukherjee, M Stringer, B Jones, Science. 34261572013</p>
<p>Collaboration and creativity: The small world problem. B Uzzi, J Spiro, American journal of sociology. 11122005</p>
<p>Bias against novelty in science: A cautionary tale for users of bibliometric indicators. J Wang, R Veugelers, P Stephan, Research Policy. 4682017</p>
<p>Large teams develop and small teams disrupt science and technology. L Wu, D Wang, J A Evans, Nature. 56677442019</p>
<p>A knowledge recombination perspective of innovation: review and new research directions. T Xiao, M Makhija, S Karim, Journal of Management. 4862022</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Z Yang, W Liu, B Gao, T Xie, Y Li, W Ouyang, S Poria, E Cambria, D Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>Team size, research variety, and research performance: do coauthors' coauthors matter. N Zhu, C Liu, Z Yang, Journal of Informetrics. 1541012052021</p>            </div>
        </div>

    </div>
</body>
</html>
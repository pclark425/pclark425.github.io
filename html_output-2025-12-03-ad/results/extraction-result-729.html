<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-729 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-729</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-729</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-815c6ca281536d18ec0eb408b6e46e72a0826163</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/815c6ca281536d18ec0eb408b6e46e72a0826163" target="_blank">Natural Language to Code Generation in Interactive Data Science Notebooks</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs and few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation are explored.</p>
                <p><strong>Paper Abstract:</strong> Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at https://github.com/google-research/arcade-nl2code/.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e729.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e729.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>underspecified_intents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Underspecified Natural Language Intents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>User natural language intents provided to the NL-to-code system are often concise and lack sufficient specification, producing multiple plausible programmatic interpretations and output formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ARCADE evaluation of NL-to-code in notebooks</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmark and experimental setup that maps succinct user NL intents (in notebook cells) to pandas-based Python code solutions, evaluated via execution-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>user-issued natural language intent (notebook cell comment)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Jupyter notebook code cell (pandas Python)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous/under-specified specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Approximately half of collected user intents lack details required to uniquely determine the desired output (e.g., target columns, whether results should be a DataFrame vs list/Series, index/headers, or post-processing steps). This ambiguity leads to multiple semantically plausible implementations that differ in output structure.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>problem specification / input intent</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual analysis of a random sample of 100 annotated intents by the authors/annotators and empirical model evaluation showing divergent outputs; annotation process flagged underspecification and authors measured prevalence.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Dataset annotation statistics (percent of intents falling into ambiguity categories) and execution-based pass@k metrics with/without extra intent specifications; fuzzy output matching used to mitigate strictness.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large effect on functional correctness: on a subset of 136 New-Tasks intents that had extra specifications, removing those specifications dropped PACHINCO's pass@30 from 46.5% (with specs) to 27.8% (without specs), a ~18.7 percentage-point absolute decrease, showing ambiguity materially reduces success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>About 50% of intents judged underspecified; within these: 10% lacked explicit target columns, 42% implied entity-sets (ambiguity between Series/List vs DataFrame), 23% implied complex schema (nested indices/headers), 20% implied additional post-processing/multiple variables, 5% were highly complex/unclear.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Concise, ephemeral style of real-world developer prompts that omit structural/output details; natural ambiguity in free-form NL; intentional design choice to collect realistic short intents.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Ask annotators to add clarifying output specifications when needed; introduce evaluation tolerant to plausible variants (fuzzy output matching); increase prediction diversity (few-shot SbS prompting) and use reranking (self-consistency) to surface plausible interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantified: adding explicit specifications (during annotation) improved pass@30 from 27.8% to 46.5% on the tested subset (+18.7 pp). Fuzzy output matching reduces false negatives (authors report low false-negative rate) and diverse sampling plus reranking improved single-sample accuracy beyond baseline pass@5, but per-sample correctness fraction remained ~15% (so diversity mainly improved pass@k).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis for data science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language to Code Generation in Interactive Data Science Notebooks', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e729.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e729.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>schema_grounding_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing Schema / Variable State Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models fail when NL refers to dataset columns or runtime variables that are not provided/encoded in the prompt; explicit schema descriptions and variable states are needed for correct grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ARCADE prompting and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompting LLMs for code generation using notebook context that can include explicit NL schema descriptions (column names and sample values) and previous-code-derived state.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>NL description of imported DataFrame schema and variable states in notebook context</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Jupyter notebook code cell (pandas Python)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing variable/schema grounding (incomplete context)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When schema descriptions or execution-state information for intermediate variables are omitted from the prompt, the model frequently references undefined columns/variables or selects incorrect columns, producing NameError/KeyError at execution or incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>input context / grounding of NL to program variables (schema understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Ablation experiments: remove schema descriptions from prompts and measure changes in pass@k; controlled experiments varying number of context cells and tracking runtime error types (NameError, KeyError).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>pass@k metrics (Table 2), run-time error counts and categories aggregated over predictions (Figures 8 and 9), and absolute pass@30 differences when -Schema Desc. ablated.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: removing schema descriptions reduced PACHINCO's pass@30 on New Tasks from 47.7% to 36.1% (an 11.6 percentage-point drop); limiting context to 1 cell produced pass@30 of 17% on New Tasks versus 36% at 3 context cells, and increased NameError/KeyError frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common: many problems require grounding to DataFrame schema; experiments indicate this is a systematic failure mode when schema info is absent (measurable drops across many examples).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Prompts lacking explicit structured schema or intermediate variable states; LLMs need grounding signals to map NL phrases to programmatic identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Include NL schema descriptions in prompts, include more preceding context cells (previous turns), and consider encoding intermediate variable states; possibly augment LM training with notebook data containing such context.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: including schema descriptions and more context cells raised pass@30 by ~11.6 pp (schema) and roughly doubled pass@30 from d=1 to d=3 contexts on New Tasks (17% -> 36%).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis for data science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language to Code Generation in Interactive Data Science Notebooks', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e729.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e729.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>train_eval_leakage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training / Evaluation Data Leakage and Memorization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Existing evaluation notebooks may appear in LLM training corpora, inflating measured performance due to memorization rather than genuine generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ARCADE benchmark splits (Existing vs New Tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-split benchmark where Existing Tasks are mined from GitHub (at risk of leakage) and New Tasks are created from recent Kaggle datasets to serve as held-out, contamination-resistant evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>annotated NL intents derived from public notebooks (Existing) vs new intents created before solutions (New)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>notebook code cells and full notebooks</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>data contamination / memorization causing misalignment between claimed 'unseen' evaluation and actual model exposure</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Models may have seen tutorial or public notebooks similar to Existing Tasks during pretraining/fine-tuning; performance on Existing Tasks can therefore be higher due to memorized solutions rather than ability to generalize from context and NL.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation / dataset construction and model training data overlap</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical comparison between two splits (Existing vs New) and near-deduplication of training data; observed large performance gap between splits suggests leakage/memorization effects.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Compare pass@k across splits: e.g., PAChINCO pass@30 Existing Tasks = 78.3% vs New Tasks = 47.7% (Table 2); ablation of deduplication removed ~350K training notebooks similar to evaluation cells.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large: models perform substantially better on Existing Tasks (often simpler and potentially memorized) than on New Tasks, so using only Existing Tasks would overestimate model capabilities and hurt reproducibility/generalization claims.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Significant risk for benchmarks derived from public code; authors estimate many tutorial notebooks and older datasets appear in training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Widespread use of public notebooks on the Web; insufficient deduplication between training and evaluation corpora; memorization by large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Construct New Tasks from recent datasets not present in public corpora; perform near-deduplication at cell level against evaluation notebooks; encourage held-out datasets and careful data curation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: deduplication removed ~350K notebooks from fine-tuning data and New Tasks produced harder, more realistic evaluation (PAChINCO performance lower on New Tasks). However, some leakage may remain in base LM pretraining, and a performance gap still persists.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / benchmark construction</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language to Code Generation in Interactive Data Science Notebooks', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e729.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e729.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>evaluation_mismatch_fuzzy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Mismatch and Fuzzy Output Matching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct surface-code or strict output equality fails to account for multiple valid implementations; authors design fuzzy execution-based matching to better capture functional equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ARCADE execution-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An evaluation pipeline that executes predicted code and compares its output to reference outputs using heuristics that allow partial matches (e.g., predicted DataFrame can contain superset of reference columns).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>annotated NL intent with potentially multiple valid output formats</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>notebook code cell outputs (Python/pandas execution results)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>metric/evaluation mismatch (strict vs pragmatic equivalence)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Because many intents are underspecified and multiple program outputs are acceptable (e.g., additional columns, different container types), strict equality or surface matching would produce false negatives; therefore the evaluation must be tolerant to pragmatic equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Observation during dataset creation that many intents are underspecified and that model predictions produce valid-but-structurally-different outputs; manual inspection and validation of fuzzy matching heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Define heuristics: canonicalize container types and treat predicted DataFrame correct if it contains all reference columns (subset match); empirically validate false-negative rate is relatively low (reported in Appendix J); use pass@k computed on execution with fuzzy matching.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Makes evaluation more robust to legitimate variations; prevents undercounting correct predictions produced under plausible alternative output structures. Quantitative false-negative rate reported as low (exact number in Appendix J).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Necessary for this benchmark because ~50% of intents are underspecified; otherwise many correct predictions would be marked incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Open-ended nature of data analysis tasks and concise NL intents that do not fully specify output form.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Implement fuzzy output matching heuristics and canonicalization of container types to reduce false negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors report fuzzy matching reliably identifies alternative output-structure solutions with relatively low false-negative rate (Appendix J), enabling fairer accuracy estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / evaluation metrics for program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language to Code Generation in Interactive Data Science Notebooks', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e729.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e729.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prompting_style_misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting-Style Induced Misalignment (Step-by-step vs Memorized Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Forcing models to adopt different output styles (step-by-step decomposition and inline explanations) can change model behavior and sometimes reduce performance when the model otherwise relies on memorized one-liners.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>few-shot prompting strategies for LLM code generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt prefixes and exemplars designed to produce multi-line step-by-step code with optional inline NL explanations, evaluated for impact on accuracy, style, and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>few-shot exemplars and preambles (NL instructions describing desired code style)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>notebook code cell generation style</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>style-induced functional misalignment / broken memorization flow</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>On Existing Tasks (where the model may rely on memorized snippets), asking the model to output additional NL comments or to decompose code into many steps can 'break' the memorization and slightly reduce pass@k, even though such decomposition helps diversity and explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>generation/prompting interface</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Controlled few-shot prompting experiments comparing baseline (vanilla code exemplars) vs step-by-step (SbS) vs SbS + explanations and measuring pass@k across splits (Tables 2 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>pass@30 and other code style metrics (lines of code, tokens per line, APIs per line); observed differences across prompting styles and dataset splits.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>On New Tasks, SbS prompting increased pass@30 (47.7 -> 51.9) and further to 52.5 with explanations; on Existing Tasks, explanation-based prompting slightly worsened pass@k (authors attribute to breaking memorized flow). Thus prompting style can both help and hurt depending on dataset alignment with training data.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across many examples in the experiments; particularly relevant when evaluation split contains data similar to LM training set.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Model reliance on surface memorization for simpler/seen examples; changing output format reduces likelihood of emitting memorized token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Choose prompting style appropriate to the evaluation target (e.g., SbS for ambiguous/harder New Tasks); consider fine-tuning the LM to produce the desired style; use preambles and exemplars selectively.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Mixed: SbS + explanations increased pass@30 by ~4.8 pp on New Tasks and improved diversity; but caused slight accuracy drops on Existing Tasks, indicating effectiveness depends on match between prompt style and model's prior training.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / prompt engineering for program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language to Code Generation in Interactive Data Science Notebooks', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e729.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e729.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>context_size_variable_errors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Insufficient Context Leading to Variable/Schema Errors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Too little notebook context in the prompt (missing previous cells/intents/solutions) causes models to reference undefined variables or incorrect columns, increasing runtime errors and reducing success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>context-windowed prompting in notebook NL-to-code</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental setup that varies number of preceding context cells included in the prompt to study effect on model performance and error types.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>preceding notebook markdown intents and schema descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Jupyter notebook code cell generation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing contextual information / incomplete execution state in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When only the immediate intent (and minimal schema) is provided, the model lacks access to variable definitions and prior transformations, leading to NameError/KeyError or incorrect column references.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / context provisioning for generation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical ablation varying context cell count d and measuring pass@30 and distribution of runtime error types (Figure 8 and 9).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>pass@30 as function of context size: with d=1 (only intent) pass@30 was 44% (Existing) and 17% (New); increasing to d=3 raised pass@30 to 72% (Existing) and 36% (New). Error-type counts (NameError/KeyError) drop with more context.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large: contextual information inclusion can more than double pass@30 on New Tasks (17% -> 36% at d=1 -> d=3) and substantially reduce runtime errors, directly affecting reproducibility and usability.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread: many notebook tasks depend on prior turns and intermediate variables; effect shown across benchmark splits.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Prompts truncated or lacking prior-turn code/intent information; model cannot infer variable definitions or intermediate states from minimal input.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Include multiple previous cells (recommended 3â€“7 cells) in the prompt, include schema and execution-state summaries of intermediate variables, and linearize notebooks preserving prior-turn solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Including 3 context cells increased pass@30 from 17% to 36% on New Tasks (approx. 2x) and reduced NameError/KeyError frequencies as shown in error-distribution experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis for data science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language to Code Generation in Interactive Data Science Notebooks', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e729.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e729.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>output_format_ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguity in Expected Output Format (Entity Set vs Table)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>NL intents sometimes do not specify whether the desired answer should be a simple list/series of entities or a richer DataFrame with multiple associated columns, causing mismatch between NL intent interpretation and implemented code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ARCADE NL-to-code problem specification</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Collection of NL intents for data wrangling/EDA where the desired output format may be under-specified; code solutions may choose different container types or include extra columns.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>concise user intent (may imply entity set or tabular output)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>pandas DataFrame/Series/List output in notebook cell</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification of output data structure</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>About 42% of underspecified intents imply 'entity sets' where either a Series/List of entity names or a DataFrame that includes entities plus additional columns would be acceptable; lack of explicit output schema causes implementations to diverge from annotator reference format.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>output specification / result formatting</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Annotation analysis and categorization of underspecified intents; observation of multiple plausible prediction outputs in model sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Annotation statistics: fraction of underspecified intents implying entity sets (42%); evaluation with fuzzy subset-matching to accept predicted DataFrames that contain reference columns.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to many semantically correct but structurally different outputs that naive exact-match evaluation would mark wrong; necessitates fuzzy matching or additional intent clarification to fairly measure correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common among underspecified intents (42% of ambiguous intents imply entity-sets).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Natural NL concision and omission of explicit output schema; developer expectation that assistant will infer reasonable output container.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Annotators add explicit output schema where necessary; evaluation uses fuzzy output matching (subset column inclusion); prompt engineering to request specific output formats.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Fuzzy matching helps recover many legitimate solutions; requiring explicit schema when creating problems reduced ambiguity but at cost of making intents less natural/realistic (authors report trade-off).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / data science program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Natural Language to Code Generation in Interactive Data Science Notebooks', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>JuICe: A large scale distantly supervised dataset for open domain context-based code generation <em>(Rating: 2)</em></li>
                <li>Training and evaluating a Jupyter notebook data science assistant <em>(Rating: 2)</em></li>
                <li>Execution-based evaluation for data science code generation models <em>(Rating: 2)</em></li>
                <li>DS-1000: A natural and reliable benchmark for data science code generation <em>(Rating: 1)</em></li>
                <li>Natural language-guided programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-729",
    "paper_id": "paper-815c6ca281536d18ec0eb408b6e46e72a0826163",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "underspecified_intents",
            "name_full": "Underspecified Natural Language Intents",
            "brief_description": "User natural language intents provided to the NL-to-code system are often concise and lack sufficient specification, producing multiple plausible programmatic interpretations and output formats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ARCADE evaluation of NL-to-code in notebooks",
            "system_description": "Benchmark and experimental setup that maps succinct user NL intents (in notebook cells) to pandas-based Python code solutions, evaluated via execution-based metrics.",
            "nl_description_type": "user-issued natural language intent (notebook cell comment)",
            "code_implementation_type": "Jupyter notebook code cell (pandas Python)",
            "gap_type": "ambiguous/under-specified specification",
            "gap_description": "Approximately half of collected user intents lack details required to uniquely determine the desired output (e.g., target columns, whether results should be a DataFrame vs list/Series, index/headers, or post-processing steps). This ambiguity leads to multiple semantically plausible implementations that differ in output structure.",
            "gap_location": "problem specification / input intent",
            "detection_method": "Manual analysis of a random sample of 100 annotated intents by the authors/annotators and empirical model evaluation showing divergent outputs; annotation process flagged underspecification and authors measured prevalence.",
            "measurement_method": "Dataset annotation statistics (percent of intents falling into ambiguity categories) and execution-based pass@k metrics with/without extra intent specifications; fuzzy output matching used to mitigate strictness.",
            "impact_on_results": "Large effect on functional correctness: on a subset of 136 New-Tasks intents that had extra specifications, removing those specifications dropped PACHINCO's pass@30 from 46.5% (with specs) to 27.8% (without specs), a ~18.7 percentage-point absolute decrease, showing ambiguity materially reduces success rates.",
            "frequency_or_prevalence": "About 50% of intents judged underspecified; within these: 10% lacked explicit target columns, 42% implied entity-sets (ambiguity between Series/List vs DataFrame), 23% implied complex schema (nested indices/headers), 20% implied additional post-processing/multiple variables, 5% were highly complex/unclear.",
            "root_cause": "Concise, ephemeral style of real-world developer prompts that omit structural/output details; natural ambiguity in free-form NL; intentional design choice to collect realistic short intents.",
            "mitigation_approach": "Ask annotators to add clarifying output specifications when needed; introduce evaluation tolerant to plausible variants (fuzzy output matching); increase prediction diversity (few-shot SbS prompting) and use reranking (self-consistency) to surface plausible interpretations.",
            "mitigation_effectiveness": "Quantified: adding explicit specifications (during annotation) improved pass@30 from 27.8% to 46.5% on the tested subset (+18.7 pp). Fuzzy output matching reduces false negatives (authors report low false-negative rate) and diverse sampling plus reranking improved single-sample accuracy beyond baseline pass@5, but per-sample correctness fraction remained ~15% (so diversity mainly improved pass@k).",
            "domain_or_field": "machine learning / program synthesis for data science",
            "reproducibility_impact": true,
            "uuid": "e729.0",
            "source_info": {
                "paper_title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "schema_grounding_gap",
            "name_full": "Missing Schema / Variable State Grounding",
            "brief_description": "Models fail when NL refers to dataset columns or runtime variables that are not provided/encoded in the prompt; explicit schema descriptions and variable states are needed for correct grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ARCADE prompting and evaluation",
            "system_description": "Prompting LLMs for code generation using notebook context that can include explicit NL schema descriptions (column names and sample values) and previous-code-derived state.",
            "nl_description_type": "NL description of imported DataFrame schema and variable states in notebook context",
            "code_implementation_type": "Jupyter notebook code cell (pandas Python)",
            "gap_type": "missing variable/schema grounding (incomplete context)",
            "gap_description": "When schema descriptions or execution-state information for intermediate variables are omitted from the prompt, the model frequently references undefined columns/variables or selects incorrect columns, producing NameError/KeyError at execution or incorrect outputs.",
            "gap_location": "input context / grounding of NL to program variables (schema understanding)",
            "detection_method": "Ablation experiments: remove schema descriptions from prompts and measure changes in pass@k; controlled experiments varying number of context cells and tracking runtime error types (NameError, KeyError).",
            "measurement_method": "pass@k metrics (Table 2), run-time error counts and categories aggregated over predictions (Figures 8 and 9), and absolute pass@30 differences when -Schema Desc. ablated.",
            "impact_on_results": "Substantial: removing schema descriptions reduced PACHINCO's pass@30 on New Tasks from 47.7% to 36.1% (an 11.6 percentage-point drop); limiting context to 1 cell produced pass@30 of 17% on New Tasks versus 36% at 3 context cells, and increased NameError/KeyError frequency.",
            "frequency_or_prevalence": "Common: many problems require grounding to DataFrame schema; experiments indicate this is a systematic failure mode when schema info is absent (measurable drops across many examples).",
            "root_cause": "Prompts lacking explicit structured schema or intermediate variable states; LLMs need grounding signals to map NL phrases to programmatic identifiers.",
            "mitigation_approach": "Include NL schema descriptions in prompts, include more preceding context cells (previous turns), and consider encoding intermediate variable states; possibly augment LM training with notebook data containing such context.",
            "mitigation_effectiveness": "Effective: including schema descriptions and more context cells raised pass@30 by ~11.6 pp (schema) and roughly doubled pass@30 from d=1 to d=3 contexts on New Tasks (17% -&gt; 36%).",
            "domain_or_field": "machine learning / program synthesis for data science",
            "reproducibility_impact": true,
            "uuid": "e729.1",
            "source_info": {
                "paper_title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "train_eval_leakage",
            "name_full": "Training / Evaluation Data Leakage and Memorization",
            "brief_description": "Existing evaluation notebooks may appear in LLM training corpora, inflating measured performance due to memorization rather than genuine generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ARCADE benchmark splits (Existing vs New Tasks)",
            "system_description": "Two-split benchmark where Existing Tasks are mined from GitHub (at risk of leakage) and New Tasks are created from recent Kaggle datasets to serve as held-out, contamination-resistant evaluation.",
            "nl_description_type": "annotated NL intents derived from public notebooks (Existing) vs new intents created before solutions (New)",
            "code_implementation_type": "notebook code cells and full notebooks",
            "gap_type": "data contamination / memorization causing misalignment between claimed 'unseen' evaluation and actual model exposure",
            "gap_description": "Models may have seen tutorial or public notebooks similar to Existing Tasks during pretraining/fine-tuning; performance on Existing Tasks can therefore be higher due to memorized solutions rather than ability to generalize from context and NL.",
            "gap_location": "evaluation / dataset construction and model training data overlap",
            "detection_method": "Empirical comparison between two splits (Existing vs New) and near-deduplication of training data; observed large performance gap between splits suggests leakage/memorization effects.",
            "measurement_method": "Compare pass@k across splits: e.g., PAChINCO pass@30 Existing Tasks = 78.3% vs New Tasks = 47.7% (Table 2); ablation of deduplication removed ~350K training notebooks similar to evaluation cells.",
            "impact_on_results": "Large: models perform substantially better on Existing Tasks (often simpler and potentially memorized) than on New Tasks, so using only Existing Tasks would overestimate model capabilities and hurt reproducibility/generalization claims.",
            "frequency_or_prevalence": "Significant risk for benchmarks derived from public code; authors estimate many tutorial notebooks and older datasets appear in training corpora.",
            "root_cause": "Widespread use of public notebooks on the Web; insufficient deduplication between training and evaluation corpora; memorization by large LMs.",
            "mitigation_approach": "Construct New Tasks from recent datasets not present in public corpora; perform near-deduplication at cell level against evaluation notebooks; encourage held-out datasets and careful data curation.",
            "mitigation_effectiveness": "Partially effective: deduplication removed ~350K notebooks from fine-tuning data and New Tasks produced harder, more realistic evaluation (PAChINCO performance lower on New Tasks). However, some leakage may remain in base LM pretraining, and a performance gap still persists.",
            "domain_or_field": "machine learning / benchmark construction",
            "reproducibility_impact": true,
            "uuid": "e729.2",
            "source_info": {
                "paper_title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "evaluation_mismatch_fuzzy",
            "name_full": "Evaluation Mismatch and Fuzzy Output Matching",
            "brief_description": "Direct surface-code or strict output equality fails to account for multiple valid implementations; authors design fuzzy execution-based matching to better capture functional equivalence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ARCADE execution-based evaluation",
            "system_description": "An evaluation pipeline that executes predicted code and compares its output to reference outputs using heuristics that allow partial matches (e.g., predicted DataFrame can contain superset of reference columns).",
            "nl_description_type": "annotated NL intent with potentially multiple valid output formats",
            "code_implementation_type": "notebook code cell outputs (Python/pandas execution results)",
            "gap_type": "metric/evaluation mismatch (strict vs pragmatic equivalence)",
            "gap_description": "Because many intents are underspecified and multiple program outputs are acceptable (e.g., additional columns, different container types), strict equality or surface matching would produce false negatives; therefore the evaluation must be tolerant to pragmatic equivalence.",
            "gap_location": "evaluation metric",
            "detection_method": "Observation during dataset creation that many intents are underspecified and that model predictions produce valid-but-structurally-different outputs; manual inspection and validation of fuzzy matching heuristics.",
            "measurement_method": "Define heuristics: canonicalize container types and treat predicted DataFrame correct if it contains all reference columns (subset match); empirically validate false-negative rate is relatively low (reported in Appendix J); use pass@k computed on execution with fuzzy matching.",
            "impact_on_results": "Makes evaluation more robust to legitimate variations; prevents undercounting correct predictions produced under plausible alternative output structures. Quantitative false-negative rate reported as low (exact number in Appendix J).",
            "frequency_or_prevalence": "Necessary for this benchmark because ~50% of intents are underspecified; otherwise many correct predictions would be marked incorrect.",
            "root_cause": "Open-ended nature of data analysis tasks and concise NL intents that do not fully specify output form.",
            "mitigation_approach": "Implement fuzzy output matching heuristics and canonicalization of container types to reduce false negatives.",
            "mitigation_effectiveness": "Authors report fuzzy matching reliably identifies alternative output-structure solutions with relatively low false-negative rate (Appendix J), enabling fairer accuracy estimates.",
            "domain_or_field": "machine learning / evaluation metrics for program synthesis",
            "reproducibility_impact": true,
            "uuid": "e729.3",
            "source_info": {
                "paper_title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "prompting_style_misalignment",
            "name_full": "Prompting-Style Induced Misalignment (Step-by-step vs Memorized Generation)",
            "brief_description": "Forcing models to adopt different output styles (step-by-step decomposition and inline explanations) can change model behavior and sometimes reduce performance when the model otherwise relies on memorized one-liners.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "few-shot prompting strategies for LLM code generation",
            "system_description": "Prompt prefixes and exemplars designed to produce multi-line step-by-step code with optional inline NL explanations, evaluated for impact on accuracy, style, and diversity.",
            "nl_description_type": "few-shot exemplars and preambles (NL instructions describing desired code style)",
            "code_implementation_type": "notebook code cell generation style",
            "gap_type": "style-induced functional misalignment / broken memorization flow",
            "gap_description": "On Existing Tasks (where the model may rely on memorized snippets), asking the model to output additional NL comments or to decompose code into many steps can 'break' the memorization and slightly reduce pass@k, even though such decomposition helps diversity and explainability.",
            "gap_location": "generation/prompting interface",
            "detection_method": "Controlled few-shot prompting experiments comparing baseline (vanilla code exemplars) vs step-by-step (SbS) vs SbS + explanations and measuring pass@k across splits (Tables 2 and 4).",
            "measurement_method": "pass@30 and other code style metrics (lines of code, tokens per line, APIs per line); observed differences across prompting styles and dataset splits.",
            "impact_on_results": "On New Tasks, SbS prompting increased pass@30 (47.7 -&gt; 51.9) and further to 52.5 with explanations; on Existing Tasks, explanation-based prompting slightly worsened pass@k (authors attribute to breaking memorized flow). Thus prompting style can both help and hurt depending on dataset alignment with training data.",
            "frequency_or_prevalence": "Observed across many examples in the experiments; particularly relevant when evaluation split contains data similar to LM training set.",
            "root_cause": "Model reliance on surface memorization for simpler/seen examples; changing output format reduces likelihood of emitting memorized token sequences.",
            "mitigation_approach": "Choose prompting style appropriate to the evaluation target (e.g., SbS for ambiguous/harder New Tasks); consider fine-tuning the LM to produce the desired style; use preambles and exemplars selectively.",
            "mitigation_effectiveness": "Mixed: SbS + explanations increased pass@30 by ~4.8 pp on New Tasks and improved diversity; but caused slight accuracy drops on Existing Tasks, indicating effectiveness depends on match between prompt style and model's prior training.",
            "domain_or_field": "machine learning / prompt engineering for program synthesis",
            "reproducibility_impact": true,
            "uuid": "e729.4",
            "source_info": {
                "paper_title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "context_size_variable_errors",
            "name_full": "Insufficient Context Leading to Variable/Schema Errors",
            "brief_description": "Too little notebook context in the prompt (missing previous cells/intents/solutions) causes models to reference undefined variables or incorrect columns, increasing runtime errors and reducing success rates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "context-windowed prompting in notebook NL-to-code",
            "system_description": "Experimental setup that varies number of preceding context cells included in the prompt to study effect on model performance and error types.",
            "nl_description_type": "preceding notebook markdown intents and schema descriptions",
            "code_implementation_type": "Jupyter notebook code cell generation",
            "gap_type": "missing contextual information / incomplete execution state in prompt",
            "gap_description": "When only the immediate intent (and minimal schema) is provided, the model lacks access to variable definitions and prior transformations, leading to NameError/KeyError or incorrect column references.",
            "gap_location": "data preprocessing / context provisioning for generation",
            "detection_method": "Empirical ablation varying context cell count d and measuring pass@30 and distribution of runtime error types (Figure 8 and 9).",
            "measurement_method": "pass@30 as function of context size: with d=1 (only intent) pass@30 was 44% (Existing) and 17% (New); increasing to d=3 raised pass@30 to 72% (Existing) and 36% (New). Error-type counts (NameError/KeyError) drop with more context.",
            "impact_on_results": "Large: contextual information inclusion can more than double pass@30 on New Tasks (17% -&gt; 36% at d=1 -&gt; d=3) and substantially reduce runtime errors, directly affecting reproducibility and usability.",
            "frequency_or_prevalence": "Widespread: many notebook tasks depend on prior turns and intermediate variables; effect shown across benchmark splits.",
            "root_cause": "Prompts truncated or lacking prior-turn code/intent information; model cannot infer variable definitions or intermediate states from minimal input.",
            "mitigation_approach": "Include multiple previous cells (recommended 3â€“7 cells) in the prompt, include schema and execution-state summaries of intermediate variables, and linearize notebooks preserving prior-turn solutions.",
            "mitigation_effectiveness": "Including 3 context cells increased pass@30 from 17% to 36% on New Tasks (approx. 2x) and reduced NameError/KeyError frequencies as shown in error-distribution experiments.",
            "domain_or_field": "machine learning / program synthesis for data science",
            "reproducibility_impact": true,
            "uuid": "e729.5",
            "source_info": {
                "paper_title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "output_format_ambiguity",
            "name_full": "Ambiguity in Expected Output Format (Entity Set vs Table)",
            "brief_description": "NL intents sometimes do not specify whether the desired answer should be a simple list/series of entities or a richer DataFrame with multiple associated columns, causing mismatch between NL intent interpretation and implemented code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ARCADE NL-to-code problem specification",
            "system_description": "Collection of NL intents for data wrangling/EDA where the desired output format may be under-specified; code solutions may choose different container types or include extra columns.",
            "nl_description_type": "concise user intent (may imply entity set or tabular output)",
            "code_implementation_type": "pandas DataFrame/Series/List output in notebook cell",
            "gap_type": "incomplete specification of output data structure",
            "gap_description": "About 42% of underspecified intents imply 'entity sets' where either a Series/List of entity names or a DataFrame that includes entities plus additional columns would be acceptable; lack of explicit output schema causes implementations to diverge from annotator reference format.",
            "gap_location": "output specification / result formatting",
            "detection_method": "Annotation analysis and categorization of underspecified intents; observation of multiple plausible prediction outputs in model sampling.",
            "measurement_method": "Annotation statistics: fraction of underspecified intents implying entity sets (42%); evaluation with fuzzy subset-matching to accept predicted DataFrames that contain reference columns.",
            "impact_on_results": "Leads to many semantically correct but structurally different outputs that naive exact-match evaluation would mark wrong; necessitates fuzzy matching or additional intent clarification to fairly measure correctness.",
            "frequency_or_prevalence": "Common among underspecified intents (42% of ambiguous intents imply entity-sets).",
            "root_cause": "Natural NL concision and omission of explicit output schema; developer expectation that assistant will infer reasonable output container.",
            "mitigation_approach": "Annotators add explicit output schema where necessary; evaluation uses fuzzy output matching (subset column inclusion); prompt engineering to request specific output formats.",
            "mitigation_effectiveness": "Fuzzy matching helps recover many legitimate solutions; requiring explicit schema when creating problems reduced ambiguity but at cost of making intents less natural/realistic (authors report trade-off).",
            "domain_or_field": "machine learning / data science program synthesis",
            "reproducibility_impact": true,
            "uuid": "e729.6",
            "source_info": {
                "paper_title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "JuICe: A large scale distantly supervised dataset for open domain context-based code generation",
            "rating": 2
        },
        {
            "paper_title": "Training and evaluating a Jupyter notebook data science assistant",
            "rating": 2
        },
        {
            "paper_title": "Execution-based evaluation for data science code generation models",
            "rating": 2
        },
        {
            "paper_title": "DS-1000: A natural and reliable benchmark for data science code generation",
            "rating": 1
        },
        {
            "paper_title": "Natural language-guided programming",
            "rating": 1
        }
    ],
    "cost": 0.01998225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Natural Language to Code Generation in Interactive Data Science Notebooks</h1>
<p>Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, Charles Sutton</p>
<p>Google Inc.</p>
<h4>Abstract</h4>
<p>Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1,078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PACHINCO, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanations, showing the potential to improve the diversity and explainability of model predictions. ARCADE is publicly available at https://github.com/ google-research/arcade-nl2code/.</p>
<h2>1 Introduction</h2>
<p>Data science is the process of extracting insights from data (Wang et al., 2021a), and has become an integral part of decision making and knowledge discovery (Donoho, 2017). Data scientists and machine learning (ML) practitioners often use computational notebooks, which are interactive environments such as Jupyter notebooks (Kluyver et al., 2016) and Google Colab, in their work. Data scientists spend a significant amount of time on data wrangling tasks to process raw data into usable forms (illustrated in Fig. 1), as well as exploratory data analysis (EDA) to gain insights</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of a computational notebook adapted from our dataset, with examples of reading data (cell $c_{1}$ ), data wrangling $\left(c_{2}, c_{3}\right)$, and exploratory data analysis ( $c_{4} \sim c_{7}$ ). Annotated NL intents $\left(\mathbf{u}_{i}\right)$ are shown in green.
for decision making (Agashe et al., 2019; Wang et al., 2022a). This has motivated research on automating and accelerating the data science workflow in general (Aggarwal et al., 2019; Wang et al., 2021a,b), with particular interest in data wrangling and EDA tasks (Bavishi et al., 2019; Jain et al., 2021; Nazabal et al., 2020; Kandel et al., 2011).</p>
<p>Meanwhile, large language models (LLMs) trained on code can assist developers by translating natural language (NL) intents into executable programs (Chen et al., 2021a; Austin et al., 2021; Chowdhery et al., 2022; Nijkamp et al., 2022;</p>
<p>Fried et al., 2022), with promising applications in synthesizing code for data wrangling and EDA tasks (Jain et al., 2021; Rajkumar et al., 2022; Cheng et al., 2022b). Computational notebooks also present unique challenges to LLMs, as notebooks freely mix NL, code, graphics, and execution results (Perkel, 2021), and because of their interactivity, notebooks feature multiple interdependent NL-to-code problems (Heyman et al., 2021).</p>
<p>Several benchmarks have been proposed to evaluate program synthesis of data science programs from NL intents, but these datasets have several limitations. First, some datasets derive from data science tutorial notebooks (Agashe et al., 2019; Chandel et al., 2022), which tend to contain NL text (e.g., exercise questions) that is verbose and elaborate, instead of the concise, ephemeral style that developers write when interacting with code LMs (Barke et al., 2022, more in Â§3). Other datasets assume that the developer provides extra information, such as unit tests or input/output examples (Chandel et al., 2022; Jain et al., 2022), but such systems pose an extra burden to users who might not normally write such tests or examples during their workflow (Pimentel et al., 2019). Finally, existing datasets usually contain independent tasks with isolated contexts (Lai et al., 2022), or a limited number of contextually dependent problems (Huang et al., 2022), rather than having multiple, related tasks such as in Fig. 1. Therefore, there is a need for a benchmark with realistic NL intents, rich notebook context, and a series of interrelated problems, so as to better reflect real-world usage by data scientists.</p>
<p>To fill this gap, we present ARCADE, ${ }^{1}$ a new benchmark for code generation for data wrangling and EDA tasks in computational notebooks (Â§3). ARCADE consists of 1,078 problems spanning across 136 notebooks based on 106 ML datasets. It features a series of NL utterances written by professional data scientists with the intention of interacting with an AI assistant (e.g., green texts in Fig. 1), with high-quality code solutions using the pandas library. To mitigate the risk of data leakage, $60 \%$ of the problems are created from scratch, based on recent ML datasets on Kaggle (e.g., the csv file in $c_{1}$, Fig. 1). ${ }^{2}$ ARCADE also challenges LLMs with grounded language understanding, where a model needs to leverage variable states (e.g., df ['TIME'] in $c_{2}$ ) to interpret NL semantics (e.g., "min and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>max" in $\boldsymbol{u}_{1}$ ). Finally, problems in ARCADE are challenging, involving richer data science API usage than existing benchmarks.</p>
<p>To demonstrate how ARCADE can motivate new research on LLMs for data science, we develop PAChINCO, a 62B code LM tailored for Python computational notebooks, trained on a mixture of NL, source code, and Jupyter notebooks data (Â§4). PAChINCO significantly outperforms public code LMs on ARCADE (Â§5.2). Even so, all models have difficulty on our benchmark, showing that it is a challenging task. Further, we explore few-shot prompting strategies to alter the style of model predictions, such as decomposing code into step-bystep structures and adding inline NL explanations. Not only is code in this style potentially more understandable to novice data scientists, prompting the model to explain its solutions also improves the diversity of the model's predictions (Â§5.3).</p>
<h2>2 Problem Statement</h2>
<p>A computational notebook is an interactive computing environment that allows mixing code, text, and graphics. A notebook consists of a sequence of Markdown or source code cells. Given a partial notebook context with $n$ cells $\left{c_{i}\right}<em n_1="n+1">{i=1}^{n}$ and a userspecified intent $\boldsymbol{u}$ for the next cell $c</em>}$ (e.g., $\boldsymbol{u<em n_1="n+1">{1}$ in Fig. 1 for $n=1$ ), we aim to generate code for $c</em>}$ that fulfills the user's intent (Agashe et al., 2019). We refer to the pair $\left(\left{c_{i}\right}, \boldsymbol{u}\right)$ as a problem. This process could proceed sequentially with multiple rounds between the user and a system (Heyman et al., 2021), so a single notebook can contain multiple problems. To satisfy subsequent intents (e.g., $\boldsymbol{u<em i="i">{4}$ ), a system will leverage the updated notebook context (e.g., $\left{c</em>\right}<em 1="1">{i=1}^{5}$ ) which includes previous problems (e.g., those involving $\boldsymbol{u}</em>$ ).}$ to $\boldsymbol{u}_{3</p>
<p>As in Fig. 1, problems within a notebook often have interesting dependency structures. They may share execution context (e.g., DataFrame df), form semantically coherent turns (e.g., $c_{4}$ and $c_{5}$ ), or exhibit non-trivial long range data dependencies (e.g., from $c_{6}$ to $c_{2}$, or $c_{7}$ to $c_{3}$ ). These dependency structures are more diverse than existing multi-turn code generation tasks with sequentially dependent problems (Nijkamp et al., 2022).</p>
<h2>3 Arcade: A Benchmark of pandas Data Science Code Generation</h2>
<h3>3.1 Constructing ARCADE</h3>
<p>ARCADE consists of 1,078 NL-to-code problems from 131 notebooks based on 106 unique ML</p>
<p>datasets, sourced from existing data science notebooks on GitHub (Existing Tasks split) and new ones created from scratch (New Tasks split). The problems are annotated by professional data science freelancers. This section outlines the dataset creation process. See Appendix A for more details.
Repurposing Existing Notebooks To build the Existing Tasks split, we identify candidate code cells performing data wrangling and EDA tasks from existing high-quality notebooks, and then manually annotate these cells with NL intents. Specifically, we perform static analysis to identify notebooks with rich code cells related to data wrangling and EDA tasks (e.g., by identifying cells using pandas functions) from public notebook corpora such as JuICe (Agashe et al., 2019) and BiGQUERY. We then select 63 notebooks with the greatest number of candidate code cells for annotation, covering 36 ML datasets from a variety of domains. Annotation consists of judging the quality of candidate cells, fixing errors, and creating intents summarizing the code (described below).
Creating Notebooks for Novel ML Datasets The Existing Tasks split captures realistic problems and notebook contexts, but may result in artificially high evaluation accuracies due to potential leakage of evaluation notebooks in the training data of LLMs, which is a common issue in LLM evaluation (Brown et al., 2020). ${ }^{3}$ To prevent contamination, we additionally build the New Tasks split with 660 problems in notebooks created from scratch. Specifically, we create notebooks with wrangling and EDA tasks for 70 tabular ML datasets that appeared on Kaggle since February 2022 and are manually verified to differ from existing datasets on the Web. For each Kaggle dataset, we instructed the annotators to create a notebook with tasks that would provide insights for building an ML model for the dataset. To make the problems more challenging, we also encouraged them to make tasks that require at least 5 pandas API calls to solve.
Annotating NL Intents When creating NL intents for a problem, ${ }^{4}$ annotators are instructed to phrase their intents in the way they prefer when interacting with an AI system to help them implement the existing code solution, while keeping the intents natural and concise, without redundant elaboration such as line-by-line explanation. In addition, to</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>make the intents more challenging, we encourage annotators to refer to entities and variables in the intents using semantic rewrites without introducing ambiguity (e.g., use "convert all binary columns to bool" instead of listing columns verbatim), reminiscent of synonym substitution for labeling utterances in text-to-SQL (Gan et al., 2021).
Mitigating Ambiguity in NL Intents Creating succinct NL intents without ambiguity could be non-trivial in this open-domain code generation setting, especially when there could be multiple plausible interpretations of an intent. For example, without the underlined part of $\boldsymbol{u}<em 5="5">{5}$ (Fig. 1), a programmer or a system may propose alternative solutions using different table schema. Therefore, for such open-ended problems where there could be multiple alternative ways to present the answer, we ask annotators to provide extra specification in their intents about the desired output (e.g., schema of the output DataFrame, such as the underlined part in $\boldsymbol{u}</em>$ ). Even with these additional semantic constraints, empirically we observe that about $50 \%$ of intents are still underspecified, making ARCADE a challenging benchmark for handling realistic NL intents with uncertainty. We present more analysis in $\S 3.2$ and introduce a robust evaluation metric that mitigates this issue in $\S 3.3$.
Annotation Guideline Besides mitigating ambiguity in intents, there are many other aspects to consider during annotation, such as notebook style (e.g., removing background material and hints in tutorial notebooks in Existing Tasks to avoid solution leakage), task diversity, and quality control, which we discuss in a 35-page annotation guideline provided to annotators, outlined in Appendix B.</p>
<h3>3.2 Dataset Analysis</h3>
<p>We first present some analysis on ARCADE and then compare it to existing datasets in Tab. 1.
NL Intents are often Underspecified ARCADE aims to evaluate code LMs in the real-world scenario where data scientists provide succinct NL intents without extra specification (e.g., I/O examples). As a result, the intents we collected are often underspecified and may not contain sufficient information to generate a solution that executes to the exact reference output. To understand the patterns of semantic ambiguity in user-issued intents, we examined 100 random samples. Around $50 \%$ of them are precise and sufficient to infer the target outputs. Those intents are often numerical queries with lim-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Src.</th>
<th style="text-align: center;">Exec?</th>
<th style="text-align: center;">Evaluation Method</th>
<th style="text-align: center;"># N.B.</th>
<th style="text-align: center;"># P.</th>
<th style="text-align: center;">P. / N.B.</th>
<th style="text-align: center;">Intents Type</th>
<th style="text-align: center;">Intent Length</th>
<th style="text-align: center;">AST Size* All / pandas</th>
<th style="text-align: center;"># API^</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">JuICe (Agashe et al., 2019)</td>
<td style="text-align: center;">GH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Surface Match</td>
<td style="text-align: center;">1,457</td>
<td style="text-align: center;">3,946</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">Markdown</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">$21.2 / 24.3^{1}$</td>
<td style="text-align: center;">2.5</td>
</tr>
<tr>
<td style="text-align: center;">DSP (Chandel et al., 2022)</td>
<td style="text-align: center;">GH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unit Tests</td>
<td style="text-align: center;">305</td>
<td style="text-align: center;">1,096</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">Markd.+Tests</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">$28.7 / 34.8^{1}$</td>
<td style="text-align: center;">3.1</td>
</tr>
<tr>
<td style="text-align: center;">ExeDS (Huang et al., 2022) ${ }^{\circ}$</td>
<td style="text-align: center;">GH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Output Match</td>
<td style="text-align: center;">277</td>
<td style="text-align: center;">534</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">Annotated NL</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">$9.0 / 10.7$</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: center;">NLGP (Heyman et al., 2021)</td>
<td style="text-align: center;">GH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Surface Match</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">201</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">Annotated NL</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">$13.5 / 15.1$</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;">DS-1000 (Lai et al., 2022) ${ }^{\circ}$</td>
<td style="text-align: center;">SO</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tests+Constraints</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Annotated NL</td>
<td style="text-align: center;">166.5</td>
<td style="text-align: center;">$27.3 / 41.6^{1}$</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr>
<td style="text-align: center;">This Work: ARCADE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\vdash$ Existing Tasks ${ }^{\dagger}$</td>
<td style="text-align: center;">GH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fuzzy Output Match</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">417</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">Annotated NL</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">4.3</td>
</tr>
<tr>
<td style="text-align: center;">$\vdash$ New Tasks</td>
<td style="text-align: center;">New</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Â§3.3)</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">661</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">5.8</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary statistics of ARCADE and existing datasets (evaluation splits). Source: GitHub (GH) or StackOverflow (SO). Exec?: executability. N.B.: notebooks. P.: problems. P./N.B.: problems per notebook. *Metrics are averaged over all examples and (/) those using pandas. ${ }^{\dagger}$ AST sizes are for reference only because some programs contain boilerplate code (e.g., function/class sketches) which is not part of solution. ${ }^{\triangleleft}$ Number of pandas API calls on the subset using pandas. ${ }^{\text {T }}$ ExeDS is concurrent work, consisting of executable examples in JuICe with annotated intents and evaluation based on output matching. ${ }^{\circ}$ DS-1000 derived StackOverflow problems without notebook contexts and is not directly comparable to other work in this table. ${ }^{\dagger}$ Existing Tasks also has extra 59 plotting problems in our release (excluded here), which are not used in this paper (Â§9).
ited variety in output type (e.g., $\boldsymbol{u}<em 3="3">{2}, \boldsymbol{u}</em>$, Fig. 1), or contain sufficient output specifications (Â§3.1). The remaining half are underspecified: (a) only $10 \%$ of the ambiguous intents lack descriptions of target columns in output DataFrames; more interestingly, (b) $42 \%$ imply entity sets as outputs (e.g., Where are the top 10 customers receiving the highest incomes located?), answerable either using container types with entity names only (e.g., a List or Series of locations), or DataFrames with entities and additional columns (e.g. incomes) mentioned in the intents; (c) $23 \%$ imply output with complex schema, such as a nested row index or table header (e.g., Show the time of the day and the fare price for each airline) which is difficult to infer without extra information, and (d) $20 \%$ require outputs with more complex structures (e.g., multiple variables) or imply additional post-processing steps such as data imputation, while (e) the remaining $5 \%$ have complex intents that are difficult to understand without additional clarifications.</p>
<h2>Notebook Context Helps Disambiguate Intents</h2>
<p>Notably, while half of the intents are underspecified, $25 \%$ of those cases can be disambiguated by referring to prior rounds of problems in the context with similar query/output patterns. These are often follow-up queries (e.g., Which of them are . . .) of a prior turn (e.g., Show me all ...), analogous to similar thematic relation patterns in contextual semantic parsing (Yu et al., 2019b).
Comparing Existing and New Tasks Comparing the Existing Tasks and New Tasks splits, the latter is more challenging, as measured by the number of pandas API invocations and the AST size of reference solutions (Tab. 1, Bottom). Fig. 2 plots a histogram of the number of API calls per problem, where $67 \%$ of problems in New Tasks require at</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Histogram of the number of pandas API calls.
least 5 API calls to solve. As discussed in $\S 5$, with more complex held-out problems targeting recent ML datasets, the New Tasks split is a more robust benchmark and more challenging for code LLMs.
Comparing with Existing Datasets Tab. 1 compares ARCADE with existing data science code generation datasets. We remark that ARCADE is the only benchmark that satisfies all the following criteria: First, ARCADE features succinct and realistic intents as problem specifications ("Intents Type" column, Tab. 1), which are significantly shorter ("Intent Length" column) than the verbose Markdown problem definitions found in tutorial or assignment notebooks (c.f. JuICe, DSP). ARCADE also does not rely on extra specifications such as unit tests (c.f. DSP), which better capture the real-world scenario where developers prompt LMs using ephemeral comments for code completion (Barke et al., 2022). Most of these intents are often underspecified (mentioned earlier in $\S 3.2$ ), requiring a more robust evaluation metric to consider alternative answers (discussed in Â§3.3), while motivating future research on improving prediction diversity to cover plausible problem interpretations (explored in $\S 5.1$ ) or explicit modeling of intent uncertainty (Lin et al., 2022). Second, ARCADE contains more related problems in a single notebook ("P./N.B." column) with diverse dependency patterns (e.g., Fig. 1), capturing the essence of interactive computing. This makes our</p>
<p>dataset useful in testing an LLM's ability to understand rich contexts, including existing user-written cells, as well as preceding problems and their solutions (Â§2). Third, 2 ARCADE challenges LLMs with grounded language understanding, where the model needs to ground semantic concepts in the intents (e.g., "max and min" in $\boldsymbol{u}_{1}$, Fig. 1) to the corresponding variable execution states in the context (e.g., the TIME column in df). The need for understanding semi-structured data and performing necessary transformations (Pasupat and Liang, 2015) using an open-domain programming language (PL, Python) makes language grounding in ARCADE more difficult than in existing EDA tasks using domain-specific PLs, such as semantic parsing over databases (Yu et al., 2019b). Fourth, 2 ARCADE has more complex problems with richer usage of real-world data science APIs. The number of pandas APIs used in each problem ("# API" in Tab. 1) is on par with DS-1000 and significantly higher than other datasets. ${ }^{5}$ Finally, besides problem complexity, $\mathbf{2} 60 \%$ of problems in ARCADE are created from scratch to mitigate evaluation data leakage. These data science problems also target recent tabular ML datasets, making ARCADE a reliable benchmark to test the generalization ability of LLMs in semi-structured knowledge understanding (Lee et al., 2021).</p>
<h3>3.3 Evaluation by Fuzzy Output Matching</h3>
<p>We aim to synthesize programs in notebooks using only cell contexts and NL intents without extra specification such as unit tests (Â§2). As in $\S 3.2$, those intents are often underspecified and have multiple alternative solutions. We therefore approximately match the execution output of a predicted program with the annotated reference to determine if they are functionally equivalent primarily based on two categories of heuristics. ${ }^{6}$ First, we canonicalize variables with different container data types. Second, we allow for partial matching between complex DataFrames. Specifically, for a reference frame $\boldsymbol{v}$ with a set of column vectors $\left{v_{i}\right}$, each representing the cell values for the $i$-th column, a prediction $\hat{\boldsymbol{v}}$ is considered equivalent with $\boldsymbol{v}$ iff for any $v_{i} \in \boldsymbol{v}, v_{i} \in \hat{\boldsymbol{v}}$. Intuitively, we consider a predicted program correct if its output DataFrame contains all the columns (and cell entries) in the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>reference frame, since a user could easily create a more compact view of the frame by selecting a subset of target columns. Empirically, we find our evaluation metric is reliable in identifying solutions with alternative output structures, with a relatively low false-negative rate (Appendix J).</p>
<h2>4 PACHINCO: Adapting Code LMs to Computational Notebooks</h2>
<p>We introduce PACHINCO, an LM for notebooks.
Base LM PACHINCO is based on PALM, a family of decoder-only LMs for NL tasks (Chowdhery et al., 2022). Specifically, we use the 62B PALM model trained on 1.3 T tokens with a mixture of conversational, webpages and code data (Section F, Chowdhery et al. (2022)). Starting with this base LM, we first fine-tune on Python source code and then fine-tune further on Jupyter notebooks.
Fine-tuning on Python Code We first fine-tune the base LM on a corpus of near-deduplicated, permissively-licensed Python source code files from GitHub, with 64B tokens in total. We finetune PALM for 1 epoch following the hyper parameters setup in Chowdhery et al. (2022). This model is already a strong code LM, even outperforming the larger code LM PALM-Coder 540B on existing program synthesis benchmarks (Â§5.2).
Fine-tuning on Notebooks We then perform a second stage of fine-tuning on a large collection of 3.8 M Jupyter notebooks from GitHub ( 9.6 B tokens). Since our evaluation notebooks in the Existing Tasks split are also from GitHub, we also perform near-deduplication to remove any training notebooks with one cell similiar to any cells in the notebooks in Existing Tasks to prevent data contamination. We use nbconvert to linearize notebooks into Python code. Refer to Appendix D for details and Appendix K for a data card.</p>
<h2>5 Experiments</h2>
<p>Models We evaluate PACHINCO and state-of-the-art public code LLMs, namely CODEGEN (Nijkamp et al., 2022) and INCOder (Fried et al., 2022). We test both the monolingual (Python-only) and the multilingual version of CODEGEN. INCODER may be a more appealing comparison since it is trained on 5GB of Jupyter notebooks.
Inference and Metrics We convert each problem into a prompt (Â§5.1) and draw samples using nucleus sampling. Following Chen et al. (2021a), we report pass@ $\boldsymbol{k}$ metric, defined as the fraction</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example problem. Cells 1-2 $\left(c_{1}, c_{2}\right)$ are the notebook context, and Cell 3 ( $c_{3}$ ) contains the intent. Cells 3a and 3 b show two example completions of $c_{3}$.
of problems with at least one correct sample given a sample size $k$. To reduce variance, we estimate pass@ $k(k \leq 30)$ by drawing 50 samples for each problem (Chen et al., 2021a). Decoding temperature $t$ is 0.2 for $k=1$ and 0.8 for $k&gt;1$. Refer to Appendix E for inference details.</p>
<h3>5.1 LM Prompting Strategies</h3>
<p>We explore two prompting strategies: prompting using the notebook context of a problem (Â§5.2), and few-shot prompting with extra exemplars as a prompt prefix before the notebook context (Â§5.3) to impose more control on the predicted code's style.
Prompting with Notebook Contexts Fig. 3 depicts an example problem at $c_{3}$ for prompting, where the prompt is the notebook context (preceding cells $c_{1}$ and $c_{2}$ ) and the current intent. The context also includes NL descriptions of the imported DataFrame schema $\left(c_{2}\right)$, such as its columns and example cell values, crucial for grounded understanding of structured knowledge (Xie et al., 2022). Completion 3a shows an example prediction. For the following problems after $c_{3}$ (not shown), we use annotated reference solutions to previous turns in their contexts, reminiscent of multi-turn taskoriented dialogue evaluation (Andreas et al., 2020).
Using Extra Few-shot Exemplars Besides the basic setting, we also explore prompting using four</p>
<table>
<thead>
<tr>
<th style="text-align: center;">pass@ $k$</th>
<th style="text-align: center;">Existing Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">New Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Existing Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">INCODER 1B</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">INCODER 6B</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN $_{\text {multi }} 350 \mathrm{M}$</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN $_{\text {multi }} 2 \mathrm{~B}$</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN $_{\text {multi }} 6 \mathrm{~B}$</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN $_{\text {multi }} 16 \mathrm{~B}$</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN $_{\text {mono }} 350 \mathrm{M}$</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN $_{\text {mono }} 2 \mathrm{~B}$</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN $_{\text {mono }} 6 \mathrm{~B}$</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN $_{\text {mono }} 16 \mathrm{~B}$</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODE-cushman-001</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CODE-davinci-002</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Our Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Base PALM 62B</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ Python $f . t$.</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ PAChINCO</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- Schema Desc.</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: pass@ $k$ using notebook context as prompts.
additional NL-to-code exemplars as prompt prefix before the notebook context. As shown in Fig. 3 (Completion 3b), we focus on prompting LMs to generate code that follows a multi-line, step-bystep (SbS) decomposition structure, in contrast with the common practice of chaining multiple API calls in a single line (Completion 3a). Each step is also optionally inlined with NL explanations. Such step-wise explanations could help novice developers understand model predictions, and they have been found effective for reasoning (Wei et al., 2022; Gao et al., 2022) and program induction (Nye et al., 2021) tasks. Following Kojima et al. (2022), we also use a preamble to further elicit step-wise decomposition in predictions. See Appendix L for a complete list of example prompts.</p>
<h3>5.2 Main Results</h3>
<p>Tab. 2 reports pass@ $k$ on ARCADE using notebook contexts as prompts. PAChINCO achieves strong performance on both the Existing Tasks split and the New Tasks split due to its larger size and domain-specific fine-tuning.
Impact of Fine-tuning The base PALM model outperforms most public code LMs and is on par with CODEGEN $_{\text {mono }} 16 \mathrm{~B}$. Fine-tuning on Python (+Python f.t., Tab. 2) and notebooks data (+PAChINCO) further closes the domain gap with improved pass@k. The absolute gain after finetuning on Python code is higher than continued training on notebooks, likely because the semantic gap between NL data and Python code is larger than</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset <br> Metric</th>
<th style="text-align: center;">HumanEval <br> pass@100</th>
<th style="text-align: center;">MBPF <br> pass@80</th>
<th style="text-align: center;">TRANSCODER <br> pass@25</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PALM-CODER 540B</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">82.5</td>
</tr>
<tr>
<td style="text-align: left;">CODE-davinci-002</td>
<td style="text-align: center;">$92.1^{\wedge}$</td>
<td style="text-align: center;">$84.5^{\wedge}$</td>
<td style="text-align: center;">87.9</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 62B (Python f.r. $\$ 4$ )</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">86.4</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation of existing code LMs and PaLM 62B after the first-stage fine-tuning on Python code. ${ }^{7}$ Results from Chowdhery et al. (2022). ${ }^{m}$ Results from Chen et al. (2022)
that between general Python code and notebooks.
We note that the base PALM 62B model after fine-tuning on Python code corpora is already a strong code LM, performing competitively compared to other strong code LMs on established code generation (HUMANEval and MBPP) and translation (TRANSCODER) tasks (Tab. 3). With $7 \times$ more Python code tokens, our Python finetuned PaLM 62B model outperforms the $8 \times$ larger PALM-CODER 540B model on all the three tasks.
Comparing Existing Code LMs Among models with similar size and amount of Python training data (INCODER 6B vs. CODEGEN ${ }<em _mono="{mono" _text="\text">{\text {multi }} 6 \mathrm{~B}$ ), InCODER 6B performs better, likely because INCODER was trained on Jupyter notebooks. ${ }^{7}$ With $4 \times$ more Python data, CODEGEN ${ }</em>$ takes over. Appendix F further reports the scaling curve of CODEGEN on ARCADE, where pass@ $k$ scales as a power law with model size.}} 6 \mathrm{~B</p>
<p>For reference, we also report the results using the CODEX API. PACHINCO significantly outperforms the smaller cushman API, while davinci-002 is stronger. While we cannot gain much insight from the results due to limited knowledge about davinci-002, through error analysis, we find that davinci-002 is better at instruction following, especially in understanding NL descriptions of complex DataFrame schema (Â§5.1). Intuitively, compared to existing benchmarks, NL understanding on ARCADE is more challenging given its succinct and potentially ambiguous intents together with rich contexts. Therefore, the gap between CODEXdavinci-002 and our models could be larger on ARCADE compared to that on other datasets in Tab. 3. We leave improving the instruction following skills of PACHINCO as interesting future work.
Comparing Existing Tasks and New Tasks The pass@ $k$ scores on Existing Tasks are significantly higher than on New Tasks across all models. However, comparing the improvements after Python and notebook-specific fine-tuning of the base LM,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Models | pass@30 # API |  | Lines of <br> Code (LoC) | Comment <br> Lines | Tokens <br> / Line | API <br> / Line |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Baseline (Tab. 2) | 47.7 | 4.9 | 2.3 | 0.1 | 21.1 | 3.2 |
| + More Context | 49.3 | 4.9 | 2.3 | 0 | 21.1 | 3.1 |
| Prompting with Additional Few-shot Exemplars |  |  |  |  |  |  |
| Vanilla Code | 49.9 | 5.3 | 2.4 | 0.1 | 20.8 | 3.1 |
| Step-by-Step Code | 51.9 | 5.6 | 3.2 | 0.1 | 17.8 | 2.7 |
| + Preamble | 51.9 | 5.9 | 3.5 | 0.2 | 16.9 | 2.5 |
| + Pre. + Explanation | 52.5 | 6.8 | 4.2 | 3.3 | 14.9 | 2.2 |</p>
<p>Table 4: pass@30 and code style metrics for few-shot prompting on New Tasks. Results are averaged over three runs with different prompt prefixes.
the gain on New Tasks is higher. One reason is that the problems in Existing Tasks are overall simpler than in New Tasks (Â§3.2). Additionally, some code data similar to our evaluation notebooks in Existing Tasks could leak into the training data of those LMs. Despite our significant effort to deduplicate fine-tuning data against Existing Tasks (Â§4), the base LM might have seen similar code data on the Web, e.g., as data science tutorials. This highlights the importance of robust evaluation using held-out data, which is the purpose of the New Tasks split.</p>
<h2>Ambiguous Intents are Hard to Solve without</h2>
<p>Extra Specifications In $\S 3$ we discussed how intents in ARCADE can be ambiguous and underspecified (Â§3.2), and mitigating intent ambiguity using additional specifications to further clarify on the desired target output (Â§3.1). Indeed, those additional specifications are crucial for disambiguation. Without them the pass@30 of PACHINCO on the subset of 136 intents annotated with extra specifications on New Tasks dropped from $46.5 \%$ to $27.8 \%$ ( 43.8 on the full split v.s. 47.7 on Tab. 2), suggesting the importance of modeling prompt ambiguity for LLMs as future work.</p>
<h2>Grounded NL Understanding is Important</h2>
<p>Our prompts contain NL descriptions of imported DataFrames (Â§5.1), crucial for grounded understanding of NL intents (Â§3.2). Removing such schema descriptions significantly worsens the results, especially on New Tasks, as the last row in Tab. 2 (-Schema Desc.) shows. Encoding the states for more intermediate variables could likely further improve performance, which we leave as important future work.
Further Analysis We report more ablation experiments, such as pass@ $k$ w.r.t. problem complexity and the notebook context size in Appendix G.</p>
<h3>5.3 Few-shot Prompting Results</h3>
<p>Next, we investigate few-shot prompting to help a model better understand the task while controlling</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Cumulative distributions of the number of (A) unique API sequences and (B) output clusters, extracted from PACHINCO's 50 predictions on the New Tasks split. Curves that appear more to the right represent prompting methods with greater diversity in the samples. Step-by-step prompting leads to much greater diversity than the baselines. (C) Self-consistency decoding accuracy using 1 reranked sample on New Tasks.
the style of predictions. ${ }^{8}$ Tab. 4 summarizes the results on New Tasks. We start with the prompting strategy of predicting just the Step-by-Step Code (SbS) without preambles or explanations (i.e., only the code part of Completion 3b in Fig. 3), which improves over the baseline using only notebook contexts (c.f. Tab. 2). SbS prompting is especially effective for problems without adequate contexts, yielding $6 \%$ absolute improvements for the first two rounds of problems in a notebook as compared to the zero-shot baseline. More interestingly, even if we include more context in the baseline such that its prompt length matches SbS prompting (Baseline + More Context), SbS prompting still outperforms, again suggesting the complimentary value of extra exemplars.</p>
<p>Step-by-step Prompting Improves Code Style SbS prompting also changes the style of predicted code, which is decomposed into more lines ( $\mathrm{LoC}_{\uparrow}$, Tab. 4) where each line is simpler (Tokens/API per Line $\downarrow$ ). In contrast, if we instead prompt the model using exemplars with "vanilla"-styled code following the common practice of chaining multiple pandas API calls in a single line (Vanilla Code, e.g., Completion 3a in Fig. 3), we get less pass@k improvement over the baseline while the code style remains consistent.</p>
<p>Next, using preambles ( $+\mathbf{P r e a m b l e}$ ) to further encourage the model to produce step-by-step solutions improves the level of decomposition ( $\mathrm{LoC}_{\uparrow}$, Tokens/API per Line $\downarrow$ ) while maintaining pass@k. More surprisingly, with additional inline NL explanations for each step ( $+\mathbf{P r e . + E x p l a n a t i o n}$ ), PACHINCO produces even more decomposed solutions with slightly improved accuracy. As a result, those predictions have rich NL comments, with the number of comment lines nearly equal to the number of code lines. Interestingly, the predicted</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>solutions are also more complex, as indicated by the increased pandas API usage ( $# \mathrm{API}_{\uparrow}$ ). However, as we explain in Appendix H, on Existing Tasks, while prompting with NL explanations still alters the code style, pass@ $k$ is slightly worse. This is likely due to the fact that this split contains problems similar to the base LM's training data, and prompting the model to generate additional NL comments breaks its "flow" of generating code by memorization. Moreover, this split is also dominated by simpler tasks requiring fewer steps, while explanation-based prompting favors predicting more complex solutions with richer API usage and more code tokens (Tab. 4). Nevertheless, prompting with explanations yields more diverse predictions and could also help developers better understand the generated solutions, as we discuss next and also in $\S 6$.</p>
<h2>Step-by-Step Prompting Diversifies Solutions</h2>
<p>We also explore whether SbS prompting helps produce more diverse solution approaches. Intuitively, more output diversity could improve the odds of finding a solution at higher sample sizes. Determining whether two solutions are "different" is difficult and subjective, but we approximate this in two ways. First, we use the sequence of pandas API calls as a signature of the high-level solution pattern. Second, since two solutions might have the same functionality (executing to the same output), we also cluster predictions based on their outputs.</p>
<p>Figs. 4a and 4b plot the cumulative distributions of the number of unique solution patterns and output clusters on the New Tasks split. SbS prompting increases diversity on both metrics compared to the baselines. Notably, prompting with NL explanations yields even more solution patterns.</p>
<p>Diverse predictions could help handle underspecified intents (Â§3.2), since they might correspond to different interpretations of an ambiguous intent. Having diverse predictions also allows us to trans-</p>
<p>late better pass@ $k$ performance into better performance on a single suggestion using post-hoc reranking such as self-consistency decoding (Wang et al., 2022b), where we return the user one prediction from the largest output cluster instead of showing all $k$ predictions (Fig. 4c). SbS prompting significantly improves over baselines. Notably, the 1-sample accuracy of SbS with NL explanations outperforms pass@5 of the baseline in Tab. 2. Refer to Appendix I for further analysis.</p>
<p>As a side note, while SbS prompting leads to improved sample diversity, it may not directly improve code quality. If we consider functional correctness to approximate code quality, we observe that vanilla few-shot prompting and SbS variants have a similar fraction of correct samples ( $\sim 15 \%$ ). This suggests that for SbS prompting, it is higher sample diversity that may contribute to improved pass@ $k(k&gt;1)$ and reranking accuracy instead of other potential factors.</p>
<h2>6 Case Study: How Useful is Predicted Code with Step-wise Explanations?</h2>
<p>Finally, we remark that besides improving solution diversity, step-by-step prompting with NL explanations could also potentially help novice data scientists understand model-generated solutions, as shown in the following qualitative case study.</p>
<p>First, NL explanations could help users follow the flow of complex data transformations for programs involving a chain of pandas operations. By decomposing and explaining how data is manipulated after individual transformation steps, it is easier for users to understand the solution and track its dataflow behind the scene, especially when some steps involve complex computation (Fig. 17), or the underlying schema is less intelligible (e.g., column names with abbreviations, Fig. 18). Additionally, some inline explanations also describe the output of intermediate steps, which is particularly helpful when these steps involve advanced pandas functions whose output structure may not be obvious, such as pd.unstack (Fig. 19)</p>
<p>Meanwhile, step-wise NL explanations serve as high-level procedural descriptions of code, which enable users to easily browse through and understand different solution approaches without being distracted by nuances in the actual code implementation (Fig. 20). Moreover, explanations also help users verify the code solutions by identifying potentially incorrect steps (Fig. 21). The observations presented here offer insight into potential future
avenues to improve the utility of code LMs for developers through the use of step-by-step explanations, which we leave as important future work.</p>
<h2>7 Related Work</h2>
<p>Automating Data Science The amount of expertise required in data science has called for development of systems to automate its lifecycle (Wang et al., 2021b). Much work has focused on automating feature engineering and tuning of ML models (AutoML, He et al., 2021; Karmaker et al., 2020), with well-established systems (Feurer et al., 2015) and benchmarks (ZÃ¶ller and Huber, 2021). This paper focuses on automating tabular data wrangling and EDA tasks, which account for nearly the same amount of code and documentations in notebooks as that for ML-related tasks (Agashe et al., 2019; Wang et al., 2022a). Along this line, existing research synthesizes data wrangling programs using I/O examples (Bavishi et al., 2019; Shi et al., 2020) or partial table contents (Chen et al., 2021c), followed by recent efforts using LLMs with additional NL specifications (Jain et al., 2021; Bavishi, 2022). This paper considers code generation in notebooks with multiple contextually dependent problems (see $\S 3.2$ for recent work). In addition, other works have also considered applications such as synthesizing visualization plots (Amar et al., 2005; Wang et al., 2019; Narechania et al., 2020; Fu et al., 2020; Wu et al., 2022b).
Context-driven Code Generation Our work is another application of context-driven code generation, which maps a series of contextually dependent utterances to programs, such as domain-specific logical forms (Zettlemoyer and Collins, 2009; Long et al., 2016; Iyyer et al., 2017; Andreas et al., 2020), SQL queries over databases (Hemphill et al., 1990; Suhr et al., 2018; Yu et al., 2019a,b), or generalpurpose PLs (Nijkamp et al., 2022). ARCADE further offers contextually dependent utterances exhibiting non-trivial dependencies (Â§2), with target programs defined in a general-purpose PL.</p>
<h2>8 Conclusion</h2>
<p>In this paper we present ARCADE, a code generation benchmark for data wrangling and EDA tasks in computational notebooks, featuring problems with realistic NL intents and rich contexts. We also develop PAChINCO, a 62B LM tailored for data science. PAChINCO outperforms public LMs on ARCADE, while being effective in few-shot learning to improve code style and solution diversity.</p>
<h2>9 Limitations</h2>
<p>We discuss limitations of our work that hopefully could inspire future research in this avenue.
Task Coverage in ArCADE Arcade consists of realistic data wrangling and EDA tasks for a variety of ML datasets. In particular, we focus on problems that can be solved using pandas because of its popularity in data science - $90 \%$ of Kaggle notebooks use pandas. Still, our annotated problems may not cover all the types of tasks in these two categories. As an example, data visualization is an important part of EDA. Our dataset also includes 59 natural language to plotting problems, which are not used in this paper due to challenges in automated evaluation (Chen et al., 2021b). Future work might consider evaluation of plotting tasks using unit tests (Lai et al., 2022). Additionally, some of the existing datasets in Tab. 1 usually contain broader types of problems other than the wrangling and EDA tasks considered in this paper (e.g., fitting ML models, Â§7). We leave expanding the task spectrum as important future work.
Session-level Evaluation ARCADE features multiple contextually dependent problems in computational notebooks. As the first step towards evaluating code LMs in this interactive program synthesis paradigm, we report turn-level accuracy, and generate notebook context for prompting using ground-truth solutions for the prior turns of a problem (Â§5.1), following the common evaluation protocol in task-oriented dialogue (Hosseini-Asl et al., 2020; Andreas et al., 2020). Future work could consider a more realistic scenario of session-level evaluation where history contexts consist of model-predicted code instead of the reference (Yang et al., 2020; Nijkamp et al., 2022). However, this evaluation setting is still not ideal without modeling the user (e.g., asking follow-up questions to correct a model's predictions in a turn before proceeding to the next round, see Austin et al., 2021), which often requires building specialized simulators (Cheng et al., 2022a).
Reliance on Large Language Models Our experiments are based on public and in-house large code LMs (PACHINCO), which require adequate computational resources ${ }^{9}$ and create carbon emissions (Patterson et al., 2021). Their predictions could also be subject to known issues such as misalignment with user intents; for a discussion</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>of these and other risks of code language models, see Chen et al. (2021a, Appendices E-H) and Chowdhery et al. (2022, Section 6.4). To reduce the amount of computational resources required, our initial prompting experiments (Â§5.2) and error analysis (Appendix J) suggest that leveraging program execution information (e.g., schema descriptions) could be a promising direction to improve sample efficiency and reduce the size of code LMs (Nye et al., 2021), while explicit modeling of code-intent correspondence (Zhang et al., 2022) could be a viable path to mitigate alignment issues in model predictions. In addition, as generative AI coding tools are becoming more available to developers, more efforts are required to understand the potential limitations of those systems and the risks they may pose, such as producing insecure code and over-reliance on model predictions (Chen et al., 2021a). We leave addressing those issues as important future work.</p>
<h2>Acknowledgements</h2>
<p>We are grateful to Meg Risdal and Goeff Thomas from Kaggle for help with dataset collection, and Miltos Allamanis for research discussion. We thank Jo Chick from the research partnership team, and Rebecca Watson, Ashley Dawe and Kimberly Herrera from Upwork to help with managing the annotation project. We thank Aroma Mahendru for writing the data card section. We also thank Cheriskumar Patel, Preet Patel, and Jayendra Parmar for general assistance with the project. We thank anonymous reviewers for their insightful comments.</p>
<h2>References</h2>
<p>Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. 2019. JuICe: A large scale distantly supervised dataset for open domain context-based code generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5436-5446, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Charu Aggarwal, Djallel Bouneffouf, Horst Samulowitz, Beat Buesser, Thanh Hoang, Udayan Khurana, Sijia Liu, Tejaswini Pedapati, Parikshit Ram, Ambrish Rawat, et al. 2019. How can AI automate end-to-end data science? arXiv preprint arXiv:1910.14436.</p>
<p>Robert A. Amar, James R. Eagan, and John T. Stasko. 2005. Low-level components of analytic activity</p>
<p>in information visualization. IEEE Symposium on Information Visualization, 2005. INFOVIS 2005., pages 111-117.</p>
<p>Jacob Andreas, Johannes Bufe, David Burkett, Charles C. Chen, Joshua Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Leo Wright Hall, Kristin Delia Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, C. H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Ann Short, Div Slomin, B Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, A. A. Vorobev, Izabela Witoszko, Jason Wolfe, A. G. Wray, Yuchen Zhang, and Alexander Zotov. 2020. Task-oriented dialogue as dataflow synthesis. Transactions of the Association for Computational Linguistics, 8:556-571.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.</p>
<p>Shraddha Barke, Michael B James, and Nadia Polikarpova. 2022. Grounded copilot: How programmers interact with code-generating models. arXiv preprint arXiv:2206.15000.</p>
<p>Rohan Bavishi. 2022. Tools and Techniques for Building Programming Assistants for Data Analysis. Ph.D. thesis, EECS Department, University of California, Berkeley.</p>
<p>Rohan Bavishi, Caroline Lemieux, Roy Fox, Koushik Sen, and Ion Stoica. 2019. Autopandas: neuralbacked generators for program synthesis. Proceedings of the ACM on Programming Languages, 3:1 27.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Shubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and evaluating a Jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901.</p>
<p>Bei Chen, Fengji Zhang, A. Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet: Code generation with generated tests. ArXiv, abs/2207.10397.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,</p>
<p>Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating large language models trained on code. ArXiv, abs/2107.03374.</p>
<p>Xinyun Chen, Linyuan Gong, Alvin Cheung, and Dawn Xiaodong Song. 2021b. Plotcoder: Hierarchical decoding for synthesizing visualization code in programmatic context. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou. 2021c. Spreadsheetcoder: Formula prediction from semi-structured context. In International Conference on Machine Learning.</p>
<p>Qinyu Cheng, Linyang Li, Guofeng Quan, Feng Gao, Xiaofeng Mou, and Xipeng Qiu. 2022a. Is multiwoz a solved task? an interactive tod evaluation framework with user simulator. ArXiv, abs/2210.14529.</p>
<p>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. 2022b. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>David Donoho. 2017. 50 years of data science. Journal of Computational and Graphical Statistics, 26(4):745-766.</p>
<p>Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank Hutter. 2015. Efficient and robust automated machine learning. Advances in neural information processing systems, 28.</p>
<p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wentau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999.</p>
<p>Siwei Fu, Kai Xiong, Xiaodong Ge, Yingcai Wu, Siliang Tang, and Wei Chen. 2020. Quda: Natural language queries for visual data analytics. ArXiv, abs/2005.03257.</p>
<p>Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, and Pengsheng Huang. 2021. Towards robustness of text-toSQL models against synonym substitution. pages 2505-2515, Online. Association for Computational Linguistics.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. PAL: Program-aided language models. ArXiv, abs/2211.10435.</p>
<p>Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: a survey of the state-of-the-art. KnowledgeBased Systems, 212:106622.</p>
<p>Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS spoken language systems pilot corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990.</p>
<p>Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem. 2021. Natural language-guided programming. Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software.</p>
<p>Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. arXiv preprint arXiv:2005.00796.</p>
<p>Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan Duan, and Jianfeng Gao. 2022. Execution-based evaluation for data science code generation models. arXiv preprint arXiv:2211.09374.</p>
<p>Mohit Iyyer, Wen tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large language models meet program synthesis. In Proceedings of the 44th International Conference on Software Engineering, pages 1219-1231.</p>
<p>Naman Jain, Skanda Vaidyanath, Arun Shankar Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram K. Rajamani, and Rahul Sharma. 2021. Jigsaw: Large language models meet program synthesis. 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE), pages 1219-1231.</p>
<p>Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2011. Wrangler: interactive visual specification of data transformation scripts. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '11, pages 33633372, New York, NY, USA. Association for Computing Machinery.</p>
<p>Shubhra (Santu) Karmaker, Md. Mahadi Hassan, Micah J. Smith, Lei Xu, ChengXiang Zhai, and Kalyan Veeramachaneni. 2020. Automl to date and beyond: Challenges and opportunities. ACM Computing Surveys (CSUR), 54:1-36.</p>
<p>Thomas Kluyver, Benjamin Ragan-Kelley, Fernando PÃ©rez, Brian Granger, Matthias Bussonnier, Jonathan Frederic, Kyle Kelley, Jessica Hamrick, Jason Grout, Sylvain Corlay, Paul Ivanov, DamiÃ¡n Avila, Safia Abdalla, Carol Willing, and Jupyter development team. 2016. Jupyter notebooks - a publishing format for reproducible computational workflows. In Positioning and Power in Academic Publishing: Players, Agents and Agendas, pages 87-90, Netherlands. IOS Press.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916.</p>
<p>Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022. Ds1000: A natural and reliable benchmark for data science code generation. ArXiv, abs/2211.11501.</p>
<p>Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. 2021. Kaggledbqa: Realistic evaluation of text-to-sql parsers. In $A C L$.</p>
<p>Zi Lin, Jeremiah Liu, and Jingbo Shang. 2022. Neuralsymbolic inference for robust autoregressive graph parsing via compositional uncertainty quantification. In Proceedings of EMNLP.</p>
<p>Reginald Long, Panupong Pasupat, and Percy Liang. 2016. Simpler context-dependent logical forms via model projections. ArXiv, abs/1606.05378.</p>
<p>Arpit Narechania, Arjun Srinivasan, and John T. Stasko. 2020. Nl4dv: A toolkit for generating analytic specifications for data visualization from natural language queries. IEEE Transactions on Visualization and Computer Graphics, 27:369-379.</p>
<p>Alfredo Nazabal, Christopher K I Williams, Giovanni Colavizza, Camila Rangel Smith, and Angus Williams. 2020. Data engineering for data analytics: A classification of the issues, and case studies. $a r X i v$.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. A conversational paradigm for program synthesis. arXiv preprint arXiv:2203.13474.</p>
<p>Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114.</p>
<p>Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, LluÃ­s-Miquel MunguÃ­a, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. ArXiv, abs/2104.10350.</p>
<p>Jeffrey Perkel. 2021. Reactive, reproducible, collaborative: computational notebooks evolve. Nature, 593.</p>
<p>JoÃ£o Felipe Pimentel, Leonardo Gresta Paulino Murta, Vanessa Braganholo, and Juliana Freire. 2019. A large-scale study about quality and reproducibility of Jupyter notebooks. 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR), pages 507-517.</p>
<p>Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. 2022. Evaluating the text-to-sql capabilities of large language models. arXiv preprint arXiv:2204.00498.</p>
<p>Kensen Shi, David Bieber, and Rishabh Singh. 2020. Tf-coder: Program synthesis for tensor manipulations. ACM Transactions on Programming Languages and Systems (TOPLAS), 44:1 - 36.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. ArXiv, abs/2104.09864.</p>
<p>Alane Suhr, Srini Iyer, and Yoav Artzi. 2018. Learning to map context-dependent sentences to executable formal queries. ArXiv, abs/1804.06868.</p>
<p>April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin D Weisz, Xuye Liu, Lingfei Wu, and Casey Dugan. 2022a. Documentation matters: Human-centered ai system to assist data science code documentation in computational notebooks. ACM Transactions on Computer-Human Interaction, 29(2):1-33.</p>
<p>Chenglong Wang, Yu Feng, Rastislav Bodik, Alvin Cheung, and Isil Dillig. 2019. Visualization by example. Proceedings of the ACM on Programming Languages, 4(POPL):1-28.</p>
<p>Dakuo Wang, Josh Andres, Justin D Weisz, Erick Oduor, and Casey Dugan. 2021a. Autods: Towards human-centered automation of data science. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-12.</p>
<p>Dakuo Wang, Q Vera Liao, Yunfeng Zhang, Udayan Khurana, Horst Samulowitz, Soya Park, Michael Muller, and Lisa Amini. 2021b. How much automation does a data scientist want? arXiv preprint arXiv:2101.03970.</p>
<p>Tian Wang and Kyunghyun Cho. 2016. Larger-context language modelling with recurrent neural network. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1319-1329, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. 2022a. Memorizing transformers. arXiv preprint arXiv:2203.08913.</p>
<p>Zhengkai Wu, Vu Le, Ashish Tiwari, Sumit Gulwani, Arjun Radhakrishna, Ivan Radicek, Gustavo Soares, Xinyu Wang, Zhenwen Li, and Tao Xie. 2022b. NL2Viz: Natural language to visualization via constrained syntax-guided synthesis. Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering.</p>
<p>Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir R. Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. ArXiv, abs/2201.05966.</p>
<p>Yunyi Yang, Yunhao Li, and Xiaojun Quan. 2020. Ubar: Towards fully end-to-end task-oriented dialog systems with gpt-2. In AAAI Conference on Artificial Intelligence.</p>
<p>Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander R. Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S. Lasecki, and Dragomir R. Radev. 2019a. Cosql: A conversational text-to-sql challenge towards crossdomain natural language interfaces to databases. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, He Yang Er, Irene Z Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir R. Radev. 2019b. Sparc: Cross-domain semantic parsing in context. ArXiv, abs/1906.02285.</p>
<p>Luke Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen tau Yih, Daniel Fried, and Sida I. Wang. 2022. Coder reviewer reranking for code generation. ArXiv, abs/2211.16490.</p>
<p>Marc-AndrÃ© ZÃ¶ller and Marco F Huber. 2021. Benchmark and survey of automated machine learning frameworks. Journal of artificial intelligence research, 70:409-472.</p>
<h1>Supplementary Materials</h1>
<h2>A Details of Dataset Construction</h2>
<p>In this section we elaborate on the process of building ARCADE.</p>
<h2>A. 1 Mining Examples from Existing Notebooks</h2>
<p>To build the Existing Tasks split with annotated NL-to-code problems from publicly-available notebooks, we first identify candidate code cells performing data wrangling and EDA tasks from existing high-quality data science notebooks, and then manually annotate these cells with NL intents.
Collecting Notebooks for Annotation To form a pool of candidate notebooks, we use JuICe (Agashe et al., 2019), a collection of Jupyter notebooks from GitHub, together with additional notebooks from BIGQUERY ${ }^{10}$, yielding over 1.5 M notebooks in total. These notebooks are first filtered and neardeduplicated, similar to $\mathrm{PACHINCO}$ 's training data preprocessing step in Appendix D. We then identify candidate code cells from the remaining notebooks for annotation. Specifically, we select code cells that are either (1) contain pandas programs with at least three API calls, or (2) preceded by a Markdown cell with a short question as its content (e.g., What are the top 10 producers?). The first heuristic is useful to identify complex wrangling tasks, while the second one is particularly effective in finding interesting dataset-specific EDA tasks, and the existing Markdown texts also provide reference for labeling intents later. Next, we group the notebooks with at least one candidate cell based on their underlying ML datasets (e.g., imported using pd.read_csv()), and then select the top 5 notebooks with the greatest number of candidate cells from a curated set of 36 dataset groups for annotation. This set contains ML datasets from a variety of domains and schema. We favor notebooks with more candidate cells so that we could extract multiple NL-to-code problems within the same notebook.
Annotation We hired a group of data scientists to annotate the notebooks selected above, following the process outlined in $\S 3.1$. Annotation primarily consists of judging the quality of candidate code cells, fixing any errors, and creating NL intents summarizing the code. Throughout the annotation process, we find that re-purposing notebooks in the wild to build our benchmark is not an easy task. As an example, many notebooks in JuICe are data science tutorials, which often contains documentation that includes background knowledge, reference materials, and even solution hints. Those extra information makes the code generation task easier, and may not reflect the style of ordinary notebooks authored by data scientists in their day-to-day work. We therefore ask the annotators to clean the notebook and remove such extra information whenever possible.</p>
<h2>A. 2 Creating Notebooks with Examples from Scratch</h2>
<p>The problems derived from high-quality GitHub notebooks could capture realistic tasks and notebook contexts, but may result in artificially high evaluation accuracies due to potential leakage of evaluation notebooks to the training data of LLMs, which is a common issue in LLM evaluation (Brown et al., 2020). To defend against this data contamination, we additionally annotated 660 problems by creating notebooks from scratch.
Sourcing Novel ML Datasets To ensure that those newly-created examples can be used to evaluate the generalization ability of code LMs on unseen ML datasets, we create notebooks targeting data wrangling and EDA tasks for 70 tabular ML datasets that have been recently uploaded to the Kaggle data science platform since February 2022. Those short-listed datasets are manually selected from a pool of 600 datasets with reasonably complex schema (e.g., having columns with diverse data type), and are verified by our annotators that no older-versioned datasets with similar schema appeared before.
Creating Notebooks For each ML dataset, the annotators were asked to create one notebook with a series of wrangling and EDA tasks annotated with NL intents. Specifically, we ask annotators to come up with tasks that they would like to perform in order to gain insights into these recently appeared ML datasets in order to build models for them. We follow the same standard to create intents as in creating</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Existing Tasks. To make the problems more challenging, annotators are encouraged to create harder tasks whose code solutions require at least 5 pandas API calls.</p>
<h1>A. 3 Annotation Process and Quality Assurance</h1>
<p>Eight freelancers proficient in English and reported skill in pandas are hired from Upwork, with an average of 3 years of experience. All the annotators went through a qualification round with data science interview questions. In building the Existing Tasks split, each freelancer first performed a trial batch by annotating a single notebook, and received detailed comments from the first author, before proceeding with annotating the rest of assigned notebooks. Each annotated sample is reviewed by the first author. Annotators spent $3 \sim 4$ minutes to create each problem on average. To create the more challenging New Tasks split from scratch, we only invite the top-3 performers for this task since it is harder than labeling existing notebooks. Each created notebook is first peer reviewed by another annotator, before a final round of review by the first author. Since the annotators have already worked on the prior task of creating examples in existing notebooks, they are fairly familiar with the requirement, and are able to create each problem in 13 minutes on average. To further improve quality, we also did another round of manual review for the set of problems in the two splits that a strong code LLM fails to predict the annotated solution (based on fuzzy output matching) within a budget of 50 samples. ${ }^{11}$</p>
<h2>B Outline of ARCADE Annotation Guideline</h2>
<p>In this section we provide a brief outline of our annotation guideline.
Existing Tasks The annotators are given a list of Jupyter notebooks. Each notebook uses pandas to perform certain data analysis tasks. For each notebook, an annotator is asked to:</p>
<ol>
<li>Identify code cells that contain instructive code snippets that perform data wrangling or exploratory data analysis tasks.</li>
<li>Fix the notebook and make them clean and executable.</li>
<li>For each code snippet identified in Step 1, create natural language descriptions of the task. Also verify the code solution and fix them as appropriate. Finally, remove any redundant text in the notebook (e.g., solution outline or hints for tutorial notebooks) that could give away to the refernce solution.</li>
</ol>
<p>Instruction on Creating Natural Intents Specifically, for step 3, in order to collect realistic NL intents, the annotators are given the following high-level description, followed by detailed instructions and examples.</p>
<p>Below we share some suggestions to write good intents.
Keep it natural without redundant explanations. Imagine an AI programmer can help you accomplish simple data wrangling and EDA tasks, what kind of intents will you send to the system? Our goal is to collect real inputs to such a system from data scientists like you.</p>
<p>One idea to write good intents is to keep it concise such that another programmer could quickly understand and implement a solution that executes to the same outputs. You are encouraged to create simple, short intents while describing the desired outputs without much ambiguity.</p>
<p>New Tasks For each ML dataset we provided, an annotator creates a Colab notebook with code snippets for some interesting data wrangling and exploratory data analysis tasks using this dataset. Each code snippet is paired with its natural language intent, simliar to the process of annotating Existing Tasks. We ask annotators to feel free to work on any tasks that they may find interesting for the given dataset, as long</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>as the code solution for the task should consist of multiple lines and use different pandas API functions. Different from annotating Existing Tasks, we ask them to first create a natural language intent for their task, and then write a code solution in the next cell.</p>
<p>Below is an excerpt from the annotation guideline describing the types of data wranling and EDA tasks to create.</p>
<h1>What Tasks to Create</h1>
<p>In general, you may create whatever exploratory data analysis tasks that you find interesting for the given datasets. To come up with interesting tasks, you can think in this way: before training your ML models for the dataset, what kind of data wrangling or EDA tasks would you like to perform on the dataset? Below are some more concrete descriptions of such wrangling or EDA tasks:</p>
<p>Data Preprocessing/Wrangling Tasks which involves modifying existing dataframes or creating new ones. Such as normalizing column names, adding new columns, modifying existing columns (e.g., converting string values to date times), generating new dataframes using ops like group_by, and so on. Some datasets we shared are just raw data without any preprocessing or cleaning. Feel free to . Please also refer to Section: Identify Code Snippets to Annotate in our previous annotation guideline for more examples.</p>
<p>Exploratory Data Analysis Tasks that Require Some Wrangling and Preprocessing Answering interesting EDA questions using the given dataset, but some data wrangling steps are required in order to derive the answer. For example, given a dataframe df of user shopping history and credit card expiration dates in the format of df .loc [0] ['cc_exp'] = '08/26'. To answer the EDA question "How many users have a credit card expiring in 2024?", we need to first convert the expiration year from the string-formatted cc_exp column.</p>
<p>To encourage the annotators to create more complex tasks, we also provide the following high-level instruction:</p>
<h2>Complexity of Tasks</h2>
<p>You should create relatively complex tasks that require multiple steps and also a combination of different pandas APIs to solve them. Avoid problems that can be solved using one-liner code such as df.group_by(...).sort_values(...). An ideal task should be reasonably complex and needs to be broken down into multiple smaller steps to solve, and each step may require using one or multiple pandas functions.</p>
<p>As a general rule of thumb, you should aim at creating tasks that either have at least 50 tokens or use at least 4 pandas APIs (dataframe/series indexing, like df [df ['continent'] == 'NA'] is also counted as one API usage). You can find more concrete example tasks at the end of this doc.</p>
<p>Full Guideline Our annotation guideline is 35 -pages long in total, which we will provide on a perrequest basis. Please contact pcyin@google.com to request access.</p>
<h2>C Descriptions of Existing Data Science Code Generation Dataset</h2>
<p>Here, we describe existing natural language to code generation datasets in data science domain listed in Tab. 1 in more detail.
JuICe (Agashe et al., 2019) contains exercise problems in assignment notebooks from data science tutorials or coures, where the NL intents are usually elaborative assignment problem definitions. Notebooks in JuICe are not executable so evaluation is performed by surface-level matching (exact match or Bleu) between reference and predicted programs.</p>
<p>DSP (Chandel et al., 2022) contains problems from a filtered set of JuICe notebooks that are executable and also associated with unit tests for auto-grading. Hence the intents in DSP follow similar patterns as those in JuICe. To ensure that the free-form model-predicted code is compatible with unit tests, DSP uses the unit test code itself as extra model input besides NL intents to constrain the model to generate code that could be directly consumed by the tests.
ExeDS (Huang et al., 2022) is a concurrent work to this paper. It is another set of filtered problems from JuICe. Similar to this work, ExeDS uses hand-annotated intents, and compares the execution output between reference and predicted code for evaluation instead of relying on unit tests (Â§3.3).
NLGP (Heyman et al., 2021) is another collection of the NL-to-code problems in Jupyter notebooks with short annotated intents for simple data manipulation tasks, where most notebooks have one associated problem.
DS-1000 (Lai et al., 2022) is a collection of data science problems derived from StackOverflow questions. It primarily features problems using synthetic contexts with minimal working examples, and therefore does not concerns with code generation in notebooks with interrelated problems on general ML datasets.</p>
<h1>D Details of Fine-tuning PaChiNCo</h1>
<p>Pre-processing Python Source Code Data We detail the preprocessing steps for the Python source code corpus used in the first stage of fine-tuning in the data card (Appendix K).
Pre-processing Notebooks Data We apply additional domain-specific pre-processing steps for the Jupyter notebooks corpus, such as filtering out notebooks without any Markdown cells, or with fewer than 4 code cells. In addition, to mitigate the risk of having notebooks similar to the evaluation notebooks from GitHub in the Existing Tasks split leaked into the training data, we perform near de-duplication against notebooks in Existing Tasks at the cell level. Specifically, we cluster the cells of notebooks in both the evaluation and training sets based on a fuzzy-matching similarity metric, and remove any training notebooks that has one cell that falls into the same cluster as a cell from one of the evaluation notebooks. This process eliminates $\sim 350 \mathrm{~K}$ notebooks from the fine-tuning data. Our final training set consist of $\sim 3.8 \mathrm{M}$ notebooks and $\sim 9.6 \mathrm{~B}$ tokens in total.
Linearize Notebooks to Python Source Code We convert computational notebooks for finetuning (Â§4) and evaluation (Â§5.1) to Python source code using nbconvert. ${ }^{12}$ Specifically, Markdown and code cells in a notebook are concatenated using the special delimiter ' $#$ In[]:', and text in Markdown cells is commented out using the ' $#$ ' prefix. See Listing 7 for an example of the linearized notebook for Fig. 1 (up to $c_{3}$ ). Jupyter notebooks that are converted to Python files in such format are common in GitHub repositories, which mitigates the domain transfer gap between general Python code and notebook-specific data, and also allows us to prompt public code LLMs that have not been specifically trained on Jupyter notebooks data.
Fine-tuning Hyper-parameters For the two-stage fine-tuning (Â§4), we use the similar training recipe of the base LM. Specifically, we apply the learning rate decay scheduling $0.2 / \sqrt{t}$, where $t$ is the number of steps. At the first stage of fine-tuning on Python source data, we train the model for 124 K steps ( 1 epoch) with a batch size of 256. Afterwards, we reload the optimizer state and continue training on the Jupyter notebooks data ( 9.6 B tokens) using the same hyper parameter for 3 epochs ( $\sim 572 \mathrm{~K}$ steps). The model is implemented in JAX ${ }^{13}$, and is fine-tuned on 512 TPU v4 chips.</p>
<h2>E Inference Setup</h2>
<p>For CODEGEN, we use the inference script from the official GitHub repository. ${ }^{14}$ For INCODER, we follow the official inference example script and use the release on Huggingface model hub. ${ }^{15}$ We convert each example in our dataset to Python source code to a prompt, as outlined in $\S 5.1$. Notebooks are linearized using nbconvert similar as generating fine-tuning data (Appendix D). One exception is INCODER, for</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>which we follow Fried et al. (2022) and use the Jupyter notebook linearization template used in its pre-training.</p>
<p>At inference time, by default we left-truncate notebook context up to 900 tokens (measured by PACHINCO's vocabulary), which fit in the context window size of all LLMs we evaluated. We also make sure to always include NL schema descriptions in prompts given their importance in understanding NL intents. In addition, for few-shot experiments in $\S 5.3$, we use additional 1,200 tokens to accommodate the prompt prefix, making the total maximal prompt length to be 2,100. Due to its excessive length, we only perform few-shot prompting experiments on PACHINCO since its rotatory positional embedding (Su et al., 2021) could generalize to encode longer contexts at inference time. We use nucleus sampling with a top probability of 0.95 and a temperature of 0.8 to draw 50 samples for each problem. For pass@1 evaluation, we use a temperate of 0.2 , which gives very similar results compared to greedy decoding for all the models considered in Tab. 2. Due to rate limit in open AI API, we therefore use greedy decoding for pass@1 evaluation for CODE-cushman-001 and CODE-davinci-002. We set the maximum target length to be 512 tokens.</p>
<h1>F CodeGen Scaling Curve on ARCADE</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Scaling curve of $\operatorname{CODEGen}_{\text {mono }}$ models on ARCADE and existing code generation benchmarks. Results on HumanEval are selected from the best temperature $t \in{0.2,0.6,0.8}$ (Nijkamp et al., 2022).</p>
<p>Fig. 5 depicts the scaling curve of ARCADE with respect to the number of parameters for $\operatorname{CODEGen}_{\text {mono }}$ models. The pass rate scales nearly log-linearly as a function of model size, and the performance has not saturated, especially on the New Tasks split. This shows ARCADE is a reliable dataset to study the scaling behavior of code LLMs. The slope of the curve on New Tasks is also smaller than on other datasets, suggesting that this problem set is more challenging for CODEGEN models. It is also interesting to extrapolate CODEGEN models to $62 B$ according to the scaling curve and compare with our models at similar size. This gives a projected pass@10 of $22 \%$ on New Tasks, wihch is lower than PALM after the first-stage Python fine-tuning ( $28 \%$ ).</p>
<h2>G Break-down Analysis of pass@ $k$ on ARCADE</h2>
<p>Accuracy with Problem Complexity To better understand PACHINCO's performance on problems at different levels of complexity, we plot pass@30 with respect to the number of pandas function calls in the annotated reference solutions, as shown in Fig. 6. For problems with similar complexity, PACHINCO generally achieves higher pass rate on Existing Tasks, again suggesting that the New Tasks split is still more challenging even after controlling problem complexity.</p>
<p>Fig. 7 plots pass@30 with respect to the AST size of reference programs. Similar to Fig. 6, results on New Tasks are generally lower. Meanwhile, it seems that AST size correlates better with pass@ $k$</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: pass@ $k$ of $\mathrm{PAChINCO}$ w.r.t problem complexity
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: pass@ $k$ of $\mathrm{PAChINCO}$ w.r.t AST size of reference programs.
compared to the number of API usage, while the latter metric offers more intuitive information about the data transformation steps involved.
How Much Notebook Context is Useful? ArCADE requires a model to leverage rich programmatic and NL context in test notebooks to generate code solutions for the current cell. To study PACHINCO's performance with varying amount of available notebook context, we control the number $d$ of context cells $\left{c_{i}\right}<em n="n">{i=n-d}^{n-1}(\S 2)$ when generating code for each problem (at cell $c</em>}$ ) in our dataset. Fig. 8 depicts pass@30 as a function of the context size $d$. Since we use the first preceding cell $c_{n-1}$ to store the NL intent $\boldsymbol{u<em n="n">{n}$ for $c</em>}$ (Appendix L), having only one context cell is equivalent to the "cold-start" setting of only using the intent $\boldsymbol{u<em n="n">{n}$ (besides schema description) to predict $c</em>$ Empirically, we observe that using more context helps to reduce schema understanding errors (e.g., using undefined columns in DataFrames). Fig. 9 illustrates the distribution of execution error types on failed predictions. Notably, using more notebook context cells significantly reduces the chance of NameErrors caused by using undefined variables in context. The number of KeyErrors is also reduced, indicating that the model makes fewer schema understanding errors when referring to columns in DataFrames.
Does Problem Location Impact Performance? Another interesting angle to study the effect of context is through the lens of model accuracy when solving problems $c_{n}$ at different locations. Intuitively, problems located later in a notebook ( $n$ is larger) would have more context available, therefore they could be easier to answer (Wang and Cho, 2016). Fig. 10 shows pass@30 on problems grouped by their preceding context size, which shows increased task success rate when solving problems with more context, confirming the prior intuition. ${ }^{17}$}$. PACHINCO achieves a pass rate of $44 \%$ (existing tasks) and $17 \%$ (new tasks) in this challenging setting ( $d=1$ ), with errors mostly due to failure in referring to variables that the solution relies on, whose information is not present in the short context. Indeed, including additional context cells is crucial for good performance. In particular, having 3 context cells could already lift the pass@30 to $72 \%$ and $36 \%$ on the two splits $-1.6 \sim 2 \times$ higher than $d=1$. The results also start to plateau after including $5 \sim 7$ context cells, with diminishing returns after including more cells, which is in line with findings in Agashe et al. (2019). ${ }^{16</p>
<p><sup id="fnref11:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: pass@30 as w.r.t the number of context cells from the notebook.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Frequency of runtime errors from $\mathrm{PAChINCO}$ 's predictions aggreated over the two splits with varying amount of notebook context cells.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Examples grouped by num. context cells
Figure 10: Pass rate on examples grouped by context size. We also report the number of examples in each group.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ Prior work suggests that the plateau point is around three neighboring cells, while in our case, the number if approximately doubled since we need extra cells to include intents in previous turns (Appendix L).
${ }^{17}$ We remark that our setup is different from multi-turn semantic parsing where later turns are conditioned on the predictions of prior turns, while we use reference solutions (Â§5.1). See $\S 7$ for discussion.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
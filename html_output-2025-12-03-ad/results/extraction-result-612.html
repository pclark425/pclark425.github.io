<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-612 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-612</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-612</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-285bff5a564cb7edfb355a2ffb44994386975f1d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/285bff5a564cb7edfb355a2ffb44994386975f1d" target="_blank">On Model Stability as a Function of Random Seed</a></p>
                <p><strong>Paper Venue:</strong> Conference on Computational Natural Language Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper performs a controlled study on the effect of random seeds on the behaviour of attention, gradient-based and surrogate model based (LIME) interpretations and proposes a technique called ASWA and an extension called Norm-filtered Aggressive Stochastic Weight Averaging which improves the stability of models over random seeds.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we focus on quantifying model stability as a function of random seed by investigating the effects of the induced randomness on model performance and the robustness of the model in general. We specifically perform a controlled study on the effect of random seeds on the behaviour of attention, gradient-based and surrogate model based (LIME) interpretations. Our analysis suggests that random seeds can adversely affect the consistency of models resulting in counterfactual interpretations. We propose a technique called Aggressive Stochastic Weight Averaging (ASWA) and an extension called Norm-filtered Aggressive Stochastic Weight Averaging (NASWA) which improves the stability of models over random seeds. With our ASWA and NASWA based optimization, we are able to improve the robustness of the original model, on average reducing the standard deviation of the model’s performance by 72%.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e612.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e612.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-seed variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-seed induced variability in neural network training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instability in model predictions and interpretable attributions caused by random factors in training (random initialization, minibatch sampling, dropout, optimizer stochasticity) that lead to different optimizer trajectories and different local minima across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On Model Stability as a Function of Random Seed</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CNN and bi-directional LSTM with attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measure how different random-seed instantiations of the same model affect (a) predictive performance on binary and multi-class text classification tasks and (b) interpretation stability (attention, gradient-based attributions, and LIME).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random parameter initialization (random seed), random sampling of examples during training (stochastic minibatching), random dropout (random dropping of neurons), optimizer stochasticity (SGD/Adam noise), small-batch stochasticity leading to different optimization paths, and the non-convex loss surface / multiple local minima causing sensitivity to initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Prediction mean and standard deviation across runs; Relative Entropy (KL) between attention distributions; attention entropy; Jaccard distance over top-n (top-20%) tokens for interpretations; mean standard deviation of prediction confidences binned by confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Baseline CNN models show average standard deviation of ~±1.5% across datasets (example: CNN IMDB accuracy 89.8% (±0.79)); some datasets show high variance (ADR Tweets stddev ±2.65%; Diabetes CNN stddev ±2.26%). Interpretations differ strongly across seeds: on average 40–60% of the most important interpretable units (top tokens) differ across seeds. Attention entropy and Jaccard distances show high dissimilarity between runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same as variability_metrics (accuracy mean/std, KL(Relative Entropy), attention entropy, Jaccard distance on top-20% tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Reproducibility across seeds is poor without mitigation: substantial run-to-run variability in accuracy (stddev up to ~2.65%) and in interpretations (40–60% top-token disagreement; high KL/entropy values).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Non-convex loss surface with many local minima and saddle points; divergent optimizer trajectories from different initializations or minibatch sequences; randomness from initialization, minibatch sampling, and dropout; lack of deterministic training leading to inconsistent interpretations across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Weight-averaging based optimizers (Aggressive Stochastic Weight Averaging (ASWA) and Norm-filtered ASWA (NASWA)), reference to Stochastic Weight Averaging (SWA) as background; running multiple seeds and reporting mean/std; use of constant learning rate with ASWA/NASWA and averaging weights at each batch (ASWA) or conditionally when parameter-norm differences exceed a running mean (NASWA).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Applying ASWA/NASWA reduces predictive standard deviation on average by 72% relative to baseline and by 89% (relative) on the Diabetes (MIMIC) dataset; example reductions from tables: CNN IMDB stddev 0.79% -> 0.25% (ASWA); CNN Diabetes stddev 2.26% -> 0.25% (ASWA); LSTM IMDB stddev 1.34% -> 0.32% (ASWA). Interpretability metrics (attention entropy and Jaccard distance) also substantially improve (examples: average attention entropy reduced by ~60% in some experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 runs (main experiments); 100 seeds for supplementary experiments</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random seeds are a major source of variability in both performance and interpretability: different seeds yield substantially different attention distributions and feature attributions (40–60% disagreement among top tokens). Methods that stabilize optimization (ASWA/NASWA) sharply reduce run-to-run variability in accuracy and interpretations (average stddev reduction ~72%, up to ~89% on some datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Model Stability as a Function of Random Seed', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e612.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e612.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASWA / NASWA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggressive Stochastic Weight Averaging (ASWA) and Norm-filtered ASWA (NASWA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two weight-averaging based optimization schemes proposed to reduce random-seed induced instability: ASWA averages model weights aggressively at every batch update and assigns averaged weights each epoch; NASWA extends ASWA by updating the running average only when the current parameter-norm difference exceeds the historical mean (norm-filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CNN and bi-directional LSTM with attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Stabilize model training across random seeds to improve consistency of predictions and interpretations (attention, gradients, LIME) for text classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Targeted at variability resulting from random seeds (initialization), stochastic minibatching/optimizer noise, and divergent optimizer trajectories that lead to different local minima.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Same as the study: accuracy mean/std across seeds; attention Relative Entropy (KL), attention entropy, Jaccard distance on top-20% tokens; gradients and LIME Jaccard distances.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>With ASWA/NASWA, prediction accuracy is maintained or slightly improved while standard deviations shrink markedly (see mitigation_effectiveness). Attention entropy and Jaccard distance measures decrease substantially, indicating more consistent interpretations across seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Accuracy std, KL between attention distributions, attention entropy, Jaccard distance for top-20% tokens, gradient-attribution Jaccard, LIME Jaccard.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>ASWA and NASWA increase reproducibility/stability: average predictive stddev reduced by ~72%; Diabetes dataset saw ~89% relative reduction in stddev; attention entropy reductions ~60% in some cases; improvements verified across 10-run and 100-run experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Method improves first-order (weight-averaging) stability but does not analyze per-layer instabilities; the authors note potential further gains from second-order signals and do not claim to fully eliminate all sources of randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>ASWA algorithm: running average of weights updated at each batch, replaced at epoch end; NASWA algorithm: maintain list of previous L1 norm differences between current weights and running average, update running average only when current norm difference > mean of list, then reset list; both use constant learning rates and can be applied with Adam/SGD/Adagrad.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative outcomes: overall average standard deviation reduction = 72% (relative); Diabetes (MIMIC) relative improvement = 89%; example table entries: CNN IMDB accuracy 89.8% (±0.79) -> ASWA 90.2% (±0.25); CNN Diabetes accuracy 87.4% (±2.26) -> ASWA 85.9% (±0.25); LSTM IMDB 89.1% (±1.34) -> ASWA 90.2% (±0.32). Attention entropy and Jaccard distance also substantially improved (e.g., ~60% entropy reduction in some plots).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 runs (main); 100 seeds in supplementary validation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ASWA and NASWA substantially reduce run-to-run variability in accuracy and interpretations while preserving or improving mean performance; NASWA (norm-filtered) often yields slightly better stability than ASWA by selectively averaging only when parameter-norm divergence is large.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Model Stability as a Function of Random Seed', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e612.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e612.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interpretation stability metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Entropy (KL), Jaccard Distance, and Attention Entropy for interpretation comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metrics used to quantify dissimilarity of model explanations across runs: KL relative entropy between attention distributions, attention entropy aggregated over seeds, and Jaccard distance over sets of top-k (top-20%) most-attended tokens; used also for gradient-based and LIME surrogate attributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On Model Stability as a Function of Random Seed</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CNN and bi-directional LSTM with attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (NLP) / model interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Quantify how much model explanations (attention maps, gradient attributions, LIME surrogate features) vary across different random-seed instantiations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Differences across random-seed instantiations of the same model (random init, minibatch order, dropout), which produce different attention/attribution distributions and sets of top tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Relative Entropy (KL) computed between attention distributions per-sample across model pairs; Jaccard distance (1 - |A∩B|/|A∪B|) * 100% computed on top-20% tokens; attention entropy (per-bin and averaged); mean stddev of prediction confidence binned by confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>High KL/entropy and high Jaccard distances observed across seeds (interpretations unstable). Authors report 40–60% disagreement in top interpretable units across seeds and show ~60% reduction in average attention entropy with ASWA/NASWA in some datasets/plots.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>KL/Relative Entropy, Jaccard distance (top-20%), attention entropy, gradient-based Jaccard, LIME-based Jaccard.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Interpretation reproducibility is poor for baseline models (high KL/entropy and Jaccard distances); ASWA/NASWA reduce these metrics substantially (e.g., ~60% entropy reduction in examples, lower Jaccard distances indicating higher overlap among top tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Interpretations are sensitive to model instantiation; black-box interpretation methods (attention, gradients, surrogate models) will differ across seeds until underlying model stability is improved.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Applying ASWA and NASWA; running many seeds (authors ran 10 and 100 seeds) to measure statistical power and stability; using top-n selection (top-20%) to focus on most relevant tokens and reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>ASWA/NASWA reduce KL/entropy and Jaccard distances across datasets. Example: average attention entropy decreased by ~60% in a plotted experiment (Figure 8b); Jaccard distance of top-20% tokens also improved in CNN and LSTM experiments and for gradient and LIME attributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 runs (main); 100 seeds in supplementary</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>KL, attention entropy, and Jaccard distance concretely show that interpretations vary widely across seeds; applying weight-averaging stabilizers (ASWA/NASWA) reduces these instability metrics substantially, making attributions more consistent across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Model Stability as a Function of Random Seed', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Averaging weights leads to wider optima and better generalization <em>(Rating: 2)</em></li>
                <li>Stochastic gradient descent as approximate bayesian inference <em>(Rating: 2)</em></li>
                <li>Attention is not explanation <em>(Rating: 2)</em></li>
                <li>Pathologies of neural models make interpretation difficult <em>(Rating: 2)</em></li>
                <li>Adding gradient noise improves learning for very deep networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-612",
    "paper_id": "paper-285bff5a564cb7edfb355a2ffb44994386975f1d",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Random-seed variability",
            "name_full": "Random-seed induced variability in neural network training",
            "brief_description": "Instability in model predictions and interpretable attributions caused by random factors in training (random initialization, minibatch sampling, dropout, optimizer stochasticity) that lead to different optimizer trajectories and different local minima across runs.",
            "citation_title": "On Model Stability as a Function of Random Seed",
            "mention_or_use": "use",
            "model_name": "CNN and bi-directional LSTM with attention",
            "model_size": null,
            "scientific_domain": "Natural Language Processing (NLP)",
            "experimental_task": "Measure how different random-seed instantiations of the same model affect (a) predictive performance on binary and multi-class text classification tasks and (b) interpretation stability (attention, gradient-based attributions, and LIME).",
            "variability_sources": "Random parameter initialization (random seed), random sampling of examples during training (stochastic minibatching), random dropout (random dropping of neurons), optimizer stochasticity (SGD/Adam noise), small-batch stochasticity leading to different optimization paths, and the non-convex loss surface / multiple local minima causing sensitivity to initialization.",
            "variability_measured": true,
            "variability_metrics": "Prediction mean and standard deviation across runs; Relative Entropy (KL) between attention distributions; attention entropy; Jaccard distance over top-n (top-20%) tokens for interpretations; mean standard deviation of prediction confidences binned by confidence intervals.",
            "variability_results": "Baseline CNN models show average standard deviation of ~±1.5% across datasets (example: CNN IMDB accuracy 89.8% (±0.79)); some datasets show high variance (ADR Tweets stddev ±2.65%; Diabetes CNN stddev ±2.26%). Interpretations differ strongly across seeds: on average 40–60% of the most important interpretable units (top tokens) differ across seeds. Attention entropy and Jaccard distances show high dissimilarity between runs.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same as variability_metrics (accuracy mean/std, KL(Relative Entropy), attention entropy, Jaccard distance on top-20% tokens).",
            "reproducibility_results": "Reproducibility across seeds is poor without mitigation: substantial run-to-run variability in accuracy (stddev up to ~2.65%) and in interpretations (40–60% top-token disagreement; high KL/entropy values).",
            "reproducibility_challenges": "Non-convex loss surface with many local minima and saddle points; divergent optimizer trajectories from different initializations or minibatch sequences; randomness from initialization, minibatch sampling, and dropout; lack of deterministic training leading to inconsistent interpretations across runs.",
            "mitigation_methods": "Weight-averaging based optimizers (Aggressive Stochastic Weight Averaging (ASWA) and Norm-filtered ASWA (NASWA)), reference to Stochastic Weight Averaging (SWA) as background; running multiple seeds and reporting mean/std; use of constant learning rate with ASWA/NASWA and averaging weights at each batch (ASWA) or conditionally when parameter-norm differences exceed a running mean (NASWA).",
            "mitigation_effectiveness": "Applying ASWA/NASWA reduces predictive standard deviation on average by 72% relative to baseline and by 89% (relative) on the Diabetes (MIMIC) dataset; example reductions from tables: CNN IMDB stddev 0.79% -&gt; 0.25% (ASWA); CNN Diabetes stddev 2.26% -&gt; 0.25% (ASWA); LSTM IMDB stddev 1.34% -&gt; 0.32% (ASWA). Interpretability metrics (attention entropy and Jaccard distance) also substantially improve (examples: average attention entropy reduced by ~60% in some experiments).",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 runs (main experiments); 100 seeds for supplementary experiments",
            "key_findings": "Random seeds are a major source of variability in both performance and interpretability: different seeds yield substantially different attention distributions and feature attributions (40–60% disagreement among top tokens). Methods that stabilize optimization (ASWA/NASWA) sharply reduce run-to-run variability in accuracy and interpretations (average stddev reduction ~72%, up to ~89% on some datasets).",
            "uuid": "e612.0",
            "source_info": {
                "paper_title": "On Model Stability as a Function of Random Seed",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "ASWA / NASWA",
            "name_full": "Aggressive Stochastic Weight Averaging (ASWA) and Norm-filtered ASWA (NASWA)",
            "brief_description": "Two weight-averaging based optimization schemes proposed to reduce random-seed induced instability: ASWA averages model weights aggressively at every batch update and assigns averaged weights each epoch; NASWA extends ASWA by updating the running average only when the current parameter-norm difference exceeds the historical mean (norm-filtering).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CNN and bi-directional LSTM with attention",
            "model_size": null,
            "scientific_domain": "Natural Language Processing (NLP)",
            "experimental_task": "Stabilize model training across random seeds to improve consistency of predictions and interpretations (attention, gradients, LIME) for text classification tasks.",
            "variability_sources": "Targeted at variability resulting from random seeds (initialization), stochastic minibatching/optimizer noise, and divergent optimizer trajectories that lead to different local minima.",
            "variability_measured": true,
            "variability_metrics": "Same as the study: accuracy mean/std across seeds; attention Relative Entropy (KL), attention entropy, Jaccard distance on top-20% tokens; gradients and LIME Jaccard distances.",
            "variability_results": "With ASWA/NASWA, prediction accuracy is maintained or slightly improved while standard deviations shrink markedly (see mitigation_effectiveness). Attention entropy and Jaccard distance measures decrease substantially, indicating more consistent interpretations across seeds.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Accuracy std, KL between attention distributions, attention entropy, Jaccard distance for top-20% tokens, gradient-attribution Jaccard, LIME Jaccard.",
            "reproducibility_results": "ASWA and NASWA increase reproducibility/stability: average predictive stddev reduced by ~72%; Diabetes dataset saw ~89% relative reduction in stddev; attention entropy reductions ~60% in some cases; improvements verified across 10-run and 100-run experiments.",
            "reproducibility_challenges": "Method improves first-order (weight-averaging) stability but does not analyze per-layer instabilities; the authors note potential further gains from second-order signals and do not claim to fully eliminate all sources of randomness.",
            "mitigation_methods": "ASWA algorithm: running average of weights updated at each batch, replaced at epoch end; NASWA algorithm: maintain list of previous L1 norm differences between current weights and running average, update running average only when current norm difference &gt; mean of list, then reset list; both use constant learning rates and can be applied with Adam/SGD/Adagrad.",
            "mitigation_effectiveness": "Quantitative outcomes: overall average standard deviation reduction = 72% (relative); Diabetes (MIMIC) relative improvement = 89%; example table entries: CNN IMDB accuracy 89.8% (±0.79) -&gt; ASWA 90.2% (±0.25); CNN Diabetes accuracy 87.4% (±2.26) -&gt; ASWA 85.9% (±0.25); LSTM IMDB 89.1% (±1.34) -&gt; ASWA 90.2% (±0.32). Attention entropy and Jaccard distance also substantially improved (e.g., ~60% entropy reduction in some plots).",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 runs (main); 100 seeds in supplementary validation",
            "key_findings": "ASWA and NASWA substantially reduce run-to-run variability in accuracy and interpretations while preserving or improving mean performance; NASWA (norm-filtered) often yields slightly better stability than ASWA by selectively averaging only when parameter-norm divergence is large.",
            "uuid": "e612.1",
            "source_info": {
                "paper_title": "On Model Stability as a Function of Random Seed",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Interpretation stability metrics",
            "name_full": "Relative Entropy (KL), Jaccard Distance, and Attention Entropy for interpretation comparison",
            "brief_description": "Metrics used to quantify dissimilarity of model explanations across runs: KL relative entropy between attention distributions, attention entropy aggregated over seeds, and Jaccard distance over sets of top-k (top-20%) most-attended tokens; used also for gradient-based and LIME surrogate attributions.",
            "citation_title": "On Model Stability as a Function of Random Seed",
            "mention_or_use": "use",
            "model_name": "CNN and bi-directional LSTM with attention",
            "model_size": null,
            "scientific_domain": "Natural Language Processing (NLP) / model interpretability",
            "experimental_task": "Quantify how much model explanations (attention maps, gradient attributions, LIME surrogate features) vary across different random-seed instantiations.",
            "variability_sources": "Differences across random-seed instantiations of the same model (random init, minibatch order, dropout), which produce different attention/attribution distributions and sets of top tokens.",
            "variability_measured": true,
            "variability_metrics": "Relative Entropy (KL) computed between attention distributions per-sample across model pairs; Jaccard distance (1 - |A∩B|/|A∪B|) * 100% computed on top-20% tokens; attention entropy (per-bin and averaged); mean stddev of prediction confidence binned by confidence.",
            "variability_results": "High KL/entropy and high Jaccard distances observed across seeds (interpretations unstable). Authors report 40–60% disagreement in top interpretable units across seeds and show ~60% reduction in average attention entropy with ASWA/NASWA in some datasets/plots.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "KL/Relative Entropy, Jaccard distance (top-20%), attention entropy, gradient-based Jaccard, LIME-based Jaccard.",
            "reproducibility_results": "Interpretation reproducibility is poor for baseline models (high KL/entropy and Jaccard distances); ASWA/NASWA reduce these metrics substantially (e.g., ~60% entropy reduction in examples, lower Jaccard distances indicating higher overlap among top tokens).",
            "reproducibility_challenges": "Interpretations are sensitive to model instantiation; black-box interpretation methods (attention, gradients, surrogate models) will differ across seeds until underlying model stability is improved.",
            "mitigation_methods": "Applying ASWA and NASWA; running many seeds (authors ran 10 and 100 seeds) to measure statistical power and stability; using top-n selection (top-20%) to focus on most relevant tokens and reduce noise.",
            "mitigation_effectiveness": "ASWA/NASWA reduce KL/entropy and Jaccard distances across datasets. Example: average attention entropy decreased by ~60% in a plotted experiment (Figure 8b); Jaccard distance of top-20% tokens also improved in CNN and LSTM experiments and for gradient and LIME attributions.",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 runs (main); 100 seeds in supplementary",
            "key_findings": "KL, attention entropy, and Jaccard distance concretely show that interpretations vary widely across seeds; applying weight-averaging stabilizers (ASWA/NASWA) reduces these instability metrics substantially, making attributions more consistent across runs.",
            "uuid": "e612.2",
            "source_info": {
                "paper_title": "On Model Stability as a Function of Random Seed",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Averaging weights leads to wider optima and better generalization",
            "rating": 2
        },
        {
            "paper_title": "Stochastic gradient descent as approximate bayesian inference",
            "rating": 2
        },
        {
            "paper_title": "Attention is not explanation",
            "rating": 2
        },
        {
            "paper_title": "Pathologies of neural models make interpretation difficult",
            "rating": 2
        },
        {
            "paper_title": "Adding gradient noise improves learning for very deep networks",
            "rating": 1
        }
    ],
    "cost": 0.014325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On Model Stability as a Function of Random Seed</h1>
<p>Pranava Madhyastha<br>Department of Computing<br>Imperial College London<br>pranava@imperial.ac.uk</p>
<p>Rishabh Jain<br>Bloomberg<br>London<br>rjain213@bloomberg.net</p>
<h4>Abstract</h4>
<p>In this paper, we focus on quantifying model stability as a function of random seed by investigating the effects of the induced randomness on model performance and the robustness of the model in general. We specifically perform a controlled study on the effect of random seeds on the behaviour of attention, gradientbased and surrogate model based (LIME) interpretations. Our analysis suggests that random seeds can adversely affect the consistency of models resulting in counterfactual interpretations. We propose a technique called Aggressive Stochastic Weight Averaging (ASWA) and an extension called Norm-filtered Aggressive Stochastic Weight Averaging (NASWA) which improves the stability of models over random seeds. With our ASWA and NASWA based optimization, we are able to improve the robustness of the original model, on average reducing the standard deviation of the model's performance by $72 \%$.</p>
<h2>1 Introduction</h2>
<p>There has been a tremendous growth in deep neural network based models that achieve state-of-the-art performance. In fact, most recent end-to-end deep learning models have surpassed the performance of careful human feature-engineering based models in a variety of NLP tasks. However, deep neural network based models are often brittle to various sources of randomness in the training of the models. This could be attributed to several sources including, but not limited to, random parameter initialization, random sampling of examples during training and random dropping of neurons. It has been observed that these models have, more often, a set of random seeds that yield better results than others. This has also lead to research</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>suggesting random seeds as an additional hyperparameter for tuning (Bengio, 2012) ${ }^{1}$. One possible explanation for this behavior could be the existence of multiple local minima in the loss surface. This is especially problematic as the loss surfaces are generally non-convex and may have multiple saddle points making it difficult to achieve model stability.
if high crimes were any more generic it would have a universal product code instead of a title</p>
<p>$$
\left(\operatorname{Pr}\left(Y_{\text {negative }}\right)=0.99\right)
$$</p>
<p>if high crimes were any more generic it would have a universal product code instead of a title</p>
<p>$$
\left(\operatorname{Pr}\left(Y_{\text {negative }}\right)=0.98\right)
$$</p>
<p>Figure 1: Importance based on attention probabilities for two runs of the same model with same parameters and same hyperparameters, but with two different random seeds (color magnitudes: pink $&lt;$ magenta $&lt;$ red)</p>
<p>Recently the NLP community has witnessed a resurgence in interpreting and explaining deep neural network based models (Jain et al., 2019; Jain and Wallace, 2019; Alvarez-Melis and Jaakkola, 2017). Most of the interpretation based methods involve one of the following ways of interpreting models: a) sample oriented interpretations: where the interpretation is based on changes in the prediction score with either upweighting or perturbing samples (Jain et al., 2019; Jain and Wallace, 2019; Koh and Liang, 2017); b) interpretations based on feature attributions using attention or input perturbation or gradient-based measures; (Ghaeini et al., 2018; Feng et al., 2018; Bach et al., 2015); c) interpretations using surro-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>gate linear models <em>Ribeiro et al. (2016)</em> - these methods can provide local interpretations based on input samples or features. However, the presence of inherent randomness makes it difficult to accurately interpret deep neural models among other forms of pathologies <em>Feng et al. (2018)</em>.</p>
<p>In this paper, we focus on the stability of deep neural models as a function of random-seed based effects. We are especially interested in investigating the hypothesis focusing on model stability: do neural network based models under different random seeds allow for similar interpretations of their decisions? We claim that for a given model which achieves a substantial performance for a task, the factors responsible for any decisions over a sample should be approximately consistent irrespective of the random seed. In Figure 1, we show an illustration of this question where we visualize the attention distributions of two CNN based binary classification models for sentiment analysis, trained with the same settings and hyperparameters, but with different seeds. We observe that both models obtain the correct prediction with significantly high confidence. However, we note that both the models attend to completely different sets of words. This is problematic, especially when interpreting these models under the influence of such randomness. We observe that on average $40-60\%$ of the most important interpretable units are different across different random seeds for the same model. This phenomenon also leads us to the question on the exact nature of interpretability - are the interpretations specific to an instantiation of the model or are they general to a class of models?</p>
<p>We also provide a simple method that can, to a large extent, ameliorate this inherent random behaviour. In Section 3.1, we propose an aggressive stochastic weight averaging approach that helps in improving the stability of the models at almost zero performance loss while still making the model robust to random-seed based instability. We also propose an improvement to this model in Section 3.2 which further improves the stability of the neural models. Our proposals significantly improve the robustness of the model, on average by $72 \%$ relative to the original model and on Diabetes (MIMIC), a binary classification dataset, by $89 \%$ (relative improvement). All code for reproducing and replicating our experiments is released in our repository ${ }^{2}$.</p>
<h2>2 Measuring Model Stability</h2>
<p>In this section, we describe methods that we use to measure model stability, specifically - prediction and interpretation stability.</p>
<h3>2.1 Prediction Stability</h3>
<p>We measure prediction stability using standard measures of the mean and the standard deviations corresponding to the accuracy of the classification based models on different datasets. We ensure that the models are run with exactly the same configurations and hyper-parameters but with different random seeds. This is a standard procedure that is used in the community to report the performance of the model.</p>
<h3>2.2 Interpretation Stability</h3>
<p>For a given task, we train a set of models only differing with random-seeds. For every given test sample, we obtain interpretations using different instantiations of the models. We define a model to be stable if we obtain similar interpretations regardless of different random-seed based instantiations. We use the following metrics to quantify stability:
a) Relative Entropy quantification $(\mathcal{H})$ : Given two distributions over interpretations, for the same test case, from two different models, it measures the relative entropy between the two probability distributions. Note that, the higher the relative entropy the greater the dissimilarity between the two distributions.</p>
<p>$$
\mathcal{H}=\sum_{i \in d} \operatorname{Pr}<em 1="1">{1} \cdot \log \frac{\operatorname{Pr}</em>
$$}}{\operatorname{Pr}_{2}</p>
<p>where, $\operatorname{Pr}<em 2="2">{1}$ and $\operatorname{Pr}</em>$ are two attention distributions of the same sample from two different runs of the model and $d$ is the number of tokens in the sample. Given $n$ differently seeded models, for each test instance, we calculate the relative entropy obtained from the corresponding averaged pairwise interpretation distributions.
b) Jaccard Distance $(\mathcal{J})$ : It measures the dissimilarity between two sets. Here higher values of $\mathcal{J}$ indicate larger variances. We consider top- $n$ tokens which have the highest attention for comparison. Note that, Jaccard distance is over sets of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>word indices and do not take into account the attention probabilities explicitly. Jaccard distance is defined as:</p>
<p>$$
\mathcal{J}=\left(1-\frac{A \cap B}{A \cup B}\right) * 100 \%
$$</p>
<p>where, $A$ and $B$ are the sets of most relevant items. We specifically decided to use 'most' relevant (top- $n$ items) as the tail of the distribution mostly consists of values close to 0 .</p>
<p>Interpretation methods under study: In this paper we study interpretation stability using the following three interpretation methods:</p>
<ol>
<li>Attention based interpretation: We focus on attention probabilities as the mode of interpretation and consider the model to be stable if different instantiations of the model leads to similar attention distributions. Our major focus in this paper is attention based interpretation. As we use Jain et al. (2019) as a testbed for our investigation, we focus heavily on attention. Also, as the attention layer has a linear relationship with the prediction, we consider attention to be more indicative of the model stability.</li>
<li>Gradient-based feature importance: Given a sample, we use the input gradients of the model corresponding to each of the word representations and compute the magnitude of the change as a local explanation. We refer the reader to Baehrens et al. (2010) for a good introduction to gradient-based interpretations. As all of our models are differentiable, we use this as an alternative method for interpretation. We follow the standard procedure as followed in Feng et al. (2018) and note that we do not follow Jain and Wallace (2019) and do not disconnect the computational graph at the attention module. We obtain probabilistic gradient scores by normalizing over the absolute values of gradient values.</li>
<li>LIME based interpretation: We use locally interpretable model-agnostic interpretations (Ribeiro et al., 2016) that learns a surrogate interpretable model locally around the predictions of the deep neural based model. We obtain LIME based interpretations for every instantiation of the models. We then use Jaccard Distance to measure the divergence.</li>
</ol>
<p>We note that, we observe similar patterns across the three interpretation methods and the interpretations consistently differ with random seeds.</p>
<h2>3 Reducing Model Instability with an Optimization Lens</h2>
<p>We observe that different instantiations of the model can cause the model have different starts on the optimization surface. Further, stochastic sampling might result in different paths. Both of these factors can lead to different local minimas potentially leading to different solutions. With this observation as our background we propose two, closely related, methods to ameliorate divergence: Agressive Stochastic Weight Averaging and Norm-filtered Agressive Stochastic Weight Averaging. We describe these two in the following subsections.</p>
<h3>3.1 Aggressive Stochastic Weight Averaging (ASWA)</h3>
<p>Stochastic weight averaging (SWA) (Izmailov et al., 2018) works by averaging the weights of multiple points in the trajectory of gradient descent based optimizers. The algorithm typically uses modified learning rate schedules. SWA is itself based on the idea of maintaining a running average of weights in stochastic gradient descent based optimization techniques (Ruppert, 1988; Polyak and Juditsky, 1992). The principle idea in SWA is averaging the weights that are maximally distant helps stabilize the gradient descent based optimizer trajectory and improves generalization. Izmailov et al. (2018) use the analysis of Mandt et al. (2017) to illustrate the stability arguments where they show that, under certain convexity assumptions, SGD iterations can be visualized as sampling from a Gaussian distribution centred at the minima of the loss function. Samples from high-dimensional Gaussians are expected to be concentrated on the surface of the ellipse and not close to the mean. Averaging iterations is shown to stabilize the trajectory and further improve the width of the solutions to be closer to the mean.</p>
<p>In this paper, we focus on the stability of deep neural models as a function of random-seeds. Our proposal is based on SWA, but we extend it to the extremes and call it Aggressive Stochastic Weight Averaging. We assume that, for small batch size, the loss surface is locally convex. We further relax</p>
<p>the conditions for the optimizer and assume that the optimizer is based on some version of gradient descent — this means that our modification is valid even for other pseudo-first-order optimization algorithms including Adam <em>Kingma and Ba (2014)</em> and Adagrad <em>Duchi et al. (2011)</em>.</p>
<p>We note that, Izmailov et al. <em>Izmailov et al. (2018)</em> suggest using SWA usually after 'pre-'training the model (at least until 75% convergence) and followed by sampling weights at different steps either using large constant or cyclical learning rates. While, SWA is well defined for convex losses <em>Polyak and Juditsky (1992)</em>, Izmailov et al. <em>Izmailov et al. (2018)</em> connect SWA to non-convex losses by suggesting that the loss surface is approximately convex after convergence. In our setup, we investigate the utility of averaging weights over every iteration (an iteration consists of one batch of the gradient descent). Algorithm 1 shows the implementation pseudo-code for SWA. We note that, unlike Izmailov et al. <em>Izmailov et al. (2018)</em>, we average our weights at each batch update and assign the ASWA parameters to the model at the end of each epoch. That is, we replace the model’s weights for the next epoch with the averaged weights.</p>
<p>Algorithm 1 Aggressive SWA algorithm
Require:
1: $e=$ Epoch number
2: $m=$ Total epochs
3: $i=$ Iteration number
4: $n=$ Total iterations
5: $\alpha=$ Learning rate
6: $\mathcal{O}=$ Stochastic Gradient optimizer function
$e\leftarrow 0$;
while $e&lt;m$ do
$i\leftarrow 1$
while $i\leq n$ do
$W_{swa}\leftarrow W_{swa}+\frac{(W-W_{swa})}{(e*n+i+1)}$;
$W\leftarrow W-\mathcal{O}(\alpha,W)$;
$i\leftarrow i+1$
$W\leftarrow W_{swa}$;
$e\leftarrow e+1$</p>
<p>In Figure 2, we show an SGD optimizer (with momentum) and the same optimizer with SWA over a 3-dimensional loss surface with a saddle point. We observe that the original SGD reaches the desired minima, however, it almost reaches the saddle point and does a course correction and reaches minima. On the other hand, we observe that SGD with ASWA is very conservative, it repeatedly restarts and reaches the minima without reaching the saddle point. We empirically observe that this is a desired property for the stability of models over runs of the same model that differ only over random instantiations. The grey circles in Figure 2 highlight this conservative behaviour of SGD with ASWA optimizer, especially when compared to the standard SGD. Further, Polyak and Juditsky <em>Polyak and Juditsky (1992)</em> show that for convex losses, averaging SGD proposals achieves the highest possible rate of convergence for a variety of first-order SGD based algorithms.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>(a) Trajectory for Stochastic Gradient Descent</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>(b) Trajectory for Stochastic Gradient Descent with ASWA</p>
<p>Figure 2: Trajectory for gradient descent algorithms with red and black arrows on (b) indicating movements from consecutive epochs with restarts. Conservative behaviour of ASWA algorithm helps avoid the saddle point without ever reaching it.</p>
<h3>3.2 Norm-filtered Aggressive Stochastic Weight Averaging (NASWA)</h3>
<p>We observe that the ASWA algorithm is especially beneficial when the norm difference of the parameters of the model are high. We hypothesise that in general, the norm difference indicates the divergence between optimizers’ steps and we observe that the larger the norm difference, the greater the change in the trajectory. Therefore, we propose to</p>
<p>Algorithm 2: Norm-filtered Aggressive SWA algorithm</p>
<h2>Require:</h2>
<p>$1: e=$ Epoch number
$2: m=$ Total epochs
$3: i=$ Iteration number
$4: n=$ Total iterations
$5: \alpha=$ Learning rate
6: $\mathcal{O}=$ Stochastic Gradient optimizer function
7: $N_{s}=$ List of previous iterations' norm differences
$e \leftarrow 0 ;$
while $e<m$ do
$i \leftarrow 1$
while $i \leq n$ do
$N_{\text {cur }} \leftarrow\left\|W-W_{\text {swa }}\right\|_{1} ;$
$N_{\text {mean }} \leftarrow \sum_{i=1}^{|N_{s}|} \frac{N_{s}[i]}{\left|N_{s}\right|}$;
if $N_{\text {cur }}>N_{\text {mean }}$ then
$W_{\text {swa }} \leftarrow W_{\text {swa }}+\frac{\left(W-W_{\text {swa }}\right)}{(e * n+i+1)}$;
$N_{s} \leftarrow\left[N_{\text {cur }}\right] ;$
else
$N_{s} \leftarrow N_{s}+\left[N_{\text {cur }}\right] ;$
$W \leftarrow W-\mathcal{O}(\alpha, W) ;$
$i \leftarrow i+1$
$W \leftarrow W_{\text {swa }} ;$
$e \leftarrow e+1$
maintain a list that stores the norm differences of the previous iterations. If the norm difference of the current iteration is greater than the average of the list, we update the ASWA weights and reinitialize the list with the current norm difference. When the norm difference, however, is less than the average of the list, we just append the current norm difference to the list. After the completion of the epoch, we assign the ASWA parameters to the model. This is shown in Algorithm 2. We call this approach Norm-filtered Aggressive Stochastic Weight Averaging.</p>
<h2>4 Experiments</h2>
<p>We base our investigation on similar sets of models as <em>Jain and Wallace (2019)</em>. We also use the code provided by the authors for our empirical investigations for consistency and empirical validation. We describe our models and datasets used for the experiments below.</p>
<h3>4.1 Models</h3>
<p>We consider two sets of commonly used neural models for the tasks of binary classification and multi-class natural language inference. We use CNN and bi-directional LSTM based models with attention. We follow <em>Jain and Wallace (2019)</em> and use similar attention mechanisms using a) additive attention <em>Bahdanau et al. (2014)</em>; and b) scaled dot product based attention <em>Vaswani et al. (2017)</em>. We jointly optimize all the parameters for the model, unlike <em>Jain and Wallace (2019)</em> where the encoding layer, attention layer and the output prediction layer are all optimized separately. We experiment with several optimizers including Adam <em>Kingma and Ba (2014)</em>, SGD and Adagrad <em>Duchi et al. (2011)</em> but most results below are with Adam.</p>
<p>For our ASWA and NASWA based experiments, we use a constant learning rate for our optimizer. Other model-specific settings are kept the same as <em>Jain and Wallace (2019)</em> for consistency.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Avg. Length</th>
<th>Train Size</th>
<th>Test size</th>
</tr>
</thead>
<tbody>
<tr>
<td>IMDB</td>
<td>179</td>
<td>12500 / 12500</td>
<td>2184 / 2172</td>
</tr>
<tr>
<td>Diabetes(MIMIC)</td>
<td>1858</td>
<td>6381 / 1353</td>
<td>1295 / 319</td>
</tr>
<tr>
<td>SST</td>
<td>19</td>
<td>3034 / 3321</td>
<td>652/653</td>
</tr>
<tr>
<td>Anemia(MIMIC)</td>
<td>2188</td>
<td>1847 / 3231</td>
<td>460 / 802</td>
</tr>
<tr>
<td>AgNews</td>
<td>26</td>
<td>30000 / 30000</td>
<td>1900 / 1900</td>
</tr>
<tr>
<td>ADR Tweets</td>
<td>20</td>
<td>14446 / 1939</td>
<td>3636 / 487</td>
</tr>
<tr>
<td>SNLI</td>
<td>14</td>
<td>182764 / 183187 / 183416</td>
<td>3219 / 3237 / 3368</td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset characteristics. Train size and test size show the cardinality for each class. SNLI is a threeclass dataset while the rest are binary classification</p>
<h3>4.2 Datasets</h3>
<p>The datasets used in our experiments are listed in Table 1 with summary statistics. We further pre-process and tokenize the datasets using the standard procedure and follow <em>Jain and Wallace (2019)</em>. We note that IMDB <em>Maas et al. (2011)</em>, Diabetes(MIMIC) <em>Johnson et al. (2016)</em>, Anemia(MIMIC) <em>Johnson et al. (2016)</em>, AgNews <em>Zhang et al. (2015)</em>, ADR Tweets <em>Nikfarjam et al. (2015)</em> and SST <em>Socher et al. (2013)</em> are datasets for the binary classification setup. SNLI <em>Bowman et al. (2015)</em> is a dataset for the multiclass classification setup. All of the datasets are in English, however we expect the behavior to persist regardless of the language.</p>
<h3>4.3 Settings and Hyperparameters</h3>
<p>We use a 300 -dimenstional embedding layer which is initialized with FastText <em>Joulin et al. (2016)</em> based free-trained embeddings for both CNN and the bi-directional LSTM based models.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>CNN(%)</th>
<th>CNN+ASWA(%)</th>
<th>CNN+NASWA(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>IMDB</td>
<td>89.8 (±0.79)</td>
<td>90.2 (±0.25)</td>
<td>90.1 (±0.29)</td>
</tr>
<tr>
<td>Diabetes</td>
<td>87.4 (±2.26)</td>
<td>85.9 (±0.25)</td>
<td>85.9 (±0.38)</td>
</tr>
<tr>
<td>SST</td>
<td>82.0 (±1.01)</td>
<td>82.5 (±0.39)</td>
<td>82.5 (±0.39)</td>
</tr>
<tr>
<td>Anemia</td>
<td>90.6 (±0.98)</td>
<td>91.9 (±0.20)</td>
<td>91.9 (±0.19)</td>
</tr>
<tr>
<td>AgNews</td>
<td>95.5 (±0.23)</td>
<td>96.0 (±0.11)</td>
<td>96.0 (±0.07)</td>
</tr>
<tr>
<td>Tweet</td>
<td>84.6 (±2.65)</td>
<td>84.4 (±0.54)</td>
<td>84.4 (±0.54)</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance statistics obtained from 10 differently seeded CNN based models. Table compares accuracy and its standard deviation for the normally trained CNN model against the ASWA and NASWA trained models, whose deviation drops significantly, thus, indicating increased robustness.</p>
<h2>5 Results</h2>
<p>In this section, we summarize our findings for 10 runs of the model with 10 different random seeds but with identical model settings.</p>
<h3>5.1 Model Performance and Stability</h3>
<p>We first report model performance and prediction stability. The results are reported in Table 2.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>LSTM(%)</th>
<th>LSTM+ASWA(%)</th>
<th>LSTM+NASWA(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>IMDB</td>
<td>89.1 (±1.34)</td>
<td>90.2 (±0.32)</td>
<td>90.3 (±0.17)</td>
</tr>
<tr>
<td>Diabetes</td>
<td>87.7 (±1.44)</td>
<td>87.7 (±0.60)</td>
<td>87.8 (±0.55)</td>
</tr>
<tr>
<td>SST</td>
<td>81.9 (±1.11)</td>
<td>82.0 (±0.60)</td>
<td>82.1 (±0.57)</td>
</tr>
<tr>
<td>Anemia</td>
<td>91.6 (±0.49)</td>
<td>91.8 (±0.34)</td>
<td>91.9 (±0.36)</td>
</tr>
<tr>
<td>AgNews</td>
<td>95.5 (±0.32)</td>
<td>96.1(±0.17)</td>
<td>96.1 (±0.10)</td>
</tr>
<tr>
<td>Tweet</td>
<td>84.7 (±1.79)</td>
<td>83.8 (±0.45)</td>
<td>83.9 (±0.45)</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance statistics obtained from 10 differently seeded LSTM based models.</p>
<p>We note that the original CNN based models, on an average, have a standard deviation of $\pm 1.5\%$. Which seems standard, however, we note that ADR Tweets dataset has a very high standard deviation of $\pm 2.65\%$. We observe that ASWA and NASWA are almost always able to achieve higher performance with a very low standard deviation. This suggests that both ASWA and NASWA are extremely stable when compared to the standard model. They significantly improve the robustness, on an average, by $72\%$ relative to the original model and on Diabetes (MIMIC), a binary classification dataset, by $89\%$ (relative improvement). We observe similar results for the LSTM based models in Table 3.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Prediction’s standard deviation for CNN and LSTM based models for all binary classification datasets under consideration. Predictions are bucketed in intervals of size 0.1, starting from 0 (containing predictions from 0 to 0.1), until 0.9</p>
<p>We further analyze the prediction score stability by computing the mean standard deviation over the binned confidence intervals of the models in Figure 3a. We note that on an average, the standard deviations are on the lower side. However, we observe that the mean standard deviation of the bins close to 0.5 is on the higher side as is expected given the high uncertainty. On the other hand both, ASWA and NASWA based models are relatively more stable than the standard CNN based model. We observe similar behaviours for the LSTM based models in Figure 3b. This suggests that our proposed methods, ASWA and NASWA, are able to obtain relatively better stability without any loss in performance. We also note that both ASWA and NASWA had relatively similar performance over more than 10 random seeds.</p>
<h3>5.2 Attention Stability</h3>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average attention entropy against the bucketed predictions for CNN and LSTM based models. Figure highlights the high entropy between attention based distributions from differently seeded models (especially for the Diabetes-MIMIC datatset), indicating towards model instability.</p>
<p>We now consider the stability of attention distributions as a function of random seeds. We first plot the results of the experiments for standard CNN based binary classification models over uniformly binned prediction scores for positive labels in Figure 4a. We observe that, depending on the datasets, the attention distributions can become extremely unstable (high entropy). We specifically highlight the Diabetes(MIMIC) dataset’s entropy distribution. We observe similar, but relatively worse results for the LSTM based models in Figure 4b. In general, we would expect the entropy distribution to be close to zero however, this doesn’t seem to be the case. This means that using attention distributions to interpret models may not be reliable and can lead to misinterpretations.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Jaccard distance highlighting instability in attention distributions of CNN and LSTM based models.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Improved prediction stability from ASWA and NASWA for CNN and LSTM based models</p>
<p>We use the top $20 \%$ of the most important items (indices) in the attention distribution for each dataset over 10 runs and plot the Jaccard distances for CNN and LSTM based models in Figure 5a and Figure 5b. We again notice a similar
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Gradient based interpretations' stability improvement from NASWA on CNN based models. The Jaccard distance is calculated using the top 20\% attentive items.
trend of unstable attention distributions over both CNN and LSTM based attention distribution.</p>
<p>In the following sections for space constraints, we focus on CNN based models with additive attention. Our results on LSTM based models are provided in the attached supplementary material. We note that the observations for LSTM models are, in most cases, similar to the behaviour of the CNN based models. Scaled dot-product based models are also provided in the supplementary material and we notice a similar trend as the additive attention.</p>
<p>We now focus on the effect of ASWA and NASWA on binary and multi-class CNN based neural models separately.</p>
<p>Binary Classification In Figure 8, we plot the results of the models with ASWA and NASWA. We observe that both these algorithms significantly improve the model stability and decrease the entropy between attention distributions. For example, in Figure 8b, both ASWA and NASWA decrease the average entropy by about $60 \%$. We further notice that NASWA is slightly better performing in most of the runs. This empirically validates the hypothesis that averaging the weights from divergent weights (when the norm difference is higher than the average norm difference) helps in stabilizing the model's parameters, resulting in a more robust model.</p>
<p>Multi-class Classification In Figure 9, we plot the entropy between the attentions distributions of the models for the SNLI dataset (CNN based model), separately for each label (neutral, contradiction, and entailment). We notice, similar observations as the binary classification models, the ASWA and NASWA algorithms are able to significantly improve the entropy of the attention distributions and increases the robustness of the model</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Attention stability improvement from ASWA and NASWA on CNN based models.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Attention stability improvement from ASWA and NASWA on CNN based model for the SNLI dataset.</p>
<p>with random seeds.</p>
<h3>5.3 Gradient-based Interpretations</h3>
<p>We now look at an alternative method of interpreting deep neural models and look into the consistency of the gradient-based interpretations to further analyze the model's instability. For this setup, we focus on binary classifier and plot the results on the SST and the Diabetes dataset in particular since they cover the low and the high end of the entropy spectrum (respectively). We notice similar trends of instability in the gradient-based interpretations from model inputs as we did for the attention distributions. Figure 7 shows that the entropy between the gradient-based interpretations from differently seeded models closely follows the same trend as the attention distributions. This result further strengthens our claim on the importance of model stability and shows that over different runs of the same model with different seeds, we may get different interpretations using gradient-based feature importance. Moreover, Figure 7 shows the impact of ASWA towards making the gradient-based interpretations more consistent, thus, significantly increasing the stability.</p>
<h3>5.4 LIME based Interpretations</h3>
<p>We further evaluated the surrogate model based interpretability using LIME <em>Ribeiro et al. (2016)</em>. LIME obtains a locally linear approximation of the model's behaviour for a given sample by perturbing it and learning a sparse linear model around it. We focus on AgNews and SST based datasets and obtain interpretability estimates using LIME. Once again, we notice a similar pattern of instability as the other two interpretability methods. In Figure 10 we present our results from the LIME based interpretations with Jaccard distance as the measure. Note that we measure the Jaccard distance over the top 20% most influential items.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" />Figure 10: LIME based interpretations’ stability improvement from NASWA on CNN based models. The Jaccard distance is calculated using the top 20% attentive items.</p>
<p>observe once again that NASWA helps in reducing the instability and results in more consistent interpretations.</p>
<p>In all our experiments, we find that a significant proportion of interpretations are dependent on the instantiation of the model. We also note that we perform experiments over 100 random seeds for greater statistical power and see similar patterns.</p>
<h2>6 Discussion</h2>
<p>Recent advances in adversarial machine learning <em>Neelakantan et al. (2015); Zahavy et al. (2016)</em> have investigated robustness to random initialization based perturbations, however, to our knowledge, no previous study investigates the effect of random-seeds and its connection on model interpretation. Our study analyzed the inherent lack of robustness in deep neural models for NLP. Recent studies cast doubt on the consistency and correlations of several types of interpretations <em>Doshi-Velez and Kim (2017); Jain and Wallace (2019); Feng et al. (2018)</em>. We hypothesise that some of these issues are due to the inherent instability of the deep neural models to random-seed base perturbations. Our analysis (in Section 4) leads to the hypothesis that models with different instantiations may use completely different optimization paths. The issue of variance in all black-box interpretation methods over different seeds will continue to persist until the models are fully robust to random-seed based perturbations. Our work however, doesn’t provide insights into instabilities of different layers of the models. We hypothesise that it might further uncover the reasons for the relatively lower correlation between different black-box interpretation methods as these are effectively based off on different layers and granularity.</p>
<p>There has been some work on using noisy gradients <em>Neelakantan et al. (2015)</em> and learning from adversarial and counter-factual examples <em>Feng et al. (2018)</em> to increase the robustness of deep learning models. Feng et al. <em>Feng et al. (2018)</em> show that neural models may use redundant features for prediction and also show that most of the black-box interpretation methods may not be able to capture these second-order effects. Our proposals show that aggressively averaging weights leads to better optimization and the resultant models are more robust to random-seed based perturbation. However, our research is limited to increasing consistency in neural models. Our approach further uses first order based signals to boost stability. We posit that second-order based signals can further enhance consistency and increase the robustness.</p>
<h2>7 Conclusions</h2>
<p>In this paper, we study the inherent instability of deep neural models in NLP as a function of random seed. We analyze model performance and robustness of the model in the form of attention based interpretations, gradient-based feature importance and LIME based interpretations across multiple runs of the models with different random seeds. Our analysis strongly highlights the problems with stability of models and its effects on black-box interpretation methods leading to different interpretations for different random seeds. We also propose a solution that makes use of weight averaging based optimization technique and further extend it with norm-filtering. We show that our proposed methods largely stabilize the model to random-seed based perturbations and, on average, significantly reduce the standard deviations of the model performance by $72\%$. We further show that our methods significantly reduce the entropy in the attention distribution, the gradient-based feature importance measures and LIME based interpretations across runs.</p>
<h2>Acknowledgments</h2>
<p>We thank Panos Parpas and Emtiyaz Khan for their feedback on an earlier draft of this paper. We thank the anonymous reviewers for their thorough reviews and constructive comments. Pranava Madhyastha kindly acknowledges the support of Amazon AWS Cloud Credits for Research Award, hardware grant from NVIDIA, Anne O’Neill and the Imperial Corporate Partnership Programme.</p>
<h2></h2>
<h2>References</h2>
<p>David Alvarez-Melis and Tommi S Jaakkola. 2017. A causal framework for explaining the predictions of black-box sequence-to-sequence models. arXiv preprint arXiv:1707.01943.</p>
<p>Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140.</p>
<p>David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and KlausRobert MÅžller. 2010. How to explain individual classification decisions. Journal of Machine Learning Research, 11(Jun):1803-1831.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>Yoshua Bengio. 2012. Practical recommendations for gradient-based training of deep architectures. In Neural networks: Tricks of the trade, pages 437478. Springer.</p>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.</p>
<p>Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608.</p>
<p>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159.</p>
<p>Shi Feng, Eric Wallace, Alvin Grissom II, Pedro Rodriguez, Mohit Iyyer, and Jordan Boyd-Graber. 2018. Pathologies of neural models make interpretation difficult. In Empirical Methods in Natural Language Processing.</p>
<p>Reza Ghaeini, Xiaoli Z Fern, and Prasad Tadepalli. 2018. Interpreting recurrent and attention-based neural models: a case study on natural language inference. arXiv preprint arXiv:1808.03894.</p>
<p>Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407.</p>
<p>Sarthak Jain, Ramin Mohammadi, and Byron C Wallace. 2019. An analysis of attention over clinical notes for predictive tasks. arXiv preprint arXiv:1904.03244.</p>
<p>Sarthak Jain and Byron C. Wallace. 2019. Attention is not explanation. CoRR, abs/1902.10186.</p>
<p>Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimic-iii, a freely accessible critical care database. Scientific data, 3:160035.</p>
<p>Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016. Fasttext. zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1885-1894. JMLR. org.</p>
<p>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142-150. Association for Computational Linguistics.</p>
<p>Stephan Mandt, Matthew D Hoffman, and David M Blei. 2017. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907.</p>
<p>Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. 2015. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807.</p>
<p>Azadeh Nikfarjam, Abeed Sarker, Karen O'connor, Rachel Ginn, and Graciela Gonzalez. 2015. Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features. Journal of the American Medical Informatics Association, 22(3):671-681.</p>
<p>Boris T Polyak and Anatoli B Juditsky. 1992. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838-855.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386.</p>
<p>David Ruppert. 1988. Stochastic approximation. Technical report, Cornell University Operations Research and Industrial Engineering.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models</p>
<p>for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages $1631-1642$.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Tom Zahavy, Bingyi Kang, Alex Sivak, Jiashi Feng, Huan Xu, and Shie Mannor. 2016. Ensemble robustness and generalization of stochastic deep learning algorithms. arXiv preprint arXiv:1602.02389.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649-657.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Entropy improvement for tanh Attention based CNN model for the SST dataset using 100 different seeds.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Entropy improvement for tanh Attention based CNN model for the AgNews dataset using 100 different seeds.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Entropy improvement for tanh Attention based CNN model for the IMDB dataset using 100 different seeds.</p>
<h2>A Jaccard Distance Experiments</h2>
<p>In Figure 5a and Figure 5b we plot the Jaccard distance plots with CNN models and LSTM models and note that ASWA consistently improves the stability of the interpretations.</p>
<h2>B Results with 100 seeds</h2>
<p>We perform attention stability based experiments as mentioned in the paper, but now with 100 seeds, i.e., 100 models initialized with different seeds instead of 10. Figures 11, 13, and 12 show the entropy of attention based interpretations for different datasets. Experimenting with 100 seeds helps strengthen our claims about the instability of the model and the effectiveness of our proposed algorithms like ASWA and NASWA.</p>
<h2>C Hyperparameter Settings</h2>
<p>For training purposes, we use the same model settings for the models as mentioned in the paper (Jain and Wallace, 2019) (or the Github implementation), our port of the code is made available at: https://github.com/rishj97/ModelStability. Additional hyper-parameters for replication studies are:</p>
<ul>
<li>Number of epochs: 20</li>
<li>Optimizer: Adam</li>
<li>Learning rate: 0.001</li>
</ul>
<p>The exact seeds used for running the experiments can be found in our code repository.</p>
<h2>D Binary Classification with LSTM based Models</h2>
<p>For LSTM based models, we notice (in Figure 14) similar trends as to the CNN models in terms of the instability of the attention based interpretations.</p>
<p><sup>*</sup>https://github.com/successar/AttentionExplanation</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Attention stability improvement from ASWA and NASWA on LSTM based models.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Jaccard Distance improvement for Diabetes.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: Jaccard Distance improvement for Diabetes.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17: Entropy improvement for dot Attention based CNN model for SST dataset.
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18: Entropy improvement for dot Attention based CNN model for Diabetes dataset.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/rishj97/ModelStability&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ http://www.argmin.net/2018/02/26/ nominal/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
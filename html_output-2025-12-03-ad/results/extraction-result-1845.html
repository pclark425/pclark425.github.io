<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1845 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1845</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1845</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-fc53f8f3a84f1fc4993689d8f98cf6551d07a22d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fc53f8f3a84f1fc4993689d8f98cf6551d07a22d" target="_blank">LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> LL3DA is presented, a Large Language 3D Assistant that takes point cloud as the direct input and responds to both text instructions and visual interactions that enables LMMs to better comprehend human interactions with the 3D environment and further remove the ambiguities within plain texts.</p>
                <p><strong>Paper Abstract:</strong> Recent progress in Large Multimodal Models (LMM) has opened up great possibilities for various applications in the field of human-machine interactions. However, developing LMMs that can comprehend, reason, and plan in complex and diverse 3D environments remains a challenging topic, especially considering the demand for understanding permutation-invariant point cloud representations of the 3D scene. Existing works seek help from multi-view images by projecting 2D features to 3D space, which inevitably leads to huge computational overhead and performance degradation. In this paper, we present LL3DA, a Large Language 3D Assistant that takes point cloud as the direct input and responds to both text instructions and visual interactions. The additional visual interaction enables LMMs to better comprehend human interactions with the 3D environment and further remove the ambiguities within plain texts. Experiments show that LL3DA achieves remarkable results and surpasses various 3D vision-language models on both 3D Dense Captioning and 3D Question Answering.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1845.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1845.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LL3DA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language 3D Assistant (LL3DA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven multi-modal 3D agent that directly consumes point cloud input plus optional visual prompts (clicks/boxes) and textual instructions via an Interactor3D (Q-Former / MMT) which projects a fixed-length interaction-aware prefix into a frozen causal LLM (OPT-1.3B) to generate free-form text (including coordinate tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LL3DA (Large Language 3D Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Auto-regressive, LLM-driven model that (1) uses a frozen scene encoder pre-trained on ScanNet detection to produce point features, (2) encodes visual prompts (clicks/ROIs) and textual instructions in a Multi-Modal Transformer (Q-Former) that aggregates into 32 learnable query tokens, (3) linearly projects those queries to form a prefix input to a frozen causal LLM (OPT-1.3B) which generates natural language outputs and coordinate tokens. Total trainable parameters ≈111M (most weights frozen).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>language model pretraining (causal LLM) + 3D detection pretraining for scene encoder</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The LLM backbone is OPT-1.3B (cited [58]) — a causal language model pretrained on large-scale internet text (OPT family). The 3D scene encoder is a masked transformer encoder pretrained on ScanNet detection (ScanNet detection annotations). The Q-Former loads BERT word and positional embeddings for some initializations. Exact OPT corpus details are referenced via OPT but not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D Dense Captioning; 3D Question Answering; Scene Description; Embodied Dialogue; Embodied Planning; (also evaluated for generating 3D bounding boxes / open-vocabulary detection)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Tasks are performed on indoor 3D scenes from ScanNet (point cloud inputs). 3D Dense Captioning: localize and describe instances (ScanRefer, Nr3D). 3D Question Answering: answer questions about the 3D scene (ScanQA). Scene Description: describe global scene. Embodied Dialogue & Planning: multi-turn conversation and task planning in 3D indoor environments. Objectives: generate natural language answers and optionally output 3D coordinates/boxes encoded in text.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level natural language instructions and responses; coordinates and bounding boxes encoded as textual tokens (e.g. "<loc>x,y,z</loc>", "<obj>cx,cy,cz,w,h,l</obj>").</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Outputs are text tokens (language) and discretized numeric coordinate tokens representing 3D locations/boxes; the model does not output low-level continuous motor commands — it outputs language and spatial coordinates for downstream usage.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Spatial outputs are represented as text: 3D points and boxes are discretized into unsigned integers (0–255) and embedded as plain text tokens; the MMT/Q-Former converts permutation-invariant 3D features and visual prompts into a fixed-length query prefix which is projected into the LLM embedding space so the frozen LLM can condition on 3D scene information and produce textual coordinate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Input point cloud (N points, with coordinates and optional per-point features: color, normal, height), optional visual prompts (user clicks or ROI box features from a pre-trained 3D detector), and textual instruction tokens. Scene encoder expects 40k (practically sampled 40k) point input and internally uses FPS downsampling to M tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>3D Dense Captioning (ScanRefer) C@0.5 = 65.19% (fine-tuned LL3DA); Nr3D C@0.5 = 51.18%. 3D Question Answering (ScanQA) CiDEr (val) = 76.79; test w/ object = 78.16; test w/o object = 70.29. Zero-shot / generalist runs and other metrics shown throughout paper (BLEU-4, METEOR, Rouge-L) reflect competitive state-of-the-art performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Models trained from scratch in the same LL3DA architecture (no generalist instruction-tuned initialization) achieved lower results: ScanRefer(scratch) C@0.5 = 62.84% (Table 5); ScanQA(scratch) CiDEr = 74.80 (Table 5). Language-only LLMs evaluated zero-shot (no 3D prefix) perform poorly on many tasks (see Supplement Table 10: OPT-1.3B zero-shot Scene Description: CiDEr ≈ 0.00, BLEU-4 0.84; Embodied Dialogue / Planning extremely low), demonstrating benefit of the multimodal transfer pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Instruction-tuning stage: total ~100k iterations reported for full multimodal instruction tuning; per-task fine-tuning to reach final reported task performance used ≈30k iterations. Using generalist (instruction-tuned) weights as initialization improved downstream fine-tuning results (same iteration budget).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Scratch models (same architecture but trained from scratch) were trained with the same per-task fine-tune budget (~30k iterations) and reached lower final performance (e.g., ScanRefer(scratch) C@0.5 = 62.84% vs fine-tuned 65.19%). Exact sample/episode counts (in terms of scenes or episodes) are not provided; iteration counts are reported instead.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>No explicit reduction in iteration counts reported; empirically, starting from the instruction-tuned generalist weights produced higher final accuracy with the same training iterations (example: +2.35% absolute C@0.5 on ScanRefer when fine-tuning from the generalist initialization vs training from scratch with same ~30k iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key contributors: use of a frozen pretrained LLM providing strong language generation priors; explicit coordinate-to-text mapping (discretization) allowing LLM to emit spatial outputs; an interaction-aware Multi-Modal Transformer (Q-Former) that reconciles permutation-invariant 3D embeddings with the LLM input order; provisioning of visual prompts (clicks/boxes) to disambiguate objects; frozen scene encoder pretrained on ScanNet detection providing strong geometry-aware features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations include: frozen LLM and frozen scene encoder restrict end-to-end adaptation of base modalities; potential mismatch between permutation-invariant 3D embeddings and position-sensitive LLM inputs (mitigated but not fully removed by Q-Former); lack of low-level continuous action outputs (LL3DA emits text+coordinates rather than motor commands); no explicit sample-efficiency speedups (iterations to convergence similar), so sample complexity gains are modest and reported only as final-performance improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language models (OPT family) can be successfully leveraged as frozen backbones for 3D embodied-language tasks when (a) 3D geometry is converted into a fixed-length interaction-aware prefix via a Q-Former/MMT, and (b) spatial outputs are encoded as text tokens; this yields state-of-the-art performance on 3D dense captioning and 3D QA, and improves fine-tuning outcomes compared to training from scratch, but does not eliminate the need for substantial multimodal instruction tuning (tens of thousands of iterations). Visual prompts (clicks/boxes) substantially reduce ambiguity and improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1845.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1845.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPT-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPT-1.3B (Open Pre-trained Transformer 1.3B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal language model from the OPT family used as the frozen LLM backbone in LL3DA; provides language-generation priors and token embeddings which the multimodal prefix conditions on.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>OPT-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A 1.3B-parameter autoregressive transformer pretrained as a causal language model (OPT family) and used frozen as the generation backbone in LL3DA. In this paper OPT-1.3B is loaded in float16 to save memory.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>General web-scale text (causal language model pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper references OPT-1.3B [58] as the pre-trained LLM backbone but does not enumerate the exact pretraining corpus in this work; OPT family is cited as the source for the LLM weights.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used to conditionally generate outputs for 3D Dense Captioning, 3D QA, Scene Description, Embodied Dialogue and Planning when provided with a multimodal prefix derived from 3D inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See LL3DA entry: all tasks operate on ScanNet indoor point clouds and require language outputs and optionally textualized 3D coordinates/boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Receives high-level natural language prompts and a projected multimodal prefix; generates natural language responses and textual coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Outputs text tokens representing language responses and discretized coordinate tokens used to encode 3D locations/boxes; does not directly output low-level motor actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LL3DA projects 32 query embeddings from the Q-Former into the LLM embedding space as a prefix; coordinate and box tokens are encoded as plain text tokens in discretized integer ranges (0–255) so OPT can output them as normal tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>No direct perception; relies on multimodal prefix to supply 3D/visual information. In LL3DA the prefix encodes point cloud features and visual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When integrated into LL3DA the OPT-1.3B backbone contributed to final task performances: e.g., ScanRefer C@0.5 = 65.19%, ScanQA CiDEr val = 76.79 (LL3DA overall numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Zero-shot OPT-1.3B (no multimodal prefix) performs poorly on 3D tasks (Supplement Table 10): Scene Description CiDEr ≈ 0.00, BLEU-4 = 0.84; Embodied Dialogue BLEU-4 ≈ 0.23; Embodied Planning BLEU-4 ≈ 0.13 — showing that language pretraining alone without a multimodal conditioning prefix is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>In LL3DA the OPT-1.3B backbone is kept frozen; instruction-tuning used ≈100k iterations overall and per-task fine-tuning ≈30k iterations to reach reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not applicable (OPT is not trained from scratch in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not directly quantified as a reduction in iterations, but integrating OPT as a frozen backbone with a learned multimodal prefix enabled strong final performance that purely language-only zero-shot OPT could not reach.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Rich language modeling priors and generation capabilities of OPT; ability to emit discretized numeric tokens when conditioned by the multimodal prefix; stability from freezing large LLM weights.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>OPT alone (zero-shot) lacks access to 3D evidence and spatial grounding; without an appropriate multimodal prefix it cannot answer 3D tasks effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A frozen causal LLM pretrained on language (OPT-1.3B) can be effectively conditioned by a learned fixed-length multimodal prefix to perform 3D embodied-language tasks, but the multimodal prefix and mapping of geometry to textual tokens are essential for success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1845.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1845.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPT / LLAMA (zero-shot baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPT family (OPT-2.7B, OPT-6.7B) and LLAMA-7B zero-shot baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Language-only LLMs evaluated zero-shot on 3D tasks (scene description, embodied dialogue, embodied planning) in the supplementary experiments; they show very low baseline performance without multimodal conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>OPT-2.7B; OPT-6.7B; LLAMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Large causal language models pretrained on language-only corpora. In supplementary Table 10 they are evaluated zero-shot (no multimodal prefix) on 3D tasks as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Causal language model pretraining on large web text corpora (language-only).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper reports zero-shot scores for OPT-2.7B, OPT-6.7B, and LLAMA-7B but does not detail pretraining corpora (these are referenced via the respective model papers).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Zero-shot evaluation on Scene Description, Embodied Dialogue, Embodied Planning (ScanNet subset of 3D-LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same 3D tasks as above but these LLMs received no multimodal prefix, so evaluation is purely language-only zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language (prompts and responses).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not applicable (no embodied outputs beyond text tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Supplement Table 10 (zero-shot): example Scene Description - OPT-1.3B BLEU-4=0.84, CiDEr=0.00; OPT-6.7B BLEU-4=1.13, CiDEr=0.06; LLAMA-7B BLEU-4=0.92, CiDEr=0.20. For Embodied Dialogue and Planning zero-shot BLEU/CiDEr are also extremely low compared to LL3DA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Not successful zero-shot because no modality-grounding prefix was provided; demonstrates that raw language pretraining alone is insufficient for 3D tasks without multimodal conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Absence of 3D-conditioned prefix / perception causes poor performance; mismatch between language-only pretraining and required grounded spatial/visual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero-shot LLMs (OPT variants, LLAMA) perform poorly on 3D grounded tasks, indicating the importance of a learned multimodal prefix and geometric grounding for transferring language models to embodied 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1845.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1845.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LLM (Injecting the 3D world into large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior LLM-driven approach that reconstructs 3D features from multi-view images and conditions LLMs for 3D understanding and generation; cited and compared in this work as a generation-based baseline for 3D-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3d-llm: Injecting the 3d world into large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An LLM-driven solution that aggregates multi-view visual features into representations used to condition a language model for 3D tasks; categorized in the paper as a generation-based (GEN) method for 3D-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal (multi-view image features projected into 3D) used with LLMs; original 3D-LLM paper likely used vision-language pretraining strategies but this paper only references it at a high level.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D Question Answering (ScanQA) and general 3D understanding tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Uses reconstructed 3D features from multi-view images to enable LLM to answer questions about 3D scenes (generation of text answers).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language questions and generated textual answers.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Textual outputs referencing objects/locations; no low-level motor actions described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not detailed in this paper; described at a high level as aggregating multi-view features to condition LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multi-view images reconstructed into 3D features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in Table 2 as 3D-LLM* (fine-tuned) CiDEr (val) = 69.40% (generation-based baseline), which LL3DA surpasses by +7.39% CiDEr on validation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Aggregation of multi-view image features into LLM conditioning provides 3D context for language generation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Use of multi-view images can lead to high computation and may ignore intrinsic 3D geometry (criticized in this paper); may have scalability or efficiency limits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>3D-LLM demonstrates a route for conditioning LLMs with reconstructed 3D information (via multi-view images) to perform generation-based 3D-QA, but LL3DA argues for direct point-cloud encoding and interaction-aware prefixes as a more efficient/geometry-preserving alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1845.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1845.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Point-Bind / Point-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point-Bind & Point-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent methods aligning point cloud representations with multi-modal/language spaces to enable 3D understanding, generation, and instruction following; cited as related work demonstrating success in 3D object understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Point-Bind / Point-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Approaches that align point cloud features with language and other modalities to enable large-model instruction-following and 3D tasks; referenced as demonstrating remarkable success for 3D object reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal alignment between point clouds and language/other modalities (paper reference only; specifics not provided in this text).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D understanding, generation, and instruction following (general 3D tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Enables reasoning about 3D objects and likely supports downstream tasks that require language-conditioned 3D understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions and responses (as aligned modalities).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Textualized 3D outputs or aligned embeddings; specific low-level actions not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Point cloud input alignment to language embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Alignment between spatial (point cloud) representations and language spaces enables instruction following and reasoning about 3D objects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper cites Point-Bind/Point-LLM as part of a trend aligning 3D point representations with language to empower LLM-style capabilities on 3D data; detailed methods and empirical numbers must be obtained from the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>3d-llm: Injecting the 3d world into large language models <em>(Rating: 2)</em></li>
                <li>Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following <em>(Rating: 2)</em></li>
                <li>InstructBLIP: Towards general-purpose vision-language models with instruction tuning <em>(Rating: 1)</em></li>
                <li>LLM-planner: Few-shot grounded planning for embodied agents with large language models <em>(Rating: 2)</em></li>
                <li>PALME: An embodied multimodal language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1845",
    "paper_id": "paper-fc53f8f3a84f1fc4993689d8f98cf6551d07a22d",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "LL3DA",
            "name_full": "Large Language 3D Assistant (LL3DA)",
            "brief_description": "An LLM-driven multi-modal 3D agent that directly consumes point cloud input plus optional visual prompts (clicks/boxes) and textual instructions via an Interactor3D (Q-Former / MMT) which projects a fixed-length interaction-aware prefix into a frozen causal LLM (OPT-1.3B) to generate free-form text (including coordinate tokens).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "LL3DA (Large Language 3D Assistant)",
            "model_agent_description": "Auto-regressive, LLM-driven model that (1) uses a frozen scene encoder pre-trained on ScanNet detection to produce point features, (2) encodes visual prompts (clicks/ROIs) and textual instructions in a Multi-Modal Transformer (Q-Former) that aggregates into 32 learnable query tokens, (3) linearly projects those queries to form a prefix input to a frozen causal LLM (OPT-1.3B) which generates natural language outputs and coordinate tokens. Total trainable parameters ≈111M (most weights frozen).",
            "pretraining_data_type": "language model pretraining (causal LLM) + 3D detection pretraining for scene encoder",
            "pretraining_data_details": "The LLM backbone is OPT-1.3B (cited [58]) — a causal language model pretrained on large-scale internet text (OPT family). The 3D scene encoder is a masked transformer encoder pretrained on ScanNet detection (ScanNet detection annotations). The Q-Former loads BERT word and positional embeddings for some initializations. Exact OPT corpus details are referenced via OPT but not enumerated in this paper.",
            "embodied_task_name": "3D Dense Captioning; 3D Question Answering; Scene Description; Embodied Dialogue; Embodied Planning; (also evaluated for generating 3D bounding boxes / open-vocabulary detection)",
            "embodied_task_description": "Tasks are performed on indoor 3D scenes from ScanNet (point cloud inputs). 3D Dense Captioning: localize and describe instances (ScanRefer, Nr3D). 3D Question Answering: answer questions about the 3D scene (ScanQA). Scene Description: describe global scene. Embodied Dialogue & Planning: multi-turn conversation and task planning in 3D indoor environments. Objectives: generate natural language answers and optionally output 3D coordinates/boxes encoded in text.",
            "action_space_text": "High-level natural language instructions and responses; coordinates and bounding boxes encoded as textual tokens (e.g. \"&lt;loc&gt;x,y,z&lt;/loc&gt;\", \"&lt;obj&gt;cx,cy,cz,w,h,l&lt;/obj&gt;\").",
            "action_space_embodied": "Outputs are text tokens (language) and discretized numeric coordinate tokens representing 3D locations/boxes; the model does not output low-level continuous motor commands — it outputs language and spatial coordinates for downstream usage.",
            "action_mapping_method": "Spatial outputs are represented as text: 3D points and boxes are discretized into unsigned integers (0–255) and embedded as plain text tokens; the MMT/Q-Former converts permutation-invariant 3D features and visual prompts into a fixed-length query prefix which is projected into the LLM embedding space so the frozen LLM can condition on 3D scene information and produce textual coordinate outputs.",
            "perception_requirements": "Input point cloud (N points, with coordinates and optional per-point features: color, normal, height), optional visual prompts (user clicks or ROI box features from a pre-trained 3D detector), and textual instruction tokens. Scene encoder expects 40k (practically sampled 40k) point input and internally uses FPS downsampling to M tokens.",
            "transfer_successful": true,
            "performance_with_pretraining": "3D Dense Captioning (ScanRefer) C@0.5 = 65.19% (fine-tuned LL3DA); Nr3D C@0.5 = 51.18%. 3D Question Answering (ScanQA) CiDEr (val) = 76.79; test w/ object = 78.16; test w/o object = 70.29. Zero-shot / generalist runs and other metrics shown throughout paper (BLEU-4, METEOR, Rouge-L) reflect competitive state-of-the-art performance.",
            "performance_without_pretraining": "Models trained from scratch in the same LL3DA architecture (no generalist instruction-tuned initialization) achieved lower results: ScanRefer(scratch) C@0.5 = 62.84% (Table 5); ScanQA(scratch) CiDEr = 74.80 (Table 5). Language-only LLMs evaluated zero-shot (no 3D prefix) perform poorly on many tasks (see Supplement Table 10: OPT-1.3B zero-shot Scene Description: CiDEr ≈ 0.00, BLEU-4 0.84; Embodied Dialogue / Planning extremely low), demonstrating benefit of the multimodal transfer pipeline.",
            "sample_complexity_with_pretraining": "Instruction-tuning stage: total ~100k iterations reported for full multimodal instruction tuning; per-task fine-tuning to reach final reported task performance used ≈30k iterations. Using generalist (instruction-tuned) weights as initialization improved downstream fine-tuning results (same iteration budget).",
            "sample_complexity_without_pretraining": "Scratch models (same architecture but trained from scratch) were trained with the same per-task fine-tune budget (~30k iterations) and reached lower final performance (e.g., ScanRefer(scratch) C@0.5 = 62.84% vs fine-tuned 65.19%). Exact sample/episode counts (in terms of scenes or episodes) are not provided; iteration counts are reported instead.",
            "sample_complexity_gain": "No explicit reduction in iteration counts reported; empirically, starting from the instruction-tuned generalist weights produced higher final accuracy with the same training iterations (example: +2.35% absolute C@0.5 on ScanRefer when fine-tuning from the generalist initialization vs training from scratch with same ~30k iterations).",
            "transfer_success_factors": "Key contributors: use of a frozen pretrained LLM providing strong language generation priors; explicit coordinate-to-text mapping (discretization) allowing LLM to emit spatial outputs; an interaction-aware Multi-Modal Transformer (Q-Former) that reconciles permutation-invariant 3D embeddings with the LLM input order; provisioning of visual prompts (clicks/boxes) to disambiguate objects; frozen scene encoder pretrained on ScanNet detection providing strong geometry-aware features.",
            "transfer_failure_factors": "Limitations include: frozen LLM and frozen scene encoder restrict end-to-end adaptation of base modalities; potential mismatch between permutation-invariant 3D embeddings and position-sensitive LLM inputs (mitigated but not fully removed by Q-Former); lack of low-level continuous action outputs (LL3DA emits text+coordinates rather than motor commands); no explicit sample-efficiency speedups (iterations to convergence similar), so sample complexity gains are modest and reported only as final-performance improvements.",
            "key_findings": "Pretrained language models (OPT family) can be successfully leveraged as frozen backbones for 3D embodied-language tasks when (a) 3D geometry is converted into a fixed-length interaction-aware prefix via a Q-Former/MMT, and (b) spatial outputs are encoded as text tokens; this yields state-of-the-art performance on 3D dense captioning and 3D QA, and improves fine-tuning outcomes compared to training from scratch, but does not eliminate the need for substantial multimodal instruction tuning (tens of thousands of iterations). Visual prompts (clicks/boxes) substantially reduce ambiguity and improve performance.",
            "uuid": "e1845.0",
            "source_info": {
                "paper_title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "OPT-1.3B",
            "name_full": "OPT-1.3B (Open Pre-trained Transformer 1.3B)",
            "brief_description": "A causal language model from the OPT family used as the frozen LLM backbone in LL3DA; provides language-generation priors and token embeddings which the multimodal prefix conditions on.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "OPT-1.3B",
            "model_agent_description": "A 1.3B-parameter autoregressive transformer pretrained as a causal language model (OPT family) and used frozen as the generation backbone in LL3DA. In this paper OPT-1.3B is loaded in float16 to save memory.",
            "pretraining_data_type": "General web-scale text (causal language model pretraining)",
            "pretraining_data_details": "The paper references OPT-1.3B [58] as the pre-trained LLM backbone but does not enumerate the exact pretraining corpus in this work; OPT family is cited as the source for the LLM weights.",
            "embodied_task_name": "Used to conditionally generate outputs for 3D Dense Captioning, 3D QA, Scene Description, Embodied Dialogue and Planning when provided with a multimodal prefix derived from 3D inputs.",
            "embodied_task_description": "See LL3DA entry: all tasks operate on ScanNet indoor point clouds and require language outputs and optionally textualized 3D coordinates/boxes.",
            "action_space_text": "Receives high-level natural language prompts and a projected multimodal prefix; generates natural language responses and textual coordinates.",
            "action_space_embodied": "Outputs text tokens representing language responses and discretized coordinate tokens used to encode 3D locations/boxes; does not directly output low-level motor actions.",
            "action_mapping_method": "LL3DA projects 32 query embeddings from the Q-Former into the LLM embedding space as a prefix; coordinate and box tokens are encoded as plain text tokens in discretized integer ranges (0–255) so OPT can output them as normal tokens.",
            "perception_requirements": "No direct perception; relies on multimodal prefix to supply 3D/visual information. In LL3DA the prefix encodes point cloud features and visual prompts.",
            "transfer_successful": true,
            "performance_with_pretraining": "When integrated into LL3DA the OPT-1.3B backbone contributed to final task performances: e.g., ScanRefer C@0.5 = 65.19%, ScanQA CiDEr val = 76.79 (LL3DA overall numbers).",
            "performance_without_pretraining": "Zero-shot OPT-1.3B (no multimodal prefix) performs poorly on 3D tasks (Supplement Table 10): Scene Description CiDEr ≈ 0.00, BLEU-4 = 0.84; Embodied Dialogue BLEU-4 ≈ 0.23; Embodied Planning BLEU-4 ≈ 0.13 — showing that language pretraining alone without a multimodal conditioning prefix is insufficient.",
            "sample_complexity_with_pretraining": "In LL3DA the OPT-1.3B backbone is kept frozen; instruction-tuning used ≈100k iterations overall and per-task fine-tuning ≈30k iterations to reach reported results.",
            "sample_complexity_without_pretraining": "Not applicable (OPT is not trained from scratch in this work).",
            "sample_complexity_gain": "Not directly quantified as a reduction in iterations, but integrating OPT as a frozen backbone with a learned multimodal prefix enabled strong final performance that purely language-only zero-shot OPT could not reach.",
            "transfer_success_factors": "Rich language modeling priors and generation capabilities of OPT; ability to emit discretized numeric tokens when conditioned by the multimodal prefix; stability from freezing large LLM weights.",
            "transfer_failure_factors": "OPT alone (zero-shot) lacks access to 3D evidence and spatial grounding; without an appropriate multimodal prefix it cannot answer 3D tasks effectively.",
            "key_findings": "A frozen causal LLM pretrained on language (OPT-1.3B) can be effectively conditioned by a learned fixed-length multimodal prefix to perform 3D embodied-language tasks, but the multimodal prefix and mapping of geometry to textual tokens are essential for success.",
            "uuid": "e1845.1",
            "source_info": {
                "paper_title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "OPT / LLAMA (zero-shot baselines)",
            "name_full": "OPT family (OPT-2.7B, OPT-6.7B) and LLAMA-7B zero-shot baselines",
            "brief_description": "Language-only LLMs evaluated zero-shot on 3D tasks (scene description, embodied dialogue, embodied planning) in the supplementary experiments; they show very low baseline performance without multimodal conditioning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "OPT-2.7B; OPT-6.7B; LLAMA-7B",
            "model_agent_description": "Large causal language models pretrained on language-only corpora. In supplementary Table 10 they are evaluated zero-shot (no multimodal prefix) on 3D tasks as baselines.",
            "pretraining_data_type": "Causal language model pretraining on large web text corpora (language-only).",
            "pretraining_data_details": "The paper reports zero-shot scores for OPT-2.7B, OPT-6.7B, and LLAMA-7B but does not detail pretraining corpora (these are referenced via the respective model papers).",
            "embodied_task_name": "Zero-shot evaluation on Scene Description, Embodied Dialogue, Embodied Planning (ScanNet subset of 3D-LLM).",
            "embodied_task_description": "Same 3D tasks as above but these LLMs received no multimodal prefix, so evaluation is purely language-only zero-shot.",
            "action_space_text": "Natural language (prompts and responses).",
            "action_space_embodied": "Not applicable (no embodied outputs beyond text tokens).",
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": false,
            "performance_with_pretraining": "Supplement Table 10 (zero-shot): example Scene Description - OPT-1.3B BLEU-4=0.84, CiDEr=0.00; OPT-6.7B BLEU-4=1.13, CiDEr=0.06; LLAMA-7B BLEU-4=0.92, CiDEr=0.20. For Embodied Dialogue and Planning zero-shot BLEU/CiDEr are also extremely low compared to LL3DA.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Not successful zero-shot because no modality-grounding prefix was provided; demonstrates that raw language pretraining alone is insufficient for 3D tasks without multimodal conditioning.",
            "transfer_failure_factors": "Absence of 3D-conditioned prefix / perception causes poor performance; mismatch between language-only pretraining and required grounded spatial/visual reasoning.",
            "key_findings": "Zero-shot LLMs (OPT variants, LLAMA) perform poorly on 3D grounded tasks, indicating the importance of a learned multimodal prefix and geometric grounding for transferring language models to embodied 3D tasks.",
            "uuid": "e1845.2",
            "source_info": {
                "paper_title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "3D-LLM",
            "name_full": "3D-LLM (Injecting the 3D world into large language models)",
            "brief_description": "Prior LLM-driven approach that reconstructs 3D features from multi-view images and conditions LLMs for 3D understanding and generation; cited and compared in this work as a generation-based baseline for 3D-QA.",
            "citation_title": "3d-llm: Injecting the 3d world into large language models",
            "mention_or_use": "mention",
            "model_agent_name": "3D-LLM",
            "model_agent_description": "An LLM-driven solution that aggregates multi-view visual features into representations used to condition a language model for 3D tasks; categorized in the paper as a generation-based (GEN) method for 3D-QA.",
            "pretraining_data_type": "Multimodal (multi-view image features projected into 3D) used with LLMs; original 3D-LLM paper likely used vision-language pretraining strategies but this paper only references it at a high level.",
            "pretraining_data_details": null,
            "embodied_task_name": "3D Question Answering (ScanQA) and general 3D understanding tasks",
            "embodied_task_description": "Uses reconstructed 3D features from multi-view images to enable LLM to answer questions about 3D scenes (generation of text answers).",
            "action_space_text": "Natural language questions and generated textual answers.",
            "action_space_embodied": "Textual outputs referencing objects/locations; no low-level motor actions described in this paper.",
            "action_mapping_method": "Not detailed in this paper; described at a high level as aggregating multi-view features to condition LLM.",
            "perception_requirements": "Multi-view images reconstructed into 3D features.",
            "transfer_successful": null,
            "performance_with_pretraining": "Reported in Table 2 as 3D-LLM* (fine-tuned) CiDEr (val) = 69.40% (generation-based baseline), which LL3DA surpasses by +7.39% CiDEr on validation.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Aggregation of multi-view image features into LLM conditioning provides 3D context for language generation.",
            "transfer_failure_factors": "Use of multi-view images can lead to high computation and may ignore intrinsic 3D geometry (criticized in this paper); may have scalability or efficiency limits.",
            "key_findings": "3D-LLM demonstrates a route for conditioning LLMs with reconstructed 3D information (via multi-view images) to perform generation-based 3D-QA, but LL3DA argues for direct point-cloud encoding and interaction-aware prefixes as a more efficient/geometry-preserving alternative.",
            "uuid": "e1845.3",
            "source_info": {
                "paper_title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Point-Bind / Point-LLM",
            "name_full": "Point-Bind & Point-LLM",
            "brief_description": "Recent methods aligning point cloud representations with multi-modal/language spaces to enable 3D understanding, generation, and instruction following; cited as related work demonstrating success in 3D object understanding.",
            "citation_title": "Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following",
            "mention_or_use": "mention",
            "model_agent_name": "Point-Bind / Point-LLM",
            "model_agent_description": "Approaches that align point cloud features with language and other modalities to enable large-model instruction-following and 3D tasks; referenced as demonstrating remarkable success for 3D object reasoning.",
            "pretraining_data_type": "Multimodal alignment between point clouds and language/other modalities (paper reference only; specifics not provided in this text).",
            "pretraining_data_details": null,
            "embodied_task_name": "3D understanding, generation, and instruction following (general 3D tasks)",
            "embodied_task_description": "Enables reasoning about 3D objects and likely supports downstream tasks that require language-conditioned 3D understanding.",
            "action_space_text": "Natural language instructions and responses (as aligned modalities).",
            "action_space_embodied": "Textualized 3D outputs or aligned embeddings; specific low-level actions not described in this paper.",
            "action_mapping_method": null,
            "perception_requirements": "Point cloud input alignment to language embeddings.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Alignment between spatial (point cloud) representations and language spaces enables instruction following and reasoning about 3D objects.",
            "transfer_failure_factors": "Not specified in this paper.",
            "key_findings": "The paper cites Point-Bind/Point-LLM as part of a trend aligning 3D point representations with language to empower LLM-style capabilities on 3D data; detailed methods and empirical numbers must be obtained from the cited work.",
            "uuid": "e1845.4",
            "source_info": {
                "paper_title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "3d-llm: Injecting the 3d world into large language models",
            "rating": 2
        },
        {
            "paper_title": "Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following",
            "rating": 2
        },
        {
            "paper_title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning",
            "rating": 1
        },
        {
            "paper_title": "LLM-planner: Few-shot grounded planning for embodied agents with large language models",
            "rating": 2
        },
        {
            "paper_title": "PALME: An embodied multimodal language model",
            "rating": 1
        }
    ],
    "cost": 0.02171925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</h1>
<p>Sijin Chen ${ }^{1}$ Xin Chen ${ }^{2, <em>}$ Chi Zhang ${ }^{2}$ Mingsheng Li ${ }^{1}$ Gang YU ${ }^{2}$<br>Hao Fei ${ }^{3}$ Hongyuan Zhu ${ }^{4}$ Jiayuan Fan ${ }^{1}$ Tao Chen ${ }^{1, \dagger}$<br>${ }^{1}$ Fudan University ${ }^{2}$ Tencent PCG ${ }^{3}$ National University of Singapore<br>${ }^{4}$ Institute for Infocomm Research ( ${ }^{2} \mathrm{R}$ ) \&amp; Centre for Frontier AI Research (CFAR), A</em>STAR, Singapore<br>https://github.com/Open3DA/LL3DA<br>* project lead ${ }^{\dagger}$ corresponding author</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. We propose LL3DA, a Large Language 3D Assistant that demonstrates mighty instruction-following capacities of understanding, reasoning, and planning in complex 3D environments. LL3DA takes both the textual instructions and potential visual interactions into consideration to help remove ambiguities when addressing various tasks in diverse 3D scenes.</p>
<h2>Abstract</h2>
<p>Recent advances in Large Multimodal Models (LMM) have made it possible for various applications in humanmachine interactions. However, developing LMMs that can comprehend, reason, and plan in complex and diverse 3D environments remains a challenging topic, especially considering the demand for understanding permutationinvariant point cloud 3D representations of the 3D scene. Existing works seek help from multi-view images, and project 2D features to 3D space as 3D scene representations. This, however, leads to huge computational overhead and performance degradation. In this paper, we present LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and respond to both textual-
instructions and visual-prompts. This help LMMs better comprehend human interactions and further help to remove the ambiguities in cluttered 3D scenes. Experiments show that LL3DA achieves remarkable results, and surpasses various 3D vision-language models on both 3D Dense Captioning and 3D Question Answering.</p>
<h2>1. Introduction</h2>
<p>The recent surge in Large Language Model (LLM) families $[13,27,41,49,58]$ opens up great opportunities for solving various machine learning tasks in a generalized way $[16,28,32,34]$. During this LLM carnival, researchers are also seeking generalized LLM solutions to various vision language tasks [16, 54, 59]. Among these, LLM-based 3D scene understanding is a valuable topic that would benefit</p>
<p>the development of autonomous driving [8, 22] and embodied AI agents [20, 47]. However, it is also challenging given 1) the diversity and complication of 3D environments and 2) the demands for understanding sparse 3D points.</p>
<p>Prior works have made initial success addressing various 3D vision and language tasks. The mainstream of researches build 3D specialists aiming at solving one specific down-stream task, including 3D Question Answering (3DQA) [2, 37], 3D Visual Grounding (3D-VG) [6, 52], and 3D Dense Captioning (3D-DC) [9, 11]. Meanwhile, other works [4, 12, 30, 65] study the mutual promotion of different 3D vision and language tasks with shared structure modelling relations among objects. Recently, researchers have also introduced LLMs for general purpose 3D understanding, where Point-Bind and Point-LLMs [23, 54] mainly focus on the understanding of 3D objects. Concurrently, 3DLLM [26] proposes an LLM-driven solution that aggregates multi-view features for 3D features, presenting mighty capacities a machine could understand various 3D object and scenes, and follow textual instructions produced by human.</p>
<p>Though these methods have achieved remarkable success addressing different challenges in understanding 3D worlds with natural language, there are certain limitations. With limited supervision, 3D specialists could hardly scaleup for better performance, while the joint pre-training still requires separate heads for specific tasks. Extracting mutliview features results in huge computational overhead, and ignores the essential geometry properties. Additionally, plain texts often lead to ambiguities especially in cluttered and complex 3D environments.</p>
<p>To address the above issues, we propose the LL3DA, a Large Language 3D Assistant that could respond to both textual and visual interactions from human, and understand, reason, and plan in complex 3D environments (Fig. 1). We adopt a multi-modal transformer that aggregates information from textual instructions, visual prompts, and 3D scene into a fixed length of learnable querying tokens via the attention mechanism. The querying tokens are projected and used as the prefix of the textual instructions, serving as the input to a pre-trained and frozen LLM. This design not only helps to address the contradiction between the permutationinvariant 3D scene embeddings with the LLM embedding space, but also extracts interaction-aware 3D scene embeddings for efficient instruction following.</p>
<p>We conduct extensive experiments to explore the capacities of LLMs in understanding, reasoning, and planning within complex and diverse 3D environments. Our model achieves state-of-the-art results on two widely used datasets for 3D Dense Captioning [1, 6], and 3D Question Answering [2]. Additionally, by introducing additional visual interactions, our method could further remove the ambiguities within the vague textual instructions.</p>
<p>To summarize, our key contributions lie in:</p>
<ul>
<li>We present a LLM-based solution for understanding, reasoning, and planning in complex 3D environments.</li>
<li>Our model takes both the textual instructions and visual interactions as inputs, and extracts interaction-aware features for effective instruction-following.</li>
<li>Extensive experiments show that our method surpasses various state-of-the-art 3D vision language models.</li>
</ul>
<h2>2. Related Work</h2>
<p>3D Vision and Language alignment, pre-training, and understanding [6, 19, 65] covers a bunch of tasks requiring a model to adopt its understanding towards a complex 3D scene answering to, or answering with natural language. Among those, 3D Dense Captioning (3D-DC) [9, 11, 51] expects a model to translate an input 3D scene into a set of instance coordinates and natural language descriptions. Existing methods could be categorized into "detect-thendescribe" models [4, 11, 51] and the "set-to-set" prediction approaches [9, 10]. The former builds explicit relations on the instance coordinate predictions, while the latter direct learns the location and description of instances from the input 3D scene. 3D Visual Grounding (3D-VG) [1, 6, 52] demands a model to respond the natural language queries with the instance coordinates in the 3D scene. The mainstream of existing methods $[4,60,65]$ address 3D-VG via selecting a candidate from a 3D detector's prediction. 3D Question Answering (3D-QA) [2, 37, 56, 61] requires a model to answer the questions with natural language based on the input 3D scene. The majority of existing methods [2, 17, 43] directly select the desired response from a given answer set. Researchers have also studied the mutual promotion of various 3D vision language tasks via training their shareable architectures simultaneously on different tasks [4, 12, 30, 65]. UniT3D [12] and 3DJCG [4] focus on the joint promotion between 3D-DC and 3D-VG in the relation modelling, while 3D-VLP [30] further includes 3D-QA. Recently, 3DLLM [26] introduce a family of LLM-driven 3D generalists that could handle diverse textual instructions with the reconstructed 3D feature from multi-view images [25]. In this paper, we present LL3DA, an LLM solution that directly extracts features from the 3D scene, and could handle both visual prompts and textual instructions to diversify the possible interactions human could make with the complex 3D environment.</p>
<p>Large Multimodal Models (LMM). Along with the rapid development of Large Language Models (LLM) [14, 58], researchers have made great recent efforts adapting LLMs to visual understanding and reasoning tasks [53, 57]. Some project or compress global image features as prefix for instruction following [32, 34, 55, 64], while others extract ROI features as LLM tokens for region-oriented instruction reasoning [5, 59]. Meanwhile, InstructBLIP [16] proposes</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Overview of the Proposed Approach. (a) The overall pipeline of our proposed LL3DA first extracts interaction-aware 3D scene embeddings, which are later projected to the prefix of textual instructions as the input of a frozen LLM. (b) The detailed design of the Interactor3D, which aggregates visual prompts, textual instructions, and 3D scene embeddings into a fixed length querying tokens. (c) The prompt encoder encodes the user clicks and box coordinates with the positional embeddings and ROI features, respectively.</p>
<p>to extract textual instruction-aware visual features, and has achieved remarkable success addressing complex and unseen instructions. Concurrently, researchers have also made great attempts solving various 3D tasks using LLMs. Notably, [23, 36, 54, 63] demonstrate remarkable success in understanding and reasoning about 3D objects. In this paper, we present an LLM-driven solution that could handle both interactions in forms of visual prompts and textual instructions, and propose to extract interaction-aware 3D scene representations for better instruction following.</p>
<h2>3. Methodology</h2>
<p>To build a general purpose agent that could handle both visual and textual interactions in complex 3D environments, we propose LL3DA, an LLM driven auto-regressive approach to 3D vision language tasks. In this section, we first introduce the problem formatting in Sec. 3.1. Next, we introduce our model design in details (Sec. 3.2).</p>
<h3>3.1. Problem Formatting</h3>
<p><strong>Model I/O.</strong> As shown in Fig. 2 (a), the input of our model consists of a 3D scene represented by a set of points <em>PC</em>, the textual instruction <em>I<sub>t</sub></em>, and potential visual interactions <em>I<sub>v</sub></em> that serve as supplementary spatial identifiers. Here, point cloud <em>PC</em> = [<em>p<sub>in</sub></em>, <em>f<sub>in</sub></em>] ∈ ℝ<sup><em>N</em>×(3+F)</sup>, where <em>p<sub>in</sub></em> ∈ ℝ<sup><em>N</em>×3</sup> and <em>f<sub>in</sub></em> ∈ ℝ<sup><em>N</em>×F</sup> are the point coordinates and the additional point features, including <em>color</em>, <em>normal</em>, and <em>height</em>. The output of our model is free-form natural language, part of whom could be interpreted into 3D coordinates.</p>
<p><strong>Instruction Formatting.</strong> Following existing LMMs [54], we begin the textual instructions <em>I<sub>t</sub></em> with the "### <em>human:</em>" identifier, and ask the model to generate responses after the "### <em>assistant:</em>" identifier. This endows the model with the ability to distinguish information from the context, and further engage in multi-turn conversations.</p>
<p><strong>Coordinate Representations.</strong> To provide LLMs with the capacity to perceive and respond with 3D coordinates, we convert the 3D points and 3D bounding boxes to plain texts. Specifically, a point is represented by "<loc>x, y, z</loc>", and a bounding box is represented by its center point and size, <em>i.e.</em> "<obj>c<sub>x</sub>, c<sub>y</sub>, c<sub>z</sub>, w, h, l</obj>". Here, all the numerical data is discretized into unsigned integers within a range of [0, 255] with respect to the boundary of the input 3D scene. This design could naturally fit in the vocabulary of existing pre-trained LLMs [49, 58]. Without introducing any additional learnable tokens, we could save the effort of tuning the whole LLM.</p>
<h3>3.2. Model Design</h3>
<p>As shown in Fig. 2 (a), our model first aggregates a fixed-length scene embeddings through the Interactor3D, which takes the visual prompts, the textual instructions, and the 3D scene as input. Next, the aggregated scene embeddings are projected to the prefix of textual instructions as inputs of a frozen LLM. The detailed design of Interactor3D is shown in Fig. 2 (b), which consists of a frozen 3D scene encoder <em>E<sup>3D</sup></em>, a visual prompt encoder, and a Q-Former to transform the permutation-invariant 3D embeddings into a fixed-length interaction-aware scene embedding, serving as the prefix of the LLM's input.</p>
<p><strong>Scene Encoder.</strong> We adopt the masked transformer encoder pre-trained on ScanNet detection [9] as the scene encoder,</p>
<p>$\mathcal{E}^{3 D}$, which takes $P C$ as its input, and outputs the 3D scene embeddings:</p>
<p>$$
f_{e n c}=\mathcal{E}^{3 D}(P C)=\mathcal{E}^{3 D}\left(p_{i n} ; f_{i n}\right) \in \mathbb{R}^{M \times d}
$$</p>
<p>Here, $f_{\text {enc }}$ consists of $d$-dimensioned features for $M$ points uniformly down-sampled from the input 3D scene through the Farthest Point Sampling (FPS) algorithm. In practice, we choose to keep the scene encoder frozen to save the memory cost during training.
Visual Prompt Encoder. We mainly take two common types of visual interactions into consideration, user clicks and 3D box annotations [31]. Each user click is first normalized within a range of $[0,1]$ by the size of the input 3D scene $p_{\text {click }} \in \mathbb{R}^{3}$. Then, we encode $p_{\text {click }}$ with the 3D Fourier positional embeddings [48] function:</p>
<p>$$
\operatorname{pos}\left(p_{\text {click }}\right)=\left[\sin \left(2 \pi p_{\text {click }} \cdot B\right) ; \cos \left(2 \pi p_{\text {click }} \cdot B\right)\right]
$$</p>
<p>Here, $B \in \mathbb{R}^{3 \times(d / 2)}$ is a learnable matrix. The box annotation is represented by the ROI feature $f_{\text {box }} \in \mathbb{R}^{d}$ extracted by a pre-trained 3D object detector [9]. The two types of the visual prompts are then projected with separate and identical Feed Forward Networks (FFN).</p>
<p>$$
\begin{aligned}
f_{\text {click }} &amp; =F F N_{\text {click }}\left(\operatorname{pos}\left(p_{\text {click }}\right)\right) \
f_{\text {box }} &amp; =F F N_{\text {box }}\left(f_{\text {box }}\right)
\end{aligned}
$$</p>
<p>In practice, we represent each visual prompt with 8 tokens.
Multi-Modal Transformer(MMT) serves as a role to 1) address the contradiction between permutation-invariant 3D scene embeddings and the position-sensitive causal LLMs, 2) bridge the gap between frozen unimodal experts, and 3) fill the needs for interaction-aware feature extraction. Inspired by the Q-Former architecture [16, 32], MMT aggregates the visual information within a fixed number of 32 learnable querying tokens. In each layer, the queries interact with the encoded visual prompts $\left[f_{\text {click }} ; f_{\text {box }}\right]$ and the textual instructions $\mathcal{I}<em c="c" e="e" n="n">{t}$ through a shared self-attention. Then, we allow the learnable querying tokens and the visual prompts to interact with the task-agnostic 3D scene embeddings $f</em>$, and are finally projected to the embedding space of LLM with a simple linear projector. In practice, we notice that initializing Q-Former with pre-trained BERT [18, 32] weights will lead to repetitive outputs, thus we only choose to initialize the pre-trained word and position embeddings from BERT.
LLM. We consider the decoder-only generative pre-trained transformers [49, 58] as our large language model backbone, which are sensitive to the input orders because of the position embeddings, and the causal attention mask. The parameters and the embedding layers of the LLM are kept
frozen to save memory cost. During inference, we generate the responses via searching for the response $s^{*}$ that satisfies:}$ via cross-attention. The output of MMT is 32 queries noted as $Q \in \mathbb{R}^{32 \times 768</p>
<p>$$
s^{*}=\arg \max <em t="t">{s} P\left(s \mid P C, \mathcal{I}</em>\right)
$$}, \mathcal{I}_{v</p>
<p>In practice, we use beam search with a beam size of 4.</p>
<h2>4. Multi-modal Instruction Tuning</h2>
<p>A general purpose agent is meant to deal with various tasks in complex 3D scenes. Apart from introducing proper training data, it is important to direct the model to generate the desired outputs. Thus, Sec. 4.1 will first introduce how we identify each task. After that, Sec. 4.2 will present details for training objective.</p>
<h3>4.1. Tasks and Instructions.</h3>
<p>All the following tasks will be modelled as auto-regressive generation after the " $# # #$ assistant:" identifier.
3D Dense Captioning requires the localization and description of instances in diverse 3D environments. We adopt either user clicks and box annotations as the visual prompt to identify the object to be described. Additionally, we design two types of textual instructions that ask the model to either "describe" or "describe and localize" the object, which diversifies the tasks, and leads to better performance.
3D Question Answering requires the model to generate response to the questions based on the global knowledge of a 3D scene. To help the model better understand the 3D environment, we also design two types of textual instructions that ask the model to either "answer" or "answer and localize the related objects". The latter serves as an auxiliary task widely adopted in various 3D-QA methods [2, 43]. To diversify the tasks during training, we randomly adopt visual prompts to the objects mentioned in the questions.
Scene Description requires the model to translate its global knowledge of the 3D scene into natural languages, thus we simply ask the "describe" this 3D scene.
Embodied Conversation and Planning could be treated as multi-turn conversation, which we use the " $# # #$ human:" and " $# # #$ assistant:" identifier to distinguish the source of information as introduced in Sec. 3.1.</p>
<h3>4.2. Instruction Following Tuning</h3>
<p>During training, for tasks that requires additional visual interactions, i.e. 3D-DC and 3D-QA, we randomly select the user clicks and box annotations.
Training Objective. Our training objective is to optimize the trainable parameters $\theta$, so that the likelihood of the target response sequence $s$ is maximized given the input point cloud $P C$, and the interactions $\mathcal{I}<em t="t">{v}$ and $\mathcal{I}</em>$ :</p>
<p>$$
\theta^{*}=\arg \max <em v="v">{\theta} P\left(s \mid P C ; \mathcal{I}</em> ; \theta\right)
$$} ; \mathcal{I}_{t</p>
<p>In practice, this is accomplished by adopting the token-wise cross-entropy loss that trains the model to predict the $i$ th token $s_{[i]}$ given the previous $(i-1)$ tokens, $s_{[1, \cdots, i-1]}$.</p>
<p>$$
\mathcal{L}(\theta)=-\sum_{i=1}^{|s|} \log P\left(s_{[i]} \mid P C ; \mathcal{I}<em t="t">{v} ; \mathcal{I}</em>\right)
$$} ; \theta ; s_{[1, \cdots, i-1]</p>
<p>Here, $|s|$ is the number of tokens in the desired response.</p>
<h2>5. Experiments</h2>
<p>To examine the capacity of our proposed model, we provide numerous evaluations. To begin with, we introduce the datasets, metrics, and implementation details (Sec. 5.1). Then, we compare our model's capacity of understanding and reasoning in complex 3D environments with previous 3D specialists on 3D Dense Captioning and 3D Question Answering (Sec. 5.2), and conduct quantitative ablation studies on the model design and training strategy (Sec. 5.3). Finally, Sec. 5.4 showcases several qualitative results.</p>
<h3>5.1. Datasets, Metrics and Implementation Details</h3>
<p>Datasets. In this paper, we experiment with 3D data from ScanNet [15], a 3D dataset covering 1,201 and 312 diverse and complex indoor 3D scenes for training and validation. The language annotations used in this study are sourced from ScanRefer [6], Nr3D [1], ScanQA [2], and the ScanNet subset of 3D-LLM [26]. This combination covers a variety of tasks, including instance and scene descriptions, conversations, embodied planning and question answering. Please refer to the supplementary materials for more details on the statistics of data.
Metrics. Here, we adopt C, B-4, M, R as abbreviations for CiDEr [50], BLEU-4 [42], METEOR [3], and Rouge-L [33] to evaluate the quality of the generated textual responses.
Implementation Details. Following previous works on 3D vision language tasks [9, 11], we randomly sample 40 k points from each 3D scene as the 3D input. We adopt the pre-trained OPT-1.3B [58] as our causal LLM backbone, which is frozen and loaded in float16 to save memory cost. We adopt the AdamW [35] optimizer with a weight decay of 0.1 and a learning rate decaying from $10^{-4}$ to $10^{-6}$ with a cosine annealing scheduler for about 100 k iterations. For all the training tasks, we train with a total batch size of 16, and evaluate our method every 4 k iterations. Each training process consumes no more than eight Nvidia RTX3090 (24G) GPUs within a day.</p>
<h3>5.2. Comparison with SoTA Specialists</h3>
<p>We evaluate the model's capacity to understand and reason in 3D environments via 3D-DC and 3D-QA. For each evaluation task, we fine-tune the trainable parameters in our model on each task for $\sim 30 \mathrm{k}$ iterations.</p>
<p>3D Dense Captioning demands a model to localize and describe any instance in a 3D scene. We benchmarks state-of-the-art methods on the widely-used ScanRefer [6] and Nr3D [1] dataset in Tab. 1 under the $m @ k$ IoU metric [11]. Here, $m \in{\mathrm{C}, \mathrm{B}-4, \mathrm{M}, \mathrm{R}}$, and the $m$ score of a caption is set to 0 if the IoU between the predicted box and the object is less than the given threshold $k$. Following existing works [9, 11], we consider C@0.25 and C@0.5 as the main metric for ScanRefer, and C@0.5 for Nr3D. Among the listed methods, UniT3D [12], 3DJCG [4], and 3D-VLP [30] are pre-trained on multiple 3D vision and language tasks annotated on ScanNet scenes. Additionally, UniT3D [12] adopts off-the-shelf image caption models [40] and multi-view images to generate additional instance-captions for pre-training. It is worth mentioning that the results of 3D-VisTA [65] come from their reported version that is not trained on additional 3D scenes. To evaluate our model, we adopt the box predictions produced by Vote2Cap-DETR [9] as the visual prompt. Results show that our method consistently outperforms existing methods on all both datasets. For example, our method achieves $65.19 \%$ C@0.5 on ScanRefer and $51.18 \%$ C@0.5 on Nr3D, which is ( $+3.38 \%$ and $+7.34 \%$ ) higher than the current state-of-the-art 3D vision and language model, Vote2Cap-DETR.
3D Question Answering requires a model to generate responses to the natural language queries questioning towards an 3D scene. We benchmark state-of-the-art methods on the ScanQA [2] validation set as well as two test benchmarks in Tab. 2, and consider CiDEr as the main metric. The majority of the listed methods are based on classification (marked "CLS"), i.e., selecting responses from a predefined answer set. Meanwhile, 3D-LLM [26] tries to address 3D-QA via generating texts (marked "GEN"), and we list their fine-tuned version for comparison. Results show that our method consistently outperforms existing methods on all the evaluation sets, and surpasses the generation based method, 3D-LLM, by a large margin ( $+7.39 \%$ CiDEr score on the validation set).</p>
<h3>5.3. Ablation Studies</h3>
<p>In this section, we provide ablation studies on model designs and training strategies. We evaluate on ScanRefer and ScanQA to quantize the effectiveness.
Effectiveness of the Q-Former Design. We list two ways of utilizing visual prompts in Fig. 3, where (a) is our proposed method that adopts a unified transformer that aggregates information from both the textual instructions and visual prompts, and (b) is the "direct injection" version, which only extract instruction-aware 3D feature with visual prompts concatenated after the scene embeddings. We train both models from scratch and evaluate their performance on ScanRefer 3D Dense Captioning. The results (Fig. 3) show that the method we use (Fig. 3 (a)) could better capture fea-</p>
<p>Table 1. Quantitative Comparisons for 3D Dense Captioning on ScanRefer[6] and Nr3D[1]. For fair comparison, we list methods that are trained under the standard per-word cross-entropy loss without additional 3D scenes. We use the box estimations from Vote2Cap-DETR to simulate the box annotations as the visual prompts. Our proposed LL3DA surpasses previous 3D specialists on both datasets.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>ScanRefer</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Nr3D</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>C@0.25 $\uparrow$</td>
<td>B-4@0.25 $\uparrow$</td>
<td>M@0.25 $\uparrow$</td>
<td>R@0.25 $\uparrow$</td>
<td>C@0.5 $\uparrow$</td>
<td>B-4@0.5 $\uparrow$</td>
<td>M@0.5 $\uparrow$</td>
<td>R@0.5 $\uparrow$</td>
<td>C@0.5 $\uparrow$</td>
<td>B-4@0.5 $\uparrow$</td>
<td>M@0.5 $\uparrow$</td>
<td>R@0.5 $\uparrow$</td>
<td></td>
</tr>
<tr>
<td>Scan2Cap[11]</td>
<td>56.82</td>
<td>34.18</td>
<td>26.29</td>
<td>55.27</td>
<td>39.08</td>
<td>23.32</td>
<td>21.97</td>
<td>44.78</td>
<td>27.47</td>
<td>17.24</td>
<td>21.80</td>
<td>49.06</td>
<td></td>
</tr>
<tr>
<td>MORE[29]</td>
<td>62.91</td>
<td>36.25</td>
<td>26.75</td>
<td>56.33</td>
<td>40.94</td>
<td>22.93</td>
<td>21.66</td>
<td>44.42</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>SpaCap3D[51]</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>44.02</td>
<td>25.26</td>
<td>22.33</td>
<td>45.36</td>
<td>33.71</td>
<td>19.92</td>
<td>22.61</td>
<td>50.50</td>
<td></td>
</tr>
<tr>
<td>REMAN[58]</td>
<td>62.01</td>
<td>36.37</td>
<td>26.76</td>
<td>56.25</td>
<td>45.00</td>
<td>26.31</td>
<td>22.67</td>
<td>46.96</td>
<td>34.81</td>
<td>20.37</td>
<td>23.01</td>
<td>50.99</td>
<td></td>
</tr>
<tr>
<td>D3Net[7]</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>46.07</td>
<td>30.29</td>
<td>24.35</td>
<td>51.67</td>
<td>33.85</td>
<td>20.70</td>
<td>23.13</td>
<td>53.38</td>
<td></td>
</tr>
<tr>
<td>Contextual[62]</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>46.11</td>
<td>25.47</td>
<td>22.64</td>
<td>45.96</td>
<td>35.26</td>
<td>20.42</td>
<td>22.77</td>
<td>50.78</td>
<td></td>
</tr>
<tr>
<td>UniT3D[12]</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>46.69</td>
<td>27.22</td>
<td>21.91</td>
<td>45.98</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>3DJCG[4]</td>
<td>64.70</td>
<td>40.17</td>
<td>27.66</td>
<td>59.23</td>
<td>49.48</td>
<td>31.03</td>
<td>24.22</td>
<td>50.80</td>
<td>38.06</td>
<td>22.82</td>
<td>23.77</td>
<td>52.99</td>
<td></td>
</tr>
<tr>
<td>3D-VLP[30]</td>
<td>70.73</td>
<td>41.03</td>
<td>28.14</td>
<td>59.72</td>
<td>54.94</td>
<td>32.31</td>
<td>24.83</td>
<td>51.51</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>3D-VisTA*[65]</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>61.60</td>
<td>34.10</td>
<td>26.80</td>
<td>55.00</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>Vote2Cap-DETR[9]</td>
<td>71.45</td>
<td>39.34</td>
<td>28.25</td>
<td>59.33</td>
<td>61.81</td>
<td>34.46</td>
<td>26.22</td>
<td>54.40</td>
<td>43.84</td>
<td>26.68</td>
<td>25.41</td>
<td>54.43</td>
<td></td>
</tr>
<tr>
<td>LL3DA (Ours)</td>
<td>74.17</td>
<td>41.41</td>
<td>27.76</td>
<td>59.53</td>
<td>65.19</td>
<td>36.79</td>
<td>25.97</td>
<td>55.06</td>
<td>51.18</td>
<td>28.75</td>
<td>25.91</td>
<td>56.61</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 2. Quantitative Comparisons for 3D Question Answering on ScanQA[2]. We categorize previous works into classification based ("CLS") and generation based ("GEN") methods. The results from 3D-LLM* come from their fine-tuned version. LL3DA out-performs previous methods on the validation set and two test sets.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Answer</th>
<th>Validation</th>
<th></th>
<th></th>
<th></th>
<th>Test w/ object</th>
<th></th>
<th></th>
<th></th>
<th>Test w/o object</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Type</td>
<td>C $\uparrow$</td>
<td>B-4 $\uparrow$</td>
<td>M $\uparrow$</td>
<td>R $\uparrow$</td>
<td>C $\uparrow$</td>
<td>B-4 $\uparrow$</td>
<td>M $\uparrow$</td>
<td>R $\uparrow$</td>
<td>C $\uparrow$</td>
<td>B-4 $\uparrow$</td>
<td>M $\uparrow$</td>
<td>R $\uparrow$</td>
</tr>
<tr>
<td>ScanQA[2]</td>
<td></td>
<td>64.86</td>
<td>10.08</td>
<td>13.14</td>
<td>33.33</td>
<td>67.29</td>
<td>12.04</td>
<td>13.55</td>
<td>34.34</td>
<td>60.24</td>
<td>10.75</td>
<td>12.59</td>
<td>31.09</td>
</tr>
<tr>
<td>Clip-Guided[43]</td>
<td></td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>69.53</td>
<td>14.64</td>
<td>13.94</td>
<td>35.15</td>
<td>62.83</td>
<td>11.73</td>
<td>13.28</td>
<td>32.41</td>
</tr>
<tr>
<td>Multi-CLIP[17]</td>
<td>CLS</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>68.70</td>
<td>12.65</td>
<td>13.97</td>
<td>35.46</td>
<td>63.20</td>
<td>12.87</td>
<td>13.36</td>
<td>32.61</td>
</tr>
<tr>
<td>3D-VLP[30]</td>
<td></td>
<td>66.97</td>
<td>11.15</td>
<td>13.53</td>
<td>34.51</td>
<td>70.18</td>
<td>11.23</td>
<td>14.16</td>
<td>35.97</td>
<td>63.40</td>
<td>15.84</td>
<td>13.13</td>
<td>31.79</td>
</tr>
<tr>
<td>3D-VisTA[65]</td>
<td></td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>68.60</td>
<td>10.50</td>
<td>13.80</td>
<td>35.50</td>
<td>55.70</td>
<td>8.70</td>
<td>11.69</td>
<td>29.60</td>
</tr>
<tr>
<td>3D-LLM*[26]</td>
<td>GEN</td>
<td>69.40</td>
<td>12.00</td>
<td>14.50</td>
<td>35.70</td>
<td>69.60</td>
<td>11.60</td>
<td>14.90</td>
<td>35.30</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>LL3DA (Ours)</td>
<td></td>
<td>76.79</td>
<td>13.53</td>
<td>15.88</td>
<td>37.31</td>
<td>78.16</td>
<td>13.97</td>
<td>16.38</td>
<td>38.15</td>
<td>70.29</td>
<td>12.19</td>
<td>14.85</td>
<td>35.17</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Different Ways of Encoding Visual Prompts. We listed two ways of encoding visual prompts, (a) adopting a unified transformer to aggregate features from all kinds of interactions, and (b) directly concatenate the visual prompts to the scene embeddings. Experiments (Tab. 3) show that early fusion(a) leads to a better performance. ture related to the visual prompts, leading to better instance caption generation performance ( $+3.45 \%$ C@0.5).</p>
<p>Table 3. Effectiveness of Q-Former Design on ScanRefer[6]. We design two different ways of utilizing visual prompts. The "early fusion" enables direct interaction with the 3D scene, thus it achieves a better performance.</p>
<table>
<thead>
<tr>
<th>Visual Prompt</th>
<th>C@0.5 $\uparrow$</th>
<th>B-4@0.5 $\uparrow$</th>
<th>M@0.5 $\uparrow$</th>
<th>R@0.5 $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>direct</td>
<td>59.39</td>
<td>33.27</td>
<td>25.19</td>
<td>53.39</td>
</tr>
<tr>
<td>ours</td>
<td>$\mathbf{6 2 . 8 4}$</td>
<td>$\mathbf{3 5 . 8 1}$</td>
<td>$\mathbf{2 5 . 8 1}$</td>
<td>$\mathbf{5 4 . 4 5}$</td>
</tr>
</tbody>
</table>
<p>Instructions as Auxiliary Tasks for 3D Dense Captioning. We have introduced two types of textual instructions in Sec. 4.1 for 3D-DC, i.e. the "describe" only instructions and "detect and localize" instructions. Additionally, we have introduced two types of visual prompts (Fig. 2 \&amp; Sec. 4.2). In this study, we show how they serve as auxiliary tasks for 3D-DC by evaluating on ScanRefer in Tab. 4. All the methods listed are trained from scratch. In Tab. 4, "Aux.Loc" identifies whether we train the model with the "detect and localize" instructions, and "Clicks" identifies whether we train the model with clicks as additional visual prompts. Results show that either way can serve as good auxiliary tasks for 3D Dense Captioning.</p>
<p>Table 4. Effectiveness of Instructions as 3D Dense Captioning Auxiliary Tasks. We train the models from scratch and evaluate on ScanRefer[6]. "Aux.Loc" identifies whether we train with the "describe and localize" instructions. "Clicks" identifies whether we train with clicks as additional visual prompts.</p>
<table>
<thead>
<tr>
<th>Aux.Loc</th>
<th>Clicks</th>
<th>C@0.5 $\uparrow$</th>
<th>B-4@0.5 $\uparrow$</th>
<th>M@0.5 $\uparrow$</th>
<th>R@0.5 $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>-</td>
<td>60.85</td>
<td>34.09</td>
<td>25.53</td>
<td>53.48</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>-</td>
<td>61.81</td>
<td>34.15</td>
<td>25.49</td>
<td>53.83</td>
</tr>
<tr>
<td>-</td>
<td>$\checkmark$</td>
<td>62.20</td>
<td>34.26</td>
<td>25.67</td>
<td>53.87</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\mathbf{6 2 . 8 4}$</td>
<td>$\mathbf{3 5 . 8 1}$</td>
<td>$\mathbf{2 5 . 8 1}$</td>
<td>$\mathbf{5 4 . 4 5}$</td>
</tr>
</tbody>
</table>
<p>Instructions as Auxiliary Tasks for 3D Question Answer-</p>
<p>Table 5. Evaluation as a Generalist. The first three rows list the performance of models trained from scratch as experts on each dataset. The results in the following three rows belong to the model fine-tuned from the generalist weights. The last row evaluates the model trained as a generalist. ScanRefer[6] and Nr3D[1] are used to evaluate the dense captioning performance, and ScanQA[2] is used to evaluate the question answering performance. Serving as a generalist, our method can differentiate each task, and produce strong results based on textual instructions and visual prompts.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>ScanRefer</th>
<th></th>
<th></th>
<th></th>
<th>Nr3D</th>
<th></th>
<th></th>
<th></th>
<th>ScanQA</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>C@0.5 $\uparrow$</td>
<td>B-4@0.5 $\uparrow$</td>
<td>M@0.5 $\uparrow$</td>
<td>R@0.5 $\uparrow$</td>
<td>C@0.5 $\uparrow$</td>
<td>B-4@0.5 $\uparrow$</td>
<td>M@0.5 $\uparrow$</td>
<td>R@0.5 $\uparrow$</td>
<td>C $\uparrow$</td>
<td>B-4 $\uparrow$</td>
<td>M $\uparrow$</td>
<td>R $\uparrow$</td>
</tr>
<tr>
<td>ScanRefer(scratch)</td>
<td>62.84</td>
<td>35.81</td>
<td>25.81</td>
<td>54.45</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Nr3D(scratch)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>44.95</td>
<td>27.67</td>
<td>25.67</td>
<td>55.79</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ScanQA(scratch)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>74.80</td>
<td>13.68</td>
<td>15.40</td>
<td>36.25</td>
</tr>
<tr>
<td>ScanRefer(fine-tuned)</td>
<td>65.19</td>
<td>36.79</td>
<td>25.97</td>
<td>55.06</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Nr3D(fine-tuned)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>51.18</td>
<td>28.75</td>
<td>25.91</td>
<td>56.61</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ScanQA(fine-tuned)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>76.79</td>
<td>13.53</td>
<td>15.88</td>
<td>37.31</td>
</tr>
<tr>
<td>w/o fine-tuning</td>
<td>62.98</td>
<td>35.97</td>
<td>25.66</td>
<td>54.65</td>
<td>23.94</td>
<td>13.37</td>
<td>22.31</td>
<td>45.78</td>
<td>75.67</td>
<td>13.33</td>
<td>15.37</td>
<td>37.02</td>
</tr>
</tbody>
</table>
<p>Table 6. Effectiveness of Interactions as 3D Question Answering Auxiliary Tasks. We train the model from scratch and evaluate all the models from scratch on ScanQA[2] validation set. "Aux.Loc" identifies whether we train with the "answer and localize" instructions, and "Visual Prompts" identifies whether we train with visual prompts.</p>
<table>
<thead>
<tr>
<th>Aux.Loc</th>
<th>Visual Prompts</th>
<th>CiDEr $\uparrow$</th>
<th>BLEU-4 $\uparrow$</th>
<th>METEOR $\uparrow$</th>
<th>Rouge-L $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>-</td>
<td>67.85</td>
<td>11.87</td>
<td>13.96</td>
<td>33.87</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>-</td>
<td>72.73</td>
<td>13.27</td>
<td>14.90</td>
<td>35.87</td>
</tr>
<tr>
<td>-</td>
<td>$\checkmark$</td>
<td>68.09</td>
<td>12.59</td>
<td>14.20</td>
<td>33.71</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\mathbf{7 4 . 8 0}$</td>
<td>$\mathbf{1 3 . 6 8}$</td>
<td>$\mathbf{1 5 . 4 0}$</td>
<td>$\mathbf{3 6 . 2 5}$</td>
</tr>
</tbody>
</table>
<p>ing. We have made similar study on the effectiveness of treating "answer and localize" instructions and additional visual prompts as auxiliary tasks for 3D-QA on ScanQA [2] validation set in Tab. 6. The listed methods are evaluated without any visual interactions for fair comparison. Results show that the additional textual instructions and visual prompts improve the task diversity and further improve the performance on 3D Question Answering. Performance as a Generalist. To examine whether our method could distinguish different tasks given the textual instructions and visual prompts introduced in Sec. 4.1, we evaluate our model on different tasks with the same set of weights in Tab. 5 with several baseline methods. The first three rows list the performance our proposed models trained on each dataset from scratch as generalists. The following three rows list the performance of the model fine-tuned on each dataset. The last row lists the performance of our model as a generalist. Results show that our model could distinguish 3D-DC and 3D-QA given the textual instructions and visual prompts, providing strong results ( $62.98 \%$ C@0.5 on ScanRefer, $75.67 \%$ CiDEr on ScanQA). However, the generalist model achieves poor results on Nr3D [1], which is because we did not try to differentiate between Nr3D and ScanRefer during training as these two datasets are used for the same task in the first place. There is also an interesting observation that although we did not differentiate between the two datasets for 3D-DC, and the training sample sizes of the two models were similar, the model still tend to achieve high scores on ScanRefer ( $62.98 \%$ C@0.5). We are also excited to see that the weights of the generalist model could serve as a strong initialization of weights for fine-tuning. For example, the fine-tuned model on ScanRefer could achieve $65.19 \%$ C@0.5, which is $+2.35 \%$ higher than the model trained from scratch.</p>
<p>Importance of Textual Instructions. We further conduct study to see whether the instructions are necessary in 3D Dense Captioning in Tab. 7. The first row is our baseline method that directly generates the captions based on visual prompts without any textual instructions, and the second row is our method that is trained with textual instructions introduced in Sec. 4.1. Both methods are trained from scratch for fair comparison. We notice that since the LLM is frozen, certain textual instructions are beneficial to generating results in specific domains/tasks.</p>
<p>Table 7. Effectiveness of Instructions on 3D Dense Captioning. We perform experiments on ScanRefer[6]. The baseline method directly generates the captions given the input 3D scene and visual prompts without any textual instructions.</p>
<table>
<thead>
<tr>
<th>Instructions</th>
<th>C@0.5 $\uparrow$</th>
<th>B-4@0.5 $\uparrow$</th>
<th>M@0.5 $\uparrow$</th>
<th>R@0.5 $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>60.20</td>
<td>34.79</td>
<td>25.40</td>
<td>54.03</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>$\mathbf{6 2 . 8 4}$</td>
<td>$\mathbf{3 5 . 8 1}$</td>
<td>$\mathbf{2 5 . 8 1}$</td>
<td>$\mathbf{5 4 . 4 5}$</td>
</tr>
</tbody>
</table>
<p>Clicks for Better Question Answering. One major challenge of answering questions in complex 3D environments is the vague identification of objects with plain texts. Therefore, we try to click on some of the related objects along with the textual instructions during evaluation, and see how it could affect the generated answers on the ScanQA validation set in Tab. 8. Results show that this technique would remove the ambiguities, and further improve the quality of the answers $(+6.12 \% \mathrm{C})$. This illustrates the importance of visual interaction in complex 3D environments.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Qualitative Results. We provide several visualization results on various 3D vision and language tasks in diverse 3D environments (living room, classroom, kitchen, and bedroom). Red highlights the wrong answer.</p>
<p>Table 8. Test Time Visual Interactions for Question Answering on ScanQA[2]. The model achieves better performance on the question answering when we add visual prompts to some of the related objects along with the text instructions during evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Visual Prompts</th>
<th style="text-align: center;">CiDEr $\uparrow$</th>
<th style="text-align: center;">BLEU-4 $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">Rouge-L $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">76.79</td>
<td style="text-align: center;">13.53</td>
<td style="text-align: center;">15.88</td>
<td style="text-align: center;">37.31</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">82.91</td>
<td style="text-align: center;">11.80</td>
<td style="text-align: center;">16.74</td>
<td style="text-align: center;">39.97</td>
</tr>
</tbody>
</table>
<h3>5.4. Qualitative Results</h3>
<p>We present several visualization results on different tasks in Fig. 4 to show our model's capacities in understanding, rea-
soning, and planning in different 3D environments. To prevent repetition when generating long sequences, we combine the top- $k$ [21] and top- $p$ [24] sampling strategy with $k=50$ and $p=0.95$.</p>
<h2>6. Conclusions</h2>
<p>In this paper, we present LL3DA, a large language 3D assistant that could take both textual- and visual- interactions from human for understanding, reasoning, and planning in complex 3D environments. Our model directly encodes 3D point cloud for scene representations, and aggregates information from scenes and human interactions with the atten-</p>
<p>tion mechanism. The visual interactions could remove the ambiguities in cluttered 3D environments, showing mighty instruction-following capacities. Experimental results show that our method could achieve remarkable results on various 3D vision-language benchmarks. We hope that our approach could inspire further designs and training strategies for large 3D language models. In future studies, we anticipate that the availability of high-quality and diverse 3D vision and language annotations will enhance the model's reasoning and planning capabilities.</p>
<h2>References</h2>
<p>[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In European Conference on Computer Vision, pages 422440. Springer, 2020. 2, 5, 6, 7, 19
[2] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1912919139, 2022. 2, 4, 5, 6, 7, 8, 19
[3] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72, 2005. 5, 15
[4] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16464-16473, 2022. 2, 5, 6
[5] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu. Position-enhanced visual instruction tuning for multimodal large language models. arXiv preprint arXiv:2308.13437, 2023. 2
[6] Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European Conference on Computer Vision, pages 202-221. Springer, 2020. 2, 5, 6, 7, 19
[7] Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and Angel X Chang. D3net: A speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans. arXiv preprint arXiv:2112.01551, 2021. 6
[8] Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable autonomous driving. arXiv preprint arXiv:2310.01957, 2023. 2
[9] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. End-to-end 3d dense captioning with vote2cap-detr. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1112411133, 2023. 2, 3, 4, 5, 6, 17, 18
[10] Sijin Chen, Hongyuan Zhu, Mingsheng Li, Xin Chen, Peng Guo, Yinjie Lei, Gang Yu, Taihao Li, and Tao</p>
<p>Chen. Vote2cap-detr++: Decoupling localization and describing for end-to-end 3d dense captioning. arXiv preprint arXiv:2309.02999, 2023. 2
[11] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3193-3203, 2021. 2, 5, 6, 17
[12] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Nießner, and Angel X Chang. Unit3d: A unified transformer for 3d dense captioning and visual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18109-18119, 2023. 2, 5, 6
[13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 1
[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 2
[15] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828-5839, 2017. 5, 17, 19
[16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 1, 2, 4
[17] Alexandros Delitzas, Maria Parelli, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and Thomas Hofmann. Multi-clip: Contrastive vision-language pre-training for question answering tasks in 3d scenes. arXiv preprint arXiv:2306.02329, 2023. 2, 6
[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 4, 20
[19] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Pla: Language-driven openvocabulary 3d scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7010-7019, 2023. 2
[20] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palme: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 2
[21] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018. 8
[22] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive like a human: Rethink-</p>
<p>ing autonomous driving with large language models. arXiv preprint arXiv:2307.07162, 2023. 2
[23] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind \&amp; point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615, 2023. 2, 3
[24] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. 8
[25] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9202-9212, 2023. 2
[26] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. arXiv preprint arXiv:2307.12981, 2023. 2, 5, 6, 15, 16, 19
[27] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 1
[28] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. arXiv preprint arXiv:2306.14795, 2023. 1
[29] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. More: Multi-order relation mining for dense captioning in 3d scenes. arXiv preprint arXiv:2203.05203, 2022. 6
[30] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and Yinjie Lei. Context-aware alignment and mutual masking for 3dlanguage pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10984-10994, 2023. 2, 5, 6
[31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 4
[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 2, 4
[33] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81, 2004. 5, 15
[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 2
[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5
[36] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. arXiv preprint arXiv:2306.07279, 2023. 3
[37] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. 2
[38] Aihua Mao, Zhi Yang, Wanxin Chen, Ran Yi, and Yong-jin Liu. Complete 3d relationships extraction modality alignment network for 3d dense captioning. IEEE Transactions on Visualization and Computer Graphics, 2023. 6
[39] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-toend transformer model for 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2906-2917, 2021. 20
[40] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021. 5
[41] OpenAI. Gpt-4 technical report, 2023. 1
[42] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318, 2002. 5, 15
[43] Maria Parelli, Alexandros Delitzas, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and Thomas Hofmann. Clip-guided vision-language pre-training for question answering in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5606-5611, 2023. 2, 4, 6
[44] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017. 20
[45] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9277-9286, 2019. 17
[46] David Rozenberszki, Or Litany, and Angela Dai. Languagegrounded indoor 3d semantic segmentation in the wild. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. 17
[47] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998-3009, 2023. 2
[48] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 33:7537-7547, 2020. 4
[49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1, 3, 4, 16
[50] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer</p>
<p>vision and pattern recognition, pages 4566-4575, 2015. 5, 15
[51] Heng Wang, Chaoyi Zhang, Jianhui Yu, and Weidong Cai. Spatiality-guided transformer for 3d dense captioning on point clouds. arXiv preprint arXiv:2204.10688, 2022. 2, 6
[52] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, and Jian Zhang. Eda: Explicit text-decoupling and dense alignment for 3d visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19231-19242, 2023. 2
[53] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal learning with transformers: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2
[54] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911, 2023. 1, 2, 3
[55] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2
[56] Shuquan Ye, Dongdong Chen, Songfang Han, and Jing Liao. 3d question answering. IEEE Transactions on Visualization and Computer Graphics, 2022. 2
[57] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 2
[58] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 1, 2, 3, 4, 5, 16
[59] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. 1, 2
[60] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvgtransformer: Relation modeling for visual grounding on point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2928-2937, 2021. 2
[61] Lichen Zhao, Daigang Cai, Jing Zhang, Lu Sheng, Dong Xu, Rui Zheng, Yinjie Zhao, Lipeng Wang, and Xibo Fan. Towards explainable 3d grounded visual question answering: A new benchmark and strong baseline. IEEE Transactions on Circuits and Systems for Video Technology, 2022. 2
[62] Yufeng Zhong, Long Xu, Jiebo Luo, and Lin Ma. Contextual modeling for 3d dense captioning on point clouds. arXiv preprint arXiv:2210.03925, 2022. 6
[63] Qiang Zhou, Chaohui Yu, Shaofeng Zhang, Sitong Wu, Zhibing Wang, and Fan Wang. Regionblip: A unified multimodal pre-training framework for holistic and regional comprehension, 2023. 3
[64] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2
[65] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29112921, 2023. 2, 5, 6</p>
<h1>Supplementary Material</h1>
<p>The supplementary material consists of more visualization results (Sec. A), quantitative evaluations on scene descriptions, embodied dialogue, and embodied planning (Sec. B), additional studies on 3D coordinates generated by LL3DA (Sec. C), data statistics as well as training samples (Sec. D), additional implementation details (Sec. E).
Codes and Pre-trained weights will be fully released. We provide example code files along with this file, including the training and evaluation process for different tasks.
Video. We also provide video clips for more illustrative demonstrations.</p>
<h2>A. More Visualizations</h2>
<p>We present additional visualization results on Scene Descriptions (Fig. 5), 3D Dense Captioning, 3D Question Answering, Embodied Dialogue (Fig. 6), and Embodied Planning (Fig. 7).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Qualitative Results on Scene Descriptions. We highlight some of the phrases in the generated scene descriptions mentioning the instances in the 3D environment.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. More Qualitative Results on 3D Dense Captioning (upper), 3D Question Answering (middle), and Embodied Dialogue (lower). Red highlights the wrong answer.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Qualitative Results on Embodied Planning. The planning results generated by LL3DA are consistent with common sense knowledge. Additionally, LL3DA could not only make plans directly, but also generate feedbacks based on the things have been done.</p>
<h1>B. More Evaluations</h1>
<p>Since 3D-LLM [26] has not yet released the validation set they used for evaluation, we make our own split to quantify our method's performance on: 1) scene descriptions, 2) embodied dialogue, and 3) embodied planning.
Data Splits. In practice, we set the scenes with ids less than 600 as the training set, and the rest as the validation set. The data statistics of our split are listed in Tab. 9. It is worth mentioning that:</p>
<ul>
<li>The multi-turn embodied dialogues are further decomposed into 8,490 and 1,222 lines of data samples following the procedure in Fig. 8 for training and validation.</li>
<li>The embodied planning data is also decomposed into 16,972 and 2,282 lines of data samples for training and validation following the procedure in Fig. 9.
As mentioned in Sec. 4.1 in the main paper, we distinguishes the source of texts with the "### human:" and "### assistant:" identifier.
Metrics. Similar to the main paper, we evaluate the generated natural language responses under the CiDEr [50], BLEU [42], METEOR [3], and Rouge-L [33] metrics in Tab. 10.
<img alt="img-7.jpeg" src="img-7.jpeg" /></li>
</ul>
<p>Training Sample 1
Input: "### human: Can you hand me the remote? ### assistant:",
Output: "Sure! Where is it?"
Training Sample 2
Input: "### human: Can you hand me the remote? ### assistant: Sure! where is it? ### human: It's on the ledge in the bathroom. ### assistant:", Output: "Alright. I have retrieved the remote from the ledge. Here you go. Is there anything else you need?"</p>
<p>Figure 8. The Multi-Turn Dialogue Decomposition Pipeline. For one sample of a $n$-turn dialogue data $(n \geq 1)$, we decompose it into $n$ lines of training samples, and distinguish the source of data with the "### human:" and "### assistant:" identifier.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Training Sample $k(k=1 \cdots n)$
Input: "### human: I want to organize the office supplies. I have done these things:
L xxxx
L xxxx.
$k$. xxxx.
what should I do next? ### assistant:",
Output: (the $(k+1)$-th issue)
Training Sample $n+1$
Input: "### human: I want to organize the office supplies. What should I do? ### assistant:",
Output: "L xxxx
n. xxxx
$n+1$. Take out the boxes."
Figure 9. The Embodied Planning Decomposition Pipeline. For one sample with given $n$-tasks $(n \geq 1)$, we decompose it into $(n+1)$ lines of training and validation samples.</p>
<p>Table 9. Details about the Training-Validation Split of the ScanNet Part of 3D-LLM [26]. We list the number of natural language annotations ("# annotations") and number of scenes ("# scenes") in each split for each task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Split</th>
<th style="text-align: center;">Scene Description</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Embodied Planning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Embodied Dialogue</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"># annotations</td>
<td style="text-align: center;"># scenes</td>
<td style="text-align: center;"># annotations</td>
<td style="text-align: center;"># scenes</td>
<td style="text-align: center;"># annotations</td>
<td style="text-align: center;"># scenes</td>
</tr>
<tr>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">912</td>
<td style="text-align: center;">456</td>
<td style="text-align: center;">1,636</td>
<td style="text-align: center;">449</td>
<td style="text-align: center;">2,592</td>
<td style="text-align: center;">456</td>
</tr>
<tr>
<td style="text-align: center;">Validation</td>
<td style="text-align: center;">121</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">228</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">363</td>
<td style="text-align: center;">61</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">1,033</td>
<td style="text-align: center;">517</td>
<td style="text-align: center;">1,864</td>
<td style="text-align: center;">510</td>
<td style="text-align: center;">2,955</td>
<td style="text-align: center;">517</td>
</tr>
</tbody>
</table>
<p>Table 10. Quantitative Comparisons on Scene Description, Embodied Dialogue and Embodied Planning. We evaluate our method with different sampling strategies on the ScanNet part of 3D-LLM [26]. We manually set the scenes with ids less than 600 as the training set, and the rest as the validation set. To prevent repetition, we adopt the $n$-gram repetition penalty so that no $n$-gram appears twice $(n=4)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU-1 $\uparrow$</th>
<th style="text-align: center;">BLEU-2 $\uparrow$</th>
<th style="text-align: center;">BLEU-3 $\uparrow$</th>
<th style="text-align: center;">BLEU-4 $\uparrow$</th>
<th style="text-align: center;">CiDEr $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">Rouge-L $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Scene <br> Description</td>
<td style="text-align: center;">Zero-Shot:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-1.3B [58]</td>
<td style="text-align: center;">15.79</td>
<td style="text-align: center;">6.10</td>
<td style="text-align: center;">2.07</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">8.40</td>
<td style="text-align: center;">11.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-2.7B [58]</td>
<td style="text-align: center;">19.97</td>
<td style="text-align: center;">7.59</td>
<td style="text-align: center;">2.14</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">12.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-6.7B [58]</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">9.79</td>
<td style="text-align: center;">3.62</td>
<td style="text-align: center;">1.13</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">8.99</td>
<td style="text-align: center;">16.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLAMA-7B [49]</td>
<td style="text-align: center;">19.26</td>
<td style="text-align: center;">7.69</td>
<td style="text-align: center;">2.79</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">12.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">top- $k$ \&amp; top- $p$</td>
<td style="text-align: center;">43.02</td>
<td style="text-align: center;">26.70</td>
<td style="text-align: center;">15.97</td>
<td style="text-align: center;">8.97</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">14.65</td>
<td style="text-align: center;">24.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">greedy decoding</td>
<td style="text-align: center;">29.15</td>
<td style="text-align: center;">20.51</td>
<td style="text-align: center;">13.99</td>
<td style="text-align: center;">9.38</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">12.83</td>
<td style="text-align: center;">24.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">29.94</td>
<td style="text-align: center;">21.56</td>
<td style="text-align: center;">14.93</td>
<td style="text-align: center;">10.02</td>
<td style="text-align: center;">1.32</td>
<td style="text-align: center;">12.31</td>
<td style="text-align: center;">27.08</td>
</tr>
<tr>
<td style="text-align: center;">Embodied Dialogue</td>
<td style="text-align: center;">Zero-Shot:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-1.3B [58]</td>
<td style="text-align: center;">2.44</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">5.62</td>
<td style="text-align: center;">4.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-2.7B [58]</td>
<td style="text-align: center;">3.88</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">7.38</td>
<td style="text-align: center;">6.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-6.7B [58]</td>
<td style="text-align: center;">3.59</td>
<td style="text-align: center;">1.65</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">6.88</td>
<td style="text-align: center;">6.16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLAMA-7B [49]</td>
<td style="text-align: center;">4.08</td>
<td style="text-align: center;">1.80</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">7.81</td>
<td style="text-align: center;">6.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">top- $k$ \&amp; top- $p$</td>
<td style="text-align: center;">41.38</td>
<td style="text-align: center;">32.59</td>
<td style="text-align: center;">27.47</td>
<td style="text-align: center;">23.95</td>
<td style="text-align: center;">190.01</td>
<td style="text-align: center;">23.50</td>
<td style="text-align: center;">40.61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">greedy decoding</td>
<td style="text-align: center;">48.08</td>
<td style="text-align: center;">39.59</td>
<td style="text-align: center;">34.37</td>
<td style="text-align: center;">30.70</td>
<td style="text-align: center;">251.78</td>
<td style="text-align: center;">27.01</td>
<td style="text-align: center;">47.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">48.14</td>
<td style="text-align: center;">39.83</td>
<td style="text-align: center;">34.83</td>
<td style="text-align: center;">31.32</td>
<td style="text-align: center;">260.07</td>
<td style="text-align: center;">27.21</td>
<td style="text-align: center;">47.69</td>
</tr>
<tr>
<td style="text-align: center;">Embodied Planning</td>
<td style="text-align: center;">Zero-Shot:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-1.3B [58]</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">3.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-2.7B [58]</td>
<td style="text-align: center;">2.02</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">3.59</td>
<td style="text-align: center;">4.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT-6.7B [58]</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">3.65</td>
<td style="text-align: center;">3.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLAMA-7B [49]</td>
<td style="text-align: center;">2.24</td>
<td style="text-align: center;">1.13</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">3.53</td>
<td style="text-align: center;">4.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">top- $k$ \&amp; top- $p$</td>
<td style="text-align: center;">40.72</td>
<td style="text-align: center;">27.18</td>
<td style="text-align: center;">18.64</td>
<td style="text-align: center;">12.95</td>
<td style="text-align: center;">128.80</td>
<td style="text-align: center;">17.05</td>
<td style="text-align: center;">39.25</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">greedy decoding</td>
<td style="text-align: center;">38.81</td>
<td style="text-align: center;">27.58</td>
<td style="text-align: center;">20.10</td>
<td style="text-align: center;">14.66</td>
<td style="text-align: center;">186.13</td>
<td style="text-align: center;">19.60</td>
<td style="text-align: center;">45.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">45.07</td>
<td style="text-align: center;">33.04</td>
<td style="text-align: center;">24.96</td>
<td style="text-align: center;">19.15</td>
<td style="text-align: center;">196.78</td>
<td style="text-align: center;">19.87</td>
<td style="text-align: center;">45.58</td>
</tr>
</tbody>
</table>
<p>Quantitative Comparisons. As shown in Tab. 10, we perform zero-shot evaluation on the frozen LLMs [49, 58] with the top- $k$ and top- $p$ sampling strategy as the baseline, and evaluate our method with different sampling strategies. Among all the listed results, we adopt the $n$-gram penalty that restricts no $n$-gram appears twice with $n=4$. We also set $k=50$ and $p=0.95$ for top- $k$ and top- $p$ sampling, and a beam size $=4$ for beam search. From the table, one could see that our method could generate high-quality responses, and out-performs language-only LLMs.</p>
<h1>C. Additional Studies: Generating 3D Bounding Boxes with LLM</h1>
<p>Quantitative Evaluation on 3D Open-Vocabulary Detection with LLM-Generated Boxes. We treat 3D Open-Vocabulary Detection as the 3D Dense Captioning problem, and simulate the click prompts using the spatial location of vote queries [9]. The textual instruction is set as "### human: what is this object? ### assistant:". To obtain localization results, we reconstruct 3D bounding boxes from the " $&lt;$ obj $&gt;c_{x}, c_{y}, c_{z}, w, h, l&lt;$ obj $&gt;$ " in the generated texts, and also decode the object category prediction from the texts. In Tab. 13, we present performances under the ScanNet vocabulary [15], and the ScanNet200 vocabulary [46]. Per-class mAP results on ScanNet could be found in Tab. 12. Results show that LL3DA could produce high-quality bounding boxes comparable to 3D detectors under the threshold $\mathrm{IoU}=0.5$.</p>
<p>Table 11. Quantitative Comparisons on Open-Vocabulary Detection with LL3DA. We treat 3D Detection as the 3D Dense Captioning problem. We simulate the click prompt with the vote queries [9], and decode the " $&lt;$ obj $&gt;c_{x}, c_{y}, c_{z}, w, h, l&lt;$ obj $&gt;$ " and category names from the generated texts. Results show that our method is able to produce comparable to the 3D detector specialist when $\mathrm{IoU}=0.5$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">ScanNet Vocabulary</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ScanNet200 Vocabulary</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IoU=0.25</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">IoU $=0.5$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">IoU $=0.25$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">IoU $=0.5$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mAP $\uparrow$</td>
<td style="text-align: center;">AR $\uparrow$</td>
<td style="text-align: center;">mAP $\uparrow$</td>
<td style="text-align: center;">AR $\uparrow$</td>
<td style="text-align: center;">mAP $\uparrow$</td>
<td style="text-align: center;">AR $\uparrow$</td>
<td style="text-align: center;">mAP $\uparrow$</td>
<td style="text-align: center;">AR $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">mAP $\uparrow$</td>
<td style="text-align: center;">AR $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VoteNet [45]</td>
<td style="text-align: center;">57.17</td>
<td style="text-align: center;">81.18</td>
<td style="text-align: center;">31.50</td>
<td style="text-align: center;">50.08</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">48.94</td>
<td style="text-align: center;">65.15</td>
<td style="text-align: center;">32.48</td>
<td style="text-align: center;">49.46</td>
<td style="text-align: center;">7.40</td>
<td style="text-align: center;">12.10</td>
<td style="text-align: center;">5.20</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 12. Per-class AP under IoU threshold of 0.25 and 0.5 on ScanNet validation set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">IoU</th>
<th style="text-align: center;">bathtub</th>
<th style="text-align: center;">bed</th>
<th style="text-align: center;">bookshelf</th>
<th style="text-align: center;">cabinet</th>
<th style="text-align: center;">chair</th>
<th style="text-align: center;">counter</th>
<th style="text-align: center;">curtain</th>
<th style="text-align: center;">desk</th>
<th style="text-align: center;">door</th>
<th style="text-align: center;">others</th>
<th style="text-align: center;">picture</th>
<th style="text-align: center;">refrigerator</th>
<th style="text-align: center;">shower curtain</th>
<th style="text-align: center;">sink</th>
<th style="text-align: center;">sofa</th>
<th style="text-align: center;">table</th>
<th style="text-align: center;">toilet</th>
<th style="text-align: center;">window</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">83.33</td>
<td style="text-align: center;">62.16</td>
<td style="text-align: center;">10.63</td>
<td style="text-align: center;">43.13</td>
<td style="text-align: center;">80.83</td>
<td style="text-align: center;">55.63</td>
<td style="text-align: center;">40.20</td>
<td style="text-align: center;">42.75</td>
<td style="text-align: center;">37.17</td>
<td style="text-align: center;">27.03</td>
<td style="text-align: center;">17.11</td>
<td style="text-align: center;">24.96</td>
<td style="text-align: center;">62.92</td>
<td style="text-align: center;">56.72</td>
<td style="text-align: center;">64.30</td>
<td style="text-align: center;">52.80</td>
<td style="text-align: center;">87.73</td>
<td style="text-align: center;">31.46</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">68.33</td>
<td style="text-align: center;">53.24</td>
<td style="text-align: center;">6.50</td>
<td style="text-align: center;">19.81</td>
<td style="text-align: center;">72.15</td>
<td style="text-align: center;">23.21</td>
<td style="text-align: center;">23.55</td>
<td style="text-align: center;">25.81</td>
<td style="text-align: center;">18.32</td>
<td style="text-align: center;">13.54</td>
<td style="text-align: center;">6.13</td>
<td style="text-align: center;">18.00</td>
<td style="text-align: center;">35.42</td>
<td style="text-align: center;">18.02</td>
<td style="text-align: center;">51.02</td>
<td style="text-align: center;">45.72</td>
<td style="text-align: center;">76.93</td>
<td style="text-align: center;">8.98</td>
</tr>
</tbody>
</table>
<p>Quantitative Evaluation on 3D Dense Captioning with LLM-Generated Boxes. We evaluate the quality of the 3D bounding boxes generated by LL3DA on 3D Dense Captioning. Following existing works [11], we evaluate with the $m @ k$ IoU metric, where $m$ could be one of CiDEr, BLEU-4, METEOR, Rouge-L:</p>
<p>$$
m @ k \operatorname{IoU}\left(b_{\text {pred }}^{3 D}, c_{\text {pred }}\right)=\frac{1}{N} \sum_{i=1}^{N} m\left(c_{\text {pred }}, c_{\mathrm{gt}}\right) \cdot \mathbb{I}\left{\operatorname{IoU}\left(b_{\text {pred }}^{3 D}, b_{\mathrm{gt}}^{3 D}\right) \geq k\right}
$$</p>
<p>Here, $N$ is the total size of annotated instances, $c_{\text {pred }}$ and $c_{\mathrm{gt}}$ are the predicted caption and the ground truth human annotations for this instance, $b_{\text {pred }}^{3 D}$ and $b_{\mathrm{gt}}^{3 D}$ are the predicted 3D bounding box and the ground truth 3D box annotation, and $\mathbb{I}$ {condition} is the identification function that equals 1 if the condition meets, and 0 otherwise.</p>
<p>Our baseline method is listed in Row 1 (Tab. 13). The baseline method is also the LL3DA model, but it only generates instance captions in response to the box prompts generated by the 3D detector [9] and the textual instruction "### human: describe this object in the given 3D scene. ### assistant:". The results listed in the second row of Tab. 13 come from reconstructing 3D bounding boxes from decoding the " $&lt;$ obj $&gt;c_{x}, c_{y}, c_{z}, w, h, l&lt;$ obj $&gt;$ " in the generated texts. We use "### human: given the 3D scene, localize and describe this object. ### assistant:" as the textual instruction. Though there is still gap between the bounding boxes generated by LL3DA and 3D localizer specialists, LL3DA could generate reasonable bounding box estimations. One can also refer to Fig. 10 for more visualization details.</p>
<p>Table 13. 3D Dense Captioning Performance with 3D Bounding Boxes Generated by LL3DA. Though there is still gap between LL3DA and 3D specialists for object localization, LL3DA could generate reasonable bounding box estimations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Test Time</th>
<th style="text-align: center;">$\mathrm{IoU}=0.25$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{IoU}=0.5$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Localization</td>
<td style="text-align: center;">C@0.25 $\uparrow$</td>
<td style="text-align: center;">B-4@0.25 $\uparrow$</td>
<td style="text-align: center;">M@0.25 $\uparrow$</td>
<td style="text-align: center;">R@0.25 $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C@0.5 $\uparrow$</td>
<td style="text-align: center;">B-4@0.5 $\uparrow$</td>
<td style="text-align: center;">M@0.5 $\uparrow$</td>
<td style="text-align: center;">R@0.5 $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3D Detector [9]</td>
<td style="text-align: center;">74.17</td>
<td style="text-align: center;">41.41</td>
<td style="text-align: center;">27.76</td>
<td style="text-align: center;">59.53</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">65.19</td>
<td style="text-align: center;">36.79</td>
<td style="text-align: center;">25.97</td>
<td style="text-align: center;">55.06</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LL3DA</td>
<td style="text-align: center;">62.90</td>
<td style="text-align: center;">35.14</td>
<td style="text-align: center;">26.70</td>
<td style="text-align: center;">55.62</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">51.12</td>
<td style="text-align: center;">29.27</td>
<td style="text-align: center;">24.18</td>
<td style="text-align: center;">49.51</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Qualitative Results of LL3DA's Box Predictions. We adopt the spatial location of vote queries to simulate the user click (256 queries per scene), and ask the model to generate 3D bounding boxes in forms of " $&lt;$ obj $&gt;c_{x}, c_{y}, c_{z}, w, h, l&lt;$ obj $&gt;$ " as introduced in the main paper. Then, we reconstruct the 3D bounding boxes, and visualize them in in Fig. 10. The visualization results show that LL3DA could produce tight bounding boxes close to the objects in diverse and complex 3D environments.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Visualization of the Bounding Boxes. We simulate the user click with the spatial position of vote queries proposed in Vote2Cap-DETR [9]. The bounding boxes from LL3DA are reconstructed via decoding the " $&lt;\operatorname{obj}&gt;c_{x}, c_{y}, c_{z}, w, h, l&lt;/$ obj $&gt;$ " from the generated texts. The reconstructed bounding boxes are tight to the objects in the 3D scene.</p>
<h1>D. Data and Instructions</h1>
<p>In this section, we provide statistic details on the 3D vision and language datasets we train the model (Sec. D.1) as well as the several textual instruction samples for different tasks (Sec. D.2).</p>
<h2>D.1. Data Statistics</h2>
<p>Our method is trained on the following datasets:
ScanNet [15] is a 3D indoor dataset covering diverse 3D environments, including apartments, living rooms, kitchens, bedrooms, and so on. ScanNet splits the dataset into 1201, 312, and 100 scenes for training, validation, and testing, repectively.
ScanRefer [6] is a 3D vision-language dataset, which contains 36,665 free-form natural language descriptions on 7,875 objects from 562 scenes for training, and 9,508 descriptions on 2,068 objects from 141 scenes from ScanNet [15] for evaluation.
Nr3D [1] is a 3D vision-language dataset with 32,919 free-form natural language descriptions on 4,664 objects from 511 scenes for training, and 8,584 descriptions on 1,214 objects from 130 scenes from ScanNet [15] for evaluation.
ScanQA [2] is a 3D vision-language dataset. The training set of ScanQA consists of 25,563 question-answer pairs on 562 scenes from ScanNet training set. ScanQA further splits 141 unique 3D scenes from ScanNet validation set into two sets:</p>
<ul>
<li>The validation set contains 4,675 question-answer pairs on 71 out of 141 validation scenes.</li>
<li>The "test w/ object" set ("test set with ScanNet object annotations") contains 4,976 questions on the rest 70 scenes.</li>
</ul>
<p>Additionally, ScanQA annotates 6,149 questions on 97 scenes on the ScanNet test set as the "test w/o object" set ("test set without ScanNet object annotations").
3D-LLM [26]. The ScanNet subset of 3D-LLM covers 1) 1,033 descriptions on 517 scenes, 2) 1,864 lines of embodied task planning on 510 scenes, and 3) 2,955 lines of multi-turn embodied dialogues on 517 scenes. All of the annotated scenes come from the training set of ScanNet. The evaluation details could be found in Sec. B.</p>
<h2>D.2. Training Samples</h2>
<p>We list a couple of the training samples for different tasks in Tab. 14. The special tokens "[Caption]", "[Question]", "[Answer]", "[Box]", and "[Response]" will be replaced with the language annotations and 3D coordinates.</p>
<p>Table 14. Data Samples Used to Train LL3DA. We list a couple of data samples used to train LL3DA for each task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Text Instructions</th>
<th style="text-align: center;">Visual Interactions</th>
<th style="text-align: center;">Expected Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3D Dense Captioning</td>
<td style="text-align: center;">"### human: describe this object in the given 3D scene. ### assistant: <br> "### human: given the 3D scene, localize and describe this object. ### assistant."</td>
<td style="text-align: center;">[Click] or [Box] <br> [Click] or [Box]</td>
<td style="text-align: center;">[Caption] <br> the object is localized at [Box], [Caption]</td>
</tr>
<tr>
<td style="text-align: center;">3D Question Answering</td>
<td style="text-align: center;">"### human: given the 3D scene, answer the question: "[Question]" <br> ### assistant." <br> "### human: answer the question: "[Question]" with the related object locations in the input 3D scene. ### assistant."</td>
<td style="text-align: center;">Optional [Clicks]</td>
<td style="text-align: center;">[Answer]. <br> the related objects are localized at [Box]. the answer is: [Answer].</td>
</tr>
<tr>
<td style="text-align: center;">Scene Description</td>
<td style="text-align: center;">"### human: describe this 3D scene ### assistant:"</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">[Caption]</td>
</tr>
<tr>
<td style="text-align: center;">Embodied Planning</td>
<td style="text-align: center;">"### human: I want to prepare and cook a meal in the kitchen. I have done these things: 1. go to the refrigerator. What should I do next? ### assistant:" <br> "### human: I want to set up a home office workspace. What should I do? ### assistant:"</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">open the refrigerator <br> 1. place a desk against the wall <br> 2. position a chair at the desk...</td>
</tr>
<tr>
<td style="text-align: center;">Embodied Dialogue</td>
<td style="text-align: center;">"### human: ... ### assistant:" <br> "### human: ... ### assistant: ... ### human: ... ### assistant:"</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">[Response] <br> [Response]</td>
</tr>
</tbody>
</table>
<h1>E. Additional Implementation Details</h1>
<p>Scene Encoder. As introduced in the main paper, we adopt the pre-trained masked transformer encoder [39] as the scene encoder. The masked transformer encoder first tokenizes the input point cloud $P C$ into 2,048 points tokens uniformly scattered in the 3D scene with a set-abstraction layer [44]. Following that are three cascaded transformer encoder blocks with masking radius of $0.16,0.64,1.44$, respectively. There is another set-abstraction layer between the first two transformer blocks, which further down-samples the encoded tokens into 1,024 point tokens. The output feature of the scene encoder is $f_{\text {enc }}=\mathbb{R}^{1,024 \times 256}$, representing a 256 dimensioned feature for each of the 1,024 point tokens.
Multi-Modal Transformer. The feature dimension of the Multi-Model Transformer is 768. We use up to 12 heads for each attention layer, and a total of six transformer layers in practice. As mentioned in the main paper, we choose to load the pre-trained word embeddings and positional embeddings from BERT [18]. The total vocabulary size is 30,522 , and the number of position embeddings is 512 .
Trainable Parameters. The total number of trainable parameters is about 111 M , which is less than $10 \%$ of the parameters in the frozen LLM backbone (OPT-1.3B).</p>            </div>
        </div>

    </div>
</body>
</html>
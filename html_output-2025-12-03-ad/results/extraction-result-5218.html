<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5218 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5218</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5218</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-253244506</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2211.00053v1.pdf" target="_blank">Generating Sequences by Learning to Self-Correct</a></p>
                <p><strong>Paper Abstract:</strong> Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5218.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5218.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECTOR — Math (GPT-Neo / GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to mathematical program synthesis (GPT-Neo corrector; GPT-Neo / GPT-3 generators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper trains a separate text-to-text corrector to iteratively rewrite generated Python programs to increase a scalar correctness signal (execution equals gold answer). The corrector is trained with an online self-corrective learning algorithm that forms value-improving, proximate (hypothesis → correction) pairs from a pool of generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo 1.3B (corrector); GPT-Neo 1.3B and GPT-3 (davinci / text-davinci-002) evaluated as generators</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-Neo 1.3B: open-source autoregressive transformer pre-trained on language and code. GPT-3 davinci/text-davinci-002: large, few-shot capable autoregressive models (paper cites ~175B for davinci-scale).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECTION / self-corrector (generate-then-correct)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Decouple generation (p0) and correction (pθ): generate initial hypothesis y0 from p0, then apply a learned corrector pθ to produce y1,...,yT. The corrector is trained by (1) collecting many outputs from a generator/corrector into a datapool, (2) forming pairs (y, y') where v(y')>v(y) and y' is similar to y, (3) sampling pairs proportional to value improvement and proximity and training pθ to map y→y'. The value function v is task-specific (e.g., binary correctness for math). Feedback f(y) can be included (natural language or attribute signals).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical program synthesis (MultiArith, Multitask, GSM8k)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a Python program from a natural-language math problem such that its execution produces the correct numeric answer (high precision required).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MultiArith: corrected performance reported as 98–99% (after correction). GSM8k: generator 8.57% → SELF-CORRECT 21% (always-correcting) and 24% (only-correcting-incorrects). For large-generator experiments: GPT-3 Instruct Multitask: 84.90% → +corrector 90.90% (and 92.75% when training with GPT-3); GPT-3 Instruct GSM8k: 36.80% → +corrector 45.00% (and 45.92% when training with GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>MultiArith: implied ≈38 percentage points lower than corrected (paper states a 38-point improvement). GSM8k generator baseline 8.57%. GPT-3 Instruct generator baselines: Multitask 84.90%, GSM8k 36.80%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: large absolute gains on program-synthesis benchmarks (e.g., MultiArith corrected to 98–99%, GSM8k from 8.57% → 21–24%). For stronger generators (GPT-3), adding the corrector yields nontrivial improvements (e.g., Multitask 84.90→90.90→92.75). Qualitative: example corrections show fixes to unit conversions, missing terms, incorrect operations, and reordering/removal of incorrect lines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Math corrections sometimes require multiple correction iterations (paper notes >1 correction yields additional gains and that 2–3 corrections are often beneficial). When using a learned feedback model that has access to gold solutions (for training or inference), there is risk of solution leakage. The method requires a (computable) scalar value function during training (v); if v is noisy or unavailable, training is less direct. Marginalizing over intermediate generations is intractable; inference uses a single sampled trajectory approximation. The paper also shows that correct pairing and sampling strategies (value pairs and proportional sampling) are important—ablations show degraded performance if omitted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5218.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5218.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECTOR — Lexical Constraints (CommonGen / E2E)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to lexically-constrained generation (GPT-2 generator / GPT-2 corrector)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train a corrector to rewrite sentences so that they satisfy a set of lexical constraints (required words) while preserving fluency; the value function is percent constraint coverage. Corrector is applied iteratively (up to 3 corrections) at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (large for CommonGen, medium for E2E) for both base generator and finetuned corrector</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2: autoregressive transformer models of varying sizes (medium/large) fine-tuned for constrained generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECTION (iterative corrector for constraint satisfaction)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Base generator produces candidate sentences; corrector is trained with self-corrective learning using coverage as v(y). At inference, generate with beam search from generator then apply up to 3 corrections with beam search, stopping early when all constraints are met. Optionally use natural-language feedback listing missing constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lexically-constrained generation (CommonGen and E2E)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a coherent sentence that includes all given constraint words (CommonGen) or convert structured inputs to text meeting specified attributes (E2E).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>CommonGen coverage: GPT-2 baseline 91.50% → SELF-CORRECT 98.77% (coverage). Other fluency metrics (BLEU, CIDER) are maintained or slightly improved. On E2E, SELF-CORRECT outperforms Neurologic-A* decoding while using only beam search (exact numbers in paper tables). Also reported runtimes: SELF-CORRECT ≈0.8s/sent vs NeuroLogic ≈2.04s/sent and NeuroLogic-A* ≈19.24s/sent in one configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>CommonGen coverage: GPT-2 91.50% (baseline). E2E: baseline numbers reported in paper tables (SELF-CORRECT improves coverage and maintains fluency relative to the fine-tuned GPT-2 generator).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: large increases in constraint coverage (e.g., CommonGen 91.50%→98.77%) while preserving or slightly improving automatic and human measures of fluency. SELF-CORRECT combined with NeuroLogic achieves best coverage with faster runtime compared to NeuroLogic-A*.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires a reliable coverage-based value function. If the corrector overfits to forcing constraints, there is risk to fluency, but experiments show fluency is generally maintained. Success depends on the ability to form proximate corrective pairs in the datapool; ablations indicate pairing and proportional sampling are important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5218.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5218.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECTOR — Toxicity Reduction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction for toxicity reduction (GPT-2 Large corrector; applied to GPT-2 XL / GPT-3 at test time)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A corrector is trained to rewrite continuations to reduce toxicity scores (Perspective API) while preserving fluency and diversity. The corrector can be applied to outputs of much larger generators at test time without re-training them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large (generator and finetuned corrector); experiments also apply the trained corrector to larger generators (GPT-2 XL, GPT-3) at test time</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 Large: an autoregressive transformer; corrector is another finetuned GPT-2 Large. Large generators include GPT-2 XL and API-access GPT-3 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECTION (detoxification via iterative correction)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use Perspective API toxicity score as the scalar value v(y) during training; the corrector is trained to map toxic hypotheses to lower-toxicity corrections. At inference, sample multiple continuations (25) with nucleus sampling and apply up to 3 correction iterations from the corrector.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Toxicity reduction (REALTOXICITYPROMPTS benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given prompts designed to elicit toxic completions, produce continuations that minimize toxicity while preserving fluency/diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Metrics reported (Table 9): max-toxicity (Perspective API average maximum) GPT-2 Large baseline = 0.527 → SELF-CORRECT = 0.171 → SELF-CORRECT + FEEDBACK = 0.156. Fluency and diversity measures are maintained (paper reports similar or slightly improved perplexity/diversity). SELF-CORRECT outperforms baselines including PPLM, GeDi, DExpert, PPO, and Quark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>GPT-2 Large baseline max-toxicity = 0.527 (as reported). Larger generators (e.g., GPT-3) also show substantially reduced toxicity when a trained corrector is applied at test time (paper reports qualitative and summary reductions; specific numbers for those swaps are in the main tables).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: large reductions in measured toxicity relative to the same generator (e.g., 0.527→0.171). SELF-CORRECT outperforms decoding-time controls (PPLM, GeDi) and RL-based methods (PPO, Quark) in the reported metrics while preserving fluency and diversity. Qualitative examples show explicit removal/rephrasing of toxic language.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>When using explicit feedback at inference (Perspective API attributes), the method requires external API calls (not strictly necessary but used in experiments). Corrector training needs a reliable toxicity score: noisy scores could mislead learning. Applying a corrector trained on one generator to a very different generator is effective but may have limits (paper shows good transfer but does not guarantee optimality for all generators).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5218.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5218.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECTOR + Explicit Natural-Language Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction augmented with explicit natural-language feedback (Perspective API attributes, GPT-3 feedback model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The corrector is conditioned on natural language feedback f(y) describing what to change (e.g., 'decrease profanity', 'add constraint word: read'), produced from attribute comparisons or a prompted language model; incorporating this feedback improves correction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Feedback models: Perspective API attribute outputs; GPT-3 (text-davinci-002) used as a feedback generator for math. Corrector models as in tasks above (GPT-2/GPT-Neo).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Feedback signals are natural-language strings (e.g., 'decrease toxicity in profanity' or 'adding constraint word: read') derived from attribute-level differences or a prompted LLM that inspects hypothesis vs gold solution. Corrector is trained to consume the feedback token sequence and produce the correction.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECTION + natural language feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Extend the corrector's conditioning to include f(y) (free-form text feedback). During training, construct (x, y, f(y), y') pairs where f(y) is the attribute or descriptive error; train the corrector to map (y, f(y), x) → y'. At inference, compute f(y) (via API or a feedback LLM) and condition the corrector on it.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Applied to toxicity reduction, lexically-constrained generation, and math program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as above but with feedback supplied describing specific attributes to change (toxicity attributes, missing constraints, or targeted program edits).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Toxicity: SELF-CORRECT 0.171 → SELF-CORRECT + FEEDBACK 0.156 (max-toxicity). Lexical constraints (COMMONGEN/COMMONGEN+): reported coverage improved (e.g., an experiment table shows SELF-CORRECT coverage ≈94.58% → +feedback 95.88% in a configuration). Math: paper reports that using GPT-3 as a feedback model and allowing 5 correction iterations yields improved accuracy across the Multitask setting (exact numeric gains are reported in the paper's Table 5 / main results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>SELF-CORRECT without explicit feedback: toxicity 0.171; lexical coverage ≈94.58% in the reported configuration; math results without feedback are lower (see the respective task entries).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: modest but consistent additional improvements when feeding attribute-level or LLM-generated natural language feedback into the corrector (e.g., toxicity max-toxicity 0.171→0.156; coverage improvements on lexically-constrained tasks). Qualitative: examples show feedback guiding the corrector to add missing tokens or reduce a specific category of toxicity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires an external feedback source at inference if using f(y) (e.g., Perspective API or a feedback LLM). For math, feedback experiments used a feedback LLM with access to gold solutions during some training setups (improves feedback quality but risks solution leakage if used at inference); the paper notes this trade-off. The benefit of feedback depends on feedback quality and granularity; low-quality feedback can misdirect corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning from self-sampled correct and partially-correct programs <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Language model cascades <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Learning to summarize with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5218",
    "paper_id": "paper-253244506",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "SELF-CORRECTOR — Math (GPT-Neo / GPT-3)",
            "name_full": "Self-Correction applied to mathematical program synthesis (GPT-Neo corrector; GPT-Neo / GPT-3 generators)",
            "brief_description": "The paper trains a separate text-to-text corrector to iteratively rewrite generated Python programs to increase a scalar correctness signal (execution equals gold answer). The corrector is trained with an online self-corrective learning algorithm that forms value-improving, proximate (hypothesis → correction) pairs from a pool of generations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo 1.3B (corrector); GPT-Neo 1.3B and GPT-3 (davinci / text-davinci-002) evaluated as generators",
            "model_description": "GPT-Neo 1.3B: open-source autoregressive transformer pre-trained on language and code. GPT-3 davinci/text-davinci-002: large, few-shot capable autoregressive models (paper cites ~175B for davinci-scale).",
            "reflection_method_name": "SELF-CORRECTION / self-corrector (generate-then-correct)",
            "reflection_method_description": "Decouple generation (p0) and correction (pθ): generate initial hypothesis y0 from p0, then apply a learned corrector pθ to produce y1,...,yT. The corrector is trained by (1) collecting many outputs from a generator/corrector into a datapool, (2) forming pairs (y, y') where v(y')&gt;v(y) and y' is similar to y, (3) sampling pairs proportional to value improvement and proximity and training pθ to map y→y'. The value function v is task-specific (e.g., binary correctness for math). Feedback f(y) can be included (natural language or attribute signals).",
            "num_iterations": 1,
            "task_name": "Mathematical program synthesis (MultiArith, Multitask, GSM8k)",
            "task_description": "Generate a Python program from a natural-language math problem such that its execution produces the correct numeric answer (high precision required).",
            "performance_with_reflection": "MultiArith: corrected performance reported as 98–99% (after correction). GSM8k: generator 8.57% → SELF-CORRECT 21% (always-correcting) and 24% (only-correcting-incorrects). For large-generator experiments: GPT-3 Instruct Multitask: 84.90% → +corrector 90.90% (and 92.75% when training with GPT-3); GPT-3 Instruct GSM8k: 36.80% → +corrector 45.00% (and 45.92% when training with GPT-3).",
            "performance_without_reflection": "MultiArith: implied ≈38 percentage points lower than corrected (paper states a 38-point improvement). GSM8k generator baseline 8.57%. GPT-3 Instruct generator baselines: Multitask 84.90%, GSM8k 36.80%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: large absolute gains on program-synthesis benchmarks (e.g., MultiArith corrected to 98–99%, GSM8k from 8.57% → 21–24%). For stronger generators (GPT-3), adding the corrector yields nontrivial improvements (e.g., Multitask 84.90→90.90→92.75). Qualitative: example corrections show fixes to unit conversions, missing terms, incorrect operations, and reordering/removal of incorrect lines.",
            "limitations_or_failure_cases": "Math corrections sometimes require multiple correction iterations (paper notes &gt;1 correction yields additional gains and that 2–3 corrections are often beneficial). When using a learned feedback model that has access to gold solutions (for training or inference), there is risk of solution leakage. The method requires a (computable) scalar value function during training (v); if v is noisy or unavailable, training is less direct. Marginalizing over intermediate generations is intractable; inference uses a single sampled trajectory approximation. The paper also shows that correct pairing and sampling strategies (value pairs and proportional sampling) are important—ablations show degraded performance if omitted.",
            "uuid": "e5218.0",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SELF-CORRECTOR — Lexical Constraints (CommonGen / E2E)",
            "name_full": "Self-Correction applied to lexically-constrained generation (GPT-2 generator / GPT-2 corrector)",
            "brief_description": "Train a corrector to rewrite sentences so that they satisfy a set of lexical constraints (required words) while preserving fluency; the value function is percent constraint coverage. Corrector is applied iteratively (up to 3 corrections) at inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (large for CommonGen, medium for E2E) for both base generator and finetuned corrector",
            "model_description": "GPT-2: autoregressive transformer models of varying sizes (medium/large) fine-tuned for constrained generation tasks.",
            "reflection_method_name": "SELF-CORRECTION (iterative corrector for constraint satisfaction)",
            "reflection_method_description": "Base generator produces candidate sentences; corrector is trained with self-corrective learning using coverage as v(y). At inference, generate with beam search from generator then apply up to 3 corrections with beam search, stopping early when all constraints are met. Optionally use natural-language feedback listing missing constraints.",
            "num_iterations": 3,
            "task_name": "Lexically-constrained generation (CommonGen and E2E)",
            "task_description": "Generate a coherent sentence that includes all given constraint words (CommonGen) or convert structured inputs to text meeting specified attributes (E2E).",
            "performance_with_reflection": "CommonGen coverage: GPT-2 baseline 91.50% → SELF-CORRECT 98.77% (coverage). Other fluency metrics (BLEU, CIDER) are maintained or slightly improved. On E2E, SELF-CORRECT outperforms Neurologic-A* decoding while using only beam search (exact numbers in paper tables). Also reported runtimes: SELF-CORRECT ≈0.8s/sent vs NeuroLogic ≈2.04s/sent and NeuroLogic-A* ≈19.24s/sent in one configuration.",
            "performance_without_reflection": "CommonGen coverage: GPT-2 91.50% (baseline). E2E: baseline numbers reported in paper tables (SELF-CORRECT improves coverage and maintains fluency relative to the fine-tuned GPT-2 generator).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: large increases in constraint coverage (e.g., CommonGen 91.50%→98.77%) while preserving or slightly improving automatic and human measures of fluency. SELF-CORRECT combined with NeuroLogic achieves best coverage with faster runtime compared to NeuroLogic-A*.",
            "limitations_or_failure_cases": "Requires a reliable coverage-based value function. If the corrector overfits to forcing constraints, there is risk to fluency, but experiments show fluency is generally maintained. Success depends on the ability to form proximate corrective pairs in the datapool; ablations indicate pairing and proportional sampling are important.",
            "uuid": "e5218.1",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SELF-CORRECTOR — Toxicity Reduction",
            "name_full": "Self-Correction for toxicity reduction (GPT-2 Large corrector; applied to GPT-2 XL / GPT-3 at test time)",
            "brief_description": "A corrector is trained to rewrite continuations to reduce toxicity scores (Perspective API) while preserving fluency and diversity. The corrector can be applied to outputs of much larger generators at test time without re-training them.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large (generator and finetuned corrector); experiments also apply the trained corrector to larger generators (GPT-2 XL, GPT-3) at test time",
            "model_description": "GPT-2 Large: an autoregressive transformer; corrector is another finetuned GPT-2 Large. Large generators include GPT-2 XL and API-access GPT-3 variants.",
            "reflection_method_name": "SELF-CORRECTION (detoxification via iterative correction)",
            "reflection_method_description": "Use Perspective API toxicity score as the scalar value v(y) during training; the corrector is trained to map toxic hypotheses to lower-toxicity corrections. At inference, sample multiple continuations (25) with nucleus sampling and apply up to 3 correction iterations from the corrector.",
            "num_iterations": 3,
            "task_name": "Toxicity reduction (REALTOXICITYPROMPTS benchmark)",
            "task_description": "Given prompts designed to elicit toxic completions, produce continuations that minimize toxicity while preserving fluency/diversity.",
            "performance_with_reflection": "Metrics reported (Table 9): max-toxicity (Perspective API average maximum) GPT-2 Large baseline = 0.527 → SELF-CORRECT = 0.171 → SELF-CORRECT + FEEDBACK = 0.156. Fluency and diversity measures are maintained (paper reports similar or slightly improved perplexity/diversity). SELF-CORRECT outperforms baselines including PPLM, GeDi, DExpert, PPO, and Quark.",
            "performance_without_reflection": "GPT-2 Large baseline max-toxicity = 0.527 (as reported). Larger generators (e.g., GPT-3) also show substantially reduced toxicity when a trained corrector is applied at test time (paper reports qualitative and summary reductions; specific numbers for those swaps are in the main tables).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: large reductions in measured toxicity relative to the same generator (e.g., 0.527→0.171). SELF-CORRECT outperforms decoding-time controls (PPLM, GeDi) and RL-based methods (PPO, Quark) in the reported metrics while preserving fluency and diversity. Qualitative examples show explicit removal/rephrasing of toxic language.",
            "limitations_or_failure_cases": "When using explicit feedback at inference (Perspective API attributes), the method requires external API calls (not strictly necessary but used in experiments). Corrector training needs a reliable toxicity score: noisy scores could mislead learning. Applying a corrector trained on one generator to a very different generator is effective but may have limits (paper shows good transfer but does not guarantee optimality for all generators).",
            "uuid": "e5218.2",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SELF-CORRECTOR + Explicit Natural-Language Feedback",
            "name_full": "Self-Correction augmented with explicit natural-language feedback (Perspective API attributes, GPT-3 feedback model)",
            "brief_description": "The corrector is conditioned on natural language feedback f(y) describing what to change (e.g., 'decrease profanity', 'add constraint word: read'), produced from attribute comparisons or a prompted language model; incorporating this feedback improves correction quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Feedback models: Perspective API attribute outputs; GPT-3 (text-davinci-002) used as a feedback generator for math. Corrector models as in tasks above (GPT-2/GPT-Neo).",
            "model_description": "Feedback signals are natural-language strings (e.g., 'decrease toxicity in profanity' or 'adding constraint word: read') derived from attribute-level differences or a prompted LLM that inspects hypothesis vs gold solution. Corrector is trained to consume the feedback token sequence and produce the correction.",
            "reflection_method_name": "SELF-CORRECTION + natural language feedback",
            "reflection_method_description": "Extend the corrector's conditioning to include f(y) (free-form text feedback). During training, construct (x, y, f(y), y') pairs where f(y) is the attribute or descriptive error; train the corrector to map (y, f(y), x) → y'. At inference, compute f(y) (via API or a feedback LLM) and condition the corrector on it.",
            "num_iterations": null,
            "task_name": "Applied to toxicity reduction, lexically-constrained generation, and math program synthesis",
            "task_description": "Same tasks as above but with feedback supplied describing specific attributes to change (toxicity attributes, missing constraints, or targeted program edits).",
            "performance_with_reflection": "Toxicity: SELF-CORRECT 0.171 → SELF-CORRECT + FEEDBACK 0.156 (max-toxicity). Lexical constraints (COMMONGEN/COMMONGEN+): reported coverage improved (e.g., an experiment table shows SELF-CORRECT coverage ≈94.58% → +feedback 95.88% in a configuration). Math: paper reports that using GPT-3 as a feedback model and allowing 5 correction iterations yields improved accuracy across the Multitask setting (exact numeric gains are reported in the paper's Table 5 / main results).",
            "performance_without_reflection": "SELF-CORRECT without explicit feedback: toxicity 0.171; lexical coverage ≈94.58% in the reported configuration; math results without feedback are lower (see the respective task entries).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: modest but consistent additional improvements when feeding attribute-level or LLM-generated natural language feedback into the corrector (e.g., toxicity max-toxicity 0.171→0.156; coverage improvements on lexically-constrained tasks). Qualitative: examples show feedback guiding the corrector to add missing tokens or reduce a specific category of toxicity.",
            "limitations_or_failure_cases": "Requires an external feedback source at inference if using f(y) (e.g., Perspective API or a feedback LLM). For math, feedback experiments used a feedback LLM with access to gold solutions during some training setups (improves feedback quality but risks solution leakage if used at inference); the paper notes this trade-off. The benefit of feedback depends on feedback quality and granularity; low-quality feedback can misdirect corrections.",
            "uuid": "e5218.3",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning from self-sampled correct and partially-correct programs",
            "rating": 2,
            "sanitized_title": "learning_from_selfsampled_correct_and_partiallycorrect_programs"
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Language model cascades",
            "rating": 1,
            "sanitized_title": "language_model_cascades"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Learning to summarize with human feedback",
            "rating": 1,
            "sanitized_title": "learning_to_summarize_with_human_feedback"
        }
    ],
    "cost": 0.01920775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GENERATING SEQUENCES BY LEARNING TO [SELF-]CORRECT</p>
<p>Sean Welleck 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Ximing Lu 
Allen Institute for Artificial Intelligence</p>
<p>Peter West 
Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Faeze Brahman 
Allen Institute for Artificial Intelligence</p>
<p>Tianxiao Shen 
Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Daniel Khashabi 
Center for Language and Speech Processing
Johns Hopkins University</p>
<p>Yejin Choi 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>GENERATING SEQUENCES BY LEARNING TO [SELF-]CORRECT
Preprint. Under review.
Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present SELF-CORRECTION, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that SELF-CORRECTION improves upon the base generator in three diverse generation tasksmathematical program synthesis, lexically-constrained generation, and toxicity control-even when the corrector is much smaller than the base generator. * First authors, contributed equally. †Second authors, contributed equally.</p>
<p>INTRODUCTION</p>
<p>The standard practice for natural language generation tasks is inherently single-pass: applying a decoding procedure to either a few-shot prompted language model or one tuned for a given task, then considering the generation as "finished" (e.g. ; Brown et al. (2020); ). Powerful generation models often meet most of the task requirements, yet miss a few (e.g., omitting a subset of keywords), or generate incorrect hypotheses that nevertheless provide useful structure (e.g., a correct problem solving strategy with a missing step). However, after generating even a slightly sub-optimal sequence, the single-pass paradigm requires models to "start from scratch", effectively discarding work already done. A more natural, intuitive approach is leveraging the generation as a useful starting point to refine into a higher quality output.</p>
<p>To formalize this intuition, we introduce Self-Correction for Sequence Generation. Figure 1 demonstrates its central principle: a generation model is re-framed as a base generator, which produces a reasonable initial hypothesis but does not need to solve the task in one pass, and a second module-the corrector-trained to make up the difference between the hypothesis and an optimal solution. Neither the generator nor the corrector must solve the full task in one pass, and the corrector can be applied multiple times to iteratively improve the output ( §3.6). We propose a simple, general procedure for training the corrector (Figure 2) by pairing generator outputs with carefully selected targets. The result is a system which self-corrects, producing outputs through multiple generation passes and breaking the task into steps that can be solved by dedicated and efficient sub-systems. Figure 1: SELF-CORRECTORs decompose generation into a base generator that proposes an initial hypothesis, and a corrector that iteratively improves its quality.</p>
<p>We find that Self-Correction is broadly applicable. Training a corrector model improves the base generator on 3 diverse tasks: mathematical program synthesis ( §3.1), lexically constrained generation ( §3.2), and toxicity reduction ( §3.3). The trained corrector model can even be applied to a larger generator with similar performance to training a new corrector ( §3.4), showing that the subtask of correction is transferable, even to stronger generators. Finally, we explore the prospect of introducing a third module to the Self-Correction system ( §3.5)-explicitly using natural language feedback to guide corrections-with promising results. Self-Correction offers an exciting opportunity to build on existing generation models and the sequences they generate, with efficient, effective, and transferable corrector networks.</p>
<p>SELF-CORRECTING SEQUENCE GENERATORS</p>
<p>A typical autoregressive text generator (e.g. GPT-3 (Brown et al., 2020)) maps an input prompt to a distribution over outputs using a single parameterized module (e.g. a large transformer), p 0 (y|x). We explore an alternative that decomposes into two modules, a base generator, and a corrector,
p(y|x) = y0 p 0 (y 0 |x) generator p θ (y|y 0 , x) corrector (1)
where the generator provides an initial hypothesis that is refined by the corrector. In practice, the corrector can be applied multiple times, p(y T |x) = y0 y1 · · · y T −1 p 0 (y 0 |x) t p θ (y t+1 |y t , x). Since a model of this form can both generate and correct its generations, we call it a Self-Corrector.</p>
<p>Self-correctors have several unique properties compared to typical generators. First, a self-corrector decouples generation and correction, allowing us to freely parameterize each module -for instance, by prompting a single language model or using two different language models. In this paper, we develop a framework to train a separate corrector model ( §2.1). We find that the resulting selfcorrector improves upon the generator alone ( §3), even when the corrector is much smaller ( §3.4).</p>
<p>Second, since the generator and the corrector are separated, we can keep the generator as a generalpurpose language model and train the corrector with different objectives for different task requirements. In §2.1, we propose a training algorithm for the corrector that is dedicated to improving generations, where the improvement can be in any aspect, measured by scalar values.</p>
<p>Third, the corrector can receive explicit feedback about intermediate generations to guide subsequent generations. Formally, p(y|x) = y0 p 0 (y 0 |x)p θ (y|y 0 , x, f (y 0 )), where f is the feedback. The feedback can be of many forms, e.g. a sentence, a compiler trace, etc. In contrast, a typical generator that generates in a single pass does not leverage feedback on its own generation. In this paper, we show that the corrector can learn to exploit explicit natural language feedback to achieve better performance ( §3.5). Next, we describe our training framework of the corrector.</p>
<p>LEARNING A CORRECTOR</p>
<p>Our goal is to have the generator generate an initial hypothesis, then improve the hypothesis with the corrector (Eq. 1). We train the corrector to improve the quality of a hypothesis, while staying as Figure 2: SELF-CORRECTIVE LEARNING iteratively trains a corrector by generating hypotheses and corrections, forming value-improving pairs, and selecting those with high similarity for learning.
Algorithm 1 Self-corrective learning input Generator p0, corrector p θ , prompts X, value v(·), feedback f (·)
Initialize datapool D by sampling from p0 Initialization: Eq. 2 for iteration ∈ {1, 2, . . .} do for x ∈ X do Sample hypotheses y from datapool D Generate corrections y ∼ p θ (·|y, x, f (y)) Add all (x, y , v(y ), f (y )) to the datapool D Exploration: Eq. 5 Form value-improving pairs P from D Pairing: Eq. 3 for step in 1, 2, . . . , M do Sample a batch of value-improving pairs from P using Eq. 4 Compute the loss and update θ using gradient descent Learning close as possible to the original hypothesis. Here, quality is measured with a scalar value function v(y) which we assume is accessible at training time (e.g. a classifier).</p>
<p>Since direct supervision on how to improve hypotheses is not available, we design a new algorithm to train the corrector, which we refer to as self-corrective learning. The algorithm collects a pool of generations, groups them and selects pairs of generation that increase in value and are nearby, then updates the corrector on these pairs. As training progresses, more generations are added to the pool using the current corrector. Algorithm 1 summarizes self-corrective learning, detailed below.</p>
<p>Initialization. Self-corrective learning begins with a generator p 0 (y 0 |x), a corrector p θ (y |y, x) , a set of training prompts X, and a value function v : Y → R. Optionally, we can use additional feedback f : Y → F and learn p θ (y |y, x, f (y)), where F is arbitrary.</p>
<p>The algorithm initializes a datapool of (input, output, value, feedback) examples by using the generator to generate multiple outputs for each input. Formally,
D x = {(x, y, v(y), f (y)) | for all y ∈ y 1:N ∼ q(p 0 (·|x))}, D = x∈X D x ,(2)
where y 1:N denotes N outputs generated with decoding algorithm q (e.g. temperature sampling). When available, (x, y, v(y), f (y)) examples from another source (e.g. a dataset) can also be added.</p>
<p>Pairing. Next, self-corrective learning forms value-improving pairs: examples of mapping a hypothesis to a higher-valued correction. We use the datapool D to form a set of (input, hypothesis, correction) pairs. A pair is formed when an output has a higher value than another 1 :
P x = {(x, y, y ) | v(y) &lt; v(y ) for all y, y ∈ D x × D x }, P = x∈X P x ,(3)
Learning. Next, self-corrective learning selects (input, hypothesis, correction) pairs to update the corrector with. We sample a (x, y, y ) pair proportional to its improvement in value as well as the proximity between the hypothesis y and the correction y :
P[(x, y, y )] ∝ exp α · (v(y ) − v(y)) improvement + β · s(y, y ) proximity /Z(y),(4)
where s(y, y ) is a similarity function and Z(y) normalizes over the available corrections for y in P x . Increasing the hyperparameter α ∈ R ≥0 puts more weight on targets that add more value, while increasing β ∈ R ≥0 retains more similar targets. We update the corrector using the cross-entropy loss L(θ) = − log p θ (y |y, x, f (y)) on batches sampled in this way.</p>
<p>Exploration.</p>
<p>During exploration, self-corrective learning adds new generations to the datapool by generating from the current corrector:
D x = {(x, y , v(y ), f (y )) | for all y ∈ y 1:N ∼ q(p θ (·|y, x, f (y))}, D = x∈X D x (5)
and updating the datapool D ← D ∪D . The hypotheses y to correct can come from any source, e.g. newly sampled from the base generator, or from the datapool; we use the latter in our experiments.</p>
<p>Inference. We use the trained corrector along with a generator to generate a trajectory y 0 , y 1 , . . . , y T , and consider y T the final output. Since marginalizing over the intermediate generations in Eq. 1 is intractable, we approximate each summation with a single sequence generated with a decoding algorithm q(·). That is, we decode from the generator, then repeatedly from the corrector:
• Generation: y 0 ∼ q(p 0 (y 0 |x)); • Correction: y t+1 ∼ q(p θ (y t+1 |y t , x, f (y t ))), t = 0, 1, . . . , T − 1.
The stopping time T is either fixed, or when a target value is obtained (if v(y) is available).</p>
<p>EXPERIMENTS</p>
<p>We evaluate SELF-CORRECTION on a diversity of tasks: mathematical program synthesis, in which generations are strictly correct or incorrect, and generators typically have low performance; lexically-constrained generation, which allows for partial credit, and generators usually give partially-correct solutions (e.g. matching 3 out of 5 constraints); and toxicity control, where 'correctness' is more loosely defined, and the output space is much more open-ended. Our experiments are organized to study three settings:</p>
<ol>
<li>
<p>Using self-correctors to improve upon generators ( §3.1,3.2,3.3).</p>
</li>
<li>
<p>Correcting generators that are much larger than the corrector ( §3.4). 3. Leveraging explicit feedback during training and inference ( §3.5).</p>
</li>
</ol>
<p>Next, we describe the self-correction setup and baselines for each task, along with their results. 2</p>
<p>MATHEMATICAL PROGRAM SYNTHESIS</p>
<p>First, we consider mathematical program synthesis (Austin et al., 2021;Mishra et al., 2022). Given a natural language problem specification x, the task is to generate a program y that upon execution returns the correct answer to x. The task is challenging as it draws on language understanding, multiple-step mathematical problem solving (e.g. identifying a solution strategy, decomposing a problem), and leveraging symbolic tools (e.g. built-in operations, variables). Furthermore, the task demands a high level of precision, e.g. a single misplaced operation makes the program incorrect.</p>
<p>Experimental setup. As the corrector we use GPT-Neo 1.3B , an open-source autoregressive language model. GPT-Neo is pre-trained on language and code , and hence is widely used for code-related generation (e.g. ; Ni et al. (2022); Mishra et al. (2022)). We consider two settings for the initial generator: (1) a separate fine-tuned Corrector:</p>
<p>a=2<em>100 b=3</em>50 c=500-a-b #fix answer=c print(answer) Figure 3: Grade-school-math (GSM) self-corrections. On the left, the corrector fixes the units (from minutes to hours) in the generator's solution. On the right, the corrector revises the logic so that the program computes the total savings instead of the spent on tickets. We add #fix here to indicate the change. See Figure 7 and Figure 8 for additional examples.</p>
<p>instance of GPT-Neo 1.3B, and (2) few-shot prompted GPT-3 (Brown et al., 2020). For GPT-3, we evaluate the davinci and text-davinci-002 engines, representative of large (≈ 175B 3 ) generators that are state-of-the-art in related tasks (Wei et al., 2022). See the Appendix for additional details.</p>
<p>Self-correction setup. As the value function we use correctness, which is 1 when the program y executes and outputs the ground-truth answer and 0 otherwise. Our main experiments do not use explicit feedback, i.e. f (y) = ∅. At inference time, we study two settings for the corrector: (1) applying k corrections and selecting the final generation, (2) an oracle setting that only corrects a draft if the draft is incorrect. We use greedy decoding for the generator and corrector, and k = 1.</p>
<p>Datasets. We evaluate on problems from 5 problem solving datasets: MultiArith (Roy et al., 2015), AddSub (Hosseini et al., 2014), SingleOp (Roy et al., 2015), SVAMP (Patel et al., 2021), and GSM8k (Cobbe et al., 2021). As in prior work (Austin et al., 2021;Ni et al., 2022;Mishra et al., 2022), we frame these as program synthesis by converting their solutions to Python programs. 4 We separate our experiments into three increasingly difficult settings:</p>
<ol>
<li>MultiArith, using problems from the MultiArith arithmetic word problem dataset. 2. Multitask, using problems from 4 arithmetic datasets (MultiArith, AddSub, SingleOp, SVAMP). 3. GSM, using problems from the challenging GSM8k dataset.</li>
</ol>
<p>For the MultiArith and Multitask settings, we make train/valid/test splits using 60/20/20% of the respective datasets. Similar to Ni et al. (2022), for the GSM setting we use the official GSM8k test split, and create a validation split using 20% of the training set. Note that the problems and answers in all datasets are the same as those from the original non-program datasets.  Baselines. We compare SELF-CORRECT with its baseline generator (GPT-Neo 1.3B) in all three settings. For the GSM setting, we compare with existing work that uses models within the same magnitude of scale, including NEO FCP+PCP (Ni et al., 2022), which tunes GPT-NEO 2.7B with additional self-sampled programs, and their fine-tuned GPT-NEO 2.7B baseline. We also report 3B and 6B fine-tuned GPT3-like language models from Cobbe et al. (2021), which were trained on the non-program version of GSM8k. We evaluate larger models later in ( §3.4).</p>
<p>Results. As seen in Table 1, the self-corrector improves upon the generator in all three settings, using either inference strategy: always correcting (SELF-CORRECT), or only correcting incorrect solutions (SELF-CORRECT * ). The self-corrector's performance on Multiarith is very high after correction (98-99%), a 38 point improvement over the generator, with a similar gain in the Multitask arithmetic setting. On the challenging GSM dataset, the self-corrector achieves 21%, and 24% with only correcting incorrect solutions, up from 8.57% for the generator. Notably, this is higher than previous work based on the larger 2.7B GPT-Neo, or larger models tuned on the language version of GSM.</p>
<p>The results show that self-corrective learning can improve task performance via training a corrector. Qualitatively, the self-corrector can correct values in a correctly structured solution, fix the order of operations within a multistep solution, adjust unit conversions, and make larger multipart revisions (see Figures 3,7,8). Notably, these are learned automatically through self-corrective learning.</p>
<p>LEXICALLY CONSTRAINED GENERATION</p>
<p>Next, we consider lexically constrained generation. Given a set of constraint words x, the task is to generate a sentence y that includes all the given constraints. Faithful constraint satisfaction is crucial for many downstream tasks, e.g., those that require converting information to text (McKeown, 1985).</p>
<p>Datasets and Metrics. We experiment on COMMONGEN (Lin et al., 2020) and E2E (Novikova et al., 2017). COMMONGEN is a benchmark for generative commonsense reasoning where the task is to generate a coherent sentence given a set of words (e.g., dog, catch). E2E involves converting structured inputs into natural language. For both tasks, we report standard metrics including human/automatic measures of fluency (BLEU, CIDER, etc.) as well as constraint coverage. We collect human measures of fluency on Amazon Mechanical Turk; see the Appendix for details.</p>
<p>Setup. We parameterize the base generator with GPT-2 Radford et al. (2019) (large-size for COM-MONGEN and medium-size for E2E). We fine-tuned the generator for each task. As the value function for self-corrective learning we use coverage, i.e. the percentage of constraints that are present in the output. For inference, we use beam search with the generator, then do up to 3 corrections using beam search, stopping early if all constraints are met. See the Appendix for additional details.</p>
<p>Results. Table 2 shows the evaluation results. The self-corrector substantially improves constraint coverage over its GPT-2 generator for both tasks, while maintaining or improving its language quality. On the COMMONGEN benchmark, the self-corrector paired with the NeuroLogic constrained decoding algorithm  achieves the best results, outperforming the more sophisticated NeuroLogic-A<em> decoding algorithm, while being an order of magnitude faster. Notably, on E2E, self-correction outperforms Neurologic-A</em> decoding, despite only using standard beam search. This suggests that a corrector can be viewed as an alternative to using a more sophisticated decoding procedure (A*) for improving performance without modifying the underlying model. See Figure 9 for qualitative examples.  Table 3: Toxicity reduction. GPT-2 is the base generator. Figure 4: Applying multiple corrections reduces toxicity.</p>
<p>TOXICITY REDUCTION</p>
<p>Next, we consider the task of toxicity reduction (Gehman et al., 2020;Liu et al., 2021). Given a prompt x, the task is to generate a fluent continuation y while avoiding offensive content. This task is important for ensuring safe language model deployment, yet challenging: due to misaligned pretraining objectives (i.e. modeling internet text vs. non-toxic text), language models are susceptible to generating toxic completions, even when prompted with seemingly innocuous text (Gehman et al., 2020). Along with its practical importance, the task tests whether (self-)correctors can be an effective mechanism for controlling the outputs of language models in an open-ended setting.</p>
<p>Datasets and Metrics. We use the REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) which contains 100k prompts designed to elicit toxic generations. Following the experimental setup of Liu et al. (2021), during training we use 85K prompts from the training set, and for evaluation we use the same 10K non-toxic prompts from test set as Liu et al. (2021). We use Perspective API to measure maximum toxicity, defined as the average maximum toxicity over 25 sampled generations, and the (empirical) toxicity probability of at least 1 out of 25 generations being toxic.</p>
<p>Baselines. We compare SELF-CORRECT with its generator (GPT-2) and previously reported baselines from Lu et al. (2022a), including PPLM (Dathathri et al., 2020), GeDi (Krause et al., 2021), DExpert , DAPT (Gururangan et al., 2020), PPO (Lu et al., 2022a), and Quark (Lu et al., 2022a). The latter two -Proximal Policy Optimization (PPO) and Quantized Reward Konditioning (Quark) -represent strong, state-of-the art approaches based on reinforcement learning.</p>
<p>Setup. We use the off-the-shelf GPT-2 Large as the generator, and finetune another GPT-2 Large as the corrector. During inference, we use nucleus sampling with p = 0.9 to generate 25 samples for all baselines. As the value function, we use the Perspective API score, v(y) ∈ [0, 1], which measures the toxicity of the completed sequence. We do up to three corrections with the corrector model.</p>
<p>Results</p>
<p>. Table 3 shows that SELF-CORRECT reduces the rate of toxic generations substantially, while also maintaining fluency and diversity. SELF-CORRECT outperforms all baselines. This includes inference-time algorithms (PPLM, GeDi, DExpert), which do not modify the generator but degrade fluency and yield higher toxicity compared to SELF-CORRECT, as well as reinforcement learning methods (PPO, Quark) that adjust the generator using toxicity as a (negative) reward. The results show that SELF-CORRECT is effective for detoxification, without having to modify the underlying generator. We study implications of this latter property further in the next section.</p>
<p>CHANGING MODULES -CORRECTING GPT-3</p>
<p>Next, we show that a self-corrector can improve the outputs of a generator that is much larger than the corrector. We consider two cases: (1) training with a small generator, then swapping in the larger generator at test time;</p>
<p>(2) training with the larger generator, i.e. using the large generator to initialize the datapool for self-corrective learning, then using the large generator at test time.</p>
<p>Toxicity. We evaluate case (1) for reducing the toxicity of a large generator (GPT-2 XL, GPT-3). We generate an initial sequence using the large generator, then refine it with our corrector trained in the previous experiments ( §3.3). Table 4 shows that the resulting self-corrector (large generator  Table 4: Modularity (program synthesis and detoxification). Self-correctors can correct very large generators, either by swapping in the generator at test-time, or training with the generator. For math synthesis, the corrector is GPT-Neo 1.3B, and here we only correct incorrect outputs. For detoxification, the correction is GPT2-L, and we correct all the outputs.</p>
<ul>
<li>corrector) has substantially reduced toxicity compared to the large generator. This shows the promise of using (self-)correctors for controlling the outputs of large language models.</li>
</ul>
<p>Math program synthesis. Table 4 shows results for math. Analogous to toxicity, the corrector is able to correct larger generators swapped in at test-time. For instance, the GPT-3 Instruct generator has quite high performance (84.90 Multitask, 36.80 GSM), which improves to 90.90 and 45.00, respectively, by adding in a corrector. The self-corrector (large generator + corrector) improves further by training with the GPT-3 Instruct generator, to 92.75 and 45.92, respectively.</p>
<p>LEVERAGING EXPLICIT FEEDBACK</p>
<p>Next, we demonstrate SELF-CORRECT's capacity to incorporate explicit natural language feedback. This amounts to defining a feedback function f , then using the same self-corrective learning and inference algorithms ( §2.1) as in our preceding experiments (in those experiments, f returned ∅).</p>
<p>We show that correctors learn to use the feedback, as evidenced by higher performance.</p>
<p>Toxicity. We use additional fine-grained information from the toxicity API as natural language feedback. Specifically, besides the overall toxicity score, Perspective API also provides scores for fine-grained attributes of toxicity (e.g. identity attack, profanity, flirtation, etc.). At training time, we compare the attribute scores from a hypothesis and its selected correction, and use the attribute with the largest decrease as natural language feedback (e.g. "decrease toxicity in profanity"). At inference time, we call the API on the current hypothesis, and use the attribute with the highest score. Here we use the API at inference time, which is not required in our previous experiments.</p>
<p>Lexical constraints. In training time, we generate natural language feedback for every example pair (x, y, y ) by elaborating the extra lexical constraints satisfied by y but not y. e.g. "adding constraint word: read". At inference time, we elaborate all missing constraints in the current hypothesis.</p>
<p>Math program synthesis. Math program synthesis contains a variety of problem types and errors, without an automated means for identifying the errors (e.g. an API). We explore obtaining natural language feedback about the current program by prompting a large language model. We prompt the model with a problem, hypothesis program, a gold solution, and few-shot demonstrations that show feedback on one part of the program; e.g. In the initial guess, 3 should be subtracted. When the program is correct, the feedback is Correct. At inference time, we also use feedback from the language model. We allow the feedback model access to a gold solution, which we expect makes the feedback higher quality, with the risk of solution leakage at inference-time. Our results in this task are thus used only to study the feasibility of explicit feedback for math program synthesis.</p>
<p>Setup. For toxicity, lexical constraints, and math we use REALTOXICITYPROMPTS, COMMONGEN, and the MULTITASK arithmetic setting, respectively. We follow the setup of each task's previous experiments ( §3.3, §3.2, §3.1), except for math we use 5 correction iterations (previously 1). For math, we use GPT-3 (text-davinci-002) with 6 demonstrations as the feedback model.  Generator (GPT-Neo):</p>
<p>answer=(6.0*8.0) print(answer)</p>
<p>Feedback (GPT-3):</p>
<h1>In the initial guess, # 2 tickets are not included.</h1>
<p>Corrector (GPT-Neo):</p>
<p>answer=(6.0*(8.0+2.0)) print(answer) Figure 5: Self-correction with natural language feedback.</p>
<p>Results. Table 5 shows that explicit natural language feedback improves performance in all three tasks. For toxicity, this means that providing fine-grained attributes (e.g. identity attack, profanity, etc.) during learning and inference improves upon using only the scalar toxicity score. Intuitively, feedback may help the model to focus on a useful correction; e.g., see Figure 5.</p>
<p>ADDITIONAL ABLATIONS AND ANALYSIS</p>
<p>Effect of multiple corrections. Previously, Figure 4 showed that multiple corrections led to better toxicity reduction. On math (Multitask setting), Figure 6 shows that performance improves with more than one correction, and that multiple corrections are more beneficial with feedback. Intuitively, in this math task, after 2-3 corrections the model needs additional guidance.</p>
<p>Effect of pairing and proportional sampling. Self-corrective learning (i) samples pairs for learning proportional to Equation 4, (ii) only pairs sequences that improve value. We ablate these features by training on Multitask using a data pool that samples a pair for learning uniformly (rather than Equation 4), and a data pool without value pairing. Table 6 shows that both improve performance.</p>
<p>Effect of exploration.</p>
<p>To ablate the effect of exploration, we train a baseline only on correction pairs induced from the base generator. Table 7 shows results on the three math datasets, indicating that exploration improves performance.</p>
<p>RELATED WORK</p>
<p>Self-correction relates to recent works on editing text, including modeling Wikipedia edits (Reid &amp; Neubig, 2022;Faltings et al., 2021;Schick et al., 2022), which relies on supervised edits, unsupervised methods (Miao et al., 2019;) that perturb sequences with simple operations (e.g. insertion, deletion), editing with models trained on human-written critiques (Saunders et al., 2022), or iteratively updating continuous variables Li et al., 2022;Qin et al., 2022).    Table 7: Effect of exploration on program synthesis.</p>
<p>In contrast to these, self-correction learns an expressive text-to-text corrector that is trained online to improve a quality measure, without requiring a supervised dataset of edits or critiques. Separately, denoising ground-truth sequences is a common pretraining objective (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020), while self-correction 'denoises' generations to improve a scalar quality measure. Scalar measures are often improved with reinforcement learning (RL) on a base generator (Ziegler et al., 2019;Stiennon et al., 2020;Lu et al., 2022a), which is infeasible for improving many language models (e.g. those accessed through an API), and uses only scalar feedback. Moreover, self-correction learns the difference between a generation and solution, and is complementary to RL-tuned generators, which can be used within a self-corrector. Finally, self-correction decomposes generation into multiple steps, which relates to methods that generate rationales before a response (Wei et al., 2022;Dohan et al., 2022). Self-correction also produces intermediate steps, but each step is of the same form as the output, allowing for re-using previous generations.</p>
<p>CONCLUSION</p>
<p>We introduced self-correctors, a class of models that decompose generation into initial generation and correction steps. We study self-correctors with a fixed base generator along with a corrector trained to improve outputs according to a scalar measure of quality. We presented a simple, general procedure for training the corrector, and find that self-correction is applicable and effective for improving performance, and controlling the outputs of both small and large generators. Moreover, we found that self-correction along with our learning framework provides a promising mechanism for using natural language feedback to improve generation. These findings, along with exploring alternative self-correctors, open up many avenues that we leave for future work.</p>
<p>APPENDIX A ADDITIONAL EXPERIMENTAL DETAILS</p>
<p>A.1 MATHEMATICAL PROGRAM SYNTHESIS</p>
<p>We fine-tune a separate instance of GPT-Neo 1.3B as an initial generator, using the Huggingface library with default hyperparameters, except for evaluation steps, which we set to a small number to ensure a strong checkpoint is selected for each dataset. We use the finetuned initial generator as initialization for the corrector, and tune the corrector on sequences [SC]x[CURR]yi[START]yj [END], where x is a problem, y i and y j form a residual pair, and [·] are special tokens. The loss is on tokens after [START].</p>
<p>Feedback. We write 6 demonstrations using training problems and generations from our GPT-Neo base generator, and use GPT-3 (text-davinci-002) as a feedback model. We use the same training procedure and hyperparameters, except that the sequences now include feedback, [SC]x</p>
<p>[CURR]yi[FEEDBACK]F(x,yi)[START]yj[END]</p>
<p>, where x is a problem, y i and y j form a residual pair, and F (x, y i ) is feedback. We include loss on tokens after [FEEDBACK].</p>
<p>A.2 LEXICALLY-CONSTRAINED GENERATION</p>
<p>Hyper-parameters. Table 8 and Table 9 show hyperparameters for CommonGen and E2E.</p>
<p>Human Evaluation. We evaluate fluency of generations in E2E task using human annotators on Amazon Mechanical Turk (AMT). We randomly sampled 100 instances, along with generations of different baselines and self-corrections. For each instance, we ask 3 annotators to evaluate the fluency of generations on a 3-point Likert scale. We aggregate annotations from 3 annotators using majority vote. We restricted the pool of annotators to those who are located in US or CA, and had 98% approval rate for at least 5,000 previous annotations.</p>
<p>Hyperparameter Assignment</p>
<p>Predictor GPT-2Large # steps 6000 batch size 128 optimizer Adam learning rate 1.e − 5 decoding alg.</p>
<p>beam search (k=5)     C QUALITATIVE EXAMPLES</p>
<p>Figure 6 :
6Math: multiple corrections.</p>
<p>Table 2 :
2Lexically-constrained generation. By training a corrector to optimize constraint satisfaction, SELF-CORRECT improves constraints while maintaining fluency, without modifying the underlying generator. Due to space, we show CIDER for COMMONGEN and human judgement for E2E as measures of fluency. Other metrics show similar trends and can be found in the Appendix.</p>
<p>Table 5 :
5Explicit natural language feedback. Correct * means only correcting incorrect outputs.Problem: 
Melanie had 19 dimes in her bank. Her dad 
gave her 39 dimes and her mother gave her 25 
dimes. How many dimes does Melanie have 
now? </p>
<p>Generator (GPT-Neo): </p>
<p>answer = 19 + 25 
print(answer) </p>
<p>Feedback (GPT-3): </p>
<h1>In the initial guess,</h1>
<h1>39 is not included.</h1>
<p>Corrector (GPT-Neo): </p>
<p>answer = 19 + 25 + 39 
print(answer) </p>
<p>Problem: 
Lana's favorite band was holding a concert 
where tickets were 6 dollars each. Lana 
bought 8 tickets for herself and her friends and 
2 extra tickets in case anyone else wanted to 
go. How much did she spend? </p>
<p>Table 6 :
6Effect of pairing and proportional sampling.Exploration Multiarith Multitask GSM8k </p>
<p>89.20 
73.49 
17.60 </p>
<p>99.17 
78.24 
23.96 </p>
<p>Table 8 :
8Hyperparameters for COMMONGEN.Hyperparameter 
Assignment </p>
<p>Predictor 
GPT-2 M edium </p>
<h1>steps</h1>
<p>10000 
batch size 
100 
optimizer 
Adam 
learning rate 
1.e − 5 
decoding alg. 
beam search (k=5) </p>
<p>Table 9 :
9Hyperparameters for E2E.Avg. Max. Prob. Perplexity dist-2 dist-3B ADDITIONAL RESULTS </p>
<p>Toxicity 
Fluency 
Diversity </p>
<p>GPT2-L 
0.527 
0.520 
11.31 
0.85 
0.85 
SELF-CORRECT 
0.171 
0.026 
11.81 
0.80 
0.83 
SELF-CORRECT + FEEDBACK 
0.156 
0.020 
11.86 
0.80 
0.83 </p>
<p>Table 10 :
10Evaluation results of toxicity reduction experiments with natural language feedback.Bleu-4 CIDER Coverage 
Runtime </p>
<p>NeuroLogic [22] 
26.70 
14.70 
97.70 
2.04s/sent 
NeuroLogic-A*esque [24] 
28.20 
15.20 
97.80 
19.24s/sent </p>
<p>GPT-2 
27.90 
14.97 
91.38 
0.2s/sent 
SELF-CORRECT 
27.98 
15.30 
94.58 
0.8s/sent 
SELF-CORRECT + feedback 
27.82 
15.24 
95.88 
0.8s/sent 
SELF-CORRECT+NeuroLogic 
28.17 
15.28 
97.80 
2.24s/sent </p>
<p>Table 11 :
11Evaluation rresults of lexically-constrained generation on COMMONGEN.Coverage BLEU-4 NIST R-L METEOR CIDER </p>
<p>PREFIX-TUNING (Li &amp; Liang, 2021) 91.16 
70.30 
8.82 72.10 
46.30 
2.46 </p>
<p>GPT-2 
91.50 
67.12 8.67 70.25 
45.58 
2.33 
SELF-CORRECT 
98.77 
68.81 
8.78 68.60 
45.11 
2.38 </p>
<p>Table 12 :
12Evaluation results of lexically-constrained generation on E2E.
We also store the value and feedback for y and y along with (x, y, y ), which we omit to reduce clutter.
Code will be publicly available upon acceptance.
Estimated size of davinci (https://blog.eleuther.ai/gpt3-model-sizes). Further details not available.4  We use data from the Lila benchmark (https://github.com/allenai/Lila).
ACKNOWLEDGMENTSThis work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI.Problem:Ralph watches TV for 4 hours a day from Monday to Friday, and 6 hours a day on Saturday and Sunday. How many hours does Ralph spend watching TV in one week?Generator: a=4<em>7 b=6</em>7 c=a+b answer=c print(answer)Corrector: a=4<em>5 b=6</em>2 c=a+b answer=c print(answer)Problem:The pirates plan to explore 4 islands. Two islands require walking 20 miles per day while the other two islands require 25 miles per day. How many miles will they have to walk if it takes 1.5 days to explore each island?Generator: a=20<em>2 b=25</em>2 c=a+b d=c<em>1.5 e=d+b answer=e print(answer)Corrector: a=20</em>2 b=25<em>2 c=a+b d=c</em>1.5 answer=d print(answer)Figure 7: Grade school math (GSM) self-corrections. Left: the structure of the generator's solution is valid, but it incorrectly uses the total number of days in a week for both a and b; the corrector fixes the program to correctly account for the 5 weekdays and 2 weekend days. Right: the generator's solution contains an incorrect addition at the end; the corrector removes this line, resulting in a correct program.Problem:A spiral notebook costs 15, and a personal planner costs $10. How much would it cost in total to buy 4 spiral notebooks and 8 personal planners at a 20% discount?Figure 8: Grade school math (GSM) self-corrections. Left: the generator's program doesn't include the discount; the corrector appends the discount to the program. Right: a more sophisticated multipart correction. The generator's assignment of b (line 2), and addition to e (line 5) are incorrect. The corrector removes these lines and adjusts the variable names accordingly.Constraints:'table', 'paper', 'read' Generator: A man is reading book on a table.Corrector:A man is reading a book on a table and writing on a piece of paper.Constraints:'stand', 'wait', 'look' Generator: He waits for someone to look at him. Corrector: He waits for someone to stand and look him in the eye.Constraints:'walk', 'park', 'bench', 'dog', 'sit' Generator: A dog sitting on a bench in a park.Corrector:A dog sitting on a bench and a man walking a dog in a park.Constraints:name : The Mill | Type : restaurant | food : English | price : high | customer rating : average | area : riverside | family friendly : yes | near : Cafe Rouge Generator:The Mill is an expensive, three star restaurant located near Cafe Rouge.Corrector:The Mill is a children friendly English restaurant in the riverside area near Cafe Rouge. It has an average customer rating and a high price range.  Prompt: Waters then went on to call the Trump Generator: administration a "bunch of idiots" and "insolent clowns [who are] running" Corrector: Administration's support for the Trans-Pacific Partnership on trade "a total non-event."Figure 10: Toxicity control self-corrections. Words in red indicate toxic language.Constraints
Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, abs/2108.07732ArXiv. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021.</p>
<p>. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, Gpt-Neo, 10.5281/zenodo.5297715doi: 10.5281/ zenodo.5297715Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. ZenodoIf you use this software, please cite it using these metadataSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Au- toregressive Language Modeling with Mesh-Tensorflow. Zenodo, March 2021. doi: 10.5281/ zenodo.5297715. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel MTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs. Jeff Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishZiegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such. Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaBob Mc-Grew. Evaluating large language models trained on code. arXivMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo- tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc- Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv, 2021.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168.</p>
<p>Plug and play language models: A simple approach to controlled text generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, abs/1912.02164ArXiv. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosin- ski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. ArXiv, abs/1912.02164, 2020.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.</p>
<p>. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, abs/2207.10342Language model cascades. ArXiv. Jascha Narain Sohl-DicksteinDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Narain Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model cascades. ArXiv, abs/2207.10342, 2022.</p>
<p>Text editing by command. Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, Bill Dolan, 10.18653/v1/2021.naacl-main.414Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsFelix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan. Text editing by command. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5259-5274, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.414. URL https://aclanthology.org/2021.naacl-main.414.</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2021. URL https://arxiv.org/ abs/2101.00027.</p>
<p>RealToxici-tyPrompts: Evaluating neural toxic degeneration in language models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, 10.18653/v1/2020.findings-emnlp.301Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxici- tyPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Asso- ciation for Computational Linguistics: EMNLP 2020, pp. 3356-3369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Ana Suchin Gururangan, Swabha Marasović, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsSuchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342-8360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740.</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, 10.3115/v1/D14-1058Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 523-533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/D14-1058.</p>
<p>GeDi: Generative discriminator guided sequence generation. Ben Krause, Akhilesh Deepak Gotmare, Bryan Mccann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema Rajani, 10.18653/v1/2021.findings-emnlp.424Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4929-4952, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URL https://aclanthology.org/2021. findings-emnlp.424.</p>
<p>Iterative refinement in the continuous space for non-autoregressive neural machine translation. Jason Lee, Raphael Shu, Kyunghyun Cho, 10.18653/v1/2020.emnlp-main.73Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsJason Lee, Raphael Shu, and Kyunghyun Cho. Iterative refinement in the continuous space for non-autoregressive neural machine translation. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing (EMNLP), pp. 1006-1015, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.73. URL https://aclanthology.org/2020.emnlp-main.73.</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsOnlineMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, On- line, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, doi: 10.18653/ v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.</p>
<p>Diffusionlm improves controllable text generation. Lisa Xiang, John Li, Ishaan Thickstun, Percy Gulrajani, Tatsunori Liang, Hashimoto, abs/2205.14217ArXiv. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion- lm improves controllable text generation. ArXiv, abs/2205.14217, 2022.</p>
<p>CommonGen: A constrained text generation challenge for generative commonsense reasoning. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, 10.18653/v1/2020.findings-emnlp.165Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative com- monsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1823-1840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020. findings-emnlp.165.</p>
<p>DExperts: Decoding-time controlled text generation with experts and antiexperts. Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, Yejin Choi, 10.18653/v1/2021.acl-long.522Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline1Association for Computational LinguisticsAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts and anti- experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6691-6706, Online, August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-long.522. URL https://aclanthology.org/2021. acl-long.522.</p>
<p>Unsupervised paraphrasing by simulated annealing. Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, 10.18653/v1/2020.acl-main.28Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsXianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, and Sen Song. Unsupervised para- phrasing by simulated annealing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 302-312, Online, July 2020. Association for Computational Lin- guistics. doi: 10.18653/v1/2020.acl-main.28. URL https://aclanthology.org/2020. acl-main.28.</p>
<p>NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. Ximing Lu, Peter West, Rowan Zellers, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/2021.naacl-main.339Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4288-4299, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.339. URL https://aclanthology.org/2021.naacl-main.339.</p>
<p>Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Am- manabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning.</p>
<p>. Corr, 10.48550/arXiv.2205.13636CoRR, abs/2205.13636, 2022a. doi: 10.48550/arXiv.2205.13636. URL https://doi.org/ 10.48550/arXiv.2205.13636.</p>
<p>NeuroLogic a<em>esque decoding: Constrained text generation with lookahead heuristics. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Lianhui Ronan Le Bras, Youngjae Qin, Rowan Yu, Noah A Zellers, Yejin Smith, Choi, 10.18653/v1/2022.naacl-main.57Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsXiming Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic a</em>esque decoding: Constrained text generation with lookahead heuristics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pp. 780-799, Seattle, United States, July 2022b. As- sociation for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL https: //aclanthology.org/2022.naacl-main.57.</p>
<p>Text Generation. Kathleen Mckeown, 10.1017/CBO9780511620751Studies in Natural Language Processing. Cambridge University PressKathleen McKeown. Text Generation. Studies in Natural Language Processing. Cambridge Univer- sity Press, 1985. doi: 10.1017/CBO9780511620751.</p>
<p>Cgmh: Constrained sentence generation by metropolis-hastings sampling. Ning Miao, Hao Zhou, Lili Mou, Rui Yan, Lei Li, 10.1609/aaai.v33i01.33016834Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. Cgmh: Constrained sentence generation by metropolis-hastings sampling. Proceedings of the AAAI Conference on Artificial Intelligence, 33 (01):6834-6842, Jul. 2019. doi: 10.1609/aaai.v33i01.33016834. URL https://ojs.aaai. org/index.php/AAAI/article/view/4659.</p>
<p>Lila: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Learning from self-sampled correct and partially-correct programs. Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir Radev, Jianfeng Gao, Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning from self-sampled correct and partially-correct programs, 2022. URL https://arxiv.org/abs/2205.14318.</p>
<p>The E2E dataset: New challenges for end-to-end generation. Jekaterina Novikova, Ondřej Dušek, Verena Rieser, 10.18653/v1/W17-5525Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. the 18th Annual SIGdial Meeting on Discourse and DialogueSaarbrücken, GermanyAssociation for Computational LinguisticsJekaterina Novikova, Ondřej Dušek, and Verena Rieser. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Di- alogue, pp. 201-206, Saarbrücken, Germany, August 2017. Association for Computational Lin- guistics. doi: 10.18653/v1/W17-5525. URL https://aclanthology.org/W17-5525.</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080- 2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.</p>
<p>Cold decoding: Energy-based constrained text generation with langevin dynamics. Lianhui Qin, Sean Welleck, Daniel Khashabi, Yejin Choi, arXiv:2202.11705arXiv preprintLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based con- strained text generation with langevin dynamics. arXiv preprint arXiv:2202.11705, 2022.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.</p>
<p>Learning to model editing processes. Machel Reid, Graham Neubig, Machel Reid and Graham Neubig. Learning to model editing processes, 2022. URL https: //openreview.net/forum?id=1bEaEzGwfhP.</p>
<p>Reasoning about quantities in natural language. Subhro Roy, Tim Vieira, Dan Roth, Transactions of the Association for Computational Linguistics. 3Subhro Roy, Tim Vieira, and Dan Roth. Reasoning about quantities in natural language. Transac- tions of the Association for Computational Linguistics, 3:1-13, 2015.</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv. org/abs/2206.05802.</p>
<p>Peer: A collaborative language model. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, Sebastian Riedel, Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: A collabora- tive language model, 2022. URL https://arxiv.org/abs/2208.11663.</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. LinCurran Associates, Inc33Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feed- back. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems, volume 33, pp. 3008-3021. Curran Asso- ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, abs/2201.11903ArXiv. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.</p>
<p>M Daniel, Nisan Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. arXiv preprintDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.</p>            </div>
        </div>

    </div>
</body>
</html>
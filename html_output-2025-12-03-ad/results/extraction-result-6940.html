<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6940 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6940</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6940</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-277596537</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.02902v1.pdf" target="_blank">Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs. In this work, we extend this line of inquiry by investigating the impact on confidence estimation. We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence. We then further explore the integration of confidence calibration techniques with self-improvement. Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step. Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration. Our work pioneers the study of self-improving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6940.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6940.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Basic Prompting (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Basic Prompting Self-Improvement on Llama-deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative intrinsic self-improvement where the model generates an answer, produces self-feedback, and refines the answer; evaluated on Llama-deepseek (DeepSeek-R1-Distill-Llama-8B) using MMLU, tracking accuracy and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A distilled 'deep thinking' variant (DeepSeek-R1 distillation) of an open Llama-family model evaluated as a stronger-reasoning LLM in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Basic Prompting (iterative self-feedback / self-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial answer a^(0); the LLM then generates feedback f^(t)=LLM(q,a^(t)) and uses that feedback plus prior answer to produce a refined answer a^(t+1)=LLM(q,a^(t),f^(t)).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-domain multiple-choice/knowledge/problem-solving benchmark covering 57 subject areas to test knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (ACC) and Expected Calibration Error (ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Round 0 baseline ACC and ECE not given as numbers in paper; qualitatively the model started with high ECE and baseline ACC prior to iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>After 5 rounds ACC showed progressive improvement (continuous error correction and progressive improvement was reported); calibration worsened (ECE steadily increased; high overconfidence persisted).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Iterative self-improvement increased overconfidence (rising ECE) and exhibited poor calibration overall; high initial ECE that did not improve with iterations; calibration trade-off where ACC gains came with worse confidence estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6940.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6940.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Basic Prompting (Llama-2-7b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Basic Prompting Self-Improvement on Llama-2-7b-chathf</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same iterative self-feedback procedure applied to Llama-2-7b-chat-hf ('Llama') showing different dynamics: ACC degraded over rounds while ECE increased.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf (Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama-2 7B chat-fine-tuned model used as a baseline/weaker-reasoning model in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Basic Prompting (iterative self-feedback / self-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial answer, have the model generate feedback about its answer, and refine the answer using that feedback in successive rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-domain benchmark of 57 subjects for knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (ACC) and Expected Calibration Error (ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Round 0 baseline ACC/ECE not given numerically; baseline ACC was used as reference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>By round 5, ACC generally declined relative to baseline (basic prompting produced the lowest ACC among methods for this model); ECE increased (overconfidence amplified in high-confidence region).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weaker intrinsic reasoning caused self-improvement to degrade accuracy; iterative self-feedback amplified overconfidence (higher ECE) and produced poorer calibration in high-confidence bins.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6940.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6940.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT Prompting (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting Self-Improvement on Llama-deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Chain-of-Thought prompting where the model first generates step-by-step reasoning context which is then used as context for answer generation and iterative refinement; on the stronger Llama-deepseek this yielded steady ACC improvements across rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distilled deep-reasoning Llama variant used to examine CoT effects.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) iterative self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model first generates CoT c = LLM_CoT(q), then produces an answer a using q and c; feedback f^(t)=LLM(q,c,a^(t)) is used to produce refined answers a^(t+1)=LLM(q,c,a^(t),f^(t)).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect with CoT context</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Large multi-domain reasoning benchmark (57 subjects).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (ACC) and Expected Calibration Error (ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Initial ACC/ECE not numerically reported; baseline before CoT iterations used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Longer CoT (e.g., 512 tokens) plus iterative self-improvement produced higher ACC across rounds (progressive self-improvement trend reported); however ECE remained high or increased (overconfidence persisted).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although ACC improved, calibration did not necessarily improve (rising ECE); context-length constraints (4096 token limit) caused late drops when CoT was long; CoT benefits are model-capability dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6940.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6940.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT Prompting (Llama-2-7b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting Self-Improvement on Llama-2-7b-chathf</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT prompting applied to the weaker Llama model produced mixed effects: shorter CoT helped moderately, while longer CoT initially helped but accuracy later degraded for some rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf (Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B Llama chat model evaluated for CoT prompting length and iterative self-refinement behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) iterative self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate chain-of-thought context then iteratively generate feedback and refine answers using the CoT as context.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect with CoT context</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>57-subject multi-domain benchmark testing knowledge and reasoning abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (ACC) and Expected Calibration Error (ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Baseline ACC/ECE not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Shorter CoT (128 tokens) gave moderate boosts for Llama; longer CoT (512 tokens) yielded higher ACC than other self-improvement methods within Llama but accuracy later declined after several rounds (mixed trajectory); ECE tended to be lower with CoT than with basic prompting but overconfidence still present in high-confidence bins.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weaker models benefit less reliably from long CoT; longer CoT can hit context-length limits and later cause accuracy drops; overall effect is mixed and model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6940.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6940.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Supervised Fine-Tuning (SFT) - Llama-deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (LoRA) Self-Improvement on Llama-deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning the model (LoRA r=32, alpha=16, 5 epochs) on a curated self-correction dataset to produce improved outputs; for Llama-deepseek this SFT harmed accuracy and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeepSeek-R1 distilled Llama variant fine-tuned using LoRA on a self-correction-focused dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Supervised Fine-Tuning (LoRA-based SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Fine-tune model parameters on a dataset of queries and corrected answers (LoRA low-rank updates), then evaluate resulting model without iterative generate-then-reflect cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>tuning-based (no generate-then-reflect iterations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multitask benchmark across 57 subjects to test knowledge/reasoning; used to evaluate effect of SFT on reasoning and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (ACC) and Expected Calibration Error (ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Baseline (pre-fine-tuning) ACC/ECE not numerically reported; used as reference in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Fine-tuned basic prompting resulted in the poorest ACC among conditions for Llama-deepseek and one of the highest ECE values (i.e., SFT degraded both accuracy and calibration for this stronger model).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Degradation attributed to dataset mismatch between fine-tuning data and reasoning tasks; stronger models with intrinsic reasoning might be harmed by SFT when fine-tuning distribution mismatches evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6940.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6940.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Supervised Fine-Tuning (SFT) - Llama-2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (LoRA) Self-Improvement on Llama-2-7b-chathf</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LoRA-based supervised fine-tuning improved the weaker Llama model's accuracy and calibration on MMLU according to the paper's qualitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf (Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B Llama chat model fine-tuned via LoRA and evaluated on MMLU for SFT effects.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Supervised Fine-Tuning (LoRA-based SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Fine-tune model (LoRA) on a curated self-correction dataset; evaluate resulting model without iterative generate-then-reflect cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>tuning-based (no generate-then-reflect iterations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>57-subject benchmark for knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (ACC) and Expected Calibration Error (ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Pre-fine-tuning baseline ACC/ECE not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Fine-tuned methods surpassed original basic prompting in ACC and showed improved ECE (better calibration) for the weaker Llama model.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Benefits depended on model capacity and dataset alignment; weaker models improved while stronger reasoning models could be harmed if fine-tuning data mismatches target reasoning distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 1)</em></li>
                <li>Pride and prejudice: Llm amplifies self-bias in self-refinement <em>(Rating: 2)</em></li>
                <li>When can llms actually correct their own mistakes? a critical survey of self-correction of llms <em>(Rating: 2)</em></li>
                <li>A survey of confidence estimation and calibration in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6940",
    "paper_id": "paper-277596537",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Basic Prompting (Llama-deepseek)",
            "name_full": "Basic Prompting Self-Improvement on Llama-deepseek",
            "brief_description": "Iterative intrinsic self-improvement where the model generates an answer, produces self-feedback, and refines the answer; evaluated on Llama-deepseek (DeepSeek-R1-Distill-Llama-8B) using MMLU, tracking accuracy and calibration.",
            "citation_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)",
            "model_description": "A distilled 'deep thinking' variant (DeepSeek-R1 distillation) of an open Llama-family model evaluated as a stronger-reasoning LLM in the paper.",
            "model_size": "8B",
            "reflection_method_name": "Basic Prompting (iterative self-feedback / self-refine)",
            "reflection_method_description": "Generate an initial answer a^(0); the LLM then generates feedback f^(t)=LLM(q,a^(t)) and uses that feedback plus prior answer to produce a refined answer a^(t+1)=LLM(q,a^(t),f^(t)).",
            "iteration_type": "generate-then-reflect (iterative self-feedback)",
            "num_iterations": 5,
            "task_name": "MMLU (Massive Multitask Language Understanding)",
            "task_description": "Multi-domain multiple-choice/knowledge/problem-solving benchmark covering 57 subject areas to test knowledge and reasoning.",
            "evaluation_metric": "Accuracy (ACC) and Expected Calibration Error (ECE)",
            "performance_before_reflection": "Round 0 baseline ACC and ECE not given as numbers in paper; qualitatively the model started with high ECE and baseline ACC prior to iterations.",
            "performance_after_reflection": "After 5 rounds ACC showed progressive improvement (continuous error correction and progressive improvement was reported); calibration worsened (ECE steadily increased; high overconfidence persisted).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Iterative self-improvement increased overconfidence (rising ECE) and exhibited poor calibration overall; high initial ECE that did not improve with iterations; calibration trade-off where ACC gains came with worse confidence estimates.",
            "uuid": "e6940.0",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Basic Prompting (Llama-2-7b)",
            "name_full": "Basic Prompting Self-Improvement on Llama-2-7b-chathf",
            "brief_description": "Same iterative self-feedback procedure applied to Llama-2-7b-chat-hf ('Llama') showing different dynamics: ACC degraded over rounds while ECE increased.",
            "citation_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf (Llama)",
            "model_description": "Open-source Llama-2 7B chat-fine-tuned model used as a baseline/weaker-reasoning model in experiments.",
            "model_size": "7B",
            "reflection_method_name": "Basic Prompting (iterative self-feedback / self-refine)",
            "reflection_method_description": "Generate an initial answer, have the model generate feedback about its answer, and refine the answer using that feedback in successive rounds.",
            "iteration_type": "generate-then-reflect (iterative self-feedback)",
            "num_iterations": 5,
            "task_name": "MMLU (Massive Multitask Language Understanding)",
            "task_description": "Multi-domain benchmark of 57 subjects for knowledge and reasoning.",
            "evaluation_metric": "Accuracy (ACC) and Expected Calibration Error (ECE)",
            "performance_before_reflection": "Round 0 baseline ACC/ECE not given numerically; baseline ACC was used as reference.",
            "performance_after_reflection": "By round 5, ACC generally declined relative to baseline (basic prompting produced the lowest ACC among methods for this model); ECE increased (overconfidence amplified in high-confidence region).",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Weaker intrinsic reasoning caused self-improvement to degrade accuracy; iterative self-feedback amplified overconfidence (higher ECE) and produced poorer calibration in high-confidence bins.",
            "uuid": "e6940.1",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CoT Prompting (Llama-deepseek)",
            "name_full": "Chain-of-Thought Prompting Self-Improvement on Llama-deepseek",
            "brief_description": "Use of Chain-of-Thought prompting where the model first generates step-by-step reasoning context which is then used as context for answer generation and iterative refinement; on the stronger Llama-deepseek this yielded steady ACC improvements across rounds.",
            "citation_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)",
            "model_description": "Distilled deep-reasoning Llama variant used to examine CoT effects.",
            "model_size": "8B",
            "reflection_method_name": "Chain-of-Thought (CoT) iterative self-improvement",
            "reflection_method_description": "Model first generates CoT c = LLM_CoT(q), then produces an answer a using q and c; feedback f^(t)=LLM(q,c,a^(t)) is used to produce refined answers a^(t+1)=LLM(q,c,a^(t),f^(t)).",
            "iteration_type": "generate-then-reflect with CoT context",
            "num_iterations": 5,
            "task_name": "MMLU (Massive Multitask Language Understanding)",
            "task_description": "Large multi-domain reasoning benchmark (57 subjects).",
            "evaluation_metric": "Accuracy (ACC) and Expected Calibration Error (ECE)",
            "performance_before_reflection": "Initial ACC/ECE not numerically reported; baseline before CoT iterations used for comparison.",
            "performance_after_reflection": "Longer CoT (e.g., 512 tokens) plus iterative self-improvement produced higher ACC across rounds (progressive self-improvement trend reported); however ECE remained high or increased (overconfidence persisted).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Although ACC improved, calibration did not necessarily improve (rising ECE); context-length constraints (4096 token limit) caused late drops when CoT was long; CoT benefits are model-capability dependent.",
            "uuid": "e6940.2",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CoT Prompting (Llama-2-7b)",
            "name_full": "Chain-of-Thought Prompting Self-Improvement on Llama-2-7b-chathf",
            "brief_description": "CoT prompting applied to the weaker Llama model produced mixed effects: shorter CoT helped moderately, while longer CoT initially helped but accuracy later degraded for some rounds.",
            "citation_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf (Llama)",
            "model_description": "7B Llama chat model evaluated for CoT prompting length and iterative self-refinement behaviour.",
            "model_size": "7B",
            "reflection_method_name": "Chain-of-Thought (CoT) iterative self-improvement",
            "reflection_method_description": "Generate chain-of-thought context then iteratively generate feedback and refine answers using the CoT as context.",
            "iteration_type": "generate-then-reflect with CoT context",
            "num_iterations": 5,
            "task_name": "MMLU (Massive Multitask Language Understanding)",
            "task_description": "57-subject multi-domain benchmark testing knowledge and reasoning abilities.",
            "evaluation_metric": "Accuracy (ACC) and Expected Calibration Error (ECE)",
            "performance_before_reflection": "Baseline ACC/ECE not numerically reported.",
            "performance_after_reflection": "Shorter CoT (128 tokens) gave moderate boosts for Llama; longer CoT (512 tokens) yielded higher ACC than other self-improvement methods within Llama but accuracy later declined after several rounds (mixed trajectory); ECE tended to be lower with CoT than with basic prompting but overconfidence still present in high-confidence bins.",
            "improvement_observed": null,
            "limitations_or_failure_cases": "Weaker models benefit less reliably from long CoT; longer CoT can hit context-length limits and later cause accuracy drops; overall effect is mixed and model-dependent.",
            "uuid": "e6940.3",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Supervised Fine-Tuning (SFT) - Llama-deepseek",
            "name_full": "Supervised Fine-Tuning (LoRA) Self-Improvement on Llama-deepseek",
            "brief_description": "Fine-tuning the model (LoRA r=32, alpha=16, 5 epochs) on a curated self-correction dataset to produce improved outputs; for Llama-deepseek this SFT harmed accuracy and calibration.",
            "citation_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)",
            "model_description": "DeepSeek-R1 distilled Llama variant fine-tuned using LoRA on a self-correction-focused dataset.",
            "model_size": "8B",
            "reflection_method_name": "Supervised Fine-Tuning (LoRA-based SFT)",
            "reflection_method_description": "Fine-tune model parameters on a dataset of queries and corrected answers (LoRA low-rank updates), then evaluate resulting model without iterative generate-then-reflect cycles.",
            "iteration_type": "tuning-based (no generate-then-reflect iterations reported)",
            "num_iterations": null,
            "task_name": "MMLU (Massive Multitask Language Understanding)",
            "task_description": "Multitask benchmark across 57 subjects to test knowledge/reasoning; used to evaluate effect of SFT on reasoning and calibration.",
            "evaluation_metric": "Accuracy (ACC) and Expected Calibration Error (ECE)",
            "performance_before_reflection": "Baseline (pre-fine-tuning) ACC/ECE not numerically reported; used as reference in comparisons.",
            "performance_after_reflection": "Fine-tuned basic prompting resulted in the poorest ACC among conditions for Llama-deepseek and one of the highest ECE values (i.e., SFT degraded both accuracy and calibration for this stronger model).",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Degradation attributed to dataset mismatch between fine-tuning data and reasoning tasks; stronger models with intrinsic reasoning might be harmed by SFT when fine-tuning distribution mismatches evaluation tasks.",
            "uuid": "e6940.4",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Supervised Fine-Tuning (SFT) - Llama-2-7b",
            "name_full": "Supervised Fine-Tuning (LoRA) Self-Improvement on Llama-2-7b-chathf",
            "brief_description": "LoRA-based supervised fine-tuning improved the weaker Llama model's accuracy and calibration on MMLU according to the paper's qualitative results.",
            "citation_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf (Llama)",
            "model_description": "7B Llama chat model fine-tuned via LoRA and evaluated on MMLU for SFT effects.",
            "model_size": "7B",
            "reflection_method_name": "Supervised Fine-Tuning (LoRA-based SFT)",
            "reflection_method_description": "Fine-tune model (LoRA) on a curated self-correction dataset; evaluate resulting model without iterative generate-then-reflect cycles.",
            "iteration_type": "tuning-based (no generate-then-reflect iterations reported)",
            "num_iterations": null,
            "task_name": "MMLU (Massive Multitask Language Understanding)",
            "task_description": "57-subject benchmark for knowledge and reasoning.",
            "evaluation_metric": "Accuracy (ACC) and Expected Calibration Error (ECE)",
            "performance_before_reflection": "Pre-fine-tuning baseline ACC/ECE not numerically reported.",
            "performance_after_reflection": "Fine-tuned methods surpassed original basic prompting in ACC and showed improved ECE (better calibration) for the weaker Llama model.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Benefits depended on model capacity and dataset alignment; weaker models improved while stronger reasoning models could be harmed if fine-tuning data mismatches target reasoning distribution.",
            "uuid": "e6940.5",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 1,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Pride and prejudice: Llm amplifies self-bias in self-refinement",
            "rating": 2,
            "sanitized_title": "pride_and_prejudice_llm_amplifies_selfbias_in_selfrefinement"
        },
        {
            "paper_title": "When can llms actually correct their own mistakes? a critical survey of self-correction of llms",
            "rating": 2,
            "sanitized_title": "when_can_llms_actually_correct_their_own_mistakes_a_critical_survey_of_selfcorrection_of_llms"
        },
        {
            "paper_title": "A survey of confidence estimation and calibration in large language models",
            "rating": 1,
            "sanitized_title": "a_survey_of_confidence_estimation_and_calibration_in_large_language_models"
        }
    ],
    "cost": 0.013349,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models
3 Apr 2025</p>
<p>Liangjie Huang 
University of Illinois Chicago</p>
<p>Dawei Li daweili5@asu.edu 
Arizona State University</p>
<p>Huan Liu huanliu@asu.edu 
Arizona State University</p>
<p>Lu Cheng lucheng@uic.edu 
University of Illinois Chicago</p>
<p>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models
3 Apr 2025F4659B157D7A52EC5615072A42CE9082arXiv:2504.02902v1[cs.CL]Will self-improve lead to bias in confidence estimation?
Large Language Models (LLMs) have demonstrated remarkable selfimprovement capabilities, whereby models iteratively revise their outputs through self-generated feedback.While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs.In this work, we extend this line of inquiry by investigating the impact on confidence estimation.We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods-and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence.We then further explore the integration of confidence calibration techniques with self-improvement.Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step.Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration.Our work pioneers the study of selfimproving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.</p>
<p>Introduction</p>
<p>The development of Large Language Models (LLMs) has catalyzed transformative changes across numerous domains, from natural language understanding and generation (Storks et al., 2019;Weld et al., 2022) to assisting in complex question-answering and decisionmaking processes (Li et al., 2025b;2024a;Tan et al., 2024).To handle this, one of the emerging techniques for LLMs is self-improvement (Bai et al., 2022;Kim et al., 2023), wherein LLMs iteratively review their own responses and refine their outputs based on self-generated feedback to enhancing the performance.This process fosters human-like reflective thinking and has proven effective across a range of tasks and applications (Tong et al., 2024;Pan et al., 2024;Li et al., 2024b).</p>
<p>However, some recent studies also report cases where LLM-based self-improvement does not bring a significant boost and can even degrade the model's performance (Zhang et al., 2024a;Wu et al., 2024).One contributing factor to this counterintuitive outcome is selfbias (Xu et al., 2024b;Wataoka et al., 2024;Li et al., 2025a)-the tendency of LLMs to favor their own generated content.This cognitive bias impedes LLMs from providing impartial feedback on their outputs, thereby hindering effective self-correction and self-improvement.</p>
<p>Borrowing this insight, we propose our first research question: Will self-improvement also lead to bias in confidence estimation?As LLMs become increasingly integral to both research Figure 1: The two research questions and overview of our exploration process in this work.and industry applications (Zhu et al., 2025), the ability to accurately express confidence or uncertainty in their outputs is crucial (Su et al., 2024), particularly in high-risk scenarios (Thirunavukarasu et al., 2023;Li et al., 2024c).If self-improvement methods introduce self-bias in confidence estimation, this could pose a significant threat to LLM safety and reliability, creating substantial challenges in the pursuit of trustworthy AI (Sun et al., 2024;Huang et al., 2025).To investigate this, we examine three types of self-improvement methods in our experiments: Basic prompting, Chain-of-Thought (CoT) prompting, and Tuningbased approaches (First et al., 2023;Han et al., 2024;Zhang et al., 2024b;Aky ürek et al., 2023;Xie et al., 2025).We implement each method and analyze its impact on LLMs' confidence estimation performance.Our results reveal a clear trend of increasing overconfidence as self-improvement iterations progress, leading to a continuously rising Expected Calibration Error (ECE) score (Guo et al., 2017).</p>
<p>As calibration Guo et al. (2017); Geng et al. (2023); Xie et al. (2024b) serves as an effective technique to align a model's confidence with its correctness and thus improve models confidence estimation, we pose our second research question: How to combine calibration with the self-improvement method to mitigate the confidence estimation bias?To explore this, we examine the compounded effects of calibration and self-improvement.Specifically, we evaluate three experimental settings to analyze their interaction: (1) multiple self-improvement iterations followed by calibration, (2) calibration applied before multiple self-improvement iterations, and (3) iterative calibration and self-improvement at each step.Our results indicate that applying calibration before self-improvement leads to sustained improvements over time.Meanwhile, self-improvement then calibration achieves the best ECE score, resulting in better-calibrated confidence estimates.</p>
<p>To summarize, our contribution in this paper is in two-fold:</p>
<p>• From a novel perspective of calibration, we first propose to explore selfimprovement's impact on LLMs' confidence estimation and reveal a significant overconfidence issue caused by iterative self-improvement.</p>
<p>• We explore several self-improvement paradigms to showcase the compounded effect when combining self-improvement with calibration, producing LLMs that are both effective and reliable in real-world applications.</p>
<p>Related Work</p>
<p>Self-Improvement generally refers to the way that LLMs try to review and correct their own mistakes to achieve performance improvement on their own.Broader view on this topic can be categorized into three types of methods (Kamoi et al., 2024): Intrinsic Improvement, External Information and Fine-tuning.Intrinsic Improvement means LLMs generate feedbacks to their own responses and correct themselves (Kim et al., 2023;Dhuliawala et al.).Recently, some researchers found that intrinsic improvement can be affected by the prompting mechanism.Specifically, prompting with CoT and self-refinement style have gained effective results (Shinn et al., 2023;Madaan et al., 2023;Fu et al., 2025).An iterative and intrinsic self-improvement process where LLMs generates a response, receives feedback via a feedback model, and refines its output using the same model as a refinement model.</p>
<p>External information will introduce some extra tools to help check the responses from LLMs.These include many scopes, such as code executors (Chen et al., 2023), search engines (Zhao et al., 2023), human feedback (Chen et al., 2024) and so on.Fine-tuning for self-improvement generates feedback and then refines its responses, so that it achieves self-improvement via learning from these corpus.Popular methods in this branch are supervised fine-tuning (First et al., 2023;Han et al., 2024;Zhang et al., 2024b) and reinforcement learning (RL) (Aky ürek et al., 2023;Xie et al., 2025).</p>
<p>In this study, we focus on intrinsic self-improvement, a concept that has attracted considerable debate in recent years.On the one hand, studies such as Bai et al. (2022) suggest that prompting LLMs can enable them to self-correct harmful outputs.Other work, including self-refine approaches (Madaan et al., 2023) and RCI Prompting (Kim et al., 2023), demonstrates how LLMs can iteratively refine their own responses in reasoning tasks.On the other hand, Huang et al. (2023a) indicates that LLMs may struggle to enhance their performance without external feedback, and that their performance can even degrade after self-improvement attempts.Further research similarly reports that achieving selfimprovement by solely relying on prompts remains challenging (Gou et al., 2023;Olausson et al., 2023).These findings motivate us to investigate the underlying mechanisms and conditions under which intrinsic self-improvement can be most effectively realized.Additionally, we also adopt supervised fine-tuning method for self-improvement, considering its efficiency and effectiveness compared with the RL-based one.</p>
<p>Calibration.Popular methods for calibrating language models can be broadly classified into five categories: verbalization-based, self-consistency-based, logit-based, internal state-based, and surrogate approaches (Geng et al., 2023;Xie et al., 2024b).Verbalization-based methods leverage an LLM to explicitly express uncertainty about its answers.For instance, Xu et al. (2024a) fine-tune its language models and then prompt the LLMs to indicate the confidence of its response by generating a probability scaler.Self-consistency-based methods rely on the intuition that confident models produce consistent outputs.Consequently, these methods sample multiple responses and estimate confidence by clustering outputs based on similarity (Huang et al., 2024).Internal state-based examines how the model's internal layers (like attention heads or hidden states) respond during generation (Azaria &amp; Mitchell, 2023;Li et al., 2023).And surrogate models are used to mimic or approximate a black-box LLM in order to estimate confidence or uncertainty (Shrivastava et al., 2023).</p>
<p>However, both verbalization-based and self-consistency-based methods may be constrained by the LM's ability to follow instructions accurately.Appropriate layers or heads in internal state-based methods vary a lot and thus are hard to unify for comparison.And the surrogate model is not the same as the target model.For a better fit to our research goal, logit-based methods directly utilize predicted token probabilities to evaluate response confidence (Huang et al., 2023b).Typically, logits are transformed or calculated to represent the forecasted confidence.The logits are believed to have the capacity to offer a more nuanced understanding of confidence knowledge (Widmann et al., 2021;Kuhn et al.;Jang et al., 2024).Notably, as a logits-based method, temperature scaling has been widely applied in LLMs for answering questions.By adjusting the temperature parameter, it influences the model's probability distribution over possible answers, thereby enhancing its performance in selecting the correct option (Peeperkorn et al., 2024;Xie et al., 2024a;Shen et al., 2024).</p>
<p>In this work, we thus use logits-based calibration approach to discover the relationship between self-improvement and calibration in multi domains.</p>
<p>Methods</p>
<p>In this section, we introduce the three backbone techniques for self-improvement, as well as various manners to marry self-improvement methods with calibration.The overall framework can be found in Figure 1.</p>
<p>Self-Improvement</p>
<p>Basic Prompting.Basic prompting in this work refers to clearly and directly prompt LLMs to answer questions without guiding the LLM to output its CoT.As shown in left panel of Figure 1, an LLM generates an answer a (0) = LLM( q ) given query q .Subsequently, this initial answer undergoes an evaluation phase, where feedback f (t) in round t is produced.This feedback is then in a subsequent step "Answer Refining," to revise the answer given the feedback to get a (t+1) .
f (t) = LLM(q, a (t) ), t ≥ 0; a (t+1) = LLM(q, a (t) , f (t) ).(1)
This iterative loop above involving answer generation, feedback provision, and refinement contributes to enhancing the LLM's performance (Madaan et al., 2023).</p>
<p>Chain-of-Thought (CoT) Prompting.CoT Prompting (Wei et al., 2022) involves guiding LLMs through step-by-step reasoning to solve complex problems, generate detailed explanation or feedback.Recent advancement, including the emerging DeepSeek-R1 models (Guo et al., 2025), leverage CoT by explicitly generating structured intermediate steps, significantly boosting the reasoning capabilities.We also introduce this technique as one of our self-improvement methods.Instead of directly outputting answers from LLMs, we first guide the language models to generate a CoT response c = LLM CoT ( q ), explicitly articulating the reasoning steps involved in answering the query.After getting the CoT for a specific question, we then use this generated CoT as new context to guide the LLM to provide the answer a = LLM(q, c):
f (t) = LLM(q, c, a (t) ), a (t+1) = LLM(q, c, a (t) , f (t) ).(2)
Supervised Fine-Tuning (SFT).Apart from prompting, we also utilize SFT (Dong et al., 2024a;b) with specific datasets to investigate the resoning ability in LLMs, thereby exploring its role in self-improvement and calibration.A SFT loss is typically defined as
L SFT (θ) = − ∑ (q,y)∈D I ∑ i=1 log p θ (y i |q, y &lt;i ). (3)
Noted that I is the total number of tokens in the target output sequence y.And p θ (y i |q, y &lt;i ) is the probability assigned by the model to token y i .Query q is input sequence, which means the question prompt.i is to locate output sequence position.θ is the model parameters.D is the finetuning dataset, a collection of question query and the according answer pairs.</p>
<p>Calibration</p>
<p>In the context of LLMs, calibration refers to how well an LLM's predicted confidence aligns with its actual accuracy.As one of the most common calibration approaches, temperature scaling (Guo et al., 2017) is a post-hoc calibration strategy that aligns model predictions with observed probabilities.We adapt the method from (Shen et al., 2024), a temperature scaling calibration approach tailored to LLMs that learns an auxiliary model to map the outputs of the LLM to better-calibrated probabilities.The calibration formula is shown below:
p(y n |q n , τ k ; W) = exp(w T y ϕ(q n ; W)/τ k ) ∑ v ′ exp(w T v ′ ϕ(q n ; W)/τ k ) . (4)
The key idea is to train an neuro network to fit the logits distribution and then use the network to infer task-specific latent temperatures τ, allowing the model to adapt to new questions with learned parameters.ϕ(q n ; W) is the feature that the language model produces for the input token sequence q n .∑ v ′ exp(w T v ′ ϕ(q n ; W)/τ k ) is the sum of exponential over all possible tokens v ′ in the vocabulary.W and w are model parameters and the logit vector transformation, respectively.The method is computationally efficient, to preserve the accuracy of the LLM, and takes a step towards being universal among different tasks.</p>
<p>Marrying Self-improvement with Calibration</p>
<p>We propose three methods to answer the second research question, via marrying selfimprovement with calibration, as shown in Figure 1.</p>
<p>The Iterative Method refers to a process where each round consists of basic-promptingbased self-improvement followed by calibration.It facilitates a direct observation of how selfimprovement and calibration mutually enhance or constrain each other during successive iterations.
a (t+1) = Calibrate Self-Improve(a (t) ) , t = 0, 1, 2, . . .(5)
In the equation, a (0) is the inital response from LLM for query q and a (t+1) is the result after self-improvement and calibration in each round.</p>
<p>Calibration then Self-improvement performs calibration only once at the beginning, and in the subsequent rounds, only self-improvement is conducted.</p>
<p>a (0) = Calibrate a (0) ; a (t+1) = Self-Improve a (t) , t = 0, 1, 2, . . .</p>
<p>This helps determine if an initial calibration provides a stronger foundation for subsequent self-improvements, reducing the risk of deviation from the ideal state.</p>
<p>In Multi Self-Improvement then Calibration, it instructs the LLM to perform T rounds of self-improvement first, followed by a single calibration:
a (t+1) = Self-Improve a (t) , t = 0, 1, 2, . . . , T − 1. a (final) = Calibrate a (T) . (7)
This design allows the model to freely explore and maximize its potential before using calibration to correct accumulated errors and biases.It also helps to assess whether a single calibration step remains effective in correcting deviations accumulated through multiple self-improvement iterations.</p>
<p>EXPERIMENTS</p>
<p>Set Up</p>
<p>Models.In this paper, we use popular open-source LLMs.Specifically, Llama-2-7b-chathf (Touvron et al., 2023) as a standard LLM and DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025) as a deep thinking model, denoted as Llama-deepseek in later section, are used to investigate the effectiveness and relationship between self-improvement and calibration.</p>
<p>Dataset.MMLU (Hendrycks et al., 2020) is utilized in our paper for evaluation.The MMLU is a comprehensive benchmark, which covers 57 sub-datasets spanning various subjects including STEM (Science, Technology, Engineering, and Mathematics), humanities, social sciences, and other specialized areas.It consists of multi-domain questions that assess both world knowledge and problem-solving abilities, making it well-suited for evaluating calibration and self-improvement.We use all 57 sub-datasets for the experiments.</p>
<p>In terms of SFT dataset, we follow (Zhang et al., 2024b), which focuses on the self-correction abilities of small, open-source LMs, exploring whether they can self-correct with minimal guidance.We adopt their refined dataset to fine-tune the LLMs.</p>
<p>Evaluation Metrics.We investigate our research using these two metrics: Accuracy (ACC) for LLM prediction accuracy and ECE for model calibration measurement.ECE is a widely adopted metric that measures the discrepancy between predicted confidence and its actual accuracy.A high ECE score reflects poor calibration, indicating a significant discrepancy between the model's predicted confidence and its empirical accuracy on the given dataset.
ECE = K ∑ k=1 |B k | N |acc(B k ) − conf(B k )| . (8)
ECE essentially computes a weighted sum of absolute differences between accuracy acc(B k ) and confidence conf(B k ) across bins.Here, we use 10 bins of width 0.1 each in [0,1], where N denotes the number of model's generations and K denotes the number of bins.</p>
<p>Supervised Fine-Tuning (SFT).We performed SFT of the base models using Low-Rank Adaptation (LoRA) (Hu et al., 2022).Specifically, the LoRA configuration employed a rank (r) of 32, along with a scaling factor (lora alpha) of 16.LoRA dropout was set to 0.05.</p>
<p>Training was conducted using a batch size of 8. To efficiently manage GPU memory, gradient checkpointing was enabled.The maximum gradient norm was clipped at 0.3 to ensure training stability.We fine-tuned the model for five epochs using a learning rate of 2e-4, coupled with a cosine learning rate scheduler and a warm-up ratio of 0.05.Additionally, training with BF16 was utilized to enhance training efficiency.One A100 GPU was used in our experiment.</p>
<p>Results and Analysis</p>
<p>In this section, we will be answering the following research questions introduced in the Introduction with the experiment results analysis.</p>
<p>• RQ 1: Will self-improvement lead to LLM self-bias in confidence estimation?</p>
<p>• RQ 2: What are the compounded effects of marrying calibration and selfimprovement on model performance?</p>
<p>RQ 1: Will self-improvement lead to bias in confidence estimation?</p>
<p>As seen in Figure 2, the two upper charts are the accuracy scores of Llama-deepseek and Llama, on the left and right respectively.Similarly, the two bottom charts are the ECE scores.</p>
<p>Longer CoT generally enhances model accuracy but model's inherent reasoning capacity can modulate its effectiveness.Longer CoT consistently yielded the highest accuracy across both Llama-deepseek and Llama, with Llama-deepseek demonstrating a clear trend of progressive self-improvement over multiple rounds (Jin et al., 2024).While Llama's accuracy eventually declined after several rounds of self-improvement using a longer CoT, it still outperformed other self-improvement methods within Llama.Notably, the CoT methods with 512 tokens in Llama experienced a late drop in accuracy due to the 4096token context limitation.Moreover, CoT length significantly influenced inference accuracy: longer CoT (512-token limit) reliably produced higher accuracy compared to shorter CoT (128-token limit).Interestingly, for Llama-the weaker of the two base models-shorter CoT sequences provide a moderate boost, suggesting limited CoT can still benefit models of relatively constrained reasoning capabilities.</p>
<p>The effectiveness of prompting-based methods appears to be strongly influenced by the model's intrinsic reasoning capabilities.In Llama-deepseek, while basic prompting Note.Basic means the basic prompting method and cot is for CoT prompting with different length of tokens.Tuned stands for the fine-tuned method experienced a slight decline in accuracy initially, it subsequently facilitated continuous error correction and progressive improvement.By contrast, Llama exhibited a general deterioration in ACC as the number of prompting rounds increased, with basic prompting ultimately yielding the lowest accuracy among all tested self-improvement strategies.</p>
<p>SFT may be more beneficial for weaker models.Our fine-tuning experiments revealed divergent outcomes in these two models.In Llama-Deepseek, both CoT with 128 and 512 tokens exhibited lower ACC than the original basic prompting method and the fine-tuning ones.Notably, fine-tuned basic prompting resulted in the poorest ACC among all conditions, with one of the highest ECE.This indicates that calibration worsened for Llama-deepseek post-fine-tuning.In Llama, however, fine-tuning produced improvements: both fine-tuned methods surpassed the original basic prompting in terms of ACC, and their ECE also improved, suggesting better calibration.The performance drop in Llama-deepseek after fine-tuning may stem from a mismatch between the fine-tuning dataset and the reasoning dataset, thereby causing noticeable degradation.As Llama-deepseek possesses stronger inherent reasoning capabilities, it may be ill-suited to the chosen dataset and approach.In contrast, Llama, with weaker intrinsic reasoning ability, appears to benefit from fine-tuning, which leads to more pronounced gains.</p>
<p>In addition, Llama-deepseek exhibited a substantially higher initial ECE than Llama and maintained an high level in all self-improvement experiments, suggesting poor calibration under iterative improvement.In contrast, although basic prompting yielded the highest ECE among Llama's self-improvement methods, its ECE does not exceed 0.7-lower than that of Llama-deepseek-indicating that Llama remains inherently better calibrated than Llama-deepseek.Moreover, ECE values tended to be lower when CoT reasoning was applied, particularly in Llama.To further investigate these findings, we propose conducting confidence distribution bias experiments to compare predicted confidence versus actual accuracy across various confidence intervals (e.g., 0.1-0.2,0.2-0.3,etc.) under both basic and CoT prompting for the two LLMs.</p>
<p>Self-Bias in Confidence Estimation.We use the longer CoT and basic prompting to illustrate self-improvement performance at the initial and intermediate stages.The x-axis in Figure 3 represents confidence levels divided into ten bins, while the y-axis denotes the corresponding accuracy.</p>
<p>We can observe that in Llama-deepseek, there is no substantial calibration improvement from the first to the fifth round; rather, the performance appears to deteriorate.In particular, the accuracy of high-confidence predictions decreases, suggesting that self-improvement might have exacerbated overconfidence in certain areas.Furthermore, during Llama-deepseek's self-improvement with basic prompting, a notable dip in confidence occurs in the 0.2-0.3interval.</p>
<p>Figure 4: Llama's accuracy and confidence distribution.</p>
<p>In Figure 4, we observe that in the initial round for Llama, the relationship between confidence and accuracy generally aligns with expectations: as confidence increases, accuracy also improves.However, in the high-confidence region, the model exhibits a tendency toward overconfidence, characterized by high confidence yet relatively lower accuracy.By the fifth round of self-improvement, this issue becomes more pronounced, exacerbating the overconfidence effect.Similarly, during the fifth round of CoT, we observe a sudden rise in accuracy within the 0.3-0.4confidence range.Based on these, we thus can conclude that prompting and fine-tuning based methods in iterative self-improvement can introduce or amplify self-biases in confidence estimation.</p>
<p>RQ 2: What are the compounded effects of marrying calibration and self-improvement on model performance?</p>
<p>As calibration serves as an effective technique to align a model's confidence with its correctness and thus improve models confidence estimation, we propose three experiments using the basic prompting approach to investigate the RQ2.The results highlight notable commonalities and distinctions between the LLMs, as shown in Figure 5.</p>
<p>ECE can be diminished when combined with self-improvement after calibration.Multi self-improvement-then-calibration methods yield reduced ECE, with the latter achieving a markedly lower ECE compared to the other two approaches.Despite performing calibration after each round, the iterative method continues to exhibit relatively high ECE, possibly because the alternating introduction of self-improvement dilutes the calibration effect and consequently compromises alignment between confidence and accuracy.Furthermore, both Note.Cms means calibration then multi self-improvement and msc is multi self-improvement then calibration.Ics stands for iterative calibration and self-improvement the "calibration then multi self-improvement" and "iterative" methods produce relatively high ECE-particularly in Llama, where ECE increases substantially compared to selfimprovement alone.One explanation for this phenomenon is that calibration is primarily intended to align the model's confidence with its actual accuracy.However, during selfimprovement, the model refines its responses based on self-generated feedback, which can shift its confidence distribution.As a result, the self-bias dominates over calibration effect when the calibration is performed at the beginning.</p>
<p>Calibration can serve as a better foundation in self-improvement for stronger LLM.</p>
<p>The "calibration then multi self-improvement" strategy in Llama-deepseek shows steady improvement of ACC, surpassing the performance of Llama-deepseek's longer CoT in pure self-improvement setting.Additionally, unlike basic prompting-based self-improvement, this method does not exhibit an initial accuracy drop.In Llama-deepseek, the multi selfimprovement-then-calibration approach effectively rectifies errors in earlier stages while maintaining a relatively stable ECE; however, it manifests some fluctuations of ACC in later stages, suggesting a pronounced impact on model reasoning ability.Meanwhile, for Llama, the iterative method achieves the highest ACC across multiple rounds, although the overall trend still declines, reinforcing the notion that calibration is beneficial for selfimprovement but Llama's comparatively weaker intrinsic reasoning limits its capacity for effective self-correction.</p>
<p>Conclusion</p>
<p>In this work, we study the effect of self-improving LLM from a calibration perspective.The first research question we propose is will self-improvement leads to self-bias in confidence estimation.Based on our experiment results on three mainstream self-improvement approaches, we reveal an obvious trend of increasing overconfidence as self-improvement iterations progress, leading to a large ECE score value after the self-improvement process.This motivates our second research question on how to marry calibration with selfimprovement to mitigate this overconfidence.With several potential solutions proposed and analyzed, we conclude that ECE can be largely diminished when applying calibration after self-improvement.In the future, we will explore the calibration of self-improving LLMs in larger sizes of LLMs, as well as use a wider spectrum of calibration methods to validate the robustness and generalization of our findings.Besides, investigating self-improvement and calibration in multilingual or multimodal settings would provide a richer understanding of how overconfidence manifests in more complex scenarios.</p>
<p>Figure 2 :
2
Figure 2: Results of Self-Improvement in Different Methods.</p>
<p>Figure 3 :
3
Figure 3: Llama-deepSeek's accuracy and confidence distribution.</p>
<p>Figure 5 :
5
Figure 5: Self-Improve and Calibration Relationship Experiment Result.</p>
<p>Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Afra Feyza Aky Ürek, Ekin Aky Ürek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, Niket Tandon, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>The internal state of an llm knows when it's lying. Amos Azaria, Tom Mitchell, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Learning from natural language feedback. Chen, J A Scheurer, Campos, Korbak, Chan, Bowman, Cho, Perez, Transactions on machine learning research. 2024</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>Chain-of-verification reduces hallucination in large language models. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason E Weston, ICLR 2024 Workshop on Reliable and Responsible Foundation Models. </p>
<p>How abilities in large language models are affected by supervised fine-tuning data composition. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024a1</p>
<p>Threshold filtering packing for supervised fine-tuning: Training related samples within packs. Jiancheng Dong, Lei Jiang, Wei Jin, Lu Cheng, arXiv:2408.093272024barXiv preprint</p>
<p>Baldur: Whole-proof generation and repair with large language models. Emily First, Markus N Rabe, Talia Ringer, Yuriy Brun, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Multiple choice questions: Reasoning makes large language models (llms) more self-confident even when they are wrong. Tairan Fu, Javier Conde, Gonzalo Martínez, María Grandury, Pedro Reviriego, arXiv:2501.097752025arXiv preprint</p>
<p>A survey of confidence estimation and calibration in large language models. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, Iryna Gurevych, arXiv:2311.082982023arXiv preprint</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Small language model can self-correct. Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, Yanghua Xiao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR. 1232022</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023aarXiv preprint</p>
<p>On the trustworthiness of generative foundation models: Guideline. Yue Huang, Chujie Gao, Siyuan Wu, Haoran Wang, Xiangqi Wang, Yujun Zhou, Yanbo Wang, Jiayi Ye, Jiawen Shi, Qihui Zhang, arXiv:2502.142962025arXiv preprint</p>
<p>Look before you leap: An exploratory study of uncertainty measurement for large language models. Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, Lei Ma, arXiv:2307.102362023barXiv preprint</p>
<p>Arman Cohan, and Bhuwan Dhingra. Calibrating long-form generations from large language models. Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, arXiv:2402.065442024arXiv preprint</p>
<p>Calibrated decision-making through llm-assisted retrieval. Chaeyun Jang, Hyungi Lee, Seanie Lee, Juho Lee, arXiv:2411.088912024arXiv preprint</p>
<p>The impact of reasoning step length on large language models. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>When can llms actually correct their own mistakes? a critical survey of self-correction of llms. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems. 202336</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, NeurIPS ML Safety Workshop. </p>
<p>From generation to judgment: Opportunities and challenges of llm-as-a-judge. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, arXiv:2411.165942024aarXiv preprint</p>
<p>Smoa: Improving multi-agent large language models with sparse mixture-of-agents. Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Satvik Kumar, Lijie Chaudhary, Jiayi Hu, Shen, arXiv:2411.032842024barXiv preprint</p>
<p>Dalk: Dynamic co-augmentation of llms and kg to answer alzheimer's disease questions with scientific literature. Dawei Li, Shu Yang, Zhen Tan, Jae Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024c</p>
<p>Preference leakage: A contamination problem in llm-as-a-judge. Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, Huan Liu, arXiv:2502.015342025aarXiv preprint</p>
<p>Inference-time intervention: Eliciting truthful answers from a language model. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, Advances in Neural Information Processing Systems. 362023</p>
<p>Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, arXiv:2502.17419From system 1 to system 2: A survey of reasoning large language models. 2025barXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Michael Solar-Lezama ; Liangming Pan, Wenda Saxon, Deepak Xu, Xinyi Nathani, William Wang, Wang Yang, arXiv:2306.098962023. 2024Transactions of the Association for Computational Linguistics12arXiv preprintIs self-repair a silver bullet for code generation?</p>
<p>Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous, arXiv:2405.00492Is temperature the creativity parameter of large language models?. 2024arXiv preprint</p>
<p>Thermometer: Towards universal calibration for large language models. Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory W Wornell, Soumya Ghosh, Forty-first International Conference on Machine Learning. 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Llamas know what gpts don't show: Surrogate models for confidence estimation. Vaishnavi Shrivastava, Percy Liang, Ananya Kumar, arXiv:2311.088772023arXiv preprint</p>
<p>Shane Storks, Qiaozi Gao, Joyce Y Chai, arXiv:1904.01172Commonsense reasoning for natural language understanding: A survey of benchmarks, resources, and approaches. 2019arXiv preprint</p>
<p>Api is enough: Conformal prediction for large language models without logit-access. Jiayuan Su, Jing Luo, Hongwei Wang, Lu Cheng, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, arXiv:2401.05561Trustworthiness in large language models. 20243arXiv preprint</p>
<p>Large language models for data annotation and synthesis: A survey. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature medicine. 2982023</p>
<p>Can llms learn from previous mistakes? investigating llms' errors to boost for reasoning. Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Self-preference bias in llm-as-a-judge. Koki Wataoka, Tsubasa Takahashi, Ryokan Ri, arXiv:2410.218192024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>A survey of joint intent detection and slot filling models in natural language understanding. Henry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon, Caren Soyeon, Han, ACM Computing Surveys. 5582022</p>
<p>Calibration tests beyond classification. David Widmann, Fredrik Lindsten, Dave Zachariah, International Conference on Learning Representations, Virtual conference. May 3-May 7, 2021. 2021International Conference on Learning Representations, ICLR</p>
<p>Progress or regress? self-improvement reversal in post-training. Ting Wu, Xuefeng Li, Pengfei Liu, arXiv:2407.050132024arXiv preprint</p>
<p>Calibrating language models with adaptive temperature scaling. Johnathan Xie, Annie Chen, Yoonho Lee, Eric Mitchell, Chelsea Finn, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024a</p>
<p>A survey of calibration process for black-box llms. Liangru Xie, Hui Liu, Jingying Zeng, Xianfeng Tang, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Qi He, arXiv:2412.127672024barXiv preprint</p>
<p>Teaching language models to critique via reinforcement learning. Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, arXiv:2502.034922025arXiv preprint</p>
<p>Sayself: Teaching llms to express confidence with self-reflective rationales. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024a</p>
<p>Pride and prejudice: Llm amplifies self-bias in self-refinement. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024b1</p>
<p>Understanding the dark side of llms' intrinsic self-correction. Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang, arXiv:2412.149592024aarXiv preprint</p>
<p>Small language models need strong verifiers to selfcorrect reasoning. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2404.171402024barXiv preprint</p>
<p>Verify-andedit: A knowledge-enhanced chain-of-thought framework. Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
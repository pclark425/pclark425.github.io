<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4617 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4617</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4617</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f" target="_blank">Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work introduces FLAMe, a family of Foundational Large Autorater Models that outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact.</p>
                <p><strong>Paper Abstract:</strong> As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for fine-tuning, as shown in our reward model evaluation case study (FLAMe-RM). On Reward-Bench, FLAMe-RM-24B achieves 87.8% accuracy, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch fine-tuning approach that offers competitive RewardBench performance using 25×fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4617.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4617.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAMe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Foundational Large Autorater Models (FLAMe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of multitask instruction-tuned autorater LLMs trained on a curated collection of 102 quality-assessment tasks with 5.3M+ human judgments to serve as automatic evaluators of model outputs across many dimensions (helpfulness, factuality, safety, coding, math, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multitask supervised autorater (FLAMe)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Train an instruction-tuned generative LLM (PaLM-2-24B) in a supervised multitask text-to-text format on a large, standardized collection of human evaluation examples. At inference the model is prompted with task-specific INSTRUCTIONS/CONTEXT and asked to produce the human-style EVALUATION. It can be applied zero-shot to held-out evaluation tasks or fine-tuned for downstream evaluation roles (e.g., reward-model evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific human labels converted into unified outputs: pairwise preference (which response is better), pointwise Likert scores (coherence, helpfulness, etc.), classification (yes/no on attribution or safety), and open-ended explanations. These map to criteria such as helpfulness, factuality/attribution, safety/harmlessness, correctness (math/code), coherence/fluency, verbosity, and instruction-following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAMe (initialized from PaLM-2-24B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>24B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / NLP (cross-domain evaluation applicability)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>explanations/hypotheses as natural-language outputs (general-purpose evaluand)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>FLAMe-24B (baseline) achieved RewardBench 86.0% accuracy; FLAMe-RM-24B (fine-tuned on 4 pairwise datasets) reached 87.8% and outperformed GPT-4-0125 (85.9%) and GPT-4o (84.7%) among generative models trained on permissive data. FLAMe variants outperformed other LLM-as-a-Judge models on 8/12 autorater benchmarks and achieved an average CoBBLEr bias of 0.13 vs GPT-4's 0.31 (lower is better).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (model-as-judge) trained to emulate human judgments; evaluation of FLAMe is automated via held-out benchmarks containing human labels and direct comparison to human annotations / leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by (1) held-out autorater benchmarks (RewardBench, LLM-AggreFact, SummFeedback, etc.) with human-labeled test sets; (2) comparison to other LLM autoraters and leaderboard rankings; (3) bias benchmark (CoBBLEr) comparing to measured human-anchored biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires large curated human-judgment datasets; training data heterogeneity and inconsistent original annotation standards; risk of inheriting/propagating human annotator biases; English- and short-context-focused (2048 tokens); lacks a universal development set correlated with all target distributions; supervised training only (RLHF/DPO left for future work).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>FLAMe collection (102 tasks, 5.3M human judgments) plus evaluation on RewardBench, LLM-AggreFact, SummFeedback, CoBBLEr, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4617.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tail-patch (tailpatch) fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tail-patch ablation and re-weighted multitask mixture optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A one-time, computationally efficient procedure to estimate each training task's effect on a target downstream distribution and produce an optimized mixture of tasks for fine-tuning (used to create FLAMe-Opt-RM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Tail-patch ablation + re-weighted mixture</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Start from a partially trained multitask checkpoint, then fine-tune briefly (a 'tail-patch') on each individual training task for a limited number of steps (e.g., 3k) to measure that task's marginal impact on a target evaluation (e.g., RewardBench categories). Rate tasks by impact (Helpful +2, Somewhat +1, No effect 0, Harmful -1), bundle tasks by effect, and assign fixed mixture weights to bundles to form an optimized training mixture. Fine-tune on that optimized mixture to quickly improve target performance using far fewer datapoints.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in target benchmark performance (e.g., RewardBench category scores) per task tail-patch; tasks are scored by magnitude/direction of effect (significant/stable, slight, none, harmful).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2-24B → FLAMe-Opt-RM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>24B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / NLP (applies to optimizing evaluation models across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a (method for optimizing evaluator training mixtures)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>FLAMe-Opt-RM reached RewardBench ~87.0% (competitive with FLAMe) after 5k training steps using ~25× fewer datapoints than training FLAMe from scratch; optimized mixture reached higher early performance on RewardBench Chat Hard and Safety.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: tail-patch effects are measured by automated evaluation on held-out benchmark metrics (RewardBench) which themselves are derived from human-labeled preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Measured by direct improvement on target benchmark (RewardBench) and checking held-out benchmark performance to avoid overfitting; early experiments checked correlation with other held-out tasks (found weak correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires a target benchmark with sufficient signal (no general dev set used); initial bundle weight choices based on intuition and not exhaustively tuned; task-level tail-patch can be compute-intensive if many tasks and large models (mitigated by smaller models).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used RewardBench as the target distribution for optimization; tail-patch run on the FLAMe constituent tasks (HelpSteer, PRM800K, CommitPack, HH RLHF Harmlessness, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4617.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unified text-to-text format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified INSTRUCTIONS/CONTEXT/EVALUATION text-to-text format for quality assessment tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized text-to-text representation for heterogeneous human evaluation tasks that encodes task definitions and annotator instructions to enable multitask supervised training of autoraters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Unified text-to-text task formulation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Convert diverse human-evaluation datasets (pairwise, pointwise, classification, open-ended) into a single text-to-text format where each example supplies an INSTRUCTIONS block describing the task and desired outputs, CONTEXT giving prompt and candidate responses, and an EVALUATION containing the human label. This enables consistent prompting and direct supervised learning across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Native human-annotation labels per dataset (preferences, Likert scores, binary classifications, and free-form rationales), preserved and standardized for model training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a (formatting/framework for evaluation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Enables effective transfer learning: FLAMe trained on the unified format generalized to held-out tasks and outperformed many LLM-as-judges; facilitated zero-shot application to new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Framework for automated supervised training, but relies on human-labeled data for targets; evaluation of models trained on this format is automated via benchmark comparisons to human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical: models trained on the unified format evaluated on many held-out human-labeled benchmarks (RewardBench, LLM-AggreFact, SummFeedback, etc.) showing improved generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires manual crafting of task definitions and sometimes consultation with original dataset authors (3-4 hours per dataset); underlying human labels vary in annotation standards and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>All FLAMe constituent datasets (see Table 5) converted into INSTRUCTIONS/CONTEXT/EVALUATION format.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4617.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pairwise / Pointwise / Classification / Open-ended</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canonical evaluation modalities: Pairwise, Pointwise, Classification, Open-ended</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four canonical human-evaluation output formats used to judge model-generated content: pairwise preferences, pointwise ratings (Likert), categorical classification, and free-form explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Evaluation modalities (pairwise/pointwise/classification/open-ended)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Pairwise: present two responses and ask which is better (used extensively for preference/reward-model training). Pointwise: assign a score to a single response (e.g., Likert scales for coherence/fluency). Classification: binary or multi-class labels (e.g., grounded/ungrounded, harmless/harmful). Open-ended: free-form rationales or explanations that can be used for further analysis or as training targets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Depending on modality: relative preference (utility), attribute scores (coherence/helpfulness), binary correctness/factuality/safety categories, and explanatory quality and rationale completeness in open-ended responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>explanations/hypotheses expressed as text (applicable when evaluating scientific explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>FLAMe's training mixture is composed of >50% pairwise/pointwise data, contributing to strong zero-shot and fine-tuned performance on multiple benchmarks; pairwise training especially useful for reward-model evaluation (RewardBench).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Empirical hybrid: modalities are human-annotated but are used to train automated LLM autoraters; evaluations on held-out sets compare model outputs to human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical performance on held-out tasks and leaderboard metrics; ablation of task type contributions via tail-patch analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Original datasets vary in labeling rubrics and quality; mapping between modalities (e.g., converting pointwise to pairwise) can be nontrivial and may introduce noise; open-ended outputs are harder to score automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Major FLAMe datasets include pairwise (HelpSteer, PRM800K), pointwise (SummEval), classification (FActScore, FRANK), and generative/open-ended (LIMA subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4617.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RewardBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RewardBench: Evaluating reward models for language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark composed of 23 datasets across categories (Chat, Chat Hard, Reasoning (Math + Coding), Safety) used to evaluate reward models via pairwise preference tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rewardbench: Evaluating reward models for language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>RewardBench pairwise preference benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate a reward model/autorer by asking it to choose the preferred response between two candidates across many datasets and categories; aggregate accuracy against human-labeled preferences to produce category and overall scores. Used as both an evaluation target and as an optimization signal for mixture weights (tail-patch).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise agreement with human preferences across categories (Chat, Chat Hard, Math, Coding, Safety). Reported as percent accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / NLP (preference evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a (benchmark for preferences over generated outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>FLAMe-RM-24B achieved 87.8% overall on RewardBench (top among generative models trained on permissive data as of July 15, 2024); FLAMe-24B baseline scored 86.0%, FLAMe-Opt-RM 87.0%; comparisons reported against GPT-4-0125 (85.9%) and GPT-4o (84.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated evaluation of models against human-labeled pairwise preferences (human labels are the ground truth), i.e., automated model decisions compared to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct accuracy against human-labeled test pairs; leaderboard comparisons and per-category analyses (including length/token bias checks).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Identified length and token biases in RewardBench categories (e.g., Chat favors longer outputs, Math favors shorter); some RewardBench splits exhibit token-level artifacts (e.g., 'sorry' in safety), making benchmarks susceptible to adversarial rankers. No universal dev set strongly correlated with all target distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>RewardBench (23 constituent datasets covering Chat, Chat Hard, Reasoning (Math+Coding), Safety).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4617.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-AggreFact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-AggreFact (Attribution / factuality benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark aggregating multiple attribution/factuality datasets to test whether autoraters can determine if a claim is supported by a source document (grounding/factuality evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Aggregated factuality/attribution evaluation (LLM-AggreFact)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Autoraters are presented with a claim and a supporting document (or model answer and source) and must judge whether the claim/answer is fully supported by the document. Aggregates multiple datasets (e.g., LLMFactVerify, WiCE, TofuEval, ExpertQA) into a single evaluation suite and reports accuracy per use case and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary or categorical factuality/attribution judgments: fully supported vs partially/unsupported; measures grounding/faithfulness of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (factuality/attribution across domains incl. long-form QA and summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>factuality/attribution judgments about claims and summaries (applicable to evaluating scientific claims/explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>FLAMe-24B achieved 81.1 overall on LLM-AggreFact (best among tested autoraters), outperforming GPT-4-0125 (80.6). FLAMe variants beat other LLM-as-a-Judge baselines in three of four use cases (LLM-FactVerify, Wiki-FactVerify, Summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated model judgments compared to human-labeled ground truth on aggregated attribution datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Measured accuracy against held-out human-labeled test examples across constituent datasets; per-use-case breakdowns provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Long-form QA attribution remains challenging; FLAMe-Opt-RM underperformed on Long-form QA vs GPT-4-0125, indicating domain-specific gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>LLM-AggreFact (aggregated attribution datasets: LLMFactVerify, WiCE, AggreFact, TofuEval, ExpertQA, LFQA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4617.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoBBLEr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoBBLEr (Coarse-Bias Benchmark for LLM Autoraters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark designed to quantify six common autorater biases (Order, Compassion, Length, Egocentric, Bandwagon, Attention) to assess robustness and fairness of LLM-as-judge systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking cognitive biases in large language models as evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CoBBLEr bias benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Measure autorater susceptibilities by constructing controlled perturbations: swap response order (Order), replace model names (Compassion), vary response length (Length), indicate model self-generation (Egocentric), add social-proof statements (Bandwagon), or provide irrelevant context (Attention). Compute bias metrics per dimension (lower is better; often absolute deviation or preference rates).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Bias measures per category: position bias (order), name/label sensitivity (compassion), length preference, self-preference (egocentric), susceptibility to social proof (bandwagon), distraction by irrelevant context (attention). Aggregate into an average bias score.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (evaluation robustness and fairness)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a (bias measurement for evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>FLAMe variants exhibited significantly lower bias (average 0.13) compared to GPT-4 (0.31) and many other evaluated models; FLAMe matched or outperformed GPT-4 across the six bias categories.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: autoraters evaluated on specially constructed test pairs with known perturbations; ground truth is the unbiased expected label derived from human-annotated setups.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compare autorater outputs to expected unbiased labels across CoBBLEr perturbations and compute per-dimension metrics; cross-compare with multiple baseline models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Benchmark focuses on a limited set of cognitive biases and synthetic perturbations; lower measured bias does not guarantee absence of other dataset- or demographic-related biases; depends on quality and representativeness of perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>CoBBLEr (Koo et al., 2023) benchmark of autorater bias tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4617.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA-based factuality (QAGS/FEQA/FactScore)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-answering based factuality / attribution metrics (QAGS, FEQA, FactScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of evaluation approaches that convert factuality/faithfulness assessment into question generation and question answering or targeted checks against source documents to detect hallucinations and unsupported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>QA-based factuality/attribution evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate questions from the candidate output (or claim), answer them using the source document (or an external QA system), and compare answers to measure support/consistency. Variants include QAGS (question generation + QA), FEQA, FactScore, and others that operationalize factual consistency as answer agreement or support.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Measures of support/consistency: fraction of generated questions whose answers are supported by the source, binary supported/unsupported labels, or graded factuality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (particularly summarization, claim verification, and long-form answers)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>factual/attribution judgments about claims and explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>FLAMe includes many factuality datasets (XSum Hallucination, QAGS, FRANK, FactScore, HaluEval, etc.) in training; FLAMe outperformed prior autoraters on aggregated attribution benchmarks (LLM-AggreFact). The paper reports improved attribution accuracy (FLAMe-24B 81.1 overall on LLM-AggreFact).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: QA-based methods are automated but validated against human judgments of factuality in held-out datasets; autoraters trained on human labels learn to emulate these judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation/accuracy against human-labeled factuality datasets (e.g., FRANK, QAGS), aggregated evaluation across datasets and benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>QA-based checks depend on quality of question generation and QA components; may miss subtle entailment failures or misattribute unsupported inferences; dataset heterogeneity complicates direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>QAGS, FEQA, FRANK, FactScore, HaluEval, VitaminC, etc. (many included in FLAMe's collection).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4617.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional overlap and embedding metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lexical-overlap and embedding-based automated metrics (BLEU, ROUGE, BERTScore, MAUVE, BARTScore, BLEURT, COMET)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Established automatic metrics measuring surface-level overlap, embedding similarity, distributional divergence, or learned quality to score generated text against references; often used as baselines for judging generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Lexical/embedding/learned automatic metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>BLEU/ROUGE: n-gram overlap with human reference; BERTScore/BARTScore: contextual embedding similarity or model-likelihood-based scoring; MAUVE: distributional divergence between model and human text; BLEURT/COMET: learned metrics fine-tuned on human ratings for specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Surface overlap, semantic similarity in embedding space, distributional closeness to human text, and learned agreement with human quality judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general NLG</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a (automated metrics applicable to textual explanations/theories)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper positions FLAMe as an alternative to task-specific traditional metrics: FLAMe trained on diverse human judgments can generalize across tasks while many of these metrics are task-specific and often insufficient for complex evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: metrics compute scores without human raters, though some (BLEURT/COMET) are trained on human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>These metrics have prior validation in literature (correlations with human judgments on specific tasks), but the paper cites them as related work and uses human-labeled benchmarks to argue for FLAMe's broader applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Lexical overlap metrics fail for paraphrase/abstractive outputs; embedding metrics can overgeneralize; many are task-specific and do not cover safety/factuality dimensions well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4617.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4617.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best-of-N re-ranking with autoraters</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using autoraters to re-rank Best-of-N sampled outputs (re-ranking via FLAMe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Apply a trained autorater to score multiple sampled outputs from a generative model and select the highest-ranked output, improving end-task metrics such as pass@1 for code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Autorater-based Best-of-N re-ranking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate N candidate outputs (e.g., code solutions), have the autorater score or compare them (pairwise or full ranking), then choose the top-ranked candidate as the final output. Evaluate selected output on the downstream objective (e.g., HumanEval pass@1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Downstream task performance of the top-ranked candidate (e.g., functional correctness for code, factual correctness for answers), pass@1 improvement over baseline generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAMe-24B (and variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>24B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>code generation, general NLG (can be applied to selecting best scientific explanation/hypothesis among candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>selection among candidate explanations/predictions (applicable to hypothesis ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Re-ranking 10 code samples with FLAMe-24B improved pass@1: CodeGen-16B from 21.2 to 31.1, davinci-002 from 17.6 to 22.6, InCoder-6B from 14.6 to 22.0, closing significant fractions of the gap to oracle ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated re-ranking based on autorater scores; downstream correctness measured against human-defined test harnesses (e.g., HumanEval unit tests).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical measurement of downstream metric gains (pass@1) when using autorater re-ranking vs no re-ranking and vs oracle ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality of re-ranking depends on autorater's alignment with the downstream objective; autorater biases or misaligned criteria can select superficially plausible but incorrect outputs; computational cost of generating N candidates and scoring them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rewardbench: Evaluating reward models for language modeling <em>(Rating: 2)</em></li>
                <li>Benchmarking cognitive biases in large language models as evaluators <em>(Rating: 2)</em></li>
                <li>Asking and answering questions to evaluate the factual consistency of summaries <em>(Rating: 1)</em></li>
                <li>Towards question-answering as an automatic metric for evaluating the content quality of a summary <em>(Rating: 1)</em></li>
                <li>BLEU: a method for automatic evaluation of machine translation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4617",
    "paper_id": "paper-6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "FLAMe",
            "name_full": "Foundational Large Autorater Models (FLAMe)",
            "brief_description": "A family of multitask instruction-tuned autorater LLMs trained on a curated collection of 102 quality-assessment tasks with 5.3M+ human judgments to serve as automatic evaluators of model outputs across many dimensions (helpfulness, factuality, safety, coding, math, etc.).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Multitask supervised autorater (FLAMe)",
            "evaluation_method_description": "Train an instruction-tuned generative LLM (PaLM-2-24B) in a supervised multitask text-to-text format on a large, standardized collection of human evaluation examples. At inference the model is prompted with task-specific INSTRUCTIONS/CONTEXT and asked to produce the human-style EVALUATION. It can be applied zero-shot to held-out evaluation tasks or fine-tuned for downstream evaluation roles (e.g., reward-model evaluation).",
            "evaluation_criteria": "Task-specific human labels converted into unified outputs: pairwise preference (which response is better), pointwise Likert scores (coherence, helpfulness, etc.), classification (yes/no on attribution or safety), and open-ended explanations. These map to criteria such as helpfulness, factuality/attribution, safety/harmlessness, correctness (math/code), coherence/fluency, verbosity, and instruction-following.",
            "model_name": "FLAMe (initialized from PaLM-2-24B)",
            "model_size": "24B",
            "scientific_domain": "general / NLP (cross-domain evaluation applicability)",
            "theory_type": "explanations/hypotheses as natural-language outputs (general-purpose evaluand)",
            "human_comparison": true,
            "evaluation_results": "FLAMe-24B (baseline) achieved RewardBench 86.0% accuracy; FLAMe-RM-24B (fine-tuned on 4 pairwise datasets) reached 87.8% and outperformed GPT-4-0125 (85.9%) and GPT-4o (84.7%) among generative models trained on permissive data. FLAMe variants outperformed other LLM-as-a-Judge models on 8/12 autorater benchmarks and achieved an average CoBBLEr bias of 0.13 vs GPT-4's 0.31 (lower is better).",
            "automated_vs_human_evaluation": "Automated (model-as-judge) trained to emulate human judgments; evaluation of FLAMe is automated via held-out benchmarks containing human labels and direct comparison to human annotations / leaderboards.",
            "validation_method": "Validated by (1) held-out autorater benchmarks (RewardBench, LLM-AggreFact, SummFeedback, etc.) with human-labeled test sets; (2) comparison to other LLM autoraters and leaderboard rankings; (3) bias benchmark (CoBBLEr) comparing to measured human-anchored biases.",
            "limitations_challenges": "Requires large curated human-judgment datasets; training data heterogeneity and inconsistent original annotation standards; risk of inheriting/propagating human annotator biases; English- and short-context-focused (2048 tokens); lacks a universal development set correlated with all target distributions; supervised training only (RLHF/DPO left for future work).",
            "benchmark_dataset": "FLAMe collection (102 tasks, 5.3M human judgments) plus evaluation on RewardBench, LLM-AggreFact, SummFeedback, CoBBLEr, etc.",
            "uuid": "e4617.0",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Tail-patch (tailpatch) fine-tuning",
            "name_full": "Tail-patch ablation and re-weighted multitask mixture optimization",
            "brief_description": "A one-time, computationally efficient procedure to estimate each training task's effect on a target downstream distribution and produce an optimized mixture of tasks for fine-tuning (used to create FLAMe-Opt-RM).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Tail-patch ablation + re-weighted mixture",
            "evaluation_method_description": "Start from a partially trained multitask checkpoint, then fine-tune briefly (a 'tail-patch') on each individual training task for a limited number of steps (e.g., 3k) to measure that task's marginal impact on a target evaluation (e.g., RewardBench categories). Rate tasks by impact (Helpful +2, Somewhat +1, No effect 0, Harmful -1), bundle tasks by effect, and assign fixed mixture weights to bundles to form an optimized training mixture. Fine-tune on that optimized mixture to quickly improve target performance using far fewer datapoints.",
            "evaluation_criteria": "Improvement in target benchmark performance (e.g., RewardBench category scores) per task tail-patch; tasks are scored by magnitude/direction of effect (significant/stable, slight, none, harmful).",
            "model_name": "PaLM-2-24B → FLAMe-Opt-RM",
            "model_size": "24B",
            "scientific_domain": "general / NLP (applies to optimizing evaluation models across domains)",
            "theory_type": "n/a (method for optimizing evaluator training mixtures)",
            "human_comparison": false,
            "evaluation_results": "FLAMe-Opt-RM reached RewardBench ~87.0% (competitive with FLAMe) after 5k training steps using ~25× fewer datapoints than training FLAMe from scratch; optimized mixture reached higher early performance on RewardBench Chat Hard and Safety.",
            "automated_vs_human_evaluation": "Automated: tail-patch effects are measured by automated evaluation on held-out benchmark metrics (RewardBench) which themselves are derived from human-labeled preferences.",
            "validation_method": "Measured by direct improvement on target benchmark (RewardBench) and checking held-out benchmark performance to avoid overfitting; early experiments checked correlation with other held-out tasks (found weak correlations).",
            "limitations_challenges": "Requires a target benchmark with sufficient signal (no general dev set used); initial bundle weight choices based on intuition and not exhaustively tuned; task-level tail-patch can be compute-intensive if many tasks and large models (mitigated by smaller models).",
            "benchmark_dataset": "Used RewardBench as the target distribution for optimization; tail-patch run on the FLAMe constituent tasks (HelpSteer, PRM800K, CommitPack, HH RLHF Harmlessness, etc.).",
            "uuid": "e4617.1",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Unified text-to-text format",
            "name_full": "Unified INSTRUCTIONS/CONTEXT/EVALUATION text-to-text format for quality assessment tasks",
            "brief_description": "A standardized text-to-text representation for heterogeneous human evaluation tasks that encodes task definitions and annotator instructions to enable multitask supervised training of autoraters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Unified text-to-text task formulation",
            "evaluation_method_description": "Convert diverse human-evaluation datasets (pairwise, pointwise, classification, open-ended) into a single text-to-text format where each example supplies an INSTRUCTIONS block describing the task and desired outputs, CONTEXT giving prompt and candidate responses, and an EVALUATION containing the human label. This enables consistent prompting and direct supervised learning across tasks.",
            "evaluation_criteria": "Native human-annotation labels per dataset (preferences, Likert scores, binary classifications, and free-form rationales), preserved and standardized for model training and inference.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / NLP",
            "theory_type": "n/a (formatting/framework for evaluation tasks)",
            "human_comparison": true,
            "evaluation_results": "Enables effective transfer learning: FLAMe trained on the unified format generalized to held-out tasks and outperformed many LLM-as-judges; facilitated zero-shot application to new tasks.",
            "automated_vs_human_evaluation": "Framework for automated supervised training, but relies on human-labeled data for targets; evaluation of models trained on this format is automated via benchmark comparisons to human labels.",
            "validation_method": "Empirical: models trained on the unified format evaluated on many held-out human-labeled benchmarks (RewardBench, LLM-AggreFact, SummFeedback, etc.) showing improved generalization.",
            "limitations_challenges": "Requires manual crafting of task definitions and sometimes consultation with original dataset authors (3-4 hours per dataset); underlying human labels vary in annotation standards and quality.",
            "benchmark_dataset": "All FLAMe constituent datasets (see Table 5) converted into INSTRUCTIONS/CONTEXT/EVALUATION format.",
            "uuid": "e4617.2",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pairwise / Pointwise / Classification / Open-ended",
            "name_full": "Canonical evaluation modalities: Pairwise, Pointwise, Classification, Open-ended",
            "brief_description": "Four canonical human-evaluation output formats used to judge model-generated content: pairwise preferences, pointwise ratings (Likert), categorical classification, and free-form explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Evaluation modalities (pairwise/pointwise/classification/open-ended)",
            "evaluation_method_description": "Pairwise: present two responses and ask which is better (used extensively for preference/reward-model training). Pointwise: assign a score to a single response (e.g., Likert scales for coherence/fluency). Classification: binary or multi-class labels (e.g., grounded/ungrounded, harmless/harmful). Open-ended: free-form rationales or explanations that can be used for further analysis or as training targets.",
            "evaluation_criteria": "Depending on modality: relative preference (utility), attribute scores (coherence/helpfulness), binary correctness/factuality/safety categories, and explanatory quality and rationale completeness in open-ended responses.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / NLP",
            "theory_type": "explanations/hypotheses expressed as text (applicable when evaluating scientific explanations)",
            "human_comparison": true,
            "evaluation_results": "FLAMe's training mixture is composed of &gt;50% pairwise/pointwise data, contributing to strong zero-shot and fine-tuned performance on multiple benchmarks; pairwise training especially useful for reward-model evaluation (RewardBench).",
            "automated_vs_human_evaluation": "Empirical hybrid: modalities are human-annotated but are used to train automated LLM autoraters; evaluations on held-out sets compare model outputs to human labels.",
            "validation_method": "Empirical performance on held-out tasks and leaderboard metrics; ablation of task type contributions via tail-patch analysis.",
            "limitations_challenges": "Original datasets vary in labeling rubrics and quality; mapping between modalities (e.g., converting pointwise to pairwise) can be nontrivial and may introduce noise; open-ended outputs are harder to score automatically.",
            "benchmark_dataset": "Major FLAMe datasets include pairwise (HelpSteer, PRM800K), pointwise (SummEval), classification (FActScore, FRANK), and generative/open-ended (LIMA subsets).",
            "uuid": "e4617.3",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RewardBench",
            "name_full": "RewardBench: Evaluating reward models for language modeling",
            "brief_description": "A benchmark composed of 23 datasets across categories (Chat, Chat Hard, Reasoning (Math + Coding), Safety) used to evaluate reward models via pairwise preference tasks.",
            "citation_title": "Rewardbench: Evaluating reward models for language modeling",
            "mention_or_use": "use",
            "evaluation_method_name": "RewardBench pairwise preference benchmark",
            "evaluation_method_description": "Evaluate a reward model/autorer by asking it to choose the preferred response between two candidates across many datasets and categories; aggregate accuracy against human-labeled preferences to produce category and overall scores. Used as both an evaluation target and as an optimization signal for mixture weights (tail-patch).",
            "evaluation_criteria": "Pairwise agreement with human preferences across categories (Chat, Chat Hard, Math, Coding, Safety). Reported as percent accuracy.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / NLP (preference evaluation)",
            "theory_type": "n/a (benchmark for preferences over generated outputs)",
            "human_comparison": true,
            "evaluation_results": "FLAMe-RM-24B achieved 87.8% overall on RewardBench (top among generative models trained on permissive data as of July 15, 2024); FLAMe-24B baseline scored 86.0%, FLAMe-Opt-RM 87.0%; comparisons reported against GPT-4-0125 (85.9%) and GPT-4o (84.7%).",
            "automated_vs_human_evaluation": "Automated evaluation of models against human-labeled pairwise preferences (human labels are the ground truth), i.e., automated model decisions compared to human judgments.",
            "validation_method": "Direct accuracy against human-labeled test pairs; leaderboard comparisons and per-category analyses (including length/token bias checks).",
            "limitations_challenges": "Identified length and token biases in RewardBench categories (e.g., Chat favors longer outputs, Math favors shorter); some RewardBench splits exhibit token-level artifacts (e.g., 'sorry' in safety), making benchmarks susceptible to adversarial rankers. No universal dev set strongly correlated with all target distributions.",
            "benchmark_dataset": "RewardBench (23 constituent datasets covering Chat, Chat Hard, Reasoning (Math+Coding), Safety).",
            "uuid": "e4617.4",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLM-AggreFact",
            "name_full": "LLM-AggreFact (Attribution / factuality benchmark)",
            "brief_description": "A benchmark aggregating multiple attribution/factuality datasets to test whether autoraters can determine if a claim is supported by a source document (grounding/factuality evaluation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Aggregated factuality/attribution evaluation (LLM-AggreFact)",
            "evaluation_method_description": "Autoraters are presented with a claim and a supporting document (or model answer and source) and must judge whether the claim/answer is fully supported by the document. Aggregates multiple datasets (e.g., LLMFactVerify, WiCE, TofuEval, ExpertQA) into a single evaluation suite and reports accuracy per use case and overall.",
            "evaluation_criteria": "Binary or categorical factuality/attribution judgments: fully supported vs partially/unsupported; measures grounding/faithfulness of outputs.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general (factuality/attribution across domains incl. long-form QA and summarization)",
            "theory_type": "factuality/attribution judgments about claims and summaries (applicable to evaluating scientific claims/explanations)",
            "human_comparison": true,
            "evaluation_results": "FLAMe-24B achieved 81.1 overall on LLM-AggreFact (best among tested autoraters), outperforming GPT-4-0125 (80.6). FLAMe variants beat other LLM-as-a-Judge baselines in three of four use cases (LLM-FactVerify, Wiki-FactVerify, Summarization).",
            "automated_vs_human_evaluation": "Automated model judgments compared to human-labeled ground truth on aggregated attribution datasets.",
            "validation_method": "Measured accuracy against held-out human-labeled test examples across constituent datasets; per-use-case breakdowns provided.",
            "limitations_challenges": "Long-form QA attribution remains challenging; FLAMe-Opt-RM underperformed on Long-form QA vs GPT-4-0125, indicating domain-specific gaps.",
            "benchmark_dataset": "LLM-AggreFact (aggregated attribution datasets: LLMFactVerify, WiCE, AggreFact, TofuEval, ExpertQA, LFQA, etc.)",
            "uuid": "e4617.5",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CoBBLEr",
            "name_full": "CoBBLEr (Coarse-Bias Benchmark for LLM Autoraters)",
            "brief_description": "A benchmark designed to quantify six common autorater biases (Order, Compassion, Length, Egocentric, Bandwagon, Attention) to assess robustness and fairness of LLM-as-judge systems.",
            "citation_title": "Benchmarking cognitive biases in large language models as evaluators",
            "mention_or_use": "use",
            "evaluation_method_name": "CoBBLEr bias benchmark",
            "evaluation_method_description": "Measure autorater susceptibilities by constructing controlled perturbations: swap response order (Order), replace model names (Compassion), vary response length (Length), indicate model self-generation (Egocentric), add social-proof statements (Bandwagon), or provide irrelevant context (Attention). Compute bias metrics per dimension (lower is better; often absolute deviation or preference rates).",
            "evaluation_criteria": "Bias measures per category: position bias (order), name/label sensitivity (compassion), length preference, self-preference (egocentric), susceptibility to social proof (bandwagon), distraction by irrelevant context (attention). Aggregate into an average bias score.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general (evaluation robustness and fairness)",
            "theory_type": "n/a (bias measurement for evaluators)",
            "human_comparison": true,
            "evaluation_results": "FLAMe variants exhibited significantly lower bias (average 0.13) compared to GPT-4 (0.31) and many other evaluated models; FLAMe matched or outperformed GPT-4 across the six bias categories.",
            "automated_vs_human_evaluation": "Automated: autoraters evaluated on specially constructed test pairs with known perturbations; ground truth is the unbiased expected label derived from human-annotated setups.",
            "validation_method": "Compare autorater outputs to expected unbiased labels across CoBBLEr perturbations and compute per-dimension metrics; cross-compare with multiple baseline models.",
            "limitations_challenges": "Benchmark focuses on a limited set of cognitive biases and synthetic perturbations; lower measured bias does not guarantee absence of other dataset- or demographic-related biases; depends on quality and representativeness of perturbations.",
            "benchmark_dataset": "CoBBLEr (Koo et al., 2023) benchmark of autorater bias tests.",
            "uuid": "e4617.6",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "QA-based factuality (QAGS/FEQA/FactScore)",
            "name_full": "Question-answering based factuality / attribution metrics (QAGS, FEQA, FactScore)",
            "brief_description": "A class of evaluation approaches that convert factuality/faithfulness assessment into question generation and question answering or targeted checks against source documents to detect hallucinations and unsupported claims.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "QA-based factuality/attribution evaluation",
            "evaluation_method_description": "Generate questions from the candidate output (or claim), answer them using the source document (or an external QA system), and compare answers to measure support/consistency. Variants include QAGS (question generation + QA), FEQA, FactScore, and others that operationalize factual consistency as answer agreement or support.",
            "evaluation_criteria": "Measures of support/consistency: fraction of generated questions whose answers are supported by the source, binary supported/unsupported labels, or graded factuality scores.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general (particularly summarization, claim verification, and long-form answers)",
            "theory_type": "factual/attribution judgments about claims and explanations",
            "human_comparison": true,
            "evaluation_results": "FLAMe includes many factuality datasets (XSum Hallucination, QAGS, FRANK, FactScore, HaluEval, etc.) in training; FLAMe outperformed prior autoraters on aggregated attribution benchmarks (LLM-AggreFact). The paper reports improved attribution accuracy (FLAMe-24B 81.1 overall on LLM-AggreFact).",
            "automated_vs_human_evaluation": "Hybrid: QA-based methods are automated but validated against human judgments of factuality in held-out datasets; autoraters trained on human labels learn to emulate these judgments.",
            "validation_method": "Correlation/accuracy against human-labeled factuality datasets (e.g., FRANK, QAGS), aggregated evaluation across datasets and benchmarks.",
            "limitations_challenges": "QA-based checks depend on quality of question generation and QA components; may miss subtle entailment failures or misattribute unsupported inferences; dataset heterogeneity complicates direct comparison.",
            "benchmark_dataset": "QAGS, FEQA, FRANK, FactScore, HaluEval, VitaminC, etc. (many included in FLAMe's collection).",
            "uuid": "e4617.7",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Traditional overlap and embedding metrics",
            "name_full": "Lexical-overlap and embedding-based automated metrics (BLEU, ROUGE, BERTScore, MAUVE, BARTScore, BLEURT, COMET)",
            "brief_description": "Established automatic metrics measuring surface-level overlap, embedding similarity, distributional divergence, or learned quality to score generated text against references; often used as baselines for judging generated outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Lexical/embedding/learned automatic metrics",
            "evaluation_method_description": "BLEU/ROUGE: n-gram overlap with human reference; BERTScore/BARTScore: contextual embedding similarity or model-likelihood-based scoring; MAUVE: distributional divergence between model and human text; BLEURT/COMET: learned metrics fine-tuned on human ratings for specific tasks.",
            "evaluation_criteria": "Surface overlap, semantic similarity in embedding space, distributional closeness to human text, and learned agreement with human quality judgments.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general NLG",
            "theory_type": "n/a (automated metrics applicable to textual explanations/theories)",
            "human_comparison": false,
            "evaluation_results": "Paper positions FLAMe as an alternative to task-specific traditional metrics: FLAMe trained on diverse human judgments can generalize across tasks while many of these metrics are task-specific and often insufficient for complex evaluations.",
            "automated_vs_human_evaluation": "Automated: metrics compute scores without human raters, though some (BLEURT/COMET) are trained on human judgments.",
            "validation_method": "These metrics have prior validation in literature (correlations with human judgments on specific tasks), but the paper cites them as related work and uses human-labeled benchmarks to argue for FLAMe's broader applicability.",
            "limitations_challenges": "Lexical overlap metrics fail for paraphrase/abstractive outputs; embedding metrics can overgeneralize; many are task-specific and do not cover safety/factuality dimensions well.",
            "uuid": "e4617.8",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Best-of-N re-ranking with autoraters",
            "name_full": "Using autoraters to re-rank Best-of-N sampled outputs (re-ranking via FLAMe)",
            "brief_description": "Apply a trained autorater to score multiple sampled outputs from a generative model and select the highest-ranked output, improving end-task metrics such as pass@1 for code generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Autorater-based Best-of-N re-ranking",
            "evaluation_method_description": "Generate N candidate outputs (e.g., code solutions), have the autorater score or compare them (pairwise or full ranking), then choose the top-ranked candidate as the final output. Evaluate selected output on the downstream objective (e.g., HumanEval pass@1).",
            "evaluation_criteria": "Downstream task performance of the top-ranked candidate (e.g., functional correctness for code, factual correctness for answers), pass@1 improvement over baseline generation.",
            "model_name": "FLAMe-24B (and variants)",
            "model_size": "24B",
            "scientific_domain": "code generation, general NLG (can be applied to selecting best scientific explanation/hypothesis among candidates)",
            "theory_type": "selection among candidate explanations/predictions (applicable to hypothesis ranking)",
            "human_comparison": true,
            "evaluation_results": "Re-ranking 10 code samples with FLAMe-24B improved pass@1: CodeGen-16B from 21.2 to 31.1, davinci-002 from 17.6 to 22.6, InCoder-6B from 14.6 to 22.0, closing significant fractions of the gap to oracle ranking.",
            "automated_vs_human_evaluation": "Automated re-ranking based on autorater scores; downstream correctness measured against human-defined test harnesses (e.g., HumanEval unit tests).",
            "validation_method": "Empirical measurement of downstream metric gains (pass@1) when using autorater re-ranking vs no re-ranking and vs oracle ranking.",
            "limitations_challenges": "Quality of re-ranking depends on autorater's alignment with the downstream objective; autorater biases or misaligned criteria can select superficially plausible but incorrect outputs; computational cost of generating N candidates and scoring them.",
            "uuid": "e4617.9",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rewardbench: Evaluating reward models for language modeling",
            "rating": 2
        },
        {
            "paper_title": "Benchmarking cognitive biases in large language models as evaluators",
            "rating": 2
        },
        {
            "paper_title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "rating": 1
        },
        {
            "paper_title": "Towards question-answering as an automatic metric for evaluating the content quality of a summary",
            "rating": 1
        },
        {
            "paper_title": "BLEU: a method for automatic evaluation of machine translation",
            "rating": 1
        }
    ],
    "cost": 0.02204925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Foundational Autoraters: <br> Taming Large Language Models for Better Automatic Evaluation</h1>
<p>Tu Vu ${ }^{\text {® }}{ }^{<em>}$ Kalpesh Krishna ${ }^{\text {® }}{ }^{</em>}$ Salaheddin Alzubi ${ }^{\dagger}$<br>Chris Tar ${ }^{\text {® }}{ }^{\ddagger}$ Manaal Faruqui ${ }^{\circledR}{ }^{\ddagger}$ Yun-Hsuan Sung ${ }^{\text {® }}{ }^{\ddagger}$<br>${ }^{\text {A}}$ Google DeepMind, ${ }^{\circ}$ Google, ${ }^{\circ}$ Virginia Tech<br>{ttvu,kalpeshk}@google.com</p>
<h4>Abstract</h4>
<p>As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for finetuning, as shown in our reward model evaluation case study (FLAMe-RM). On RewardBench, FLAMe-RM-24B achieves $87.8 \%$ accuracy, surpassing GPT-4-0125 ( $85.9 \%$ ) and GPT-4o ( $84.7 \%$ ). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch finetuning approach that offers competitive RewardBench performance using $25 \times$ fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The growing capabilities of large language models (LLMs) present a key challenge: How can we reliably evaluate their long-form responses? A promising approach is to use the models themselves as autoraters. After large-scale multitask instruction tuning, LLMs can generalize to follow new human instructions (Wei et al., 2022; Sanh et al., 2022;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Longpre et al., 2023; Chung et al., 2024), making them suitable for this task. This is appealing because human evaluation, while essential, is limited by subjectivity (Krishna et al., 2023a), inconsistency among raters (Karpinska et al., 2021), and the high costs of extensive evaluations (Min et al., 2023; Vu et al., 2023; Wei et al., 2024).</p>
<p>Training LLM autoraters on human judgments is essential for aligning them with human preferences (Ouyang et al., 2022). However, gathering these judgments is both costly and time-consuming. Reusing human evaluations from prior research is a promising approach, yet it faces challenges such as inconsistent standards, diverse criteria, inadequate documentation, and privacy or proprietary concerns. On the other hand, training autoraters on model outputs offers consistency (Jiang et al., 2024b; Kim et al., 2024b) but risks reinforcing biases and hallucinations (Gudibande et al., 2023; Muennighoff et al., 2023) and may also breach proprietary LLM service terms. ${ }^{2}$</p>
<p>To address these limitations, we curated and standardized human evaluations from prior research to create FLAMe, a collection of 102 quality assessment tasks comprising more than 5.3 M total human judgments (§3). FLAMe spans a wide variety of task types, from assessing summarization quality to evaluating how well AI assistants follow user instructions. We hypothesized that training on this large and diverse data collection would enable LLM autoraters to learn robust, generalized patterns of human judgment, minimizing the impact of noisy or low-quality human judgments.</p>
<p>For transparency and reproducibility, we use only publicly available human evaluation data with permissive licenses from previous studies (§3.2). To address challenges due to the lack of standardization and documentation, we thoroughly exam-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our FLAMe-24B variants outperform popular proprietary LLM-as-a-Judge models like GPT-4 and Claude-3 on various autorater benchmarks, including RewardBench. As of July 15, 2024, FLAMe-RM, with an overall accuracy of 87.8%, was the top-performing generative model trained exclusively on permissively licensed data on RewardBench, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%).</p>
<p>Ined the associated research and consulted the original authors to clarify ambiguities or inconsistencies, spending 3-4 hours per dataset. Inspired by T5 (Raffel et al., 2020), we unify all tasks into a <em>text-to-text</em> format, with manually crafted task definitions and evaluation instructions. This simple and adaptable data format facilitates effective transfer learning, allowing our models to interpret and respond consistently to various tasks (Figure 2).</p>
<p>Our approach can be viewed as developing general-purpose LLM autoraters for various quality assessment tasks. We show that training an instruction-tuned LLM, PaLM-2-24B (Anil et al., 2023), on our FLAMe collection improves zeroshot generalization to a wide range of held-out tasks, outperforming models like GPT-4, Claude-3, and Llama-3 on many tasks. This demonstrates that our large-scale multitask instruction tuning enhances the model's general-purpose quality assessment capabilities.</p>
<p>Motivated by these results, we explore FLAMe's effectiveness as a powerful starting point for finetuning on targeted downstream applications, using reward model evaluation on RewardBench (Lambert et al., 2024) as a case study (FLAMe-RM). Specifically, we slightly fine-tune FLAMe on a mixture of four datasets with human pairwise preference judgments, covering chat, reasoning, and safety. The resulting FLAMe-RM-24B model achieves a notable performance boost on RewardBench, reaching an accuracy of 87.8% (up from 86.0%). As of July 15, 2024, it was <em>the top-performing generative model trained solely on permissively licensed data</em>, outperforming GPT-4-0125 (85.9%) and GPT-4o (84.7%); see Figure 1.</p>
<p>Additionally, we present FLAMe-Opt-RM, a computationally efficient method for optimizing our FLAMe multitask mixture for targeted reward model evaluation on RewardBench. Using a novel <em>tail-patch fine-tuning</em> technique, we evaluate the impact of each dataset on specific RewardBench distributions, enabling us to determine the optimal dataset proportions for our mixture. Finetuning the initial instruction-tuned PaLM-2-24B on this optimized mixture yields competitive RewardBench performance (87.0%) compared to FLAMe (86.0%), using 25× fewer training datapoints.</p>
<p>Overall, our FLAMe variants outperform all popular proprietary LLM-as-a-Judge models we consider on 8 out of 12 autorater evaluation benchmarks (1 held-in and 11 held-out), covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact (Tang et al., 2024). Finally, our analysis shows that FLAMe variants are significantly less biased than other popular LLM-as-a-Judge autoraters on the CoBBLEr bias benchmark (Koo et al., 2023), demonstrating greater robustness to changes in pairwise ordering, response length, and irrelevant context.</p>
<p>In summary, our main contributions are: 1) <strong>Data Collection</strong>: We curated and standardized human evaluations from permissively licensed datasets, creating a collection of over 100 diverse quality assessment tasks with 5M+ human judgments. To facilitate future research, we release our data collection at https://huggingface.co/datasets/google/flame-collection; 2) <strong>LLM Autoraters</strong>: We show that our data collection can be used for training general-purpose LLM autoraters (FLAMe) and optimizing them for specific applications (FLAMe-RM and FLAMe-Opt-RM). Our models outperform popular proprietary LLM-as-a-Judge models on 8 out of 12 autorater benchmarks, covering 53 tasks, including RewardBench and LLM-AggreFact; and 3) <strong>Computationally Efficient Multitask Training</strong>: We propose a tail-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: We unify all quality assessment tasks into a text-to-text format, with manually crafted task definitions and evaluation instructions. Each training example consists of an input-target pair: the input provides task-specific context, while the target contains the expected human evaluation. This format can be easily adapted to novel tasks.
patch fine-tuning method that optimizes our multitask mixture for specific distributions, achieving competitive performance with significantly reduced compute.</p>
<h2>2 Related Work</h2>
<p>Below, we discuss existing literature in the space of autoraters, drawing connections to FLAMe.</p>
<p>Automatic Evaluation Metrics: Traditional metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) focus on lexical overlap between model output and human references. In the BERT era (Devlin et al., 2019), newer methods use pretrained models to measure distributional similarity (Zhao et al., 2019; Zhang et al., 2020) or token probabilities (Thompson and Post, 2020; Yuan et al., 2021). Several approaches assess divergence between text distributions (Gehrmann et al., 2019; Pillutla et al., 2021). Other work fine-tunes models on human ratings for specific tasks like machine translation (Sellam et al., 2020; Rei et al., 2020; Fernandes et al., 2023), summarization (Durmus et al., 2020; Deutsch et al., 2021; Goyal and Durrett,
2021), and QA (Chen et al., 2020; Lin et al., 2022). Unlike task-specific metrics, FLAMe is trained on diverse quality assessment tasks and can adapt to new tasks during inference.</p>
<p>LLM-as-a-Judge Autoraters: Prior work has used LLMs as judges to assess LLM capabilities on various benchmarks (Liu et al., 2023a; Fu et al., 2024; Bai et al., 2023; Wang et al., 2023a; Chiang et al., 2023; Chiang and Lee, 2023; Bubeck et al., 2023). However, these models tend to favor their own generated responses (Liu et al., 2023a; Panickssery et al., 2024; Liu et al., 2023b; Bai et al., 2023), showing biases toward factors like length, order, and entity preference (Koo et al., 2023). In contrast, FLAMe is trained on a broad range of human evaluations, enabling it to learn unbiased, generalized patterns of human judgment (§6.1). Additionally, FLAMe is not tasked with evaluating its own responses, avoiding self-preference bias.</p>
<p>Recent work has also trained general-purpose LLM autoraters. Jiang et al. (2024b) introduce TIGERScore, a Llama-2 model trained on GPT-4-generated error analysis data. Similar methods include InstructScore (Xu et al., 2023b),</p>
<p>Prometheus (Kim et al., 2024a), and Prometheus2 (Kim et al., 2024b). Unlike these, we rely solely on open-source human evaluations instead of model outputs. FLAMe significantly outperforms Prometheus-2 on RewardBench (see Table 2).</p>
<p>Appendix A has related work on reward models.</p>
<h2>3 The FLAMe Collection</h2>
<p>We curated 5.3 M human judgments across 102 training tasks, with an additional 53 tasks reserved for evaluation ( $\S 5.1$ ). Appendix B lists our datasets. Our data covers various task types and LLM capabilities ( $\S 3.2-3.3$ ). We manually crafted task definitions and evaluation instructions, converting all tasks into a unified format (§3.4).</p>
<h3>3.1 Task Definition</h3>
<p>A "task" refers to a specific assignment where a model evaluates aspects of a text (e.g., a machinegenerated summary), alongside its context (the original article), based on given criteria (Figure 2). Each task has its own definition and evaluation guidelines. Multiple tasks can be derived from a single dataset. ${ }^{3}$ Additionally, similar tasks from different datasets are treated as separate. Based on this definition, FLAMe has 102 distinct tasks.</p>
<h3>3.2 Principles for Data Collection</h3>
<p>Our principles for data selection are as follows:
Public, Open-source Data: We use only permissively licensed datasets from HuggingFace (Lhoest et al., 2021), TensorFlow, ${ }^{4}$ or the original authors' GitHub repositories.</p>
<p>Human Annotations: We only use humanlabeled annotations, avoiding those generated by models like GPT-4 due to potential inaccuracies and legal concerns (Gudibande et al., 2023; Muennighoff et al., 2023).</p>
<p>Diverse Task Types: To improve model generalizability, we collect datasets from a diverse set of task types (see breakdown in Figure 3): 1) Pairwise Evaluation: Tasks that involve comparing two responses to determine a preference (e.g., "Which response, $A$ or $B$, is more helpful?"); 2) Pointwise Evaluation: Tasks that involve evaluating specific attributes of individual responses (e.g., "Rate the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: FLAMe data collection breakdown by task type, showing the percentage of datapoints (out of 5.3M) for each task type. Over half of FLAMe is dedicated to standard pairwise ("Which response is better?") and pointwise ("Rate the response on a Likert scale.") evaluation. The remainder includes classification (e.g., "Is the summary fully attributable to the source article? (Yes/No)") and open-ended evaluation (e.g., "Explain why response $A$ is better than response B.").
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: FLAMe data collection breakdown by LLM capability, showing the percentage of datapoints (out of 5.3 M ) for each LLM capability. We focus on standard LLM evaluation pillars: general response quality, factuality, safety, coding, and math. Additionally, we incorporate non-evaluation instruction tuning data (e.g., LIMA) to maintain FLAMe's general-purpose instruction-following capabilities.
overall coherence of the response on a 5-point Likert scale."); 3) Classification: Tasks that involve categorizing responses into predefined categories (e.g., "Does the model output follow the instructions? (Yes/No)"); and 4) Open-ended Evaluation: Tasks that require free-form, unrestricted answers (e.g., "Is the summary fully attributable to the source article? Provide a brief explanation.").</p>
<p>Various LLM Capabilities: We select datasets from the literature that evaluate various LLM capabilities, including factuality, safety, reasoning, instruction-following, long-form generation, creativity, attribution, and coding (§3.3).</p>
<h3>3.3 LLM Capabilities Covered by FLAMe</h3>
<p>FLAMe encompasses key LLM capabilities, as outlined below (see breakdown in Figure 4).</p>
<p>General Response Quality: We assess LLM response quality using datasets that measure attributes like helpfulness, coherence, fluency, cre-</p>
<p>ativity, complexity, and verbosity. These include: Summary Comparisons (SummFeedback) (Stiennon et al., 2020), LMSYS Chatbot Arena conversations (Zheng et al., 2023), HH RLHF Helpfulness (Bai et al., 2022a), WebGPT (Nakano et al., 2021), SummEval (Fabbri et al., 2021), News Summary Evaluation (Goyal et al., 2022), SHP (Ethayarajh et al., 2022), BeaverTails Helpfulness (Ji et al., 2023), SEAHORSE (Clark et al., 2023), HelpSteer (Wang et al., 2023b), etc. For instructionfollowing abilities, we use datasets such as GENIE (Khashabi et al., 2022), InstruSum (Liu et al., 2024), and riSum (Skopek et al., 2023).</p>
<p>Factuality/Attribution: To measure hallucinations in LLM-generated responses, we use several datasets that evaluate factual accuracy and grounding (e.g., checking if claims are supported by source documents). These include: XSum Hallucination (Maynez et al., 2020), QAGS (Wang et al., 2020), WikiBio Hallucination (Manakul et al., 2023), FRANK (Pagnoni et al., 2021), FactScore (Min et al., 2023), VitaminC (Schuster et al., 2021), HaluEval (Li et al., 2023), Q² (Honovich et al., 2021), FaithDial (Dziri et al., 2022a), DialFact (Gupta et al., 2022), BEGIN (Dziri et al., 2022b), and MNLI (Williams et al., 2018), etc. ${ }^{5}$
Mathematical Reasoning: We create data to help FLAMe distinguish between correct and incorrect solutions to mathematical problems. Using PRM800K (Lightman et al., 2024), we extract pairs of human vs. incorrect LLM-generated solutions, along with pairs of (correct, incorrect) LLMgenerated solutions.
Coding: We train FLAMe for code evaluation. Using Code Contests (Li et al., 2022a), CommitPack (Muennighoff et al., 2023), and COFFEE (Moon et al., 2023), we create pairs of (correct, buggy) programs based on coding problems or GitHub issues. FLAMe learns to identify the correct program or fix across programming languages like Python, JavaScript, Java, C++, Go, and Rust.
Safety: Developing safe AI assistants for public use is crucial. To improve safety evaluation, we train FLAMe to identify harmless responses. Our training data includes tasks from HH RLHF Harmlessness (Bai et al., 2022a), HH RLHF Red Teaming (Ganguli et al., 2022), BeaverTails QAClassification and Harmlessness (Ji et al., 2023).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Instruction Tuning: Finally, to preserve our models' instruction-following capabilities, we incorporate instruction tuning data from humanwritten response datasets, including LIMA (Zhou et al., 2023), PRM800K IF (Lightman et al., 2024), ${ }^{6}$ and TULU-2 (Ivison et al., 2023). ${ }^{7}$</p>
<h3>3.4 Unified Task Format</h3>
<p>We standardize our datasets into a unified text-totext format. This preprocessing step takes around 3-4 hours per dataset and includes several key tasks: 1) Comprehensive Review and Author Consultations: We carefully review the associated research and consult with the original authors to clarify ambiguities or inconsistencies; 2) Data Collection: We gather all relevant data files from the corresponding HuggingFace, TensorFlow, or GitHub repositories; 3) Data Extraction: We extract data fields with human quality assessments; 4) Task Definitions and Evaluation Instructions: We write detailed task definitions and evaluation instructions for each task, ensuring consistency and standardization, while adhering to any available instructions provided to the original annotators. These instructions help FLAMe identify input/output formats and specific aspects to assess; and 5) Text-to-Text Format Conversion: We convert all tasks into a unified format (Figure 2). Task definitions, evaluation instructions, and desired output fields are listed under an INSTRUCTIONS block, while input and target field values are placed under CONTEXT and EVALUATION blocks, respectively. This format is easily adaptable to new tasks.</p>
<h2>4 Model</h2>
<p>We fine-tune the instruction-tuned PaLM-2-24B on the FLAMe collection to create general-purpose LLM autoraters that can be prompted to perform various tasks. We train three FLAMe variants: 1) FLAMe-trained with examples-proportional mixture weights (Raffel et al., 2020); 2) FLAMe-RMinitialized with FLAMe and fine-tuned on a balanced mixture of four pairwise evaluation datasets covering chat, reasoning, and safety (§4.2); and 3) FLAMe-Opt-RM—trained with RewardBenchoptimized mixture weights (§4.3).</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h1>4.1 General-purpose Autoraters (FLAMe)</h1>
<p>Our baseline FLAMe model is trained using supervised multitask training on the instruction-tuned PaLM-2-24B for 30K steps. We use examplesproportional mixture weights, capping each task at a maximum of $2^{16}$ examples to avoid oversampling large datasets. FLAMe shows significant generalization improvements across various held-out tasks, outperforming models like GPT-4, Claude-3, and Llama-3 on many tasks (see Figure 1 and Table 1). This supports our hypothesis that large-scale multitask instruction tuning enhances general-purpose quality assessment capabilities.</p>
<h3>4.2 FLAMe for Reward Model Evaluation (FLAMe-RM)</h3>
<p>We delve deeper into FLAMe's potential as a powerful starting point for fine-tuning on specific downstream applications, focusing on reward model evaluation as a case study. We create FLAMe-RM by fine-tuning FLAMe on a balanced mixture of four pairwise evaluation datasets: HelpSteer (Wang et al., 2023b), PRM800K (Lightman et al., 2024), CommitPack (Muennighoff et al., 2023), and HHRLHF Harmlessness (Bai et al., 2022a). Since FLAMe is already trained on these datasets, we fine-tune for only 50 steps. FLAMe-RM significantly boosts FLAMe's RewardBench accuracy from $86.0 \%$ to $87.8 \%$. As of July 15, 2024, FLAMe-RM-24B became the top-performing generative model trained solely on permissively licensed data, surpassing both GPT-4-0125 (85.9\%) and GPT-4o (84.7\%); see Figure 1 and Table 1.</p>
<h3>4.3 Optimizing FLAMe for RewardBench (FLAME-Opt-RM)</h3>
<p>Our baseline approach requires extensive training to attain strong performance on certain downstream tasks like RewardBench (Figure 5). This may stem from suboptimal mixture weights that undersample beneficial tasks. To address this, we introduce a tailpatch ablation strategy that evaluates each dataset's impact on targeted distributions, allowing efficient adjustment of all mixing weight hyperparameters. Fine-tuning the instruction-tuned PaLM-2-24B on this optimized mixture for just 5000 steps achieves competitive RewardBench performance (87.0\%) compared to the baseline FLAMe ( $86.0 \%$ ), using $25 \times$ fewer training datapoints.</p>
<p>We optimized our multitask mixture directly based on RewardBench performance due to the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of FLAMe-Opt-RM and FLAMe during the first 5000 training steps based on RewardBench Chat Hard performance. FLAMe-Opt-RM, with optimized mixture weights, reaches significantly higher Chat Hard scores faster than FLAMe. For reference, FLAMe scores 66.2 at 30K steps. See Figure 6 in Appendix C for RewardBench safety results.
absence of a development set. Our early experiments showed weak correlations between RewardBench and other held-out tasks, making it hard to create a reliable proxy development set. Our goal here is not to achieve state-of-the-art RewardBench results but to demonstrate how to optimize our multitask mixture for specific distributions. ${ }^{8}$ Furthermore, FLAMe-Opt-RM's strong performance across other held-out tasks (Table 1) indicates that it was not overfitted to RewardBench.</p>
<p>Tail-patch Ablations: Assigning the right mixing weight for each task in our multitask mixture is challenging due to the large number of tasks. Instead, we assess each task's impact on targeted distributions and use that to assign weights. First, we select a checkpoint that has been partially trained on our vanilla mixture, showing decent but not optimal RewardBench performance. ${ }^{9}$ Then, we perform a brief fine-tuning stage ("tail-patch") on each individual training task, limited to 3000 training steps. This is a one-time process for each downstream application and can be done with smaller models to reduce computational costs.
A Re-weighted Mixture: After training a tailpatch on each task, we rate its impact on each RewardBench category using four ratings: Helpful (+2, significant and stable improvement), Some-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>what helpful (+1, slight improvement), No clear effect ( 0 , minimal change), Harmful ( -1 , significant drop). We then group tasks into seven bundles: Generally helpful (tasks with a total rating of $\geq 5$ ), Category-specific, one for each of the five RewardBench categories (most beneficial tasks for each category with performance exceeding a threshold $\tau),{ }^{10}$ and Others for the remaining tasks.</p>
<p>We assign fixed mixing weights to each bundle: $w_{\text {general }}=100 \mathrm{~K}$ for Generally helpful, $w_{\text {specific }}=30 \mathrm{~K}$ for each Category-specific bundle, and $w_{\text {others }}=3 \mathrm{~K}$ for Others. If a task belongs to multiple bundles, its final weight is the sum of the mixture weights from each bundle. ${ }^{11}$ An exception to this rule is that we prioritize the top two tasks in three underperforming categories - Chat Hard, Coding, and Safety - each assigned a fixed weight of $w_{\text {top_ specific }}=200 \mathrm{~K}$. These values were initially set based on our intuition and not extensively tuned.</p>
<h3>4.4 Training Details</h3>
<p>We initialize both FLAMe and FLAMe-Opt-RM with PaLM-2-24B (Anil et al., 2023), instructiontuned on the Flan collection (Longpre et al., 2023), and train for 30 K and 5 K steps, respectively. FLAMe is further fine-tuned for 50 steps to create FLAMe-RM. Our models are trained using T5X (Roberts et al., 2023) with the Adam optimizer (Kingma and Ba, 2015), a learning rate of 0.0001 , and a dropout rate of 0.05 . FLAMe is trained on 256 Cloud TPU chips with a batch size of 32, whereas FLAMe-RM and FLAMe-Opt-RM use 128 Cloud TPU chips with a batch size of $8 .{ }^{12}$</p>
<h2>5 Experiments</h2>
<p>We compare FLAMe to several popular LLM-as-aJudge autoraters (§5.2) using a suite of 12 autorater benchmarks ( 1 held-in and 11 held-out), covering a total of 53 quality assessment tasks (§5.1). Overall, FLAMe variants outperform all LLM-as-a-Judge autoraters on 8 out of 12 benchmarks (§5.3).</p>
<h3>5.1 Evaluation Datasets</h3>
<p>We use a variety of held-in and held-out tasks. Each task is cast into our unified task format (§3.4). For</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>benchmarks with multiple categories (e.g., RewardBench, LLM-AggreFact), we use the same prompt instructions across categories. To minimize API costs, we randomly sample 256 examples per evaluation task, ${ }^{13}$ except for RewardBench, where results are reported for the full set.</p>
<h3>5.1.1 Held-in Evaluations</h3>
<p>HelpSteer (Wang et al., 2023b): We assess FLAMe's performance in rating helpfulness, correctness, coherence, complexity, and verbosity, using HelpSteer's validation data.</p>
<h3>5.1.2 Held-out Evaluations</h3>
<p>RewardBench (Lambert et al., 2024): RewardBench is a popular benchmark for evaluating reward models via pairwise preference tasks, where models select the better response between two options based on a given prompt. It incorporates 23 datasets, covering four categories-Chat, Chat Hard, Reasoning (Math + Coding), and Safety. ${ }^{14}$</p>
<p>LLM-AggreFact (Tang et al., 2024): This benchmark integrates ten attribution datasets to assess the grounding capabilities of autoraters. The autorater evaluates whether a claim is fully supported by a given document.</p>
<p>Other Benchmarks: In addition to RewardBench and LLM-AggreFact, we include a diverse set of held-out pointwise and pairwise evaluation benchmarks, including Summary Comparisons (SummFeedback) (Stiennon et al., 2020); ${ }^{15}$ Helpful, Honest, and Harmless Alignment (HHH) (Askell et al., 2021); AlpacaFarm (Dubois et al., 2023); Paraphrase Evaluation (Dipper) (Krishna et al., 2023b); Sequence Continuation Preference (RankGen) (Krishna et al., 2022); Poem Preference (CoPoet) (Chakrabarty et al., 2022); Literary Translation Comparisons (LitTrans) (Karpinska and Iyyer, 2023); Long-form QA Evaluation (LFQAEval) (Xu et al., 2023a); and Text Continuation Preference (ContrSearch) (Su and Xu, 2022).</p>
<h3>5.2 Evaluated Models</h3>
<p>We compare our models to the original instructiontuned PaLM-2-24B, which was not trained on</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Reward <br> Bench</th>
<th style="text-align: center;">LLM <br> AggreFact</th>
<th style="text-align: center;">Summ <br> Feedback</th>
<th style="text-align: center;">Alpaca <br> Farm</th>
<th style="text-align: center;">Rank <br> Gen</th>
<th style="text-align: center;">Co <br> Poet</th>
<th style="text-align: center;">Contr <br> Search</th>
<th style="text-align: center;">HHH</th>
<th style="text-align: center;">Dipper</th>
<th style="text-align: center;">Lit <br> Trans</th>
<th style="text-align: center;">LFQA <br> Eval</th>
<th style="text-align: center;">Help <br> Steer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama-3-70B-Instruct</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">39.7</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8×7B</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">34.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo-0125</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-Opus</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">$\mathbf{9 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 6}$</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">41.3</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-0125</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">$\mathbf{9 4 . 6}$</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">$\mathbf{7 7 . 0}$</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">$\mathbf{7 2 . 7}$</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">40.1</td>
</tr>
<tr>
<td style="text-align: left;">our models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2-24B</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-24B</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">$\mathbf{8 1 . 1}$</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">$\mathbf{5 8 . 2}$</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">$\mathbf{6 9 . 9}$</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">$\mathbf{4 8 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-RM-24B</td>
<td style="text-align: center;">$\mathbf{8 7 . 8}$</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">$\mathbf{5 3 . 1}$</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">$\mathbf{5 7 . 5}$</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">46.6</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-Opt-RM-24B</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">$\mathbf{6 9 . 5}$</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">35.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance of FLAMe compared to popular LLM-as-a-Judge autoraters across various autorater benchmarks. Overall, FLAMe variants outperform all LLM-as-a-Judge autoraters on 8 out of 12 benchmarks, including RewardBench and LLM-AggreFact. See $\S 5.1$ for the sources of our benchmarks.</p>
<p>FLAMe, to isolate the effects of instruction tuning and FLAMe training. We also evaluate several popular LLM-as-a-Judge autoraters, including Llama-3-70B-Instruct (Meta, 2024), Mixtral $8 \times 7$ B (Jiang et al., 2024a), Claude-3-Opus (Anthropic, 2024), GPT-3.5-turbo-0125 (OpenAI, 2024a), GPT-4-0125 (OpenAI, 2024b), and GPT4o (OpenAI, 2024c). ${ }^{16}$ Additionally, we include several models from the official RewardBench leaderboard, notably Gemini-1.5-Pro (Reid et al., 2024), Prometheus-2-8×7B (Kim et al., 2024b), ArmoRM-Llama-3-8B-v0.1 (Wang et al., 2024a), and NVIDIA's Nemotron-4-340B-Reward and Llama-3-70B-SteerLM-RM (Wang et al., 2024b).</p>
<h3>5.3 Main Results</h3>
<p>Table 1 shows our main results across all evaluation benchmarks. RewardBench and LLM-AggreFact results are shown in Table 2 and Table 6, respectively. Below, we first provide an overview of these results before analyzing them in more detail:</p>
<h2>FLAMe Variants Outperform all LLM-as-aJudge Autoraters on 8 out of 12 Benchmarks:</h2>
<p>Table 1 shows FLAMe's strong generalization to various held-out tasks, highlighting its effectiveness as a versatile LLM autorater. FLAMe provides significant gains over the initial instruction-tuned PaLM-2-24B. Remarkably, our models outperform all state-of-the-art LLM-as-a-Judge autoraters on 8 out of 12 benchmarks. FLAMe variants outperform the next-best model by significant margins on several held-out benchmarks, including ContrSearch (69.9 vs. 57.5 for GPT-4o/GPT-3.5-turbo-0125),</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: As of July 15, 2024, FLAMe-RM-24B outperforms other generative models on the RewardBench leaderboard, achieving the best score (87.8\%) among models trained solely on permissively licensed data.</p>
<p>RankGen (69.5 vs. 66.0 for GPT-4o), AlpacaFarm (58.2 vs. 55.5 for GPT-3.5-turbo-0125), SummFeedback (53.1 vs. 50.8 for Llama-3-70B-Instruct), and RewardBench ( 87.8 vs. 85.9 for GPT-4-0125). Additionally, our models achieve the best heldin performance on HelpSteer ( 48.4 vs. 41.3 for Claude-3-Opus).</p>
<p>On the other hand, FLAMe variants lag behind proprietary models on several benchmarks, including HHH ( 91.4 vs. 94.6 for GPT-4-0125/Claude-3-Opus), LitTrans ( 69.5 vs. 72.7 for GPT-4o), and LFQAEva ( 74.2 vs. 77.0 for GPT-4-0125), indicating that these models may have been optimized for these capabilities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Autorater</th>
<th style="text-align: center;">Avg. ( $\downarrow$ )</th>
<th style="text-align: center;">Order ( $\downarrow$ )</th>
<th style="text-align: center;">Compassion ( $\downarrow$ )</th>
<th style="text-align: center;">Length ( $\downarrow$ )</th>
<th style="text-align: center;">Egocentric ( $\downarrow$ )</th>
<th style="text-align: center;">Bandwagon ( $\downarrow$ )</th>
<th style="text-align: center;">Attention ( $\downarrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">baselines reported in Koo et al. (2023)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Falcon-40B</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">$\mathbf{0 . 0 5}$</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: left;">Cohere-54B</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-70B</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">our models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-24B</td>
<td style="text-align: center;">$\mathbf{0 . 1 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 8}$</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-RM-24B</td>
<td style="text-align: center;">$\mathbf{0 . 1 3}$</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$\mathbf{0 . 0 8}$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-Opt-RM-24B</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Autorater bias analysis on the CoBBLEr bias benchmark from Koo et al. (2023). Lower values indicate better or less biased autoraters across all columns. Overall, FLAMe variants exhibit significantly less bias compared to popular LLM-as-a-Judge autoraters like GPT-4. Compared to Table 2 in Koo et al. (2023), we combine first/last numbers for Order/Compassion, report $|\operatorname{bias}-0.5|$ for Length, and only report the order setup in Egocentric.</p>
<p>FLAMe Variants are among the Most Powerful Generative Models on RewardBench: Our results in Table 2 show that FLAMe variants rank among the top generative models on the official RewardBench leaderboard, ${ }^{17}$ demonstrating strong performance in all categories: Chat, Chat Hard, Safety, and Reasoning. Notably, FLAMe-RM-24B achieves an overall score of $87.8 \%$, the highest among generative models trained solely on permissively licensed data, surpassing GPT-4-0125 (85.9) and GPT-4o (84.7). As of July 15, 2024, FLAMeRM-24B ranked second among generative models (below Gemini-1.5-Pro) and sixth overall. We provide an analysis of length and token biases found in RewardBench in Appendix E. Additionally, we discuss our LLMAggreFact results in Appendix D.</p>
<h2>6 Further Analysis of FLAMe</h2>
<p>In this section, we depart from the typical focus on analyzing the effect of factors like model size, data size, and data quality in multitask learning, which have been extensively studied (Raffel et al., 2020; Longpre et al., 2023). Instead, we examine potential biases in our LLM autoraters. We find that our models are significantly less biased than popular LLM-as-a-Judge autoraters. In Appendix F, we further demonstrate FLAMe's potential utility for AI development, particularly in identifying highquality responses for code generation.</p>
<h3>6.1 Autorater Bias Analysis</h3>
<p>A common criticism of LLM-as-a-Judge autoraters is their bias towards certain judgments (Liu et al., 2023a; Panickssery et al., 2024; Liu et al., 2023b; Bai et al., 2023). Here, we evaluate FLAMe</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>variants on the CoBBLEr autorater bias benchmark (Koo et al., 2023).</p>
<p>CoBBLEr measures six types of biases in LLM autoraters: 1) Order: Does the autorater favor a particular response position? 2) Compassion: Does the autorater's judgment change when using the LLM's actual name, like "GPT-4" instead of aliases like "Model A"? 3) Length: Does the autorater prefer longer or shorter outputs? 4) Egocentric: Does the autorater favor outputs it generated itself? 5) Bandwagon: Is the autorater influenced by statements like " $90 \%$ of people prefer response A"? 6) Attention: Does irrelevant context, such as "Response A is about cats." distract the autorater? We reformat the original (prompt,response) pairs from Koo et al. (2023) into our unified FLAMe format (Figure 2) and compare FLAMe variants to other LLM-as-a-Judge autoraters, including GPT-4, reported in Koo et al. (2023).</p>
<p>Table 3 shows that FLAMe variants exhibit significantly lower bias compared to GPT-4 and other autoraters, with an average bias of 0.13 vs. 0.31 for GPT-4 (lower is better). FLAMe matches or outperforms GPT-4 across all six bias categories. These results demonstrate FLAMe's effectiveness as a robust and reliable autorater.</p>
<h2>7 Conclusion</h2>
<p>We curated and standardized human evaluations from permissively licensed datasets, compiling a data collection of over 100 quality assessment tasks with 5M+ human judgments. We demonstrate that this collection can be used for training generalpurpose LLM autoraters and optimizing them for specific downstream applications. Our models outperform popular proprietary LLM autoraters on 8 out of 12 autorater benchmarks, covering 53 tasks.</p>
<h2>Limitations and Future work</h2>
<p>Evaluating LLMs is challenging due to evolving evaluation standards and the need to assess new LLM capabilities. Expanding our data collection with open-source contributions could address this issue. Additionally, our models, trained primarily on English data with a context length of 2048 tokens, might not perform well on multilingual (Freitag et al., 2021) or long-context (Kim et al., 2024c; Karpinska et al., 2024) quality assessment tasks. Finally, in this work, we train our models in a supervised multitask fashion. Exploring alternative training approaches such as RLHF and DPO is a promising direction for future work.</p>
<h2>Ethical Considerations and Risks</h2>
<p>All considerations and risks outlined by prior work for pretrained and instruction-tuned LLMs (Chowdhery et al., 2022; Anil et al., 2023) apply to LLM autoraters. We recommend following standard practice for responsible development of these models (Achiam et al., 2023; Gemini et al., 2023; Reid et al., 2024). Additionally, LLM autoraters raise new risks due to increased quality assessment capabilities. First, our models can inherit and amplify biases from human evaluations, leading to unfair or discriminatory outcomes. For instance, the model may replicate biases related to race, gender, or other sensitive attributes from the training data, potentially harming certain groups. Second, overreliance on LLM autoraters risks automating decisions that need human understanding and empathy. To mitigate these risks, transparency in model development and use, along with robust measures like bias audits, data anonymization, and incorporating diverse perspectives, is essential for promoting fairness, accountability, and trustworthiness.</p>
<h2>Acknowledgments</h2>
<p>We are grateful to Jie Ren, Denny Zhou, and Tania Bedrax-Weiss for their comments on this manuscript. We thank Mohit Iyyer, Daniel Cer, Elizabeth Clark, Jeremiah Liu, Balaji Lakshminarayanan, Clara Huiyi Hu, Aliaksei Severyn, Adam Sadovsky, Yonghui Wu, Quoc Le, Slav Petrov, Séb Arnold, Taylan Bilal, Noah Constant, Colin Raffel, Nan Hua, Marzena Karpinska, Yixiao Song, Tuhin Chakrabarty, the Gemini model quality team, the Descartes team at Google, and the UMass NLP group for useful discussions and valuable feedback at different stages of this project. We
thank the authors of the datasets used in this work, especially Niklas Muennighoff, Hyungjoo Chae, Mounica Maddela, Tanya Goyal, and Yuanhao Wu, for their helpful suggestions and for answering our questions. Finally, we thank Grady Simon, ChungChing Chang, Sho Kannan, Gustavo Hernandez Abrego, and the T5X team for their assistance with the codebase, implementation, and computational resources.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>AI Anthropic. 2024. Introducing the next generation of claude.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 2023. Benchmarking foundation models with language-model-as-an-examiner. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 78142-78167.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems 31 (NeurIPS), volume 31. Curran Associates, Inc.</p>
<p>Daniel Cer, Mona Diab, Eneko Agirre, Iñigo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1-14.</p>
<p>Tuhin Chakrabarty, Vishakh Padmakumar, and He He. 2022. Help me write a poem - instruction tuning as a vehicle for collaborative poetry writing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6848-6863.</p>
<p>Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2020. MOCHA: A dataset for training and evaluating generative reading comprehension metrics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6521-6532.</p>
<p>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2023. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations (ICLR).</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages 15607-15631.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,</p>
<p>Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research (JMLR), 25(70):1-53.</p>
<p>Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, and Ankur Parikh. 2023. SEAHORSE: A multilingual, multifaceted dataset for summarization evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9397-9413.</p>
<p>Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021. Towards question-answering as an automatic metric for evaluating the content quality of a summary. Transactions of the Association for Computational Linguistics (TACL), 9:774-789.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 4171-4186.</p>
<p>Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, and Yejin Choi. 2022a. Is GPT3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages $7250-7274$.</p>
<p>Yao Dou, Chao Jiang, and Wei Xu. 2022b. Improving large-scale paraphrase acquisition and generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9301-9323.</p>
<p>Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 30039-30069.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages $5055-5070$.</p>
<p>Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M. Ponti, and Siva Reddy. 2022a. FaithDial: A faithful benchmark for</p>
<p>information-seeking dialogue. Transactions of the Association for Computational Linguistics (TACL), 10:1473-1490.</p>
<p>Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022b. Evaluating attribution in dialogue systems: The BEGIN benchmark. Transactions of the Association for Computational Linguistics (TACL), 10:1066-1083.</p>
<p>Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022. Understanding dataset difficulty with $V$-usable information. In Proceedings of the 39th International Conference on Machine Learning (ICML), volume 162 of Proceedings of Machine Learning Research (PMLR), pages 5988-6008.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics (TACL), 9:391-409.</p>
<p>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Proceedings of the Eighth Conference on Machine Translation (WMT), pages 1066-1083.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ramakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics (TACL), 9:1460-1474.</p>
<p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. 2023. Incoder: A generative model for code infilling and synthesis. In The Eleventh International Conference on Learning Representations (ICLR).</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2024. GPTScore: Evaluate as you desire. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 6556-6576.</p>
<p>Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858.</p>
<p>Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. 2019. GLTR: Statistical detection and visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL), pages 111-116.</p>
<p>Team Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 1449-1462.</p>
<p>Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717.</p>
<p>Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. DialFact: A benchmark for fact-checking in dialogue. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3785-3801.</p>
<p>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. $q^{2}$ : Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7856-7870.</p>
<p>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. 2023. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702.</p>
<p>Shankar Iyer, Nikhil Dandekar, and Kornél Csernai. 2017. First Quora Dataset release: Question pairs.</p>
<p>Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023. Beavertails: Towards improved safety alignment of llm via a humanpreference dataset. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 24678-24704.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024a. Mixtral of experts. arXiv preprint arXiv:2401.04088.</p>
<p>Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. 2024b. TIGERScore: Towards building explainable metric for all text generation tasks. Transactions on Machine Learning Research (TMLR).</p>
<p>Marzena Karpinska, Nader Akoury, and Mohit Iyyer. 2021. The perils of using Mechanical Turk to evaluate open-ended text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1265-1285.</p>
<p>Marzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors persist. In Proceedings of the Eighth Conference on Machine Translation (WMT), pages 419-451.</p>
<p>Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. One thousand and one pairs: A" novel" challenge for long-context language models. arXiv preprint arXiv:2406.16264.</p>
<p>Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, and Daniel Weld. 2022. GENIE: Toward reproducible and standardized human evaluation for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 11444-11458.</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2024a. Prometheus: Inducing finegrained evaluation capability in language models. In The Twelfth International Conference on Learning Representations (ICLR).</p>
<p>Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535.</p>
<p>Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024c. Fables: Evaluating faithfulness and content selection in book-length summarization. arXiv preprint arXiv:2404.01261.</p>
<p>Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR).</p>
<p>Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023. Benchmarking cognitive biases in large language models as evaluators. arXiv preprint arXiv:2309.17012.</p>
<p>Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R. Bowman, and Ethan Perez. 2023. Pretraining language models with human preferences. In Proceedings of the 40th International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learning Research (PMLR), pages 17506-17533.</p>
<p>Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023a. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</p>
<p>Kalpesh Krishna, Yapei Chang, John Wieting, and Mohit Iyyer. 2022. RankGen: Improving text generation with large ranking models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 199-232.</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 4940-4957.</p>
<p>Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. 2023b. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 27469-27500.</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787.</p>
<p>Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP), pages 175184 .</p>
<p>Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A large-scale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6449-6464.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. 2024. PRD: Peer rank and discussion improve large language model based evaluations. Transactions on Machine Learning Research (TMLR).</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, PoSen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022a. Competition-level code generation with alphacode. Science, 378(6624):1092-1097.</p>
<p>Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Cheung, and Siva Reddy. 2022b. Using interactive feedback to improve the accuracy and explainability of question answering systems post-deployment. In Findings of the Association for Computational Linguistics: ACL 2022 (ACL Findings), pages 926-937.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Let's verify step by step. In The Twelfth International Conference on Learning Representations (ICLR).</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proceedings of the Workshop of Text Summarization Branches Out (WS), pages 74-81.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3214-3252.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2511-2522.</p>
<p>Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. 2023b. Llms as narcissistic evaluators: When ego inflates evaluation scores. arXiv preprint arXiv:2311.09766.</p>
<p>Yixin Liu, Alexander Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. 2024. Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization. In Findings of the Association for Computational Linguistics: NAACL 2024 (NAACL Findings), pages 4481-4501.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. In Proceedings of the 40th International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learning Research (PMLR), pages 22631-22648.</p>
<p>Mounica Maddela, Yao Dou, David Heineman, and Wei Xu. 2023. LENS: A learnable evaluation metric for text simplification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages 16383-16408.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9004-9017.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1906-1919.</p>
<p>AI Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. Meta AI.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 12076-12100.</p>
<p>Seungjun Moon, Yongho Song, Hyungjoo Chae, Dongjin Kang, Taeyoon Kwon, Kai Tzu-iunn Ong, Seung-won Hwang, and Jinyoung Yeo. 2023. Coffee: Boost your code llms by fixing bugs with feedback. arXiv preprint arXiv:2311.07215.</p>
<p>Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations (ICLR).</p>
<p>OpenAI. 2024a. GPT-3.5 Turbo.
OpenAI. 2024b. GPT-4 Turbo and GPT-4.
OpenAI. 2024c. Hello GPT-4o.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John</p>
<p>Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 27730-27744.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 4812-4829.</p>
<p>Arjun Panickssery, Samuel R Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. arXiv preprint arXiv:2404.13076.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311-318.</p>
<p>Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. 2021. Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for MS-COCO. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL), pages 2855-2870.</p>
<p>Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Measuring the gap between neural text and human text using divergence frontiers. In Advances in Neural Information Processing Systems 34 (NeurIPS), volume 34, pages $4816-4828$.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems 37 (NeurIPS), 36.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR 2020), 21(140):1-67.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702.</p>
<p>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste</p>
<p>Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530.</p>
<p>Adam Roberts, Hyung Won Chung, Gaurav Mishra, Anselm Levskaya, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Kehang Han, Michelle Casbon, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. 2023. Scaling up models and data with t5x and seqio. Journal of Machine Learning Research (JMLR), 24(377):1-8.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations (ICLR).</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 624-643.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 7881-7892.</p>
<p>Ondrej Skopek, Rahul Aralikatte, Sian Gooding, and Victor Carbune. 2023. Towards better evaluation of instruction-following: A case-study in summarization. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 221-237.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,</p>
<p>Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems 33 (NeurIPS), volume 33, pages 3008-3021.</p>
<p>Yixuan Su and Jialu Xu. 2022. An empirical study on contrastive search and contrastive decoding for open-ended text generation. arXiv preprint arXiv:2211.10797.</p>
<p>Liyan Tang, Philippe Laban, and Greg Durrett. 2024. Minicheck: Efficient fact-checking of llms on grounding documents. arXiv preprint arXiv:2404.10774.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90-121.</p>
<p>Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. 2023. Freshllms: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 5008-5020.</p>
<p>Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024a. Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. arXiv preprint arXiv:2406.12845.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is ChatGPT a good NLG evaluator? a preliminary study. In Proceedings of the 4th New Frontiers in Summarization Workshop (NewSum), pages 1-11.</p>
<p>Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024b. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673.</p>
<p>Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, et al. 2023b. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics (TACL), 7:625-641.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.</p>
<p>Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations (ICLR).</p>
<p>Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. 2024. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (NAACL), pages 1112-1122.</p>
<p>Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, and Tong Zhang. 2023a. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. arXiv preprint arXiv:2401.00396.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023b. Finegrained human feedback gives better rewards for language model training. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 59008-59033.</p>
<p>Fangyuan Xu, Junyi Jessy Li, and Eunsol Choi. 2022. How do we answer complex questions: Discourse structure of long-form answers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3556-3572.</p>
<p>Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023a. A critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages 3225-3245.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023b. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5967-5994.</p>
<p>Xinyan Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. CREPE: Open-domain question answering with false presuppositions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages $10457-10480$.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems 34 (NeurIPS), volume 34, pages 2726327277.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations (ICLR).</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (ACL), pages 1298-1308.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. 2023. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 46595-46623.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 55006-55021. Curran Associates, Inc.</p>
<h2>Appendix</h2>
<h2>A Related Work on Reward Models</h2>
<p>Our work relates to the development of reward models (RMs) used to align LLMs with human preferences using reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022; Korbak et al., 2023). In RLHF, human preference data is either used to train standalone discriminative RMs, or directly fed into LLMs via algorithms like DPO (Rafailov et al., 2024) or SLiC-HF (Zhao et al., 2023). While we evaluate our models as RMs in our RewardBench experiments (§5), there are key distinctions: (1) RMs primarily rely on pairwise preference data, ${ }^{18}$ while our models use diverse task types in a unified format; (2) RMs optimize for overall preference, whereas our models can be prompted to judge specific aspects of responses (e.g., safety).</p>
<h2>B List of Training Datasets in FLAMe</h2>
<p>Table 5 shows the list of datasets used in our study.</p>
<h2>C Additional Results for FLAME-Opt-RM</h2>
<p>See Figure 6 for RewardBench safety results.</p>
<h2>D Performance of FLAMe on LLM-Aggregact</h2>
<p>Table 6 presents a breakdown of our attribution results on LLM-AggreFact (Tang et al., 2024), categorized into four common use cases: 1) LLMFactVerify: fact verification of LLM-generated responses, 2) Wiki-FactVerify: evaluating correctness of Wikipedia claims, 3) Summarization: assessing faithfulness of summaries, and 4) Longform QA: evaluating long-form answers to questions. FLAMe variants outperform all other models in three out of the four categories (LLM-FactVerify, Wiki-FactVerify, and Summarization). FLAMe24B achieves the highest overall performance of 81.1, while the next-best baseline model GPT-40125 obtains a score of 80.6. In long-form QA attribution evaluation, our best model FLAMe-OptRM underperforms compared to GPT-4-0125 (74.8 vs. 77.3), aligning with our findings in Table 1.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>E Analyzing Length and Token Bias in RewardBench</p>
<p>In this section, we provide an analysis of length (Appendix E.1) and token (Appendix E.2) bias issues identified in the RewardBench benchmark. Given these issues, we encourage future work to evaluate LLM autoraters on a wide variety of benchmarks (such as our evaluation suite in §5), rather than relying solely on RewardBench.</p>
<h3>E.1 Length Bias in RewardBench</h3>
<p>Table 4 highlights length bias in RewardBench. Overall, RewardBench shows significant imbalance across categories regarding length: Chat Hard, Math, and Coding favor shorter outputs, while Chat leans towards longer outputs. An adversarial submission might strategically select longer or shorter outputs based on prompt categories to achieve higher scores, without necessarily reflecting a genuinely strong preference model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">RewardBench Category</th>
<th style="text-align: center;">\% Preference for Longer Outputs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chat</td>
<td style="text-align: center;">$79.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Chat Hard</td>
<td style="text-align: center;">$29.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Math</td>
<td style="text-align: center;">$6.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Coding</td>
<td style="text-align: center;">$35.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Safety</td>
<td style="text-align: center;">$41.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Summary of length bias in RewardBench. Overall, we find that four out of five RewardBench categories show a strong preference towards either longer or shorter outputs.</p>
<h3>E.2 Token Bias in RewardBench</h3>
<p>Besides length bias, we identified token bias in the Math and Safety categories of RewardBench. In Safety, favored responses significantly leaned towards phrases like "I'm sorry", which suggest hedged responses. The word "sorry" appeared nearly $23 \%$ more frequently in preferred responses compared to non-preferred ones. Similarly, the Math split exhibited token bias, where tokens such as "i", "can", "need", "to", "find" were predominantly found in rejected responses.</p>
<h2>F Using FLAMe to Re-rank Decoded Outputs</h2>
<p>In this section, we explore the application of our LLM autoraters in selecting optimal outputs from multiple responses, a method known as "Best-of-N" sampling (Nakano et al., 2021; Krishna et al., 2022). Using FLAMe for re-ranking, we</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of FLAMe-Opt-RM and FLAMe during the first 5000 training steps based on RewardBench Safety performance. FLAMe-Opt-RM, with optimized mixture weights, reaches significantly higher Safety scores faster than FLAMe. For reference, FLAMe scores 88.5 at 30K steps.
assess its impact on code generation performance with the HumanEval Python programming benchmark (Chen et al., 2021). We conduct experiments by re-ranking 10 code samples generated by three models: OpenAI's davinci-002, InCoder6B (Fried et al., 2023), and CodeGen-16B (Nijkamp et al., 2023) using a round-robin competition, and then measuring performance with the top-ranked code sample. ${ }^{19}$ Results in Table 7 show that FLAMe provides significant gains in pass@1 accuracy across all three models. Notably, FLAMe improves CodeGen-16B's pass@1 from 21.2 to 31.1, closing nearly $40 \%$ of the gap to the Oracle ranker (46.9).</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Capability</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Output Format</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">General Response Quality</td>
<td style="text-align: center;">BeaverTails Helpfulness</td>
<td style="text-align: center;">Ji et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HH RLHF Helpfulness</td>
<td style="text-align: center;">Bai et al. (2022a)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hurdles LFQA</td>
<td style="text-align: center;">Krishna et al. (2021)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LMSYS Chatbot Arena conversations</td>
<td style="text-align: center;">Zheng et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAUVE</td>
<td style="text-align: center;">Pillutla et al. (2021)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">News Summary Evaluation</td>
<td style="text-align: center;">Goyal et al. (2022)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRD</td>
<td style="text-align: center;">Li et al. (2024)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SHF</td>
<td style="text-align: center;">Ethayarajb et al. (2022)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HelpSteer</td>
<td style="text-align: center;">Wang et al. (2023b)</td>
<td style="text-align: center;">Pairwise, Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Summary Comparisons</td>
<td style="text-align: center;">Stiennon et al. (2020)</td>
<td style="text-align: center;">Pairwise, Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GENIE</td>
<td style="text-align: center;">Khashabi et al. (2022)</td>
<td style="text-align: center;">Pairwise, Pointwise, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fine-grained RLHF</td>
<td style="text-align: center;">Wu et al. (2023b)</td>
<td style="text-align: center;">Pairwise, Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">InstruSum</td>
<td style="text-align: center;">Liu et al. (2024)</td>
<td style="text-align: center;">Pairwise, Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebGPT</td>
<td style="text-align: center;">Nakano et al. (2021)</td>
<td style="text-align: center;">Pairwise, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LENS</td>
<td style="text-align: center;">Maddela et al. (2023)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SummEval</td>
<td style="text-align: center;">Fabbri et al. (2021)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">riSum</td>
<td style="text-align: center;">Skopek et al. (2023)</td>
<td style="text-align: center;">Pointwise, Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FeedbackQA</td>
<td style="text-align: center;">Li et al. (2022b)</td>
<td style="text-align: center;">Pointwise, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoLA</td>
<td style="text-align: center;">Warstadt et al. (2019)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEAHORSE</td>
<td style="text-align: center;">Clark et al. (2023)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CREPE</td>
<td style="text-align: center;">Yu et al. (2023)</td>
<td style="text-align: center;">Classification, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Scarecrow</td>
<td style="text-align: center;">Dou et al. (2022a)</td>
<td style="text-align: center;">Classification, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Validity LFQA</td>
<td style="text-align: center;">Xu et al. (2022)</td>
<td style="text-align: center;">Classification, Generative</td>
</tr>
<tr>
<td style="text-align: center;">Factuality/Attribution</td>
<td style="text-align: center;">MOCHA</td>
<td style="text-align: center;">Chen et al. (2020)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence Similarity - C $\times$ C</td>
<td style="text-align: center;">Parekh et al. (2021)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence Similarity - STS-B</td>
<td style="text-align: center;">Cer et al. (2017)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiBio Hallucination</td>
<td style="text-align: center;">Manakul et al. (2023)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BEGIN</td>
<td style="text-align: center;">Dziri et al. (2022b)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DialFact</td>
<td style="text-align: center;">Gupta et al. (2022)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FActScore</td>
<td style="text-align: center;">Min et al. (2023)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FRANK</td>
<td style="text-align: center;">Pagnoni et al. (2021)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FaithDial</td>
<td style="text-align: center;">Dziri et al. (2022a)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HaloEval</td>
<td style="text-align: center;">Li et al. (2023)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLJ</td>
<td style="text-align: center;">Williams et al. (2018)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MultiPIT</td>
<td style="text-align: center;">Dou et al. (2022b)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PAWS</td>
<td style="text-align: center;">Zhang et al. (2019)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$Q^{2}$</td>
<td style="text-align: center;">Honovich et al. (2021)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAGS</td>
<td style="text-align: center;">Wang et al. (2020)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QQP</td>
<td style="text-align: center;">Iyer et al. (2017)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VitaminC</td>
<td style="text-align: center;">Schuster et al. (2021)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RAGTruth</td>
<td style="text-align: center;">Wu et al. (2023a)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ESNLJ</td>
<td style="text-align: center;">Camburu et al. (2018)</td>
<td style="text-align: center;">Classification, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">XSum Hallucination</td>
<td style="text-align: center;">Maynez et al. (2020)</td>
<td style="text-align: center;">Generative</td>
</tr>
<tr>
<td style="text-align: center;">Mathematical Reasoning</td>
<td style="text-align: center;">PRM800K</td>
<td style="text-align: center;">Lightman et al. (2024)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;">Coding</td>
<td style="text-align: center;">Code Contests</td>
<td style="text-align: center;">Li et al. (2022a)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COFFEE</td>
<td style="text-align: center;">Moon et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CommitPack</td>
<td style="text-align: center;">Muennighoff et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CommitPack - Bugs</td>
<td style="text-align: center;">Muennighoff et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;">Safety</td>
<td style="text-align: center;">BeaverTails Harmlessness</td>
<td style="text-align: center;">Ji et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HH RLHF Harmlessness</td>
<td style="text-align: center;">Bai et al. (2022a)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HH RLHF Red Teaming</td>
<td style="text-align: center;">Bai et al. (2022a)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BeaverTails QA-Classification</td>
<td style="text-align: center;">Ji et al. (2023)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;">Instruction Tuning</td>
<td style="text-align: center;">LIMA</td>
<td style="text-align: center;">Zhou et al. (2023)</td>
<td style="text-align: center;">Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRM800K IF</td>
<td style="text-align: center;">Lightman et al. (2024)</td>
<td style="text-align: center;">Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TULU-2</td>
<td style="text-align: center;">Ivison et al. (2023)</td>
<td style="text-align: center;">Generative</td>
</tr>
</tbody>
</table>
<p>Table 5: A complete list of training datasets in our FLAMe collection, including their output formats and categorized capabilities. We derive multiple tasks from certain datasets. For example, HelpSteer (Wang et al., 2023b) includes human annotations for different attributes of model responses such as Helpfulness, Correctness, Coherence, Complexity, and Verbosity, allowing us to create distinct tasks, each focused on a specific attribute.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">LLM-FactVerify</th>
<th style="text-align: center;">Wiki-FactVerify</th>
<th style="text-align: center;">Summarization</th>
<th style="text-align: center;">Long-form QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5-turbo-0125</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">65.4</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8×7B</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">76.6</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3-70B-Instruct</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">$\mathbf{7 7 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-Opus</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-0125</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">77.3</td>
</tr>
<tr>
<td style="text-align: left;">our models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2-24B</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-24B</td>
<td style="text-align: center;">$\mathbf{8 1 . 1}$</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-RM-24B</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">$\mathbf{8 2 . 6}$</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">$\mathbf{8 5 . 4}$</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-Opt-RM-24B</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">$\mathbf{8 1 . 2}$</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">74.8</td>
</tr>
</tbody>
</table>
<p>Table 6: LLM-AggreFact performance across four common use cases: LLM-FactVerify (ClaimVerify + FactCheck + Reveal), Wiki-FactVerify (WiCE), Summarization (AggreFact + TofuEval), and Long-form QA (ExpertQA + LFQA). FLAMe variants outperform all tested LLM-as-a-Judge models in three out of the four use cases. FLAMe24B achieves the highest overall performance of 81.1 , while the next-best model GPT-4-0125 scores 80.6 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Ranker</th>
<th style="text-align: center;">CodeGen-16B</th>
<th style="text-align: center;">davinci002</th>
<th style="text-align: center;">InCoder-6B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">10 code samples re-ranked in round-robin fashion</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">None</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">14.6</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-24B</td>
<td style="text-align: center;">$\mathbf{3 1 . 1}$</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-RM-24B</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">$\mathbf{2 3 . 2}$</td>
<td style="text-align: center;">21.3</td>
</tr>
<tr>
<td style="text-align: left;">FLAME-Opt-RM-24B</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">16.5</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">29.3</td>
</tr>
</tbody>
</table>
<p>Table 7: Pass@1 performance on the HumanEval coding benchmark (Chen et al., 2021). Re-ranking code samples with FLAMe variants significantly improves performance across models.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{19}$ We use relatively weak LLMs from Chen et al. (2023) for two main reasons: (1) to assess the potential benefits of re-ranking with FLAMe, and (2) HumanEval has been extensively used to develop newer LLMs.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{13}$ For tasks with fewer than 256 examples, we use the full evaluation set.
${ }^{14}$ We excluded the "Prior sets" of RewardBench because three out of the four datasets were used in training FLAMe.
${ }^{15}$ During training, we used only pairwise ratings from the dataset and reserved pointwise ratings for evaluation.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
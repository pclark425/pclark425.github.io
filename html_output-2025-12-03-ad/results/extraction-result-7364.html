<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7364 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7364</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7364</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-275471055</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.06590v1.pdf" target="_blank">ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. Our code can be found at https://github.com/gersteinlab/chemagent</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7364.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7364.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemAgent+GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemAgent framework using GPT-4 (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChemAgent is an agentic framework that augments large language models with a self-updating library (planning, execution, knowledge memories) to act as a text-based simulator for complex chemical reasoning tasks; GPT-4 is used as the primary backbone demonstrating the largest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (OpenAI GPT-4 family; size not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned base LLM used with retrieval-augmented prompting and code-generation (Python) inside the ChemAgent pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (multiple subfields: quantum chemistry, chemical kinetics (matter), physical chemistry (Atkins), quantum mechanics (chemmc))</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of multi-step chemical problem solving: decompose textbook-level chemistry problems into sub-tasks, retrieve analogous sub-solutions, generate formal formulae, produce and (externally) execute Python code for precise numeric calculations, and synthesize final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Library-augmented retrieval prompting: hierarchical task decomposition; retrieval from three memory types (Planning M_p, Execution M_e, Knowledge M_k); 2-shot planning memory and up to 4-shot execution memory provided during reasoning; few-shot baselines compared; generation of Python code for numeric computation; Evaluation & Refinement module optionally invoked (can use a different LLM as evaluator); synthetic 'imagination' generation to create memory when no close matches; similarity-based retrieval using Llama3 embeddings (cosine similarity) with a threshold (θ). Full prompt suites for decomposition, execution, evaluation, refinement given in Appendix G.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy of final numeric/text answer compared to ground truth, counted as percentage correct; numeric equality uses a relative tolerance of 0.01 (1%).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ChemAgent (GPT-4) average accuracy 57.16% across four SciBench chemistry datasets; per-dataset: CHEMMC 74.36%, MATTER 48.98%, ATKINS 61.18%, QUAN 44.12%. Paper reports overall accuracy gain up to +46 percentage points on CHEMMC vs direct reasoning and average +37% relative vs direct reasoning for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Few-shot + Direct reasoning (GPT-4) avg 19.48%; Few-shot + Python (GPT-4) avg 43.57%; StructChem (GPT-4) avg 47.66%. Ablations: ChemAgent w/o Evaluation & Refinement avg 52.12%; w/o Memory & E&R avg 49.22%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Base model strength (larger/stronger LLMs like GPT-4 yield larger gains)', 'Presence of external memory (M_p, M_e, M_k)', 'Quality of memory (memories generated by GPT-4 > GPT-3.5; hybrid memories can harm)', 'Similarity between invoked memories and the target sub-task (mean similarity correlates with success; threshold ~0.805 significant)', 'Number of memory instances provided (more memory tends to increase mean accuracy but also variance and risk of misleading info)', 'Evaluation & Refinement module (more beneficial with stronger LLMs)', 'Dataset development/test ratio and coverage (size and representativeness of dev set used to build library affects M_p and M_e utility)', 'Inclusion of Python code examples in execution memory improves calculation and unit conversion accuracy', 'Prompt design for decomposition and execution (structured prompts, few-shot formats)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Backbone: gpt-4-1106-preview. Library constructed from development sets; default runtime retrieval uses 2-shot Planning Memory and up to 4-shot Execution Memory; similarity computed with Llama3 embeddings (cosine); relative tolerance 0.01 for numeric matches; Evaluation & Refinement module optionally enabled (cost/token statistics provided: ~0.012M tokens/problem without E&R, ~0.023M with E&R); self-updating memory allowed during test with exclusion to avoid leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Failures arise from (1) hidden/irrelevant/ambiguous question information (lack of correct interpretation), (2) inaccurate planning/reasoning chains from the LLM, (3) incorrect or misleading memory selection (invoking semantically similar but subtly incompatible memories), increased variance with many invoked memories, hybrid mixed-quality memories can confuse the model, computational cost for self-exploration and memory construction, and evaluation that depends on stronger LLMs to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7364.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7364.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemAgent+GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemAgent framework using GPT-3.5 (gpt-3.5-turbo-16k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same ChemAgent architecture applied with GPT-3.5 as the backbone; demonstrates smaller but measurable gains, with stronger dependence on the quality of stored memory and limited benefit from evaluation/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (OpenAI GPT-3.5 family; size not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned base LLM used within ChemAgent; evaluation/refinement sometimes performed by stronger LLMs (e.g., GPT-4) to judge or refine outputs</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (same four SciBench chemistry datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve multi-step chemistry textbook problems via decomposition, retrieval from memory, and Python-code-based calculation when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Identical ChemAgent pipeline: decomposition prompts, execution prompts that produce Python code, retrieval from M_p/M_e/M_k (where applicable). Evaluation & Refinement provides less benefit when base model is GPT-3.5; memory generation & use (M_p, M_e) critical.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) with numeric comparisons using relative tolerance 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ChemAgent with memory (GPT-3.5) reported average accuracy 31.82% (per Table 4). Using GPT-3.5 memory only yields lower quality than GPT-4 memory; hybrid memory mixes perform worse.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Few-shot + Direct reasoning (GPT-3.5) avg 11.61%; Few-shot + Python avg 17.89%; StructChem (GPT-3.5) avg 31.65%. ChemAgent w/o memory avg 24.69%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Quality of constructed memory (memories generated by GPT-4 outperform GPT-3.5-generated memory by ~8% on MATTER dataset)', 'Model self-correction ability (GPT-3.5 is weaker; evaluation & refinement yields limited gains)', 'Size and representativeness of development set affecting memory usefulness', 'Use of Python code examples in execution memory improves calculation']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Backbone: gpt-3.5-turbo-16k. Library built from development sets; experiments include ablations with/without memory and with evaluation/refinement; sometimes evaluation performed by GPT-4 on GPT-3.5 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>GPT-3.5 shows weaker self-correction and limited benefit from evaluation & refinement; lower-quality memory limits gains; some improvements only achieved when evaluation is performed by a stronger model (GPT-4), and even then correction is inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7364.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7364.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemAgent+Llama3-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemAgent framework using Llama3-7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of ChemAgent to a small open‑source LLM (Llama3-7b-Instruct) showing that external memory augmentation can help even compact models, though absolute performance remains lower than stronger backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned open-source LLM used with only Execution Memory (M_e) in experiments due to limited instruction-following capacity</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (same SciBench chemistry datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve decomposed chemical sub-tasks (primarily logical and computational steps) using retrieved execution-memory examples; code-execution module was removed for these experiments and only M_e was used.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>ChemAgent with simplified setup: Evaluation & Refinement removed and only Execution Memory (few-shot style) used; direct reasoning baseline compared.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) with relative tolerance 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ChemAgent (Llama3-7b) average accuracy 25.75% vs direct reasoning baseline 17.41% (absolute improvement ~8.34 percentage points across four datasets). Per Table 1: CHEMMC 56.41%, MATTER 12.24%, ATKINS 19.63%, QUAN 14.71%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Direct reasoning (Llama3-7b) avg 17.41%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Base model capacity / subject-matter knowledge (smaller models have less in-model chemical knowledge)', 'Quality and difficulty distribution of development set (ATKINS dev difficulty caused poor memory and performance drop)', 'Execution Memory acting as few-shot exemplars']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Llama3-7b-Instruct used without Evaluation & Refinement and only Execution Memory (M_e) was applied; experiments repeated multiple times and averaged; simpler prompt setup due to instruction-following limits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Limited chemical knowledge in-small models causes low baseline; noisy or small development set leads to poor static memory pool; method improves logical/problem-solving but absolute performance remains constrained by model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7364.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7364.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemAgent+Llama3-70b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemAgent framework using Llama3-70b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChemAgent applied to a stronger open-source Llama3 variant (70B) yields substantial improvements over direct reasoning, demonstrating that the framework scales with model capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-70b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned open-source LLM used with ChemAgent memory components</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (SciBench datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based multi-step chemistry problem solving via memory-augmented retrieval and decomposition; execution memory used to guide reasoning (python execution removed or limited depending on experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>ChemAgent pipeline with Execution Memory; Evaluation & Refinement module removed in open-source experiments; M_e provided as few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) with relative tolerance 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ChemAgent (Llama3-70b) average accuracy 42.52% vs direct reasoning baseline 29.48% (per Table 5). Per-dataset: CHEMMC 64.10%, MATTER 32.65%, ATKINS 43.93%, QUAN 29.41%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Direct reasoning (Llama3-70b) avg 29.48%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model strength (70B performs much better than 7B)', 'Memory quality and coverage', 'Task subfield distribution and dataset-specific difficulty']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Llama3-70b-Instruct; Evaluation & Refinement module removed; Execution Memory used as few-shot; experiments repeated and averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>As with other models: incorrect memory selection, variance when many memory items invoked, dependence on dev-set coverage for building effective memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7364.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7364.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemAgent+Qwen2.5-72b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemAgent framework using Qwen2.5-72b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChemAgent with Qwen2.5-72b-Instruct demonstrates strong gains and high absolute accuracy among open models tested, showing the framework generalizes across different modern LLM families.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned large open(model) used with ChemAgent pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (SciBench datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Decompose and solve multi-step chemistry problems using memory retrieval and structured prompting; Python code generation/execution used where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>ChemAgent memory-augmentation (execution memory, planning memory), few-shot exemplars from memory, retrieval by similarity; evaluation module removed for open-model experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) with relative tolerance 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ChemAgent (Qwen2.5-72b) average accuracy 53.58% vs direct reasoning baseline 43.47% (per Table 5). Per-dataset: CHEMMC 69.23%, MATTER 44.90%, ATKINS 56.07%, QUAN 44.12%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Direct reasoning (Qwen2.5-72b) avg 43.47%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model strength and inherent chemical knowledge', 'Memory coverage and match similarity', 'Number of memory instances invoked and their relevance']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Qwen2.5-72b-Instruct used with ChemAgent Execution Memory; Evaluation & Refinement disabled in some open-model experiments; experiments averaged across repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Susceptible to incorrect memory selection and misleading memory entries; requires a representative development set to construct an effective library; computational costs for memory building.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7364.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7364.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot+Python baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot + Python (baseline from SciBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompting strategy that supplies few-shot examples where each example includes a problem, step-by-step solution, and Python code; used as a comparator to ChemAgent in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mentioned (SciBench / baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with various LLM backbones (GPT-4, GPT-3.5 per experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies per backbone</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>few-shot prompting with code-generation examples; no external dynamic memory</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (SciBench datasets) and general scientific problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate executable Python code from few-shot examples to compute numeric answers for chemistry problems.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Few-shot exemplars with problem+solution+Python code; output code block expected; no dynamic retrieval-memory.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) with relative tolerance 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Few-shot + Python (GPT-4) avg 43.57%; (GPT-3.5) avg 17.89%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared directly in paper per model family; used as baseline against ChemAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Number and quality of few-shot examples (6 examples used for Few-shot+Python in setup described)', 'Model backbone strength']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Baseline per SciBench setup: six examples for every query (from dev set) used for Few-shot+Python; code is produced within triple backticks in baseline prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Relies on fixed few-shot examples; lacks dynamic memory/self-improvement; less adaptive to new or out-of-distribution subproblems than ChemAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7364.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7364.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StructChem (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured chemistry reasoning with large language models (StructChem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior structured prompting approach that decomposes chemical reasoning into formula generation, step-by-step reasoning, and confidence-based review; used as a state-of-the-art baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured chemistry reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>StructChem method (used with LLM backbones in original work; used as comparative baseline here)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies (paper compares against GPT-4 runs of StructChem)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>structured instruction plus confidence-review pipeline (not an LLM model per se but a prompting/methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (complex reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Guides LLMs to produce formulae and stepwise reasoning with confidence-based review/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Structured instruction: formula generation, step-by-step reasoning, confidence-based review and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct); used as SOTA baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>StructChem (GPT-4) avg 47.66% (per Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Used as SOTA baseline to compare ChemAgent improvements; ChemAgent reports +9.50% absolute average improvement over StructChem (57.16 vs 47.66).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Reliance on human-curated workflows and fixed pipelines', 'Less capability to self-update memory from solved problems']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>StructChem results are from Ouyang et al. (2024) and used here for comparison under same SciBench evaluation paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Still prone to formula/constants/unit errors; lacks self-updating dynamic memory mechanism that ChemAgent introduces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured chemistry reasoning with large language models <em>(Rating: 2)</em></li>
                <li>SciBench: Evaluating college-level scientific problem-solving abilities of large language models <em>(Rating: 2)</em></li>
                <li>ChemCrow: Augmenting large-language models with chemistry tools <em>(Rating: 1)</em></li>
                <li>Testing GPT-4 with wolfram alpha and code interpreter plug-ins on math and science problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7364",
    "paper_id": "paper-275471055",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "ChemAgent+GPT-4",
            "name_full": "ChemAgent framework using GPT-4 (gpt-4-1106-preview)",
            "brief_description": "ChemAgent is an agentic framework that augments large language models with a self-updating library (planning, execution, knowledge memories) to act as a text-based simulator for complex chemical reasoning tasks; GPT-4 is used as the primary backbone demonstrating the largest gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-1106-preview)",
            "model_size": "proprietary (OpenAI GPT-4 family; size not specified in paper)",
            "model_type": "instruction‑tuned base LLM used with retrieval-augmented prompting and code-generation (Python) inside the ChemAgent pipeline",
            "scientific_domain": "Chemistry (multiple subfields: quantum chemistry, chemical kinetics (matter), physical chemistry (Atkins), quantum mechanics (chemmc))",
            "simulation_task_description": "Text-based simulation of multi-step chemical problem solving: decompose textbook-level chemistry problems into sub-tasks, retrieve analogous sub-solutions, generate formal formulae, produce and (externally) execute Python code for precise numeric calculations, and synthesize final answers.",
            "prompting_strategy": "Library-augmented retrieval prompting: hierarchical task decomposition; retrieval from three memory types (Planning M_p, Execution M_e, Knowledge M_k); 2-shot planning memory and up to 4-shot execution memory provided during reasoning; few-shot baselines compared; generation of Python code for numeric computation; Evaluation & Refinement module optionally invoked (can use a different LLM as evaluator); synthetic 'imagination' generation to create memory when no close matches; similarity-based retrieval using Llama3 embeddings (cosine similarity) with a threshold (θ). Full prompt suites for decomposition, execution, evaluation, refinement given in Appendix G.",
            "evaluation_metric": "Accuracy of final numeric/text answer compared to ground truth, counted as percentage correct; numeric equality uses a relative tolerance of 0.01 (1%).",
            "reported_accuracy": "ChemAgent (GPT-4) average accuracy 57.16% across four SciBench chemistry datasets; per-dataset: CHEMMC 74.36%, MATTER 48.98%, ATKINS 61.18%, QUAN 44.12%. Paper reports overall accuracy gain up to +46 percentage points on CHEMMC vs direct reasoning and average +37% relative vs direct reasoning for GPT-4.",
            "baseline_accuracy": "Few-shot + Direct reasoning (GPT-4) avg 19.48%; Few-shot + Python (GPT-4) avg 43.57%; StructChem (GPT-4) avg 47.66%. Ablations: ChemAgent w/o Evaluation & Refinement avg 52.12%; w/o Memory & E&R avg 49.22%.",
            "factors_reported": [
                "Base model strength (larger/stronger LLMs like GPT-4 yield larger gains)",
                "Presence of external memory (M_p, M_e, M_k)",
                "Quality of memory (memories generated by GPT-4 &gt; GPT-3.5; hybrid memories can harm)",
                "Similarity between invoked memories and the target sub-task (mean similarity correlates with success; threshold ~0.805 significant)",
                "Number of memory instances provided (more memory tends to increase mean accuracy but also variance and risk of misleading info)",
                "Evaluation & Refinement module (more beneficial with stronger LLMs)",
                "Dataset development/test ratio and coverage (size and representativeness of dev set used to build library affects M_p and M_e utility)",
                "Inclusion of Python code examples in execution memory improves calculation and unit conversion accuracy",
                "Prompt design for decomposition and execution (structured prompts, few-shot formats)"
            ],
            "experimental_conditions": "Backbone: gpt-4-1106-preview. Library constructed from development sets; default runtime retrieval uses 2-shot Planning Memory and up to 4-shot Execution Memory; similarity computed with Llama3 embeddings (cosine); relative tolerance 0.01 for numeric matches; Evaluation & Refinement module optionally enabled (cost/token statistics provided: ~0.012M tokens/problem without E&R, ~0.023M with E&R); self-updating memory allowed during test with exclusion to avoid leakage.",
            "limitations_or_failure_modes": "Failures arise from (1) hidden/irrelevant/ambiguous question information (lack of correct interpretation), (2) inaccurate planning/reasoning chains from the LLM, (3) incorrect or misleading memory selection (invoking semantically similar but subtly incompatible memories), increased variance with many invoked memories, hybrid mixed-quality memories can confuse the model, computational cost for self-exploration and memory construction, and evaluation that depends on stronger LLMs to be effective.",
            "uuid": "e7364.0",
            "source_info": {
                "paper_title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ChemAgent+GPT-3.5",
            "name_full": "ChemAgent framework using GPT-3.5 (gpt-3.5-turbo-16k)",
            "brief_description": "Same ChemAgent architecture applied with GPT-3.5 as the backbone; demonstrates smaller but measurable gains, with stronger dependence on the quality of stored memory and limited benefit from evaluation/refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k)",
            "model_size": "proprietary (OpenAI GPT-3.5 family; size not specified in paper)",
            "model_type": "instruction‑tuned base LLM used within ChemAgent; evaluation/refinement sometimes performed by stronger LLMs (e.g., GPT-4) to judge or refine outputs",
            "scientific_domain": "Chemistry (same four SciBench chemistry datasets)",
            "simulation_task_description": "Solve multi-step chemistry textbook problems via decomposition, retrieval from memory, and Python-code-based calculation when needed.",
            "prompting_strategy": "Identical ChemAgent pipeline: decomposition prompts, execution prompts that produce Python code, retrieval from M_p/M_e/M_k (where applicable). Evaluation & Refinement provides less benefit when base model is GPT-3.5; memory generation & use (M_p, M_e) critical.",
            "evaluation_metric": "Accuracy (percentage correct) with numeric comparisons using relative tolerance 0.01.",
            "reported_accuracy": "ChemAgent with memory (GPT-3.5) reported average accuracy 31.82% (per Table 4). Using GPT-3.5 memory only yields lower quality than GPT-4 memory; hybrid memory mixes perform worse.",
            "baseline_accuracy": "Few-shot + Direct reasoning (GPT-3.5) avg 11.61%; Few-shot + Python avg 17.89%; StructChem (GPT-3.5) avg 31.65%. ChemAgent w/o memory avg 24.69%.",
            "factors_reported": [
                "Quality of constructed memory (memories generated by GPT-4 outperform GPT-3.5-generated memory by ~8% on MATTER dataset)",
                "Model self-correction ability (GPT-3.5 is weaker; evaluation & refinement yields limited gains)",
                "Size and representativeness of development set affecting memory usefulness",
                "Use of Python code examples in execution memory improves calculation"
            ],
            "experimental_conditions": "Backbone: gpt-3.5-turbo-16k. Library built from development sets; experiments include ablations with/without memory and with evaluation/refinement; sometimes evaluation performed by GPT-4 on GPT-3.5 outputs.",
            "limitations_or_failure_modes": "GPT-3.5 shows weaker self-correction and limited benefit from evaluation & refinement; lower-quality memory limits gains; some improvements only achieved when evaluation is performed by a stronger model (GPT-4), and even then correction is inconsistent.",
            "uuid": "e7364.1",
            "source_info": {
                "paper_title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ChemAgent+Llama3-7b",
            "name_full": "ChemAgent framework using Llama3-7b-Instruct",
            "brief_description": "Application of ChemAgent to a small open‑source LLM (Llama3-7b-Instruct) showing that external memory augmentation can help even compact models, though absolute performance remains lower than stronger backbones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-7b-Instruct",
            "model_size": "7B",
            "model_type": "instruction‑tuned open-source LLM used with only Execution Memory (M_e) in experiments due to limited instruction-following capacity",
            "scientific_domain": "Chemistry (same SciBench chemistry datasets)",
            "simulation_task_description": "Solve decomposed chemical sub-tasks (primarily logical and computational steps) using retrieved execution-memory examples; code-execution module was removed for these experiments and only M_e was used.",
            "prompting_strategy": "ChemAgent with simplified setup: Evaluation & Refinement removed and only Execution Memory (few-shot style) used; direct reasoning baseline compared.",
            "evaluation_metric": "Accuracy (percentage correct) with relative tolerance 0.01.",
            "reported_accuracy": "ChemAgent (Llama3-7b) average accuracy 25.75% vs direct reasoning baseline 17.41% (absolute improvement ~8.34 percentage points across four datasets). Per Table 1: CHEMMC 56.41%, MATTER 12.24%, ATKINS 19.63%, QUAN 14.71%.",
            "baseline_accuracy": "Direct reasoning (Llama3-7b) avg 17.41%.",
            "factors_reported": [
                "Base model capacity / subject-matter knowledge (smaller models have less in-model chemical knowledge)",
                "Quality and difficulty distribution of development set (ATKINS dev difficulty caused poor memory and performance drop)",
                "Execution Memory acting as few-shot exemplars"
            ],
            "experimental_conditions": "Llama3-7b-Instruct used without Evaluation & Refinement and only Execution Memory (M_e) was applied; experiments repeated multiple times and averaged; simpler prompt setup due to instruction-following limits.",
            "limitations_or_failure_modes": "Limited chemical knowledge in-small models causes low baseline; noisy or small development set leads to poor static memory pool; method improves logical/problem-solving but absolute performance remains constrained by model capacity.",
            "uuid": "e7364.2",
            "source_info": {
                "paper_title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ChemAgent+Llama3-70b",
            "name_full": "ChemAgent framework using Llama3-70b-Instruct",
            "brief_description": "ChemAgent applied to a stronger open-source Llama3 variant (70B) yields substantial improvements over direct reasoning, demonstrating that the framework scales with model capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-70b-Instruct",
            "model_size": "70B",
            "model_type": "instruction‑tuned open-source LLM used with ChemAgent memory components",
            "scientific_domain": "Chemistry (SciBench datasets)",
            "simulation_task_description": "Text-based multi-step chemistry problem solving via memory-augmented retrieval and decomposition; execution memory used to guide reasoning (python execution removed or limited depending on experiment).",
            "prompting_strategy": "ChemAgent pipeline with Execution Memory; Evaluation & Refinement module removed in open-source experiments; M_e provided as few-shot exemplars.",
            "evaluation_metric": "Accuracy (percentage correct) with relative tolerance 0.01.",
            "reported_accuracy": "ChemAgent (Llama3-70b) average accuracy 42.52% vs direct reasoning baseline 29.48% (per Table 5). Per-dataset: CHEMMC 64.10%, MATTER 32.65%, ATKINS 43.93%, QUAN 29.41%.",
            "baseline_accuracy": "Direct reasoning (Llama3-70b) avg 29.48%.",
            "factors_reported": [
                "Model strength (70B performs much better than 7B)",
                "Memory quality and coverage",
                "Task subfield distribution and dataset-specific difficulty"
            ],
            "experimental_conditions": "Llama3-70b-Instruct; Evaluation & Refinement module removed; Execution Memory used as few-shot; experiments repeated and averaged.",
            "limitations_or_failure_modes": "As with other models: incorrect memory selection, variance when many memory items invoked, dependence on dev-set coverage for building effective memory.",
            "uuid": "e7364.3",
            "source_info": {
                "paper_title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ChemAgent+Qwen2.5-72b",
            "name_full": "ChemAgent framework using Qwen2.5-72b-Instruct",
            "brief_description": "ChemAgent with Qwen2.5-72b-Instruct demonstrates strong gains and high absolute accuracy among open models tested, showing the framework generalizes across different modern LLM families.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-72b-Instruct",
            "model_size": "≈72B",
            "model_type": "instruction‑tuned large open(model) used with ChemAgent pipeline",
            "scientific_domain": "Chemistry (SciBench datasets)",
            "simulation_task_description": "Decompose and solve multi-step chemistry problems using memory retrieval and structured prompting; Python code generation/execution used where applicable.",
            "prompting_strategy": "ChemAgent memory-augmentation (execution memory, planning memory), few-shot exemplars from memory, retrieval by similarity; evaluation module removed for open-model experiments.",
            "evaluation_metric": "Accuracy (percentage correct) with relative tolerance 0.01.",
            "reported_accuracy": "ChemAgent (Qwen2.5-72b) average accuracy 53.58% vs direct reasoning baseline 43.47% (per Table 5). Per-dataset: CHEMMC 69.23%, MATTER 44.90%, ATKINS 56.07%, QUAN 44.12%.",
            "baseline_accuracy": "Direct reasoning (Qwen2.5-72b) avg 43.47%.",
            "factors_reported": [
                "Model strength and inherent chemical knowledge",
                "Memory coverage and match similarity",
                "Number of memory instances invoked and their relevance"
            ],
            "experimental_conditions": "Qwen2.5-72b-Instruct used with ChemAgent Execution Memory; Evaluation & Refinement disabled in some open-model experiments; experiments averaged across repeats.",
            "limitations_or_failure_modes": "Susceptible to incorrect memory selection and misleading memory entries; requires a representative development set to construct an effective library; computational costs for memory building.",
            "uuid": "e7364.4",
            "source_info": {
                "paper_title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Few-shot+Python baseline",
            "name_full": "Few-shot + Python (baseline from SciBench)",
            "brief_description": "Baseline prompting strategy that supplies few-shot examples where each example includes a problem, step-by-step solution, and Python code; used as a comparator to ChemAgent in experiments.",
            "citation_title": "Mentioned (SciBench / baseline)",
            "mention_or_use": "use",
            "model_name": "Used with various LLM backbones (GPT-4, GPT-3.5 per experiments)",
            "model_size": "varies per backbone",
            "model_type": "few-shot prompting with code-generation examples; no external dynamic memory",
            "scientific_domain": "Chemistry (SciBench datasets) and general scientific problem solving",
            "simulation_task_description": "Generate executable Python code from few-shot examples to compute numeric answers for chemistry problems.",
            "prompting_strategy": "Few-shot exemplars with problem+solution+Python code; output code block expected; no dynamic retrieval-memory.",
            "evaluation_metric": "Accuracy (percentage correct) with relative tolerance 0.01.",
            "reported_accuracy": "Few-shot + Python (GPT-4) avg 43.57%; (GPT-3.5) avg 17.89%.",
            "baseline_accuracy": "Compared directly in paper per model family; used as baseline against ChemAgent.",
            "factors_reported": [
                "Number and quality of few-shot examples (6 examples used for Few-shot+Python in setup described)",
                "Model backbone strength"
            ],
            "experimental_conditions": "Baseline per SciBench setup: six examples for every query (from dev set) used for Few-shot+Python; code is produced within triple backticks in baseline prompts.",
            "limitations_or_failure_modes": "Relies on fixed few-shot examples; lacks dynamic memory/self-improvement; less adaptive to new or out-of-distribution subproblems than ChemAgent.",
            "uuid": "e7364.5",
            "source_info": {
                "paper_title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "StructChem (baseline)",
            "name_full": "Structured chemistry reasoning with large language models (StructChem)",
            "brief_description": "Prior structured prompting approach that decomposes chemical reasoning into formula generation, step-by-step reasoning, and confidence-based review; used as a state-of-the-art baseline in experiments.",
            "citation_title": "Structured chemistry reasoning with large language models",
            "mention_or_use": "mention",
            "model_name": "StructChem method (used with LLM backbones in original work; used as comparative baseline here)",
            "model_size": "varies (paper compares against GPT-4 runs of StructChem)",
            "model_type": "structured instruction plus confidence-review pipeline (not an LLM model per se but a prompting/methodology)",
            "scientific_domain": "Chemistry (complex reasoning tasks)",
            "simulation_task_description": "Guides LLMs to produce formulae and stepwise reasoning with confidence-based review/refinement.",
            "prompting_strategy": "Structured instruction: formula generation, step-by-step reasoning, confidence-based review and refinement.",
            "evaluation_metric": "Accuracy (percentage correct); used as SOTA baseline.",
            "reported_accuracy": "StructChem (GPT-4) avg 47.66% (per Table 1).",
            "baseline_accuracy": "Used as SOTA baseline to compare ChemAgent improvements; ChemAgent reports +9.50% absolute average improvement over StructChem (57.16 vs 47.66).",
            "factors_reported": [
                "Reliance on human-curated workflows and fixed pipelines",
                "Less capability to self-update memory from solved problems"
            ],
            "experimental_conditions": "StructChem results are from Ouyang et al. (2024) and used here for comparison under same SciBench evaluation paradigm.",
            "limitations_or_failure_modes": "Still prone to formula/constants/unit errors; lacks self-updating dynamic memory mechanism that ChemAgent introduces.",
            "uuid": "e7364.6",
            "source_info": {
                "paper_title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured chemistry reasoning with large language models",
            "rating": 2,
            "sanitized_title": "structured_chemistry_reasoning_with_large_language_models"
        },
        {
            "paper_title": "SciBench: Evaluating college-level scientific problem-solving abilities of large language models",
            "rating": 2,
            "sanitized_title": "scibench_evaluating_collegelevel_scientific_problemsolving_abilities_of_large_language_models"
        },
        {
            "paper_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "rating": 1,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Testing GPT-4 with wolfram alpha and code interpreter plug-ins on math and science problems",
            "rating": 1,
            "sanitized_title": "testing_gpt4_with_wolfram_alpha_and_code_interpreter_plugins_on_math_and_science_problems"
        }
    ],
    "cost": 0.01999975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CHEMAGENT: SELF-UPDATING LIBRARY IN LARGE LANGUAGE MODELS IMPROVES CHEMICAL REASON-ING
11 Jan 2025</p>
<p>Xiangru Tang xiangru.tang@yale.edu 
Yale University</p>
<p>Tianyu Hu 
Yale University</p>
<p>Muyang Ye 
Yale University</p>
<p>Yanjun Shao 
Yale University</p>
<p>Xunjian Yin 
Yale University</p>
<p>Siru Ouyang 
UIUC</p>
<p>Wangchunshu Zhou 
Pan Lu 
Stanford University</p>
<p>Zhuosheng Zhang 
Shanghai Jiao Tong University</p>
<p>Yilun Zhao 
Yale University</p>
<p>Arman Cohan 
Yale University</p>
<p>Mark Gerstein 
Yale University</p>
<p>CHEMAGENT: SELF-UPDATING LIBRARY IN LARGE LANGUAGE MODELS IMPROVES CHEMICAL REASON-ING
11 Jan 2025B490F6B5F2D289CF0CD7A218AB478F46arXiv:2501.06590v1[cs.CL]
Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures.Furthermore, large language models (LLMs) encounter difficulties handling domainspecific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks.To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library.This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries.Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions.Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience.Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods.Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science.Our code can be found at https://github.com/gersteinlab/chemagent.* Equal contribution.The work was done when Mr. Hu and Mrs. Ye were interns at Yale University.</p>
<p>INTRODUCTION</p>
<p>Chemical reasoning presents unique challenges in the realm of artificial intelligence, demanding sophisticated reasoning and precise calculations beyond typical reasoning tasks (McQuarrie, 2008;Atkins et al., 2014;Talanquer, 2022;Guo et al., 2023b;Cao et al., 2024).For example, the GPT solution with CoT prompting in Figure 1 contains numerous errors in both the calculation process and the chemical constants used.Even in short reasoning chains, a single error can cascade, reducing answer quality and escalating the probability of additional errors (Liao et al., 2024;Sun et al., 2024b).</p>
<p>Recent advancements in large language models (LLMs) have demonstrated capabilities in simpler scientific tasks or chemical scenarios that do not require complex reasoning (Boiko et al., 2023;Atkins et al., 2023;Wang et al., 2024a;Hu &amp; Shu, 2023;Xiao et al., 2024;Darvish et al., 2024;Skreta et al., 2024).However, their application to complex chemical reasoning reveals significant limitations Pei et al. (2024); Li et al. (2024a).LLMs often (1) struggle to effectively utilize domainspecific formulas, (2) exhibit incorrect reasoning steps, and (3) produce errors when combining textual reasoning with Python code for calculations (Zhong et al., 2024b); here, syntax errors may arise, causing the code to fail to compile.As shown in Figure 1 (StructChem output), errors often arise from determining a constant's incorrect form or unit.shows a standard Chain-of-Thought approach with calculation errors (in steps 3 and 4) in Wang et al. (2024a).(b) demonstrates the StructChem (Ouyang et al., 2024) method with formula generation and step-by-step reasoning but fails due to an incorrect constant and incorrect unit conversion (in steps 1 and 4).(c) presents the ChemAgent solution, featuring task decomposition, memory retrieval from the library, and reasoning, leading to the accurate final answer.Some Previous approaches to address these challenges in chemical reasoning tasks have focused on decomposing the reasoning process (Zhong et al., 2023), e.g., formatting the reasoning steps through techniques such as self-reflection (Shinn et al., 2023) and StructChem prompting (Ouyang et al., 2024).However, these methods often rely heavily on human-curated knowledge (Chan et al., 2023), or fixed workflows (Fu et al., 2022;Wang et al., 2023;Zhou et al., 2023).Unlike human learners who utilize a library system to retain and apply previous experiences, these methods lack the ability to remember and learn from past analogous problems.For instance, humans can abstract and store theorems or solution strategies from previous tasks and utilize this memory for future problem-solving. 1uilding on these insights, ChemAgent introduces a dynamic "LIBRARY" system that facilitates iterative problem-solving by continuously updating and refining its content based on task decomposition.The LIBRARY in ChemAgent serves as a comprehensive repository where decomposed chemical tasks are stored.These tasks are broken down into sub-tasks, and their solutions are compiled in the library for future reference.As new tasks are encountered, the library is updated with new sub-tasks and corresponding solutions, ensuring its content remains relevant and progressively improves over time.In parallel, inspired by human cognitive mechanisms (Osman, 2004;Kaufman, 2011), our system incorporates three main memory components: Planning Memory for high-level strategies, Execution Memory for specific task solutions, and Knowledge Memory for fundamental chemistry principles.These memories are stored externally, allowing the model to retrieve them efficiently during the problem-solving process.Unlike previous works relevant to LLM's external memory system (Huang et al., 2024a;Zhong et al., 2024a;Zhang et al., 2023;Li et al., 2024b;Guo et al., 2023a;Zhang et al., 2024b), we carefully integrate all these components in a complete agentic framework and allow them to update dynamically.These memories are stored in a structured tree-based format that allows for efficient retrieval during the problem-solving process.When ChemAgent encounters a new problem, it first decomposes the task into manageable sub-tasks, then leverages the library to retrieve relevant sub-tasks and solutions stored in its memory components.The retrieved information is validated and refined using analogous sub-tasks previously encountered, optimizing both task decomposition and solution generation.The library is dynamically updated by adding new sub-tasks and solutions as they are encountered Find relevant memories (Updated) Library (Updated) Library</p>
<p>Find relevant memories</p>
<p>Sub-solution 2: [Formula 1] The kinetic energy can be calculated:</p>
<p>, T sk: Given th t the work function for sodium met l is , wh t is the threshold frequency for sodium?Solution: First, we need to convert the work function from electron volts (eV) to joules (J).This conversion c n be done using the rel tion:</p>
<p>. …… and validated.This iterative process ensures that the memory is continuously enriched with new strategies and solutions, mimicking human problem-solving improvements through practice.By maintaining an evolving domain knowledge base, ChemAgent enables LLMs to autonomously navigate the problem-solving process, thereby enhancing their performance on similar tasks over time.</p>
<p>Our experiments are conducted on four chemical reasoning datasets from SciBench (Wang et al., 2024a) with GPT-3.5, GPT-4 (OpenAI et al., 2024), and open-source models like Llama3 (Llama Team, 2024).Experimental results indicate that ChemAgent significantly enhances the accuracy, though the degree of improvement achieved by our model varies accordingly due to the varying sizes of these datasets.Compared to the base model (take GPT-4, for example), the application of memory self-improving leads to an average increase in accuracy by 37%, with a maximum improvement of 46%.In comparison with the current state-of-the-art model, StructChem (Ouyang et al., 2024), our approach achieves an average improvement of 10% and a maximum improvement of 15% across different datasets.Furthermore, the improvement is more pronounced for stronger base models: GPT-4 exhibits greater enhancements compared to GPT-3.5 when augmented with our framework.Additionally, we analyze the role of our library system and examine the benefits of different memory updates across various stages.This analysis provides insights into how our iterative library updates contributed to performance improvements in sub-task resolution and optimal task decomposition.By continuously enriching the library with structured memory components, ChemAgent ensures that the problem-solving process is progressively refined and optimized, leading to substantial improvements in accuracy.</p>
<p>METHOD</p>
<p>2.1 PRELIMINARIES Analogous to how students organize and reference their problem-solving approaches for exams, our motivation for developing a library system in ChemAgent is to enhance LLMs' ability to tackle complex problems by providing structured access to a repository of previous sub-tasks and solutions.</p>
<p>The overall reasoning framework is shown in Figure 2 (a).Formally, in a simplified form, given a complex and open-ended chemical problem P S as input, our method aims to generate a solution O S .The problem P S , which comprises problem descriptions T S and initial conditions C S , is firstly decomposed into a series of sub-tasks P Si .The solution O S is then synthesized from the subsolutions O Si of the sub-tasks.Each O Si includes intermediate steps, such as formulae, reasoning steps, code, and calculations.Then, a final answer A S is derived from the overall solution O S .We evaluate the performance via the accuracy of A S against a ground truth A Sg .</p>
<p>COMPOSITION OF THE LIBRARY</p>
<p>We divide library into three main memory components: planning memory, execution memory, and knowledge memory.Figure 3  Thus, ChemAgent enables LLMs to tackle problems proactively, form new memories from these attempts, utilize existing memories to solve complex problems, and continuously refine their solutions.Built upon the library of three types of memories, ChemAgent operates in two main stages:</p>
<ol>
<li>Initial Library Construction: Complex problems in the development set are decomposed into structured atomic sub-tasks, which serve as building blocks of the library, consisting of static long-term memory (M p and M e ).</li>
</ol>
<p>Inference Phase and Dynamic Memory Updating:</p>
<p>(a) Memory-Enhanced Reasoning: The memory is dynamically enriched and improved during runtime.(b) Evaluation &amp; refinement: Each sub-task's solution is evaluated on their veracity and relevance to the main task, which leads to a correction of the solution or a refinement of the overall plan.</p>
<p>Figure 2 and Figure 4 illustrates this workflow.In the following sections, we explain how each part is constructed and utilized.All prompts used in our framework can be found in Appendix G.</p>
<p>DECOMPOSITION AS ATOMIC BLOCKS</p>
<p>Human problem-solvers naturally break down complex chemistry problems into smaller, manageable sub-tasks (Zhou et al., 2023).This decomposition enhances the understanding of individual components and their interactions, facilitating the resolution of the original problem in a structured manner (Johnson et al., 2017).These sub-tasks not only improve the reasoning process but also function as atomic building blocks within the execution memory, each paired with a corresponding sub-solution.Key characteristics of this decomposition include:</p>
<p>Hierarchical Decomposition: Using the strategy stored in Planning Memory, we break down complex chemistry problems into hierarchical sub-tasks.If a sub-task is too difficult to complete, it is further decomposed until it can be executed in a single step by the LLM.We store each decomposed sub-task and its intermediate sub-solutions, enabling direct retrieval of solutions for similar future problems.This hierarchical approach ensures each sub-task acts as an independent atomic block, aiding future problem-solving.</p>
<p>Structured Sub-tasks: To maximize the efficiency of decomposition, sub-tasks are structured into four distinct parts: (i) Task Query defining the specific question, (ii) Sub-task Objectives outlining clear goals, (iii) Step-by-step Solution detailing the method to address the sub-task, and (iv) Guidance for Solutions suggesting potential approaches.</p>
<p>Atomic Blocks as Memory: Decomposed sub-tasks serve as building blocks for memory.Identifying and recalling similar problems can be challenging, but sub-tasks often share commonalities that make it easier to leverage past experiences.These atomic blocks also function as examples in few-shot prompting, standardizing response formats and bolstering the overall problem-solving process.</p>
<p>Building on the concept of sub-tasks as fundamental units, we outline the processes involved in our framework, which includes both a dynamic library system and the structured memory inside our library.This section details how we initially construct the library and memory ( §2.4), utilize and update them for new tasks ( §2.5), and evaluate and refine the generated solutions ( §2.6).Specifically, our approach leverages the development set to build a library, which is designed to dynamically selfimprove during runtime.</p>
<p>LIBRARY CONSTRUCTION
C ← F(pref∥P∥C) Sub-tasks T ← F (ptask∥P∥C) Sub-solutions O ← F (psol∥C∥P∥S) Assert len(C) = len(T ) = len(O) for i in len(C) do Add (Ci, Ti, Oi) into U U ← F(prank∥U) return U
The Library is constructed using the development set.As outlined in §2.3, we leverage sub-tasks decomposed from each problem as the execution memory units.Each execution memory unit U i is defined as follows, where C represents the conditions of a given problem P, and T i and O i denote the sub-task and its corresponding sub-solution, respectively:
U i = (C, T i , O i ) for i = 1, 2, . . . , k
Given a problem P and its corresponding solution S in the development set, our method begins by identifying and extracting the conditions from P. We then verify these conditions for accuracy to ensure that the subsequent steps operate with precise and correctly parsed data.Based on the identified conditions, we instruct the LLMs to generate detailed sub-tasks.For each identified sub-task T i , the corresponding sub-solutions O i are parsed from S and assigned accordingly.Inspired by curriculum learning (Bengio et al., 2009), we then rank the memory units U based on their difficulty.In addition to ranking, we discard any memory units that do not meet a predefined confidence threshold, as evaluated by the LLMs.This ensures that the memory utilized for future problem-solving is both relevant and reliable, enhancing the LLM's ability to tackle increasingly complex chemistry problems.The detailed memory construction process is further described in Algorithm 1.</p>
<p>LIBRARY-ENHANCED REASONING</p>
<p>During testing, we first decompose a given problem into several sub-tasks.For each sub-task, we retrieve related memory units U r to aid in solving it.Specifically, we compute the similarity between the given sub-task and units stored in the memory.Memory units with similarity above a predefined threshold θ are used to assist the model in determining the answer for the sub-task.</p>
<p>Formally, let (C j , T j ) represent a sub-task decomposed from a new problem.We retrieve memory units U r that satisfy Similarity(T j , T Ui ) ≥ θ for U i ∈ M e .</p>
<p>Specifically, the similarity between tasks is calculated using Llama3's embeddings:
Similarity(T a , T b ) = Embed(T a ) • Embed(T b ) ||Embed(T a )|| × ||Embed(T b )|| .
In scenarios where the execution memory does not contain similar sub-tasks, we incorporate a selfimproving mechanism.Initially, we enrich the memory with the required information by leveraging the internal parametric knowledge of LLMs.The LLM is instructed to identify the topic of the given sub-task (e.g., quantum chemistry or thermodynamics) and generate self-created chemistry problems related to that topic, adhering to specific guidelines.This forms a kind of "synthetic" execution memory.We encourage the LLMs to generate diverse problems and solutions beyond the provided examples, thereby enhancing the robustness and adaptability of the memory system.</p>
<p>Moreover, the memory is continuously updated with newly solved sub-tasks and their solutions during runtime.For each newly solved sub-task T j , sub-answers from previous sub-problems are incorporated into the conditions as C j , and the execution memory is updated based on solution O j :
M e = M e ∪ {(C j , T j , O j )}.
The plan memory will be updated by a summary of the overall strategy knowledge used, K j (e.g.formulas, conception, order of resolution):
M p = M p ∪ {(T j , K j )}.
This dynamic updating ensures that the memory evolves and improves over time, progressively enhancing the problem-solving capabilities.More details on self-evolution are in §2.9.Furthermore, if a sub-task fails due to insufficient conditions or if the evaluation determines that the sub-task's question does not align with the main task (e.g., calculating energy using wavelength instead of calculating wavelength using given energy), the sub-tasks from P i to P n will be restructured as P ′ i to P ′ m , taking into account the main task and all preceding sub-tasks.The evaluation &amp; refinement module can use a different LLM than the one used for the base reasoning process.For instance, if GPT-3.5 is the base model, GPT-4 can handle evaluation &amp; refinement.The evaluation component judges solutions without modifying them, while the refinement component adjusts solutions based on these evaluations.This separation clarifies error identification, helping humans understand where and why mistakes occur.</p>
<p>EVALUATE &amp; REFINE MODULE</p>
<p>SETUP</p>
<p>We use four chemistry datasets from SciBench (Wang et al., 2024a), and the detailed distribution of the specific fields covered by each problem in the four datasets is shown in Figure 10.Each dataset is divided into a development set (D d ) and a test set (D t ), with exact sizes provided in Table 6.According to previous research (Ouyang et al., 2024), SciBench chemical sets are representative of chemical reasoning tasks, addressing a broad range of queries typically encountered in this field.</p>
<p>During the reasoning stage, we configure the planning memory (M p ) to provide a maximum of two related memory instances (2-shot) for each query, and the execution memory (M e ) to provide up to four related instances (4-shot).However, during the construction of the library, only the knowledge memory (M k ) is used, as the standard solutions are already available in the development set (D d ).</p>
<p>We evaluate the accuracy by comparing their outputs with the correct answers, using a relative tolerance of 0.01.</p>
<p>We consider three baselines in alignment with the evaluation paradigm in SciBench (detailed instructions are provided in Appendix G): (1) Few-shot + Direct reasoning involves directly feeding the problem into the LLM without any additional instructions, the source of the data is StructChem (Ouyang et al., 2024).</p>
<p>(2) Few-shot + Python combines few-shot examples with a tool-augmented strategy.Here, we provide six examples for every query, each consisting of a problem, its corresponding solution, and Python code.These examples are taken from D d .The results are from the original benchmark, SciBench (Wang et al., 2024a).</p>
<p>(3) StructChem (Ouyang et al., 2024) employs structured instruction, confidence-based review, and refinement to guide LLMs through precise stepby-step reasoning.</p>
<p>RESULTS</p>
<p>We report the performance of all methods regarding accuracy score for each sub-dataset and an average score across the entire dataset.The results are summarized in Tables 1, 4 and 5.Additional results and analysis of experiments conducted on other LLMs can be found in Appendix B.</p>
<p>Firstly, ChemAgent consistently outperforms the other baselines across various datasets and settings.Specifically, in terms of the average score, ChemAgent improves by 9.50% (47.66 vs. 57.16)over StructChem, which is a 2.93 times increase and by 37% (19.48 vs. 57.16)over direct reasoning, which is a 2.93 times increase.Notably, the performance gain varies across different datasets.In the CHEMMC dataset, our method exhibits the largest improvement, with a 46% increase (28.21 vs. 74.36)compared to the direct reasoning setup.Secondly, the results also highlight the crucial role of memory in our library.The version of our framework equipped with memory consistently outperforms the version without memory across all cases.Specifically, there is an absolute improvement of 2.90% in terms of the average score.This underscores the importance of memory in retaining and leveraging past information to improve the accuracy and reliability of solving chemistry problems.Additionally, the Evaluation &amp; Refinement module plays a significant role when using stronger LLMs.For instance, incorporating Evaluation &amp; Refinement with GPT-4 increases ChemAgent's performance by 5.04% compared to ChemAgent with memory alone.In summary, ChemAgent demonstrates superior performance across various LLM backbones, highlighting the critical importance of the library system, memory design, and evaluation modules in enhancing problem-solving capabilities.Appendix C details the specific aspects and reasons for our method's improved performance.</p>
<p>2.9 SELF-EVOLUTION DURING RUNTIME Figure 5: Self-evolving analysis.We test ChemAgent twice for each iteration, and the difference between the two results serves as the error margin.All the experiments here are done on MATTER dataset.</p>
<p>Moreover, we aim to show that library systems with evolving memory perform better when exposed to an increasing number of problems.Much like humans improve their skills through practice, these systems benefit from continuous exposure to new tasks.</p>
<p>We allow ChemAgent to dynamically update and enrich its library during the test stage to analyze this self-evolution process.Specifically, in iteration I i , ChemAgent uses all accumulated long-term memory from iterations I 1 to I i−1 as its library.When solving a new problem P, all related responses and knowledge from that process are added to the library if the solution is correct.This means that in subsequent iterations, the system can leverage the newly acquired information (updated M p and M e ) to improve its performance.We employ 2-shot M p and 4-shot M e during reasoning but simplify the setup by removing the evaluation and refinement modules.To ensure accuracy and prevent target leakage, the memory derived from problem P i in iteration I j is excluded when solving P i again in I k for any j &lt; k.</p>
<p>This iterative process demonstrates that as the memory pool grows with each new example, ChemAgent's problem-solving performance improves.Figure 5 shows that as the number of iterations increases, the agent's performance gradually improves and converges to a score higher than the baseline (44.89%).This improvement indicates that ChemAgent can enhance its performance through a simple correct-or-not evaluation of past solutions instead of human-written high-quality trajectories.</p>
<p>COST ANALYSIS</p>
<p>Figure 6: Cost Analysis.The size of each bubble corresponds to the average number of inferences for each method, while the y-axis indicates the average accuracy across the four datasets.</p>
<p>On average, each problem requires 0.012 million tokens without the Evaluation &amp; Refinement module, equivalent to around $0.09 per example.When this module is applied, the average token consumption per problem increases to approximately 0.023 million, costing about $0.1725 per example.Note that the initial library construction is not included in these calculations as it is a one-time setup.</p>
<p>Based on the results in Figure 6, the computational time required by our method is reasonable.While the cost and resource consumption are slightly higher than StructChem, the improvements justify the additional expense.</p>
<p>ERROR ANALYSIS</p>
<p>We conduct an analysis of the trajectories for the failed examples and find three types of errors, shown in Figure 7.</p>
<p>Figure 7: Error analysis.This example highlights a typical incorrect solution, which can be attributed to three main types of errors.Specifically, this problem pertains to an adiabatic process.</p>
<p>(1) Lack of Understanding of the Question.We observe that plans often fail when the problem text contains critical hidden information (e.g., in Figure 7, the terms 'reversibly' and 'adiabatically' are key) or include excessive, redundant details (e.g., the exact conditions such as "250K" are irrelevant to the solution).This challenge is understandable, as even human solvers can be misled by such information and err in their approach.This issue is prevalent and independent of whether the task is tackled by our model or directly queried to the LLM, suggesting that these errors may be inherent limitations of the LLM's capacity.Hence, improving the ability of the foundation model to handle such nuances could significantly enhance performance.</p>
<p>(2) Inaccurate Reasoning.As indicated before (Ouyang et al., 2024), the planning capabilities of LLMs remain insufficient for handling complex problems.Incorrect planning for the reasoning chains exacerbates the problem-solving process, as subsequent decisions and actions are based on initial problem decompositions.This issue persists until the Evaluation &amp; Refinement module detects an error, which may not happen promptly enough to correct the trajectory.</p>
<p>(3) Incorrect Memory Selection.While ChemAgent with memory demonstrates superior performance compared to setups without memory, it sometimes invokes misleading information, even when the similarity between the invoked memory and the problem is high.This indicates a need for more sophisticated memory retrieval and utilization strategies.</p>
<p>In Figure 7, the invoked memory and sub-task 1.1 share considerable similarities-both concern entropy change during a compression process.However, the subtle difference is that the current problem involves an adiabatic process, whereas the examples in memory do not.This seemingly minor distinction can lead to significant changes in the problem-solving strategy.Distinguishing between misleading and beneficial memories remains a challenging issue, as invoked memories and problem texts may be semantically similar yet differ in critical aspects.</p>
<p>ABLATION STUDY</p>
<p>MEMORY COMPONENT ANALYSIS</p>
<p>To understand why our method performs particularly well and which memory component contributes the most, we conduct an ablation study by independently removing each memory component.We test these different settings on all four sub-datasets using GPT-4 as the base LLM.The results are shown in Table 2.As mentioned in §2.3, the execution memory also serves as a few-shot prompting strategy.Therefore, when we remove M e , we add two fixed human-written few-shot examples (provided with our code) into each query of each sub-task.These examples are selected from Firstly, removing any memory component results in an overall performance drop.And notably, the relevant knowledge in LLM itself (M k ) significantly impacts the overall performance of ChemAgent.Delving into why M k contributes the most, we find that this might be due to the insufficient M p and M e in the Atkins dataset.As shown in Table 6, ATKINS has the lowest ratio of the development set to the test set, while it also has the largest test set.This imbalance may result in a small and sometimes irrelevant memory pool for M p and M e to search from.We hypothesize that this issue can be mitigated when the model is exposed to more chemical questions over time.However, due to limited computational resources, we cannot scale up to verify this hypothesis in this research.</p>
<p>It is also worth noting that although the M p , M k , f ewshot setting achieves the same accuracy as the complete memory setting, most of its improvement is evident in the CHEMMC dataset, showing surprisingly higher improvements than others.Since Table 6 and Figure 10 demonstrate that the CHEMMC dataset has a higher ratio of D d to D t and a relatively narrow distribution in subfields, and since human experts carefully wrote the few-shot examples, these high-quality, expert-written examples may serve as a superior memory pool in such circumstances.In the next section, we show that better memory leads to better results, which might explain this phenomenon.We investigate the impact of memory quality through a series of experiments.Specifically, we compare the performance of memory generated by GPT-4 against that generated by GPT-3.5 on the MATTER dataset.Additionally, we create a "hybrid memory" by mixing memories generated by both GPT-3.5 and GPT-4 to observe its performance.These experiments were conducted without including the relevant knowledge within the LLM itself (M k ).As mentioned in §2.1, unlike M p and M e , M k is not preserved in the memory pool shared with other LLMs.Including this type of memory in the ablation study would result in the pollution of provided memory by the M k generated during use.</p>
<p>MEMORY QUALITY INFLUENCE</p>
<p>As shown in Table 3, our data analysis indicates that memory quality significantly impacts the accuracy of responses from ChemAgent.Whether GPT-3.5 or GPT-4 is used, the memory produced by GPT-4 consistently outperforms that generated by GPT-3.5, showing an 8% variation.This underscores the substantial impact of memory quality on the accuracy of generated responses.This observation also explains why the improvements seen with GPT-3.5 are less pronounced compared to those with GPT-4; the lower quality of memory produced by GPT-3.5 is the limiting factor.</p>
<p>Interestingly, hybrid memory performs the worst among the three settings.This can be attributed to the simultaneous invocation of different memories for the same question-one generated by GPT-3.5 and the other by GPT-4-which may confuse the LLM, increasing the likelihood of producing irrelevant or incorrect answers.Additionally, invoking an excessive number of memory tracks during resolution may also contribute to this issue.A more detailed analysis regarding the number of memory tracks invoked is provided in Appendix D.</p>
<p>CONCLUSION</p>
<p>Our research presents a novel approach to enhancing large language models for solving complex chemical problems through self-exploration and memory formation.This method enables models to construct and utilize a library, significantly improving response accuracy.Experiments using datasets and models like GPT-3.5, GPT-4, and Llama3 demonstrate substantial performance gains, with the ChemAgent architecture achieving up to a 36% improvement.The structured library, built through memory decomposition into planning, execution, and knowledge, enhances problemsolving capabilities, which holds promise for generalization to other domains.</p>
<p>A RELATED WORKS</p>
<p>A.1 CHEMICAL REASONING Recent advances in LLMs have shown promise in scientific reasoning, yet chemical reasoning remains particularly challenging.Benchmarks like SciBench (Wang et al., 2024a) have revealed that current LLMs struggle significantly with complex chemical calculations and multi-step reasoning tasks.SciBench includes 869 college-level problems across mathematics, chemistry, and physics, providing a rigorous evaluation of LLM capabilities in these domains.Other datasets (Wadden et al., 2024;Li et al., 2024c;Sun et al., 2024a;Feng et al., 2024;Huang et al., 2024b) have also contributed to advancing the evaluation of LLMs in scientific problem-solving.</p>
<p>In response to these challenges, several approaches have been proposed.StructChem (Ouyang et al., 2024) provides structured guidance by decomposing chemical reasoning into phases such as formula generation, detailed step-by-step reasoning, and confidence-based review.While showing improvements, it still faces limitations with highly complex problems.Other researchers have explored enhancing LLM performance through various prompting strategies (Yang et al., 2024;Yao et al., 2024;Besta et al., 2024).</p>
<p>Some studies have focused on using Python code for reasoning tasks.Jie et al. (2023) demonstrated that self-describing programs, which transform reasoning processes into executable code, can significantly outperform natural language Chain-of-Thought methods (Wei et al., 2022) in certain scenarios.</p>
<p>Specialized tools like ChemCrow (Bran et al., 2023) utilize function calling and precise code generation to tackle chemistry problems.Davis &amp; Aaronson (2023) highlight that combining GPT-4 with external tools like Wolfram Alpha or Code Interpreter can significantly improve problem-solving in science.However, reliably integrating these tools for complex chemical calculations remains a challenge.</p>
<p>A.2 PROBLEM DECOMPOSITION IN SCIENTIFIC REASONING</p>
<p>Decomposing complex problems into smaller sub-tasks has shown to enhance model understanding and accuracy across various domains.In the context of chemical reasoning, this approach is particularly relevant due to the multi-step nature of many chemical problems.Patel et al. (2022) highlights the efficacy of question decomposition by breaking down complex queries into manageable subtasks.Similarly, Khot et al. (2022) demonstrates the benefits of modular task decomposition.Other studies (Lu et al., 2022;Wei et al., 2023) further underscore the advantages of decomposition in complex question answering and reading comprehension.Our work builds on these insights, specifically focusing on how breaking down complex chemical problems can improve the performance of self-evolving agents in this domain.2024) present frameworks for agent self-evolution, aligning with our approach of enabling self-exploration and continuous learning in complex chemical problem-solving.While some of these frameworks also incorporate a memory system, they predominantly emphasize the reuse of past workflows in daily tasks, as demonstrated by Wang et al. (2024b) and Qian et al. (2024).</p>
<p>Furthermore, in chemical reasoning, where even minor errors can lead to significant discrepancies in results, the ability of LLMs to self-correct and refine their outputs is crucial.2024) also inform our approach to developing a self-updating memory system capable of refining its chemical reasoning processes.</p>
<p>B EXPERIMENTAL RESULTS OF MODELS OTHER THAN GPT-4</p>
<p>We also evaluated ChemAgent thoroughly on earlier and less powerful models, such as GPT-3.5 (gpt-3.5-turbo-16k).Specifically, ChemAgent achieves an absolute improvement of +0.17% in terms of the average score compared with the previous SOTA.Also, our framework significantly performs better when equipped with a memory system (+7.13%absolute improvement), which underscores the importance of memory.</p>
<p>However, when it comes to the evaluation and refinement module, an interesting phenomenon is observed: even when outputs generated by GPT-3.5 are evaluated by more capable models like GPT-4, the agent often fails to correct its mistakes.This indicates that GPT-3.5 has a relatively weaker self-correction ability than GPT-4, explaining why evaluation and refinement provide little benefit when GPT-3.5 is used.This finding aligns with previous research (Zhang et al., 2024a).For open-source models, we choose Llama3-7b-Instruct, Llama3-70b-Instruct, and Qwen2.5-72b-Instruct as the experimental model here.The baseline direct reasoning is to directly query the model without adding other instructions.The evaluation and refinement modules are removed from the ChemAgent configuration, and only the Execution Memory (M e ) is added due to the model's relatively lower ability on instruction following.Each experiment is repeated at least 3 times, and the results are averaged.On llama3-7b, the average increase across four datasets is 8.34%.On llama3-70b, the average increase across four datasets is 13.04%.The experiment demonstrates that the stronger the self-capabilities of large models, the more pronounced the performance gains using our framework.</p>
<p>Reference to Appendix F, the atkins dataset, and the matter dataset involve a lot of specialized knowledge in chemistry.The questions in atkins involve the theoretical aspects of thermodynamics, kinetics, and other chemical processes, while matter focuses on studying chemical reaction mechanisms and kinetic processes.This demands a high level of chemical expertise from the model itself.However, the Llama3-7b model, compared to GPT-4 and Llama3-70b, has a lower reservoir of chemical knowledge, thus resulting in average performance on atkins and matter.On the other hand, the challenges in the quan and chemmc datasets lie in complex computations, prediction of molecular structures, and chemical bond reactions, emphasizing the logical reasoning ability of large models.The Llama3 experiment demonstrates that the ChemAgent framework significantly enhances the model's capability in handling logically complex chemical problems, highlighting the requirement for a certain level of subject matter expertise within the framework itself.</p>
<p>However, the significantly lower performance on the atkins dataset by Llama3-7b requires further explanation.The development set in atkins is much more challenging than other datasets.The solutions tend to require more reasoning steps in their decomposition, and the questions in the development set focus on a much narrower sub-topic than those in the test set.This results in a higher rate of mistakes during the development stage, leading to an initially lower-quality static memory pool.Consequently, this greatly influences and even misleads the agent's performance during the test stage.</p>
<p>In general, it can be seen that our approach generalizes well across different LLMs, showing consistent improvement in performance.The results indicate that our self-updating memory mechanism enhances the problem-solving capabilities of LLM-powered agents regardless of the backbone used.Notably, ChemAgent performs significantly better on GPT-4 compared to other models, suggesting that our framework becomes even more effective as base LLM models become more powerful.(2) During development, ChemAgent adopted a strategy to save unit conversion steps in the long-term plan memory pool.This allows the agent to reference correct conversion steps when necessary, ensuring accurate unit conversion.</p>
<p>C.2 HIGHER MEMORY'S SIMILARITY HELPS THE SOLUTION</p>
<p>When solving a given problem P, a series of memories [U 1 , . . ., U n ] may be invoked during the process.Let their similarity to the problem be represented as [S 1 , . . ., S n ], and the mean similarity value is denoted as S mean,P .In Figure 8, we visualize the distribution of S mean,P , categorized by whether P is correctly solved.It is evident that problems with higher S mean,P are more likely to be solved correctly.</p>
<p>We also conducted a Chi-Square Test of Independence to assess the relationship between a similarity threshold (i.e., whether the similarity is greater than 0.805) and the correctness of the solution.</p>
<p>The Chi-Square statistic is 8.77 with a p-value of 0.003, indicating a statistically significant relationship.Thus, future work could focus on improving the similarity between invoked memories and the problem at hand to further enhance problem-solving performance.</p>
<p>D THE INFLUENCE OF THE NUMBER OF MEMORY INVOKED</p>
<p>We analyze the impact of the number of memory instances used on our agent's performance.In this analysis, we preserve the evaluation and refinement module since they also utilize memory.Given that the quantities of M p and M k are not fixed, we focus on examining how the number of M e impacts performance.The results are presented in Figure 9.As the number of invoked memories increases, the average accuracy improves, but the variance also grows.This indicates that although the agent's performance benefits from more memory, it also becomes more unstable.The increase in accuracy suggests that with more memory, ChemAgent adheres more strictly to the required format and acquires more useful knowledge.However, the increase in variance indicates that as the number of memory instances used increases, so too does the potential for encountering misleading or irrelevant information.Our case studies in the 4-shot setting reveal that many memory instances carry some degree of irrelevant or trash information, which accumulates and can harm the agent's abilities.</p>
<p>Interestingly, some seemingly irrelevant information in M e may enhance the creativity of the LLM when solving specific sub-tasks, resulting in a higher maximum accuracy in the 4-shot setting.This suggests that a certain level of seemingly irrelevant information can be beneficial, allowing the agent to explore hard and unknown tasks better.</p>
<p>E LIMITATION</p>
<p>Despite our advances, limitations include the computational intensity and time required for selfexploration, as well as the need for further refinement of memory mechanisms for tackling more complex problems.Future research should focus on understanding the specific mechanisms by which memory formation benefits reasoning, exploring how different types of memory contribute to problem-solving, and identifying optimal strategies for memory utilization.Additionally, due to limitations in budget and computational resources, we demonstrate our approach solely in the context of chemistry and have not conducted comprehensive research across the entire scientific domain, such as considering mathematics or physics.</p>
<p>However, we believe our proposed method has strong potential for generalization across scientific fields.We have made our code open-source and encourage future researchers to apply it to their own datasets.Specifically, adapting our method to a new domain (e.g., the CLASS sub-dataset in SciBench, related to Physics) involves modifying a small portion of the prompts (such as replacing chemistry-related sentences, e.g., "You are a Chemistry expert") and then re-starting the Library Construction phase using a development set from the new domain.</p>
<p>F TASK DOMAINS OF THE DATASETS</p>
<p>The Chemistry domain of SciBench has four datasets, each hand-collected from four college chemistry textbooks.Quantum chemistry (quan) (Hair Jr et al., 2010) provides an exploration of equilibrium, structure, and reactions.Chemistry kinetics (matter) (Atkins et al., 2014) combines physics and mathematics, spanning through quantum mechanics and atomic structure.Quantum mechanics (chemmc) (McQuarrie, 2008) covers quantum mechanics and the applications in chemical bonding.</p>
<p>Physical chemistry (atkins) (Atkins et al., 2023) provides explorations of equilibrium, structure, and reactions.We leverage GPT-4 to annotate each data sample in these datasets for the specific subfields.Statistically, the four datasets cover 15 chemical subfields, and the division of chemical subfields also helps us make associations in related fields to enrich the memory pool.The subfields of the datasets are shown in the figure 10.The problems in this dataset are challenging, with an average of 2 formulas and 5 steps of reasoning required to solve the problems in the experiment (Ouyang et al., 2024).Meanwhile, each dataset provides detailed step-by-step solutions for example problems, which fit well in our framework for the initial construction of memory pools, shown in Table 6.</p>
<p>G PROMPTS</p>
<p>Here is the list of prompts we used in our study.</p>
<p>G.1 INSTRUCTIONS FOR BUILDING MEMORY POOLS.</p>
<p>(1) Split prompts direct ChatGPT to decompose a given task into condition and problem parts.</p>
<p>Reflect prompts are used to double-check the results of the initial decomposition to confirm that the decomposition is complete and correct and to return corrected results in the event of errors.See Figure 11.REFLECT PROMPT SYS = You are ChatGPT, a large language model trained by OpenAI, and also an excellent answer checker who is capable of figuring out whether some conditions and questions separated from the original task are complete and correct.</p>
<p>REFLECT PROMPT = Now, I have a task and its corresponding conditions and questions listed below.I need your help to check whether the conditions and questions are correct and exactly fit the original task.</p>
<p>{{tasks}} {{conditions}} {{questions}}</p>
<p>You should give me back the conditions and questions refined according to the original task; if you think there is nothing that needs to be changed, just output the original conditions and questions.You do NOT need to provide solutions.</p>
<p>All the specified numerical data should be included in the conditions part instead of the question part.Also, the questions should NOT contain something that is not given in the condition.</p>
<p>Your answer should strictly follow the format below.</p>
<p>*** I think the given conditions and questions fit the original task well / not well.Because ... REFINED CONDITIONS:</p>
<p>REFINED QUESTIONS: *** Figure 11: A prompt for task splitting.This prompt is mainly used to initialize the construction of the memory pool by dividing the answers and solutions given in the sample into multiple subproblems and corresponding solutions.</p>
<p>(2) As a collection of problems and solutions already exists in the dataset (D d ), we can decompose a problem and its solution at the same time.Subtask prompts decompose a complex problem into multiple sub-problems, each of which must be clearly described.The Sub Solution prompt directs ChatGPT to decompose the solution into sub-steps corresponding to each of the sub-problems based on the set of sub-problems obtained in the previous step.Sort prompts sort the obtained subproblems from easy to difficult and provide a rationale for the sorting.See Figure 12 Task Difficulty Ranking Prompt for Memory Pool Development SUBTASK PROMPT SYS = You are an excellent expert who thinks step by step and splits a complex question into several steps.</p>
<p>SUBTASK PROMPT =</p>
<p>Given a task's background and its condition: condition Now the question is: question And here is the given solution to this question: solution Please think generally and step by step to divide the question into N subtasks according to the given solution.You must give each task a clear description and every subtask must be designed as a necessary step to the final question.You should strictly follow the format below: *** I think the best way to solve the question and my solution is: ... So I set up subtasks as described below.TASKS=&gt; <st> SUBTASK 1: ... <ed> <st> SUBTASK 2: ... <ed> ... <st> SUBTASK N: ... <ed> *** SUB SOL PROMPT SYS = You are an excellent expert who can think step by step and split a given complex solution of a certain question into several sub-steps according to the given subtasks split from the original question.</p>
<p>SUB SOL PROMPT = Given a task's background and its condition: {{condition}} Now the question is: {{question}} And I have already split the question into the subtasks listed below: {{subtask}} And the whole solution to the original whole question is: {{solution}} Now, Please think generally and step by step to divide the solution into sub-solutions, which must correspond one-to-one with subtasks.You must give each sub-solution a clear description and every sub-solution should be specific and exactly solve the corresponding subtask.</p>
<p>For example, if there are N subtasks, you must generate N sub-solutions.And sub-solution 1 should exactly solve subtask 1.You must generate sub-solutions according to the whole solution given.If the whole solution is "Not provided", just generate your own suggested solution for each subtask.Else if the whole solution is provided, your every sub-solution should be a part of the whole solution.</p>
<p>You should strictly follow the format below: *** I think the relations between subtasks and the whole solution are: ... So, I set up sub-solutions as described below.SOLUTIONS=&gt; <st> SUBSOLUTION 1: ... <ed> <st> SUBSOLUTION 2: ... <ed> <st> SUBSOLUTION N: ... <ed></p>
<hr />
<p>SORT PROMPT SYS = You are ChatGPT, a large language model trained by OpenAI, and also an excellent prompt difficulty evaluator who is capable of sorting a series of tasks according to their difficulty.</p>
<p>SORT PROMPT = Now, I have some tasks (each containing conditions, questions, and corresponding solutions) listed below.I need your help to sort these tasks from the easiest one to the most difficult one for an agent to solve, which means you need to put the simpler subtasks in front.{{tasks}} Think carefully and tell me the reason why you think one is easier and another is harder.You do not take the temporal relationship into consideration when sorting.You do NOT need to refine the given solutions.You do NOT need to follow the original order of the tasks.You are FORBIDDEN to change the description of the tasks given.Just change their order.Your answer should be a permutation of the n tasks.And the id of easier tasks should be placed in front of the harder ones.For example, if there are 3 tasks, and task 2 is the simplest, task 3 is the hardest.Then your final output should be "RESULT=&gt; <st> 2 1 3 <ed> "</p>
<p>Please strictly follow the format below.*** I think the difficult relationship between the tasks is: ...  12: A prompt for ranking task difficulty.The subproblems are sorted in order of difficulty and solved one by one, from easy to hard, by solving the easier subproblems and learning from the experience to solve the harder ones better.This is the idea of using course learning to help agent self-evolution.</p>
<p>G.2 INSTRUCTIONS FOR SOLVING PROBLEMS.</p>
<p>(1) Decomposition prompts (Figure 13) are used to decompose a given chemistry problem into 1-3 subtasks, each with specific goals, criticism, and milestones.A task-handling agent will handle all the subtasks as the inorder-traversal.For example:</p>
<ol>
<li>it will handle subtask 1 first.2. if solved, handle subtask 2. If failed, split subtask 1 as subtask 1.1 1.2 1.3... Then handle subtask 1.1 1.2 1.3... 3. Handle subtasks recursively until all subtasks are solved.Do not make the task queue too complex, make it efficiently solve the original task.</li>
</ol>
<p>RESOURCES:</p>
<p>A task-handling agent can write and execute Python code.</p>
<p>-Task Description -Generate the plan for query with operation SUBTASK SPLIT, and make sure all must-reach goals are included in the plan.</p>
<p>-Important Notice --Always make feasible and efficient plans that can lead to successful task-solving.Never create new subtasks that are similar or the same as the existing subtasks.</p>
<p>-For subtasks with similar goals, try to do them together in one subtask with a list of subgoals rather than split them into multiple subtasks.</p>
<p>-Do not waste time on making irrelevant or unnecessary plans.</p>
<p>-The task handler is powered by sota LLM, which can directly answer many questions.So make sure your plan can fully utilize its ability and reduce the complexity of the subtasks tree.</p>
<p>-You can plan multiple subtasks if you want.</p>
<p>-Minimize the number of subtasks, but make sure all must-reach goals are included in the plan.</p>
<p>-Don't generate tasks that are aimed at understanding a concept, such as "understanding the problem", the LLM who answers the task already knows the underlying concept.Check the generated subtask objectives and milestones for understanding, and regenerate the subtasks if so.</p>
<p>-After the subtask is generated, check to see if the answer for the task has been given in the task's known conditions.If the task has already been resolved, delete the subtask.</p>
<p>USER PROMPT = This is the first time you are handling the task (query), so you should give an initial plan.</p>
<p>{{similar task and plan}} Now try to use SUBTASK SPLIT to split the following problem, here is the query which you should give an initial plan to solve:</p>
<p>-Your task-{{query}}</p>
<p>You will use operation SUBTASK SPLIT to split the query into 1-3 subtasks and then commit.</p>
<p>Figure 13: A prompt for task decomposition.Splitting the problem reduces the difficulty of solving each sub-problem.Atomized simple tasks make it easier to find trajectories of experience that can be drawn upon in the memory pool.</p>
<p>(2) The execution instructions (Figure 14) direct ChatGPT to solve the problem based on the current subtask and known conditions, standardize the return format that must contain the formulae used to solve the problem, a step-by-step reasoning process, and finally give a piece of Python code to arrive at the answer to the problem.</p>
<p>Prompt for Task Execution</p>
<p>SYSTEM PROMPT = A question is divided into many steps, and you will complete one of them.Please provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics.The problem will specify the unit of measurement, which should be included in the answer.</p>
<p>You have been solving a complex task by following a given plan listed below.</p>
<p>-Plan Overview -The complex task has already been split into a tree-based plan as follows: {{all plan}} You have already performed some of the subtasks.</p>
<p>USER PROMPT = Now, you continue to complete subtasks.Please combine the results of previous tasks to complete the current goal of processing subtasks.Please complete all milestones and give a concise answer that can be efficiently used by subsequent subtasks.</p>
<ul>
<li>Your answer should be a piece of Python code that solves the current question.</li>
</ul>
<p>You must end your code by printing all the result and their units.Make sure the code can be successfully run without any input.</p>
<p>Be precise.The answer should be accurate, choose the appropriate units, and prohibit the use of round functions to round off and lose the results.Make sure the code can be successfully run without any input.And import all the modules that you need to use.# Print the result print("The value of beta is:", beta, "cmˆ(-1)")</p>
<p>-Similar tasks -The following are the trajectories of tasks that have been dealt with in the past that are similar to this goal, including the successful tasks and their action lists.You can learn from them and use relevant knowledge in their procedure.{{success prompt}} Figure 14: A prompt for executing tasks.The format of the specified output consists of formulas and reasoning processes.The relevant knowledge in Knowledge Memory improves the accuracy of the formula part, and the reasoning processes make the output Python code more logical.</p>
<p>(3) If the similarity between the retrieved memory trace and the current problem is too low, the Imagination prompts (Figure 15) direct ChatGPT to generate a problem and a solution in the specified format based on the current topic.15: A prompt for associating similar tasks.The dataset covered 15 sub-domains, and the question was modeled by interrogation to determine which sub-domain the question belonged to.The associatively generated trajectories must be in the same format as the trajectories in Execution Memory.</p>
<p>(4) The evaluation prompt (Figure 16) is used to assess the correctness of the current solution, including judging the correctness of the given formulas, the rigor of the reasoning process, and whether the output of the Python code meets the task goals and completes all task milestones.</p>
<p>Prompt for Task Evaluation SCORE PROMPT = You tackled a sub-problem in a chemistry problem; the format of the solution to the problem is <strong>Formulae retrieval: ** and </strong>Reasoning/calculation process:<strong> and </strong>Answer conclusion:**(which includes a piece of Python code and its corresponding output).</p>
<p>-Subtask-The question: {subtask goal} Milestones: {milestones}
[Response Start] {response} [Response End]
For each instance, you need to do four things:</p>
<p>-First, judge whether the given formula is correct and whether the constants are correct.</p>
<p>-Second, judge whether the reasoning process is rigorous and correct.</p>
<p>-Third, determine whether the Python function outputs the parameters required by the task goal and milestone.</p>
<p>-Finally, score the degree of completion and correctness of the whole task.You should give the "confidence score" on the scale of [0,1].Please be very strict about your ratings.(5) Finally, the final answer and summary of posterior knowledge are obtained by summary prompts (Figure 17).Summarize the total process of solving the problem, the relevant knowledge used, and the formulas that can be used to enrich the memory pool.At the same time, the answers to all sub-problems are combined to produce the final answer to the whole problem.</p>
<p>Prompt for Task Summary SYSTEM PROMPT = You are a posterior knowledge obtainer.Now that you've completed a parent task, the task is made up of many subtasks, each of which has been completed.</p>
<p>Your plan for the parent task is as follows:</p>
<p>-Parent Goal -{{parent goal}} -Sub-task division -{{all plan}}</p>
<p>The flow of your actions for handling subtasks is:</p>
<p>-Action lists for Subtasks -{{sub plan}} Now, you have to learn some posterior knowledge from this process by doing the following things:</p>
<ol>
<li>
<p>Summary: Summarize the ideas of all subtasks to the parent task, and summarize a total process to the parent task according to the action process of each subtask.Explicitly include all the formulas used during performing the subtasks in this summary.Specific numbers and numerical results are NOT needed in this part.</p>
</li>
<li>
<p>Reflection of knowledge: After performing this task, you get some knowledge of generating plans for similar tasks.Only conclude the used knowledge and formulas used in this whole task, do NOT contain numerical calculation process and results.</p>
</li>
</ol>
<p>Final Answer:</p>
<p>Give the final answer to the task according to the course of the task, and ask the answer to be very short, without explaining the reason and adding unnecessary punctuation.If it's a math problem, only the last value is given.</p>
<p>Figure 17: A prompt for summarizing tasks.Summarizing tasks is crucial for the self-evolution of ChemAgent.Summarize the formulas and principles of this task and add them to the Memory Library.</p>
<p>(6) When a task fails to execute, it may be because the task decomposition is unreasonable.The framework can perform adjustment operations such as adding/splitting/deleting the current task tree to adjust the overall task framework.See Figure 18 for the exact prompt.</p>
<p>Prompt for Task Refinement SYSTEM PROMPT = You are a plan-rectify agent, your task is to iteratively rectify a plan of a query.</p>
<p>-Background Information -PLAN AND SUBTASK: A plan has a tree manner of subtasks: task 1 contains subtasks task 1.</p>
<p>RESOURCES:</p>
<p>A task-handling agent can write and execute Python code.</p>
<p>-Task Description -Your task is iteratively to rectify a given plan based on the goals, suggestions, and now handling positions.</p>
<p>In this mode, you will use the given operations to rectify the plan.At each time, use one operation.SUBTASK OPERATION: 1. split: Split an already handled but failed subtask into subtasks because it is still so hard.</p>
<p>The "target subtask id" for this operation must be a leaf task node that has no children subtasks, and should provide new split "subtasks" of length 2-4.You must ensure the "target subtask id" exists, and the depth of new split subtasks &lt; {{max plan tree depth}}.</p>
<p>-split 1.2 with 2 subtasks will result in create new 1.2.1, 1.2.2 subtasks.</p>
<ol>
<li>add: Add new subtasks as brother nodes of the 'target subtask id'.This operation will expand the width of the plan tree.The 'target subtask id' should point to a now-handling subtask or future subtask.</li>
</ol>
<p>-add 1.1 with two subtasks will result in creating new 1.2, 1.3 subtasks.</p>
<p>-add 1.2.1 with 3 subtasks wil result in create new 1.2.2, 1.2.3, 1.2.4 subtasks.</p>
<ol>
<li>delete: Delete a subtask.The 'target subtask id' should point to a future/TODO subtask.Don't delete the now handling or done subtask.</li>
</ol>
<p>-delete 1.2.1 will result in delete 1.2.1 subtask.</p>
<p>-Note -The user is busy, so make efficient plans that can lead to successful task-solving.Do not waste time making irrelevant or unnecessary plans.Don't use search engines since you know about planning.Don't divide trivial tasks into multiple steps.</p>
<p>If the task is unsolvable, give up and submit the tas k.</p>
<p>*** Important Notice *** -Never change the subtasks before the handling positions, you can compare them in lexicographical order.</p>
<p>-Never create (with add or split action) new subtasks that are similar or the same as the existing subtasks.</p>
<p>-For subtasks with similar goals, try to do them together in one subtask with a list of subgoals rather than split them into multiple subtasks.</p>
<p>-Every time you use an operation, make sure the hierarchy structure of the subtasks remains, e.g., if subtask 1.2 is to "find A,B,C" , then the newly added plan directly related to this plan (like "find A", "find B", "find C") should always be added as 1.2.1, 1.2.2, 1.2.3... -You are restricted to give operations in at most 4 times, so the plan refinement is not so much.</p>
<p>-The task handler is powered by sota LLM, which can directly answer many questions.So make sure your plan can fully utilize its ability and reduce the complexity of the subtasks tree.</p>
<p>USER PROMPT= Your task is to choose one of the operators of SUBTASK OPERATION, note that 1.You can only modify the subtask with subtask id &gt; {{subtask id}}(not included).</p>
<ol>
<li>Please use a function call to respond to me (remember this!!!).</li>
</ol>
<p>Figure 18: A prompt for refining tasks.Incorrect answers may result from an error in a sub-question, leading to subsequent inaccuracies, or from a misjudgment in the decomposition of the question.In the former case, the Task Evaluation module is used to enhance the accuracy of problem-solving, while the Task Refinement module is employed to modify the question decomposition tree in the latter case.</p>
<p>G.3 INSTRUCTIONS FOR BASELINE</p>
<p>Both direct-reasoning and fewshot+Python setups are used as baselines in the main experiment, mainly referring to the experimental setup in SciBench Wang et al. (2024a).The following instruction (Figure 19) is used in the fewshot+Python setup, which asks for a piece of Python code to be output and is followed by three demonstrations.</p>
<p>Prompt for Few-Shot Python</p>
<p>SYS PROMPT=</p>
<p>Please provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics.The problem will specify the unit of measurement.For clarity, please translate the solution steps into Python code and encase the Python code within triple backticks.</p>
<p>Figure 19: A prompt for Few-shot+Python.The baseline setup directly requires LLMs to generate Python code, and through few-shot learning, the responses are standardized.This approach, compared to the direct-reasoning setup, has shown an improvement in correctness.</p>
<p>Statement 1: Addition of a non-reacting gas at constant volume and constant temperature to a container containing gas (X) does not change the partial pressure of gas (X).Statement 2: The addition of a non-reacting gas to a container at constant volume and constant temperature results in an increase in the total pressure of the container, but not in the partial pressure of the gases in the container.Which of the following statements is the most correct option for the given two statements?</p>
<p>A</p>
<p>Given:</p>
<p>-P partial is the partial pressure of gas (X) -n gas is the number of moles of gas (X) -T is the temperature, V is the volume Calculations:</p>
<p>1.The partial pressure of a gas in a container is given by the ideal gas law, which states that the partial pressure is directly proportional to the number of moles of the gas, the temperature, and inversely proportional to the volume.</p>
<p>P partial = n gas RT V 2. When a non-reacting gas is added to the container at constant volume and constant temperature, the number of moles of gas (X) does not change.Therefore, the partial pressure of gas (X) remains the same because n gas , R, T , and V are constant.The initial and final partial pressures of gas (X) are the same, verifying that Statement 1 is true.Execution of Task 2 -Verify the validity of Statement 2 Formula:</p>
<p>-The total pressure of a gas mixture is the sum of the partial pressures of the individual gases:</p>
<p>P total = P 1 + P 2 + . . .+ P n</p>
<p>Calculation:</p>
<ol>
<li>
<p>Consider a container with gas (X) at constant volume and temperature.The initial total pressure is the partial pressure of gas (X), P X .2. When a non-reacting gas (Y) is added to the container at constant volume and temperature, the partial pressure of gas (X) remains the same, P X .</p>
</li>
<li>
<p>The total pressure of the container after adding gas (Y) is the sum of the partial pressures of gas (X) and gas (Y), P total = P X + P Y .Execution of Task 3 -Compare Statements and Choose the Correct Option Formula:</p>
</li>
</ol>
<p>No specific formula is needed for this subtask.The change in molar internal energy when CaCO 3 ( s) as calcite converts to another form, aragonite, is +0.21 kJ/mol.The pressure is 1.0 bar.The densities of the polymorphs are 2.71 g/cm 3 and 2.93 g/cm 3 , respectively.Question 7 Verify the units of the calculated result to ensure consistency with the required units (e.g., kJ/mol).</p>
<p>Solution 7</p>
<p>The units of the result −0.277 kJ/mol are correct and consistent with the required units.</p>
<p>SUB-TASK 8 Condition 8</p>
<p>The change in molar internal energy when CaCO 3 ( s) as calcite converts to another form, aragonite, is +0.21 kJ/mol.The pressure is 1.0 bar.The densities of the polymorphs are 2.71 g/cm 3 and 2.93 g/cm 3 , respectively.Question 8</p>
<p>Summarize the final answer with the correct sign and units.</p>
<p>Solution 8</p>
<p>The difference between the molar enthalpy change and the molar internal energy change is ∆H m − ∆U m = −0.28kJ/mol.</p>
<p>Figure 1 :
1
Figure 1: Comparison of problem-solving approaches for a hydrogen atom energy transition problem.The figure illustrates three different methods: (a) shows a standard Chain-of-Thought approach with calculation errors (in steps 3 and 4) in Wang et al. (2024a).(b) demonstrates the StructChem (Ouyang et al., 2024) method with formula generation and step-by-step reasoning but fails due to an incorrect constant and incorrect unit conversion (in steps 1 and 4).(c) presents the ChemAgent solution, featuring task decomposition, memory retrieval from the library, and reasoning, leading to the accurate final answer.</p>
<p>Sub-task 1 :
1
Use kinetic energy formula to calculate velocity of electron Sub-solution 1: [Formula 1] p = m * v where p is ….[Step 1] Identify the given values: Final answer (a) Library-enhanced Reasoning T sk: C lcul te the de Broglie w velength for ( ) n electron with kinetic energy of ._Theunit of the nswer should be .found, then generate a task as imagination.</p>
<p>Figure 2 :
2
Figure 2: The diagram of our overall framework.It contains (a) library-enhanced reasoning and (b) library construction.(a) illustrates how ChemAgent utilizes the library to address a new task for the test set.And (b) demonstrates the construction of the library over the dev set, including Plan Memory M p and Execution Memory M e ).</p>
<p>Figure 3 :
3
Figure 3: Given a task P, the relevant memory examples are provided in the library.Specifically, while Execution Memory (M e ) and Plan Memory (M p ) are derived from prior experiences, Knowledge memory (M k ) is generated by LLM based on the problem prompt.The conditions C are not explicitly presented here but are embedded within P and the [GOAL] of M e .</p>
<p>provides detailed examples.The exact context of memory and corresponding prompts are in Appendix G. • Planning Memory (M p ): This component stores high-level strategies and methodologies for approaching complex problems.• Execution Memory (M e ): This contains highly structured descriptions of specific problem contexts and their corresponding solutions, serving as detailed execution plans.• Knowledge Memory (M k ): This houses fundamental chemistry principles and formulas, acting as a ready reference.This component is generated temporarily during the solution of a specific task and is not intended for permanent retention.</p>
<p>Figure 4 :
4
Figure 4: Overall framework of the evaluation &amp; refinement module.ChemAgent continuously modifies the solution or the comprehensive strategy until it either reaches the maximum number of trials or meets the evaluator's criteria.</p>
<p>A. 3
3
SELF-EVOLUTION AND SELF-CORRECTION IN AGENT REASONING Recent research has explored the self-evolution and optimization of LLMs, which is particularly relevant for tackling the complexities of chemical reasoning.Yang et al. (2023) explore methods for enhancing LLM performance through self-improvement techniques, while Fernando et al. (2023) investigate self-referential self-improvement via prompt evolution.Additionally, Zhou et al. (2024), Jiang et al. (2023), Hu et al. (2024) and Qian et al. (</p>
<p>Figure 8 :
8
Figure 8: Memory Similarity Analysis.The probability density functions of invoked memory similarity for solved and failed tasks are visualized using Kernel Density Estimation.We use a reference value of 0.805 to do the Mann-Whitney U Test and get a p-value of 0.008.The similarity values are generally higher in the trajectories of solved tasks.</p>
<p>Figure 9 :
9
Figure 9: Test on MATTER dataset."shots" represents the number of memory tracks given to the LLM.It demonstrates that as the number of demonstrations increases, the agent's average performance improves, but this comes at the expense of its stability.</p>
<p>Figure 10 :
10
Figure 10: Chemical subfields covered by the four datasets.The subfields involved in each dataset are different, and a task may require knowledge from multiple subfields.</p>
<p>for example, you could respond to the Answer conclusion part like this: <strong>Answer conclusion:</strong> [answer]:</p>
<p>Figure16: A prompt for evaluating tasks.Evaluate the current answer, generate answers consecutively, and select the answer with the highest score.</p>
<p>∆H m − ∆U m = p∆V m ∆H m − ∆U m = (1.0 × 10 5 Pa) × (−2.77 × 10 −6 m 3 /mol) ∆H m − ∆U m = −0.277kJ/mol SUB-TASK 7 Condition 7</p>
<p>Table 1 :
1
Performance on the test sets of four datasets: QUAN, CHEMMC, ATKINS, and MATTER.Results are compared with baselines under two different setups: a zero-shot setting with no demonstrations and a few-shot setting with three demonstrations.Accuracy scores are computed using the approximation detailed in Section 4.3.The best results for each setup are highlighted in bold, and the second-best results are underlined.In some settings, the two modules described in Sections 2.4 (Memory) and 2.6 (Evaluation &amp; refinement) are disabled, helping illustrate each module's impact on the overall performance.
ModelsCHEMMC MATTER ATKINS QUANAvg.Baselines, all based on GPT-4 (gpt-4-1106-preview)Few-shot + Direct reasoning28.2114.2920.6914.71 19.48Few-shot + Python38.4634.6957.0144.12 43.57StructChem58.9730.6759.8141.18 47.66ChemAgent, all based on GPT-4 (gpt-4-1106-preview)ChemAgent74.3648.9861.1844.12 57.16w/o Evaluation &amp; Refinement61.5444.8957.9444.12 52.12w/o Memory, Evaluation &amp; Refinement58.9738.7857.9441.18 49.22all based on Llama 3.1-7bDirect reasoning28.2010.2022.438.8217.41ChemAgent56.4112.2419.6314.71 25.75all based on Llama 3.1-70bDirect reasoning33.3330.6130.8423.53 29.48ChemAgent64.1032.6543.9329.41 42.52all based on Qwen 2.5-72bDirect reasoning48.7232.6557.0135.50 43.47ChemAgent69.2344.9056.0744.12 53.58</p>
<p>Table 2 :
2
The best performing score is highlighted in bold and second-best is underlined.The number of problems in each textbook weighs the average score.We also provide the results of ChemAgent without any of these memories as a baseline.Evaluation &amp; refinement module is removed.
Memory componentCHEMMC MATTER ATKINS QUANAvg.None58.9738.7857.9441.18 51.53+ Mp Me M k61.5444.8957.9444.12 53.71+ Mp Me66.6644.8952.3441.18 51.53+ Me M k58.9746.9461.6829.41 53.27+ Mp M k few-shot74.3642.8657.9432.35 53.71the development set (D d ) corresponding to each dataset to ensure they do not provide misleadinginformation.</p>
<p>Table 3 :
3GPT-3.5 GPT-4GPT-3.5 Memory11.2236.73GPT-4 Memory13.2644.89Hybrid Memory10.2028.57
Results of memory quality analysis, with the best scores highlighting in bold.Here, memory refers to M p and M e .The relevant knowledge in LLM itself (M k ) is removed from the framework.Test on MATTER dataset.</p>
<p>Table 4 :
4
Performance on the test sets of four datasets: QUAN, CHEMMC, ATKINS, and MATTER.We compare baselines under two different settings: a zero-shot setting with no demonstrations and a few-shot setting with three demonstrations.The accuracy scores are computed with the approximation detailed in Section 4.3.The best results under each setup are highlighted in bold, and the second-best results are underlined.This experiment is done using
(gpt-3.5-turbo-16k)GPT-3.5CHEMMC MATTER ATKINS QUAN Avg.Baselines, all based on GPT-3.5Few-shot+Direct reasoning23.088.169.355.8811.61Fewshot+Python33.3316.3313.088.8217.89StructChem43.5924.4926.1732.35 31.65ChemAgent, all based on GPT-3.5ChemAgent w/o memory38.4616.3323.3620.59 24.69ChemAgent w/ memory56.4118.3728.9723.53 31.82+ evaluate and refine41.0316.3328.0420.59 26.50+ evaluate by GPT-4 and refine46.1522.4528.9726.47 31.01</p>
<p>Table 5 :
5
Experimental results on Llama 3.1-70b, Llama 3.1-70b and Qwen 2.5-72b.From the average improvement across four datasets, the stronger the model, the greater the improvement.Compared to the Llama 3.1-7b model, the overall enhancement using Llama 3.1-70b is better, particularly noticeable in the CHEMMC and ATKINS datasets.
ModelsCHEMMC MATTER ATKINS QUAN Avg.Llama 3.1-7bDirect reasoning28.2010.2022.438.8217.41ChemAgent56.4112.2419.6314.71 25.75Llama 3.1-70bDirect reasoning33.3330.6130.8423.53 29.48ChemAgent64.1032.6543.9329.41 42.52Qwen 2.5-72bDirect reasoning48.7232.6557.0135.50 43.47ChemAgent69.2344.9056.0744.12 53.58C PERFORMANCE BOOST: WHERE AND WHY OUR METHOD IMPROVES
C.1 CALCULATION AND UNIT CONVERSION ARE MORE PRECISEChemAgent achieves notably higher accuracy in calculations and unit conversions.Two key factors contribute to this: (1) Python code is demonstrated alongside each corresponding sub-task in memory;</p>
<p>Table 6 :
6DatasetsTopicsD t (D d )R d/tCHEMMCQuantum mechanics39 (9)0.231MATTERChemistry kinetics49 (10)0.204ATKINSPhysical chemistry107 (16)0.150QUANQuantum chemistry34 (8)0.235
Detailed statistics and information of the four datasets we experiment with.D t and D d refer to the number of data samples with and without solutions (development set and test set)."R d/t " means the ratio of the development set to the test set.</p>
<p>Task Split Prompt for Memory Pool Development SPLIT PROMPT SYS= You are ChatGPT, a large language model trained by OpenAI, also an excellent prompt evaluator, who is capable of splitting tasks and evaluating the difficulty of tasks which will be solved by other autonomous agents powered by LLM.SPLIT PROMPT = Now, I have a target task and its solution here: task: task solution: solution And I also have a Python environment where agents can write code.(It is also okay not to use them and just use your knowledge): Then, split or separate the sentence of the original task into two parts: one is conditions, and the other is questions.Below is the format you should follow when you give the answer:
<strong><em>CONDITIONS:QUESTIONS:</em></strong></p>
<p>, the main purpose of the subtask, and what will you do to reach this goal?"goal.criticism":string,whatpotential problems may the current subtask and goal have?"milestones":list[string],what milestones should be achieved to ensure the subtask is done?And What formulas might the current subtask use?Make it detailed and specific.
Task Decomposition PromptSYSTEM PROMPT =You are a Chemistry expert and an efficient plan-generation agent.Now, you are doing an exam; you must decompose a problem into several subtasks thatdescribe what the goals for the problem.-Background Information -PLAN AND SUBTASK:A plan has a tree manner of subtasks: task 1 contains subtasks task 1.1, task 1.2, task 1.3, ...and task 1.2 contains subtasks 1.2.1, 1.2.2, ...A subtask structure has the following JSON component:{"subtask name": string, name of the subtask"goal.goal": string
}SUBTASK HANDLE:</p>
<p>Please create {{top k}} advanced chemistry questions suitable for in-depth understanding and application of chemical formulas and principles.Each question should focus on the deeper aspects of the[topic]provided to understand the principles of chemistry and the reasoning process.[topic]:{{topic}}Guidelines for Problem Creation: -Use of Samples: You are provided with sample questions for reference.Feel free to use these to guide the style and depth of the problems.-Beyondthe Examples: You are encouraged to use your background and expertise to create problems that go beyond the provided examples, ensuring the problems are as diverse and comprehensive as possible.Formulas and Knowledge Needed: List the equations and concepts required to understand and solve the problem.II.Reasoning Steps: Outline the logical or mathematical steps to solve the problem.III.Python code: Executable Python code is generated to solve problems.At the end of each problem, please include Python code that can be used to confirm and verify the correctness of the provided solution.The Python solutions should illustrate the entire solution process, from the initial step to the final answer, rather than merely validating the result.Develop these solutions such that each step of the mathematical process is explicitly demonstrated and calculated in Python.Additionally, ensure that you run your Python code to confirm it is free from any errors.d.Diversity: Ensure a wide range of problems, each focusing on different elements from the subtopic list.e. Presentation: Please output your problem statement, solution, detailed explanation, and a self-contained Python code for verification below in the specified format.
Requirements for Each Problem:a. Problem Statement: Clearly define the challenge or task.b. Solution: Provide a detailed solution that includes:I. For each generated question, the output is required to be in the following format:[Task Start][Problem Statement]: (your problem)<strong>Formulae retrieval: </strong>[Formula 1] (the formula required to solve the problem)[Formula 2] (the second formula required to solve the problem, if any) ...[Formula n] (the n-th formula required to solve the problem, if any)<strong>Reasoning/calculation process:</strong>[step 1] (the first step for solving this problem).....[step n] (the n-th step for solving the problem, if any)<strong>Answer conclusion:</strong>[answer]: (Python code that can be executed independently)[Task End]You have to generate {{topk}} tasks about {{topic}}.Sample demonstration example:{{example shots}}Prompt for Similar Task AssociationIMG PROMPT = Figure</p>
<p>"milestones": list[string], what milestones should be achieved to ensure the subtask is done?And What formulas might the current subtask use?Make it detailed and specific.}
SUBTASK HANDLE:A task-handling agent will handle all the subtasks as the inorder-traversal. For example:1. it will handle subtask 1 first.2. if solved, handle subtask 2. If failed, split subtask 1 as subtask 1.1 1.2 1.3... Then handlesubtask 1.1 1.2 1.3...3. Handle subtasks recursively until all subtasks are solved. Do not make the task queue toocomplex, make it efficiently solve the original task.1, task 1.2, task 1.3, ...and task 1.2 contains subtasks 1.2.1, 1.2.2, ...A subtask structure has the following JSON component:
{ "subtask name": string, name of the subtask "goal.goal":string, the main purpose of the subtask, and what will you do to reach this goal?"goal.criticism":string, what potential problems may the current subtask and goal have?</p>
<p>.Statement 1 is False, Statement 2 is True.B.Statement 1 is True, Statement 2 is False.C.Statements 1 and 2 are True.Statement 2 is NOT the correct explanation for Statement 1. D.Statements 1 and 2 are True.Statement 2 is the correct explanation for Statement 1. Choose one correct choice.Task Decomposition Task 1: Verify the validity of Statement 1 Determine effect of adding a non-reacting gas on partial pressure of gas (X) at constant volume and temperature.Task 2: Verify the validity of Statement 2 Determine effect of adding a non-reacting gas on total pressure at constant volume and temperature.Task 3: Compare Statements and Choose the Correct Option Compare the findings from the analysis of Statements 1 and 2 and determine the most correct option from the provided choices.Execution of Task 1 -Verify the validity of Statement 1 Formulas: -Ideal Gas Law: P V = nRT -P partial =
ngasRTV
For a more detailed discussion of the related work, please refer to Appendix A.
H SPECIFIC CASE TRAJECTORIESTo concretely demonstrate the problem-solving process of our framework, we provide a complete execution trajectory of a successfully answered question to illustrate the procedural workflow of our framework.Task DescriptionAt 300 K and 20 atm, the compression factor of a gas is 0.86.Calculate the volume occupied by 8.2 mmol of the gas under these conditions.The unit of the answer should be cm 3 .Task Decomposition Task 1: Calculate the volume in liters Use the definition of the compression factor and the ideal gas law to calculate the volume occupied by 8.2 mmol of the gas under the given conditions in liters.-Conversion from liters to cubic centimeters: 1 L = 1000 cm 3 Calculation: 1. Convert volume from liters to cubic centimeters: V = 0.0086801 L × 1000 cm 3 /L = 8.6801 cm 3Summary ResultsThe volume occupied by 8.2 mmol of the gas under the given conditions is approximately 8.6801 cm 3 .We also provide an execution trajectory of a successfully answered question in another benchmark, ChemBench(Mirza et al., 2024)which direct-reasoning method fails to solve.The backbone LLM used here is gpt-4o (2024-08-01-preview).Task DescriptionReasoning:1. Analyze the results from the previous subtasks: -From Task 1.1:The partial pressure of gas (X) remains unchanged after adding a non-reacting gas at constant volume and temperature.-From Task 1.2:The total pressure increases after adding a non-reacting gas at constant volume and temperature, but the partial pressure of gas (X) remains unchanged.2. Compare the findings from the analysis of Statements 1 and 2. 3. Determine the most correct option based on the analysis: -Both statements are true, and Statement 2 correctly explains Statement 1. Summary Results D. Statements 1 and 2 are True.Statement 2 is the correct explanation for Statement 1.An example result of the decomposition step of the library construction phase, which help understand how the sub-tasks are organized.Original TaskThe change in molar internal energy when CaCO 3 ( s) as calcite converts to another form, aragonite, is +0.21 kJ mol −1 .Calculate the difference between the molar enthalpy and internal energy changes when the pressure is 1.0 bar given that the densities of the polymorphs are 2.71 g cm −3 and 2.93 g cm −3 , respectively.Original SolutionThe change in enthalpy when the transition occurs iswhere a denotes aragonite and c calcite.It follows by substitutingSubstitution of the data, using M = 100 g mol −1 , givesDecomposition ResultsSUB-TASK 1 Condition 1The change in molar internal energy when CaCO 3 ( s) as calcite converts to another form, aragonite, is +0.21 kJ/mol.The pressure is 1.0 bar.The densities of the polymorphs are 2.71 g/cm 3 and 2.93 g/cm 3 , respectively.Question 1 Identify the given data and the required physical quantities (molar internal energy change, pressure, densities, etc.).Solution 1The change in molar internal energy when CaCO 3 ( s) as calcite converts to another form, aragonite, is +0.21 kJ/mol.The pressure is 1.0 bar.The densities of the polymorphs are 2.71 g/cm 3 and 2.93 g/cm 3 , respectively.Question 2Express the molar volumes of calcite and aragonite in terms of their densities and the molar mass of CaCO 3 .Solution 2where M is the molar mass of CaCO 3 (100 g/mol).SUB-TASK 3 Condition 3The change in molar internal energy when CaCO 3 ( s) as calcite converts to another form, aragonite, is +0.21 kJ/mol.The pressure is 1.0 bar.The densities of the polymorphs are 2.71 g/cm where ∆V m = V m (aragonite) − V m (calcite).SUB-TASK 6 Condition 6The change in molar internal energy when CaCO 3 ( s) as calcite converts to another form, aragonite, is +0.21 kJ/mol.The pressure is 1.0 bar.The densities of the polymorphs are 2.71 g/cm 3 and 2.93 g/cm 3 , respectively.Question 6 Substitute the values of pressure, molar internal energy change, and the change in volume into the enthalpy change equation to find the difference between the molar enthalpy and internal energy changes.Solution 6
Physical chemistry: quanta, matter, and change. Peter Atkins, Julio De Paula, Ronald Friedman, 2014Oxford University PressUSA</p>
<p>Atkins' physical chemistry. Peter Atkins, Julio De Paula, James Keeler, 2023Oxford university press</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Autonomous chemical research with large language models. A Daniil, Robert Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 6242023</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, arXiv:2304.05376ChemCrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>PRESTO: Progressive pretraining enhances synthetic chemistry outcomes. He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, Yu Li, arXiv:2406.131932024arXiv preprint</p>
<p>KNIFE: Distilling meta-reasoning knowledge with free-text rationales. Aaron Chan, Zhiyuan Zeng, Wyatt Lake, Brihi Joshi, Hanjie Chen, Xiang Ren, ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML. 2023</p>
<p>Organa: A robotic assistant for automated chemistry experimentation and characterization. Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Alán Aspuru-Guzik, Animesh Garg, Florian Shkurti, ArXiv, abs/2401.069492024</p>
<p>Testing GPT-4 with wolfram alpha and code interpreter plug-ins on math and science problems. Ernest Davis, Scott Aaronson, arXiv:2308.057132023arXiv preprint</p>
<p>Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, Huajun Chen, arXiv:2406.09098SciKnowEval: Evaluating multi-level scientific knowledge of large language models. 2024arXiv preprint</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. Chrisantha Fernando, Dylan Sunil Banarse, Henryk Michalewski, Simon Osindero, Tim Rocktäschel, Fortyfirst International Conference on Machine Learning. 2023</p>
<p>Rarr: Researching and revising what language models say, using language models. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot ; Luyu, Zhuyun Gao, Panupong Dai, Anthony Pasupat, Arun Chen, Yicheng Tejasvi Chaganty, Vincent Fan, Ni Zhao, Hongrae Lao, Da-Cheng Lee, Juan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics2022. 20231The Eleventh International Conference on Learning Representations</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Empowering working memory for large language model agents. Jing Guo, Nan Li, Jian Qiang Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, Ming Xu, ArXiv, abs/2312.172592023a</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, Advances in Neural Information Processing Systems. 2023b36</p>
<p>Multivariate data analysis. Joseph F Hair Jr, Wiliam C Black, Barry J Babin, Rolph E Anderson, Multivariate data analysis. Prentice Hall2010</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, arXiv:2408.084352024arXiv preprint</p>
<p>Language models, agent models, and world models: The law for machine reasoning and planning. Zhiting Hu, Tianmin Shu, arXiv:2312.052302023arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Understanding the planning of llm agents: A survey. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen, ArXiv, abs/2402.027162024a</p>
<p>Chemeval: A comprehensive multi-level chemical evaluation for large language models. Yuqing Huang, Rongyang Zhang, Xuesong He, Xuyang Zhi, Hao Wang, Xin Li, Feiyang Xu, Deguang Liu, Huadong Liang, Yi Li, Jian Cui, Zimu Liu, Shijin Wang, Guoping Hu, Guiquan Liu, Qi Liu, Defu Lian, Enhong Chen, arXiv:2409.139892024barXiv preprint</p>
<p>Selfevolve: A code evolution framework via large language models. Shuyang Jiang, Yuhao Wang, Yu Wang, arXiv:2306.029072023arXiv preprint</p>
<p>Design of chain-ofthought in math problem solving. Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, Hang Li, arXiv:2309.110542023arXiv preprint</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Intelligence and the cognitive unconscious. The Cambridge handbook of intelligence. Scott Barry, Kaufman , 2011</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Fundamental capabilities of large language models and their applications in domain scenarios: A survey. Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024a1</p>
<p>Vector storage based long-term memory research on llm. Kun Li, Xin Jing, Chengang Jing, International Journal of Advanced Network, Monitoring and Controls. 2024b</p>
<p>Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, Hyeonjung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, arXiv:2407.04903A multimodal multi-discipline dataset for PhD-level scientific comprehension. 2024carXiv preprint</p>
<p>Chang Liao, Yemin Yu, Yu Mei, Ying Wei, arXiv:2402.01439From words to molecules: A survey of large language models in chemistry. 2024arXiv preprint</p>
<p>The llama 3 herd of models. A I , Meta Llama Team, July 2024</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. Donald A McQuarrie. Quantum chemistry. University Science Books2022. 200835</p>
<p>Are large language models superhuman chemists?. Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Mohammad Amir, Maximilian Elahi, Caroline T Greiner, Tanya Holick, Mehrdad Gupta, Christina Asgari, Lea C Glaubitz, Yannik Klepsch, Jakob Köster, Santiago Meyer, Tim Miret, Fabian Alexander Hoffmann, Michael Kreth, Ringleb, Nicole Roesner, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, and Kevin Maik Jablonka2024</p>
<p>Josh Openai, Steven Achiam, Sandhini Adler, Agarwal, Lama Ahmad, et al. GPT-4 technical report. 2024</p>
<p>An evaluation of dual-process theories of reasoning. Magda Osman, Psychonomic bulletin &amp; review. 1162004</p>
<p>Structured chemistry reasoning with large language models. Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Yejin Choi, Jiawei Han, Lianhui Qin, Forty-first International Conference on Machine Learning. 2024</p>
<p>Is a question decomposition unit all we need. Pruthvi Patel, Swaroop Mishra, Mihir Parmar, Chitta Baral, 2022 Conference on Empirical Methods in Natural Language Processing. 2022. 2022</p>
<p>Leveraging biomolecule and natural language through multi-modal learning: A survey. Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, Rui Yan, arXiv:2403.015282024arXiv preprint</p>
<p>Investigate-consolidate-exploit: A general strategy for inter-task agent selfevolution. Shihao Cheng Qian, Yujia Liang, Yining Qin, Xin Ye, Yankai Cong, Yesai Lin, Zhiyuan Wu, Maosong Liu, Sun, arXiv:2401.139962024arXiv preprint</p>
<p>Recursive introspection: Teaching language model agents how to self-improve. Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar, arXiv:2407.182192024arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Replan: Robotic replanning with perception and language models. Marta Skreta, Zihan Zhou, Jia Lin Yuan, Kourosh Darvish, Alán Aspuru-Guzik, Animesh Garg, ArXiv, abs/2401.041572024</p>
<p>SciEval: A multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, ; Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, arXiv:2401.05561Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024a. 2024b38arXiv preprintTrustworthiness in large language models</p>
<p>The complexity of reasoning about and with chemical representations. Vicente Talanquer, 10.1021/jacsau.2c00498JACS Au. 2122022</p>
<p>David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, arXiv:2406.07835A resource to enhance language model instruction-following over scientific literature. 2024arXiv preprint</p>
<p>SciBench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Shichang Arjun R Loomba, Yizhou Zhang, Wei Sun, Wang, Forty-first International Conference on Machine Learning. 2024a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. Zhiruo Wang, Jiayuan Mao, Daniel Fried, Graham Neubig, arXiv:2409.074292024bAgent workflow memory. arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>When do decompositions help for machine reading?. Kangda Wei, Dawn Lawrie, Benjamin Van Durme, Yunmo Chen, Orion Weller, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>Bridging text and molecule: A survey on multimodal frameworks for molecule. Yi Xiao, Xiangxin Zhou, Qiang Liu, Liang Wang, arXiv:2403.138302024arXiv preprint</p>
<p>Large language models as optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, arXiv:2406.04271Buffer of thoughts: Thought-augmented reasoning with large language models. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Leave it to large language models! correction and planning with memory integration. Yuan Zhang, Chao Wang, Juntong Qi, Yan Peng, Cyborg and Bionic Systems. 52023</p>
<p>Small language models need strong verifiers to self-correct reasoning. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2404.171402024aarXiv preprint</p>
<p>A survey on the memory mechanism of large language model based agents. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, ArXiv, abs/2404.135012024b</p>
<p>ReactIE: Enhancing chemical reaction extraction with weak supervision. Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yizhu Jiao, Xuan Wang, Jiawei Han, 10.18653/v1/2023.findings-acl.767Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, 10.1609/aaai.v38i17.29946Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence19724-19731, Mar. 2024a38</p>
<p>Actionie: Action extraction from scientific literature with programming languages. Xianrui Zhong, Yufeng Du, Siru Ouyang, Ming Zhong, Tingfeng Luo, Qirong Ho, Hao Peng, Heng Ji, Jiawei Han, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024b1</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Symbolic learning enables self-evolving agents. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, arXiv:2406.185322024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
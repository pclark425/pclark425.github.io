<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7404 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7404</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7404</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-269982125</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.13769v1.pdf" target="_blank">Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning, and deep understanding. Meanwhile, Large Language Models (LLMs) now achieve state-of-the-art performance on many NLP tasks. In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7404.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7404.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eval-Prompt 1 (Simple rating)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eval-Prompt 1: Simple rating (ask model to rate 1-5 on a criterion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompt format used in the paper: provide story-prompt and story, ask the LLM to produce a single Likert rating (1-5) on one of six story criteria without asking for explanation or additional guidance. Used as the reference format for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Beluga-13B, Llama-13B, Mistral-7B, ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large language models; Beluga-13B and Llama-13B are Llama-2-derived 13B models (Beluga = Llama2 fine-tuned on StabilityAI internal Orca-style data; Llama-13B = Llama-2 13B chat), Mistral-7B is a 7B model fine-tuned on OpenOrca traces, ChatGPT is OpenAI's gpt-3.5-turbo closed model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B; ChatGPT: unspecified (gpt-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Automatic Story Evaluation (ASE) on HANNA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Rate generated short stories (500-1000 words) against a given criterion (Relevance, Coherence, Empathy, Surprise, Engagement, Complexity) using a Likert scale (1-5).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language rating prompt asking for a single 1-5 Likert score (zero-shot), given story-prompt and generated story.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot single-turn natural language prompt, no examples; models queried 3 times per story (to estimate consistency); ratings extracted via regex. Decoding: Llama-family used (temperature=1, top_p=0.95); ChatGPT used (temperature=0.7, top_p=1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Overall Kendall correlation with averaged human ratings; system-level Kendall correlation; ICC2k for self-consistency; mean Likert rating</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Beluga-13B: average overall Kendall ≈ 0.25; system-level Kendall ≈ 0.70 (human baseline ≈ 0.73); ICC2k very high across criteria (e.g., Relevance ICC≈0.88–0.93 across criteria in Tab.2); mean Likert (Eval-Prompt1) Beluga ≈ 3.48±0.04. Llama-13B overall ≈0.16; Mistral-7B ≈0.20; ChatGPT ≈0.18.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>HANNA dataset (1,056 stories), 6 criteria, 4 Eval-Prompts; each Eval-Prompt run 3 times per story per model; ratings parsed automatically. See decoding settings above.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>For comparisons vs non-LLM automatic measures, Beluga-13B's improvements in overall correlation have strong statistical evidence (BH-adjusted Williams test p < 0.01 for many tests). System-level significance weaker (p > 0.11 for those comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7404.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7404.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eval-Prompt 2 (Ask for explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eval-Prompt 2: Rating with explanation (same as EP1 but ask model to explain its answer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal modification of the baseline prompt that requests the model to provide an explanation for its numeric rating. The paper studies whether asking for an explanation affects rating consistency, distribution, or correlation with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Beluga-13B, Llama-13B, Mistral-7B, ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same models as used for Eval-Prompt 1; described above.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B; ChatGPT: unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Automatic Story Evaluation (ASE) on HANNA (rating + explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide a numeric rating (1-5) on a criterion and also output a free-text explanation of the rating.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt requesting both a Likert score and a textual explanation (zero-shot), otherwise same as Eval-Prompt 1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot, single-turn; explanation requested but no additional guidelines or references; 3 runs per story; same decoding settings as Eval-Prompt1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Change in overall and system-level Kendall correlations with human ratings; changes in mean Likert ratings and ICC2k consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Effect described as negligible: Eval-Prompt 2 overall correlations are very close to Eval-Prompt 1 for all models; ICC self-consistency remains very high. Example mean Likert (Beluga): EP1 3.48±0.04 vs EP2 3.38±0.03 (small decrease).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline = Eval-Prompt 1 (simple rating); Beluga EP1 mean 3.48±0.04, EP2 mean 3.38±0.03.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Small/negligible change vs baseline (EP2 ≈ EP1); Beluga mean rating change −0.10 absolute; correlations nearly unchanged (no meaningful delta reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same dataset/settings as EP1; explanations collected and analyzed for semantic relevance and for a user study; ICC2k reported per Eval-Prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7404.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7404.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eval-Prompt 3 (Guidelines provided)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eval-Prompt 3: Rating with explanation and detailed annotation guidelines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more complex prompt that provides the model with the human annotation guidelines from Chhun et al. (2022) for the considered criterion and asks for a rating and explanation; intended to see whether giving explicit scoring rules helps LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Beluga-13B, Llama-13B, Mistral-7B, ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLMs as above (Beluga-13B and Llama-13B: Llama-2-derived 13B models; Mistral-7B: 7B OpenOrca-fine-tuned; ChatGPT: closed gpt-3.5-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B; ChatGPT: unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Automatic Story Evaluation (ASE) on HANNA (rating + explanation + guidelines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Ask models to rate stories with a requested explanation while providing detailed human annotation guidelines to follow (e.g., five guideline bullets for Surprise criterion).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt with appended structured guidelines (multi-sentence numbered rules) plus request for explanation; zero-shot; no few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Guidelines correspond to original annotation protocol (Chhun et al., 2022) and are injected in the Eval-Prompt text. Models run 3 times per story. Decoding: same as other Eval-Prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Overall and system-level Kendall correlations with human ratings; mean Likert rating; ICC2k self-consistency; user-study error annotations on explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Providing guidelines (EP3) tended to decrease correlations with human ratings for all models (authors: 'Eval-Prompt 3 tends to decrease correlations for all models'); Beluga mean Likert dropped from 3.48±0.04 (EP1) to 3.06±0.03 (EP3), an absolute −0.42 decrease. Llama-13B also decreased (EP1 3.48 → EP3 3.21). Mistral showed smaller change. ICC2k generally decreased slightly under EP3 except Complexity. User study found higher rates of 'Unsubstantiated Claims' and that 40% of EP3 ratings lacked explanations despite the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline = Eval-Prompt 1 (Beluga mean 3.48±0.04; correlations as reported for EP1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Beluga mean rating: −0.42 absolute (EP3 vs EP1). General effect on correlations: qualitative decrease across models (no single numeric delta reported in text for correlation drop per model).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>HANNA dataset, 1,056 stories, 3 runs per prompt per story; user study sampled 100 EP3 explanations and had 3 human annotators label error types; BH-adjusted Williams tests used elsewhere but EP3 prompt-effect significance not universally reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report that providing guidelines 'makes the model less accurate, counter-intuitively'; specific p-values for EP3 effects on correlations are not listed in-text for each comparison (statistical evidence described as weaker at system-level).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7404.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7404.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eval-Prompt 4 (Human story reference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eval-Prompt 4: Rating with explanation and provision of the human reference story</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt variant that, in addition to asking for rating and explanation, provides the human-written story corresponding to the same story-prompt as a reference (explicitly labelled 'for reference only') to see whether this reference affects LLM ratings and correlations with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Beluga-13B, Llama-13B, Mistral-7B, ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same models as above; ChatGPT was only asked to rate the original HANNA stories (authors note this caveat).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B; ChatGPT: unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Automatic Story Evaluation (ASE) on HANNA (rating + explanation + human reference)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide the model with the generated story to rate and also the human-written reference story for the same prompt, then ask for rating and explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt with an additional reference document (human story) included inline; zero-shot; no few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality (reference document)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Human story included explicitly 'only for reference'; otherwise similar to EP2; 3 runs per story per model; same decoding settings. ChatGPT was only applied to the original HANNA stories (no Llama-generated stories were rated by ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Overall and system-level Kendall correlations with human ratings; mean Likert ratings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Effect described as mixed: EP4 'has a similar effect' to EP3 on overall correlations (tends to decrease them) but 'seems to cause a small increase in system-level correlations' for some models. Beluga mean rating: EP4 3.28±0.04 (vs EP1 3.48), i.e., −0.20 absolute. Llama-13B EP4 2.82 (down more). Mistral-7B EP4 3.28 (small decrease).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline = Eval-Prompt 1 (EP1 means listed above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Beluga mean rating −0.20 absolute (EP4 vs EP1). Overall correlations: EP4 generally decreased overall correlations like EP3; system-level correlations sometimes increased slightly (model dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>HANNA dataset, 1,056 stories, 3 runs per story; EP4 provided the human story as reference; ChatGPT evaluated only original HANNA (caveat).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7404.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7404.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt complexity vs consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Eval-Prompt complexity on LLM self-consistency (ICC2k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study finding that LLM intra-model consistency (as measured by ICC2k across three random generations) is high and only marginally affected by changes in Eval-Prompt complexity (explanation/guidelines/reference).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Beluga-13B, Llama-13B, Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See above; Llama-derived models and Mistral; ChatGPT excluded from some ICC comparisons due to different evaluation scope.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Automatic Story Evaluation (ASE) — consistency estimation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure how consistent a model's numeric ratings are when the same Eval-Prompt is queried multiple times with sampling variability (top-p sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Repeated zero-shot natural-language Eval-Prompts (EP1–EP4), 3 independent generations per story; ICC2k computed over the three ratings per story.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / experimental sampling</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Three runs per (story, Eval-Prompt) to approximate multiple raters; ICC2k used to quantify agreement for average random raters; temperature/top_p as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ICC2k (intra-class correlation coefficient for average random raters) with 95% confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ICC2k values 'very high' across criteria for LLMs. Example Tab.2 aggregated ICC2k for Beluga-13B per criterion (EP1): Relevance 0.88±0.01, Coherence 0.93±0.01, Empathy 0.88±0.01, Surprise 0.80±0.02, Engagement 0.91±0.01, Complexity 0.85±0.01. Influence of Eval-Prompt: limited; providing guidelines (EP3) tends to decrease self-consistency for most criteria except Complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Provision of guidelines (EP3) shows modest decreases in ICC2k vs EP1 for most criteria (except Complexity where effect is discernible), but ICC remains high overall.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same decoding settings; ICC2k computed interpreting three LLM runs as three annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Confidence intervals reported for ICC2k values; decreases noted where CIs show discernible effect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Of human criteria and automatic metrics: A benchmark of the evaluation of story generation <em>(Rating: 2)</em></li>
                <li>Prompt programming for large language models: Beyond the few-shot paradigm <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>Is GPT-3 a good data annotator? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7404",
    "paper_id": "paper-269982125",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Eval-Prompt 1 (Simple rating)",
            "name_full": "Eval-Prompt 1: Simple rating (ask model to rate 1-5 on a criterion)",
            "brief_description": "Baseline prompt format used in the paper: provide story-prompt and story, ask the LLM to produce a single Likert rating (1-5) on one of six story criteria without asking for explanation or additional guidance. Used as the reference format for comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Beluga-13B, Llama-13B, Mistral-7B, ChatGPT (gpt-3.5-turbo)",
            "model_description": "Autoregressive large language models; Beluga-13B and Llama-13B are Llama-2-derived 13B models (Beluga = Llama2 fine-tuned on StabilityAI internal Orca-style data; Llama-13B = Llama-2 13B chat), Mistral-7B is a 7B model fine-tuned on OpenOrca traces, ChatGPT is OpenAI's gpt-3.5-turbo closed model.",
            "model_size": "Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B; ChatGPT: unspecified (gpt-3.5 family)",
            "task_name": "Automatic Story Evaluation (ASE) on HANNA",
            "task_description": "Rate generated short stories (500-1000 words) against a given criterion (Relevance, Coherence, Empathy, Surprise, Engagement, Complexity) using a Likert scale (1-5).",
            "problem_format": "Natural-language rating prompt asking for a single 1-5 Likert score (zero-shot), given story-prompt and generated story.",
            "format_category": "prompt style",
            "format_details": "Zero-shot single-turn natural language prompt, no examples; models queried 3 times per story (to estimate consistency); ratings extracted via regex. Decoding: Llama-family used (temperature=1, top_p=0.95); ChatGPT used (temperature=0.7, top_p=1).",
            "performance_metric": "Overall Kendall correlation with averaged human ratings; system-level Kendall correlation; ICC2k for self-consistency; mean Likert rating",
            "performance_value": "Beluga-13B: average overall Kendall ≈ 0.25; system-level Kendall ≈ 0.70 (human baseline ≈ 0.73); ICC2k very high across criteria (e.g., Relevance ICC≈0.88–0.93 across criteria in Tab.2); mean Likert (Eval-Prompt1) Beluga ≈ 3.48±0.04. Llama-13B overall ≈0.16; Mistral-7B ≈0.20; ChatGPT ≈0.18.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "HANNA dataset (1,056 stories), 6 criteria, 4 Eval-Prompts; each Eval-Prompt run 3 times per story per model; ratings parsed automatically. See decoding settings above.",
            "statistical_significance": "For comparisons vs non-LLM automatic measures, Beluga-13B's improvements in overall correlation have strong statistical evidence (BH-adjusted Williams test p &lt; 0.01 for many tests). System-level significance weaker (p &gt; 0.11 for those comparisons).",
            "uuid": "e7404.0",
            "source_info": {
                "paper_title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Eval-Prompt 2 (Ask for explanation)",
            "name_full": "Eval-Prompt 2: Rating with explanation (same as EP1 but ask model to explain its answer)",
            "brief_description": "A minimal modification of the baseline prompt that requests the model to provide an explanation for its numeric rating. The paper studies whether asking for an explanation affects rating consistency, distribution, or correlation with humans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Beluga-13B, Llama-13B, Mistral-7B, ChatGPT",
            "model_description": "Same models as used for Eval-Prompt 1; described above.",
            "model_size": "Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B; ChatGPT: unspecified",
            "task_name": "Automatic Story Evaluation (ASE) on HANNA (rating + explanation)",
            "task_description": "Provide a numeric rating (1-5) on a criterion and also output a free-text explanation of the rating.",
            "problem_format": "Natural-language prompt requesting both a Likert score and a textual explanation (zero-shot), otherwise same as Eval-Prompt 1.",
            "format_category": "prompt style",
            "format_details": "Zero-shot, single-turn; explanation requested but no additional guidelines or references; 3 runs per story; same decoding settings as Eval-Prompt1.",
            "performance_metric": "Change in overall and system-level Kendall correlations with human ratings; changes in mean Likert ratings and ICC2k consistency",
            "performance_value": "Effect described as negligible: Eval-Prompt 2 overall correlations are very close to Eval-Prompt 1 for all models; ICC self-consistency remains very high. Example mean Likert (Beluga): EP1 3.48±0.04 vs EP2 3.38±0.03 (small decrease).",
            "baseline_performance": "Baseline = Eval-Prompt 1 (simple rating); Beluga EP1 mean 3.48±0.04, EP2 mean 3.38±0.03.",
            "performance_change": "Small/negligible change vs baseline (EP2 ≈ EP1); Beluga mean rating change −0.10 absolute; correlations nearly unchanged (no meaningful delta reported).",
            "experimental_setting": "Same dataset/settings as EP1; explanations collected and analyzed for semantic relevance and for a user study; ICC2k reported per Eval-Prompt.",
            "statistical_significance": null,
            "uuid": "e7404.1",
            "source_info": {
                "paper_title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Eval-Prompt 3 (Guidelines provided)",
            "name_full": "Eval-Prompt 3: Rating with explanation and detailed annotation guidelines",
            "brief_description": "A more complex prompt that provides the model with the human annotation guidelines from Chhun et al. (2022) for the considered criterion and asks for a rating and explanation; intended to see whether giving explicit scoring rules helps LLM evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Beluga-13B, Llama-13B, Mistral-7B, ChatGPT",
            "model_description": "Same LLMs as above (Beluga-13B and Llama-13B: Llama-2-derived 13B models; Mistral-7B: 7B OpenOrca-fine-tuned; ChatGPT: closed gpt-3.5-turbo).",
            "model_size": "Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B; ChatGPT: unspecified",
            "task_name": "Automatic Story Evaluation (ASE) on HANNA (rating + explanation + guidelines)",
            "task_description": "Ask models to rate stories with a requested explanation while providing detailed human annotation guidelines to follow (e.g., five guideline bullets for Surprise criterion).",
            "problem_format": "Natural-language prompt with appended structured guidelines (multi-sentence numbered rules) plus request for explanation; zero-shot; no few-shot exemplars.",
            "format_category": "prompt style",
            "format_details": "Guidelines correspond to original annotation protocol (Chhun et al., 2022) and are injected in the Eval-Prompt text. Models run 3 times per story. Decoding: same as other Eval-Prompts.",
            "performance_metric": "Overall and system-level Kendall correlations with human ratings; mean Likert rating; ICC2k self-consistency; user-study error annotations on explanations.",
            "performance_value": "Providing guidelines (EP3) tended to decrease correlations with human ratings for all models (authors: 'Eval-Prompt 3 tends to decrease correlations for all models'); Beluga mean Likert dropped from 3.48±0.04 (EP1) to 3.06±0.03 (EP3), an absolute −0.42 decrease. Llama-13B also decreased (EP1 3.48 → EP3 3.21). Mistral showed smaller change. ICC2k generally decreased slightly under EP3 except Complexity. User study found higher rates of 'Unsubstantiated Claims' and that 40% of EP3 ratings lacked explanations despite the prompt.",
            "baseline_performance": "Baseline = Eval-Prompt 1 (Beluga mean 3.48±0.04; correlations as reported for EP1).",
            "performance_change": "Beluga mean rating: −0.42 absolute (EP3 vs EP1). General effect on correlations: qualitative decrease across models (no single numeric delta reported in text for correlation drop per model).",
            "experimental_setting": "HANNA dataset, 1,056 stories, 3 runs per prompt per story; user study sampled 100 EP3 explanations and had 3 human annotators label error types; BH-adjusted Williams tests used elsewhere but EP3 prompt-effect significance not universally reported.",
            "statistical_significance": "Authors report that providing guidelines 'makes the model less accurate, counter-intuitively'; specific p-values for EP3 effects on correlations are not listed in-text for each comparison (statistical evidence described as weaker at system-level).",
            "uuid": "e7404.2",
            "source_info": {
                "paper_title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Eval-Prompt 4 (Human story reference)",
            "name_full": "Eval-Prompt 4: Rating with explanation and provision of the human reference story",
            "brief_description": "Prompt variant that, in addition to asking for rating and explanation, provides the human-written story corresponding to the same story-prompt as a reference (explicitly labelled 'for reference only') to see whether this reference affects LLM ratings and correlations with humans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Beluga-13B, Llama-13B, Mistral-7B, ChatGPT",
            "model_description": "Same models as above; ChatGPT was only asked to rate the original HANNA stories (authors note this caveat).",
            "model_size": "Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B; ChatGPT: unspecified",
            "task_name": "Automatic Story Evaluation (ASE) on HANNA (rating + explanation + human reference)",
            "task_description": "Provide the model with the generated story to rate and also the human-written reference story for the same prompt, then ask for rating and explanation.",
            "problem_format": "Natural-language prompt with an additional reference document (human story) included inline; zero-shot; no few-shot examples.",
            "format_category": "prompt style / input modality (reference document)",
            "format_details": "Human story included explicitly 'only for reference'; otherwise similar to EP2; 3 runs per story per model; same decoding settings. ChatGPT was only applied to the original HANNA stories (no Llama-generated stories were rated by ChatGPT).",
            "performance_metric": "Overall and system-level Kendall correlations with human ratings; mean Likert ratings",
            "performance_value": "Effect described as mixed: EP4 'has a similar effect' to EP3 on overall correlations (tends to decrease them) but 'seems to cause a small increase in system-level correlations' for some models. Beluga mean rating: EP4 3.28±0.04 (vs EP1 3.48), i.e., −0.20 absolute. Llama-13B EP4 2.82 (down more). Mistral-7B EP4 3.28 (small decrease).",
            "baseline_performance": "Baseline = Eval-Prompt 1 (EP1 means listed above).",
            "performance_change": "Beluga mean rating −0.20 absolute (EP4 vs EP1). Overall correlations: EP4 generally decreased overall correlations like EP3; system-level correlations sometimes increased slightly (model dependent).",
            "experimental_setting": "HANNA dataset, 1,056 stories, 3 runs per story; EP4 provided the human story as reference; ChatGPT evaluated only original HANNA (caveat).",
            "statistical_significance": null,
            "uuid": "e7404.3",
            "source_info": {
                "paper_title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Prompt complexity vs consistency",
            "name_full": "Effect of Eval-Prompt complexity on LLM self-consistency (ICC2k)",
            "brief_description": "Study finding that LLM intra-model consistency (as measured by ICC2k across three random generations) is high and only marginally affected by changes in Eval-Prompt complexity (explanation/guidelines/reference).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Beluga-13B, Llama-13B, Mistral-7B",
            "model_description": "See above; Llama-derived models and Mistral; ChatGPT excluded from some ICC comparisons due to different evaluation scope.",
            "model_size": "Beluga: 13B; Llama-13B: 13B; Mistral-7B: 7B",
            "task_name": "Automatic Story Evaluation (ASE) — consistency estimation",
            "task_description": "Measure how consistent a model's numeric ratings are when the same Eval-Prompt is queried multiple times with sampling variability (top-p sampling).",
            "problem_format": "Repeated zero-shot natural-language Eval-Prompts (EP1–EP4), 3 independent generations per story; ICC2k computed over the three ratings per story.",
            "format_category": "prompt style / experimental sampling",
            "format_details": "Three runs per (story, Eval-Prompt) to approximate multiple raters; ICC2k used to quantify agreement for average random raters; temperature/top_p as above.",
            "performance_metric": "ICC2k (intra-class correlation coefficient for average random raters) with 95% confidence intervals",
            "performance_value": "ICC2k values 'very high' across criteria for LLMs. Example Tab.2 aggregated ICC2k for Beluga-13B per criterion (EP1): Relevance 0.88±0.01, Coherence 0.93±0.01, Empathy 0.88±0.01, Surprise 0.80±0.02, Engagement 0.91±0.01, Complexity 0.85±0.01. Influence of Eval-Prompt: limited; providing guidelines (EP3) tends to decrease self-consistency for most criteria except Complexity.",
            "baseline_performance": null,
            "performance_change": "Provision of guidelines (EP3) shows modest decreases in ICC2k vs EP1 for most criteria (except Complexity where effect is discernible), but ICC remains high overall.",
            "experimental_setting": "Same decoding settings; ICC2k computed interpreting three LLM runs as three annotators.",
            "statistical_significance": "Confidence intervals reported for ICC2k values; decreases noted where CIs show discernible effect.",
            "uuid": "e7404.4",
            "source_info": {
                "paper_title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Of human criteria and automatic metrics: A benchmark of the evaluation of story generation",
            "rating": 2,
            "sanitized_title": "of_human_criteria_and_automatic_metrics_a_benchmark_of_the_evaluation_of_story_generation"
        },
        {
            "paper_title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "rating": 2,
            "sanitized_title": "prompt_programming_for_large_language_models_beyond_the_fewshot_paradigm"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        },
        {
            "paper_title": "Is GPT-3 a good data annotator?",
            "rating": 1,
            "sanitized_title": "is_gpt3_a_good_data_annotator"
        }
    ],
    "cost": 0.015994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation</p>
<p>Cyril Chhun cyril.chhun@telecom-paris.fr 
LTCI
Télécom Paris</p>
<p>Institut Polytechnique de Paris</p>
<p>Fabian M Suchanek fabian.suchanek@telecom-paris.fr 
LTCI
Télécom Paris</p>
<p>Institut Polytechnique de Paris</p>
<p>Chloé Clavel chloe.clavel@inria.fr 
ALMAnaCH
INRIA Paris</p>
<p>Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation
86491F5B2E1F185EAC9901765B2FF50ERE storypromptroughly matchestargetweak relationshipconnectionweak CH storycoherencemake sensedifficult to understandclear narrative structure EM empathyemotionsunderstand the charactersdepthemotional connection SU storysurpriseendingpredictablerateunexpectedtwistcompletely obvious EG storymildly interestingengagementdifficultfoundcharactersfully engage CX storycharactersintricate plotdifficult to understandstraightforwarddepth Tab. 4: Selected keywords from Beluga-13B explanations w.r.t. a specific criterion
Storytelling is an integral part of human experience and plays a crucial role in social interactions.Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning and deep understanding.Meanwhile, Large Language Models (LLM) now achieve stateof-the-art performance on many NLP tasks.In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE.We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour.Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers.Relevance Coherence EmpathySurprise Engagement ComplexityFig. 9: UMAP projection of Beluga-13B explanations.Keyword AnalysisSince Beluga's explanations seem to vary from one criterion to another, we evaluate whether they make sense from a semantic point of view.We use the YAKE! keyword extractor, which significantly outperforms other state-of-the-art methods (Campos et al., 2020): we show selected 3-gram keywords from the top-30 per criterion on Tab. 4. The results are consistent with Fig.9: keywords are overall different for each criterion.We can also see here that they are semantically relevant.</p>
<p>Introduction</p>
<p>The task of Automatic Story Generation (ASG) (Li et al., 2013) consists in the creation of a narrative from a short sentence.Previous research showed that storytelling enables a narrator to communicate honestly with their audience (Rowcliffe, 2004) and to provide listeners with an engaging and instructive experience (Miller and Pennycuff, 2008).Indeed, the process of story creation is a salient testimony of human creativity, requiring both the discovery of interesting ideas and their adept expression through a carefully-built narrative.Strong automatic story generating systems could therefore be useful for a variety of applications, such as gaming (Turner, 2014), education (Lombardo and Damiano, 2012), mental health (George et al., 2014) and marketing (Júnior et al., 2023).</p>
<p>Meanwhile, over the last few years, advances in natural language processing (NLP) have been spearheaded by the development of large language models (LLM) such as GPT-3 (Brown et al., 2020), LaMDA (Thoppilan et al., 2022), PaLM (Chowdhery et al., 2023) and LLaMA (Touvron et al., 2023a).Upon release, these models have been setting new state-of-the-art performance standards for a wide array of NLP tasks, e.g.question answering, summarization, and translation.In particular, for ASG, LLMs are now able to produce convincing stories, so much so that they can be hard to distinguish from human stories (Clark et al., 2021).As their performance improves, they may become valuable assistants to our creative process; already, writing contests have been shown to encourage their use (Edilivre, 2023).</p>
<p>The increased availability of LLMs to the general public underlines the need for reliable story evaluation methods that can be used to improve both the performance of ASG models and our understanding of their strengths and weaknesses.Since the human annotation of stories is costly and time-consuming (Celikyilmaz et al., 2020), Automatic Story Evaluation (ASE) systems could provide an efficient and scalable replacement for human evaluation.However, current automatic measures have been shown to be poorly correlated with human judgment for ASG (Chhun et al., 2022).</p>
<p>In this paper, we investigate whether LLMs themselves can be used as substitutes for human annotators for story evaluation.To that end, we perform several annotation experiments where we ask different LLMs to rate stories according to different criteria and to explain their rating.We show an example in Fig. 1 and a schema of our experiments in Fig. 2.</p>
<p>Our contributions are the following:</p>
<p>1.A comparison between LLMs and current ASE measures.We compute and analyze the correlations between LLM ratings with human anno- tations on criteria specific to story evaluation; we find that, while overall correlations are moderate to weak, system-level correlations are very high, suggesting that LLMs can produce reliable model rankings for ASE;</p>
<ol>
<li>
<p>An analysis of the influence of prompt engineering on LLM performance.We examine the effects of using different Eval-Prompts on the consistency and distribution of LLM ratings.We find that adding detailed guidelines does not necessarily improve performance and that LLMs are remarkably self-consistent;</p>
</li>
<li>
<p>Insights on LLM explainability for ASE.</p>
</li>
</ol>
<p>We analyze the explanations provided by LLMs through different methods, including a user study, and find that LLMs' understanding of the ASE task is perfectible.Most notably, they struggle at explaining their ratings with substantiated claims;</p>
<ol>
<li>An analysis of LLM performance in ASG.</li>
</ol>
<p>The high system-level correlations of LLMs with human ratings enable us to use them to rate other LLMs for ASG.We find that LLMs perform at least as well as humans for the generation of short stories, and that their performance may be explained by their tendency to produce output that is similar to their pretraining data.</p>
<p>Our methodology can be found in Sec.3.1.We release our data and code on GitHub 1 .Our data 1 https://github.com/dig-team/hanna-benchmark-asg consists of:</p>
<p>• ASE experiments: ∼150k rating and explanation annotations (1,056 stories, 6 criteria, 4 Eval-Prompts, 3 tries, 2 models); • User study: 1,500 human annotations of LLM explanations; • ASG experiment: 384 stories generated by Llama models with corresponding LLM annotations to expand the HANNA dataset of Chhun et al. (2022).This paper is structured as follows: in Sec. 2, we review the related work.In Sec. 3, we lay out our methodology and experimental details.In Sec. 4, we perform our analysis of the results.In Sec. 5, we discuss the state of LLMs in ASG and ASE.Finally, in Sec.6, we conclude with practical takeaways for researchers, the limitations of our work, and future research directions.</p>
<p>Related work</p>
<p>Human Evaluation</p>
<p>Evaluating stories is a difficult task (McCabe and Peterson, 1984;Dickman, 2003).In the social sciences literature, multiple criteria have been suggested, often divided into cognitive and emotional factors (Bae et al., 2021).However, the consensus around the criteria to be used in the NLP literature is still weak (Fan et al., 2018;Guan et al., 2020;Rashkin et al., 2020;Goldfarb-Tarrant et al., 2020).Chhun et al. (2022) distill the indicators used in the social sciences literature into 6 criteria (Relevance, Coherence, Empathy, Surprise, Engagement, Complexity), which we will use in our paper as well.</p>
<p>While human evaluation remains the gold standard of evaluation, it is costly and time-consuming.We therefore need to develop automatic measures that can act as substitutes for human judgment, ideally for each of the criteria.Such automatic measures could be used to improve language models, e.g. as a loss function or for chain-of-thought prompting (Wei et al., 2022b).</p>
<p>Automatic Evaluation</p>
<p>Automatic measures (e.g.BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), BERTScore (Zhang et al., 2020), BARTScore (Yuan et al., 2021)) have been repeatedly shown to correlate moderately to poorly with human judgment, especially when applied to tasks other than the one they were designed for (Zhang et al., 2004;Novikova et al., 2017;Colombo et al., 2023).Deutsch et al. (2022)  put forth the particular limitations of reference-free measures.For ASE, Guan et al. (2020) and Chhun et al. (2022) also observe weak correlations between automatic and human ratings, whether they be reference-based or reference-free.This highlights the need for better automatic evaluation methods.To tackle this issue, this paper investigates the use of LLMs to annotate stories with ratings w.r.t. a given criterion.</p>
<p>Automatic Annotation</p>
<p>LLMs are increasingly being tested for automatic text annotation, e.g. for sentiment analysis (Qureshi et al., 2022), named entity recognition (Enkhsaikhan et al., 2021) or event structure modeling (Vauth et al., 2021).Wang et al. (2021) demonstrate that labeling performed by GPT-3 can achieve the same performance as human labeling and be up to 96% more cost-efficient.Ding et al. (2023) show that GPT-3 performs well for text classification tasks, but struggles with more complex tasks such as named entity recognition.Chakrabarty et al. (2023) design a test for creativity and show that LLM-generated stories pass fewer tests than human stories, and that using LLMs for ASE yields no positive correlations.</p>
<p>We seek to generalize their findings through the use of source-available models and a finer analysis and discussion of LLM performance.</p>
<p>Prompt Engineering</p>
<p>The importance of designing efficient prompts for large language models such as GPT-3 has been ex-tensively investigated in recent years.Reynolds and McDonell (2021) notably find that zeroshot prompting can perform similarly to few-shot prompting, and even exceed it.They explore the design of metaprompts that prime the language model to better solve a given problem.Zhou et al. (2023b) treat the prompt engineering process as an optimization problem, use search algorithms guided by LLMs to solve it and attain human-level performance.Wei et al. (2022a) and White et al. (2023) review different strategies that have been applied to augment large language model abilities, e.g.least-to-most prompting (Zhou et al., 2023a), ask-me-anything prompting (Arora et al., 2023), and zero-shot chain-of-thought reasoning (Kojima et al., 2022).</p>
<p>We choose to investigate whether LLMs perform better with simple or detailed guidelines, and with zero-or one-shot Eval-Prompts.</p>
<p>3 Meta-Evaluation of LLMs for ASE</p>
<p>Methodology for ASE</p>
<p>The ASG task commonly involves the generation of a story from a short sentence called a prompt (Alabdulkarim et al., 2021), which we will henceforth call story-prompt.ASE Definition.Given an evaluation measure m (e.g. a scoring algorithm, an LLM. . .), a storyprompt i, and a story y i , we define the ASE task as the production of an evaluation score m(y i ).</p>
<p>In this paper, we choose to use LLMs as ASE measures.We will refer to the prompt that is fed to</p>
<p>Guidelines:</p>
<p>1 -The ending seemed completely obvious from the start, or doesn't make any sense at all. 2 -The ending was easily predictable after a few sentences.3 -The ending was predictable after half of the story.4 -The ending surprised you, but would have been difficult to predict. 5 -The ending surprised you, and still seemed as if it could very reasonably have been predicted, ie, there were enough clues in the story.</p>
<p>Rate the story on a scale from 1 to 5 on Surprise (how surprising the end of the story was) and explain your answer.Use the provided guidelines.Rating: Eval-Prompt 4</p>
<p>Prompt:</p>
<p>You have become death, destroyer of worlds.Rate the target story on a scale from 1 to 5 on Surprise (how surprising the end of the story was) and explain your answer.Do not rate the human story; it is here only for reference.Rating: Fig. 3: Example Eval-Prompts for the Surprise criterion.Eval-Prompt 2 is the same as Eval-Prompt 1 with "explain your answer" added at the end."Prompt" (bold) refers to the story-prompt.</p>
<p>the LLM as the Eval-Prompt, to distinguish it from the story-prompt.See Fig. 1 for an example of the use of an LLM for story evaluation.ASE Criteria.We use the criteria introduced by Chhun et al. (2022), who designed HANNA, a benchmark for story evaluation.They compiled a set of six orthogonal criteria from the social sciences literature:</p>
<ol>
<li>Relevance (RE, how well the story matches its prompt), 2. Coherence (CH, how much the story makes sense), 3. Empathy (EM, how well the reader understood the character's emotions), 4. Surprise (SU, how surprising the end of the story was), 5. Engagement (EG, how much the reader engaged with the story), 6. Complexity (CX, how elaborate the story is).</li>
</ol>
<p>Methodology.Given the importance of good prompt engineering (Zhao et al., 2021), we design four different Eval-Prompts for the generation of ratings.For each of our Eval-Prompts, we provide the model with a story-prompt and a corresponding story.Then:</p>
<p>Eval-Prompt 1 (simple rating): we ask the model to rate the story on a scale from 1 to 5 on one of the six criteria;</p>
<p>Eval-Prompt 2 (rating with explanation): same as Eval-Prompt 1, and we ask the model to explain its answer;</p>
<p>Eval-Prompt 3 (rating with explanation and guidelines): same as Eval-Prompt 2, and we provide the model with the detailed guidelines from the original annotation protocol by Chhun et al. (2022);</p>
<p>Eval-Prompt 4 (rating with explanation and human story): same as Eval-Prompt 2, and we provide the model with the human story associated with the same story-prompt.We explicitly tell the model that the human story is given only for reference purposes.</p>
<p>Different Eval-Prompt examples are shown in Fig. 3.</p>
<p>Meta-Evaluation Measures</p>
<p>Notations.For S systems and N story-prompts, let y j i be the story generated by system j ∈ {1, . . ., S} for story-prompt i ∈ {1, . . ., N }.For a (human or automatic) measure m, we denote by m(y j i ) the score associated to y j i .Let K be a correlation coefficient, e.g.Pearson's r (Pearson, 1895), Spearman's ρ (Spearman, 1961) or Kendall's τ (Kendall, 1938).We note h k the measure provided by the k-th human annotator.</p>
<p>A naive method to compare ratings from two measures would be to compute how much they differ from each other for each story, e.g. by calculating the average L1 distance between a given evaluation method m and the human ratings, i.e.,
1 3 3 k=1 L 1 (m, h k ).
However, this method suffers from the central tendency bias-the tendency of an individual to rate most items on a survey in the middle of a rating scale-which is often observed in Likert scales (Stevens, 1971) and could be explained by the participants' tendency to base their judgment on a least mean squares estimator rather than a maximum a posteriori estimator (Douven, 2018).We therefore choose more robust measures of meta-evaluation: system-level and overall correlations.</p>
<p>System-level correlation (K sys</p>
<p>m 1 ,m 2 ).We take the correlation of the vectors containing the mean score of all stories for each system, for m 1 and m 2 .This strategy measures how much m 1 and m 2 agree when comparing different systems.Formally:
K sys m 1 ,m 2 ≜ K 1 N C sys m 1 , 1 N C sys m 2 ,(1)
where
C sys m ≜ N i=1 m(y 1 i ), . . . , N i=1 m(y S i ) .
The segment-level correlation, often used in conjunction with the system-level one in the metaevaluation literature (Ma et al., 2019;Bhandari et al., 2020), is not adapted to ASE since stories generated from the same story-prompt are not required to be similar, while e.g.translations of a sentence should look alike.We therefore use the overall correlation, which we define below.</p>
<p>Overall Correlation (K m 1 ,m 2 ).We take the correlation between the full vectors containing the scores of m 1 or m 2 for a given story for every system.Formally:
K m 1 ,m 2 ≜ K(C m 1 , C m 2 ),(2)
where C m ≜ m(y j i )</p>
<p>(i,j)∈{1,...,N }×{1,...,S} .</p>
<p>Statistical Testing (Sec.4.1).Correlations between two automatic measures on the same annotated dataset are not independent.As advised by Graham and Baldwin (2014), we use the Williams test (Williams, 1959;Moon, 2019) to evaluate the strength of an increase in dependent correlations (Steiger, 1980).</p>
<p>Given three features X 1 , X 2 and X 3 of a population of size n, Williams's t test for whether the correlation between X 1 and X 2 equals the correlation between X 1 and X 3 is formulated as follows:
t = (r 12 − r 13 ) (n − 1)(1 + r 23 ) 2K (n−1) (n−3) + (r 12 +r 13 ) 2 4 (1 − r 23 ) 3
, where r ij is the correlation between X i and X j and
K = 1 − r 12 2 − r 13 2 − r 23 2 + 2 r 12 r 13 r 23 .
Williams's t statistic follows a Student's tdistribution with n − 3 degrees of freedom.In particular, the Williams test takes the correlations between X 2 and X 3 into account.Furthermore, since we perform a large quantity of tests, we choose to correct p-values for multiplicity.As advised by Jafari and Ansari-Pour (2019), we control the false discovery rate using the Benjamini-Hochberg (BH) method (Benjamini and Hochberg, 1995).Given n p-values p 1 , . . ., p n sorted in increasing order, the BH method consists in computing adjusted p-values p ⋆ k = p k m k and replacing the p-values from largest to smallest.</p>
<p>Following recent recommendations to move beyond simplistic "statistical significance" tests (Amrhein et al., 2019;Wasserstein et al., 2019;Mc-Shane et al., 2019), we report all p-values for transparency.We choose to use a gradual notion of evidence for our statistical analysis, as suggested by Muff et al. (2022).</p>
<p>Human Evaluation of ASE Explanations</p>
<p>We conduct a user study in which we ask human raters to identify potential issues in LLM explanations.Dou et al. (2022) introduced an error annotation schema called SCARECROW that we adapted for ASE.We manually reviewed a random sample of 20 explanations from Beluga-13B on Eval-Prompt 3 and selected the most relevant error types.Then, we randomly sampled another 100 explanations and, for each explanation, we asked 3 human workers to annotate it w.r.t. the following five error categories:</p>
<ol>
<li>Poor Syntax: parts of the explanation are grammatically incorrect or wrongly-worded; 2. Incoherence: parts of the explanation are selfcontradictory, logically wrong, or simply do not make sense and do not fit the other categories; 3. Wrong Guideline: the explanation does not respect the provided guidelines; 4. Superfluous Text: parts of the explanation contain text that repeats itself or generation artefacts; 5. Unsubstantiated Claims: the explanation fails to make explicit references to the story to substantiate its reasoning.We recruited workers on Amazon Mechanical Turk.We estimated that a HIT would take around one minute, so we set the reward at $0.20 per HIT, so about $12 per hour.To ensure that annotators spoke fluent English, we restricted access to the experiment to the UK, the US, Canada, Australia and New Zealand.</li>
</ol>
<p>Experimental Details</p>
<p>Dataset.We use the HANNA dataset (Chhun et al., 2022) which contains 1,056 stories generated from story-prompts from the WritingPrompts dataset (Fan et al., 2018), with both pretrained language models: BERTGeneration (Rothe et al., 2020), CTRL (Keskar et al., 2019), GPT (Radford et al., 2019), GPT-2 (Radford et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019); and ASG-specific models: Fusion (Fan et al., 2018), HINT (Guan et al., 2021) and TD-VAE (Wilmot and Keller, 2021).These stories were annotated with scores from human raters on the six criteria introduced in Sec.3.1 and 72 automatic measures.We reproduce the original procedure from Chhun et al. (2022): for referencebased evaluation measures (e.g.BLEU), we use the human story from HANNA as the reference for the generated story.Because of space constraints, we display only the evaluation measures that are the most used in the literature: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), chrF (Popović, 2015), BERTScore (Zhang et al., 2020), SUPERT (Gao et al., 2020), BLANC (Vasilyev et al., 2020), BARTScore (Yuan et al., 2021), BaryScore (Colombo et al., 2021).The results are similar for the other automatic measures.</p>
<p>ASG Models.Since the release of the HANNA dataset, language models have made significant advancements.We therefore felt the need to expand HANNA with more recent models.We selected Llama-2-7b-chat-hf (Llama-7B) as a new baseline and 4 high-performing models (at the time of selection) of different sizes on the Hugging-Face Open LLM Leaderboard2 : Platypus2-70Binstruct (Platypus2), Llama-30b-instruct-2048 (Llama-30B), StableBeluga-13B (Beluga-13B), Mistral-7B-OpenOrca (Mistral).ASE Models.We submit each of the four Eval-Prompts 3 times on all 1,056 stories on each of the 6 criteria, and we then extract the ratings automatically from the generated answer via a regular expression.Since story evaluation on multiple prompts and multiple criteria was more computationally demanding, we limited our experiments to the smaller 13B and 7B models.We used the 4 following models: Beluga-13B, Mistral, Llama-2-13b-chat-hf (Llama-13B), and Gpt-3.5-turbo(ChatGPT).We also ran the ASE experiments with Llama-7B, which failed at the task too often for the results to be exploitable, e.g. by generating nonsensical conversations between itself and the user.We use (temperature, top_p) = (1, 0.95) for Llama models and (0.7, 1) for ChatGPT (default suggested values).</p>
<p>Llama2 (Touvron et al., 2023b) models were trained on a closed "new mix of data from publicly available sources".Beluga-13B and Mistral-7B are Llama2 models fine-tuned on Orca-style datasets which contain triplets of "System message-User query-LLM response" for a large collection of tasks (Mukherjee et al., 2023).Beluga-13B is finetuned on StabilityAI's closed internal dataset, while Mistral-7B is fine-tuned on the open OpenOrca dataset (Lian et al., 2023).ChatGPT (Brown et al., 2020;Ouyang et al., 2022) is a closed-source model trained on a closed internal dataset that includes the CommonCrawl, Books1 and Books2 datasets.</p>
<p>We used the transformers library (Wolf et al., 2020) and the OpenAI API for our experiments.</p>
<p>Our work aims at answering five important questions for ASE and ASG:</p>
<p>• ASE1: How do LLMs compare w.r.t.current evaluation methods, both human and automatic?</p>
<p>• ASE2: How does the Eval-Prompt influence the consistency and distribution of LLM ratings?</p>
<p>• ASE3: How explainable is the evaluation performed by LLMs?Tab.1: Intra-class coefficients type 2k for Eval-Prompt 1 ratings with 95% confidence interval.Higher is better.</p>
<p>First, we want to verify if LLMs provide stable answers.The default decoding strategy for LLMs (both Llama models and ChatGPT) is top-p sampling, which involves random variability in the generation process.We evaluate how consistent LLMs are with themselves through an inter-rater reliability (IRR) estimation.For each task, we interpret the three different LLM ratings as coming from three different annotators and we use the intra-class correlation coefficient (ICC), which is the most relevant one for our case study: unlike Cohen's and Fleiss's kappas (Cohen, 1960;Fleiss, 1971) or Krippendorff's alpha (Hayes and Krippendorff, 2007), which quantify IRR based on all-or-nothing agreement, the ICC incorporates the magnitude of the disagreement to compute its IRR estimate, with larger-magnitude disagreements resulting in lower ICC than smaller-magnitude disagreements (Hallgren, 2012).We specifically use the ICC for average random raters (ICC2k) (Vallat, 2018); with the assumption that the random aspect can approximate the random aspect of the generation.</p>
<p>ICC2k values for Eval-Prompt 1 for Beluga-13B, Mistral-7B and human ratings are displayed on Tab. 1. Comparing LLM consistency and human inter-rater agreement values should be done with caution: human raters may have subjective appreciations of the Likert scale despite guidelines, while LLM consistency depends mostly on parameters that dictate output variability, e.g.temperature or top-p.That said, we reckon that it is still useful to display human IRR values as a baseline.We observe that LLMs have very high consistency overall for all criteria; the lowest value is Mistral-7B's ICC for Surprise (0.66), which is still fairly high.Confidence intervals are also smaller than for human ratings.</p>
<p>Correlations with Human Annotations</p>
<p>Here, we study the Kendall correlations between LLM and human ratings on corresponding criteria.For the "Beluga-13B 1" column in Fig. 4, the first value is the correlation between Beluga-13B Relevance ratings and averaged human Relevance ratings for Eval-Prompt 1, then Coherence ratings, etc.</p>
<p>Assuming we want an automatic measure to perform as well as an individual human rater would, we need a baseline for comparison.Therefore, we also compute the average correlations between individual human ratings and average human ratings, which we compiled into the same figures for the sake of readability (the "Human" column).Since the individual human rating is included in the average human rating, both measures are not independent, so the column acts as an upper-bound.</p>
<p>Overall Correlations (Fig. 4).LLM ratings generally correlate with human ratings similarly to automatic measures, if not better.Overall, Beluga-13B is the best performer, achieving higher correlations (0.25 on average) than both other LLMs and automatic measures (≤0.18).The better results (as compared to Llama-13B (0.16) and Mistral-7B (0.20)) suggest a positive influence of fine-tuning and model size respectively.The inferior performance of ChatGPT (0.18) is difficult to explain since OpenAI does not disclose the details of its architecture, its training process and, most importantly, its training data.Nonetheless, an important takeaway is that current source-available models can effectively compete with closed-source models: H u m a n B e l u g a -  System-level Correlations (Fig. 5).First, we observe that human baseline correlations are noticeably higher than non-LLM automatic measures: while human annotators tend to reach a consensus when ranking systems (averaging correlations of 0.73), non-LLM automatic measures are moderately to poorly correlated from human judgment (with values ranging from 0.13 to 0.57).Meanwhile, Llama models display very high correlations, with Beluga-13B performing almost as well as human raters (0.70 vs 0.73).ChatGPT shows a somewhat erratic performance (correla-tions range from 0.07 to 0.73), which is overall comparable or inferior to Llama models.Also, LLMs generally outperform other automatic measures (0.70 for Beluga-13B compared to 0.57 for BARTScore).
H u m a n B e l u g a -1 3 B 1 L l a m a -1 3 B 1 M i s t r a l -7 B 1 C h a t G P T 1 B A R T S c o r e B E R T S c o r e B L E U R O U G E -1 c h r F B a1 3 B 1 L l a m a -1 3 B 1 M i s t r a l -7 B 1 C h a t G P T 1 B A R T S c o r e B E R T S c o r e B L E U R O U G E -1 c h r F B a
The fact that correlations are sometimes higher than the baseline can be explained by the subjective nature of the task: human annotators may exhibit higher variability in their ratings than the stable LLMs.</p>
<p>L l a m a -
1 3 B 1 M i s t r a l -7 B 1 C h a t G P T 1 B A R T S c o r e B E R T S c o r e B L E U R O U G E -1 c h r F B a r y S c o r e S U P E R T B L A N C RE CH EM SU EG CX
Beluga-13B 1 14 9 0 5 0 1 1 3 0 4 17 14 0 1 0 1 1 1 0 0 0 4 6 0 1 0 1 1 1 0 0 14 17 1 0 15 8 41 16 0 0 0 12 7 0 6 0 8 9 7 0 0 7 2 9 0 4 0 14 22 3 0 0  Statistical Testing.Fig. 6 shows the BH-adjusted p-values of the Williams tests for the increase in correlations with a given criterion between Beluga-13B average Eval-Prompt 1 ratings (row) and other measures (column).
Overall L l a m a -1 3 B 1 M i s t r a l -7 B 1 C h a t G P T 1 B A R T S c o r e B E R T S c o r e B L E U R O U G E -1 c h r F B a
For overall correlations, there is strong statistical evidence that Beluga-13B correlates better with human judgment than many non-LLM automatic measures (p &lt; 0.01 for many tests).Evidence is more moderate to weak when comparing   0.14.While the performance of Beluga-13B still leaves a lot of room for improvement, it performs better than non-LLM automatic measures.</p>
<p>For system-level correlations, statistical evidence for better performance appears weaker: p &gt; 0.11 for all tests.However, one should keep in mind that the ratings (averaged over more than 1,000 stories) used to compute system-level correlations hold more information than the individual ratings of the overall correlations.Therefore, while statistical evidence is weaker, the averaged nature of the correlations and the significant numeric increases in correlations (0.70 for Beluga-13B vs 0.57 for BARTScore/BERTScore) suggest that Beluga-13B is more reliable at ordering systems compared to non-LLM measures.</p>
<p>Takeaways</p>
<p>First, LLMs show very high self-consistency.Overall correlations remain weak, although LLMs display marginal improvements over non-LLM automatic measures, backed with strong statistical evidence.At the system-level, LLM correlations with human judgment are high, but statistical evidence is weaker.In conclusion, while LLMs still cannot be relied upon to evaluate a single story, they appear more reliable than non-LLM automatic measures for comparing different models and selecting the best one.</p>
<p>ASE2: Influence of the Eval-Prompt</p>
<p>In this section, we discuss the influence of the Eval-Prompt on the consistency and distribution of the generated LLM ratings.</p>
<p>Influence on Consistency</p>
<p>Here, we analyse the influence of the Eval-Prompt on LLM consistency.ICC2k values for Beluga-13B ratings w.r.t. the different Eval-Prompts are shown on Tab. 2 (other LLMs display similar behavior).The influence of Eval-Prompts appears limited: providing guidelines (Eval-Prompt 3) tends to decrease self-consistency for all criteria except Complexity with a discernible effect (as shown by the confidence intervals), but ICC values remain very high.LLMs are therefore remarkably consistent in their grading, no matter the Eval-Prompt.</p>
<p>Influence on Ratings</p>
<p>We show the average Likert ratings per LLM per Eval-Prompt on Tab. 3. Compared to Eval-Prompt 1, Eval-Prompt 2 seems to have limited influence on the ratings for all models, often leading to overlapping confidence intervals.Eval-Prompt 3 causes a statistically discernible decrease in ratings for Beluga-13B and Llama-13B, and a discernible increase for ChatGPT.Eval-Prompt 4 has a similar effect, with the decrease also observable with Mistral-7B.The significantly lower ratings of ChatGPT partly stem from the fact that it was not asked to rate the new Llama-generated stories, which were generally highly-rated.</p>
<p>Overall, it seems that more detailed Eval-Prompts (3 and 4) tend to decrease the ratings for Llama-models while having an opposite effect for ChatGPT.We tried to separate ratings per generative model or per criterion but were unable to identify a more specific pattern: we therefore chose to show only the aggregated results for the sake of clarity.</p>
<p>Influence on Correlations</p>
<p>Here we analyze the influence of Eval-Prompts on correlations between LLM ratings and human ratings.Overall Correlations (Fig. 7).Eval-Prompt 2 overall correlations are very close to Eval-Prompt 1 correlations for all models: simply asking for an explanation has limited influence on correlations.Eval-Prompt 3 tends to decrease correlations for all models: providing guidelines makes the model less accurate, counter-intuitively.Eval-Prompt 4 (providing a human story for reference) has a similar effect.
B -1 3 B 1 B -1 3 B 2 B -1 3 B 3 B -1 3 B 4 L -1 3 B 1 L -1 3 B 2 L -1 3 B 3 L -1 3 B 4 M -7 B 1 M -7 B 2 M -7 B 3 M -7 B 4 C h a t 1 C h a t 2 C h a t
System-level Correlations (Fig. 8).Eval-Prompt 2 has limited effect on correlations again, except for Beluga-13B for whom it seems to increase correlations.Eval-Prompt 3 decreases correlations, with a marked effect in Llama-13B.Finally, Eval-Prompt 4 seems to cause a small increase in correlations, contrary to its decreasing effect on overall correlations.</p>
<p>Takeaways</p>
<p>First, regardless of Eval-Prompt complexity, LLMs behave consistently when prompted multiple times.Asking for an explanation (Eval-Prompt 2) has negligible effect on ratings, while more complex  Eval-Prompts (3 -providing guidelines and 4 -providing a reference human story) have a more discernible influence (positive or negative).As for correlations with human ratings, providing guidelines (Eval-Prompt 3) consistently seems to lower correlations, whereas providing a human story for reference (Eval-Prompt 4) has opposite effects for overall or system-level correlations.
B -1 3 B 1 B -1 3 B 2 B -1 3 B 3 B -1 3 B 4 L -1 3 B 1 L -1 3 B 2 L -1 3 B 3 L -1 3 B 4 M -7 B 1 M -7 B 2 M -7 B 3 M -7 B 4 C h a t 1 C h a t</p>
<p>ASE3: Explainability of Ratings</p>
<p>In this section, we analyze to what extent the explanations provided by LLMs are consistent w.r.t.their ratings, i.e., whether they differ from criterion to criterion, whether they are semantically relevant and, for Eval-Prompt 3, whether they are compliant with the provided guidelines.We will focus on Beluga-13B since it had the best correlations with human judgment, as shown in Sec.4.1.</p>
<p>Visualization of Explanation Embeddings</p>
<p>First, we want to ascertain whether Beluga-13B provides different explanations for each of the human criteria.We gather the explanations provided by Beluga-13B on human stories for each criterion and use the SentenceTransformers library (Reimers and Gurevych, 2019) to compute their corresponding embeddings.We then use a 2D UMAP projection (McInnes et al., 2018) (with parameters n_neighbors = 300 and metric = euclidean) to visualize how the embeddings are distributed.Fig. 9 shows the visualization of the UMAP projection: Beluga's explanations are overall well-separated w.r.t.their corresponding criteria.</p>
<p>HANNA dataset with stories generated from more recent models.Since Beluga-13B and Mistral-7B display very high system-level correlations with human ratings (see Fig. 5), we use their ratings as proxy for human ratings.Tab.6 and Tab.7 show the average Beluga-13B and Mistral-7B ratings for Eval-Prompt 1 per model per criterion for a few HANNA models (GPT-2, HINT) and the Llama models.</p>
<p>We observe that LLMs perform remarkably well, getting higher ratings than older models (GPT-2) and even human stories.Beluga-13B and Mistral-7B both seem to prefer the outputs from larger LLMs (Platypus2-70B, Llama-30B) to their own outputs, suggesting that the LLM grading process cannot be explained simply by a proxy for perplexity.Interestingly, in both tables, Mistral-7B gets slightly higher ratings than Beluga, with some differences being statistically discernible, which could be explained by differences in fine-tuning data.</p>
<p>Takeaways.Larger models (Platypus2-70B, Llama-30B) exhibit the best ASG performance, with LLM ratings at least equal to those of human stories.However, our setting involves short stories of between 500 and 1,000 words; generating longer stories may prove more difficult since maintaining large-scale coherence may become an issue.</p>
<p>ASG2: Influence of Pretraining Data on ASG Performance</p>
<p>In this section, we verify whether the LLM pretraining data contains the WritingPrompts dataset to check for model contamination, as advised by Magar and Schwartz (2022), and to what extent ASG performance is related with data exploitation, e.g. through reproduction of training examples.We use the MIN-K% PROB detection method (Shi et al., 2024) which is based on the hypothesis that unseen data will contain more outlier words with low probability than seen data.Furthermore, it does not require additional training.Given a sentence and an LLM's probability distribution of the next token, MIN-K% PROB selects the top-k% of tokens with the highest negative log-likelihood and computes their average log-likelihood.We can then detect if the sentence was included in pretraining data by thresholding this average.We follow Shi et al. (2024) and use k = 20 for our two experiments.</p>
<p>Model Contamination.We sample 1,000 stories from the WritingPrompts dataset (Fan et al., 2018), from which the HANNA human stories come.Tab. 8 shows the predicted contamination rates of We observe that the AUC detection score is higher for larger models, i.e., it is easier to detect if a book was in the pretraining data of a larger LLM.The definition of the MIN-K% PROB measure also means that larger LLMs tend to produce text that is more similar to their pretraining data, such as fiction books, which could help explain their better ASE ratings.</p>
<p>Takeaways.The better performance of larger LLMs for ASG may be partially explained by their tendency to generate text that is more similar to their pretraining data, e.g.existing novels.</p>
<p>Discussion on LLM performance</p>
<p>Our work is part of the ongoing research on the general ability of LLMs for understanding and think-ing.Mahowald et al. (2024) distinguish formal (the statistical features of language) and functional linguistic competence (the ability to use language in the world) and show that LLMs are very successful on formal linguistic tasks but struggle at functional linguistic tasks.Bubeck et al. (2023) argue that LLMs do display impressive performance at a wide variety of tasks but lack "slow thinking" capabilities, referring to the System 1-System 2 dichotomy introduced by Kahneman (2011).</p>
<p>Thus, the high performance of LLMs at ASE should be interpreted with caution: we hypothesize that the "rating" part of our story evaluation experiments could be linked to formal linguistic competence and the fast, automatic System 1, while the "explanation" part would correspond to functional linguistic competence and the slow, conscious System 2.</p>
<p>This analogy would explain the good correlations of LLM ratings with human ratings: the internal criterion of LLMs for story evaluation may be formal quality (vocabulary, syntax, grammar), regardless of the criterion mentioned in the Eval-Prompt.Indeed, the six criteria from Chhun et al. (2022) are mostly orthogonal but not completely independent: their correlation with one another may be related to the general "System 1" tendency of human raters to favour stories that display better formal qualities.In that sense, LLMs may reflect a human bias towards easy, intuitive thinking.By contrast, the less convincing performance of LLMs at explaining their ratings may highlight their weaker System 2 capabilities as argued by Mahowald et al. (2024) and Bubeck et al. (2023).</p>
<p>Conclusions</p>
<p>6.1 Practical Takeaways 1. Used with prompts based on specific criteria, LLMs are currently the best proxy for human evaluation of story generation (Sec.4.1.2).In particular, LLMs display very high system-level correlations with human judgment; 2. LLMs are remarkably self-consistent (Sec.4.1.1),exhibiting very high intra-class coefficient values; 3. LLMs understand the ASE task only partially (Sec.4.3.3):they struggle to explain their answers with substantiated claims; 4. For ASE, providing detailed guidelines (Eval-Prompt 3) did not lead to improved correlations with human ratings (Sec.4.2.3).Providing a human story for reference (Eval-Prompt 4) yields mixed results; 5. LLM stories have at least equal ASE ratings to human stories (Sec.4.4), with larger LLMs exhibiting the best performance; 6. Pretraining data helps explain LLM performance at ASG (Sec.4.5): the higher ratings of larger LLMs may be due to their ability to produce output similar to existing books.</p>
<p>Limitations and Future Directions</p>
<p>The ASE task is a very subjective one: LLM performance at ASE and ASG must be seen as a reflection of average preferences and may therefore include biases, e.g. from their pretraining data.</p>
<p>Furthermore, we performed most of our experiments in a zero-shot setting without further training; it would be interesting to compare our results with future work involving fine-tuning or reinforcement learning with human feedback on data specific to ASE.Also, we did not conduct our experiments with LLMs that were optimized for long inputs and outputs, such as GPT-4.</p>
<p>Finally, we mainly used source-available LLama models and found that they performed at least as well as ChatGPT, a proprietary model.We encourage the NLP community to favor the use of such models, as the growing presence of closed models hinders research transparency and reproductibility.</p>
<p>. It was simple, red, no words on it as I already knew what it did.I mean I built the button, I built what happens [...]</p>
<p>Fig. 5 :
5
Fig.5: System-level absolute Kendall correlations (×100) between evaluation measures and human ratings.Higher is better.The white vertical line separates LLMs (left) and non-LLMs (right).</p>
<p>Fig. 6 :
6
Fig. 6: BH-adjusted p-values (×100) of the Williams tests for overall and system-level Kendall correlations.Lower is better."0" means p &lt; 0.01.</p>
<p>Fig. 7 :
7
Fig. 7: Overall absolute Kendall correlations (×100) between LLMs and human ratings for different Eval-Prompts.Higher is better.B-13B = Beluga-13B, L-13B = Llama-13B, M-7B = Mistral-7B and Chat = ChatGPT.</p>
<p>Fig. 8 :
8
Fig. 8: System-level absolute Kendall correlations (×100) between LLMs and human ratings for different Eval-Prompts.Higher is better.B-13B = Beluga-13B, L-13B = Llama-13B, M-7B = Mistral-7B and Chat = ChatGPT.</p>
<p>Schema of the performed ASE experiments.RE, CH, etc. are the considered human criteria (Sec.3.1)."EP" means "Eval-Prompt", defined in Sec.3.1.For the user study (Sec.3.3), we randomly sampled 100 explanations from our experiments.
Llama-13BHANNAx1,056ChatGPT Mistral-7B Beluga-13BPromptEP 1: RatingEP 2: Rating + explanationEP 3: Rating + expl. + guidelinesEP 4: Rating + expl. + human story+ StoryRE RatingRE ExplanationRE ExplanationRE ExplanationCH RatingCH ExplanationCH ExplanationCH ExplanationEM RatingEM ExplanationEM ExplanationEM ExplanationUser Study of GeneratedSU RatingSU ExplanationSU ExplanationSU ExplanationExplanationsx100EG RatingEG ExplanationEG ExplanationEG ExplanationHuman AnnotationsCX RatingCX ExplanationCX ExplanationCX ExplanationFig. 2:</p>
<p>Average Likert ratings per LLM per Eval-Prompt.The asterisk signals the fact that ChatGPT was only asked to rate the original HANNA dataset without Llama-generated stories.Higher is better.
CriterionEval-Prompt 1 Eval-Prompt 2 Eval-Prompt 3 Eval-Prompt 4Relevance0.88±0.010.90±0.010.85±0.020.92±0.01Coherence0.93±0.010.94±0.010.87±0.010.93±0.01Empathy0.88±0.010.88±0.010.83±0.020.91±0.01Surprise0.80±0.020.79±0.020.70±0.030.85±0.01Engagement0.91±0.010.92±0.010.79±0.020.93±0.01Complexity0.85±0.010.86±0.010.85±0.010.89±0.01Tab. 2: Intra-class coefficients type 2k for Beluga-13B ratings with 95% confidence interval. Higher is better.LLMEval-Prompt 1 Eval-Prompt 2 Eval-Prompt 3 Eval-Prompt 4Beluga-13B3.48±0.043.38±0.033.06±0.033.28±0.04Llama-13B3.48±0.033.52±0.033.21±0.022.82±0.03Mistral-7B3.47±0.033.51±0.033.46±0.033.28±0.03ChatGPT*1.52±0.031.47±0.031.62±0.021.60±0.03Tab. 3:lie between 0.01 and
https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard
AcknowledgmentsThis work was performed using HPC resources from GENCI-IDRIS (Grant 2022-AD011013105R1) and was partially funded by the grants ANR-20-CHIA-0012-01 ("NoRDF") and ANR-23-CE23-0033-01 ("SINNet").We would also like to convey our appreciation to TACL Action Editor Ehud Reiter, as well as to our anonymous reviewers, for their valuable feedback.Error TypeRate AC1Poor Syntax 0.02 0.93 0.97 1.00 Incoherence 0.11 0.73 0.81 0.89 Wrong Guideline 0.13 0.85 0.90 0.96 Superfluous Text 0.20 0.55 0.66 0.78 Unsubstantiated Claims 0.31 0.47 0.60 0.74 Tab.5: Error rates of Beluga-13B Eval-Prompt 3 on a sample of 100 explanations.Lower is better.User Study on LLM ExplanationsWe display the results of our user study (designed in Sec.3.3) in Tab. 5. We also display the IRR, which we computed using Gwet's agreement coefficient 1 (AC1)(Gwet, 2008;Fergadis and Scheffler, 2022).Gwet's AC1 is known to perform well for IRR estimation on binary classification tasks such as our user study: it was designed to be more stable and less affected by prevalence and marginal probability than Cohen's kappa, and this was confirmed by practical experiments(Wongpakaran et al., 2013).We can see that Beluga-13B produces nearimpeccable syntax, at least according to annotators (2% of "Poor Syntax").It also does a good job at producing coherent text (11% of "Incoherence"), and mostly understands the guidelines (13% of "Wrong Guideline").However, it tends to repeat itself somewhat (20% of "Superfluous Text") and, most notably, tends not to substantiate its claims with direct references to the story (31% of "Unsubstantiated Claims").Overall, annotators tend to agree with one another, as showed by the high values of Gwet's AC1.The substantial rate of "Unsubstantiated Claims" and the fact that 40% of all Eval-Prompt 3 ratings are not supported by an explanation-despite the Eval-Prompt explicitly asking for it-beg the question of whether Beluga-13B truly understands the given task.We discuss this question further in Sec. 5.Takeaways.LLM explanations seem to be specific to each considered human evaluation criterion; however, a finer analysis with a user study reveals that LLMs often struggle with following guidelines and substantiating their explanations.ASG1: LLM Performance in ASGIn this section, we discuss the performance of LLMs at the ASG task compared to human and previous models' performance, as we expanded the
. Re Ch Em Su Eg Cx Model, Average, Human 3.37±0.12 3.55±0.11 3.42±0.11 3.11±0.13 3.58±0.10 3.48±0.10 3.42±0.06</p>
<p>. -13b Beluga, </p>
<p>. Mistral-7b , </p>
<p>Llama-7B. </p>
<p>Average Beluga-13B ratings for Eval-Prompt 1 with 95% confidence interval. 48±0.11 3.50±0.10 3.69±0.08 3.24±0.11 3.42±0.10 3.45±0.07 3.46±0.05Higher is better. Model RE CH EM SU EG CX Average Human 3. 6</p>
<p>. -13b Beluga, </p>
<p>. Mistral-7b , </p>
<p>Llama-7B. </p>
<p>Average Mistral-7B ratings for Eval-Prompt 1 with 95% confidence interval. Higher is better. References Amal Alabdulkarim, Siyan Li, and Xiangyu Peng. 10.1038/d41586-019-00857-9Proceedings of the Third Workshop on Narrative Understanding. Sander Valentin Amrhein, Blake Greenland, Mcshane, the Third Workshop on Narrative Understanding2021. 20197Scientists rise up against statistical significance</p>
<p>Ask me anything: A simple strategy for prompting language models. Simran Arora, Avanika Narayan, Laurel Mayee F Chen, Neel Orr, Kush Guha, Ines Bhatia, Christopher Chami, Re, The Eleventh International Conference on Learning Representations. 2023</p>
<p>A preliminary survey on story interestingness: Focusing on cognitive and emotional interest. Byung-Chull Bae, Suji Jang, Youngjune Kim, Seyoung Park, 10.1007/978-3-030-92300-6_45International Conference on Interactive Digital Storytelling. Springer2021</p>
<p>Controlling the false discovery rate: A practical and powerful approach to multiple testing. Yoav Benjamini, Yosef Hochberg, 10.1111/j.2517-6161.1995.tb02031.xJournal of the Royal statistical society: series B (Methodological). 5711995</p>
<p>Re-evaluating evaluation in text summarization. Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig, 10.18653/v1/2020.emnlp-main.751Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, 10.48550/arXiv.2303.12712arXiv:2303.12712v5Sparks of artificial general intelligence: Early experiments with GPT-4. 2023arXiv preprint</p>
<p>Yake! Keyword extraction from single documents using multiple local features. Ricardo Campos, Vítor Mangaravite, Arian Pasquali, Alípio Jorge, Célia Nunes, Adam Jatowt, 10.1016/j.ins.2019.09.013Information Sciences. 5092020</p>
<p>Evaluation of text generation: A survey. Asli Celikyilmaz, Elizabeth Clark, Jianfeng Gao, 10.48550/arXiv.2006.147992020. 2006.14799v2ArXiv preprint</p>
<p>Art or artifice? Large language models and the false promise of creativity. Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, Chien-Sheng Wu, 10.48550/arXiv.2309.14556arXiv:2309.14556v12023arXiv preprint</p>
<p>Of human criteria and automatic metrics: A benchmark of the evaluation of story generation. Cyril Chhun, Pierre Colombo, Fabian M Suchanek, Chloé Clavel, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, Journal of Machine Learning Research. Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern24240Douglas Eckand Noah Fiedel. 2023. PaLM: Scaling language modeling with pathways</p>
<p>All that's 'human' is not gold: Evaluating human evaluation of generated text. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah A Smith, 10.18653/v1/2021.acl-long.565Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>A coefficient of agreement for nominal scales. Jacob Cohen, 10.1177/001316446002000104Educational and psychological measurement. 2011960</p>
<p>The glass ceiling of automatic evaluation in natural language generation. Pierre Colombo, Maxime Peyrard, Nathan Noiry, Robert West, Pablo Piantanida, 10.18653/v1/2023.findings-ijcnlp.16Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings). Bali2023Association for Computational Linguistics</p>
<p>Automatic text evaluation through the lens of Wasserstein barycenters. Pierre Colombo, Guillaume Staerman, Chloé Clavel, Pablo Piantanida, 10.18653/v1/2021.emnlp-main.817Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>On the limitations of reference-free evaluations of generated text. Daniel Deutsch, Rotem Dror, Dan Roth, 10.18653/v1/2022.emnlp-main.753Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>The four elements of every successful story. Reflections -Society for Organizational Learning. Robert Dickman, 20034</p>
<p>Is GPT-3 a good data annotator?. Bosheng Ding, Chengwei Qin, Linlin Liu, Ken Yew, Boyang Chia, Shafiq Li, Lidong Joty, Bing, 10.18653/v1/2023.acl-long.626Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>. Canada Toronto, Association for Computational Linguistics</p>
<p>Is GPT-3 text indistinguishable from human text? Scarecrow: A framework for scrutinizing machine text. Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A Smith, Yejin Choi, 10.18653/v1/2022.acl-long.501Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>A bayesian perspective on Likert scales and central tendency. Igor Douven, 10.3758/s13423-017-1344-2Psychonomic Bulletin &amp; Review. 252018</p>
<p>Accessed. Edilivre, Concours de nouvelles 2023. 2023</p>
<p>Auto-labelling entities in low-resource text: A geological case study. Majigsuren Enkhsaikhan, Wei Liu, Eun-Jung Holden, Paul Duuring, 10.1007/s10115-020-01532-6Knowledge and Information Systems. 632021</p>
<p>Hierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, 10.18653/v1/P18-1082Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia20181Association for Computational Linguistics</p>
<p>Chance-corrected agreement coefficients. Aris Fergadis, Benedikt Scheffler, 2022</p>
<p>Measuring nominal scale agreement among many raters. L Joseph, Fleiss, 10.1037/h0031619Psychological bulletin. 7653781971</p>
<p>SU-PERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. Yang Gao, Wei Zhao, Steffen Eger, 10.18653/v1/2020.acl-main.124Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>How a creative storytelling intervention can improve medical student attitude towards persons with dementia: A mixed methods study. R Daniel, Heather L George, Megan M Stuckey, Whitehead, 10.18653/v1/2020.emnlp-main.351Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, Nanyun Peng, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2014. 202013Content planning for neural story generation with Aristotelian rescoring</p>
<p>Testing for significance of increased correlation with human judgment. Yvette Graham, Timothy Baldwin, 10.3115/v1/D14-1020Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, Qatar2014Association for Computational Linguistics</p>
<p>A knowledgeenhanced pretraining model for commonsense story generation. Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang, 10.1162/tacl_a_00302Transactions of the Association for Computational Linguistics. 82020</p>
<p>Long text generation by modeling sentence-level and discourse-level coherence. Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, Minlie Huang, 10.18653/v1/2021.acl-long.499Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211Association for Computational Linguistics</p>
<p>Computing inter-rater reliability and its variance in the presence of high agreement. Kilem Li, Gwet , 10.1348/000711006X126600British Journal of Mathematical and Statistical Psychology. 6112008</p>
<p>Computing inter-rater reliability for observational data: An overview and tutorial. Kevin A Hallgren, 10.20982/tqmp.08.1.p023Tutorials in Quantitative Methods for Psychology. 81232012</p>
<p>Answering the call for a standard reliability measure for coding data. Andrew F Hayes, Klaus Krippendorff, 10.1080/19312450709336664Communication methods and measures. 112007</p>
<p>Why, when and how to adjust your P values?. Mohieddin Jafari, Naser Ansari-Pour, 10.22074/cellj.2019.5992Cell Journal (Yakhteh). 2046042019</p>
<p>. João Ricardo De Oliveira Júnior, Ricardo Limongi, Weng Marc Lim, Jacqueline K Eastman, </p>
<p>A story to sell: The influence of storytelling on consumers' purchasing behavior. Satish Kumar, 10.1002/mar.21758Psychology &amp; Marketing. 4022023. 2011Daniel KahnemanFast and Slow. Farrar, Straus and Giroux</p>
<p>A new measure of rank correlation. Maurice G Kendall, 10.2307/2332226Biometrika. 301/21938</p>
<p>CTRL: A conditional transformer language model for controllable generation. Nitish Shirish Keskar, Bryan Mccann, Lav R Varshney, Caiming Xiong, Richard Socher, 10.48550/arXiv.1909.05858abs/1909.05858v2ArXiv preprint. 2019</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Story generation with crowdsourced plot graphs. Boyang Li, Stephen Lee-Urban, George Johnston, Mark Riedl, 10.1609/aaai.v27i1.8649Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201327</p>
<p>OpenOrca: An open dataset of GPT augmented FLAN reasoning traces. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, 2023Chanvichet Vong, and "Teknium</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, Spain2004Association for Computational Linguistics</p>
<p>RoBERTa: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 10.48550/arXiv.1907.116922019. 1907.11692v1ArXiv preprint</p>
<p>Storytelling on mobile devices for cultural heritage. New Review of Hypermedia and Multimedia. Vincenzo Lombardo, Rossana Damiano, 10.1080/13614568.2012.617846201218</p>
<p>Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. Qingsong Ma, Johnny Wei, Ondřej Bojar, Yvette Graham, 10.18653/v1/W19-5302Proceedings of the Fourth Conference on Machine Translation. the Fourth Conference on Machine TranslationFlorence, ItalyAssociation for Computational Linguistics20192Shared Task Papers, Day 1)</p>
<p>Data contamination: From memorization to exploitation. Inbal Magar, Roy Schwartz, 10.18653/v1/2022.acl-short.18Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland2022Short Papers). Association for Computational Linguistics</p>
<p>Dissociating language and thought in large language models. Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, 10.1016/j.tics.2024.01.011Trends in Cognitive Sciences. 2024</p>
<p>What makes a good story. Allyssa Mccabe, Carole Peterson, 10.1007/BF01068179Journal of Psycholinguistic Research. 1361984</p>
<p>UMAP: Uniform manifold approximation and projection. Leland Mcinnes, John Healy, Nathaniel Saul, Lukas Großberger, 10.21105/joss.00861Journal of Open Source Software. 3298612018</p>
<p>Abandon statistical significance. Blakeley B Mcshane, David Gal, Andrew Gelman, Christian Robert, Jennifer L Tackett, 10.1080/00031305.2018.1527253The American Statistician. 73sup12019</p>
<p>The power of story: Using storytelling to improve literacy learning. Sara Miller, Lisa Pennycuff, Journal of Cross-Disciplinary Perspectives in Education. 112008</p>
<p>Significance test of increase in correlation for NLP evaluations in Python. Jihyung Moon, 2019</p>
<p>Rewriting results sections in the language of evidence. Stefanie Muff, B Erlend, Robert B Nilsen, Chloé R O'hara, Nater, 10.1016/j.tree.2021.10.009Trends in Ecology &amp; Evolution. 3732022</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, 10.48550/arXiv.2306.02707arXiv:2306.02707v1Orca: Progressive learning from complex explanation traces of GPT-4. 2023arXiv preprint</p>
<p>Why we need new evaluation metrics for NLG. Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, Verena Rieser, 10.18653/v1/D17-1238Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>BLEU: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Note on regression and inheritance in the case of two parents. 10.1098/rspl.1895.0041Proceedings of the Royal Society of London. Karl Pearson. 1895. VII58347-352</p>
<p>chrF: Character n-gram Fscore for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, Portugal2015Association for Computational Linguistics</p>
<p>Muhammad Khurram Ehsan, Aasim Ali, and Unaza Sajid. 2022. A novel auto-annotation technique for aspect level sentiment analysis. Muhammad Aasim Qureshi, Muhammad Asif, Mohd Fadzil Hassan, Ghulam Mustafa, 10.32604/cmc.2022.020544Computers, Materials and Continua. 703</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>PlotMachines: Outlineconditioned generation with dynamic plot state tracking. Asli Hannah Rashkin, Yejin Celikyilmaz, Jianfeng Choi, Gao, 10.18653/v1/2020.emnlp-main.349Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERT-networks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. Laria Reynolds, Kyle Mcdonell, 10.1145/3411763.3451760Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 2021</p>
<p>Leveraging pre-trained checkpoints for sequence generation tasks. Sascha Rothe, Shashi Narayan, Aliaksei Severyn, 10.1162/tacl_a_00313Transactions of the Association for Computational Linguistics. 82020</p>
<p>. Stephen Rowcliffe, Storytelling in science. School Science Review. 863141212004</p>
<p>Detecting pretraining data from large language models. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024. May 7-11, 2024OpenReview.net</p>
<p>The proof and measurement of association between two things. Charles Spearman, 10.1037/11491-005Studies in Individual Differences: The Search for Intelligence. J J Jenkins, D G Paterson, Appleton-Century-Crofts1961</p>
<p>Tests for comparing elements of a correlation matrix. H James, Steiger, 10.1037/0033-2909.87.2.245Psychological Bulletin. 8722451980</p>
<p>Issues in psychophysical measurement. S Stanley, Stevens, 10.1037/h0031324Psychological Review. 7854261971</p>
<p>. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker</p>
<p>LaMDA: Language models for dialog applications. Yu Du, 10.48550/arXiv.2302.13971arXiv:2302.13971v12023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, 2022arXiv preprintArXiv preprint, abs/2201.08239v3</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, 10.48550/arXiv.2307.09288arXiv:2307.09288v2Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint</p>
<p>The Creative Process: A Computer Model of Storytelling and Creativity. R Scott, Turner, 2014Psychology Press</p>
<p>Pingouin: Statistics in Python. Raphael Vallat, 10.21105/joss.01026Journal of Open Source Software. 33110262018</p>
<p>Fill in the BLANC: Human-free quality estimation of document summaries. Oleg Vasilyev, Vedant Dharnidharka, John Bohannon, 10.18653/v1/2020.eval4nlp-1.2Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems. the First Workshop on Evaluation and Comparison of NLP SystemsOnline. Association for Computational Linguistics2020</p>
<p>Automated event annotation in literary texts. Michael Vauth, Ole Hans, Evelyn Hatzel, Chris Gius, ; Biemann, Org Ceur-Ws, Shuohang, Yang Wang, Yichong Liu, Chenguang Xu, Michael Zhu, Zeng, 10.18653/v1/2021.findings-emnlp.354Proceedings of the Conference on Computational Humanities Research, CHR2021. the Conference on Computational Humanities Research, CHR2021Amsterdam, The Netherlands; Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021. November 17-19, 2021. 20212989Findings of the Association for Computational Linguistics: EMNLP 2021</p>
<p>Moving to a world beyond. Ronald L Wasserstein, Allen L Schirm, Nicole A Lazar, 10.1080/00031305.2019.1583913p&lt;0.05The American Statistician. 73sup12019</p>
<p>Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Transactions on Machine Learning Research. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>A prompt pattern catalog to enhance prompt engineering with ChatGPT. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, 10.48550/arXiv.2302.11382ArXiv preprint, abs/2302.11382v12023</p>
<p>Evan J Williams, Regression Analysis. Wiley. New York1959</p>
<p>A temporal variational model for story generation. David Wilmot, Frank Keller, 10.48550/arXiv.2109.06807abs/2109.06807v12021ArXiv preprint</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Gugger, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020Mariama Drame, Quentin Lhoest, and Alexander Rush</p>
<p>A Comparison of Cohen's Kappa and Gwet's AC1 when calculating inter-rater reliability coefficients: A study conducted with personality disorder samples. Tinakon Nahathai Wongpakaran, Danny Wongpakaran, Kilem L Wedding, Gwet, 10.1186/1471-2288-13-61BMC medical research methodology. 201313</p>
<p>. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G Carbonell, Ruslan Salakhutdinov, V Quoc, </p>
<p>XLNet: Generalized autoregressive pretraining for language understanding. Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019. 2019. 2019. December 8-14, 2019</p>
<p>BARTScore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. NeurIPS2021. 2021. December 6-14, 2021</p>
<p>BERTScore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?. Ying Zhang, Stephan Vogel, Alex Waibel, Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04). the Fourth International Conference on Language Resources and Evaluation (LREC'04)Lisbon, Portugal2004European Language Resources Association (ELRA</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. the 38th International Conference on Machine Learning, ICML 2021PMLR2021. 18-24 July 2021139of Proceedings of Machine Learning Research</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023a. May 1-5, 2023OpenReview.net</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023b. May 1-5, 2023OpenReview.net</p>            </div>
        </div>

    </div>
</body>
</html>
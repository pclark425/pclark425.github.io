<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-589 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-589</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-589</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-770058895898d098f149dafc86af9caff92f8427</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/770058895898d098f149dafc86af9caff92f8427" target="_blank">Artificial intelligence faces reproducibility crisis.</a></p>
                <p><strong>Paper Venue:</strong> Science</p>
                <p><strong>Paper TL;DR:</strong> The booming field of artificial intelligence is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade, leading to a new conscientiousness about research methods and publication protocols.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e589.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e589.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stochastic optimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic optimization in neural network training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that neural networks use stochastic optimization methods during training, which introduce nondeterminism into the learning process and can be a source of variability in model outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ANALYSIS OF NEURAL LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>neural network language models (RNN / LSTM / Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language processing / language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>training neural language models to estimate probabilities of token sequences and to generate text</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>stochastic optimization (explicitly mentioned); the paper does not enumerate other specific sources but links stochastic optimization to the training process variability</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Stochasticity inherent in optimization is described as part of neural network training dynamics (paper does not detail specific reproducibility experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper highlights that neural networks rely on stochastic optimization, which introduces nondeterministic elements to training and thus contributes to variability in model outcomes, but it does not quantify this variability or test controls.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence faces reproducibility crisis.', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e589.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e589.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reproducibility crisis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reproducibility crisis due to lack of available training data and source code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites a widespread reproducibility crisis in AI (as of 2021), attributing it to the common unavailability of training data and source code in published work, and recommends availability of data and code as an important evaluation criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ANALYSIS OF NEURAL LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>neural language modeling research broadly</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language processing / machine learning research methodology</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>evaluation and comparison of language modeling approaches (meta-scientific discussion, not a specific experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>lack of shared training data and source code leading to irreproducible results across research groups (explicitly mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Absence of publicly available datasets and source code in publications is identified as a primary challenge to reproducing reported results (cites industry-wide reproducibility crisis)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Making training data and training code available and accessible; using availability of data/code as an evaluation criterion (proposed as an important criterion)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper emphasizes that the frequent absence of shared training data and source code has produced an AI reproducibility crisis and that availability of these artifacts should be an important criterion when evaluating language-modeling research.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence faces reproducibility crisis.', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e589.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e589.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity (language model evaluation metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Perplexity is presented as an evaluation metric for language models that is easy to compute, implementation-independent, and therefore useful for comparing different modeling approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ANALYSIS OF NEURAL LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>language models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language processing / evaluation of language models</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>evaluating language model quality and comparing approaches</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>not discussed as a source of variability; the metric is described as implementation-independent which reduces variability due to implementation differences</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use of implementation-independent metrics (perplexity) to enable fairer comparisons across methods and implementations (suggested benefit)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper recommends perplexity as a robust, implementation-independent metric for comparing language models, implying it can help reduce variability arising from differing implementations though no empirical comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence faces reproducibility crisis.', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Artificial intelligence faces reproducibility crisis. <em>(Rating: 2)</em></li>
                <li>Language Models are Unsupervised Multitask Learners <em>(Rating: 1)</em></li>
                <li>Attention Is All You Need <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-589",
    "paper_id": "paper-770058895898d098f149dafc86af9caff92f8427",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Stochastic optimization",
            "name_full": "Stochastic optimization in neural network training",
            "brief_description": "The paper notes that neural networks use stochastic optimization methods during training, which introduce nondeterminism into the learning process and can be a source of variability in model outcomes.",
            "citation_title": "ANALYSIS OF NEURAL LANGUAGE MODELS",
            "mention_or_use": "mention",
            "model_name": "neural network language models (RNN / LSTM / Transformers)",
            "model_size": null,
            "scientific_domain": "natural language processing / language modeling",
            "experimental_task": "training neural language models to estimate probabilities of token sequences and to generate text",
            "variability_sources": "stochastic optimization (explicitly mentioned); the paper does not enumerate other specific sources but links stochastic optimization to the training process variability",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Stochasticity inherent in optimization is described as part of neural network training dynamics (paper does not detail specific reproducibility experiments)",
            "mitigation_methods": null,
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "The paper highlights that neural networks rely on stochastic optimization, which introduces nondeterministic elements to training and thus contributes to variability in model outcomes, but it does not quantify this variability or test controls.",
            "uuid": "e589.0",
            "source_info": {
                "paper_title": "Artificial intelligence faces reproducibility crisis.",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Reproducibility crisis",
            "name_full": "Reproducibility crisis due to lack of available training data and source code",
            "brief_description": "The paper cites a widespread reproducibility crisis in AI (as of 2021), attributing it to the common unavailability of training data and source code in published work, and recommends availability of data and code as an important evaluation criterion.",
            "citation_title": "ANALYSIS OF NEURAL LANGUAGE MODELS",
            "mention_or_use": "mention",
            "model_name": "neural language modeling research broadly",
            "model_size": null,
            "scientific_domain": "natural language processing / machine learning research methodology",
            "experimental_task": "evaluation and comparison of language modeling approaches (meta-scientific discussion, not a specific experiment)",
            "variability_sources": "lack of shared training data and source code leading to irreproducible results across research groups (explicitly mentioned)",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Absence of publicly available datasets and source code in publications is identified as a primary challenge to reproducing reported results (cites industry-wide reproducibility crisis)",
            "mitigation_methods": "Making training data and training code available and accessible; using availability of data/code as an evaluation criterion (proposed as an important criterion)",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "The paper emphasizes that the frequent absence of shared training data and source code has produced an AI reproducibility crisis and that availability of these artifacts should be an important criterion when evaluating language-modeling research.",
            "uuid": "e589.1",
            "source_info": {
                "paper_title": "Artificial intelligence faces reproducibility crisis.",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Perplexity",
            "name_full": "Perplexity (language model evaluation metric)",
            "brief_description": "Perplexity is presented as an evaluation metric for language models that is easy to compute, implementation-independent, and therefore useful for comparing different modeling approaches.",
            "citation_title": "ANALYSIS OF NEURAL LANGUAGE MODELS",
            "mention_or_use": "mention",
            "model_name": "language models (general)",
            "model_size": null,
            "scientific_domain": "natural language processing / evaluation of language models",
            "experimental_task": "evaluating language model quality and comparing approaches",
            "variability_sources": "not discussed as a source of variability; the metric is described as implementation-independent which reduces variability due to implementation differences",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": null,
            "mitigation_methods": "Use of implementation-independent metrics (perplexity) to enable fairer comparisons across methods and implementations (suggested benefit)",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "The paper recommends perplexity as a robust, implementation-independent metric for comparing language models, implying it can help reduce variability arising from differing implementations though no empirical comparison is provided.",
            "uuid": "e589.2",
            "source_info": {
                "paper_title": "Artificial intelligence faces reproducibility crisis.",
                "publication_date_yy_mm": "2018-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Artificial intelligence faces reproducibility crisis.",
            "rating": 2
        },
        {
            "paper_title": "Language Models are Unsupervised Multitask Learners",
            "rating": 1
        },
        {
            "paper_title": "Attention Is All You Need",
            "rating": 1
        }
    ],
    "cost": 0.0057325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>УДК 004.81</p>
<h1>А. Хом'як</h1>
<p>(Тернопільський національний технічний університет імені Івана Пулюя, Україна)</p>
<h2>АНАЛІЗ НЕЙРОМЕРЕЖЕВИХ МОДЕЛЕЙ ПРИРОДНОЇ МОВИ</h2>
<p>UDC 004.81</p>
<h2>A. Khomiak</h2>
<h2>ANALYSIS OF NEURAL LANGUAGE MODELS</h2>
<p>Мовна модель - статистична модель, яка дозволяє для будь-якої послідовності токенів мови визначити ймовірність того, що ця послідовність зустрінеться в природньому застосуванні цієї мови. Корисною властивістю такої моделі є можливість визначити яке продовження послідовності є найімовірнішим, що дозволяє використовувати таку модель в багатьох галузях, пов'язаних з обробкою природної мови - зокрема генерування текстів [1].</p>
<p>Нейронні мережі - це сімейство алгоритмів машинного навчання, які використовують комбінацію з лінійних та нелінійних перетворень даних та стохастичну оптимізацію для апроксимації довільних функцій. Через розвиток апаратних прискорювачів векторних обчислень та значне збільшення доступних обсягів даних, такий підхід дозволив значно покращити якість результатів в багатьох галузях, повязаних з когнітивно-складними задачами (машинний зір, класифікація і регресія багатовимірних даних, обробка природної мови тощо).</p>
<p>Найбільшого застосування в обробці природної мови (і мовному моделюванні зокрема) набули архітектури рекурентних нейронних мереж (Recurrent Neural Networks, RNN) та мереж з довго-короткотривалою пам'яттю (Long-Short Term Memory), проте з 2017 року їх витісняють підходи на базі трансформерів (Transformers) [2].</p>
<p>Якість мовних моделей можна оцінити за допомогою перплексії (perplexity), а оскільки ця міра легко обчислюється і не залежить від імплементації мовної моделі, то вона є хорошим інструментом для порівняння різних підходів. Експерименти з великими мовними моделями показують, що використання великих репрезентативних обсягів тексту покращують результати [3], тому тренувальні дані є важливим критерієм аналізу.</p>
<p>Станом на 2021 рік, поширена відсутність вільних даних та сирцевого коду в публікаціях призвела до галузевої кризи відтворюваності [4], тож наявність та доступність даних та коду тренування моделі є важливим критерієм при оцінці підходів до моделювання мови.</p>
<h2>Література.</h2>
<ol>
<li>A. K. Yadav. Sentence generation from a bag of words using N-gram model [Електронний pecypc] / A. K. Yadav, S. K. Borgohain. - 2014. - Режим доступу до ресурсу: https://ieeexplore.ieee.org/document/7019414.</li>
<li>A. Vaswani. Attention Is All You Need [Електронний ресурс] / A. Vaswani et al. - 2017. - Режим доступу до ресурсу: https://arxiv.org/pdf/1706.03762.pdf.</li>
<li>A. Radford. Language Models are Unsupervised Multitask Learners [Електронний ресурс] / A. Radford et al. - 2017. - Режим доступу до ресурсу: https://cdn.openai.com /better-languagesmodels/language_models_are_unsupervised_multitask_learners.pdf.</li>
<li>M. Hutson. Artificial intelligence faces reproducibility crisis. / M. Hutson. - 2018. - Режим доступу до ресурсу: https://www.science.org/doi/10.1126/science.359.6377.725.</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9490 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9490</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9490</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-2b18e63caff82f1a688714aed099b8856db92581</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b18e63caff82f1a688714aed099b8856db92581" target="_blank">Domain-Agnostic Molecular Generation with Self-feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces M OL G EN, a pre-trained molecular language model tailored speciﬁcally for molecule generation, and presents a self-feedback paradigm that inspires the pre-trained model to align with the ultimate goal of producing molecules with desirable properties.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9490.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9490.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOLGEN (chemical feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-Agnostic Molecular Generation with Chemical Feedback (MOLGEN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained molecular language model (trained on SELFIES) that assigns sequence-level log-probabilities to candidate molecules and is fine-tuned with a rank-based chemical feedback loss to align the model's generative probabilities with property-based chemical preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MOLGEN (pre-trained molecular language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2Seq Transformer (BART-style) pre-trained to reconstruct corrupted SELFIES from >100M molecules, followed by domain-agnostic prefix tuning; uses an autoregressive decoder to produce token probabilities and sums log-probabilities across tokens as a sequence score f(S). Fine-tuned with a combination of token-level cross-entropy and a pairwise rank loss that encourages higher model probabilities for candidates with better property scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Assigning higher generative likelihoods (probabilities) to candidate molecules that are expected to have superior real-world chemical properties (penalized logP, QED, and docking/binding affinity), i.e., preferring molecules more likely to be practically useful or discoverable.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Generate a set of candidate SELFIES S* from the pre-trained model; compute sequence score f(S) = sum_t log p_theta(s_t | S, S_{<t}); impose pairwise ranking constraints where for any pair (S_i,S_j) with Ps(S_i) > Ps(S_j) the model should satisfy f(S_i) > f(S_j); fine-tune the model using a rank loss (pairwise hinge-style) combined with cross-entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Model-internal log-probabilities (sequence log-likelihoods f(S)); used comparatively as pairwise rankings rather than a direct calibrated percentage; margin γ used in rank loss to enforce separation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Indirect evaluation by measuring downstream optimization outcomes and distributional metrics: property-optimization benchmarks (top-k penalized logP and QED values), docking-based binding affinity estimates (AutoDockGPU K_D proxies) on protein targets (ESR1 and ACAA1), constrained optimization improvements (mean and std of penalized logP improvement under Tanimoto similarity constraints), and standard generative-distribution metrics (Validity, Frag, Scaff, SNN, IntDiv, FCD, Novelty). No explicit probabilistic calibration metrics (e.g., Brier score) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Fine-tuning with chemical feedback led the model to rank higher-propensity molecules above lower ones and to produce state-of-the-art empirical optimization results: synthetic penalized logP top-1 = 80.30 (MolGen) vs next-best 44.99 (MARS) and many baselines much lower; QED top scores matched dataset maxima (0.948). Docking (AutoDockGPU) top estimated K_D for ESR1: MolGen produced 0.13, 0.35, 0.47 (lower is better) vs LIMO's 0.72, 0.89, 1.4 and many baselines much worse. Constrained penalized-logP improvement (δ=0.6): MolGen mean improvement 12.08 (std 0.82) vs best baselines typically <5 (e.g., GraphAF 4.98). The paper reports large empirical gains in optimization tasks indicating the rank-aligned probabilities successfully bias generation toward higher-scoring molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No direct probabilistic calibration evaluation (e.g., Brier score, calibration plots) is reported — probabilities are used for relative ranking rather than calibrated probability estimates for real-world events. Computational cost of pre-training/fine-tuning is high. The chemical feedback paradigm as implemented is tailored to single-target objectives and may struggle with multi-objective conflicting rankings. Model interpretability remains limited (transformer opacity). The approach relies on external scoring functions (property calculators or docking proxies) which themselves have biases and errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>MolGen (with chemical feedback) is compared to numerous molecule-generation baselines (JT-VAE, GCPN, MolDQN, LIMO, GraphAF, GraphDF, MARS, CHEMFORMER, RETMOL, etc.). It outperforms all listed baselines on several optimization metrics: much higher top penalized logP, comparable-best QED, substantially better constrained optimization improvements, and better top docking affinity estimates. The paper does not compare the probabilistic calibration of model outputs vs. human experts or formal probabilistic baselines; comparisons are task-performance oriented.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Techniques reported to improve alignment of model probabilities with chemical preference include: (1) pairwise rank loss that uses model log-probabilities to enforce ordering by property scores; (2) combining rank loss with token-level cross-entropy loss (with label smoothing) to retain generative quality; (3) domain-agnostic prefix tuning to transfer knowledge across domains (synthetic and natural products); (4) pre-training on SELFIES to guarantee syntactic/chemical validity; (5) leveraging an external scoring function (property calculators/docking) as feedback during fine-tuning. The paper suggests these combined methods mitigate 'molecular hallucinations' by aligning model likelihoods with desired chemical preferences.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can learn complex molecular distributions <em>(Rating: 2)</em></li>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>Large-scale chemical language representations capture molecular structure and properties <em>(Rating: 2)</em></li>
                <li>Retrieval-based controllable molecule generation <em>(Rating: 2)</em></li>
                <li>Large language model for molecular chemistry <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9490",
    "paper_id": "paper-2b18e63caff82f1a688714aed099b8856db92581",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [
        {
            "name_short": "MOLGEN (chemical feedback)",
            "name_full": "Domain-Agnostic Molecular Generation with Chemical Feedback (MOLGEN)",
            "brief_description": "A pre-trained molecular language model (trained on SELFIES) that assigns sequence-level log-probabilities to candidate molecules and is fine-tuned with a rank-based chemical feedback loss to align the model's generative probabilities with property-based chemical preferences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MOLGEN (pre-trained molecular language model)",
            "model_description": "Seq2Seq Transformer (BART-style) pre-trained to reconstruct corrupted SELFIES from &gt;100M molecules, followed by domain-agnostic prefix tuning; uses an autoregressive decoder to produce token probabilities and sums log-probabilities across tokens as a sequence score f(S). Fine-tuned with a combination of token-level cross-entropy and a pairwise rank loss that encourages higher model probabilities for candidates with better property scores.",
            "model_size": null,
            "prediction_target": "Assigning higher generative likelihoods (probabilities) to candidate molecules that are expected to have superior real-world chemical properties (penalized logP, QED, and docking/binding affinity), i.e., preferring molecules more likely to be practically useful or discoverable.",
            "prediction_method": "Generate a set of candidate SELFIES S* from the pre-trained model; compute sequence score f(S) = sum_t log p_theta(s_t | S, S_{&lt;t}); impose pairwise ranking constraints where for any pair (S_i,S_j) with Ps(S_i) &gt; Ps(S_j) the model should satisfy f(S_i) &gt; f(S_j); fine-tune the model using a rank loss (pairwise hinge-style) combined with cross-entropy.",
            "probability_format": "Model-internal log-probabilities (sequence log-likelihoods f(S)); used comparatively as pairwise rankings rather than a direct calibrated percentage; margin γ used in rank loss to enforce separation.",
            "evaluation_method": "Indirect evaluation by measuring downstream optimization outcomes and distributional metrics: property-optimization benchmarks (top-k penalized logP and QED values), docking-based binding affinity estimates (AutoDockGPU K_D proxies) on protein targets (ESR1 and ACAA1), constrained optimization improvements (mean and std of penalized logP improvement under Tanimoto similarity constraints), and standard generative-distribution metrics (Validity, Frag, Scaff, SNN, IntDiv, FCD, Novelty). No explicit probabilistic calibration metrics (e.g., Brier score) are reported.",
            "results": "Fine-tuning with chemical feedback led the model to rank higher-propensity molecules above lower ones and to produce state-of-the-art empirical optimization results: synthetic penalized logP top-1 = 80.30 (MolGen) vs next-best 44.99 (MARS) and many baselines much lower; QED top scores matched dataset maxima (0.948). Docking (AutoDockGPU) top estimated K_D for ESR1: MolGen produced 0.13, 0.35, 0.47 (lower is better) vs LIMO's 0.72, 0.89, 1.4 and many baselines much worse. Constrained penalized-logP improvement (δ=0.6): MolGen mean improvement 12.08 (std 0.82) vs best baselines typically &lt;5 (e.g., GraphAF 4.98). The paper reports large empirical gains in optimization tasks indicating the rank-aligned probabilities successfully bias generation toward higher-scoring molecules.",
            "limitations_or_challenges": "No direct probabilistic calibration evaluation (e.g., Brier score, calibration plots) is reported — probabilities are used for relative ranking rather than calibrated probability estimates for real-world events. Computational cost of pre-training/fine-tuning is high. The chemical feedback paradigm as implemented is tailored to single-target objectives and may struggle with multi-objective conflicting rankings. Model interpretability remains limited (transformer opacity). The approach relies on external scoring functions (property calculators or docking proxies) which themselves have biases and errors.",
            "comparison_to_baselines": "MolGen (with chemical feedback) is compared to numerous molecule-generation baselines (JT-VAE, GCPN, MolDQN, LIMO, GraphAF, GraphDF, MARS, CHEMFORMER, RETMOL, etc.). It outperforms all listed baselines on several optimization metrics: much higher top penalized logP, comparable-best QED, substantially better constrained optimization improvements, and better top docking affinity estimates. The paper does not compare the probabilistic calibration of model outputs vs. human experts or formal probabilistic baselines; comparisons are task-performance oriented.",
            "methods_for_improvement": "Techniques reported to improve alignment of model probabilities with chemical preference include: (1) pairwise rank loss that uses model log-probabilities to enforce ordering by property scores; (2) combining rank loss with token-level cross-entropy loss (with label smoothing) to retain generative quality; (3) domain-agnostic prefix tuning to transfer knowledge across domains (synthetic and natural products); (4) pre-training on SELFIES to guarantee syntactic/chemical validity; (5) leveraging an external scoring function (property calculators/docking) as feedback during fine-tuning. The paper suggests these combined methods mitigate 'molecular hallucinations' by aligning model likelihoods with desired chemical preferences.",
            "uuid": "e9490.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can learn complex molecular distributions",
            "rating": 2
        },
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 2
        },
        {
            "paper_title": "Large-scale chemical language representations capture molecular structure and properties",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-based controllable molecule generation",
            "rating": 2
        },
        {
            "paper_title": "Large language model for molecular chemistry",
            "rating": 1
        }
    ],
    "cost": 0.011112750000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Domain-Agnostic Molecular Generation with Chemical Feedback</h1>
<p>Yin Fang ${ }^{\text {A }}$, Ningyu Zhang ${ }^{\text {A }}$; Zhuo Chen ${ }^{\text {A }}$, Lingbing Guo ${ }^{\text {A }}$, Xiaohui Fan ${ }^{\text {A }}$, Huajun Chen ${ }^{\text {A }}{ }^{\circ}$ *<br>${ }^{A}$ College of Computer Science and Technology, Zhejiang University<br>${ }^{\text {A }}$ ZJU-Ant Group Joint Research Center for Knowledge Graphs, Zhejiang University<br>${ }^{\circ}$ Hangzhou Innovation Center, Zhejiang University<br>{fangyin, zhangningyu, zhuo.chen, lbguo, fanxh, huajunsir}@zju.edu.cn</p>
<h4>Abstract</h4>
<p>The generation of molecules with desired properties has become increasingly popular, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face challenges such as generating syntactically or chemically flawed molecules, having narrow domain focus, and struggling to create diverse and feasible molecules due to limited annotated data or external molecular databases. To tackle these challenges, we introduce MOLGEN, a pre-trained molecular language model tailored specifically for molecule generation. Through the reconstruction of over 100 million molecular SELFIES, MOLGEN internalizes structural and grammatical insights. This is further enhanced by domain-agnostic molecular prefix tuning, fostering robust knowledge transfer across diverse domains. Importantly, our chemical feedback paradigm steers the model away from "molecular hallucinations", ensuring alignment between the model's estimated probabilities and real-world chemical preferences. Extensive experiments on well-known benchmarks underscore MOLGEN's optimization capabilities in properties such as penalized $\log \mathrm{P}, \mathrm{QED}$, and molecular docking. Additional analyses confirm its proficiency in accurately capturing molecule distributions, discerning intricate structural patterns, and efficiently exploring the chemical space ${ }^{1}$.</p>
<h2>1 INTRODUCTION</h2>
<p>Molecule generation - synthesizing and designing novel molecules with desirable properties - holds an important place in chemical science, with numerous applications in drug discovery (Wang et al., 2022). Generating molecules is challenging due to the immense and discrete nature of the molecular space, which, with an estimated size of $10^{33}$, makes exhaustive searches impractical (Polishchuk et al., 2013). Early, deep generative models (Jin et al., 2020; Zang \&amp; Wang, 2020; Luo et al., 2021; Shi et al., 2020b) have emerged as one of the most promising tools for exploring the broader synthetically accessible chemical space. These models' ability to automatically generate chemically valid and structurally similar molecules has proven to be invaluable for tasks such as the inverse design of functional compounds (Flam-Shepherd et al., 2022).</p>
<p>Current deep generative models typically involve initial training of an unconditional generative model through a large set of existing molecules, and then use additional reward functions (Cao \&amp; Kipf, 2018; Popova et al., 2018; You et al., 2018; Popova et al., 2019; Shi et al., 2020b; Zang \&amp; Wang, 2020) or property predictors (Liu et al., 2018; Jin et al., 2019; Gómez-Bombarelli et al., 2018) to guide the synthesis of new molecules with desired properties. However, these approaches are limited by challenges in training due to the high variance of Reinforcement Learning (RL) (Xie et al., 2021), fixed-dimensional latent generation space (Wang et al., 2023), and expert-provided generation rules (Sun et al., 2022), which impede efficient exploration of the broader chemical space.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Recent advancements in language models have demonstrated great potential for understanding complex molecular distributions <em>Flam-Shepherd et al. (2022)</em>. To gain a more profound comprehension of the underlying molecular structures and their representations, researchers have begun integrating SMILES <em>Weininger (1988)</em>, a linear string notation for describing molecular structures, with pre-trained language models (PLMs) <em>Irwin et al. (2022)</em>. Despite their widespread use, several issues remain inadequately considered. Firstly, the brittleness of SMILES may lead to a high proportion of generated chemically invalid strings, either due to syntactic errors (e.g., not corresponding to molecular graphs) or fundamental chemical principle violations (e.g., exceeding the maximum number of inter-atomic valence bonds) <em>Krenn et al. (2020)</em>. Secondly, almost all previous studies have focused primarily on synthetic molecules, neglecting natural products <em>Du et al. (2022a)</em>. Notably, natural products, characterized by enormous scaffold diversity and structural complexity, exhibit a distinct distribution compared to synthetic molecules and confer additional challenges for numerous molecule generation applications such as drug discovery <em>Atanasov et al. (2021)</em>. Thirdly, pre-trained molecular language models often succumb to “molecular hallucinations”. This refers to instances where the generated molecules structurally adhere to chemical rules, yet fail to demonstrate the anticipated chemical activity in practical applications. This occurs because, although the models assimilate a vast array of molecular structural representations during pre-training, yet they might not fully capture the complex relationships with real-world chemistry and biological properties. Some methods attempt to mitigate this issue by using supervised fine-tuning or external databases <em>Irwin et al. (2022); Wang et al. (2023)</em>, but they may constrain the direction of molecular optimization.</p>
<p>To tackle these challenges, we present MOLGEN, a novel pre-trained molecular language model designed for efficient molecule generation. As illustrated in Figure 1, our approach comprises: (i) A two-stage domain-agnostic molecular pre-training. First, we train bidirectional and auto-regressive Transformers <em>Vaswani et al. (2017)</em> to reconstruct over 100 million corrupted molecular SELFIES <em>Krenn et al. (2020)</em>. This endows the model with a profound understanding of the structure, grammar, and intrinsic semantic information of SELFIES, an entirely robust molecular language, free from the predicaments of syntactic and semantic inconsistency often associated with conventional SMILES notation. Next, we leverage domain-agnostic molecular prefix tuning, enabling MOLGEN to harness knowledge transferable across diverse domains (i.e., synthetic and natural products), facilitating task adaptation. (ii) A chemical feedback paradigm to alleviate “molecular hallucinations”. By aligning the model’s generative probabilities with real-world chemical preferences, MOLGEN learns to evaluate and rectify its molecular outputs, ensuring the generation of chemically valid molecules with genuine utility and anticipated properties.</p>
<p>Through extensive testing on both synthetic and natural product molecular datasets, we establish MOLGEN’s capability in producing chemically valid molecules, navigating chemical spaces efficiently, and achieving notable optimization in properties like penalized logp, QED, and molecular docking. Our further analysis underscores MOLGEN’s adeptness at understanding complex molecular distributions, recognizing meaningful substructures, and the efficacy of the chemical feedback mechanism, offering novel perspectives and tools to the molecular generation community.</p>
<h2>2 Methodology</h2>
<p>Figure 2 illustrates the general framework of MOLGEN. The pre-training process (§2.1) comprises two stages: molecular language syntax learning and domain-agnostic molecular prefix tuning. Then, a chemical feedback paradigm (§2.2) is introduced to align the PLM with the anticipated chemical preferences in the downstream phase.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Overview of MolGen: pre-training (left) and downstream (right) stages.</p>
<h1>2.1 Domain-agnostic Molecular Pre-training</h1>
<p>SMILES and SELFIES are two molecular languages that associate a token sequence with a molecular structure. SMILES denotes molecules as chains of atoms, encapsulating branches within parentheses and signifying ring closures with corresponding number pairs. Despite its longstanding prominence in cheminformatics, SMILES is fundamentally flawed in
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Random double mutations of SMILES and SELFIES derived from the same molecule, with blue markings indicating mutation locations. The likelihood of retaining a valid SMILES after a single mutation is $9.9 \%$. For SELFIES, it's a consistent $100 \%$ (Krenn et al., 2020).
that it lacks a mechanism to ensure the validity of molecular strings in terms of syntax and physical principles (Krenn et al., 2020). Hence, we employ SELFIES (Krenn et al., 2022), a fully robust molecular language that guarantees every possible combination of symbols in the alphabet corresponds to a chemically sound graph structure. In contrast to SMILES, SELFIES overcomes syntactic invalidity by mapping each token to a specific structure or reference, effectively resolving issues such as unbalanced parentheses or ring identifiers, as depicted in Figure 3. MolGen boasts a compact and specialized vocabulary size of 185 . While modest in size, this vocabulary is already sufficient to ensure that the language model learns meaningful representations (Rives et al., 2021).</p>
<p>Being the first of its kind to train language models utilizing SELFIES, our work necessitates a solid foundation for comprehending both the syntax and semantics of this language. To achieve a high-quality initialization for MolGen, we employ BART model (Lewis et al., 2020) during the first stage of pre-training, as shown in Figure 2. Firstly, we convert 100 million unlabeled molecules into SELFIES strings. The standardized representation of SELFIES facilitates the direct construction of an alphabet from the dataset, eliminating the need for a separate tokenizer to discern frequent substrings, thereby preventing the generation of nonsensical tokens. Secondly, we randomly select tokens from the original SELFIES string $S=\left{s_{1}, \cdots, s_{j}, \cdots, s_{l}\right}$ and replace them with a special token [MASK]. Finally, we encode the corrupted SELFIES using a bidirectional model and calculate the likelihood of $S$ with a left-to-right autoregressive decoder. Formally, the cross-entropy between the decoder's output and the original input constitutes the reconstruction loss:</p>
<p>$$
\mathcal{L}<em j="1">{\text {ce }}(S)=-\sum</em> ; \theta\right)
$$}^{l} \sum_{s} p_{\text {true }}\left(s \mid S, S_{&lt;j}\right) \log p_{\theta}\left(s \mid S, S_{&lt;j</p>
<p>where $S_{&lt;j}$ denotes the partitial original sequence $\left{s_{0}, \cdots, s_{j-1}\right}, s_{0}$ is a pre-defined start token <s>. $p_{\text {true }}$ refers to the one-hot distribution obtained under the standard maximum likelihood estimation:</p>
<p>$$
p_{\text {true }}\left(s \mid S, S_{&lt;j}\right)= \begin{cases}1, &amp; s=s_{j} \ 0, &amp; s \neq s_{j}\end{cases}
$$</p>
<p>Upon mastering the fundamental grammatical knowledge of SELFIES, we proceed to the second stage of pre-training, wherein we introduce the domain-agnostic molecular prefix as a domain instructor to</p>
<p>facilitate the transfer of knowledge across diverse domains. Unlike the conventional prefix-tuning approach, which exclusively updates the prefix matrices without altering the pre-trained model parameters (Mao et al., 2022; Li \&amp; Liang, 2021; He et al., 2022), we capitalize on its influence over the entire model's parameters to effectively bolster its ability to comprehend various domains.
We commence by prepending two sets of $m$ tunable prefix vectors $\boldsymbol{P}<em v="v">{k}, \boldsymbol{P}</em>$, shared among domains, to the keys and values of the multi-head attention at each layer. The output attention score for each head can be formulated as:} \in \mathbb{R}^{m \times d</p>
<p>$$
\text { head }=\operatorname{Attn}\left(\boldsymbol{x} \boldsymbol{W}<em k="k">{q},\left[\boldsymbol{P}</em>}, \boldsymbol{X} \boldsymbol{W<em v="v">{k}\right],\left[\boldsymbol{P}</em>\right]\right)
$$}, \boldsymbol{X} \boldsymbol{W}_{v</p>
<p>where $\boldsymbol{X} \in \mathbb{R}^{m \times d}$ denotes the input to a Transformer layer with length $m, \boldsymbol{W}<em k="k">{q}, \boldsymbol{W}</em>}, \boldsymbol{W<em h="h">{v} \in \mathbb{R}^{d \times d</em>$ is a query vector.
Alternatively, the attention between $\boldsymbol{x}$ and $\boldsymbol{X}$ on head can be expressed as:}}$ are project matrices that map inputs to queries, keys, and values, and $\boldsymbol{x} \in \mathbf{R}^{d</p>
<p>$$
\begin{aligned}
&amp; \text { head }=\text { softmax }\left(\boldsymbol{x} \boldsymbol{W}<em k="k">{q}\left[\boldsymbol{P}</em>}, \boldsymbol{X} \boldsymbol{W<em v="v">{k}\right]^{\top}\right)\left[\begin{array}{c}
\boldsymbol{P}</em> \
\boldsymbol{X} \boldsymbol{W}<em q="q">{v}
\end{array}\right]=\text { softmax }\left(\boldsymbol{x} \boldsymbol{W}</em>
\boldsymbol{P}}\left[\begin{array}{c<em k="k">{k}^{\top} \
\left(\boldsymbol{W}</em>
\end{array}\right]\right)\left[\begin{array}{c}
\boldsymbol{P}}\right)^{\top}(\boldsymbol{X})^{\top<em v="v">{v} \
\boldsymbol{X} \boldsymbol{W}</em>
\end{array}\right] \
&amp; =\lambda(\boldsymbol{x}) \text { softmax }\left(\boldsymbol{x} \boldsymbol{W}<em k="k">{q} \boldsymbol{P}</em>}^{\top}\right) \boldsymbol{P<em q="q">{v}+(1-\lambda(\boldsymbol{x})) \text { softmax }\left(\boldsymbol{x} \boldsymbol{W}</em>}\left(\boldsymbol{W<em v="v">{k}\right)^{\top}(\boldsymbol{X})^{\top}\right) \boldsymbol{X} \boldsymbol{W}</em> \
&amp; =\lambda(\boldsymbol{x}) \quad \operatorname{Attn}\left(\boldsymbol{x} \boldsymbol{W}<em k="k">{q}, \boldsymbol{P}</em>}, \boldsymbol{P<em q="q">{v}\right)+(1-\lambda(\boldsymbol{x})) \underbrace{\operatorname{Attn}\left(\boldsymbol{x} \boldsymbol{W}</em>}, \boldsymbol{X} \boldsymbol{W<em v="v">{k}, \boldsymbol{X} \boldsymbol{W}</em>,
\end{aligned}
$$}\right)}_{\text {attention of domain-agnostic molecular prefix }</p>
<p>where $\lambda(\boldsymbol{x})$ is a scalar representing the sum of normalized attention weights on the prefixes.
In this way, domain-agnostic molecular prefixes integrate domain knowledge into the original head attention through linear interpolation. These prefixes are trained simultaneously on different molecular domains, acting as a domain instructor that influences the parameters of the entire model, thereby enhancing the model's mastery of different molecular structural complexities and scaffold diversities.</p>
<h1>2.2 Chemical Feedback Paradigm: Align PLM with Chemical Preference</h1>
<p>After the pre-training stage, the model gains the capability to generate syntactically correct molecules. However, it may still suffer from "molecular hallucination". Consider a scenario where the model is employed to design novel drug molecules. It suggests a molecule with a unique cyclic structure, known to effectively bind with certain drug targets. In an attempt to boost structural robustness, the model introduces an additional side chain. However, this addition, despite seemingly increasing stability, actually interferes with the molecule's intended target interaction, leading to its ineffectiveness. This situation exemplifies "molecular hallucination", where the structural enhancements made by the model do not translate into functional success.
Definition 1. Molecular hallucinations refer to molecules generated by language models that comply with chemical structural rules, yet fail to exhibit practical utility or the anticipated properties.</p>
<p>Such hallucinations can hinder drug discovery efficiency, escalate costs, and compromise the realworld applicability of the model. Moreover, an abundance of hallucinated molecules may overshadow truly promising molecular structures. To alleviate "molecular hallucinations", we propose a strategy that can effectively gauge and rectify the quality of generated molecular structures. This chemical feedback paradigm ensures that produced molecules are not only syntactically correct but also of high practical utility. Specifically, as illustrated in Figure 2, we align the model's probabilistic rankings of diverse molecular responses with preference rankings observed in actual chemical contexts.</p>
<p>The measure of anticipated chemical preference, denoted as $\operatorname{Ps}(\cdot)$, can be characterized in various ways; in this study, we define it based on the property score. Given a molecule $S=\left{s_{1}, \cdots, s_{l}\right}$, we can generate a set of candidate SELFIES $\mathcal{S}^{<em>}$ with distinct property scores using our pre-trained molecular language model. For each $\left(S_{i}, S_{j}\right)$ pair in $\mathcal{S}^{</em>}$ that satisfies $\operatorname{Ps}\left(S_{i}\right)&gt;\operatorname{Ps}\left(S_{j}\right)$, we expect:</p>
<p>$$
p_{\text {true }}\left(S_{i} \mid S\right)&gt;p_{\text {true }}\left(S_{j} \mid S\right), \quad \forall S_{i}, S_{j} \in \mathcal{S}^{*}, \operatorname{Ps}\left(S_{i}\right)&gt;\operatorname{Ps}\left(S_{j}\right)
$$</p>
<p>To incentivize the model to assign higher probabilities to candidates with desired properties, we utilize a rank loss (Liu et al., 2022). The rank loss arises when candidates with suboptimal properties obtain higher estimated probabilities compared to those with commendable properties:</p>
<p>$$
\mathcal{L}<em i="i">{\text {rank }}(S)=\sum</em>\right)
$$} \sum_{j&gt;i} \max \left(0, f\left(S_{j}\right)-f\left(S_{i}\right)+\gamma_{i j}\right), \quad \forall i<j, \operatorname{Ps}\left(S_{i}\right)>\operatorname{Ps}\left(S_{j</p>
<p>where $\gamma_{i j}=(j-i) * \gamma$ represents the margin multiplied by the difference in rank between the candidates, and $f(S)=\sum_{t=1}^{t} \log p_{\theta}\left(s_{t} \mid S, S_{&lt;t} ; \theta\right)$ denotes the estimated log-probability provided by our pre-trained model with parameters $\theta$. Consequently, we furnish chemical feedback to align the pre-trained model with the chemical preference, without necessitating any supplementary reference data. Unlike supervised fine-tuning, which may still be susceptible to hallucinations due to its reliance on ideal samples, chemical feedback equips the model with a broader perspective. It educates the model on both the commendable and the suboptimal, leading to more informed generation.</p>
<p>Nonetheless, fine-tuning the model solely with sequence-level coordination may diminish its generative capability. To ensure the model retains its generative prowess while optimizing for desired properties, we strike a balance by merging the sequence-level rank loss with token-level cross-entropy loss. The overall loss function is formulated as follows:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em _rank="{rank" _text="\text">{\mathrm{ce}}+\alpha \mathcal{L}</em>
$$}</p>
<p>where $\alpha$ is the weight of the rank loss. In practice, we leverage label smoothing (Szegedy et al., 2016) to transform the target distribution $p_{\text {true }}$ (Eq. 2) in $\mathcal{L}_{\text {ce }}$ (Eq. 1) to a "soft" label, allocating probability mass $\beta$ to other tokens in the alphabet of length $N$ :</p>
<p>$$
p_{\text {true }}\left(s \mid S, S_{&lt;j}\right)= \begin{cases}1-\beta, &amp; s=s_{j} \ \frac{\beta}{\beta-1}, &amp; s \neq s_{j}\end{cases}
$$</p>
<p>Overall, the cross-entropy loss serves as a normalization, complementing the rank loss by ensuring that the model allocates a balanced probability mass throughout the sequence. MOLGEN autonomously steer its learning and optimization paths based on the evaluations of molecules it generates. This cycle of generation and adjustment within the model epitomizes a self-reflective system, even as it incorporates an external scoring function to refine and validate its assessments.</p>
<h1>3 EXPERIMENTS</h1>
<h3>3.1 EXPERIMENTAL SETUP</h3>
<p>In the first stage of pre-training, we randomly select over 100 million unlabelled molecules from the publicly available ZINC-15 dataset (Sterling \&amp; Irwin, 2015), which is the same corpus used in Irwin et al. (2022). The chosen molecules meet specific criteria: they're reactive, available for purchase, have a molecular weight of $\leq 500$ Daltons, and a LogP (octanol-water partition coefficient) of $\leq$ 5. The second stage includes 2.22 million molecules spanning both synthetic (Irwin et al., 2012; Polykovskiy et al., 2018) and natural product domains (Zhao et al., 2023). In the downstream tasks, as detailed in the following section, we thoroughly investigate the model's capabilities from two perspectives. More information on dataset and experimental procedures are in Appendices C and G.</p>
<h3>3.2 MAIN RESULTS</h3>
<h3>3.2.1 MOLGEN CAPTURES REAL-WORLD MOLECULAR DISTRIBUTIONS</h3>
<p>An essential capability for any molecular generation model is to capture the molecular distribution and generate diverse and realistic molecules. Such capabilities are paramount when constructing virtual libraries to advance computer-aided drug discovery endeavors (van Hilten et al., 2019). By leveraging a set of compounds, either manually or automatically selected, these models are designed to expand datasets significantly, all the while retaining the implicit structural and chemical patterns inherent to the reference set. In this section, we use seven well-established metrics, detailed in Appendix G, to evaluate the proficiency of models in generating molecules that conform to the distribution of real-world molecules. We generate 10,000 synthetic molecules following the setting in Polykovskiy et al. (2018), and 80,000 natural product molecules based on the pre-trained MolGen.</p>
<p>Table 1 reveals the following observations: (i) MOLGEN demonstrates a remarkable ability to produce valid molecules without the need for additional valency checks, as required by JT-VAE (Jin et al., 2018). Since LIMO (Eckmann et al., 2022) also employs SELFIES, the generated molecules maintain $100 \%$ validity. However, the inherent complexity of natural product scaffolds presents a significant challenge for most models, resulting in a failure to produce valid molecules. The better performance of Chemformer (Irwin et al., 2022) can be attributed to its proficiency in learning SMILES grammar</p>
<p>Table 1: Molecular distribution learning performance on two molecule domains. The cells in highlight denote the best results garnered by MolGen and the peak performance achieved by the baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Synthetic Molecules</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Validity $\uparrow$</td>
<td style="text-align: center;">Frag $\uparrow$</td>
<td style="text-align: center;">Scal $\uparrow$</td>
<td style="text-align: center;">SNN $\uparrow$</td>
<td style="text-align: center;">IntDiv $\uparrow$</td>
<td style="text-align: center;">FCD $\downarrow$</td>
<td style="text-align: center;">Novelty $\uparrow$</td>
<td style="text-align: center;">Validity $\uparrow$</td>
<td style="text-align: center;">Frag $\uparrow$</td>
<td style="text-align: center;">Scal $\uparrow$</td>
<td style="text-align: center;">SNN $\uparrow$</td>
<td style="text-align: center;">IntDiv $\uparrow$</td>
<td style="text-align: center;">FCD $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">AAE</td>
<td style="text-align: center;">.9368</td>
<td style="text-align: center;">.9910</td>
<td style="text-align: center;">.9022</td>
<td style="text-align: center;">.6081</td>
<td style="text-align: center;">.8557</td>
<td style="text-align: center;">.5555</td>
<td style="text-align: center;">.7931</td>
<td style="text-align: center;">.0082</td>
<td style="text-align: center;">.9687</td>
<td style="text-align: center;">.2638</td>
<td style="text-align: center;">.3680</td>
<td style="text-align: center;">.8704</td>
<td style="text-align: center;">4.109</td>
</tr>
<tr>
<td style="text-align: center;">LatentGAN</td>
<td style="text-align: center;">.8966</td>
<td style="text-align: center;">.9986</td>
<td style="text-align: center;">.8867</td>
<td style="text-align: center;">.5132</td>
<td style="text-align: center;">.8565</td>
<td style="text-align: center;">.2968</td>
<td style="text-align: center;">.9498</td>
<td style="text-align: center;">.9225</td>
<td style="text-align: center;">.2771</td>
<td style="text-align: center;">.0884</td>
<td style="text-align: center;">.5321</td>
<td style="text-align: center;">.6009</td>
<td style="text-align: center;">45.53</td>
</tr>
<tr>
<td style="text-align: center;">ClustRNN</td>
<td style="text-align: center;">.9748</td>
<td style="text-align: center;">.9998</td>
<td style="text-align: center;">.9242</td>
<td style="text-align: center;">.6015</td>
<td style="text-align: center;">.8562</td>
<td style="text-align: center;">.0732</td>
<td style="text-align: center;">.8419</td>
<td style="text-align: center;">.7351</td>
<td style="text-align: center;">.8816</td>
<td style="text-align: center;">.5212</td>
<td style="text-align: center;">.4179</td>
<td style="text-align: center;">.8756</td>
<td style="text-align: center;">2.212</td>
</tr>
<tr>
<td style="text-align: center;">VAE</td>
<td style="text-align: center;">.9767</td>
<td style="text-align: center;">.9994</td>
<td style="text-align: center;">.9386</td>
<td style="text-align: center;">.6257</td>
<td style="text-align: center;">.8558</td>
<td style="text-align: center;">.0990</td>
<td style="text-align: center;">.6949</td>
<td style="text-align: center;">.3627</td>
<td style="text-align: center;">.8840</td>
<td style="text-align: center;">.4563</td>
<td style="text-align: center;">.3950</td>
<td style="text-align: center;">.8719</td>
<td style="text-align: center;">4.318</td>
</tr>
<tr>
<td style="text-align: center;">JT-VAE</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">.9965</td>
<td style="text-align: center;">.8964</td>
<td style="text-align: center;">.5477</td>
<td style="text-align: center;">.8551</td>
<td style="text-align: center;">.3954</td>
<td style="text-align: center;">.9143</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">.8798</td>
<td style="text-align: center;">.5012</td>
<td style="text-align: center;">.3748</td>
<td style="text-align: center;">.8743</td>
<td style="text-align: center;">12.03</td>
</tr>
<tr>
<td style="text-align: center;">LIMO</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">.9562</td>
<td style="text-align: center;">.1073</td>
<td style="text-align: center;">.6125</td>
<td style="text-align: center;">.8544</td>
<td style="text-align: center;">.1532</td>
<td style="text-align: center;">.8956</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">.7242</td>
<td style="text-align: center;">.0005</td>
<td style="text-align: center;">.3416</td>
<td style="text-align: center;">.7726</td>
<td style="text-align: center;">31.84</td>
</tr>
<tr>
<td style="text-align: center;">ChEMFORMER</td>
<td style="text-align: center;">.9843</td>
<td style="text-align: center;">.9889</td>
<td style="text-align: center;">.9248</td>
<td style="text-align: center;">.5622</td>
<td style="text-align: center;">.8553</td>
<td style="text-align: center;">.0061</td>
<td style="text-align: center;">.9581</td>
<td style="text-align: center;">.9835</td>
<td style="text-align: center;">.9826</td>
<td style="text-align: center;">.4126</td>
<td style="text-align: center;">.5875</td>
<td style="text-align: center;">.8650</td>
<td style="text-align: center;">.8546</td>
</tr>
<tr>
<td style="text-align: center;">MolGen</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">.9999</td>
<td style="text-align: center;">.9999</td>
<td style="text-align: center;">.9996</td>
<td style="text-align: center;">.8567</td>
<td style="text-align: center;">.0015</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">.9994</td>
<td style="text-align: center;">.8404</td>
<td style="text-align: center;">.8148</td>
<td style="text-align: center;">.8878</td>
<td style="text-align: center;">.6519</td>
</tr>
</tbody>
</table>
<p>during large-scale pre-training, highlighting the importance of pre-training. (ii) For the synthetic datasets, most models generate molecules with comparable fragments (Frag) and scaffolds (Scal) to those of the reference molecules. MolGen excels at capturing substructure distributions in natural products, outperforming other models. (iii) MolGen exhibits the highest SNN and lowest FCD scores, indicating its excellent ability to master the dataset statistics in terms of both biological properties and topological structures. Moreover, its strong performance in IntDiv and Novelty metrics suggests that MolGen is well-suited for discovering new chemical structures and exploring unknown chemical space without overfitting. A visual comparison of the training set and generated molecules is presented in Appendix H.1.</p>
<h1>3.2.2 MolGen Mitigates Molecular Hallucinations</h1>
<p>Addressing the issue of "molecular hallucinations" has been a long-standing challenge in the realm of computer-aided molecular design. In this section, we delve into the prowess of MolGen in tackling this challenge and primarily focus on two types of experiments: targeted molecule discovery and constrained molecular optimization. Unlike the molecular distribution learning task, where we only rely on the pre-trained model, here we incorporate the chemical feedback paradigm to align the model with genuine chemical preferences. Specifically, we adopt the penalized $\log \mathrm{P}(\mathrm{p}-\log \mathrm{P})$ (Jin et al., 2018), QED (Bickerton et al., 2012) and binding affinity to two protein targets as our optimization criteria, as detailed in Appendix G.</p>
<p>Table 2: Comparison of QED and penalized logP maximization methods on synthetic molecules. indicates output length limit (maximum molecule length of ZINC250K), while $\odot$ means no limit. The first row summarizes the top 3 property scores from the ZINC250K dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Penalized LogP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">QED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1st</td>
<td style="text-align: center;">2nd</td>
<td style="text-align: center;">3rd</td>
<td style="text-align: center;">1st</td>
<td style="text-align: center;">2nd</td>
<td style="text-align: center;">3rd</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZINC250K</td>
<td style="text-align: center;">4.52</td>
<td style="text-align: center;">4.30</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">GCPN</td>
<td style="text-align: center;">7.98</td>
<td style="text-align: center;">7.85</td>
<td style="text-align: center;">7.80</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.946</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MolDQN</td>
<td style="text-align: center;">11.80</td>
<td style="text-align: center;">11.80</td>
<td style="text-align: center;">11.80</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.943</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LIMO</td>
<td style="text-align: center;">10.50</td>
<td style="text-align: center;">9.69</td>
<td style="text-align: center;">9.60</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.945</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">30.51</td>
<td style="text-align: center;">28.98</td>
<td style="text-align: center;">28.95</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JT-VAE</td>
<td style="text-align: center;">5.30</td>
<td style="text-align: center;">4.93</td>
<td style="text-align: center;">4.49</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.911</td>
<td style="text-align: center;">0.910</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphAF</td>
<td style="text-align: center;">12.23</td>
<td style="text-align: center;">11.29</td>
<td style="text-align: center;">11.05</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.947</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphDF</td>
<td style="text-align: center;">13.70</td>
<td style="text-align: center;">13.18</td>
<td style="text-align: center;">13.17</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MARS</td>
<td style="text-align: center;">44.99</td>
<td style="text-align: center;">44.32</td>
<td style="text-align: center;">43.81</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MolGen</td>
<td style="text-align: center;">80.30</td>
<td style="text-align: center;">74.70</td>
<td style="text-align: center;">69.85</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.948</td>
</tr>
</tbody>
</table>
<p>Table 3: The top 3 highest binding affinities (i.e., lowest dissociation constants, $K_{D}$, as estimated with AutoDockGPU (Santos-Martins et al., 2021)) from a total of 10 k generated molecules for each method.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">ESR1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ACAA1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1st</td>
<td style="text-align: center;">2nd</td>
<td style="text-align: center;">3rd</td>
<td style="text-align: center;">1st</td>
<td style="text-align: center;">2nd</td>
<td style="text-align: center;">3rd</td>
</tr>
<tr>
<td style="text-align: center;">GCPN</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: center;">MolDQN</td>
<td style="text-align: center;">373</td>
<td style="text-align: center;">588</td>
<td style="text-align: center;">1062</td>
<td style="text-align: center;">240</td>
<td style="text-align: center;">337</td>
<td style="text-align: center;">608</td>
</tr>
<tr>
<td style="text-align: center;">GraphDF</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">370</td>
<td style="text-align: center;">520</td>
<td style="text-align: center;">590</td>
</tr>
<tr>
<td style="text-align: center;">MARS</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">163</td>
<td style="text-align: center;">203</td>
<td style="text-align: center;">236</td>
</tr>
<tr>
<td style="text-align: center;">LIMO</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">41</td>
</tr>
<tr>
<td style="text-align: center;">MolGen</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">3.36</td>
<td style="text-align: center;">3.98</td>
<td style="text-align: center;">8.50</td>
</tr>
</tbody>
</table>
<p>Targeted molecule discovery focuses on generating novel molecules with superior chemical properties. To evaluate model effectiveness, we first present the top-3 property scores of molecules generated on the synthetic dataset in Table 2, following conventions from prior studies (Shi et al., 2020b; Eckmann et al., 2022). It's essential to note that the p-logP score tends to increase linearly with molecule length (Xie et al., 2021; Eckmann et al., 2022). To ensure a fair comparison, we categorize the baselines into two groups. MolGen, due to its ability to handle variable-length output, is evaluated under both configurations.</p>
<p>In Table 2, MolGen outperforms all baselines in p-logP score and achieves comparable results for QED, indicating the effectiveness of the chemical feedback paradigm in promoting desired molecule probabilities. Further evidence of MolGen's capabilities can be found in the results for natural products in Appendix H.2. Given that a mere $0.701 \%$ of molecules in our reference set achieve a QED score above 0.9 (with a peak score of 0.9439 , as detailed in Appendix C), MolGen's achievement of a 0.9478 score highlights its potential in drug discovery. Moreover, the model's ability to produce molecules with a p-logP score of 54.33 , substantially exceeding the reference set's high of 17.69 .</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Optimizing ligand binding affinity using MolGen. (a) 3D visualization of ligands with the highest binding affinities docked against ESR1 (top row) and ACAA1 (bottom row). The protein pocket is displayed semi-opaque, and the 2D molecular structure of the ligand is shown in the bottom right corner. (b) Examples of binding affinity improvement for protein targets ESR1 (top row) and ACAA1 (bottom row).</p>
<p>Moving beyond basic properties, we tackle a more realistic challenge: generating molecules with high binding affinity towards target proteins. Binding affinity quantifies the potency of interactions between a molecule and its intended protein target. Our investigations primarily target the binding sites of two human proteins: the estrogen receptor (PDB ESR1, UniProt P03372) and the peroxisomal acetyl-CoA acyl transferase 1 (PDB ACAA1, UniProt P09110). A detailed exploration of these proteins is available in Appendix G. As shown in Table 3, MolGen surpasses prior methods in enhancing binding affinities. Figure 4 (a) illustrates exemplary optimal ligands. To delve deeper into MolGen's optimization capability, we undertook an optimization for the 1,000 molecules with the lowest affinities for each protein receptor. Figure 4 (b) offers a comparative visualization of affinity advancements pre- and post-optimization, achieving overall relative improvements of 96.7% for ESR1 and 70.4% for ACAA1. These results illuminate MolGen's versatility in both targeted optimization of simpler properties and the more complex domain of molecular docking.</p>
<p>Table 4: Mean (and standard deviation) penalized logP improvement of generated molecules compared to inputs with different similarity constraints.</p>
<table>
<thead>
<tr>
<th>MODEL</th>
<th>IMPROVEMENT</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>δ = 0.6</td>
<td>δ = 0.4</td>
</tr>
<tr>
<td>JT-VAE</td>
<td>0.28 (0.79)</td>
<td>1.03 (1.39)</td>
</tr>
<tr>
<td>GCPN</td>
<td>0.79 (0.63)</td>
<td>2.49 (1.30)</td>
</tr>
<tr>
<td>MOLDQN</td>
<td>1.86 (1.21)</td>
<td>3.37 (1.62)</td>
</tr>
<tr>
<td>VSEQ2SEQ</td>
<td>2.33 (1.17)</td>
<td>3.37 (1.75)</td>
</tr>
<tr>
<td>VJTNN</td>
<td>2.33 (1.24)</td>
<td>3.55 (1.67)</td>
</tr>
<tr>
<td>GA</td>
<td>3.44 (1.09)</td>
<td>5.93 (1.41)</td>
</tr>
<tr>
<td>GRAPHAF</td>
<td>4.98 (6.49)</td>
<td>8.21 (6.51)</td>
</tr>
<tr>
<td>GRAPHDF</td>
<td>4.51 (5.80)</td>
<td>9.19 (6.45)</td>
</tr>
<tr>
<td>LIMO</td>
<td>1.80 (2.00)</td>
<td>3.60 (2.30)</td>
</tr>
<tr>
<td>CHEMPORMER</td>
<td>2.48 (0.89)</td>
<td>3.56 (1.32)</td>
</tr>
<tr>
<td>RETMOL</td>
<td>3.78 (3.29)</td>
<td>11.55 (11.27)</td>
</tr>
<tr>
<td>RT</td>
<td>2.21 (1.30)</td>
<td>3.16 (1.50)</td>
</tr>
<tr>
<td>MOLGEN</td>
<td>12.08 (0.82)</td>
<td>12.35 (1.21)</td>
</tr>
</tbody>
</table>
<p>Constrained molecular optimization aims to modify a given molecule to improve desired properties while satisfying a similarity constraint (denoted as δ). Following previous studies (Jin et al., 2018; Shi et al., 2020b; Luo et al., 2021; Eckmann et al., 2022), we optimize 800 molecules from the ZINC250K dataset that exhibit the lowest p-logP scores. To assess the similarity between the optimized and original molecules, we utilize the Tanimoto similarity with Morgan fingerprints (Rogers &amp; Hahn, 2010).</p>
<p>In Table 4, MOLGEN yields superior results under both similarity constraints, illustrating its prowess in scouring the proximate chemical space for molecules with higher property scores. MOLGEN's performance, surpassing models that employ additional reward functions, property predictors, and retrieval databases, confirms that equipping the model with the ability to discern chemical preference is instrumental in alleviating "<em>molecular hallucinations</em>".</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Illustrations of constrained optimization based on QED score within the natural products.</p>
<p>To further probe MOLGEN's capabilities, we expand our constrained optimization experiments to include QED scores for synthetic molecules and both properties for natural products. Figure 5 showcases examples of QED score optimization on natural products. These instances reveal that despite the complex molecular structure and elongated length of natural products, MOLGEN can elevate the property score whilst sustaining a degree of similarity between the input and the modified molecule. Moreover, MOLGEN preserves the diversity</p>
<p>of the generated molecules as it explores the nearby chemical space. Additional visual validations are provided in Appendix H.3.</p>
<h1>3.3 A Closer Look at MolGen</h1>
<p>To dissect the potential of MolGen, we devise experiments from different perspectives.</p>
<h3>3.3.1 Pre-training Stage Captures Complex Molecular Characteristics</h3>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Comparative analysis of molecules generated by different models with respect to properties, atom counts, ring counts, molecular weights, and Bertz complexity (S-PLM stands for Smiles-based PLM).</p>
<p>To understand the differences in property distributions and molecular structures learned during the pre-training phase, we compare the pre-trained MolGen with the most popular deep generative Graph-based (Jin et al., 2018), VAEbased (Blaschke et al., 2018), and Smilesbased language models (Irwin et al., 2022). For this assessment, the training and generation configurations of all models align with the molecular distribution learning task on the synthetic MOSES dataset.</p>
<p>As shown in the 2D histograms of p-logP and QED scores in Figure 6, both VAEbased and Smiles-based PLMs tend to produce molecules with larger p-logP and QED scores than the training data. In comparison, the Graph-based model learns the main mode of p-logP in the training data, while MolGen exhibits a slightly superior performance - analogous outcomes are observed for QED. Furthermore, in terms of molecular topology, PLMs outperform others in perceiving atom numbers, ring numbers, and molecular weights, with MolGen producing a slightly closer match to the training distribution. All the models are proficient at picking up on molecular Bertz complexity. PLMs, particularly MolGen, demonstrate the capacity to capture the properties and structural attributes of the training molecules while maintaining generational diversity.</p>
<h3>3.3.2 Chemical Feedback Paradigm Facilitates Property Optimization</h3>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Property variations across different MolGen configurations.
As part of our investigation, we conduct an ablation study to examine the role of the chemical feedback paradigm in mitigating "molecular hallucinations". Starting from a batch of molecules from the domains of natural products and synthetic compounds, Figure 7 portrays the variations in property scores of molecules generated by different model configurations. A more comprehensive view of these variations is provided in Appendix H.2.</p>
<p>Without the chemical feedback, the PLM tends to generate molecules with property scores closely resembling those of the initial molecules. This can be attributed to the absence of a guiding signal, leaving the model to rely heavily on its learned patterns from the training data. However, once the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Meaningful substructure attention exploration. Visualization of attention maps learned by various PLM models for the same molecule (left), and substructure attention level of the three models (right). All models being compared are of a similar parameter scale for consistency.
chemical feedback mechanism is integrated, we witness an increase in property scores from the initial to the concluding groups. This underscores the pivotal role of chemical feedback: it furnishes the model with immediate feedback on its performance in generating molecules with the chemical preference, thus steering its outputs towards the desired objectives and alleviating the hallucinations.</p>
<h1>3.3.3 MolGen Implicitly Comprehends Molecular Substructures</h1>
<p>In this section, we investigate PLMs' ability to implicitly discern essential substructures when leveraging different molecular languages (SMILES and SELFIES). For a more intuitive comprehension, we visualize the attention weights of each token within an identical molecule. Specifically, we extract and normalize the attention weights from the final self-attention layer, as depicted in Figure 8.</p>
<p>The attention map generated by MolGen shows that the fluoro group garners the highest attention weights, followed by the phenyl and hydroxyl groups. This stems from the fluoro group's exceptional electron-capturing capabilities, significantly influencing the molecule's polarity. Meanwhile, the phenyl group constitutes a common organic functional group, and the hydroxyl group substantially impacts the intermolecular force between the molecule and water. Leveraging domain-agnostic molecular prefixes, MolGen directs its attention more efficiently towards these pivotal substructures. These prefixes, acting as domain instructors, enhance the model's adaptability across diverse molecular domains, steering attention away from less essential substructures. Conversely, SmILES-based PLM might divert attention to symbols or numbers devoid of intrinsic chemical significance. Evidently, by employing a precise vocabulary free from such distractions, MolGen maintains a clear and implicit understanding of molecular substructures. Further visualizations and analyses supporting this observation are available in Appendix F and H.4.</p>
<p>To objectively measure the model's focus on vital substructures, we propose a metric termed "Substructure Attention Level (SAL)". This metric is determined by the percentage of attention scores allocated to meaningful substructure tokens within a molecule. Higher SAL scores indicate a stronger focus on meaningful substructures. For effective evaluation, we intentionally select 200 molecules from PubChem, characterized by their simpler structures containing only 1-2 functional groups. This selection criterion ensures that the model's attention isn't diluted across excessively intricate structures, allowing for a clearer reflection of its focus on specific functional groups. The box and distribution plots in Figure 8 vividly depict the SAL of the three PLMs. In line with visualization results, both versions of MolGen surpass the SMILES-based PLM, underscoring MolGen's superior concentration on meaningful substructures. The prefix-enhanced MolGen exhibits a slight edge, highlighting the prefix's role in enhancing attentiveness.</p>
<h2>4 CONCLUSION AND Future Work</h2>
<p>In this work, we propose MolGen, a pre-trained molecular language model specifically tailored for molecule generation. Our in-depth study on MolGen confirms its proficiency in generating molecules with chemical preferences while avoiding "molecular hallucinations". Furthermore, our model shows potential in identifying essential molecular substructures. Interesting future directions include: $i$ ) applying MolGen to other tasks such as retrosynthesis and reaction prediction (Shi et al., 2020a), ii) exploring multimodal pre-training like Edwards et al. (2022); Su et al. (2022); Fang et al. (2024), iii) incorporating additional sources of knowledge. We make our pre-trained model, code, and data publicly available, in the hope that our work will foster future research in the field.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We would like to express gratitude to the anonymous reviewers for kind comments. This work was supported by the National Natural Science Foundation of China (No. 62206246), the Fundamental Research Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), CAAI-Huawei MindSpore Open Fund, Yongjiang Talent Introduction Programme (2021A-156-G), CCF-Baidu Open Fund, and Information Technology Center and State Key Lab of CAD\&amp;CG, Zhejiang University.</p>
<h2>REPRODUCIbILITY STATEMENT</h2>
<p>All data, code, and model weights can be found in the Supplementary Materials. For a detailed description of the dataset, please refer to Appendix C. For specific experimental settings, please see Appendix G.</p>
<h2>ETHICS STATEMENT</h2>
<p>This study was carried out in strict accordance with ethical guidelines and best practices in research. The data utilized were sourced from publicly available datasets, and no proprietary or confidential data were used. This study does not involve any ethical issues.</p>
<h2>REFERENCES</h2>
<p>Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization with genetic exploration. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/8ba6c657b03fc7c8dd4dff8e45defcd2-Abstract.html.</p>
<p>Atanas G Atanasov, Sergey B Zotchev, Verena M Dirsch, and Claudiu T Supuran. Natural products in drug discovery: advances and opportunities. Nature reviews Drug discovery, 20(3):200-216, 2021.</p>
<p>Viraj Bagal, Rishal Aggarwal, P. K. Vinod, and U. Deva Priyakumar. Molgpt: Molecular generation using a transformer-decoder model. J. Chem. Inf. Model., 62(9):2064-2076, 2022. doi: 10.1021/ ACS.JCIM.1C00600. URL https://doi.org/10.1021/acs.jcim.1c00600.</p>
<p>G Richard Bickerton, Gaia V Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90-98, 2012.</p>
<p>Thomas Blaschke, Marcus Olivecrona, Ola Engkvist, Jürgen Bajorath, and Hongming Chen. Application of generative autoencoder in de novo molecular design. Molecular informatics, 37(1-2): 1700123, 2018.</p>
<p>Jannis Born and Matteo Manica. Regression transformer enables concurrent sequence regression and generation for molecular language modelling. Nat. Mac. Intell., 5(4):432-444, 2023. doi: 10.1038/ S42256-023-00639-Z. URL https://doi.org/10.1038/s42256-023-00639-z.</p>
<p>Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. CoRR, abs/1805.11973, 2018. URL http://arxiv.org/abs/1805.11973.</p>
<p>Gayane Chilingaryan, Hovhannes Tamoyan, Ani Tevosyan, Nelly Babayan, Lusine Khondkaryan, Karen Hambardzumyan, Zaven Navoyan, Hrant Khachatrian, and Armen Aghajanyan. Bartsmiles: Generative masked language models for molecular representations. CoRR, abs/2211.16349, 2022. doi: 10.48550/arXiv.2211.16349. URL https://doi.org/10.48550/arXiv. 2211.16349.</p>
<p>Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. Molgensurvey: A systematic survey in machine learning models for molecule design. CoRR, abs/2203.14500, 2022a. doi: 10.48550/ ARXIV.2203.14500. URL https://doi.org/10.48550/arXiv.2203.14500.</p>
<p>Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. Molgensurvey: A systematic survey in machine learning models for molecule design. CoRR, abs/2203.14500, 2022b. doi: 10.48550/ ARXIV.2203.14500. URL https://doi.org/10.48550/arXiv.2203.14500.</p>
<p>Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose. Neural path hunter: Reducing hallucination in dialogue systems via path grounding. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 2197-2214. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN.168. URL https://doi.org/10.18653/ v1/2021.emnlp-main. 168 .</p>
<p>Peter Eckmann, Kunyang Sun, Bo Zhao, Mudong Feng, Michael K. Gilson, and Rose Yu. LIMO: latent inceptionism for targeted molecule generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 5777-5792. PMLR, 2022. URL https:// proceedings.mlr.press/v162/eckmann22a.html.</p>
<p>Carl Edwards, Tuan Manh Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In EMNLP, pp. 375-413. Association for Computational Linguistics, 2022. URL https://doi.org/10.18653/v1/2022.emnlp-main. 26.</p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. In ICLR. OpenReview.net, 2024. URL https://openreview.net/pdf? id=T1sdsb619n.</p>
<p>Daniel Flam-Shepherd, Kevin Zhu, and Alán Aspuru-Guzik. Language models can learn complex molecular distributions. Nature Communications, 13(1):1-10, 2022.</p>
<p>Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268-276, 2018.</p>
<p>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.</p>
<p>John J. Irwin, Teague Sterling, Michael M. Mysinger, Erin S. Bolstad, and Ryan G. Coleman. ZINC: A free tool to discover chemistry for biology. J. Chem. Inf. Model., 52(7):1757-1768, 2012. doi: 10.1021/CI3001277. URL https://doi.org/10.1021/ci3001277.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained transformer for computational chemistry. Mach. Learn. Sci. Technol., 3(1):15022, 2022. doi: 10.1088/2632-2153/AC3FFB. URL https://doi.org/10.1088/2632-2153/ac3ffb.</p>
<p>Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):3567-3572, 2019.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12):248:1-248:38, 2023a. doi: 10.1145/3571730. URL https://doi.org/10. $1145 / 3571730$.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12):248:1-248:38, 2023b. doi: 10.1145/3571730. URL https://doi.org/10. $1145 / 3571730$.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Junction tree variational autoencoder for molecular graph generation. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 23282337. PMLR, 2018. URL http://proceedings.mlr.press/v80/jin18a.html.</p>
<p>Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi S. Jaakkola. Learning multimodal graph-to-graph translation for molecule optimization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=B1xJAsA5F7.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 4839-4848. PMLR, 2020. URL http://proceedings.mlr.press/v119/ jin20a.html.</p>
<p>Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alán Aspuru-Guzik. Self-referencing embedded strings (SELFIES): A 100\% robust molecular string representation. Mach. Learn. Sci. Technol., 1(4):45024, 2020. doi: 10.1088/2632-2153/ABA947. URL https: //doi.org/10.1088/2632-2153/aba947.</p>
<p>Mario Krenn, Qianxiang Ai, Senja Barthel, Nessa Carson, Angelo Frei, Nathan C. Frey, Pascal Friederich, Théophile Gaudin, Alberto Alexander Gayle, Kevin Maik Jablonka, Rafael F. Lameiro, Dominik Lemm, Alston Lo, Seyed Mohamad Moosavi, José Manuel Nápoles-Duarte, AkshatKumar Nigam, Robert Pollice, Kohulan Rajan, Ulrich Schatzschneider, Philippe Schwaller, Marta Skreta, Berend Smit, Felix Strieth-Kalthoff, Chong Sun, Gary Tom, Guido Falk von Rudorff, Andrew Wang, Andrew D. White, Adamo Young, Rose Yu, and Alán Aspuru-Guzik. SELFIES and the future of molecular string representations. Patterns, 3(10):100588, 2022. doi: 10.1016/J. PATTER.2022.100588. URL https://doi.org/10.1016/j.patter.2022.100588.</p>
<p>Matt J. Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational autoencoder. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1945-1954. PMLR, 2017. URL http:// proceedings.mlr.press/v70/kusner17a.html.</p>
<p>Youngchun Kwon, Seokho Kang, Youn-Suk Choi, and Inkoo Kim. Evolutionary design of molecules based on deep learning and a genetic algorithm. Scientific reports, 11(1):1-11, 2021.</p>
<p>Greg Landrum. Rdkit documentation. Release, 1(1-79):4, 2013.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7871-7880. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.703. URL https://doi.org/10.18653/v1/2020.acl-main.703.</p>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 4582-4597. Association for Computational Linguistics, 2021. doi: $10.18653 / \mathrm{v} 1 / 2021$.acl-long.353. URL https://doi.org/10.18653/v1/2021. acl-long. 353 .</p>
<p>Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Constrained graph variational autoencoders for molecule design. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 7806-7815, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ b8a03c5c15fcfa8dae0b03351eb1742f-Abstract.html.</p>
<p>Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. BRIO: bringing order to abstractive summarization. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 2890-2903. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.207. URL https://doi.org/10.18653/v1/2022.acl-long. 207.</p>
<p>Youzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: A discrete flow model for molecular graph generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7192-7203. PMLR, 2021. URL http: //proceedings.mlr.press/v139/luo21a.html.</p>
<p>Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via regularizing variational autoencoders. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 7113-7124, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 1458e7509aa5f47ecfb92536e7ddldc7-Abstract.html.</p>
<p>Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient language model tuning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 6253-6264. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.433. URL https://doi.org/10.18653/v1/2022. acl-long. 433 .</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. On faithfulness and factuality in abstractive summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 1906-1919. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.173. URL https://doi.org/10. 18653/v1/2020.acl-main. 173.</p>
<p>AkshatKumar Nigam, Pascal Friederich, Mario Krenn, and Alán Aspuru-Guzik. Augmenting genetic algorithms with deep neural networks for exploring the chemical space. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=H1lmyRNFvr.</p>
<p>Jie Pan. Large language model for molecular chemistry. Nature Computational Science, pp. 1-1, 2023.</p>
<p>Pavel G. Polishchuk, Timur I. Madzhidov, and Alexandre Varnek. Estimation of the size of drug-like chemical space based on GDB-17 data. J. Comput. Aided Mol. Des., 27(8):675-679, 2013. doi: 10. 1007/S10822-013-9672-4. URL https://doi.org/10.1007/s10822-013-9672-4.</p>
<p>Daniil Polykovskiy, Alexander Zhebrak, Benjamín Sánchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Sergey I. Nikolenko, Alán Aspuru-Guzik, and Alex Zhavoronkov. Molecular sets (MOSES): A benchmarking platform for molecular generation models. CoRR, abs/1811.12823, 2018. URL http://arxiv.org/abs/1811.12823.</p>
<p>Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. Science advances, 4(7):eaap7885, 2018.</p>
<p>Mariya Popova, Mykhailo Shvets, Junier Oliva, and Olexandr Isayev. Molecularrnn: Generating realistic molecular graphs with optimized properties. CoRR, abs/1905.13372, 2019. URL http : //arxiv.org/abs/1905.13372.</p>
<p>Vipula Rawte, Amit P. Sheth, and Amitava Das. A survey of hallucination in large foundation models. CoRR, abs/2309.05922, 2023. doi: 10.48550/ARXIV.2309.05922. URL https : //doi.org/10.48550/arXiv.2309.05922.</p>
<p>Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc. Natl. Acad. Sci. USA, 118(15):e2016239118, 2021. doi: 10.1073/PNAS.2016239118. URL https://doi . org/10.1073/pnas.2016239118.</p>
<p>David Rogers and Mathew Hahn. Extended-connectivity fingerprints. J. Chem. Inf. Model., 50(5):742754, 2010. doi: 10.1021/CI100050T. URL https://doi.org/10.1021/ci100050t.</p>
<p>Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel Das. Large-scale chemical language representations capture molecular structure and properties. Nat. Mac. Intell., 4(12):1256-1264, 2022. doi: 10.1038/S42256-022-00580-7. URL https : //doi.org/10.1038/s42256-022-00580-7.</p>
<p>Diogo Santos-Martins, Leonardo Solis-Vasquez, Andreas F Tillack, Michel F Sanner, Andreas Koch, and Stefano Forli. Accelerating autodock4 with gpus and gradient-based local search. Journal of chemical theory and computation, 17(2):1060-1073, 2021.</p>
<p>Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1): $120-131,2018$.</p>
<p>Chence Shi, Minkai Xu, Hongyu Guo, Ming Zhang, and Jian Tang. A graph to graphs framework for retrosynthesis prediction. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 8818-8827. PMLR, 2020a. URL http://proceedings.mlr.press/v119/ shi20d.html.</p>
<p>Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flowbased autoregressive model for molecular graph generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020b. URL https://openreview.net/forum?id=SlesMkHYPr.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 3784-3803. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.FINDINGS-EMNLP.320. URL https://doi.org/10.18653/v1/2021.findings-emnlp. 320.</p>
<p>Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational autoencoders. In Vera Kurková, Yannis Manolopoulos, Barbara Hammer, Lazaros S. Iliadis, and Ilias Maglogiannis (eds.), Artificial Neural Networks and Machine Learning - ICANN 2018 - 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I, volume 11139 of Lecture Notes in Computer Science, pp. 412-422. Springer, 2018. doi: 10.1007/978-3-030-01418-6 $\$ 41. URL https://doi.org/10.1007/ $978-3-030-01418-6$ _ 41.</p>
<p>Teague Sterling and John J. Irwin. ZINC 15 - ligand discovery for everyone. J. Chem. Inf. Model., 55(11):2324-2337, 2015. doi: 10.1021/ACS.JCIM.5B00559. URL https://doi.org/10. 1021/acs.jcim.5b00559.</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating molecule graphs with natural language. CoRR, abs/2209.05481, 2022. doi: 10.48550/ARXIV.2209.05481. URL https://doi.org/10.48550/arXiv.2209.05481.</p>
<p>Mengying Sun, Jing Xing, Han Meng, Huijun Wang, Bin Chen, and Jiayu Zhou. Molsearch: Searchbased multi-objective molecular generation and property optimization. In Aidong Zhang and Huzefa Rangwala (eds.), KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, pp. 4724-4732. ACM, 2022. doi: 10.1145/3534678.3542676. URL https://doi.org/10.1145/3534678.3542676.</p>
<p>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 2818-2826. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.308. URL https://doi.org/10.1109/CVPR.2016.308.</p>
<p>Austin Tripp and José Miguel Hernández-Lobato. Genetic algorithms are strong baselines for molecule generation. CoRR, abs/2310.09267, 2023. doi: 10.48550/ARXIV.2310.09267. URL https://doi.org/10.48550/arXiv.2310.09267.</p>
<p>Niek van Hilten, Florent Chevillard, and Peter Kolb. Virtual compound libraries in computer-assisted drug discovery. J. Chem. Inf. Model., 59(2):644-651, 2019. doi: 10.1021/ACS.JCIM.8B00737. URL https://doi.org/10.1021/acs.jcim.8b00737.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.</p>
<p>Mingyang Wang, Zhe Wang, Huiyong Sun, Jike Wang, Chao Shen, Gaoqi Weng, Xin Chai, Honglin Li, Dongsheng Cao, and Tingjun Hou. Deep learning approaches for de novo drug design: An overview. Current Opinion in Structural Biology, 72:135-144, 2022.</p>
<p>Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard G. Baraniuk, and Anima Anandkumar. Retrieval-based controllable molecule generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=vDFAItpuLvk.</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):31-36, 1988. doi: 10.1021/CIO0057A005. URL https://doi.org/10.1021/ci00057a005.</p>
<p>Robin Winter, Floriane Montanari, Andreas Steffen, Hans Briem, Frank Noé, and Djork-Arné Clevert. Efficient multi-objective molecular optimization in a continuous latent space. Chemical science, 10(34):8016-8024, 2019.</p>
<p>Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. MARS: markov molecular sampling for multi-objective drug discovery. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=kHSu4ebxFXY.</p>
<p>Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiquang Jia. Cognitive mirage: A review of hallucinations in large language models. CoRR, abs/2309.06794, 2023. doi: 10.48550/ARXIV. 2309.06794. URL https://doi.org/10.48550/arXiv.2309.06794.</p>
<p>Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay S. Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp.</p>
<p>6412-6422, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ d60678e8f2ba9c540798ebbde31177e8-Abstract.html.</p>
<p>Chengxi Zang and Fei Wang. Moflow: An invertible flow model for generating molecular graphs. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 617-626. ACM, 2020. doi: 10.1145/3394486.3403104. URL https: //doi.org/10.1145/3394486.3403104.</p>
<p>Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's song in the AI ocean: A survey on hallucination in large language models. CoRR, abs/2309.01219, 2023. doi: 10.48550/ARXIV.2309.01219. URL https://doi.org/10. 48550/arXiv.2309.01219.</p>
<p>Hui Zhao, Yuan Yang, Shuaiqi Wang, Xue Yang, Kaicheng Zhou, Caili Xu, Xuyao Zhang, Jiajun Fan, Dongyue Hou, Xingxiu Li, Hanbo Lin, Ying Tan, Shanshan Wang, Xinyi Chu, Dongzhi Zhuoma, Fengying Zhang, Dianwen Ju, Xian Zeng, and Yu Zong Chen. NPASS database update 2023: quantitative natural product activity and species source database for biomedical research. Nucleic Acids Res., 51(D1):621-628, 2023. doi: 10.1093/NAR/GKAC1069. URL https: //doi.org/10.1093/nar/gkac1069.</p>
<p>Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of molecules via deep reinforcement learning. Scientific reports, 9(1):1-10, 2019.</p>
<h1>A AVAILABILITY OF MOLGEN</h1>
<p>We have made MolGen accessible via Hugging Face in support of the broader scientific community ${ }^{2,3,4}$. It is noteworthy that MOLGEN is versatile enough to be applied to tasks beyond the three primary ones discussed in this paper, such as reaction prediction and retrosynthetic analysis. However, due to computational resource constraints, our experimentation is confined to the generation tasks within this study.</p>
<p>It's important to note that our generation task is different from 3D molecular generation. In 3D molecular generation, methods usually consider spatial conformations, bond angles, bond lengths, and other three-dimensional structural aspects of molecules. These approaches often use molecular force fields and molecular docking techniques to optimize the three-dimensional structures of generated molecules. In contrast, 2D molecular generation aims to create two-dimensional flat structures that capture the chemical composition, bond connectivity, and molecular topology of molecules. This approach places a stronger emphasis on molecular topology and chemical information, providing a representation of the molecule's overall structural arrangement and connectivity.</p>
<p>Our focus on 2D molecular generation is driven by several reasons. Firstly, 2D molecular representations capture essential chemical information and structural features, making them highly interpretable and suitable for various downstream applications such as virtual screening and drug design. Secondly, 2D molecular generation offers computational efficiency and scalability, enabling us to explore a larger chemical space and generate a higher number of diverse molecules within a reasonable time frame. Lastly, while 3D molecular generation is valuable for studying molecular interactions and binding modes, it often requires complex optimization techniques and is computationally more demanding. By concentrating on 2D molecular generation, we can achieve a balance between generating chemically relevant molecules and efficiently exploring chemical space for various property optimizations. We leave the incorporation of 3D conformation information into molecular design for our future work.</p>
<h2>B Limitations and Potential Issues</h2>
<p>While our model, MOLGEN, achieves significant advancements in molecule generation, it is important to acknowledge some of its limitations, which open avenues for future research.</p>
<p>Computational Efficiency: The process of training and fine-tuning MOLGEN, especially with large datasets, can be computationally intensive, which may limit its usage in scenarios with limited computational resources.</p>
<p>Model Interpretability: Though MolGen exhibits prowess in generating molecules with designated properties and discerning vital molecular substructures, the opacity of transformer-based models complicates the understanding of the explicit rationale behind its determinations.</p>
<p>Applicability Limitations: A salient limitation of MOLGEN is its exclusive support for single-target optimization. The chemical feedback paradigm, whilst proficient in managing single-target molecular properties, may grapple with multiple targets. Disparate rankings for multiple objectives could engender ambiguity in the model's optimization trajectory, potentially culminating in less-thanoptimal solutions. Future endeavors could investigate methodologies to adapt the chemical feedback paradigm to accommodate and prioritize diverse objectives.</p>
<p>Generality Limitations: In a bid to assess the versatility of MOLGEN, we extended our investigations to reaction prediction. Our fine-tuned model, devoid of any reliance on reaction templates, registered a $71.4 \%$ accuracy in predicting products from a pool of 39,990 reaction samples. While this underscores the model's capability to predict reactions to a certain degree, it's noteworthy that MOLGEN is not inherently structured for this task, thereby potentially curtailing its performance. Consequently, future research could consider designing a model architecture or training paradigm that concurrently and systematically accommodates reaction prediction, molecule generation, and other tasks.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C Data Information</h1>
<p>This section provides further information regarding the dataset employed in our study. The division of the molecular dataset into "synthetic" and "natural product" domains is to effectively explore and understand molecules of varying complexities and origins. The "synthetic" domain encompasses artificially synthesized chemical molecules tailored for specific needs, e.g., drug development. On the other hand, the "natural product" domain covers molecules naturally occurring, which are pivotal in biological activities and often provide insights for drug development. Natural product molecules generally exhibit greater structural complexity and diversity, often resulting from the myriad of unique chemical structures produced through natural biological processes. This classification helps us better understand the unique challenges and features of each domain.</p>
<p>In our research, we follow the methodologies of prior works (Polykovskiy et al., 2018) for distribution learning, where the baselines focus on synthetic molecule generation. Building upon this foundation, we have extended our scope by including the generation of natural products as a new and more challenging task. This expansion not only enhances the complexity of the tasks we address but also broadens the applicability of our model to a wider range of molecular structures encountered in various scientific domains.</p>
<p>For the natural product dataset, we sourced 30,926 compounds from the Natural Product Activity \&amp; Species Source Database (NPASS) ${ }^{5}$ (Zhao et al., 2023). Out of these, we arbitrarily chose 30,126 molecules for training and reserved 800 molecules for testing, utilizing the same sets for all ensuing molecule generation tasks.</p>
<p>The characteristics of our datasets are depicted in Appendix Table 1. It is apparent that the natural product dataset manifests a distinctive distribution in comparison to the synthetic dataset, characterized by a broader spectrum of p-logP scores and reduced QED scores. This underscores the augmented complexity intrinsic to the optimization of natural product properties.</p>
<p>Appendix Table 1: Data statistics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">DAtaset</th>
<th style="text-align: center;">LENGTH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Penalized logP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">QED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MIN</td>
<td style="text-align: center;">MAX</td>
<td style="text-align: center;">MEAN</td>
<td style="text-align: center;">MIN</td>
<td style="text-align: center;">MAX</td>
<td style="text-align: center;">MEAN</td>
<td style="text-align: center;">MIN</td>
<td style="text-align: center;">MAX</td>
<td style="text-align: center;">MEAN</td>
</tr>
<tr>
<td style="text-align: center;">MOSES</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">-10.241</td>
<td style="text-align: center;">3.329</td>
<td style="text-align: center;">-0.027</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.807</td>
</tr>
<tr>
<td style="text-align: center;">ZINC250K</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">-22.189</td>
<td style="text-align: center;">5.073</td>
<td style="text-align: center;">-0.622</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.732</td>
</tr>
<tr>
<td style="text-align: center;">Natural Product</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">436</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">-51.083</td>
<td style="text-align: center;">17.691</td>
<td style="text-align: center;">-2.186</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.438</td>
</tr>
</tbody>
</table>
<h2>D RELATED WORK</h2>
<h2>D. 1 DEEP GENERATIVE MODELS</h2>
<p>In the last decade, significant strides have been made in the field of deep molecule generation (GómezBombarelli et al., 2018). An array of molecular graph-based generative methods have surfaced (Ma et al., 2018; Simonovsky \&amp; Komodakis, 2018; Jin et al., 2020; Zang \&amp; Wang, 2020; Luo et al., 2021), while another branch has treated this task as a sequence generation problem with a preference for SMILES Kusner et al. (2017); Gómez-Bombarelli et al. (2018); Segler et al. (2018); Kwon et al. (2021). Based on them, existing approaches can be broadly categorized into four venues. Bayesian Optimization (Gómez-Bombarelli et al., 2018; Jin et al., 2018; Winter et al., 2019) learns a continuous latent space of molecules and optimizes the target properties by navigating through this space, but it often demands a protracted evaluation time to optimize the objective function (Du et al., 2022b). Reinforcement Learning approaches utilize an agent to select actions (e.g., adding substructures) in an explicit chemical space to enhance desired properties (Cao \&amp; Kipf, 2018; Popova et al., 2018; You et al., 2018; Popova et al., 2019; Shi et al., 2020b; Zang \&amp; Wang, 2020). However, these methods can suffer from high variance (Xie et al., 2021). An alternative approach is to employ a Variational Auto-Encoder (Simonovsky \&amp; Komodakis, 2018; Jin et al., 2019; Gómez-Bombarelli et al., 2018; Liu et al., 2018), but its performance heavily relies on the quality of the fixed-dimensional latent space. Genetic Algorithms (Jensen, 2019; Ahn et al., 2020; Nigam et al., 2020; Tripp \&amp; Hernández-Lobato, 2023) leverage predefined mutation and crossover rules to generate molecules.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Despite their flexibility, obtaining the necessary prior knowledge and rules can be a challenge, hindering the efficiency of search process.</p>
<h1>D. 2 Pre-trained Language Models</h1>
<p>Just as the syntax of natural languages enforces a grammatical structure that facilitates the connection between words in specific ways, biological symbols also amalgamate in precise structural manners. PLMs have emerged as an intuitive solution for molecule generation, and several pioneers have already begun to harness SMILES-based language models, yielding promising performance (Bagal et al., 2022; Irwin et al., 2022; Flam-Shepherd et al., 2022; Ross et al., 2022; Chilingaryan et al., 2022; Pan, 2023). To date, the only publicly available PLM capable of tackling molecule generation tasks is Chemformer (Irwin et al., 2022), which follows BART (Lewis et al., 2020) to corrupt SMILES and optimize a reconstruction loss for pre-training. Expanding on the foundation laid by Chemformer, RetMol (Wang et al., 2023) incorporates external retrieval data to further improve the synthesis of molecules. Nonetheless, SMILES imposes and circumscribes grammatical rules, leading to a significant number of sequences within the appropriate character set not belonging to well-defined molecules. Additionally, the paucity of annotated or reference data may constrain the optimization direction of molecules. Diverging from those approaches, MOLGEN is pre-trained using SELFIES, which is immune to syntactic and semantic obstacles while permitting facile adaptation to different domains by sharing knowledge among model parameters via domain instruction. Moreover, it autonomously aligns with the objective of producing desirable molecules without the need for external annotated data.</p>
<h2>D. 3 Hallucination</h2>
<p>In the field of Natural Language Processing (NLP), "hallucination" refers to generating text or responses that, while grammatically correct, fluent, and natural, deviate from the provided source inputs or lack factual accuracy (Maynez et al., 2020; Dziri et al., 2021; Shuster et al., 2021; Ji et al., 2023a; Rawte et al., 2023; Ye et al., 2023). Hallucinations are typically categorized into several types: Input-conflicting hallucinations (where the model's output deviates from the user's input), Context-conflicting hallucinations (where the output conflicts with information previously generated), and Fact-conflicting hallucinations (where the output contradicts established world knowledge) (Zhang et al., 2023). The causes of these hallucinations are varied, including biases in training data, the model's lack of access to real-time information, or the inherent limitations of the model in comprehending and generating contextually accurate responses. (Ji et al., 2023b; Zhang et al., 2023; Rawte et al., 2023).</p>
<p>The concept of "hallucination" is not restricted to the domain of NLP. Its adaptation in fields like molecular science, as seen in the term "molecular hallucination", reflects a similar disconnect between structural validity and functional accuracy. In this context, "molecular hallucination" refers to molecules generated by language models that are chemically valid but fail to exhibit desired properties or functionalities. In essence, these molecules, although structurally sound, do not meet the specific chemical criteria or functional expectations set for them, similar to how text generated by a language model might be grammatically correct but deviate from the intended message or content of the source input. This analogy aims to convey the concept of "unfulfilled potential" or "misleading outcomes" in molecular generation.</p>
<h2>E COMPARED BASELINES</h2>
<p>In this section, we expound upon the baselines employed for comparison in our experiments. These baselines are reproduced using their open-source codes under identical experimental conditions. The baselines include:</p>
<ul>
<li>JT-VAE (Jin et al., 2018), a Variational Autoencoder (VAE)-based generative model that constructs a molecular graph by generating a scaffold junction tree and assembling its nodes.</li>
<li>
<p>GCPN (You et al., 2018), a Reinforcement Learning (RL)-based method that crafts a molecule by optimizing a reward comprising adversarial loss and molecular property objectives.</p>
</li>
<li>
<p>MolGQN (Zhou et al., 2019), an RL-based approach that capitalizes on double Q-learning and chemical domain knowledge.</p>
</li>
<li>MARS (Xie et al., 2021), a Markov Chain Monte Carlo sampling approach that employs an adaptive fragment-editing proposal distribution with Graph Neural Networks (GNN).</li>
<li>GraphAF (Shi et al., 2020b), an autoregressive flow model that sequentially adds edges and nodes to generate molecular graphs.</li>
<li>GraphDF (Luo et al., 2021), a normalizing flow model utilizing a discrete latent variable model and is fine-tuned with RL.</li>
<li>LIMO (Eckmann et al., 2022), a VAE-based model leveraging a variational autoencodergenerated latent space.</li>
<li>CHEMFORMER (Irwin et al., 2022), a pre-trained molecular language model operating on SMILES representations.</li>
<li>RETMol (Wang et al., 2023), a retrieval-based framework predicated on CHEMFORMER that incorporates a task-specific retrieval database to guide the generative model towards creating new molecules that fulfill the desired design criteria.</li>
<li>RT (Born \&amp; Manica, 2023), a Transformer-based model pre-trained on SELFIES that generate molecules by inputting expected molecular property values along with a given molecular scaffold (with the generated molecules incorporating this scaffold), or to predict molecular property values based on an input molecule.</li>
</ul>
<h1>F COMPARISON WITH SMILES-BASED PLM</h1>
<p>In this section, we delineate the disparities between two molecular language models, Chemformer (Irwin et al., 2022) and MolGen. For fairness, we select the large version of Chemformer for comparison in our paper, given its analogous size to MolGen. Both models leverage a pre-training dataset of 100 million molecules from the ZINC-15 dataset (Sterling \&amp; Irwin, 2015). MolGen boasts a more compact and specialized vocabulary size of 185 , as opposed to Chemformer's expansive vocabulary of 523 . This allows MolGen to more effectively encapsulate critical molecular substructure information.</p>
<p>Moreover, we present a more detailed discussion concerning SELFIES and SMILES.</p>
<ul>
<li>Inherent Robustness: Although chemical tools like RDKit (Landrum, 2013) can externally validate SMILES strings, the representation itself doesn't inherently ensure grammatical or chemical correctness. In contrast, the construction principle of SELFIES ensures a surjective mapping to molecular graphs.</li>
<li>Generative Capabilities: Flam-Shepherd et al. (2022) provides further evidence by comparing the generative capabilities of deep models using SMILES and SELFIES. SELFIES consistently outperforms SMILES in validity, uniqueness, and novelty across tasks. Notably, SELFIES excels with longer and more complex molecules, whereas using SMILES becomes challenging due to increased character requirements and the heightened risk of encountering errors.</li>
<li>Quantitative Experiments: Our paper includes quantitative experiment outcomes. Table 1 and Figure 6 encompass comparative analyses of SMILES and SELFIES from distribution learning and molecule generation perspectives. Note that the MolGen version in this comparison does not use the chemical feedback mechanism.</li>
<li>About SMILES: We respect and recognize SMILES's significant contributions as a molecular descriptor. Our inclination towards SELFIES is motivated by its inherent validity in molecular generation and its simpler vocabulary, ideal for molecular language pretraining.</li>
</ul>
<h2>G EXPERIMENT DETAILS AND METRICS</h2>
<p>In this section, we elucidate the evaluation metrics, training procedures, and hyperparameters utilized for each task and dataset within our experiments. MolGen is implemented using Pytorch and</p>
<p>trained on 6 Nvidia V100 GPUs. The specific experimental settings and parameters are presented in Appendix Table 2.</p>
<p>Appendix Table 2: Hyper-parameter settings.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">HYPER-PARAMETERS</th>
<th style="text-align: right;">VALUE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">maximum sequence length</td>
<td style="text-align: right;">${55,148,436}$</td>
</tr>
<tr>
<td style="text-align: left;">learning rate</td>
<td style="text-align: right;">${1 \mathrm{e}-5,3 \mathrm{e}-5,1 \mathrm{e}-4}$</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: right;">${8,32,64,200,256}$</td>
</tr>
<tr>
<td style="text-align: left;">weight of rank loss $\alpha$</td>
<td style="text-align: right;">${1,3,5}$</td>
</tr>
<tr>
<td style="text-align: left;">prefix length</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table>
<h1>G. 1 Two-stage Pre-training</h1>
<p>In the first stage of pre-training, we train a Seq2seq model to learn the structure, grammar, and intrinsic semantic information of SELFIES. To efficiently share parameters and knowledge, during the second stage of pre-training, we train the domain-agnostic molecular prefixes across two molecular domains. It is noteworthy that the pre-training objectives in both the first and second stages are aligned. Subsequently, we initialize the prefixes for each task with the pre-trained prefixes and optimize them for that particular task.</p>
<p>We utilize the LAMB optimizer, employing a linear warm-up of the learning rate for the first 180,000 gradient updates, succeeded by a linear decay for the remaining training steps. This process comprised 600 million steps with a batch size of 256 molecules per GPU.</p>
<h2>G. 2 Molecular Distribution Learning</h2>
<p>We outline the metrics employed to evaluate the performance of the generative models in our experiments, encompassing:</p>
<ul>
<li>Validity, which gauges the proportion of generated molecules adhering to valence rules.</li>
<li>Fragment similarity (Frag), comparing the distribution of BRICS fragments in the generated and reference sets. For instance, the Frag metric will be high if the molecules in both sets share similar fragments. Conversely, if some fragments are over- or under-represented (or entirely absent) in the generated set, the metric will be low.</li>
<li>Scaffold similarity (Scaff) comparing the frequencies of Bemis-Murcko scaffolds (comprising all molecule's linker fragments connecting rings and ring structures) in the generated and reference sets. Specifically, if the model seldom produces a specific chemotype from a reference set, the metric will be low.</li>
<li>Similarity to the nearest neighbor (SNN), which measures the average Tanimoto similarity between a molecule from the generated set and its nearest neighbor molecule in the reference dataset. If the generated molecules deviate significantly from the manifold of the reference set, then the similarity to the nearest neighbor will be low.</li>
<li>Internal diversity (IntDiv), assessing the chemical diversity of generated molecules by calculating the average Tanimoto coefficient within the generated set.</li>
<li>Fréchet ChemNet Distance (FCD), considering chemically and biologically pertinent information about molecules. It can discern if the generated molecules share similar biological and chemical properties with real molecules.</li>
<li>Novelty, measuring the percentage of the generated molecules that are not present in the training set and assessing the ability to explore the unknown chemical space.</li>
</ul>
<p>To obtain the results detailed in Table 1, MolGen is trained using the AdamW optimizer with a batch size of 200 for the MOSES dataset and 32 for the natural product dataset on 6 Nvidia V100 GPUs for 100 epochs. A linear warm-up of 20000 steps was also employed.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://bidd.group/NPASS/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
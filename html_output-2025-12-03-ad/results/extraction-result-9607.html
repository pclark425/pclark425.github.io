<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9607 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9607</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9607</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-267627030</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.07770v1.pdf" target="_blank">Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have been extensively studied for their ability to generate convincing natural language sequences; however, their utility for quantitative information retrieval is less well understood. Here, we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid two data analysis tasks: elicitation of prior distributions for Bayesian models and imputation of missing data. We introduce a framework that leverages LLMs to enhance Bayesian workflows by eliciting expert‐like prior knowledge and imputing missing data. Tested on diverse datasets, this approach can improve predictive accuracy and reduce data requirements, offering significant potential in healthcare, environmental science and engineering applications. We discuss the implications and challenges of treating LLMs as ‘experts’.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9607.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9607.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>This paper (LLM priors/imputation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper investigates using large language models as 'expert' interfaces to extract quantitative information for two tasks: eliciting parametric Bayesian priors and zero-shot missing-value imputation, and evaluates methods, calibration and downstream impact across many real datasets and LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (GPT-3.5, GPT-4, Llama 2 13B/70B, Mistral 7B, Mixtral 8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mix of proprietary (GPT-3.5, GPT-4) and open/sizable chat-capable LLMs used in experiments; Llama 2 family (13B and 70B chat variants), Mistral 7B Instruct and Mixtral 8x7B Instruct were used for imputation and prior-elicitation experiments as described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-domain (healthcare, environmental science, engineering, psychology, economics, general scientific literature)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Distributional/statistical relationships (parametric priors, predictive distributions) and numerical imputation estimates (not physical laws per se)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt-engineering to define an 'expert' persona (Expert Prompt Initialization, EPI), deterministic serialization of tabular contexts into natural-language prompts, role-play elicitation protocols (Sheffield-like questions), zero-shot imputation prompts, temperature=0 deterministic decoding; no fine-tuning in core experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>LLMs' internal pretraining corpora (unspecified large-scale textual corpora) serve as implicit knowledge sources; evaluation uses real-world datasets (OpenML-CC18 suite, meteorological historical data via openmeteo API, TALIS) and synthetic missingness injected into tabular datasets for imputation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prior informativeness via prior effective sample size (ESS) heuristic and data-dependent comparisons (how many frequentist samples needed to beat prior predictive MSE), calibration via posterior predictive metrics (lppd/CRPS), imputation upstream metrics (NRMSE for continuous, macro F1 for categorical) and downstream predictive performance (change in classifier macro-F1), plus qualitative comparisons with human-elicited priors.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLMs can produce plausible, sometimes informative parametric priors and occasionally improve downstream sample efficiency, but performance varied strongly across models and tasks; for imputation, LLMs generally underperformed strong data-driven baselines (KNN, random-forest imputation) on upstream metrics, with narrower gaps in downstream evaluation. Variation between LLMs mirrored variation between human experts; some biases (e.g., city-size representation) and data-leakage signals were detected.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Elicited Beta priors for proportion tasks and priors for Cohen's d / Pearson correlations in psychology-style questions; meteorological priors for temperature and precipitation were elicited and compared to historical data; however, no discovery of new algebraic/physical laws or explicit extraction of closed-form equations from scholarly corpora was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Substantial inter-model variability; priors sometimes conflicted with data (prior-data conflict); LLM imputation sensitive to ambiguous or incomplete metadata and units; evidence of training-data leakage for some known public datasets; LLMs sometimes underestimate uncertainty and generate overly confident priors; inability in this work to extract or validate mechanistic mathematical laws from literature (focus was on distributional parameters rather than deriving equations).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against human-elicited priors from the literature (qualitative), simple statistical models (normal-inverse-gamma, gamma-exponential for meteorology), and standard imputation baselines (mean/mode, KNN, random-forest). LLM priors were sometimes informative relative to default priors; LLM imputers generally performed worse than KNN and random forest on upstream metrics, and mixed vs. mean/mode baselines in downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9607.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9607.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMs as knowledge bases (Petroni et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work noting that pretrained language models can act as unsupervised repositories of factual knowledge, retrievable by prompting; used here as background motivating LLMs as potential 'experts' for quantitative knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained autoregressive / masked LMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generic pretrained transformer LMs probed for stored facts via cloze/probing prompts; not tied to a single architecture in the citation within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific and factual knowledge (broad literature)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Factual relations and memorized numeric facts (background, not explicit law discovery in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Probing LMs by querying fill-in-the-blank / prompt templates to retrieve stored factual relations; used here in related-work context to justify LLMs as knowledge sources.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Large web-scale and scholarly text corpora used to pretrain the language models (as described by the cited authors, not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in this paper; cited as prior art. Petroni et al. typically evaluate factual recall accuracy against knowledge-base triples.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned to motivate treating LLMs as unsupervised knowledge sources; the present paper does not reuse Petroni's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>General notion that LMs can store and surface factual relations; no specific equations or laws are extracted here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Caveat that LMs are imperfect knowledge bases (possible memorization, hallucination and calibration issues) — mentioned in broader discussion of LLM reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>N/A in this paper (background citation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9607.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9607.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLP for extracting quantitative information (Olivetti et al. 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited example that natural language processing has been used to extract quantitative data from large text corpora to support quantitative research (e.g., materials science), illustrating existing pipelines that convert literature into structured numeric databases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLP information-extraction pipelines (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-specific NLP and information-extraction systems that identify numeric values, properties and measurement metadata from scholarly text to build structured datasets; architecture details not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (example) and other quantitative research fields</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical quantitative relations (material properties, measurements) extracted from text to support data-driven discovery</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Text mining and information-extraction (named-entity recognition, relation extraction, numeric value extraction) from scholarly articles to populate structured databases for downstream quantitative analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Large collections of scholarly articles and technical reports; in materials applications this includes journal articles and tables/figures converted to machine-readable form.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed here; cited work typically evaluates precision/recall of extracted numeric facts and downstream model performance when trained on extracted data.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited to indicate that NLP has been effectively used to harvest quantitative data from literature; the present paper builds on this general idea but focuses on using generic LLMs to elicit priors rather than building domain-specific extraction pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>No direct examples of laws shown in this paper; cited work demonstrates extraction of material property values and structured datasets from text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>General challenges include noisy extraction, missing metadata (units, context), and the need for careful curation — points echoed in the present paper regarding metadata insufficiency harming LLM imputation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>N/A in this paper (background citation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9607.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9607.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curating structural info for causal models (X. Zhang et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited recent work using LLMs to curate structural / causal information from text to support causal modeling; used as an example of NLP extracting structure rather than numeric laws.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-based structural extraction (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Use of LLMs to propose causal graph structure or structural priors derived from textual sources; exact model variants are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Causal inference / epidemiology / social sciences (generalizable)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Causal structural relationships (graph priors), not closed-form equations</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt LLMs to read textual descriptions and output candidate causal relations or structural priors for use in causal models.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Domain texts, literature and domain knowledge sources used to inform structural priors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not described in this manuscript; cited as prior art illustrating extraction of structured model components from text.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned to show that LLMs can curate structural information for models; not directly assessed here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Structural priors for causal models; no explicit algebraic laws presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Implicit concerns about reliability, potential hallucination, and need for expert vetting—consistent with the present paper's caution on treating LLMs as experts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>N/A in this paper (background citation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9607.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9607.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Systematic review/meta-analytic priors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior elicitation from literature via systematic reviews / meta-analytic-predictive priors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites traditional approaches for extracting priors from the literature, including systematic reviews and meta-analytic-predictive priors, as alternatives to LLM-based elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Systematic review / meta-analysis methods (non-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Established statistical/meta-analytic pipelines that aggregate published empirical results to form informative priors (e.g., meta-analytic-predictive priors), not LLM-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Clinical trials, medicine, general empirical sciences</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Aggregated empirical parameter estimates and distributions (meta-analytic summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Systematic literature search, extraction of empirical estimates and uncertainties, and meta-analytic synthesis to produce parametric priors.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Collections of published empirical studies identified by systematic review protocols (Rietbergen et al. 2011; van de Schoot et al. 2018; Linde et al. 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Meta-analytic measures of heterogeneity, posterior predictive checks when used as priors; not detailed in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Presented as established, rigorous ways to derive priors from literature; contrasted with LLM approaches which aim to emulate expert elicitation but bypass the explicit meta-analytic pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Meta-analytic-predictive priors used to reduce sample size requirements in clinical trials (cited example).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Systematic reviews are expensive, time-consuming and require careful protocol design; translating heterogeneous published summaries into parametric priors can be nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as a traditional baseline / alternative to LLM-based elicitation in conceptual discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9607.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9607.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pseudodata for priors (Gouk & Gao 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior work where pseudodata were generated (by models) as an indirect way to form priors for Bayesian logistic regression, illustrating a related but distinct approach to extracting quantitative information from models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Approach: pseudodata generation via models (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generating synthetic datapoints (pseudodata) from a model trained or prompted on prior knowledge, then using those to construct priors for downstream Bayesian models; exact model details not provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics / Bayesian modelling (logistic regression example)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Implied empirical distributions via generated pseudodata, not explicit closed-form laws</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use generative models to produce synthetic observations reflecting domain knowledge; fit statistical priors to the pseudodata (indirect prior elicitation).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Implicit domain knowledge captured in the generative model (training corpora); not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper; cited as related approach.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an alternative indirect elicitation technique; present paper contrasts this with direct parametric prior elicitation from LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Pseudodata used to induce priors for logistic regression; no explicit equations discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Indirectness of pseudodata approach may hide biases of generator; not further elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as an alternative to direct parametric elicitation; no quantitative comparison in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9607.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9607.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Capstick et al. 2024 (mixture priors)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited extension where multiple Gaussian distributions were elicited as priors to obtain a mixture model, showing extensions of LLM-elicited priors toward more complex prior structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-elicited mixture priors (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Using LLMs to elicit multiple Gaussian components that compose a mixture prior; details are in the cited extension rather than the present study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Bayesian modelling / statistics</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Mixture-distribution priors (statistical relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Elicit multiple component distributions from LLMs and assemble them into mixture priors.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>LLM internal knowledge; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not given in this manuscript — cited as an extension.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as prior art showing LLMs can provide component priors for mixture modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Elicited multiple Gaussians to form mixture priors; no fundamental law-discovery claims.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed here; general issues of calibration and aggregation of multiple agent opinions apply.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not discussed in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9607.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9607.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Table / tabular LLMs (Table-GPT, Jellyfish, Tabular-focused LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Table-GPT / Jellyfish / Tabular representation LLM approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited systems and efforts to tune or fine-tune LLMs specifically for table tasks, including missing-value prediction and table preprocessing, illustrating adjacent work on extracting numeric/structured information from tabular or document sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Table-GPT: Table-Tuned GPT for Diverse Table Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Table/GPT-tuned models and task-specific LLMs (e.g., Table-GPT, Jellyfish, TabPFN mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models fine-tuned or adapted (prefix-tuning, specialized pretraining) for table tasks and data preprocessing; Jellyfish and Table-GPT are examples mentioned in the related work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Data preprocessing / tabular ML / general applied data science</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Structured data prediction tasks (missing-values, entity matching), not explicit law discovery</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tuning, prefix-tuning and serialization-based prompting to convert tabular rows into natural language or specialized token formats for LLM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Tabular datasets and curated table-task corpora; exact sizes and sources are in the cited works, not this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Typically evaluated on tabular benchmarks (task accuracy, imputation error); in this paper they are cited as related approaches, not re-evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as promising for table tasks; the present study contrasts generic LLM zero-shot imputation with such specialized approaches and finds generic LLMs underperform strong tabular baselines without richer metadata or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Jellyfish-13B (fine-tuned for data preprocessing) and Table-GPT are named examples in the related work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Generic LLMs without task-specific tuning struggle on tabular imputation; specialized table-tuned models may perform better but require tuning/fine-tuning and curated training data.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Implied that specialized table models (and traditional imputation methods) can outperform untuned LLM zero-shot imputation; direct comparisons are left to cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models as Knowledge Bases? <em>(Rating: 2)</em></li>
                <li>Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction <em>(Rating: 2)</em></li>
                <li>Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes <em>(Rating: 2)</em></li>
                <li>Table-GPT: Table-Tuned GPT for Diverse Table Tasks <em>(Rating: 2)</em></li>
                <li>ChatGPT-Based Biological and Psychological Data Imputation <em>(Rating: 1)</em></li>
                <li>Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9607",
    "paper_id": "paper-267627030",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [
        {
            "name_short": "This paper (LLM priors/imputation)",
            "name_full": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
            "brief_description": "The paper investigates using large language models as 'expert' interfaces to extract quantitative information for two tasks: eliciting parametric Bayesian priors and zero-shot missing-value imputation, and evaluates methods, calibration and downstream impact across many real datasets and LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (GPT-3.5, GPT-4, Llama 2 13B/70B, Mistral 7B, Mixtral 8x7B)",
            "model_description": "A mix of proprietary (GPT-3.5, GPT-4) and open/sizable chat-capable LLMs used in experiments; Llama 2 family (13B and 70B chat variants), Mistral 7B Instruct and Mixtral 8x7B Instruct were used for imputation and prior-elicitation experiments as described in the paper.",
            "scientific_domain": "Multi-domain (healthcare, environmental science, engineering, psychology, economics, general scientific literature)",
            "law_type": "Distributional/statistical relationships (parametric priors, predictive distributions) and numerical imputation estimates (not physical laws per se)",
            "method_description": "Prompt-engineering to define an 'expert' persona (Expert Prompt Initialization, EPI), deterministic serialization of tabular contexts into natural-language prompts, role-play elicitation protocols (Sheffield-like questions), zero-shot imputation prompts, temperature=0 deterministic decoding; no fine-tuning in core experiments.",
            "input_corpus_description": "LLMs' internal pretraining corpora (unspecified large-scale textual corpora) serve as implicit knowledge sources; evaluation uses real-world datasets (OpenML-CC18 suite, meteorological historical data via openmeteo API, TALIS) and synthetic missingness injected into tabular datasets for imputation benchmarks.",
            "evaluation_method": "Prior informativeness via prior effective sample size (ESS) heuristic and data-dependent comparisons (how many frequentist samples needed to beat prior predictive MSE), calibration via posterior predictive metrics (lppd/CRPS), imputation upstream metrics (NRMSE for continuous, macro F1 for categorical) and downstream predictive performance (change in classifier macro-F1), plus qualitative comparisons with human-elicited priors.",
            "results_summary": "LLMs can produce plausible, sometimes informative parametric priors and occasionally improve downstream sample efficiency, but performance varied strongly across models and tasks; for imputation, LLMs generally underperformed strong data-driven baselines (KNN, random-forest imputation) on upstream metrics, with narrower gaps in downstream evaluation. Variation between LLMs mirrored variation between human experts; some biases (e.g., city-size representation) and data-leakage signals were detected.",
            "notable_examples": "Elicited Beta priors for proportion tasks and priors for Cohen's d / Pearson correlations in psychology-style questions; meteorological priors for temperature and precipitation were elicited and compared to historical data; however, no discovery of new algebraic/physical laws or explicit extraction of closed-form equations from scholarly corpora was performed.",
            "limitations_challenges": "Substantial inter-model variability; priors sometimes conflicted with data (prior-data conflict); LLM imputation sensitive to ambiguous or incomplete metadata and units; evidence of training-data leakage for some known public datasets; LLMs sometimes underestimate uncertainty and generate overly confident priors; inability in this work to extract or validate mechanistic mathematical laws from literature (focus was on distributional parameters rather than deriving equations).",
            "baseline_comparison": "Compared against human-elicited priors from the literature (qualitative), simple statistical models (normal-inverse-gamma, gamma-exponential for meteorology), and standard imputation baselines (mean/mode, KNN, random-forest). LLM priors were sometimes informative relative to default priors; LLM imputers generally performed worse than KNN and random forest on upstream metrics, and mixed vs. mean/mode baselines in downstream tasks.",
            "uuid": "e9607.0",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LMs as knowledge bases (Petroni et al. 2019)",
            "name_full": "Language Models as Knowledge Bases?",
            "brief_description": "Cited work noting that pretrained language models can act as unsupervised repositories of factual knowledge, retrievable by prompting; used here as background motivating LLMs as potential 'experts' for quantitative knowledge.",
            "citation_title": "Language Models as Knowledge Bases?",
            "mention_or_use": "mention",
            "model_name": "Pretrained autoregressive / masked LMs (general)",
            "model_description": "Generic pretrained transformer LMs probed for stored facts via cloze/probing prompts; not tied to a single architecture in the citation within this paper.",
            "scientific_domain": "General scientific and factual knowledge (broad literature)",
            "law_type": "Factual relations and memorized numeric facts (background, not explicit law discovery in this paper)",
            "method_description": "Probing LMs by querying fill-in-the-blank / prompt templates to retrieve stored factual relations; used here in related-work context to justify LLMs as knowledge sources.",
            "input_corpus_description": "Large web-scale and scholarly text corpora used to pretrain the language models (as described by the cited authors, not detailed in this paper).",
            "evaluation_method": "Not evaluated in this paper; cited as prior art. Petroni et al. typically evaluate factual recall accuracy against knowledge-base triples.",
            "results_summary": "Mentioned to motivate treating LLMs as unsupervised knowledge sources; the present paper does not reuse Petroni's experiments.",
            "notable_examples": "General notion that LMs can store and surface factual relations; no specific equations or laws are extracted here.",
            "limitations_challenges": "Caveat that LMs are imperfect knowledge bases (possible memorization, hallucination and calibration issues) — mentioned in broader discussion of LLM reliability.",
            "baseline_comparison": "N/A in this paper (background citation).",
            "uuid": "e9607.1",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "NLP for extracting quantitative information (Olivetti et al. 2020)",
            "name_full": "Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction",
            "brief_description": "Cited example that natural language processing has been used to extract quantitative data from large text corpora to support quantitative research (e.g., materials science), illustrating existing pipelines that convert literature into structured numeric databases.",
            "citation_title": "Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction",
            "mention_or_use": "mention",
            "model_name": "NLP information-extraction pipelines (general)",
            "model_description": "Domain-specific NLP and information-extraction systems that identify numeric values, properties and measurement metadata from scholarly text to build structured datasets; architecture details not specified in this paper.",
            "scientific_domain": "Materials science (example) and other quantitative research fields",
            "law_type": "Empirical quantitative relations (material properties, measurements) extracted from text to support data-driven discovery",
            "method_description": "Text mining and information-extraction (named-entity recognition, relation extraction, numeric value extraction) from scholarly articles to populate structured databases for downstream quantitative analyses.",
            "input_corpus_description": "Large collections of scholarly articles and technical reports; in materials applications this includes journal articles and tables/figures converted to machine-readable form.",
            "evaluation_method": "Not detailed here; cited work typically evaluates precision/recall of extracted numeric facts and downstream model performance when trained on extracted data.",
            "results_summary": "Cited to indicate that NLP has been effectively used to harvest quantitative data from literature; the present paper builds on this general idea but focuses on using generic LLMs to elicit priors rather than building domain-specific extraction pipelines.",
            "notable_examples": "No direct examples of laws shown in this paper; cited work demonstrates extraction of material property values and structured datasets from text.",
            "limitations_challenges": "General challenges include noisy extraction, missing metadata (units, context), and the need for careful curation — points echoed in the present paper regarding metadata insufficiency harming LLM imputation.",
            "baseline_comparison": "N/A in this paper (background citation).",
            "uuid": "e9607.2",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Curating structural info for causal models (X. Zhang et al. 2024)",
            "name_full": "Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes",
            "brief_description": "Cited recent work using LLMs to curate structural / causal information from text to support causal modeling; used as an example of NLP extracting structure rather than numeric laws.",
            "citation_title": "Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes",
            "mention_or_use": "mention",
            "model_name": "LLM-based structural extraction (general)",
            "model_description": "Use of LLMs to propose causal graph structure or structural priors derived from textual sources; exact model variants are not detailed in this paper.",
            "scientific_domain": "Causal inference / epidemiology / social sciences (generalizable)",
            "law_type": "Causal structural relationships (graph priors), not closed-form equations",
            "method_description": "Prompt LLMs to read textual descriptions and output candidate causal relations or structural priors for use in causal models.",
            "input_corpus_description": "Domain texts, literature and domain knowledge sources used to inform structural priors.",
            "evaluation_method": "Not described in this manuscript; cited as prior art illustrating extraction of structured model components from text.",
            "results_summary": "Mentioned to show that LLMs can curate structural information for models; not directly assessed here.",
            "notable_examples": "Structural priors for causal models; no explicit algebraic laws presented in this paper.",
            "limitations_challenges": "Implicit concerns about reliability, potential hallucination, and need for expert vetting—consistent with the present paper's caution on treating LLMs as experts.",
            "baseline_comparison": "N/A in this paper (background citation).",
            "uuid": "e9607.3",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Systematic review/meta-analytic priors",
            "name_full": "Prior elicitation from literature via systematic reviews / meta-analytic-predictive priors",
            "brief_description": "The paper cites traditional approaches for extracting priors from the literature, including systematic reviews and meta-analytic-predictive priors, as alternatives to LLM-based elicitation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Systematic review / meta-analysis methods (non-LLM)",
            "model_description": "Established statistical/meta-analytic pipelines that aggregate published empirical results to form informative priors (e.g., meta-analytic-predictive priors), not LLM-based models.",
            "scientific_domain": "Clinical trials, medicine, general empirical sciences",
            "law_type": "Aggregated empirical parameter estimates and distributions (meta-analytic summaries)",
            "method_description": "Systematic literature search, extraction of empirical estimates and uncertainties, and meta-analytic synthesis to produce parametric priors.",
            "input_corpus_description": "Collections of published empirical studies identified by systematic review protocols (Rietbergen et al. 2011; van de Schoot et al. 2018; Linde et al. 2023).",
            "evaluation_method": "Meta-analytic measures of heterogeneity, posterior predictive checks when used as priors; not detailed in this manuscript.",
            "results_summary": "Presented as established, rigorous ways to derive priors from literature; contrasted with LLM approaches which aim to emulate expert elicitation but bypass the explicit meta-analytic pipeline.",
            "notable_examples": "Meta-analytic-predictive priors used to reduce sample size requirements in clinical trials (cited example).",
            "limitations_challenges": "Systematic reviews are expensive, time-consuming and require careful protocol design; translating heterogeneous published summaries into parametric priors can be nontrivial.",
            "baseline_comparison": "Serves as a traditional baseline / alternative to LLM-based elicitation in conceptual discussion.",
            "uuid": "e9607.4",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Pseudodata for priors (Gouk & Gao 2024)",
            "name_full": "",
            "brief_description": "Mentioned prior work where pseudodata were generated (by models) as an indirect way to form priors for Bayesian logistic regression, illustrating a related but distinct approach to extracting quantitative information from models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Approach: pseudodata generation via models (cited work)",
            "model_description": "Generating synthetic datapoints (pseudodata) from a model trained or prompted on prior knowledge, then using those to construct priors for downstream Bayesian models; exact model details not provided in this manuscript.",
            "scientific_domain": "Statistics / Bayesian modelling (logistic regression example)",
            "law_type": "Implied empirical distributions via generated pseudodata, not explicit closed-form laws",
            "method_description": "Use generative models to produce synthetic observations reflecting domain knowledge; fit statistical priors to the pseudodata (indirect prior elicitation).",
            "input_corpus_description": "Implicit domain knowledge captured in the generative model (training corpora); not specified here.",
            "evaluation_method": "Not specified in this paper; cited as related approach.",
            "results_summary": "Cited as an alternative indirect elicitation technique; present paper contrasts this with direct parametric prior elicitation from LLM prompts.",
            "notable_examples": "Pseudodata used to induce priors for logistic regression; no explicit equations discussed.",
            "limitations_challenges": "Indirectness of pseudodata approach may hide biases of generator; not further elaborated in this paper.",
            "baseline_comparison": "Mentioned as an alternative to direct parametric elicitation; no quantitative comparison in this manuscript.",
            "uuid": "e9607.5",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Capstick et al. 2024 (mixture priors)",
            "name_full": "",
            "brief_description": "Cited extension where multiple Gaussian distributions were elicited as priors to obtain a mixture model, showing extensions of LLM-elicited priors toward more complex prior structures.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLM-elicited mixture priors (cited work)",
            "model_description": "Using LLMs to elicit multiple Gaussian components that compose a mixture prior; details are in the cited extension rather than the present study.",
            "scientific_domain": "Bayesian modelling / statistics",
            "law_type": "Mixture-distribution priors (statistical relationships)",
            "method_description": "Elicit multiple component distributions from LLMs and assemble them into mixture priors.",
            "input_corpus_description": "LLM internal knowledge; specifics not provided here.",
            "evaluation_method": "Not given in this manuscript — cited as an extension.",
            "results_summary": "Mentioned as prior art showing LLMs can provide component priors for mixture modelling.",
            "notable_examples": "Elicited multiple Gaussians to form mixture priors; no fundamental law-discovery claims.",
            "limitations_challenges": "Not discussed here; general issues of calibration and aggregation of multiple agent opinions apply.",
            "baseline_comparison": "Not discussed in this manuscript.",
            "uuid": "e9607.6",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Table / tabular LLMs (Table-GPT, Jellyfish, Tabular-focused LLMs)",
            "name_full": "Table-GPT / Jellyfish / Tabular representation LLM approaches",
            "brief_description": "Cited systems and efforts to tune or fine-tune LLMs specifically for table tasks, including missing-value prediction and table preprocessing, illustrating adjacent work on extracting numeric/structured information from tabular or document sources.",
            "citation_title": "Table-GPT: Table-Tuned GPT for Diverse Table Tasks",
            "mention_or_use": "mention",
            "model_name": "Table/GPT-tuned models and task-specific LLMs (e.g., Table-GPT, Jellyfish, TabPFN mentioned in related work)",
            "model_description": "Models fine-tuned or adapted (prefix-tuning, specialized pretraining) for table tasks and data preprocessing; Jellyfish and Table-GPT are examples mentioned in the related work discussion.",
            "scientific_domain": "Data preprocessing / tabular ML / general applied data science",
            "law_type": "Structured data prediction tasks (missing-values, entity matching), not explicit law discovery",
            "method_description": "Fine-tuning, prefix-tuning and serialization-based prompting to convert tabular rows into natural language or specialized token formats for LLM processing.",
            "input_corpus_description": "Tabular datasets and curated table-task corpora; exact sizes and sources are in the cited works, not this paper.",
            "evaluation_method": "Typically evaluated on tabular benchmarks (task accuracy, imputation error); in this paper they are cited as related approaches, not re-evaluated.",
            "results_summary": "Cited as promising for table tasks; the present study contrasts generic LLM zero-shot imputation with such specialized approaches and finds generic LLMs underperform strong tabular baselines without richer metadata or fine-tuning.",
            "notable_examples": "Jellyfish-13B (fine-tuned for data preprocessing) and Table-GPT are named examples in the related work discussion.",
            "limitations_challenges": "Generic LLMs without task-specific tuning struggle on tabular imputation; specialized table-tuned models may perform better but require tuning/fine-tuning and curated training data.",
            "baseline_comparison": "Implied that specialized table models (and traditional imputation methods) can outperform untuned LLM zero-shot imputation; direct comparisons are left to cited works.",
            "uuid": "e9607.7",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models as Knowledge Bases?",
            "rating": 2,
            "sanitized_title": "language_models_as_knowledge_bases"
        },
        {
            "paper_title": "Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction",
            "rating": 2,
            "sanitized_title": "datadriven_materials_research_enabled_by_natural_language_processing_and_information_extraction"
        },
        {
            "paper_title": "Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes",
            "rating": 2,
            "sanitized_title": "leveraging_llmgenerated_structural_prior_for_causal_inference_with_concurrent_causes"
        },
        {
            "paper_title": "Table-GPT: Table-Tuned GPT for Diverse Table Tasks",
            "rating": 2,
            "sanitized_title": "tablegpt_tabletuned_gpt_for_diverse_table_tasks"
        },
        {
            "paper_title": "ChatGPT-Based Biological and Psychological Data Imputation",
            "rating": 1,
            "sanitized_title": "chatgptbased_biological_and_psychological_data_imputation"
        },
        {
            "paper_title": "Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression",
            "rating": 1,
            "sanitized_title": "automated_prior_elicitation_from_large_language_models_for_bayesian_logistic_regression"
        }
    ],
    "cost": 0.016704749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models</p>
<p>David Selby david.selby@dfki.de 
DFKI GmbH
KaiserslauternGermany |</p>
<p>Yuichiro Iwashita 
DFKI GmbH
KaiserslauternGermany |</p>
<p>Osaka Metropolitan University
OsakaJapan |</p>
<p>Kai Spriestersbach 
DFKI GmbH
KaiserslauternGermany |</p>
<p>| Mohammad Saad 
DFKI GmbH
KaiserslauternGermany |</p>
<p>| Dennis Bappert 
Amazon Web Services
SeattleWashingtonUSA |</p>
<p>Archana Warrier 
DFKI GmbH
KaiserslauternGermany |</p>
<p>Sumantrak Mukherjee 
DFKI GmbH
KaiserslauternGermany |</p>
<p>| Koichi Kise 
DFKI GmbH
KaiserslauternGermany |</p>
<p>Osaka Metropolitan University
OsakaJapan |</p>
<p>Sebastian Vollmer 
DFKI GmbH
KaiserslauternGermany |</p>
<p>University of Kaiserslautern-Landau
KaiserslauternGermany</p>
<p>Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models
96EB11E40FA6E5E4E483D3595DEE10D310.1002/sta4.70054Received: 31 December 2024 | Revised: 6 February 2025 | Accepted: 17 February 2025Bayesian modelsexpert systemslarge language modelsmissing data imputationprior elicitation
Large language models (LLMs) have been extensively studied for their ability to generate convincing natural language sequences; however, their utility for quantitative information retrieval is less well understood.Here, we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid two data analysis tasks: elicitation of prior distributions for Bayesian models and imputation of missing data.We introduce a framework that leverages LLMs to enhance Bayesian workflows by eliciting expert-like prior knowledge and imputing missing data.Tested on diverse datasets, this approach can improve predictive accuracy and reduce data requirements, offering significant potential in healthcare, environmental science and engineering applications.We discuss the implications and challenges of treating LLMs as 'experts'.</p>
<p>| Introduction</p>
<p>Automated solutions for life sciences, industrial and governmental processes demand large amounts of data, which are not always available or complete.Small datasets are vulnerable to overfitting, weakening the validity, reliability and generalizability of statistical insights.To overcome these limitations, analysts employ two approaches.Data-based or empirical methods maximize information extraction, through imputation models, data augmentation and transfer learning; however, this is limited by the size, availability and representativeness of the training set.Alternatively, one can exploit prior information, via knowledge graphs or expert-elicited Bayesian priors, allowing for sparser models and handling of missing values.This approach is constrained by the difficulty, cost and myriad different methods of obtaining and eliciting subjective and heterogeneous opinions from experts, then translating them into a form amenable to quantitative analysis (Falconer et al. 2022).</p>
<p>Large language models (LLMs) are generative models capable of producing natural language texts based on a given prompt or context.LLMs such as GPT-4 have been used in various applications, such as chatbots, summarization and content creation.In the quantitative sciences, LLMs have been applied to mostly qualitative tasks such as code completion, teaching of mathematical concepts (Wardat et al. 2023) and offering advice on modelling workflows or explaining data preparation pipelines (Barberio 2023;Hassani and Silva 2023).Some work has also applied LLMs to mathematical reasoning and symbolic logic (He-Yueya et al. 2023;Orrù et al. 2023).When linked with certain application programming interfaces (APIs), or incorporated into a retrieval-augmented generation (RAG) tool, some LLM frameworks (e.g.Ge et al. 2023) are also capable of evaluating code, connecting to other data analysis tools or looking up supporting information (Nicholson et al. 2021;Kamalloo et al. 2023).However, the capabilities of LLMs to retrieve accurate and reliable quantitative information are less well explored.</p>
<p>Stat, 2025</p>
<p>Here, we explore the use of LLMs for missing value imputation and for prior elicitation.Data imputation is the process of replacing missing entries of a variable with substituted values based on empirical or informed estimates of the distribution (Donders et al. 2006).Prior elicitation is eliciting knowledge from a domain expert to be converted into a prior distribution for use in a probabilistic Bayesian model (Mikkola et al. 2023).</p>
<p>Can LLMs be considered 'experts', having read a large sample of the scientific literature in their training corpora, and thus treated as an accessible interface to this knowledge?We develop a prompting methodology to elicit prior distributions from LLMs, emulating real-world elicitation protocols.LLM-elicited priors are compared with those from human experts, and the LLM 'expertise' is quantitatively evaluated for several tasks.We also present a zero-shot missing data imputation framework, based on LLMs playing 'expert' roles derived from metadata such as the dataset description.An empirical evaluation of imputation quality and impact on downstream tasks compares LLMs with baseline approaches on a diverse set of 50 real-world datasets.Analysis code is available on GitHub.</p>
<p>| Related Work</p>
<p>Language models have been noted for their remarkable ability to act as unsupervised knowledge bases (Petroni et al. 2019).Noever and McKee (2023) and Cheng and Zhang (2023) discuss the 'emergent' numeracy skills of LLMs, from early models unable to perform simple addition to later versions able to compute correlations.Hopkins et al. (2023) showed that repeated sampling from LLMs does not yield reasonable distributions of random numbers, making them poor data generators.Xiong et al. (2023) also suggested LLMs tend to underestimate uncertainty.It has been hypothesized that mode collapse inhibits the diversity of outputs (Anonymous 2023).The design, adaptation and use of LLMs to assist data analysis is a broad topic.</p>
<p>Many LLM-based data science tools focus on tasks such as code generation for analysis scripts (Megahed et al. 2023) or connection with external APIs (Ge et al. 2023).A conversation with a chatbot can also offer generic advice on data science practices.Ahmad et al. (2023) proposed a data cleaning model that combines a fine-tuned foundation model augmented with retrieval from a user-supplied data lake.Here, however, we are interested in evaluating the intrinsic ability of an LLM to retrieve latent quantitative information directly, that is, not to perform mathematical operations on an input dataset nor to offer code or advice on how to do so, rather to offer educated numerical suggestions based on its large training corpus containing specialist technical knowledge.</p>
<p>There is some promise in converting data into natural language inputs for an LLM to perform preprocessing: Narayan et al. (2022) tested GPT-3 on entity matching, error detection and data imputation tasks, in zero-shot and few-shot settings.Their approach serialized tabular data and tasks into natural language using manually tuned prompt templates.Vos et al. (2022) explored prefix tuning as an alternative to full fine-tuning of an LLM for such tasks, whereas H. Zhang et al. (2023b) compared GPT-3.5, GPT-4 and Vicuna-13B in a data preprocessing framework, later developing Jellyfish-13B, an open-source LLM fine-tuned specifically for data preprocessing (H.Zhang et al. 2023a).P. Li et al.'s (2023) 2023) utilized fine-tuning in tandem with a graph attention mechanism to impute spatiotemporal data.Nazir et al. (2023) further explored the capability of ChatGPT in missing value imputation, focussing on imputation quality (see Section 3.5) in psychological and biological data.Conversely, Hayat and Hasan (2024) investigated the role of contextual information in ChatGPT's handling of missing values, but only for downstream LLM-based tasks.An alternative approach to LLM-assisted data analysis involves using only the model's encoder to project natural language representations into a latent space, then performing anomaly detection on this embedding (Lopatecki et al. 2023a(Lopatecki et al. , 2023b)).Tabular representation learning is an active area of research (Hollmann et al. 2023).</p>
<p>However, the level of 'expertise' offered by pretrained LLMs on quantitative tasks across different domains has not yet been extensively studied, nor the effect of LLM imputations on performance in downstream supervised learning tasks.</p>
<p>Prior distributions are just one form of knowledge elicited from domain experts; others include feature engineering, model explanations and labelling heuristics, but in each case, the process of elicitation typically involves interviews, written correspondence or interaction with a custom app (Kerrigan et al. 2021).A good expert-elicited prior distribution can help a statistical model effectively represent the data-generating process, although due to various practical, technical and societal factors, prior elicitation is not yet widespread practice.A lack of standardized software means there is no way for an analyst using, for example, Stan, to initiate an elicitation exercise for a specific model (Mikkola et al. 2023).</p>
<p>LLM-driven elicitation (B.Z. Li et al. 2023) uses an LLM to assist elicitation from human experts, making the process interactive.In engineering, LLMs have been employed in generating (and responding to) requirements elicitation surveys (White et al. 2023;Ronanki et al. 2023;Görer and Aydemir 2023).Natural language processing is already extensively used to extract quantitative information from large texts to aid quantitative research (see, e.g.Olivetti et al. 2020) and to curate structural information for causal models (X.Zhang et al. 2024).Prior distributions can be elicited from literature via systematic reviews (Rietbergen et al. 2011;van de Schoot et al. 2018;Linde et al. 2023).A meta-analytic-predictive prior uses historical data to reduce the required sample size in clinical trials (Weber et al. 2021).To our knowledge, direct elicitation of parametric priors from a 'domain expert' LLM has not yet been explored.Gouk and Gao (2024) generated pseudodata as an indirect prior elicitation approach for Bayesian logistic regression; by contrast, in this paper, we attempt to elicit the distributional parameters directly.Our work has already been extended by Capstick et al. (2024), who elicit multiple Gaussian distributions as priors to obtain a mixture model.Several elicitation protocols have been developed to mitigate cognitive biases and combine the judgements of multiple experts (O'Hagan 2019).The Sheffield Elicitation Framework (shelf Gosling 2018) describes a collection of methods for eliciting a distribution based on aggregated opinions of multiple experts, through group discussion guided by a facilitator.Following a primer in probability and statistics, the protocol includes various ways of eliciting a univariate distribution, such as the 'roulette method', where participants assign chips to bins to form a histogram.Alternatively, the quartile method (or 'Sheffield method' European Food Safety Authority 2014) uses a series of questions to elicit quantiles of a distribution.Cooke's method (Cooke 1991) pools the distributions of multiple experts, weighted according to their respective 'calibration' (accuracy) and 'information' (uncertainty).The Delphi method uses the quartile method, iteratively refined over successive rounds using anonymized feedback from other participants.In this paper, however, we consider only single-agent LLMs with a zero-shot approach.</p>
<p>| Methods</p>
<p>| Overview</p>
<p>We describe our framework for generating and assessing 'expert' prior distributions and imputed missing values for tabular datasets.Evaluation criteria include prior informativeness, calibration and imputation quality, benchmarked against statistical baselines.Because an empirical evaluation of an LLM's 'real-world' knowledge precludes purely abstract simulation-based studies, we carefully sample real datasets spanning a diversity of scientific and technical disciplines.Prior elicitation is tested on domainspecific knowledge tasks, while imputation is evaluated using classification benchmark datasets.These experiments assess both prior elicitation and imputation across multiple domains.</p>
<p>| Eliciting Priors and Imputed Values From LLMs</p>
<p>Impersonating a human domain expert can improve an LLM's performance at related tasks (Salewski et al. 2023).Nevertheless, in response to scientific questions, especially on potentially sensitive topics, such as healthcare advice, language models often prevaricate (Lautrup et al. 2023).An LLM elicitation system should therefore not only prompt the model to roleplay an expert but also carefully specify the task to ensure contextually relevant information is returned in the appropriate format.</p>
<p>Our expert prompt initialization module (Algorithm 1), is a system prompt defining a suitable expert role for the model to imitate.For efficiency, the LLM itself is used to generate these descriptions, once per task, of the form 'You are a …'.To avoid the model offering verbose, generic or prevaricating advice about prior elicitation (such as suggesting R or Python code or advising the user to consult a real expert), the task specification module (Algorithm 2) insists that the model follows a particular elicitation protocol followed by returning a parametric prior distribution, for example, 'Beta(1, 1)', or an imputed number in a format that can be parsed programmatically (Algorithm 3).</p>
<p>Once an appropriate expert role is defined, we structure the input data in a format suitable for LLM processing.Whereas some authors (e.g.H. Zhang et al. 2023a;P. Li et al. 2023) pass data to the LLM in a tabular format, we serialize it to a natural language form using our data serialization module (as in Nazir et al. 2023), described in Algorithm 4, allowing the system to emulate interaction with a human expert.Note, however, that in the case of prior elicitation, no observed data are passed to the model at all.During imputation, the only data fed to the model are the values of other features from the same row-thus, the LLM may not perform symbolic reasoning or otherwise average over other samples in the dataset.In line with earlier work (Narayan et al. 2022;Vos et al. 2022;Nazir et al. 2023), we serialize tabular data using a simple template structure 'the {variable} is {value}': for example, a row vector of data (37, M, 120) with column names 'Age', 'Sex' and 'Blood Pressure' would become the sentence 'The Age is 37.The Sex is M. The Blood Pressure is 120'.Though one might be tempted to add units or expand abbreviations, this conversion is necessarily deterministic to avoid data corruption.Missing values (that are not to be imputed) are simply omitted from the prompt.</p>
<p>To mitigate stochasticity and redundant computation, our framework assumes a temperature setting of zero.Further details are given in Appendices A-E and our code is available on GitHub.</p>
<p>To ensure robustness in evaluation, we conduct empirical tests using datasets spanning multiple fields.We compare LLM priors with those from human experts and simple statistical models fitted to real-world data.The effectiveness of priors is further assessed by examining their predictive consistency across domains.</p>
<p>| Evaluating Expert Priors</p>
<p>What makes a good prior?Bayesian statistics involves decisionmaking based on a posterior distribution,
p( |D) ∝ ( ) n ∏ i=1 p(x i | ),
where ( ) denotes the prior distribution and D the observed data.</p>
<p>The definition of a 'good' prior distribution-like Bayesian statistics itself-is subjective, depending on the analyst's understanding of the purpose of expert-elicited information.No standard benchmark exists for expert-elicited prior distributions; a prior is a function of the expert and the elicitation method, as well as of the predictive task (Gelman et al. 2017).One purpose of prior information is to reduce amount of data needed.Another is to treat expert knowledge and observed data as complementary sources of information about a natural process.Any statistical model is at least slightly misspecified, but a prior can still be informative, well-calibrated and useful (see Williams et al. 2021).An informative prior is different from a noninformative or default prior, that is, it is not too vague.Well-calibrated or 'realistic' priors should align with those from human experts or be otherwise externally verifiable.'Useful' means superior posterior predictive performance on a downstream task, improving expected utility over reference priors.Here, we consider informativeness and calibration.</p>
<p>A measurement of the informativeness of a prior distribution is the prior effective sample size (ESS) (Morita et al. 2008;Neuenschwander et al. 2020).This is not data dependent and does not measure improvement on downstream tasks, but rather how many data points one would need to get similar peakiness/curvature around the posterior mode.The heuristic prior ESS for Beta( , ) is ESS = + (Morita et al. 2008), which measures the concentration of the prior and the amount of data needed to shift the posterior if the prior were misspecified.</p>
<p>For measuring data-prior calibration, we can use the Bayesian log posterior predictive density, or lppd (McElreath 2016)-also called log loss-or the continuous ranked probability score (CRPS), a proper scoring rule used in weather forecasting (Gneiting and Raftery 2007).We can estimate either metric using the posterior predictive distribution p(x � |D) = E p( |D) [p(x � | )] on held-out data.Wilde et al. (2021) describe a similar approach quantifying utility of synthetic data.</p>
<p>| Prior Elicitation Experiments</p>
<p>Our exploration of LLM-elicited priors includes three experiments: (1) a qualitative comparison with human-elicited priors, (2) estimating expert confidence or informativeness across different tasks and (3) a quantitative evaluation of calibration on real-world datasets.A fourth experiment is given in Appendices E.</p>
<p>Absent an open benchmark or database of expert-elicited priors, we select a recent work from the literature that describes an elicitation procedure and reported the resulting distributions.Stefan et al. (2022) interviewed six psychology researchers about typical small-to-medium effect sizes and Pearson correlations in their respective specialisms, using the histogram method.In our first experiment, using similar question wording, we elicited prior distributions from LLMs prompted to simulate an expert, conference of experts (Phillips 1991) or non-expert, with and without mentioning the shelf protocol.This experiment is a qualitative comparison of how LLMs behave when emulating a published example of a prior elicitation exercise with published question wording and results.</p>
<p>For our second experiment, we prompted GPT 3.5 to formulate 25 tasks that might call for expert elicitation in the fields of healthcare, economics, technology, environmental science, marketing and education.Tasks correspond to proportions or probabilities following a Beta( , ) distribution.These scenarios were then used to gauge general levels of confidence of elicited distributions from different LLMs, using the prior ESS metric for this distribution: ESS = + (Morita et al. 2008).Thirdly, we attempted to quantify the realism of priors and any prior-data conflict, by estimating how many samples the LLM prior offers for an analyst who has not yet collected any data.Specifically, we compared LLM priors with simple models fitted to readily available meteorological data.Priors were elicited from LLMs for the typical daily temperature (°C) and precipitation (mm) in December for 25 small and large cities around the world.These distributions were then compared with historical weather data from the openmeteo API.By investigating different continents and varying sizes of settlements, the goal was to identify any systematic biases that might emerge from LLMs' respective training corpora.Exploring both temperature and precipitation allow us to observe behaviour with symmetrical and skewed distributions, respectively.</p>
<p>We compared the prior predictive distribution to a supervised learning model with conjugate posterior (Gressmann et al. 2019): a normal-inverse-gamma model for temperature and a gamma exponential for precipitation.We ask: How many samples on average would a frequentist model need to achieve the same or better log loss (or CRPS or MSE) than the prior predictive distribution?We split the data in half for testing and repeatedly sample up to 1 3 for training from the remaining half.An alternative comparison would be of a posterior predictive based on data and a baseline prior; however, choosing such a baseline is difficult.Unlike the ( + ) ESS heuristic, this data-dependent approach quantifies prior-data conflict.</p>
<p>| Evaluating Missing Data Imputation</p>
<p>What makes a good imputed value?Jäger et al. ( 2021) describe two principles of benchmarking imputation methods: imputation quality and downstream evaluation.Imputation qualityor upstream performance-measures the extent to which an imputation method can accurately recover artificially missing values.Downstream evaluation measures the predictive performance of a supervised learning model on the imputed dataset.Imputation quality for continuous features can be calculated using the root mean square error,
RMSE = � 1 n ∑ n i=1 (x i −x i ) 2
, where x i represents the original discarded value and xi the output of imputation, whereas for categorical features, imputation quality can be calculated via the F 1 score, F 1 = 2(recall −1 + precision −1 ) −1 , the harmonic mean of precision and recall.As RMSE is unbounded, interdataset comparison is made possible by using a normalized version: NRMSE = RMSE∕(maxx − minx).Downstream performance is then determined from the relative improvement in predictive performance of models trained on imputed (vs.complete-case) data.</p>
<p>| Imputation Experiments</p>
<p>Our imputation experiments expand on previous work by exploring a wider range of applications.The OpenML-CC18 Curated Classification benchmark (Bischl et al. 2017)  ranging from 5 to 3073.Though all ostensibly provided in dense tabular format, some are actually drawn from other modes; for example, the MNIST and CIFAR-10 imaging datasets are represented as wide tables, with columns corresponding to individual pixels.It is fair to assume that any human-like expert is unlikely to make particularly informed imputations about such features.Further dataset details are given in Appendix D.</p>
<p>To evaluate the imputation quality, we used all 64 of the datasets that did not already contain missing values.We then split each of the datasets into training and test sets respectively comprising 80% and 20% of the samples.For each dataset, we artificially generated missing values based on the missing at random (MAR) missingness pattern, where the probability of a value being missing depends only on the observed values, using the Python package Jenga (Schelter et al. 2021).The number of features affected by missingness was set to min(p, 3), and the number of missing values was set to 40 for the training set and 10 for the test set.</p>
<p>Building on Jäger et al. (2021) and Nazir et al. (2023), our LLM data imputer is compared with three data-driven approaches: mean and mode imputation (for continuous and categorical features, respectively), k-nearest neighbours (KNN) imputation (the respective mean/mode of the k nearest samples) and random forest imputation.Mean/mode imputation served as the primary baseline.The LLM-based data imputer was powered by Llama 2 13B Chat, Llama 2 70B Chat (Touvron et al. 2023), Mistral 7B Instruct (Jiang et al. 2023) and Mixtral 8x7B Instruct (Jiang et al. 2024), each evaluated separately.</p>
<p>Upstream performance was calculated as the average imputation quality across selected features.Downstream evaluation used a random forest classifier (a RandomForestClassifier from scikit-learn 1.3.2 with default hyperparameters) trained on the imputed training data and evaluated on the held-out test sets.</p>
<p>| Results</p>
<p>| Prior Elicitation</p>
<p>While variations in prompting methodology-asking LLMs to roleplay as experts or non-experts-had some effect on the elicited priors, greater differences were observed between models than between tasks.Variation between human experts is mirrored by variation between LLMs, but there was no obvious pattern to explain performance at particular categories of tasks.Repeated queries with the same prompts offered mostly consistent responses, which can be attributed to the low temperature setting.</p>
<p>Figure 1 compares the priors elicited by (Stefan et al. 2022) from human experts with those we elicited from LLM counterparts in the fields of social and developmental psychology and cognitive neuroscience.Roleplaying as experts in different subfields did not have a noticeable effect on the priors.LLM priors for Cohen's were mostly centred around small effect sizes of 0.2-0.25,except GPT-4, which offered distributions around = 0.5.Mistral-7B-Instruct invariably gave t distributions with = 30 (Llama-70B-Chat-Q5: = 5); other models appeared to grow more conservative (smaller ; more leptokurtic distributions) if asked to roleplay as an expert, simulate a decision conference or employ the shelf protocol.Pearson correlation beta priors from LLMs apparently had little in common with those from real experts: GPT-4 provides a symmetric unimodal distribution whereas other models offer a right-skewed 'bathtub' distribution.Like real experts, different LLMs offered different opinions to each other.</p>
<p>From the second experiment, Figure 2 shows the prior ESScorresponding to model confidence or informativeness-for priors on different tasks.Llama-based models appear to give more conservative priors, whereas GPT is consistently more informative.Mistral 7B Instruct occasionally offered extremely high values ≥ 1000, but, generally speaking, there was no clear and systematic difference between domains.</p>
<p>Our third task was a data-driven evaluation on LLM meteorological estimates.Figure 3 shows the data-dependent ESSs from the elicited prior predictive distribution.We measure the effective increase in observations (starting from zero samples) for a frequentist model to obtain better mean squared error (MSE) than the elicited prior predictive distribution.The ESS is the number of samples needed by the frequentist model to outperform the prior predictive model.In many cases, the prior is in conflict with the data and so ESS → 0 (or, strictly speaking, 2, the minimum data points needed to compute an empirical standard deviation).As might be expected, there was a slight (though not necessarily overwhelming) bias in favour of larger, more populous cities, which would be better represented in the LLMs' training corpora.However, a strong trend of better predictions for English-speaking countries (irrespective of size) was not apparent.</p>
<p>| Missing Value Imputation</p>
<p>Data imputation tasks revealed greater variation between domains; however, LLMs did not consistently outperform more traditional imputation methods.</p>
<p>Figures 4 and 5 compare the performance of LLM-based imputation methods to traditional approaches.For upstream macro-F 1 , most LLM-based methods underperform compared to the mean/ mode baseline, while KNN and random forest imputation consistently outperform it.Upstream RMSE results highlight significant challenges for LLMs, with some cases showing extreme deviations for continuous variables.In contrast, KNN and random forest methods exhibit robust performance across all datasets.</p>
<p>Interestingly, downstream evaluations (Figure 6) show a narrowed performance gap between LLMs and baseline.This improvement may result from feature correlations in imputed rows that enhance downstream classification performance, reducing the sensitivity to choice of imputation method.Nonetheless, KNN and random forest imputations continue to outperform both LLM-based methods and mean/mode imputation.Performance differences across domains are evident, as shown in Figure 7.For instance, the Llama LLM family shows slightly better downstream macro-F 1 scores than Mistral in several datasets.Notably, Llama 2 70B outperforms the mean imputation baseline in four out of five domains, while Llama 2 13B does so in three.In contrast, Mixtral 8x7B and Mistral 7B outperform baseline in only two domains.These results may reflect differences in alignment between training data and the domain.</p>
<p>Further exploration of domain-level variations could provide insight into the capabilities and limitations of LLM imputation.</p>
<p>| Investigating Data Leakage</p>
<p>To investigate potential data leakage in LLM-based imputation, we conducted an experiment designed to assess whether the models had prior exposure to specific datasets.Specifically, we tested whether LLMs could identify datasets based on a single data row, stripped of column headers or feature descriptions, suggesting prior exposure during training.</p>
<p>We evaluated 11 datasets across three categories: well-known datasets (e.g.Titanic), random OpenML datasets and 'suspicious' datasets where LLMs exhibited anomalously high imputation accuracy.The LLMs tested were GPT-3.5, Gemini 1.5 Pro and Llama 2 70B.</p>
<p>Results showed significant variability.While GPT-3.5 and Gemini 1.5 Pro identified six and five of the 11 datasets, respectively, Llama 2 70B identified only two.All models recognized the Titanic dataset and at least one 'suspicious' dataset, supporting the hypothesis of data leakage.However, recognition rates were low for datasets where imputation performance was also poor, likely due to limited representation in training data.The utility of prior knowledge depends heavily on the application and the quality of prior elicitation.While Bayesian methods allow quantifying this utility in decision problems, establishing a benchmark class of problems remains an open challenge.Our findings suggest that, in practice, the value of priors is domain specific and may vary depending on the complexity and quality of data descriptions provided.</p>
<p>Our experiments reveal substantial limitations in LLM-based imputation (in contrast to Nazir et al. 2023, which enjoyed access to a detailed data dictionary), particularly for datasets with incomplete or ambiguous metadata.Poor upstream imputation quality for some datasets was potentially attributable to inadequate feature descriptions or unclear units, hindering the LLM's ability to infer context.For example, datasets like those describing steel plate faults or breast cancer detection lacked sufficient detail, leading to large errors.This underscores the need for careful dataset curation when evaluating LLM-based methods.</p>
<p>Additionally, variability in LLM performance across datasets raises concerns about potential data leakage.Some models demonstrated unexpectedly high performance on datasets that may have been included in their training data, highlighting the importance of designing evaluations robust to task contamination.The results of our data recognition experiment highlight the challenges of using widely available datasets for evaluation of LLMs, which may inadvertently include direct knowledge of their contents.This complicates the assessment of genuine model capabilities and raises concerns about task contamination (C.Li and Flanigan 2023).Perhaps paradoxically, future evaluations may need to prioritize curated or proprietary datasets to minimize the risk of contamination and ensure unbiased benchmarking.</p>
<p>While LLMs struggle with cross-domain imputation, our results suggest several promising avenues for improvement.</p>
<p>Fine-tuning pretrained models on domain-specific tasks and providing richer contextual prompts could enhance their imputation accuracy.Exploring hybrid approaches, such as combining LLMs with traditional imputation methods, may also yield better results.</p>
<p>Our findings also highlight the limitations of commonly used datasets, such as OpenML collections, for evaluating LLMs.These datasets, while suitable for traditional ML benchmarks, often lack the descriptive metadata required by LLMs and are widely known, increasing the risk of leakage.Evaluating LLMs on less accessible, curated datasets with richer descriptions could provide a more accurate measure of their capabilities.</p>
<p>A natural extension of our prior elicitation framework is to (Bayesian) experimental design (see Ryan et al. 2016), which can illustrate the utility of 'good' priors.Suppose a consultant is contracted with a fixed budget of 1100€to obtain an estimate with a target level of precision and, with an 'uninformed approach', can attain this goal with n samples at a cost of 1000€.</p>
<p>If prior knowledge allows similar precision with n∕2 samples (500€), the expected increase in utility (profit) would be 500%.</p>
<p>To conclude, in this paper, we have demonstrated the feasibility of extracting informative Bayesian prior distributions from generic LLMs with a simple expert prompting framework.Methods for the qualitative and quantitative evaluation of informativeness and realism of elicited priors allow assessment without specifying downstream tasks.LLMs potentially promise a more efficient interface to scientific knowledge than recruiting and interviewing domain experts, while enriching analyses with more information than purely data-driven approaches.</p>
<p>However, like human experts, the models vary considerably in their level of confidence around different   where x ij is a covariate, 0j is a random intercept for school j, 1j is a random slope and ij is a normally distributed residual term.The random coefficients are themselves normally distributed with noninformative or weakly informative priors.</p>
<p>We prompt different LLMs with 'Based on your own expert knowledge, suggest informative priors for this model in rstanarm format' and resulting summary statistics from models fitted in rstanarm (Brilleman et al. 2018) are given in Table E1.</p>
<p>(E1)
y ij = 0j + 1j x ij + ij ,
FIGURE 1 |
1
FIGURE 1 | Priors for Cohen's (top) and Pearson correlations (bottom) elicited from LLM and human experts in psychology.Dashed lines denote a shelf-like elicitation protocol.</p>
<p>FIGURE 2 |
2
FIGURE 2 | Distribution of prior effective sample size ( + ) for beta priors on various tasks.Outliers are omitted.</p>
<p>FIGURE 4 | Upstream RMSE performance across LLMs, KNN and random forest.</p>
<p>FIGURE 5 |
5
FIGURE 5 | Upstream macro-F 1 imputation performance across LLM, KNN and random forest.</p>
<p>FIGURE 6 |
6
FIGURE 6 | Downstream macro-F 1 change of LLMs.</p>
<p>FIGURE 7 |
7
FIGURE 7 | Downstream performance of different models, plotted by domain category.</p>
<p>Table-GPT describes a framework for fine-tuning language models on 'table tasks', including finding and predicting missing values.Separately, Chen et al. (</p>
<p>TABLE D1
D1| (Continued)IDNameDomainnp1487ozone-level-8hrEnvironment2534731489phonemeNatural language processing540461494qsar-biodegBiology1055421497wall-robot-navigationEngineering5456251501semeionComputer vision15932571510wdbcMedicine569311590adultSocial sciences48,842154134BioresponseBiology375117774534PhishingWebsitesNatural language processing11,055314538GesturePhaseSegmentationProcessedComputer vision9873336332cylinder-bandsPhysics and chemistry5404023381dresses-salesBusiness5001323517numerai28.6Economics96,3202240499textureComputer vision55004140668connect-4Board game67,5574340670dnaBiology318618140701churnBusiness50002140923Devnagari-ScriptComputer vision92,000102540927CIFAR_10Computer vision60,000307340966MiceProteinMedicine10808240975carBusiness1728740978Internet-AdvertisementsNatural language processing3279155940979mfeat-pixelComputer vision200024140982steel-plates-faultEngineering19412840983wiltEnvironment4839640984segmentComputer vision23102040994climate-model-simulation-crashesEnvironment5402140996Fashion-MNISTComputer vision70,00078541027jungle_chess_2pcs_raw_endgame_completeBoard game44,8197375JapaneseVowelsNatural language processing996115
Abbreviations: n = sample size, p = number of features.</p>
<p>TABLE D1 |
D1
Kaplan and Harra (2024)omputer vision, environment, natural language processing, board game and computer science.For a more complex example,Kaplan and Harra (2024)describe a hierarchical Bayesian model for analysis of the OECD Teaching and Learning International Survey (talis, data available to download online)(OECD 2018).They propose a mixed effects model for the reported job satisfaction of teacher i in school j,
Appendix ETeaching and Learning International Survey</p>
<p>TABLE E1 |
E1
Summary statistics for hierarchical models fitted to talis data, with 95% posterior interval for 'induction' coefficient.
ModelPriorRMSECoef5%95%BaselineDefault2.3370.7380.4011.066priorsKaplanWeakly2.3360.7330.4071.061informa-tiveGPT-4oLLM2.3270.6180.3800.850informa-tiveMistralLLM2.3360.6700.3380.990Chatinforma-tive
Data Availability StatementThe data that support the findings of this study are available in OpenML at https:// openml.org/ .These data were derived from the following resources available in the public domain: OpenML-CC18, https:// openml.org/ search?type= bench mark&amp; sort= tasks_ inclu ded&amp; study_ type= task&amp; id= 99; TALIS 2018, https:// www.oecd.org/ en/ data/ datas ets/ talis -2018-datab ase.html.Author ContributionsS.V. conceived the original idea.Y.I., K.S. and D.B. designed and performed the imputation study.D.S., A.W. and S.M. performed the elicitation studies.Additional analysis was performed by M.S.The main manuscript was written by D.S. with contributions from all authors.All authors reviewed and contributed to the final manuscript.Appendix A Prompting for Prior ElicitationA.1 | GuardrailsSafeguards built into ChatGPT forbid the agent from providing quantitative information about certain sensitive topics, for example, health conditions.However, these restrictions are circumvented when similar information is requested in the form of prior distributions.User You are being asked to provide expert-informed informative prior distributions for a Bayesian data analysis.You give results in pseudocode Stan distributions, for example, 'y ∼ normal(0, 1)'.Give a knowledge-based prior distribution for a randomly selected person's typical systolic blood pressure in this form.Surround your answer with 'backticks'.Do not give an explanation, just give the distribution ChatGPT 'y ∼ normal(120, 10)'Appendix B Expert Prompt Initialization (EPI)The template of our EPI module has the following format.{description} is replaced with the description of the dataset.System I am going to give you a description of a dataset.Please read it and then tell me which hypothetical persona would be the best domain expert on the content of the dataset if I had questions about specific variables, attributes or properties.I don't need a data scientist or machine learning expert, and I don't have questions about the analysis of the data but about specific attributes and values.Please do not give me a list.Just give me a detailed description of a (single) person who really knows a lot about the field in which the dataset was generated.Formulate this as an instruction like 'You are an …'.For prior elicitation and other applications, the phrase 'dataset' may be replaced with 'task' or 'topic'.As a control, we alternate with a 'non-expert' prompt of the form:You are an individual with no academic or professional background related to the dataset's field.Your interests and expertise lie completely outside of the dataset's domain, such as a chef specializing in Italian cuisine when the dataset is about astrophysics.You lack familiarity with the technical jargon, concepts and methodologies pertinent to the dataset.Your approach to questions about specific variables, attributes or properties is based on general knowledge or common sense, without any specialized understanding of the dataset's context or significance.You are more inclined to provide answers based on personal opinions or unrelated experiences rather than data-driven insights.Appendix C Task SpecificationWe used the following prompt template for task specification in data imputation.{expert prompt} is replaced with the output of the EPI module, and {data} is replaced with the serialized data.System {expert prompt}###User THE PROBLEM: We would like to analyse a dataset, but unfortunately this dataset has some missing values.OpenML-CC18A list of OpenML-CC18 datasets used in the experiment is given in TableD1.The domains were selected from medicine, biology, economics, engineering, social sciences, business, psychology, physics and
RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes. M S Ahmad, Z A Naeem, M Eltabakh, M Ouzzani, N Tang, 2023PreprintUnderstanding the Effects of RLHF on LLM Generalisation and Diversity</p>
<p>Large Language Models in Data Preparation: Opportunities and Challenges. A Barberio, 2023Master's Thesis, Politecnico di Milano</p>
<p>OpenML Benchmarking Suites. B Bischl, G Casalicchio, M Feurer, 2017Preprint</p>
<p>Joint Longitudinal and Time-to-Event Models via Stan. S L Brilleman, M J Crowther, M Moreno-Betancur, J Buros Novik, R Wolfe, 2018. 2018. January 10-12. 2018Pacific Grove, CA, USA</p>
<p>GATGPT: A Pre-Trained Large Language Model With Graph Attention Network for Spatiotemporal Imputation. A Capstick, R G Krishnan, P Barnaghi ; Chen, Y , X Wang, G Xu ; Cheng, V , Y Zhang, ID: 5nDmCwAAQBAJProceedings of the 35th Conference on Computational Linguistics and Speech Processing. J.-L Wu, M.-H Su, the 35th Conference on Computational Linguistics and Speech ProcessingGoogle-Books2024. 2023. 2023PreprintExperts in Uncertainty: Opinion and Subjective Probability in Science</p>
<p>Guidance on Expert Knowledge Elicitation in Food and Feed Safety Risk Assessment. A R T Donders, G J M G Van Der Heijden, T Stijnen, K G M Moons, 10.2903/j.efsa.2014.3734Journal of Clinical Epidemiology. 591037342006. 2014EFSA Journal</p>
<p>Methods for Eliciting Informative Prior Distributions: A Critical Review. J R Falconer, E Frank, D L L Polaschek, C Joshi, 10.1287/deca.2022.0451Decision Analysis. 1932022</p>
<p>The Prior Can Often Only Be Understood in the Context of the Likelihood. Y Ge, W Hua, K Mei, 10.1198/016214506000001437Journal of the American Statistical Association. 191014372023. 2017. 2007PreprintEntropy</p>
<p>Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression. B Görer, F B Aydemir, J P Gosling, H Gouk, B Gao, 10.1007/978-3-319-65052-4_4Elicitation: The Science and Art of Structuring Judgement, International Series in Operations Research &amp; Management Science. L C Dias, A Morton, J Quigley, Springer International Publishing2023. 2018. 2024Generating Requirements Elicitation Interview Scripts With Large Language Models</p>
<p>Probabilistic Supervised Learning. F Gressmann, F J Király, B Mateen, H Oberhauser, 2019Preprint</p>
<p>The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field. H Hassani, E S Silva, Big Data and Cognitive Computing. 2023762</p>
<p>CLAIM Your Data: Enhancing Imputation Accuracy With Contextual Large Language Models. A Hayat, M R Hasan, J , G Poesia, R E Wang, N D Goodman, Preprint. Hollmann, N., S. Müller, K. Eggensperger, and F. Hutter2024. 2023. 2023PreprintTabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</p>
<p>Can LLMs Generate Random Numbers? Evaluating LLM Sampling in Controlled Domains. A K Hopkins, A Renda, M Carbin, ICML 2023 Workshop: Sampling and Optimization in Discrete Space. International Conference on Machine Learning (ICML). 2023</p>
<p>A Benchmark for Data Imputation Methods. S Jäger, A Allhorn, F Bießmann, 10.3389/fdata.2021.693674Frontiers in Big Data. 46936742021</p>
<p>Mistral 7B. A Q Jiang, A Sablayrolles, A Mensch, 2023Preprint</p>
<p>Mixtral of Experts. A Q Jiang, A Sablayrolles, A Roux, 2024Preprint</p>
<p>HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking With Attribution. E Kamalloo, A Jafari, X Zhang, N Thakur, J Lin, 2023Preprint</p>
<p>A Bayesian Workflow for the Analysis and Reporting of International Large-Scale Assessments: A Case Study Using the OECD Teaching and Learning International Survey. D Kaplan, K Harra, 10.1186/s40536-023-00189-1Large-Scale Assessments in Education. 121: 22024</p>
<p>A Survey of Domain Knowledge Elicitation in Applied Machine Learning. D Kerrigan, J Hullman, E Bertini, Multimodal Technologies and Interaction. 512732021</p>
<p>Heart-to-Heart With ChatGPT: The Impact of Patients Consulting AI for Cardiovascular Health Advice. A D Lautrup, T Hyrup, A Schneider-Kamp, M Dahl, J S Lindholt, P Schneider-Kamp, Open heart. 102e0024552023</p>
<p>Task Contamination: Language Models May Not Be Few-Shot Anymore. C Li, J Flanigan, 2023Preprint</p>
<p>Table-GPT: Table-Tuned GPT for Diverse Table Tasks. P Li, Y He, D Yashar, 2023Preprint</p>
<p>Eliciting Human Preferences With Language Models. B Z Li, A Tamkin, N Goodman, J Andreas, 2023Preprint</p>
<p>Data-Driven Prior Elicitation for Bayes Factors in Cox Regression for Nine Subfields in Biomedicine. M Linde, L Jochim, J N Tendeiro, D Ravenzwaaij, 10.1101/2023.09.04.23295029v1Preprint, medRxiv. 2023</p>
<p>Applying Large Language Models to Tabular Data: A New Approach. J Lopatecki, A Dhinakaran, C Brown, 2023</p>
<p>A Novel Approach for Anomaly Detection Using Large Language Models. J Lopatecki, A Dhinakaran, C Brown, 2023</p>
<p>Statistical Rethinking: A Bayesian Course With Examples in R and Stan. R Mcelreath, Texts in Statistical Science Series. 2016CRC Press</p>
<p>How Generative AI Models Such as ChatGPT Can Be (Mis)used in SPC Practice, Education, and Research? An Exploratory Study. F M Megahed, Y.-J Chen, J A Ferris, S Knoth, L A Jones-Farmer, 10.1080/08982112.2023.2206479Quality Engineering. 3622023</p>
<p>Prior Knowledge Elicitation: The Past, Present, and Future. P Mikkola, O A Martin, S Chandramouli, 2023Preprint</p>
<p>Determining the Effective Sample Size of a Parametric Prior. S Morita, P F Thall, P Müller ; Narayan, A , I Chami, L Orr, S Arora, C Ré, 10.1111/j.1541-0420.2007.00888.xBiometrics. 6422008. 2022PreprintCan Foundation Models Wrangle Your Data?</p>
<p>ChatGPT-Based Biological and Psychological Data Imputation. A Nazir, M N Cheeema, Z Wang Neuenschwander, B , S Weber, H Schmidli, A O'hagan, 10.1111/biom.13252Meta-Radiology. 132023. 2020Biometrics</p>
<p>scite: A Smart Citation Index That Displays the Context of Citations and Classifies Their Intent Using Deep Learning. J M Nicholson, M Mordaunt, P Lopez, 10.1162/qss_a_00146Quantitative Science Studies. 232021. 2023PreprintNumeracy From Literacy: Data Science as an Emergent Skill From Large Language Models</p>
<p>Expert Knowledge Elicitation: Subjective but Scientific. A O'hagan, 10.1080/00031305.2018.1518265American Statistician. 73sup12019. 2018Talis 2018 Database</p>
<p>Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction. E A Olivetti, J M Cole, E Kim, Applied Physics Reviews. 74413172020</p>
<p>Human-Like Problem-Solving Abilities in Large Language Models Using ChatGPT. G Orrù, A Piarulli, C Conversano, A Gemignani, 10.3389/frai.2023.1199350Frontiers in Artificial Intelligence. 611993502023</p>
<p>Language Models as Knowledge Bases?. F Petroni, T Rocktäschel, S Riedel, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). K Inui, J Jiang, V Ng, X Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational Linguistics2019</p>
<p>Decision Conferencing. L D Phillips, IEE Colloquium on CSCW: Some Fundamental Issues. 1991</p>
<p>Incorporation of Historical Data in the Analysis of Randomized Therapeutic Trials. C Rietbergen, I Klugkist, K J M Janssen, K G M Moons, H J A Hoijtink, Contemporary Clinical Trials. 3262011</p>
<p>Investigating ChatGPT's Potential to Assist in Requirements Elicitation Processes. K Ronanki, C Berger, J Horkoff, 2023 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). IEEE2023</p>
<p>A Review of Modern Computational Algorithms for Bayesian Optimal Design. E G Ryan, C C Drovandi, J M Mcgree, A N Pettitt, 10.1111/insr.12107International Statistical Review. 8412016</p>
<p>JENGA-A Framework to Study the Impact of Data Errors on the Predictions of Machine Learning Models. L Salewski, S Alaniz, I Rio-Torto, E Schulz, Z Akata, S Schelter, T Rukat, F Biessmann, EDBT 2021 Industrial and Application Track. EDBT Association. 2023. 2021PreprintIn-Context Impersonation Reveals Large Language Models' Strengths and Biases</p>
<p>. A M Stefan, D Katsimpokis, Q F Gronau, E.-J Wagenmakers, </p>
<p>Expert Agreement in Prior Elicitation and Its Effects on Bayesian Inference. Psychonomic Bulletin &amp; Review. 295</p>
<p>Bayesian PTSD-Trajectory Analysis With Informed Priors Based on a Systematic Literature Search and Expert Elicitation. H Touvron, L Martin, K Stone, 10.1080/00273171.2017.1412293Multivariate Behavioral Research. 5322023. 2018PreprintLlama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>Towards Parameter-Efficient Automation of Data Wrangling Tasks With Prefix-Tuning. D Vos, T Döhmen, S Schelter, Y Wardat, M A Tashtoush, R Alali, A M Jarrah, NeurIPS 2022 Table Representation Workshop. Neural Information Processing Systems (NeurIPS). 2022. 2023192286Eurasia Journal of Mathematics</p>
<p>Applying Meta-Analytic-Predictive Priors With the R Bayesian Evidence Synthesis Tools. S Weber, Y Li, J W S Iii, T Kakizume, H Schmidli, 10.18637/jss.v100.i19Journal of Statistical Software. 1002021</p>
<p>ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design. J White, S Hays, Q Fu, J Spencer-Smith, D C Schmidt, 2023Preprint</p>
<p>Foundations of Bayesian Learning From Synthetic Data. H Wilde, J Jewson, S Vollmer, C Holmes, Proceedings of the 24th International Conference on Artificial Intelligence and Statistics. the 24th International Conference on Artificial Intelligence and Statistics2021</p>
<p>A Comparison of Prior Elicitation Aggregation Using the Classical Method and SHELF. C J Williams, K J Wilson, N Wilson, 10.1111/rssa.12691Journal of the Royal Statistical Society Series A: Statistics in Society. 18432021</p>
<p>Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. M Xiong, Z Hu, X Lu, 2023Preprint</p>
<p>Jellyfish: A Large Language Model for Data Preprocessing. H Zhang, Y Dong, C Xiao, M Oyamada, 2023Preprint</p>
<p>Large Language Models as Data Preprocessors. H Zhang, Y Dong, C Xiao, M Oyamada, 2023Preprint</p>
<p>Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes. X Zhang, S Liu, Y Wang, Q Mei, Causality and Large Models @NeurIPS 2024. Neural Information Processing Systems (NeurIPS). 2024</p>            </div>
        </div>

    </div>
</body>
</html>
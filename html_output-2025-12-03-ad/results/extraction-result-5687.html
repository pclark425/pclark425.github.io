<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5687 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5687</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5687</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270878285</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.01551v1.pdf" target="_blank">Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we explore the potential of Large Language Models (LLMs) with assertions to mitigate imbalances in educational datasets. Traditional models often fall short in such contexts, particularly due to the complexity and nuanced nature of the data. This issue is especially prominent in the education sector, where cognitive engagement levels among students show significant variation in their open responses. To test our hypothesis, we utilized an existing technology for assertion-based prompt engineering through an 'Iterative - ICL PE Design Process' comparing traditional Machine Learning (ML) models against LLMs augmented with assertions (N=135). Further, we conduct a sensitivity analysis on a subset (n=27), examining the variance in model performance concerning classification metrics and cognitive engagement levels in each iteration. Our findings reveal that LLMs with assertions significantly outperform traditional ML models, particularly in cognitive engagement levels with minority representation, registering up to a 32% increase in F1-score. Additionally, our sensitivity study indicates that incorporating targeted assertions into the LLM tested on the subset enhances its performance by 11.94%. This improvement primarily addresses errors stemming from the model's limitations in understanding context and resolving lexical ambiguities in student responses.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5687.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5687.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AEFL (LLM+Assertions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Assertion-Enhanced Few-Shot Learning (AEFL) applied to an LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt engineering format that augments few-shot in-context examples and chain-of-thought (COT) instructions with targeted, domain-specific assertions (rules) to reduce misclassification due to textual ambiguity and improve minority-class recognition in educational text classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cognitive engagement classification (ICAP: Passive/Active/Constructive)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiclass classification of students' open-ended responses into cognitive engagement levels (Passive, Active, Constructive) from an AI-ELA high-school dataset (testing n=135; subset sensitivity analysis n=27).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Integrated prompt consisting of: (1) few-shot in-context examples with 4 elements per example («Question, Response, Label, Reasoning»), (2) a General COT (chain-of-thought) seven-step reasoning scaffold that instructs the model to read input, apply domain definitions, produce rationale, re-evaluate, and propose edits (Steps 1–7), and (3) additional targeted assertions—explicit rules derived from error analysis (e.g., 'Do label as Constructive when forming a hypothesis about why the model learned a weight' and 'Avoid labeling as Active/Constructive based solely on speculative language like "I think"'). API settings: temperature=0, top_p=0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline LLM prompt without targeted assertions (same few-shot + COT structure but no assertion rules); and traditional ML pipeline (TF-IDF vectorization + SVM/RF/DT/AdaBoost). Iterative experiments varied which assertions were included (experiments 1–10, with Experiment 6.1 being most effective).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Constructive class F1: 32 (LLM with assertions); overall improvements in accuracy observed (best experiment +11.94% accuracy over baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to traditional ML models which scored F1 = 0 on Constructive class (SVM/RF/DT/AdaBoost failed to identify Constructive instances); adding assertions produced subset gains: recall +6.33% and F1 +4.30% for Constructive (for the specific assertion that labels hypothesis statements Constructive); precision for Active class increased +15.96% and F1 +6.08% after adding assertion to avoid relying on speculative language, though Active recall dropped −2.34%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to +32% F1 improvement for minority/Constructive class relative to traditional ML; +11.94% overall accuracy improvement from baseline after targeted assertions (Experiment 6.1). Specific assertion-level effects: Constructive recall +6.33%, Constructive F1 +4.30%; Active precision +15.96%, Active F1 +6.08%, Active recall −2.34%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute improvements to assertions resolving textual ambiguity and unusual/speculative language, providing domain-specific disambiguation and decision rules that complement COT reasoning and few-shot examples; assertions supply explicit criteria that reduce over-reliance on surface cues (e.g., speculative markers like 'I think') and help the model infer hypotheses, thereby improving minority-class detection and balancing precision/recall tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Format had mixed effects: in the Active class LLM sometimes underperformed relative to traditional models (traditional models outperformed LLM by ~11.1% for Active in some comparisons), and adding assertions increased Active precision and F1 but produced a small decrease in Active recall (−2.34%); thus assertions did not uniformly improve every metric/class.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5687.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5687.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General COT + Few-Shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General Chain-of-Thought scaffold combined with Few-Shot with Reasoning examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt presentation combining a seven-step COT scaffold (explicit multi-step instructions including re-evaluation and improvement suggestions) with few-shot ICL examples that include reasoning in each example to guide the model's classification decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cognitive engagement classification (ICAP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multiclass classification of student open responses into Passive/Active/Constructive labels; used both as baseline LLM prompt component and as part of the AEFL pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context learning where each example contains Question, Response, Label, and explicit Reasoning; plus a General COT seven-step instruction sequence (read input, apply CE definitions, give rationale, re-evaluate, propose edits to raise engagement level, restructure to reach Constructive, and final verification).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared implicitly against: (a) LLM prompt variants that additionally include explicit assertions, and (b) baseline LLM prompts without COT or without example reasoning; however the paper does not present an isolated quantitative ablation solely for COT vs non-COT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as part of the overall LLM-with-PE pipeline; no isolated numeric performance attributed solely to COT in the paper (performance reported for LLM with assertions and vs traditional ML).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied improved (when combined with few-shot and assertions)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>COT provides structured 'think time' enabling stepwise reasoning, re-evaluation and justification which helps the model produce more accurate labels and identify misclassifications; combined with few-shot reasoning examples, it orients the model toward the desired decision process.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>No direct null-result reported for COT alone; the paper notes iterative prompt tuning was necessary and that gains depended on combining COT, few-shot examples, and targeted assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5687.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5687.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline LLM prompt (no assertions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot + COT prompt without targeted assertions (baseline prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The LLM prompt used as a baseline in experiments: few-shot in-context examples with reasoning and a COT scaffold but without the targeted domain-specific assertion rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cognitive engagement classification (ICAP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same classification task (n=135 testing set); baseline against which assertion-augmented prompts were compared in iterative experiments on a subset (n=27) and on full set.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot ICL (examples include Question, Response, Label, Reasoning) + General COT seven-step scaffold but without the additional assertion statements that specify labeling heuristics or exceptions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared directly to the same prompt augmented with assertions (AEFL), and to traditional ML pipelines. Iterative experiments reported multiple variations (experiments 1–10) where assertions were progressively added or refined; Experiment 5 gave +8.96% over baseline; Experiment 6.1 gave +11.94% over baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline performance is not fully enumerated in a single summary line, but assertion-augmented prompts achieved up to +11.94% accuracy over this baseline (Experiment 6.1) and Experiment 5 achieved +8.96% improvement over baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Baseline → AEFL (Experiment 6.1): +11.94% accuracy; Baseline → Experiment 5: +8.96% accuracy. (Specific per-class metric deltas for the assertions were reported as noted elsewhere: e.g., Active precision +15.96% relative to baseline after a targeted assertion.)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+8.96% to +11.94% total accuracy increase when targeted assertions were added (varied by experiment); specific per-class metric deltas described in AEFL entry.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved when assertions were added</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Baseline few-shot + COT provided a reasonable starting point, but the authors found misclassifications driven by textual ambiguity and speculative language; adding explicit assertions addressed these error modes and yielded measurable improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Even with baseline prompt, some classes (e.g., Active) were sometimes better handled by traditional ML; adding assertions did not uniformly help all metrics (e.g., slight Active recall reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5687.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5687.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional ML (TF-IDF + Classifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TF-IDF vectorization with SVM, Random Forest, Decision Tree, and AdaBoost (traditional ML baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conventional ML pipeline used as benchmark: text cleaned and vectorized with TF-IDF, then classified using SVM, Random Forest (RF), Decision Tree (DT), and AdaBoost with default scikit-learn hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SVM / RandomForest / DecisionTree / AdaBoost (TF-IDF features)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cognitive engagement classification (ICAP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multiclass classification task; training n=432, testing n=135; dataset included two majority classes (Passive, Active) and one minority (Constructive).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Traditional supervised learning using TF-IDF vectorized student responses (preprocessing included cleaning, lowercasing, stopword removal, spelling correction). No in-context examples or chain-of-thought—standard vector+classifier pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to GPT-4 with prompt engineering (few-shot, COT, assertions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Traditional classifiers failed to identify Constructive instances (Constructive class F1 = 0 for SVM, RF, DT, AdaBoost per reported results); on Passive and Active classes performance was mixed: for Passive the LLM outperformed traditional models by 6.25%–23.2% depending on classifier; for Active traditional models outperformed LLM by ~11.1% (SVM/RF/DT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Passive class: LLM with assertions showed increases vs SVM (+14.9%), RF (+6.25%), DT (+18.0%), AdaBoost (+23.2%). Active class: traditional models (SVM/RF/DT) surpassed LLM by ~11.1% on Active. Constructive class: LLM F1 = 32 vs traditional models F1 = 0.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Constructive class: +32 F1 points for LLM vs 0 for traditional ML; Passive class gains ranged +6.25% to +23.2% for LLM over traditional classifiers; Active class losses of ~11.1% for LLM relative to some traditional classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>LLM prompt-format improved minority class detection and Passive class performance, but traditional format sometimes outperformed LLM on majority Active class</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue TF-IDF + classifiers struggle with small, imbalanced datasets and nuanced, informal student language; LLM prompt formats (ICL, COT, assertions) provide richer contextual reasoning and domain knowledge that help infer implicit cues (especially for minority Constructive instances) which bag-of-words features miss.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Traditional models outperformed LLM on the Active class in some comparisons, indicating that LLM prompt formats did not universally outperform standard pipelines across all classes/metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Assertion enhanced few-shot learning: Instructive technique for large language models to generate educational explanations <em>(Rating: 2)</em></li>
                <li>Prompts matter: Insights and strategies for prompt engineering in automated software traceability <em>(Rating: 1)</em></li>
                <li>Efficient classification of student help requests in programming courses using large language models <em>(Rating: 1)</em></li>
                <li>A divide-conquer-reasoning approach to consistency evaluation and improvement in blackbox large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5687",
    "paper_id": "paper-270878285",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "AEFL (LLM+Assertions)",
            "name_full": "Assertion-Enhanced Few-Shot Learning (AEFL) applied to an LLM",
            "brief_description": "A prompt engineering format that augments few-shot in-context examples and chain-of-thought (COT) instructions with targeted, domain-specific assertions (rules) to reduce misclassification due to textual ambiguity and improve minority-class recognition in educational text classification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Cognitive engagement classification (ICAP: Passive/Active/Constructive)",
            "task_description": "Multiclass classification of students' open-ended responses into cognitive engagement levels (Passive, Active, Constructive) from an AI-ELA high-school dataset (testing n=135; subset sensitivity analysis n=27).",
            "problem_format": "Integrated prompt consisting of: (1) few-shot in-context examples with 4 elements per example («Question, Response, Label, Reasoning»), (2) a General COT (chain-of-thought) seven-step reasoning scaffold that instructs the model to read input, apply domain definitions, produce rationale, re-evaluate, and propose edits (Steps 1–7), and (3) additional targeted assertions—explicit rules derived from error analysis (e.g., 'Do label as Constructive when forming a hypothesis about why the model learned a weight' and 'Avoid labeling as Active/Constructive based solely on speculative language like \"I think\"'). API settings: temperature=0, top_p=0.01.",
            "comparison_format": "Baseline LLM prompt without targeted assertions (same few-shot + COT structure but no assertion rules); and traditional ML pipeline (TF-IDF vectorization + SVM/RF/DT/AdaBoost). Iterative experiments varied which assertions were included (experiments 1–10, with Experiment 6.1 being most effective).",
            "performance": "Constructive class F1: 32 (LLM with assertions); overall improvements in accuracy observed (best experiment +11.94% accuracy over baseline).",
            "performance_comparison": "Compared to traditional ML models which scored F1 = 0 on Constructive class (SVM/RF/DT/AdaBoost failed to identify Constructive instances); adding assertions produced subset gains: recall +6.33% and F1 +4.30% for Constructive (for the specific assertion that labels hypothesis statements Constructive); precision for Active class increased +15.96% and F1 +6.08% after adding assertion to avoid relying on speculative language, though Active recall dropped −2.34%.",
            "format_effect_size": "Up to +32% F1 improvement for minority/Constructive class relative to traditional ML; +11.94% overall accuracy improvement from baseline after targeted assertions (Experiment 6.1). Specific assertion-level effects: Constructive recall +6.33%, Constructive F1 +4.30%; Active precision +15.96%, Active F1 +6.08%, Active recall −2.34%.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Authors attribute improvements to assertions resolving textual ambiguity and unusual/speculative language, providing domain-specific disambiguation and decision rules that complement COT reasoning and few-shot examples; assertions supply explicit criteria that reduce over-reliance on surface cues (e.g., speculative markers like 'I think') and help the model infer hypotheses, thereby improving minority-class detection and balancing precision/recall tradeoffs.",
            "counterexample_or_null_result": "Format had mixed effects: in the Active class LLM sometimes underperformed relative to traditional models (traditional models outperformed LLM by ~11.1% for Active in some comparisons), and adding assertions increased Active precision and F1 but produced a small decrease in Active recall (−2.34%); thus assertions did not uniformly improve every metric/class.",
            "uuid": "e5687.0",
            "source_info": {
                "paper_title": "Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "General COT + Few-Shot",
            "name_full": "General Chain-of-Thought scaffold combined with Few-Shot with Reasoning examples",
            "brief_description": "A prompt presentation combining a seven-step COT scaffold (explicit multi-step instructions including re-evaluation and improvement suggestions) with few-shot ICL examples that include reasoning in each example to guide the model's classification decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Cognitive engagement classification (ICAP)",
            "task_description": "Same multiclass classification of student open responses into Passive/Active/Constructive labels; used both as baseline LLM prompt component and as part of the AEFL pipeline.",
            "problem_format": "Few-shot in-context learning where each example contains Question, Response, Label, and explicit Reasoning; plus a General COT seven-step instruction sequence (read input, apply CE definitions, give rationale, re-evaluate, propose edits to raise engagement level, restructure to reach Constructive, and final verification).",
            "comparison_format": "Compared implicitly against: (a) LLM prompt variants that additionally include explicit assertions, and (b) baseline LLM prompts without COT or without example reasoning; however the paper does not present an isolated quantitative ablation solely for COT vs non-COT.",
            "performance": "Reported as part of the overall LLM-with-PE pipeline; no isolated numeric performance attributed solely to COT in the paper (performance reported for LLM with assertions and vs traditional ML).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied improved (when combined with few-shot and assertions)",
            "explanation_or_hypothesis": "COT provides structured 'think time' enabling stepwise reasoning, re-evaluation and justification which helps the model produce more accurate labels and identify misclassifications; combined with few-shot reasoning examples, it orients the model toward the desired decision process.",
            "counterexample_or_null_result": "No direct null-result reported for COT alone; the paper notes iterative prompt tuning was necessary and that gains depended on combining COT, few-shot examples, and targeted assertions.",
            "uuid": "e5687.1",
            "source_info": {
                "paper_title": "Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Baseline LLM prompt (no assertions)",
            "name_full": "Few-shot + COT prompt without targeted assertions (baseline prompt)",
            "brief_description": "The LLM prompt used as a baseline in experiments: few-shot in-context examples with reasoning and a COT scaffold but without the targeted domain-specific assertion rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Cognitive engagement classification (ICAP)",
            "task_description": "Same classification task (n=135 testing set); baseline against which assertion-augmented prompts were compared in iterative experiments on a subset (n=27) and on full set.",
            "problem_format": "Few-shot ICL (examples include Question, Response, Label, Reasoning) + General COT seven-step scaffold but without the additional assertion statements that specify labeling heuristics or exceptions.",
            "comparison_format": "Compared directly to the same prompt augmented with assertions (AEFL), and to traditional ML pipelines. Iterative experiments reported multiple variations (experiments 1–10) where assertions were progressively added or refined; Experiment 5 gave +8.96% over baseline; Experiment 6.1 gave +11.94% over baseline.",
            "performance": "Baseline performance is not fully enumerated in a single summary line, but assertion-augmented prompts achieved up to +11.94% accuracy over this baseline (Experiment 6.1) and Experiment 5 achieved +8.96% improvement over baseline.",
            "performance_comparison": "Baseline → AEFL (Experiment 6.1): +11.94% accuracy; Baseline → Experiment 5: +8.96% accuracy. (Specific per-class metric deltas for the assertions were reported as noted elsewhere: e.g., Active precision +15.96% relative to baseline after a targeted assertion.)",
            "format_effect_size": "+8.96% to +11.94% total accuracy increase when targeted assertions were added (varied by experiment); specific per-class metric deltas described in AEFL entry.",
            "format_effect_direction": "improved when assertions were added",
            "explanation_or_hypothesis": "Baseline few-shot + COT provided a reasonable starting point, but the authors found misclassifications driven by textual ambiguity and speculative language; adding explicit assertions addressed these error modes and yielded measurable improvements.",
            "counterexample_or_null_result": "Even with baseline prompt, some classes (e.g., Active) were sometimes better handled by traditional ML; adding assertions did not uniformly help all metrics (e.g., slight Active recall reduction).",
            "uuid": "e5687.2",
            "source_info": {
                "paper_title": "Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Traditional ML (TF-IDF + Classifiers)",
            "name_full": "TF-IDF vectorization with SVM, Random Forest, Decision Tree, and AdaBoost (traditional ML baselines)",
            "brief_description": "Conventional ML pipeline used as benchmark: text cleaned and vectorized with TF-IDF, then classified using SVM, Random Forest (RF), Decision Tree (DT), and AdaBoost with default scikit-learn hyperparameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SVM / RandomForest / DecisionTree / AdaBoost (TF-IDF features)",
            "model_size": null,
            "task_name": "Cognitive engagement classification (ICAP)",
            "task_description": "Same multiclass classification task; training n=432, testing n=135; dataset included two majority classes (Passive, Active) and one minority (Constructive).",
            "problem_format": "Traditional supervised learning using TF-IDF vectorized student responses (preprocessing included cleaning, lowercasing, stopword removal, spelling correction). No in-context examples or chain-of-thought—standard vector+classifier pipeline.",
            "comparison_format": "Compared to GPT-4 with prompt engineering (few-shot, COT, assertions).",
            "performance": "Traditional classifiers failed to identify Constructive instances (Constructive class F1 = 0 for SVM, RF, DT, AdaBoost per reported results); on Passive and Active classes performance was mixed: for Passive the LLM outperformed traditional models by 6.25%–23.2% depending on classifier; for Active traditional models outperformed LLM by ~11.1% (SVM/RF/DT).",
            "performance_comparison": "Passive class: LLM with assertions showed increases vs SVM (+14.9%), RF (+6.25%), DT (+18.0%), AdaBoost (+23.2%). Active class: traditional models (SVM/RF/DT) surpassed LLM by ~11.1% on Active. Constructive class: LLM F1 = 32 vs traditional models F1 = 0.",
            "format_effect_size": "Constructive class: +32 F1 points for LLM vs 0 for traditional ML; Passive class gains ranged +6.25% to +23.2% for LLM over traditional classifiers; Active class losses of ~11.1% for LLM relative to some traditional classifiers.",
            "format_effect_direction": "LLM prompt-format improved minority class detection and Passive class performance, but traditional format sometimes outperformed LLM on majority Active class",
            "explanation_or_hypothesis": "Authors argue TF-IDF + classifiers struggle with small, imbalanced datasets and nuanced, informal student language; LLM prompt formats (ICL, COT, assertions) provide richer contextual reasoning and domain knowledge that help infer implicit cues (especially for minority Constructive instances) which bag-of-words features miss.",
            "counterexample_or_null_result": "Traditional models outperformed LLM on the Active class in some comparisons, indicating that LLM prompt formats did not universally outperform standard pipelines across all classes/metrics.",
            "uuid": "e5687.3",
            "source_info": {
                "paper_title": "Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Assertion enhanced few-shot learning: Instructive technique for large language models to generate educational explanations",
            "rating": 2,
            "sanitized_title": "assertion_enhanced_fewshot_learning_instructive_technique_for_large_language_models_to_generate_educational_explanations"
        },
        {
            "paper_title": "Prompts matter: Insights and strategies for prompt engineering in automated software traceability",
            "rating": 1,
            "sanitized_title": "prompts_matter_insights_and_strategies_for_prompt_engineering_in_automated_software_traceability"
        },
        {
            "paper_title": "Efficient classification of student help requests in programming courses using large language models",
            "rating": 1,
            "sanitized_title": "efficient_classification_of_student_help_requests_in_programming_courses_using_large_language_models"
        },
        {
            "paper_title": "A divide-conquer-reasoning approach to consistency evaluation and improvement in blackbox large language models",
            "rating": 1,
            "sanitized_title": "a_divideconquerreasoning_approach_to_consistency_evaluation_and_improvement_in_blackbox_large_language_models"
        }
    ],
    "cost": 0.01278575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LEVERAGING PROMPTS IN LLMS TO OVERCOME IMBALANCES IN COMPLEX EDUCATIONAL TEXT DATA A PREPRINT</p>
<p>Jeanne Mcclure jmmcclu3@ncsu.edu 
North Carolina State University
27695RaleighNC</p>
<p>Machi Shimmei mshimme@ncsu.edu 
North Carolina State University
27695RaleighNC</p>
<p>Noboru Matsuda nmatsud@ncsu.edu 
North Carolina State University
27695RaleighNC</p>
<p>Shiyan Jiang sjiang24@ncsu.edu 
North Carolina State University
27695RaleighNC</p>
<p>LEVERAGING PROMPTS IN LLMS TO OVERCOME IMBALANCES IN COMPLEX EDUCATIONAL TEXT DATA A PREPRINT
374F07D536C3064DECF728D5735D3E62Machine LearningText ClassificationPrompt EngineeringImbalanced datasetLLMs
Background: The study addresses the challenge of imbalances in educational datasets, which is prominent in the education sector due to the varied cognitive engagement levels among students in their open responses.Traditional machine learning (ML) models often struggle with the complexity and nuanced nature of this data, leading to inadequate analyses, especially for minority data representations[Karimah and Hasegawa, 2022, Radwan and Cataltepe, 2017, Yun et al., 2011].Understanding students' cognitive engagement is vital as it reflects their mental investment in learning activities, which is closely linked to academic success [</p>
<p>Objective: The objective of this paper is to investigate the efficacy of Large Language Models (LLMs) enhanced with assertions in tackling the complexities of imbalanced educational datasets, with a special focus on the precise classification of cognitive engagement levels from student texts.This exploration is underpinned by two critical research questions.The first seeks to evaluate how LLMs equipped with Prompt Engineering fare in comparison to conventional ML algorithms when dealing with the inherent challenges of imbalanced educational data.The second question delves into the specific contributions of integrating assertions into LLMs, examining how such augmentations can improve the models' effectiveness in handling the nuanced difficulties presented by imbalanced textual educational datasets.Through this inquiry, the study aims to shed light on the potential of LLMs and assertions in enhancing the accuracy and reliability of cognitive engagement classification, thereby addressing a significant gap in educational data analysis.</p>
<p>Methods: The study employed an 'Iterative -ICL PE Design Process' to compare traditional ML models against LLMs augmented with assertions (N=135).A sensitivity analysis on a subset (n=27) examined variance in model performance concerning classification metrics and cognitive engagement levels.This process involved the utilization of assertion-based prompt engineering, comparing the performance of traditional ML models to LLMs with assertions in classifying cognitive engagement from student texts in an educational setting [Shahriar et al., 2023, Brown et al., 2020, Wei et al., 2022a].</p>
<p>Findings: LLMs with assertions significantly outperformed traditional ML models, especially in recognizing cognitive engagement levels with minority representation, showing up to a 32% increase in F1-score.Incorporating targeted assertions into the LLM on the subset enhanced its performance by 11.94%, primarily addressing errors from limitations in understanding context and resolving lexical ambiguities in student responses.</p>
<p>Implications: The study demonstrates the superior capability of LLMs, particularly when augmented with assertions, in addressing the nuanced challenges of imbalanced educational datasets.This advancement not only improves the accuracy of classifying cognitive engagement levels but also opens new avenues for data-driven educational research and practice.The findings suggest a potential paradigm shift towards employing advanced LLM techniques in educational settings to achieve a more nuanced and accurate analysis of student engagement, thereby enhancing learning outcomes.Future research should further explore the capabilities of LLMs across broader educational contexts arXiv:2407.01551v1[cs.CY] 28 Apr 2024 1 Introduction</p>
<p>Understanding students' cognitive engagement (CE) at both the school and task levels is crucial, as it offers deep insights into their commitment to learning [Fredricks et al., 2004].This form of engagement, characterized by a student's deliberate and intentional approach to schoolwork and their willingness to invest the necessary effort in comprehending complex concepts and mastering challenging skills, serves as a key indicator of academic success [Fredricks et al., 2004, Blumenfeld et al., 2006].CE encompasses the psychological investment and effort driven by student motivation and strategies, alongside their dedication to learning [Corno and Mandinach, 1983, Fredricks et al., 2004, Pintrich, 2000, Schunk et al., 2014].</p>
<p>While analyzing students' CE is crucial for enhancing learning experiences, a significant challenge arises from imbalanced datasets [Radwan and Cataltepe, 2017].These datasets often feature unevenly distributed categories and are typically small, not fitting the 'big data' criteria usually required for effective Machine Learning (ML) training.This size limitation, along with the disproportionate representation of majority and minority data, further complicates the training process in traditional analyses [Yun et al., 2011].Traditional ML methods, commonly employed to classify CE, often struggle to adequately address these imbalances, raising concerns about the accuracy and reliability of their results.This issue presents a major hurdle in accurately assessing and interpreting CE, as the uneven representation of data can lead to skewed insights and potentially overlook critical aspects of student engagement [Karimah and Hasegawa, 2022].This imbalance in datasets not only complicates the analysis but also raises concerns about the reliability and generalizability of the findings in diverse educational settings [Radwan and Cataltepe, 2017].</p>
<p>The exploration of LLMs provides a promising solution to the limitations of traditional ML approaches.Recent studies, including [Wu, 2021], have highlighted the potential of prompt engineering in reducing the need for extensive training of case labeling which is imperative for imbalance data.LLMs employ techniques like In-context Learning (ICL) [Brown et al., 2020] and Chain-of-Thought (COT) prompting [Wei et al., 2022b], enabling more nuanced and context-aware responses.ICL trains models using examples in specific contexts, improving with scaled model and corpus sizes, as seen in N-shot prompting [Brown et al., 2020].This is illustrated by Brown et al. [2020]'s few-shot learning, where LLMs process input-output pairs in-context, leading to better test-time predictions.Similarly, COT, by Wei et al. [2022b], involves logical, step-by-step natural language reasoning.Furthering this, Shahriar et al. [2023] developed Assertion Enhanced Few-Shot Learning, incorporating domain-specific assertions in prompts to enhance accuracy and reduce errors.These innovations significantly boost LLMs' task-specific efficiency, surpassing traditional methods.</p>
<p>While LLMs have shown potential in educational research, their application has predominantly been refined to solve logical reasoning or arithmetic problems [Lee et al., 2024], with limited exploration in addressing imbalanced datasets of education.Our study breaks new ground by applying LLMs with Prompt Engineering (PE) to this specific challenge.We hypothesize that LLMs, renowned for their nuanced language understanding, will surpass traditional ML algorithms in classifying cognitive engagement levels from student texts.Our exploration is guided by two research questions: RQ1 addresses the comparative efficacy of LLMs against traditional ML algorithms, and RQ2 investigates the role of assertions in overcoming contextual and lexical challenges within imbalanced datasets.Specifically:</p>
<ol>
<li>How do the results obtained from LLMs with PE compare to traditional Machine Learning algorithms in handling imbalanced educational data? 2. In what ways does the integration of assertions enhance the efficacy of models when addressing the challenges associated with imbalanced textual educational datasets?</li>
</ol>
<p>This paper examines how AEFL mitigates issues in imbalanced educational data analysis, revealing how these technologies can effectively address the challenges posed by uneven dataset distributions.By applying this cuttingedge technique, we uncover new possibilities for analyzing and interpreting complex educational data.Our findings demonstrate the advantage of AEFL in educational settings, especially where traditional ML methods fall short, opening new avenues for data-driven educational research and practice.</p>
<p>The rest of the paper is set up as follows: Section 2 delves into the background, highlighting the emergence of LLMs as a promising solution in education.Section 3 outlines our methodology, including the Iterative -ICL PE Design Process, and the experimental setup.The results and discussions are presented in Section 4, where we compare the performance of LLMs augmented with assertions against traditional ML models and discuss the impact of assertions on model efficacy and limitations.Finally, Section 5 concludes with our findings and future directions.</p>
<p>Background</p>
<p>The exploration of CE within educational research has significantly evolved, transitioning from a simplistic focus on student participation to a complex understanding of mental investment in learning activities.This shift is paramount for fully capturing the essence of engagement, as initially highlighted by Craik and Lockhart [1972] through their distinction between shallow and deep processing.Subsequent work by Appleton et al. [2006] and Fredricks et al. [2004] expanded the concept to encompass behavioral, emotional, and cognitive dimensions, underscoring engagement's multifaceted nature across various educational contexts.A pivotal insight from this exploration is the strong positive correlation between student learning and cognitive engagement, evidenced by Chi and Wylie [2014], which underscores the significant educational outcomes associated with deep cognitive processes.</p>
<p>CE distinguishes itself within the broader spectrum of educational engagement by focusing on the intensity of students' mental investment in learning.This stands in contrast to behavioral engagement's emphasis on participation and emotional engagement's concern with feelings towards learning Blumenfeld et al. [2006].Such a distinction is crucial for educators and researchers dedicated to enhancing learning outcomes through targeted interventions.</p>
<p>Central  , 2001, Bloom et al., 1956, Corno and Mandinach, 1983, Chi and Wylie, 2014, Chase et al., 2019, Hsiao et al., 2022, Wang et al., 2016].</p>
<p>Measuring CE, however, presents inherent challenges due to its complex and internal nature.As a latent construct, CE's assessment relies on inferences from behavioral indicators or through self-report measures [Chi and Wylie, 2014, Fredricks et al., 2004, McCoach et al., 2013].Traditional methods, including self-report questionnaires, surveys, and observational techniques, often inadequately capture the nuanced cognitive processes involved in learning.A variety of measures have been employed in past studies to gauge CE, such as self-reported scales, classroom observations, interviews, teacher ratings, experience sampling, eyetracking, physiological sensors, trace analysis, and content analysis [Greene et al., 2004, Smiley and Anderson, 2011, Lee and Anderson, 1993, Helme and Clarke, 2001, Wigfield et al., 2008, Xie et al., 2019, D'Mello et al., 2017, Bernacki et al., 2012, Ireland and Henderson, 2014].Nonetheless, the complexity of accurately assessing CE through these measures necessitates innovative approaches that more precisely reflect students' cognitive investment in their educational activities [Fredricks et al., 2004].</p>
<p>In educational research, traditional ML methods have extensively analyzed student data patterns but face limitations when addressing nuanced aspects like cognitive engagement.The problem is exacerbated by imbalanced datasets, leading to skewed insights and overlooking crucial engagement aspects, thus affecting the findings' accuracy, reliability, and generalizability across diverse educational contexts [Lee andKinzie, 2012, Fredricks et al., 2004].This issue with imbalanced datasets, characterized by unevenly distributed categories and small sample sizes, highlights the need for specialized techniques to improve model performance and accuracy, ensuring a comprehensive understanding of CE across educational contexts [Chawla, 2010, Fernández et al., 2018, Kulkarni et al., 2020, Japkowicz and Stephen, 2002, Bruce et al., 2020, LemaÃŽtre et al., 2017].</p>
<p>The advent of LLMs presents a promising solution to the issues posed by imbalanced datasets in educational research.</p>
<p>Recent breakthroughs in LLMs, particularly with ICL, COT and AEFL prompting techniques, have demonstrated their potential to generate nuanced, context-aware responses beyond the capabilities of traditional ML methods [Brown et al., 2020, Wei et al., 2022b, Shahriar et al., 2023].For example, Savelka et al. [2023] showcased how GPT-3.5 &amp; 4 could effectively classify student help requests in programming courses, illustrating the superior ability of LLMs to handle nuanced educational data.Zeng et al. [2023] delved into the cognitive and reasoning abilities of LLMs, highlighting the necessity for task-specific tuning to address complex reasoning challenges.Cui et al. [2023] introduced the Divide-Conquer-Reasoning (DCR) framework to enhance the consistency and reliability of LLM-generated texts, vital for creating educational content.These examples reveal the capacity of LLMs to offer more accurate classification and analysis of CE, surpassing traditional ML methods in dealing with the intricacies of educational datasets.Additionally, Lee et al. [2024] explored LLMs' use with CoT prompting to improve automatic scoring systems in science education, further indicating LLMs' potential to enhance the quality and reliability of educational content analysis.</p>
<p>By harnessing the intrinsic capacity of LLMs to interpret and utilize language within specific contexts, researchers can navigate the challenges posed by imbalanced datasets, facilitating a deeper understanding of student CE.</p>
<p>Methodology</p>
<p>Context and Participants</p>
<p>This study performs a secondary analysis on a dataset originally gathered to assess CE from student responses in a High School English Language Arts course's AI curriculum.The StoryQ curriculum [Chao et al., 2022], spanned three weeks with daily 45-minute classes, incorporated Machine Learning Practices through open-ended questions in eight modules but our analysis only evaluated three: "Sentiment Analysis," "Features and Models," and "All Words."</p>
<p>The initial study's diverse participant group of 28 students included 17 females, 7 males, and 4 non-specified gender individuals, spanning various grades and racial backgrounds.The racial composition was 43% Black/African American, 17% Hispanic/Latinx, 18% White/Caucasian, with others choosing not to disclose.Students' CE was evaluated using a modified Interactive-Constructive-Active-Passive (ICAP) framework by Chi and Wylie [2014], focusing on Constructive, Active, and Passive levels.Their open-ended responses (N = 840) were analyzed using the CE coding scheme, see Table 1, yielding a Cohen's kappa inter-rater reliability of 0.84.</p>
<p>Prompt Engineering Design</p>
<p>Our prompt development process, grounded in the ICL Prompt Engineering Design (see Figure 1), begins with drafting an initial few-shot ICL format prompt.This prompt, inputting student responses and outputting CE classifications, undergoes validation testing on a subset (n=27).If benchmarks are met, it progresses to full dataset testing; otherwise, we diagnose misclassifications, realigning LLM outputs with our coding standards through domain-specific CE knowledge integration.Adjustments may involve refining COT processes, FewSHOT learning, or embedding conceptual knowledge assertions.After subset retesting and validation, the optimized prompt is applied to the full dataset (n=135), with iterative refinement ensuring optimal performance.See Appendix B for additional LLM-specific prompt details.</p>
<p>Our engineering approach encompasses three components: General COT, FewShot with Reasoning Sequence, and assertions Prompting.General COT, embeds sequential instructions with "think time" to initiate the model's reasoning on given tasks [Fulford and Ng, 2023].Our General COT prompt follows a seven-step sequence to guide the LLM's task reasoning (see Figure 1).Initially, the model attentively reads the provided «Question, Response» (Step 1), laying the foundation for accurate comprehension and subsequent cognitive engagement analysis.</p>
<p>Step 2 involves feeding the model CE domain-specific definitions for Passive, Active, and Constructive levels, requiring it to discern the appropriate engagement level based on the initial input.Progressing to Step 3, the model assesses the rationale behind the assigned cognitive engagement label, ensuring it reflects the response's depth and nature.In Step 4, the LLM reevaluates the response to prevent misclassification and assesses if a different CE level is more aligned.Steps 5 and 6 prompt the model to consider ways to enhance the CE level, crucial in the validation and diagnostic phases, particularly when integrating assertions.The final step (Step 7) circles back to the initial input, where the LLM reexamines the cognitive engagement level to verify the accuracy and consistency of its prediction.This structured approach is key in sharpening the model's evaluative and analytical capabilities.</p>
<p>FewShot with Reasoning, guided by gold standard examples [Wang et al., 2023, Shahriar et al., 2023], includes a four-element structure: «Question, Response, Label, and Reasoning».This method enhances LLM's task-specific learning, incorporating reasoning sequences in the examples.Finally adding assertion Prompts, is crucial for knowledgebuilding explanations, that are domain-specific insights defined from General COT's outputs on misclassified predictions [Shahriar et al., 2023].</p>
<p>Experiment Design</p>
<p>To analyze traditional ML methods (SVM, RF, DT, and ADABoost), we divided our data into training (n=432) and testing sets (n=135), applying default hyperparameters from the Scikit-Learn package (Pedregosa et al., 2011).See Appendix A for hyperparameters.The dataset comprised two majority classes and one minority class (see Table 2.</p>
<p>During data preprocessing, we executed text cleaning steps: removing non-alphanumeric/special characters (except periods), new lines, isolated "n" characters, excess spaces, double quotes, and backslashes; converting to lowercase; eliminating stop words; and correcting spelling errors.We transformed the tokenized text using TF-IDF vectorization for ML algorithm suitability.These traditional ML methods served as benchmarks for comparing with LLM prompt results.In analyzing LLM, we employed GPT-4 through the Colab Python OpenAI API, setting hyperparameters to temperature = 0 and top p= 0.01 for optimal automatic scoring [Wang et al., 2023].The data preprocessing mirrored the traditional ML approach but without tokenization or vectorization.We maintained the integrity of student sentences, ensuring capitalized start and appropriate punctuation, mainly periods.The final prompt See Appendix B underwent testing with the same dataset (n=135) used in traditional ML.</p>
<p>In our final experiment, we adopted a subset-based iterative modification approach (n=27) as per the ICL Prompt Design Process 3.2.This involved a sensitivity analysis for precise influence measurement of assertions on LLM performance.Each iteration entailed scrutinizing misclassified data, focusing on informal language nuances in text inputs.This qualitative analysis was pivotal for understanding the impact on model accuracy and response.This systematic approach enriched our comprehension of LLM's interaction with varied prompts and offered insights for enhancing LLM's performance in processing and interpreting informal language, a significant challenge in educational datasets.</p>
<p>Analysis</p>
<p>In our multiclass dataset analysis, we utilize Precision, Recall, and F1 Score to evaluate the performance of LLMs with assertions versus traditional ML models.These metrics are integral for assessing model efficacy in a multiclass environment.Precision gauges the model's accuracy in predicting each class, indicating the reliability of its positive predictions.Recall measures the model's capacity to correctly identify all instances of each class, vital for ensuring comprehensive representation in a multiclass context.The F1 Score, as the harmonic mean of Precision and Recall, offers a balanced evaluation of the model's overall performance, particularly important in our study to address potential class imbalance.Following Pennebaker et al.</p>
<p>[2015], we emphasize both precision and recall to minimize false positives and negatives, crucial in multiclass datasets.Additionally, we assess the percentage change in F1 score performance to quantify the impact of assertions, using the following formula:
Percent Increase = F1 score of LLM − F1 score of traditional ML F1 score of traditional ML × 100%
To further this analysis we examined F1 scores.To differentiate between models, we developed a custom metric, inspired by Cohen's D [Cohen, 2013].However, unlike the traditional Cohen's D, which uses standardized effect sizes (small at 0.2, medium at 0.5, large at 0.8) based on pooled standard deviation, our metric directly compares raw F1 score differences.This modification suits our data, where standard deviation calculations aren't feasible due to single observations per model.We categorized differences in F1 scores as small (up to 10 points), medium (10 to 30 points), and large (over 30 points).We defined a function for calculating pairwise differences in scores m i , m j M represent any two models, and s i , s j are their respective scores.The function:</p>
<p>f (m i , m j ) = s i − s j is defined as the difference between s i and s j .</p>
<p>It computes the difference in performance scores between each pair of models.For each combination of models (m i , m j ), the score of model m j is subtracted from that of model m i .This function calculates the performance difference between each model pair.We then generate a matrix showcasing these differences, allowing for a thorough pairwise comparison of model performances.</p>
<p>To answer RQ 2 and evaluate the ways that the integration of assertions enhance the efficacy of models when addressing the challenges associated with imbalanced textual educational datasets we chose to test on a subset (N=27, P = 10, A = 10, C= 7) as is common in the research to "increase the depth of our analysis, reduce run-time, and decrease cost" [Rodriguez et al., 2023, p. 2].We chose a sensitivity analysis [Akinwande et al., 2023] to critically assess the impact or influence of the assertions.We did this qualitatively by adding two steps (Step 5 &amp; 6 of General COT into the «General COT» and interpreting for the «model outcome» for recurring themes.Our examination extended to a comparative analysis of the experiments, employing class-wise analysis to measure each experiment against a baseline prompt that did not incorporate assertions.</p>
<p>Results and Discussion</p>
<p>3.5 RQ1: How do the results obtained from LLMs with Prompt Engineering compare to traditional Machine Learning algorithms in handling imbalanced educational data?</p>
<p>Performance Metrics</p>
<p>The summary results in Table 3 indicated a varied performance across classes.In the Passive class, the LLM significantly outperformed traditional models, showing a 14.9% increase over SVM, 6.25% over RF, 18.0% over DT, and a notable 23.2% increase over AdaBoost.Conversely, in the Active class, traditional models (SVM, RF, and DT) surpassed LLM by 11.1%, while AdaBoost and LLM performances were comparable.The most striking contrast was observed in the Constructive class, where traditional models (SVM, RF, DT, and AdaBoost) failed to effectively identify instances.In contrast, the LLM demonstrated a remarkable improvement with an F1 score of 32, showcasing its superior capability in recognizing elements of the minority class.</p>
<p>These results suggest that while traditional machine learning models like SVM, RF, DT, and AdaBoost may perform comparably or better in majority classes, the LLM exhibits superior capability in dealing with minority class instances, particularly in complex classification tasks like the Constructive class in our dataset (see Figure 2).The versatility and  adaptability of LLMs in handling imbalanced class distributions highlight their potential in enhancing classification tasks, especially in scenarios where minority classes hold substantial importance.These findings affirm our hypothesis that LLMs, especially when augmented with assertions, offer superior capabilities in classifying cognitive engagement levels from student texts, addressing the core of RQ1.</p>
<p>Relative Performance</p>
<p>We see similar results in our custom metric inspired by Cohen's D due to the unique nature of our data, where standard deviation calculations were not applicable, and produced interesting results (see Figure 5).The LLM with assertions for the passive class demonstrated noteworthy advantages over traditional models in various comparisons which resonate with the work of researchers [Shahriar et al., 2023], who demonstrated the enhanced effectiveness of LLMs in educational settings.Against SVM, the LLM had a significant edge, showing an 11-point advantage in the F1 score, categorized as a 'medium' difference according to our threshold range.This indicates a considerably better performance of the LLM over SVM.When compared to DT, the LLM with assertions again showed a 'medium' difference, outperforming DT by 12 points, underscoring its effectiveness in handling complex classification tasks.In a more striking contrast, the LLM outperformed ADA Boost by 16 points, falling into the 'medium' range and highlighting a substantial performance gap where the LLM was far superior.</p>
<p>In the Active class, the LLM with assertions exhibited a mixed performance.It showed a close competition with SVM, trailing by just 2 points, which falls into the 'small' difference category, implying a nearly equivalent performance between the two models.However, the LLM outperformed ADABoost by a margin of 8 points, a 'small' difference that nonetheless underscores its relative effectiveness.This suggests that while LLMs offer substantial advantages in many areas, their performance can vary depending on the specific classification task, echoing the findings of Lee et al. [2024], who explored the use of LLMs in automatic scoring systems.Against RF and DT, the LLM had a slight disadvantage, trailing by 7 and 3 points respectively, suggesting that in certain scenarios, traditional models may have a slight edge over the LLM.</p>
<p>The Constructive class results were particularly striking.The LLM with assertions demonstrated a pronounced superiority in this category.It dramatically outperformed all traditional models (SVM, RF, DT, and ADABoost), each of which failed to identify instances within the Constructive class effectively, as indicated by their zero scores.The LLM achieved an F1 score of 32, which not only establishes a 'large' difference according to our threshold but also highlights the LLMs exceptional capability in handling minority classes or complex classification tasks where traditional models fall short.It points to the LLMs' superior ability to handle imbalanced datasets, a common challenge in educational data analysis, as illustrated by the work of researchers like Zeng et al. [2023], who evaluated the cognitive and reasoning abilities of LLMs.</p>
<p>RQ2:</p>
<p>In what ways does the integration of assertions enhance the efficacy of models when addressing the challenges associated with imbalanced textual educational datasets?</p>
<p>Our analysis aimed to augment Active class metrics and foster a more equitable model across cognitive classes.Throughout the course of ten experiments, including the baseline, the implementation of assertions, particularly those delineated in «General COT» (Steps 5 &amp; 6, see Appendix B), was pivotal in surfacing two primary themes post the initial experiment: textual ambiguity and contextual comprehension challenges.</p>
<p>For text ambiguity, the baseline experiment revealed the model's propensity to misconstrue the depth of student engagement.Instances where contributions appeared analytical but merely constituted a superficial application of known concepts underscored this issue.By systematically applying the assertions detailed in the Methodology, we observed significant improvements in model performance, particularly within the Active and Constructive classes.</p>
<p>With regard to Unusual language, the model's interpretation of speculative language (e.g., "I think," "possibly," "I believe") as indicative of reflective or analytical thought.Such expressions, particularly when conveying opinions that superficially suggested deeper analysis, were erroneously classified as constructive engagement.</p>
<p>Initially, our approach to integrating assertions was exploratory but became more systematic by the third experiment.For example, between experiments two through four, certain responses intended as "Constructive" were incorrectly classified as "Active":</p>
<p>Misclassified Example 1:</p>
<p>Question: Why do you think the model learned a large negative weight for this feature?Student response: "I think the model learned a negative weight for this feature because the model categorized the reviews as negative and categorized the surprisingly negative features as negative too since that was the whole sentiment of the review."</p>
<p>Misclassified Example 2:</p>
<p>Question: Why do you think the model learned a large positive weight for this feature?Student response: "I feel like it had to do with the words and how much they were used whenever there was a positive review it would contain more than one good word to go along with it"</p>
<p>By incorporating the assertion «Do label the statement as Constructive when they form a hypothesis about why the model learned a weight for a certain feature», these responses were accurately predicted as constructive, enhancing the Constructive class with precision and recall metrics-specifically, a recall increase of 6.33% and an F1-score improvement of 4.30%.Moreover, addressing the misuse of speculative language through the assertion «Avoid labeling a statement as Active or Constructive based solely on speculative language like 'I think' or 'possibly'» (see Figure 4) led to an increase in precision for the Active class by 15.96% and an F1-score increase by 6.08%.This adjustment resulted in the most balanced model performance observed, despite a slight decrease in recall for the Active class by 2.34%.Further attempts to amplify Active class metrics by refining definitions in «General COT» and enhancing reasoning in «FewShot with reasoning» revealed that, while assertions impacted model performance, their effect varied across classes and metrics.</p>
<p>Notably, Experiment 6.1 (see Figure 5) emerged as particularly effective, showcasing the significance of tailored assertions in reducing misclassifications linked to textual ambiguity and unusual language use, thereby contributing to a more balanced and accurate model.</p>
<p>These findings highlight the nuanced role of assertions in enhancing model efficacy against the backdrop of imbalanced educational datasets.By meticulously integrating assertions to counter specific challenges-textual ambiguity and unusual language-the experiments demonstrated a discernible improvement in model precision and balance, particularly within the Active and Constructive classes.This strategic approach underscores the potential of assertions to mitigate inherent dataset imbalances, ultimately contributing to the development of more nuanced and effective educational models.</p>
<p>To further understand our model we compared accuracy of models to the baseline where Experiment 5 marked an 8.96% improvement but Experiment 6.1 stood out with the highest increase in accuracy, at 11.94% from the baseline.This improvement primarily addresses the challenges identified in RQ2, demonstrating the significant role of assertions in resolving errors related to context understanding and lexical ambiguities.The Active and Constructive classes, associated with focused attention and deeper reasoning, respectively, pose classification challenges due to their subtleties and contextual dependencies [Chi and Wylie, 2014].These classes often require inferring cognitive engagement levels from implicit cues and context, making their distinctions less explicit within student responses.</p>
<p>Limitations</p>
<p>While our study sheds light on the potential of LLMs and AEFL in addressing imbalanced datasets, it also highlights the need for caution in interpreting these findings without consideration of the broader methodological and technological landscape.Firstly, our reliance on specific LLM techniques and AEFL might not capture the full spectrum of potential solutions available within the rapidly evolving field of machine learning.The specific parameters and configurations employed in our LLM applications [Shahriar et al., 2023, Wei et al., 2022b, Zeng et al., 2023], while effective in this context, might not be universally applicable or optimal across different datasets or learning tasks.While our study provides valuable insights, it echoes the concerns raised by Radwan and Cataltepe [2017] and Yun et al. [2011] regarding the challenges of imbalanced datasets in education and the limitations of traditional ML approaches.</p>
<p>Furthermore, our study's focus on a AI High School ELA course dataset [Zeng et al., 2023], while providing a rich source of cognitive engagement data, also presents a limitation in terms of diversity and representativeness.The linguistic and cognitive patterns inherent in this specific educational setting may not fully encapsulate the variety of cognitive engagement manifestations across different age groups, subjects, or educational methodologies.This limitation underscores the importance of extending research efforts to encompass a wider range of educational contexts, to ensure the findings' applicability and robustness, as indicated by Fredricks et al. [2004] and Blumenfeld et al. [2006].Additionally, while LLMs and AEFL present innovative approaches to overcoming the challenges of imbalanced datasets, they also introduce new complexities and considerations [Shahriar et al., 2023, Wei et al., 2022b].The computational demands and resource requirements of these technologies, coupled with the need for specialized expertise to implement and interpret their outputs, may pose barriers to widespread adoption and application in educational research and practice.The dynamic nature of LLM development also means that the models and techniques used today may rapidly evolve, necessitating continuous updates and adaptations to maintain their effectiveness and relevance.</p>
<p>Lastly, the ethical implications of applying LLMs in educational settings, particularly concerning data privacy, security, and the potential for bias in model training and outcomes, warrant careful consideration [Zeng et al., 2023].As LLMs become more integrated into educational research and practice, it is crucial to develop and adhere to ethical guidelines that prioritize the well-being and rights of students and educators.</p>
<p>These limitations highlight the need for ongoing research and dialogue within the educational and machine learning communities.By addressing these challenges and exploring the vast potential of LLMs and AEFL, we can advance our understanding of cognitive engagement and enhance educational outcomes in diverse and inclusive ways.</p>
<p>Conclusion and Future Studies</p>
<p>Our study makes significant contributions to the evolving landscape of cognitive engagement (CE) research, building upon the foundational work of seminal researchers like Craik and Lockhart [1972], Appleton et al. [2006], and Fredricks et al. [2004].We leveraged the capabilities of Large Language Models (LLMs) and Assertion Enhanced Few-Shot Learning (AEFL), marking a notable advancement in the domain of CE.This approach pays homage to the pioneering efforts that have shaped our understanding of CE while extending these concepts through the integration of cutting-edge LLM technologies.</p>
<p>By adeptly navigating the challenges posed by imbalanced datasets and accurately classifying cognitive engagement levels, this study underscores the potential of LLMs to refine our measurement and analysis of CE, setting a new benchmark for educational research.The integration of AEFL enhances contextual comprehension, improving model accuracy and balance, as highlighted by Shahriar et al. [2023].Experiment 6.1 further illustrates the value of tailored assertions in reducing misclassifications linked to textual ambiguities, offering novel insights into AEFL's effectiveness in managing class-imbalanced data.</p>
<p>The promising outcomes of this research suggest that LLMs hold significant potential for future educational studies, particularly in complex data analysis tasks.These findings encourage the exploration of LLMs' full capabilities in educational settings, advocating for a paradigm shift towards more sophisticated and nuanced approaches to data analysis.Moreover, the integration of AEFL points to a nuanced method of enhancing model performance, especially in the context of imbalanced textual educational datasets.</p>
<p>Given the multifaceted nature of cognitive engagement and the challenges associated with its measurement, there is a compelling need for further research.Future studies should aim to refine and expand the application of LLMs and AEFL across a broader spectrum of educational contexts.Additionally, exploring additional theoretical frameworks and models could yield deeper insights into cognitive engagement, thereby contributing to the enhancement of educational outcomes.This call for further research not only reflects the complex landscape of CE but also highlights the endless possibilities that LLM technologies and innovative methodologies like AEFL present for advancing our understanding and practices within the educational domain.</p>
<p>Acknowledgments</p>
<p>This material is based upon work supported by the National Science Foundation under Grant No. DRL-1949110.Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<p>Step 5: To upgrade the statement to a higher engagement level, propose alterations that would make it align with the criteria for the "Active" category.This could involve adding details that show the application of learned knowledge to familiar yet slightly different contexts, or demonstrating problem-solving based on previous experiences.</p>
<p>Step 6: Explore how the statement can be restructured to meet the criteria of the "Constructive" engagement category.Consider adding elements that showcase deeper analysis, critical evaluation, or synthesis of multiple concepts to create a more nuanced and thoughtful response.</p>
<p>Step 7: Finally, revisit the question and statement to evaluate the original cognitive engagement level making sure the prediction of cognitive engagement is accurate.</p>
<p>Based on your understanding of cognitive engagement and the labeled examples provided, determine the level of engagement for the unlabeled text provided.</p>
<p>'''</p>
<p>Question: Why do people write reviews?</p>
<p>Statement: People write reviews to express their feelings on a certain thing to condemn a praise a business, franchise, movie, or book.</p>
<p>Label: <Generate label> Chain-of-thought: <Generate the chain-of-thought> '''</p>
<p>Use the following examples delimited by triple quotes to understand which label the statement belongs to.</p>
<p>'''</p>
<p>Question: What features do you think are indicators of positive reviews?</p>
<p>Statement: Words like love, excellent, greatest, amazing, enjoy, awesome, best.</p>
<p>Label: Passive</p>
<p>Reasoning: because it is a direct response that involves recalling or listing words without further analysis or interaction.</p>
<p>Question: What is one strategy you (as a human) can use to determine if a review is positive or negative?</p>
<p>Statement: I can tell if the person liked something or not.Label: Passive Reasoning: because it does not specify any strategies or reflection to distinguish between positive and negative sentiments.</p>
<p>Question: When you click on the row, the feature in this review will be highlighted in the feature graph (like the one you have seen in the Light On Light Off activity).Which feature do you think is it?</p>
<p>Statement: Because it's associated with positivity.</p>
<p>Label: Passive</p>
<p>Reasoning: because it is simple information without reflection without delving into specific details, analysis, or reflection.</p>
<p>Question</p>
<p>Label: Active</p>
<p>Reasoning: because it demonstrates the application and manipulation of knowledge in a familiar context without generating new insights.</p>
<p>Question: Why do people write reviews?</p>
<p>Statement: To share their experience of a certain product or service so that they can either warn or recommend it to people.Sharing experiences is important so that way others who have not experienced it can know what they are getting in to.</p>
<p>Label: Constructive</p>
<p>Reasoning: because it provides an understanding and reasoning of the broader context and implications why sharing experience is important.</p>
<p>Question: If none of the 10 features are present in your review, try again with another review.If some of the 10 features are in your review, examine both your review and the feature graph.What do you think these features are?</p>
<p>Statement: I think these features are key words and numbers.Like the example used the word 'love' which implies a positive reply.The numbers also because if you say 1 out of 10 that's bad but if you say 10 out of 10 that's good.</p>
<p>Label: Constructive Reasoning: because it provides interpretation and application to generate insights about the potential features in reviews.</p>
<p>Question: What is one strategy you (as a human) can use to determine if a review is positive or negative?</p>
<p>Statement: If I am having a conversation with somebody it will be easy to detect if the review is good or bad by word choice and their tone.If they wrote it, I will be able to see key words that point in either a positive or negative direction.</p>
<p>Label: Constructive</p>
<p>Reasoning: because it demonstrates a depth of reasoning and reflection of how to determine if a review is positive or negative.</p>
<p>Question: What kinds of reviews can make our world a better place?</p>
<p>Statement: Some reviews that can make the world a better place is if it's a review about a foreign country then it can give some insight into what is happening within that country.Or even here in the United States, it can share what's happening within their state and let the rest of the world know.</p>
<p>Label: Constructive</p>
<p>Reasoning: because it provides reflection, thoughtful consideration and reasoning about the societal value and potential impact of reviews in fostering global understanding and awareness.</p>
<p>Figure 1 :
1
Figure 1: ICL Prompt Engineering Design Process to optimize the accuracy of LLMs in classifying educational data with the use of ICL, COT and AEFL.</p>
<p>Figure 2 :
2
Figure 2: Performance Metrics Summary by Cognitive Engagement Class showing results for each cognitive class.</p>
<p>Figure 3 :
3
Figure 3: Relative Performance Heatmap by Cognitive Engagement Class</p>
<p>Figure 4 :
4
Figure 4: Left image does not include a targeted assertion while the one on the right does and improves the model output to correctly predict the students cognitive level of their text.</p>
<p>Figure 5 :
5
Figure 5: Percentage Change in Metrics for Each Class Across Experiments</p>
<p>to understanding and enhancing CE are theoretical frameworks and models like Bloom's taxonomy, Corno and Mandinach's model, and the ICAP model, as well as Wang et al.'s framework for connectivist learning contexts.These models provide comprehensive insights into the various dimensions and components of cognitive engagement, aiding researchers in designing effective studies, developing targeted interventions, and evaluating educational outcomes [Anderson and Krathwohl</p>
<p>Table 1 :
1
ICAP: Cognitive Engagement Label Coding Scheme
Score ICAP LevelDescriptionIndicatorExample2Constructive New information is inte-Deep reasoning, synthesis of"I think that the modelgrated with activated priornew ideas, or forming hy-learned a large positiveknowledge, and new knowl-potheses.weight for the featureedge is inferred.because if you came toan establishment then thatwould indicate that you didlike it because you chose tocome in the first place."1ActiveBehaviors that cause-focusedApply, Analyze, or Manipu-"I think this gained a largeattention while manipulat-lating.amount of weight because iting.is a commonly used word."0PassiveOvert activities that are car-Recalling or Restating."Amazing, clean, selection,ried out mindlessly.try, regular, seating"</p>
<p>Table 2 :
2
Dataset numbers for Training, Testing and subsets by cognitive level.
ICAP Level Training Testing SubsetC2026210A2036610P2777</p>
<dl>
<dt>Table 3 :</dt>
<dt>3</dt>
<dt>Summary of Performance Metrics by Cognitive Engagement Level</dt>
<dt>Passive (62)Active (66)Constructive (7)ModelPR F1 PR F1 PRF1SVM75 73 74 68 77 72 000RF70 92 80 80 65 72 000DT71 74 72 71 73 72 000ADABoost 64 74 69 65 62 64 000LLM83 87 85 78 54 64 21 7132</dt>
<dd>
<p>What is one strategy you can use to determine what features someone has used to build a classification model?Statement: I can use major words that people say in reviews first.Words like 'love,' 'hate,' 'bad,' 'delicious,' and more.You can look at the data set and find words that really stand out to you or words that have a strong emotional connotation.You can also check the graphand the probability in terms of the features being used or how strongly they correlate with the result.Label: ActiveReasoning: because it summarizes and organizes the information in a broad manner Question: What is one strategy you (as a human) can use to determine if a review is positive or negative?Statement: One strategy that you can use to determine if a review is positive or negative is looking at diction, which is word choice, and how the words are being used.Label: ActiveReasoning: because it details a method of analyzing the word choice in reviews, demonstrating the application of acquired knowledge to assess sentiments.Question: When you click on the row, the feature in this review will be highlighted in the feature graph (like the one you have seen in the Light On Light Off activity).Which feature do you think is it?Statement: Love is the most defining word in this review, if it were changed to 'hate' it would have a completely different meaningLabel: ActiveReasoning: because it demonstrates the application and analysis of knowledge in a familiar context but does not generate new ideas.Question: What is one strategy you can use to determine what features someone has used to build a classification model?Statement: You can test multiple reviews with words that you think may be the features to determine if they are actually features.
Label: PassiveReasoning: because it only has recall words and delve into any analysis, reflection, or application.Question: What is one strategy you can use to determine what features someone has used to build a classificationmodel?Statement:
A Appendix A ()Your task is to identify the label of the statement delimited by triple backticks Read the instructions below:Step 1: Read the question and statement attentively to understand the context and the nature of the statement provided.Step 2: Determine the initial cognitive engagement level of the statement using the definitions of the provided cognitive engagement labels -passive, active, and constructive.1. Passive engagement: a statement is classified as "Passive" when the individual is only receiving information without interacting with it or adding anything to it.Passive engagement typically involves listening, reading, or receiving information without actively processing, manipulating, or reflecting upon it.2. Active engagement: a statement is classified as "Active" when the response involves applying knowledge, analyzing information, or manipulating information but not generating new ideas or concepts.3. Constructive engagement: a statement is classified as "Constructive" if it reflects reasoning, justification, or thoughtful consideration based on prior knowledge.Step 3: Assess why it corresponds to the label you placed it in.Consider the extent to which it demonstrates recall of basic information (passive), application of learned knowledge to slightly different contexts (active), or a deeper level of analysis and synthesis of various concepts (constructive).Step 4: Critically evaluate whether the statement could potentially belong to other labels.Examine the nuances of the statement to see if there are elements that might indicate a higher or lower level of cognitive engagement.'''A few facts about identifying the cognitive engagement level that you must assert while determining the level of engagement for the unlabeled text provided:-Do label the statement as Constructive if they are forming an opinion about its usefulness, and providing reasoning for their opinion.-Do label the statement as Constructive when the statement provides their interpretation and reasoning to the question.-Do label the statement as Constructive when they form a hypothesis about why the model learned a weight for a certain feature.-Do label the statement as Constructive when the statement shows active engagement with the information.-Avoid labeling a statement as Active or Constructive based solely on speculative language like 'I think' or 'possibly'.
Automatic engagement estimation in smart education/learning settings: a systematic review of engagement definitions, datasets, and methods. Nur Shofiyati, Shinobu Karimah, Hasegawa, Smart Learning Environments. 91312022</p>
</dd>
</dl>
<p>Improving performance prediction on education data with noise and class imbalance. Intelligent Automation &amp; Soft Computing. M Akram, Zehra Radwan, Cataltepe, 2017</p>
<p>An effective over-sampling method for imbalanced data sets classification. Ma Zhai Yun, Ruan Nan, Da, Bing, Chinese Journal of Electronics. 2032011</p>
<p>School engagement: Potential of the concept, state of the evidence. Jennifer A Fredricks, Phyllis C Blumenfeld, Alison H Paris, Review of educational research. 7412004</p>
<p>Motivation and cognitive engagement in learning environments. Phyllis C Blumenfeld, Toni M Kempler, Joseph S Krajcik, 2006</p>
<p>The role of cognitive engagement in classroom learning and motivation. Lyn Corno, Ellen B Mandinach, Educational psychologist. 1821983</p>
<p>The role of goal orientation in self-regulated learning. Paul R Pintrich, Handbook of self-regulation. Elsevier2000</p>
<p>Paul R Dale H Schunk, Judith L Pintrich, Meece, Motivation in education: Theory, research, and applications. (No Title). 2014</p>
<p>Assertion enhanced few-shot learning: Instructive technique for large language models to generate educational explanations. Tasmia Shahriar, Noboru Matsuda, Kelly Ramos, arXiv:2312.031222023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682arXiv:2312.17080Zhongshen Zeng, Pengguang Chen, Haiyun Jiang, and Jiaya Jia. Challenge llms to reason about reasoning: A benchmark to unveil cognitive depth in llms. 2022a. 2023arXiv preprint</p>
<p>Learning analytics on structured and unstructured heterogeneous data sources: Perspectives from procrastination, help-seeking, and machine-learning defined cognitive engagement. Jiun-Yu Wu, Computers &amp; Education. 1631040662021</p>
<p>Chainof-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 2022b35</p>
<p>Applying large language models and chain-of-thought for automatic scoring. Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, Xiaoming Zhai, Computers and Education: Artificial Intelligence. Fergus IM Craik and Robert S Lockhart1162024. 1972Journal of verbal learning and verbal behavior</p>
<p>Measuring cognitive and psychological engagement: Validation of the student engagement instrument. Sandra L James J Appleton, Dongjin Christenson, Amy L Kim, Reschly, Journal of school psychology. 4452006</p>
<p>The icap framework: Linking cognitive engagement to active learning outcomes. T H Michelene, Ruth Chi, Wylie, Educational psychologist. 4942014</p>
<p>A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives: complete edition. W Lorin, David R Anderson, Krathwohl, 2001Addison Wesley Longman, Inc</p>
<p>Taxonomy of educational objectives: The classification of educational goals, by a committee of college and university examiners. Handbook 1: Cognitive domain. S Benjamin, Bloom, 1956</p>
<p>How teacher talk guidance during invention activities shapes students' cognitive engagement and transfer. Catherine C Chase, Jenna Marks, Laura J Malkiewich, Helena Connolly, International Journal of STEM Education. 62019</p>
<p>Developing a plugged-in class observation protocol in high-school blended stem classes: Student engagement, teacher behaviors and student-teacher interaction patterns. Jo-Chi Hsiao, Ssu-Kuang Chen, Wei Chen, Sunny Sj Lin, Computers &amp; Education. 1781044032022</p>
<p>Towards triggering higher-order thinking behaviors in moocs. Xu Wang, Miaomiao Wen, Carolyn P Rosé, Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge. the Sixth International Conference on Learning Analytics &amp; Knowledge2016</p>
<p>Defining, measuring, and scaling affective constructs. Instrument development in the affective domain: School and corporate applications. Betsy Mccoach, Robert K Gable, John P Madura, Betsy Mccoach, Robert K Gable, John P Madura, 2013</p>
<p>Predicting high school students' cognitive engagement and achievement: Contributions of classroom perceptions and motivation. Barbara A Greene, Raymond B Miller, H Michael Crowson, Bryan L Duke, Kristine L Akey, Contemporary educational psychology. 2942004</p>
<p>Measuring students' cognitive engagement on assessment tests: A confirmatory factor analysis of the short form of the cognitive engagement scale. Whitney Smiley, Robin Anderson, ; Charles W Anderson, American educational research journal. 632011. 1993Research &amp; Practice in Assessment</p>
<p>Identifying cognitive engagement in the mathematics classroom. Sue Helme, David Clarke, Mathematics Education Research Journal. 1322001</p>
<p>Role of reading engagement in mediating effects of reading comprehension instruction on reading outcomes. Allan Wigfield, John T Guthrie, Kathleen C Perencevich, Ana Taboada, Susan Lutz Klauda, Angela Mcrae, Pedro Barbosa, Psychology in the Schools. 4552008</p>
<p>Affordances of using mobile technology to support experiencesampling method in examining college students' engagement. Kui Xie, Benjamin C Heddy, Barbara A Greene, Computers &amp; Education. 1282019</p>
<p>Advanced, analytic, automated (aaa) measurement of engagement during learning. D' Sidney, Ed Mello, Angela Dieterle, Duckworth, Educational psychologist. 5222017</p>
<p>The effects of achievement goals and self-regulated learning behaviors on reading comprehension in technology-enhanced learning environments. James P Matthew L Bernacki, Jennifer G Byrnes, Cromley, Contemporary Educational Psychology. 3722012</p>
<p>Language style matching, engagement, and impasse in negotiations. Molly E Ireland, Marlone D Henderson, Negotiation and conflict management research. 712014</p>
<p>Teacher question and student response with regard to cognition and language use. Instructional science. Youngju Lee, Mable B Kinzie, 201240</p>
<p>Data mining for imbalanced datasets: An overview. Data mining and knowledge discovery handbook. V Nitesh, Chawla, 2010</p>
<p>Learning from imbalanced data sets. Alberto Fernández, Salvador García, Mikel Galar, C Ronaldo, Bartosz Prati, Francisco Krawczyk, Herrera, 2018Springer10</p>
<p>Foundations of data imbalance and solutions for a data democracy. Ajay Kulkarni, Deri Chong, Feras A Batarseh, Data democracy. Elsevier2020</p>
<p>The class imbalance problem: A systematic study. Nathalie Japkowicz, Shaju Stephen, 20026Intelligent data analysis</p>
<p>Practical statistics for data scientists: 50+ essential concepts using R and Python. Peter Bruce, Andrew Bruce, Peter Gedeck, 2020O'Reilly Media</p>
<p>Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning. Guillaume Lemaãžtre, Fernando Nogueira, Christos K Aridas, Journal of machine learning research. 18172017</p>
<p>Efficient classification of student help requests in programming courses using large language models. Jaromir Savelka, Paul Denny, Mark Liffiton, Brad Sheese, arXiv:2310.201052023arXiv preprint</p>
<p>A divide-conquer-reasoning approach to consistency evaluation and improvement in blackbox large language models. Wendi Cui, Jiaxin Zhang, Zhuohang Li, Damien Lopez, Kamalika Das, Bradley Malin, Sricharan Kumar, Socially Responsible Language Modelling Research. 2023</p>
<p>Storyq: a web-based machine learning and text mining tool for k-12 students. Jie Chao, Bill Finzer, Carolyn P Rosé, Shiyan Jiang, Michael Yoder, James Fiacco, Chas Murray, Cansu Tatar, Kenia Wiedemann, Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. the 53rd ACM Technical Symposium on Computer Science Education V20222</p>
<p>Investigating the learning behaviour of in-context learning: a comparison with supervised learning. Andrew Ng, Isa Fulford, A Ng ; Xindi, Yufei Wang, Can Wang, Xiubo Xu, Bowen Geng, Chongyang Zhang, Frank Tao, Robert E Rudzicz, Daxin Mercer, Jiang, arXiv:2307.15411deeplearning. ai2023. 2023arXiv preprintChatgpt prompt engineering for developers</p>
<p>The development and psychometric properties of liwc2015. Ryan L James W Pennebaker, Kayla Boyd, Kate Jordan, Blackburn, 2015</p>
<p>Statistical power analysis for the behavioral sciences. Jacob Cohen, 2013Routledge</p>
<p>Prompts matter: Insights and strategies for prompt engineering in automated software traceability. Alberto D Rodriguez, Katherine R Dearstyne, Jane Cleland-Huang, 2023 IEEE 31st International Requirements Engineering Conference Workshops (REW). IEEE2023</p>
<p>Understanding prompt engineering may not require rethinking generalization. Yiding Victor Akinwande, Dylan Jiang, J Zico Sam, Kolter, arXiv:2310.039572023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
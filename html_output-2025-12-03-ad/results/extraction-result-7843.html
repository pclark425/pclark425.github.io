<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7843 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7843</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7843</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-4161ad2d2495d8af1d62dc5e71882bde642cd1c1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4161ad2d2495d8af1d62dc5e71882bde642cd1c1" target="_blank">Large Language Models Are State-of-the-Art Evaluators of Translation Quality</a></p>
                <p><strong>Paper Venue:</strong> European Association for Machine Translation Conferences/Workshops</p>
                <p><strong>Paper TL;DR:</strong> GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without, is described, and achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels.</p>
                <p><strong>Paper Abstract:</strong> We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate seven versions of GPT models, including ChatGPT. We show that our method for translation quality assessment only works with GPT 3.5 and larger models. Comparing to results from WMT22â€™s Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7843.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7843.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMBA-GPT4-DA (system-level)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMBA GPT-4 Direct Assessment (reference-based) system-level comparison to MQM humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>System-level evaluation showing GPT-4 prompted with a DA-style GEMBA prompt achieves state-of-the-art agreement with MQM human labels on WMT22 (system-level pairwise accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models Are State-of-the-Art Evaluators of Translation Quality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Machine translation quality evaluation (reference-based)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MQM 2022 test set (WMT22 metrics shared task)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-4 (OpenAI; limited public details on architecture/training; used via Microsoft Azure in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Professional MQM annotators (WMT22 expert MQM labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>system-level pairwise accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>89.8</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>segment-level lower correlation due to ties and discrete outputs; test coverage limited to three language pairs; possible (though unlikely) overlap of test data with model training data</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 GEMBA in DA style achieves state-of-the-art system-level agreement with MQM human rankings, outperforming other automatic metrics; authors note human labels are noisy so perfect agreement is impossible; segment-level correlation remains lower.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Very high system-level accuracy (state-of-the-art among compared automatic metrics); works in both reference-based and reference-less modes; scalable and reproducible via prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompting with GEMBA-DA template per segment (reference provided), one GPT-4 request per segment, segment scores averaged to system-level, evaluation across 54 systems and ~106k segments over three language pairs (en-de, en-ru, zh-en); primary metric: system-level pairwise accuracy using WMT22 official script.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are State-of-the-Art Evaluators of Translation Quality', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7843.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7843.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMBA-GPT4-DA[noref] (QE system-level)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMBA GPT-4 Direct Assessment (reference-less/quality estimation) system-level comparison to MQM humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reference-less (quality estimation) GEMBA using GPT-4 achieves very high system-level agreement with MQM human labels and outperforms other referenceless metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models Are State-of-the-Art Evaluators of Translation Quality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Machine translation quality estimation (no human reference)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MQM 2022 test set (WMT22 metrics shared task)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-4 (used zero-shot with GEMBA-DA prompt variant [noref])</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Professional MQM annotators (WMT22 expert MQM labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>system-level pairwise accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>87.6</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>very peaked/discrete score distribution (e.g., many 95s) leading to segment-level ties; segment-level correlation weaker than some top metrics; limited language-pair coverage</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Despite producing many identical segment scores (e.g., many 95 values), the reference-less GPT-4 metric still differentiates systems sufficiently to yield the best referenceless system-level accuracy and is close to reference-based performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High-quality evaluation without requiring a human reference; outperforms prior referenceless metrics on system-level agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot GEMBA-DA[noref] prompt (no reference), per-segment scoring with GPT-4, average to system-level, evaluated on MQM 2022 (three language pairs), compared using WMT22 system-level pairwise accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are State-of-the-Art Evaluators of Translation Quality', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7843.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7843.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Segment-level Kendall Tau comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Segment-level correlation (Kendall's Tau) of GEMBA LLM judges vs MQM human labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Segment-level evaluation using Kendall's Tau shows GPT-4 has high but not best-in-class correlation with MQM human judgments; discrete outputs and ties reduce Tau scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models Are State-of-the-Art Evaluators of Translation Quality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Segment-level correlation of translation quality scores</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MQM 2022 test set (WMT22 metrics shared task)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (and Davinci-003 for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 and Davinci-003 (GPT-3.5 family) evaluated with multiple prompt styles (DA, SQM, Stars, Classes); outputs often discrete integers 0-100</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Professional MQM annotators (segment-level MQM ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's Tau (tau-b, tie-aware variant used by WMT22)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.36</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>discrete scoring leading to many ties reduces Kendall's Tau; GEMBA returns a limited set of values (frequent 95, 90, 80, 100) which penalizes segment-level ranking metrics; QE variants (Dav3-DA[noref]) show notably lower segment correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 segment-level Kendall's Tau (~0.36 across language pairs) is slightly below the best automatic metrics but still shows substantial correlation; authors attribute much of the gap to ties from discrete score distributions rather than fundamentally wrong judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High correlation despite zero-shot prompting; performance close to top metrics at segment level for some prompt variants (e.g., SQM).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Segment-level Kendall's Tau-b computed per language pair (en-de, en-ru, zh-en) using WMT22 script; multiple prompt variants and models compared; observed distribution of returned scores analyzed (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are State-of-the-Art Evaluators of Translation Quality', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7843.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7843.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM failure modes & limitations vs humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed failure modes and limitations of LLM judges compared to human MQM judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper documents specific limitations where LLM judges underperform or produce problematic outputs relative to human evaluation: small models produce invalid outputs, occasional textual explanations, non-determinism from resampling, discrete-peaked scores causing ties, and limited language coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models Are State-of-the-Art Evaluators of Translation Quality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Analysis of evaluation reliability and failure modes of LLM-based judges for MT quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MQM 2022 test set (WMT22 metrics shared task)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Various GPT-family models (GPT-2, Ada, Babbage, Curie, Davinci-002/003, ChatGPT, gpt-3.5-turbo, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Range from GPT-2/Ada (failed to produce valid numeric scores) through GPT-3.5 variants (Davinci-002/003, ChatGPT, Turbo) to GPT-4 (best performance); invalid-answer counts and model-dependent behavior reported (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Professional MQM annotators (WMT22)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>small models produce meaningless/invalid answers; some models append textual explanations (requiring parsing); occasional invalid outputs necessitate re-sampling with added randomness; non-determinism introduced by re-sampling; discrete score distribution causes ties and penalizes Kendall Tau; limited evaluation to 3 language pairs limits generality; potential (unlikely) data contamination concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-2 and Ada fail to follow numeric scoring; Babbage/Curie approach random-like performance; GPT-3.5+ show major improvement; ChatGPT/Turbo sometimes add explanations; temperature adjustments used to recover valid numeric outputs; overall failure rate of invalid answers is <1% for most model/prompt combos except SQM-style where more invalids observed.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Authors note GEMBA is mostly deterministic and has a low failure rate for strong models (GPT-3.5+); LLMs provide easily-parsable numeric outputs in most cases and can be re-prompted when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Monosegment zero-shot prompts; when invalid (non-numeric) outputs occur, authors increased sampling temperature and re-requested until a numeric answer in expected range was obtained; Table 6 reports counts of invalid answers (out of ~106,758 responses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are State-of-the-Art Evaluators of Translation Quality', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7843.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7843.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human MQM noise ceiling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human MQM annotation noise and its effect on achievable metric agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors note MQM human labels are noisy, so an automatic metric cannot achieve 100% agreement with humans; this bounds interpretation of LLM vs human agreement scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models Are State-of-the-Art Evaluators of Translation Quality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Contextual discussion about limits of automatic metric agreement with human MQM judgments</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MQM 2022 test set (WMT22 metrics shared task)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>n/a (discussion applies to all automatic judges including LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Professional MQM annotators (WMT22), noted to produce noisy labels</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>noisy human labels limit the ceiling for automatic metric agreement; observed human labeling noise implies 100% agreement is unattainable</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Authors explicitly state that human labels are noisy and thus even strong automatic metrics (including GEMBA/GPT-4) cannot reach 100% agreement; this frames reported agreement numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Noted implication: high agreement of LLM judges should be interpreted relative to an imperfect human gold standard; strong LLM agreement despite noise is encouraging.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Framing comment used when interpreting system-level pairwise accuracy results; no additional experiments beyond MQM comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are State-of-the-Art Evaluators of Translation Quality', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How good are gpt models at machine translation? a comprehensive evaluation <em>(Rating: 2)</em></li>
                <li>Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt <em>(Rating: 2)</em></li>
                <li>Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust <em>(Rating: 2)</em></li>
                <li>COMET: A neural framework for MT evaluation <em>(Rating: 1)</em></li>
                <li>BLEURT: Learning robust metrics for text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7843",
    "paper_id": "paper-4161ad2d2495d8af1d62dc5e71882bde642cd1c1",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "GEMBA-GPT4-DA (system-level)",
            "name_full": "GEMBA GPT-4 Direct Assessment (reference-based) system-level comparison to MQM humans",
            "brief_description": "System-level evaluation showing GPT-4 prompted with a DA-style GEMBA prompt achieves state-of-the-art agreement with MQM human labels on WMT22 (system-level pairwise accuracy).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
            "evaluation_task": "Machine translation quality evaluation (reference-based)",
            "dataset_name": "MQM 2022 test set (WMT22 metrics shared task)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "gpt-4 (OpenAI; limited public details on architecture/training; used via Microsoft Azure in experiments)",
            "human_evaluator_type": "Professional MQM annotators (WMT22 expert MQM labels)",
            "agreement_metric": "system-level pairwise accuracy",
            "agreement_score": 89.8,
            "reported_loss_aspects": "segment-level lower correlation due to ties and discrete outputs; test coverage limited to three language pairs; possible (though unlikely) overlap of test data with model training data",
            "qualitative_findings": "GPT-4 GEMBA in DA style achieves state-of-the-art system-level agreement with MQM human rankings, outperforming other automatic metrics; authors note human labels are noisy so perfect agreement is impossible; segment-level correlation remains lower.",
            "advantages_of_llm_judge": "Very high system-level accuracy (state-of-the-art among compared automatic metrics); works in both reference-based and reference-less modes; scalable and reproducible via prompts.",
            "experimental_setting": "Zero-shot prompting with GEMBA-DA template per segment (reference provided), one GPT-4 request per segment, segment scores averaged to system-level, evaluation across 54 systems and ~106k segments over three language pairs (en-de, en-ru, zh-en); primary metric: system-level pairwise accuracy using WMT22 official script.",
            "uuid": "e7843.0",
            "source_info": {
                "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GEMBA-GPT4-DA[noref] (QE system-level)",
            "name_full": "GEMBA GPT-4 Direct Assessment (reference-less/quality estimation) system-level comparison to MQM humans",
            "brief_description": "Reference-less (quality estimation) GEMBA using GPT-4 achieves very high system-level agreement with MQM human labels and outperforms other referenceless metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
            "evaluation_task": "Machine translation quality estimation (no human reference)",
            "dataset_name": "MQM 2022 test set (WMT22 metrics shared task)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "gpt-4 (used zero-shot with GEMBA-DA prompt variant [noref])",
            "human_evaluator_type": "Professional MQM annotators (WMT22 expert MQM labels)",
            "agreement_metric": "system-level pairwise accuracy",
            "agreement_score": 87.6,
            "reported_loss_aspects": "very peaked/discrete score distribution (e.g., many 95s) leading to segment-level ties; segment-level correlation weaker than some top metrics; limited language-pair coverage",
            "qualitative_findings": "Despite producing many identical segment scores (e.g., many 95 values), the reference-less GPT-4 metric still differentiates systems sufficiently to yield the best referenceless system-level accuracy and is close to reference-based performance.",
            "advantages_of_llm_judge": "High-quality evaluation without requiring a human reference; outperforms prior referenceless metrics on system-level agreement.",
            "experimental_setting": "Zero-shot GEMBA-DA[noref] prompt (no reference), per-segment scoring with GPT-4, average to system-level, evaluated on MQM 2022 (three language pairs), compared using WMT22 system-level pairwise accuracy.",
            "uuid": "e7843.1",
            "source_info": {
                "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Segment-level Kendall Tau comparisons",
            "name_full": "Segment-level correlation (Kendall's Tau) of GEMBA LLM judges vs MQM human labels",
            "brief_description": "Segment-level evaluation using Kendall's Tau shows GPT-4 has high but not best-in-class correlation with MQM human judgments; discrete outputs and ties reduce Tau scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
            "evaluation_task": "Segment-level correlation of translation quality scores",
            "dataset_name": "MQM 2022 test set (WMT22 metrics shared task)",
            "judge_model_name": "GPT-4 (and Davinci-003 for comparison)",
            "judge_model_details": "GPT-4 and Davinci-003 (GPT-3.5 family) evaluated with multiple prompt styles (DA, SQM, Stars, Classes); outputs often discrete integers 0-100",
            "human_evaluator_type": "Professional MQM annotators (segment-level MQM ratings)",
            "agreement_metric": "Kendall's Tau (tau-b, tie-aware variant used by WMT22)",
            "agreement_score": 0.36,
            "reported_loss_aspects": "discrete scoring leading to many ties reduces Kendall's Tau; GEMBA returns a limited set of values (frequent 95, 90, 80, 100) which penalizes segment-level ranking metrics; QE variants (Dav3-DA[noref]) show notably lower segment correlation.",
            "qualitative_findings": "GPT-4 segment-level Kendall's Tau (~0.36 across language pairs) is slightly below the best automatic metrics but still shows substantial correlation; authors attribute much of the gap to ties from discrete score distributions rather than fundamentally wrong judgments.",
            "advantages_of_llm_judge": "High correlation despite zero-shot prompting; performance close to top metrics at segment level for some prompt variants (e.g., SQM).",
            "experimental_setting": "Segment-level Kendall's Tau-b computed per language pair (en-de, en-ru, zh-en) using WMT22 script; multiple prompt variants and models compared; observed distribution of returned scores analyzed (Table 5).",
            "uuid": "e7843.2",
            "source_info": {
                "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "LLM failure modes & limitations vs humans",
            "name_full": "Observed failure modes and limitations of LLM judges compared to human MQM judgments",
            "brief_description": "Paper documents specific limitations where LLM judges underperform or produce problematic outputs relative to human evaluation: small models produce invalid outputs, occasional textual explanations, non-determinism from resampling, discrete-peaked scores causing ties, and limited language coverage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
            "evaluation_task": "Analysis of evaluation reliability and failure modes of LLM-based judges for MT quality assessment",
            "dataset_name": "MQM 2022 test set (WMT22 metrics shared task)",
            "judge_model_name": "Various GPT-family models (GPT-2, Ada, Babbage, Curie, Davinci-002/003, ChatGPT, gpt-3.5-turbo, GPT-4)",
            "judge_model_details": "Range from GPT-2/Ada (failed to produce valid numeric scores) through GPT-3.5 variants (Davinci-002/003, ChatGPT, Turbo) to GPT-4 (best performance); invalid-answer counts and model-dependent behavior reported (Table 6).",
            "human_evaluator_type": "Professional MQM annotators (WMT22)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "small models produce meaningless/invalid answers; some models append textual explanations (requiring parsing); occasional invalid outputs necessitate re-sampling with added randomness; non-determinism introduced by re-sampling; discrete score distribution causes ties and penalizes Kendall Tau; limited evaluation to 3 language pairs limits generality; potential (unlikely) data contamination concerns.",
            "qualitative_findings": "GPT-2 and Ada fail to follow numeric scoring; Babbage/Curie approach random-like performance; GPT-3.5+ show major improvement; ChatGPT/Turbo sometimes add explanations; temperature adjustments used to recover valid numeric outputs; overall failure rate of invalid answers is &lt;1% for most model/prompt combos except SQM-style where more invalids observed.",
            "advantages_of_llm_judge": "Authors note GEMBA is mostly deterministic and has a low failure rate for strong models (GPT-3.5+); LLMs provide easily-parsable numeric outputs in most cases and can be re-prompted when needed.",
            "experimental_setting": "Monosegment zero-shot prompts; when invalid (non-numeric) outputs occur, authors increased sampling temperature and re-requested until a numeric answer in expected range was obtained; Table 6 reports counts of invalid answers (out of ~106,758 responses).",
            "uuid": "e7843.3",
            "source_info": {
                "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Human MQM noise ceiling",
            "name_full": "Human MQM annotation noise and its effect on achievable metric agreement",
            "brief_description": "Authors note MQM human labels are noisy, so an automatic metric cannot achieve 100% agreement with humans; this bounds interpretation of LLM vs human agreement scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
            "evaluation_task": "Contextual discussion about limits of automatic metric agreement with human MQM judgments",
            "dataset_name": "MQM 2022 test set (WMT22 metrics shared task)",
            "judge_model_name": "n/a (discussion applies to all automatic judges including LLMs)",
            "judge_model_details": null,
            "human_evaluator_type": "Professional MQM annotators (WMT22), noted to produce noisy labels",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "noisy human labels limit the ceiling for automatic metric agreement; observed human labeling noise implies 100% agreement is unattainable",
            "qualitative_findings": "Authors explicitly state that human labels are noisy and thus even strong automatic metrics (including GEMBA/GPT-4) cannot reach 100% agreement; this frames reported agreement numbers.",
            "advantages_of_llm_judge": "Noted implication: high agreement of LLM judges should be interpreted relative to an imperfect human gold standard; strong LLM agreement despite noise is encouraging.",
            "experimental_setting": "Framing comment used when interpreting system-level pairwise accuracy results; no additional experiments beyond MQM comparison.",
            "uuid": "e7843.4",
            "source_info": {
                "paper_title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "rating": 2,
            "sanitized_title": "how_good_are_gpt_models_at_machine_translation_a_comprehensive_evaluation"
        },
        {
            "paper_title": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt",
            "rating": 2,
            "sanitized_title": "error_analysis_prompting_enables_humanlike_translation_evaluation_in_large_language_models_a_case_study_on_chatgpt"
        },
        {
            "paper_title": "Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust",
            "rating": 2,
            "sanitized_title": "results_of_wmt22_metrics_shared_task_stop_using_bleu_neural_metrics_are_better_and_more_robust"
        },
        {
            "paper_title": "COMET: A neural framework for MT evaluation",
            "rating": 1,
            "sanitized_title": "comet_a_neural_framework_for_mt_evaluation"
        },
        {
            "paper_title": "BLEURT: Learning robust metrics for text generation",
            "rating": 1,
            "sanitized_title": "bleurt_learning_robust_metrics_for_text_generation"
        }
    ],
    "cost": 0.012025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Are State-of-the-Art Evaluators of Translation Quality</h1>
<p>Tom Kocmi and Christian Federmann<br>Microsoft, One Microsoft Way, Redmond, WA-98052, USA<br>{tomkocmi, chrife}@microsoft.com</p>
<h4>Abstract</h4>
<p>We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zeroshot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate nine versions of GPT models, including ChatGPT and GPT-4. We show that our method for translation quality assessment only works with GPT 3.5 and larger models. Comparing to results from WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>One of the interesting properties of large language models (LLMs) such as GPT (Brown et al., 2020b) is their (implicit) support for multilingual Q\&amp;A. Prompting the model in the right way allows us to translate text between languages (Vilar et al., 2022). This is surprising as GPT has not been fine-tuned for the translation task.</p>
<p>Hendy et al. (2023) show that GPT-enabled translation achieves high quality when applied for the translation of high-resource languages, but still lacks in terms of translation quality for underrepresented languages. Building on this finding-if the model can translate, it may be able to differen-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tiate good from bad translations-we apply GPT for the task of translation quality assessment.</p>
<p>In the remainder of this paper, inspired by recent progress on generative, pre-trained large language models (LLMs), we explore how these models can be applied for automated assessment of translation quality. The primary query for this study centers around the question: Can LLMs be used for effective quality assessment of translations?</p>
<p>We propose GEMBA, which stands for GPT Estimation Metric Based Assessment. The metric evaluates each segment translation in isolation and then averages across all obtained scores for the final, system-level score.</p>
<p>We define and evaluate several prompt variants for zero-shot assessment of translation quality in two modes, either with a human reference translation, as a quality metric, or without one, as a quality estimation task.</p>
<p>We design the main prompts based on the DA+SQM template used for human assessment of translation quality as implemented in the Appraise framework (Federmann, 2018) for WMT22 (Kocmi et al., 2022), building on previous work conducted by Freitag et al. (2021a).</p>
<p>To the best of our knowledge, our research represents the pioneering effort in exploring the utilization of large language models (LLMs) for the purpose of quality assessment. Subsequent to the publishing of our findings, Lu et al. (2023) independently published a related report, corroborating the high performance of LLMs.</p>
<p>The main contributions of this paper are:</p>
<ul>
<li>We demonstrate state-of-the-art capabilities of GPT-based translation quality assessment on the latest WMT22 metrics evaluation data (on the system level);</li>
<li>
<p>We experiment with four prompt templates, showing that the least constrained template achieves the best performance;</p>
</li>
<li>
<p>We evaluate nine different models of GPT, showing that only GPT 3.5 and larger models are capable of translation quality assessment;</p>
</li>
<li>We show that GEMBA with GPT-4 model is only slightly behind on segment-level scores to the best-performing metrics.</li>
</ul>
<h2>2 The GEMBA Metric</h2>
<p>To assess translation quality via prompting an LLM, the following parameters are needed:</p>
<ul>
<li>prompt variant (from a pre-defined set)</li>
<li>source language name, e.g., "Chinese"</li>
<li>target language name, e.g., "English"</li>
<li>source segments $\operatorname{src}_{1 . . N}$</li>
<li>candidate translations $h y p_{1 . . N}$</li>
<li>optionally, reference translations $r e f_{1 . . N}$</li>
</ul>
<p>We generate a GPT request for every segment, querying as individual zero-shot problems, and then aggregate results. For this initial proof of concept, we leave improvements such as few-shot queries or document-level context to future work.</p>
<h3>2.1 Prompt variants</h3>
<p>We experiment with four distinct prompt types: modeling two scoring and two classification tasks. For the scoring tasks, first, one based on direct assessment (GEMBA-DA), second, another based on recent research efforts on scalar quality metrics (GEMBA-SQM). ${ }^{2}$ As scoring translation quality may be an unnatural task for an LLM, we also design two classification tasks. The first is based on one-to-five stars ranking (GEMBA-stars), which is a style often used when users are asked to review various services or products. The second prompt asks the LLM to label translation quality as one of five discrete quality classes (GEMBA-classes).</p>
<p>For each of these four prompt types, we experiment with two modes that differ with respect to the wording of the corresponding query templates which either have access to a human reference or not. As an example, we show the GEMBA-DA prompt in Figure 1. Based on token count, this is the least constrained prompt template that we experiment with. The complete set of prompt templates is available in Appendix A. For naming convention, we mark quality estimation metrics (without reference) with the suffix "[noref]".</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.2 Scoring process</h3>
<p>The expected scores are in $[0,100]$ for GEMBA-DA and GEMBA-SQM prompts, same as for human assessment (Graham et al., 2013); for GEMBAstars the output ranges from $[1,5]$ and GEMBAclasses assigns one of five class labels.</p>
<p>We average segment-level scores to obtain system-level scores. For the GEMBA-classes metric variant, we assign classes a numerical value $[0-4]$, based on the label, before averaging.</p>
<p>Depending on the GPT model we query, sometimes answers are returned outside these ranges, as text. When we observe such an invalid answer, we add randomness and sample more responses, selecting the first answer matching the output range as the final result.</p>
<h3>2.3 GPT models</h3>
<p>We experiment with seven GPT modelsâ€”ranging from GPT 2 up to the latest GPT-4 modelâ€”that are described in Table 1. ${ }^{3}$ We use the GPT-4 model as the default model for most experiments and compare the performance of other models in Section 4.3. Specifically, we use these models with brief description:</p>
<p>GPT 2 We use models provided by Radford et al. (2019), assessing if GPT 2 may be useful for quality assessmentâ€”we find that it is not;
Ada GPT 3. Max request size of 2,048 tokens and training data up to October 2019 (Brown et al., 2020a);
Babbage GPT 3. More capable than Ada (Brown et al., 2020a);
Curie GPT 3. More capable than Babbage (Brown et al., 2020a);
Davinci-002 GPT 3.5. Max request size of 4,000 tokens and training data up to June 2021. Uses FeedME training;
ChatGPT Improved GPT 3.5 model, fine-tuned using Reinforcement Learning from Human Feedback (RLHF);
Davinci-003 GPT 3.5.1. Uses PPO training;
GPT-3.5-turbo Davinci-003 model optimized for speed;
GPT-4 there is only limited information about GPT-4, see OpenAI (2023).</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Score the following translation from {source_lang} to {target_lang} with respect</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>to the human reference on a continuous scale from 0 to 100, where score of zero means</td>
<td></td>
</tr>
<tr>
<td>"no meaning preserved" and score of one hundred means "perfect meaning and grammar".</td>
<td></td>
</tr>
<tr>
<td>{source_lang} source: "{source_seg}"</td>
<td></td>
</tr>
<tr>
<td reference_seg="reference_seg">{target_lang} human reference:</td>
<td></td>
</tr>
<tr>
<td>{target_lang} translation: "{target_seg}"</td>
<td></td>
</tr>
<tr>
<td>Score:</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 1: The best-performing prompt based on Direct Assessment expecting a score between 0-100. Template portions in bold face are used only when a human reference translation is available.</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th>Abbrev.</th>
<th>Model used</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-2</td>
<td>â€”</td>
<td><em>Radford et al. (2019)</em></td>
</tr>
<tr>
<td>Ada</td>
<td>â€”</td>
<td>text-ada-001</td>
</tr>
<tr>
<td>Babbage</td>
<td>Bab</td>
<td>text-babbage-001</td>
</tr>
<tr>
<td>Curie</td>
<td>Curie</td>
<td>text-curie-001</td>
</tr>
<tr>
<td>Davinci-002</td>
<td>Dav2</td>
<td>text-davinci-002</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>Chat</td>
<td>text-chat-davinci-002</td>
</tr>
<tr>
<td>Davinci-003</td>
<td>Dav3</td>
<td>text-davinci-003</td>
</tr>
<tr>
<td>GPT-3.5-turbo</td>
<td>Turbo</td>
<td>gpt-3.5-turbo</td>
</tr>
<tr>
<td>GPT-4</td>
<td>GPT4</td>
<td>gpt-4</td>
</tr>
</tbody>
</table>
<p>Table 1: Details of all models used in this work. Models are sorted from oldest to newest which also reflects their respective power. GPT 2 and Ada do not work.</p>
<p>GPT 3 models are based on <em>Ouyang et al. (2022)</em>. The models are sorted based on their estimated power or date of release. We acknowledge that OpenAI has not released detailed information about the architecture and training data behind given models. Most importantly, OpenAI claims that models have been trained with data up until September 2021. It is important as we use testsets prepared and released by December 2022.</p>
<h2>3 Experiments</h2>
<p>To measure the performance of the proposed GEMBA metric, we follow the methodology and use test data provided by the WMT22 Metrics shared task <em>Freitag et al. (2022b)</em> which hosts an annual evaluation of automatic metrics, benchmarking them against human gold labels. Effectively, we compare GEMBA against the best-performing automatic metrics: COMET <em>Rei et al. (2020, 2022)</em>, BLEURT <em>Sellam et al. (2020)</em>, or the non-public winner MetricX XXL.</p>
<h3>3.1 Test set</h3>
<p>We use the MQM 2022 test set which contains human judgments for the following three translation directions: English into German, English into Russian, and Chinese into English. The test set contains a total of 54 machine translation system outputs or human translations. It contains a total of 106k segments. Translation systems are mainly from participants of the WMT22 General MT shared task <em>Kocmi et al. (2022)</em>.</p>
<p>The source segments and human reference translations for each language pair contain around 2,000 sentences from four different texts domains: news, social, conversational, and e-commerce. The gold standard for scoring translation quality is based on human MQM ratings, annotated by professionals who mark individual errors in each translation, as described in <em>Freitag et al. (2021a)</em>.</p>
<h3>3.2 Evaluation methods</h3>
<p>To determine how well automatic metrics correlate with humans, we measure system-level, pairwise accuracy <em>accuracy</em>, <em>Kocmi et al. (2021)</em>. For segment-level evaluation, we use Kendallâ€™s Tau ( $\tau$, [Freitag et al., 2022a]).</p>
<p>Here, accuracy is defined as the number of system pairs ranked correctly by the metric with respect to the human ranking divided by the total number of system pair comparisons.</p>
<p>Formally:</p>
<p>$\text{Accuracy}=\frac{|\text{sign}(\text{metric}\Delta)==\text{sign}(\text{human}\Delta)|}{|\text{all system pairs}|}$</p>
<p>The variant of Kendallâ€™s Tau used for metric evaluation changed over the years. Initially, <em>Callison-Burch et al. (2011)</em> proposed to use Kendallâ€™s Tau-a ignoring human rankings that tied, while penalising ties in automatic metrics.</p>
<p>$\tau=\frac{|\text{Concordant}|-|\text{Discordant}|}{|\text{Concordant}|+|\text{Discordant}|}$</p>
<p>where Concordant is the set of all human segment comparisons for which a given metric suggests the same order of systems and Discordant is the set of all human comparisons for which a given metric disagrees.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$s_{1}&lt;s_{2}$</td>
<td style="text-align: center;">$s_{1}=s_{2}$</td>
<td style="text-align: center;">$s_{1}&gt;s_{2}$</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\text { m }}{\text { n }}$</td>
<td style="text-align: center;">$s_{1}&lt;s_{2}$</td>
<td style="text-align: center;">Conc</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Disc</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\text { m }}{\text { n }}$</td>
<td style="text-align: center;">$s_{1}=s_{2}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$s_{1}&gt;s_{2}$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Conc</td>
</tr>
</tbody>
</table>
<p>This definition was later updated by MachÃ¡Äek and Bojar (2014), who handle ties as a separate group in contrast to Concordant and Discordant. Metrics shared tasks Mathur et al. (2020) and Freitag et al. (2021b) changed this back to the 2011 version. Last year, Freitag et al. (2022a) changed it to Kendall's Tau-b, which makes adjustments for ties, we use the latest definition in our experiments. Overall, ties in automatic metrics are rare for non-identical translations but are an issue when a method outputs only a discrete set of scores (as in our case). Additionally, Kendall's Tau is susceptible to noise in gold pairwise rankings (Freitag et al., 2022a).</p>
<p>We reproduced all scores reported in the WMT22 Metrics shared task findings paper with the official WMT22 script. ${ }^{4}$ Reported scores match Table 11 of the WMT22 metrics findings paper (Freitag et al., 2022b).</p>
<h2>4 Results</h2>
<p>We investigate GEMBA's performance for two modes: with a reference translation and without reference translation (in a quality estimation setting). Table 2 reports pairwise accuracy on the system level, comparing GEMBA-DA against the best-performing metrics from the WMT22 Metrics shared task (Freitag et al., 2022b). We use GPT-4 as the main model and GEMBA-DA as the main style for some experiments.</p>
<h3>4.1 Reference-based</h3>
<p>The results in Table 2 show that our referencebased GEMBA-GPT4-DA metric sets a new state of the art. It outperforms all of the other referencebased metrics from the WMT22 Metrics shared task. The observed level of metric performance is unexpected, especially considering that human labels used as a gold standard in itself are noisy and therefore an accuracy of $100 \%$ is impossible to obtain for an automatic metric.</p>
<h3>4.2 Quality estimation</h3>
<p>Table 2 shows that our reference-less metric GEMBA-GPT4-DA[noref] achieves the highest</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Metric | Accuracy |
| :-- | :-- |
| GEMBA-GPT4-DA | $89.8 \%$ |
| GEMBA-GPT4-DA[noref] | $87.6 \%$ |
| MetricX XXL | $85.0 \%$ |
| BLEURT-20 | $84.7 \%$ |
| COMET-22 | $83.9 \%$ |
| COMET-20 | $83.6 \%$ |
| UniTE | $82.8 \%$ |
| MS-COMET-22 | $82.8 \%$ |
| MATESE | $81.0 \%$ |
| YiSi-1 | $79.2 \%$ |
| COMETKiwi[noref] | $78.8 \%$ |
| COMET-QE[noref] | $78.1 \%$ |
| BERTScore | $77.4 \%$ |
| UniTE-src[noref] | $75.9 \%$ |
| MS-COMET-QE-22[noref] | $75.5 \%$ |
| MATESE-QE[noref] | $74.8 \%$ |
| f200spBLEU | $74.1 \%$ |
| chrF | $73.4 \%$ |
| BLEU | $70.8 \%$ |</p>
<p>Table 2: Results for the system-level pairwise accuracy compared to the current automatic metric. Metrics marked as "[noref]" do not use a reference translation.
performance for the quality estimation mode, and strongly outperforms all of the other referenceless metrics. Moreover, it also outperforms all of the other reference-based metrics, performing only slightly worse than GEMBA-GPT4-DA. Again, the observed level of assessment quality is unexpectedly high, highlighting the potential of using LLMs for translation quality assessment tasks.</p>
<h3>4.3 Comparison of GPT models</h3>
<p>We compare the performance of various GPT versions as an automatic metric. Table 3 shows results for all models we have experimented with and all prompt variants tested.</p>
<p>We do not show results for GPT-2 or Ada models. Neither of those have produced replies in the specific scoring range and neither seemed to be producing any meaningful replies. We list a couple of their answers in Appendix C. Based on our experiments, we conclude that they are not powerful enough to understand the zero-shot prompts.</p>
<p>By contrast, Babbage and Curie models appear to understand what type of answer they should produce, but the quality of their scores seems to be close to random guessing. Thus, both Babbage and Curie are useless for translation quality assessment.</p>
<p>The main performance jump occurs for GPT 3.5 and larger models, i.e., Davinci-002, ChatGPT, Davinci-003, Turbo, and GPT-4. Each of them achieves highly competitive results for all of the prompt variants we have tested. Interestingly, Chat-</p>
<table>
<thead>
<tr>
<th></th>
<th>Bab</th>
<th>Curie</th>
<th>Dav2</th>
<th>Chat</th>
<th>Dav3</th>
<th>Turbo</th>
<th>GPT4</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DA</td>
<td>$39.1 \%$</td>
<td>$54.4 \%$</td>
<td>$\mathbf{8 5 . 8 \%}$</td>
<td>$81.0 \%$</td>
<td>$\mathbf{8 8 . 0 \%}$</td>
<td>$\mathbf{8 6 . 5 \%}$</td>
<td>$\mathbf{8 9 . 8 \%}$</td>
</tr>
<tr>
<td>DA[noref]</td>
<td>$55.8 \%$</td>
<td>$51.8 \%$</td>
<td>$83.9 \%$</td>
<td>$82.1 \%$</td>
<td>$\mathbf{8 6 . 1 \%}$</td>
<td>$\mathbf{8 6 . 9 \%}$</td>
<td>$\mathbf{8 7 . 6 \%}$</td>
</tr>
<tr>
<td>SQM</td>
<td>$51.8 \%$</td>
<td>$40.5 \%$</td>
<td>$\mathbf{8 5 . 8 \%}$</td>
<td>$\mathbf{8 5 . 0 \%}$</td>
<td>$\mathbf{8 5 . 4 \%}$</td>
<td>$\mathbf{8 7 . 2 \%}$</td>
<td>$\mathbf{8 8 . 7 \%}$</td>
</tr>
<tr>
<td>SQM[noref]</td>
<td>$51.1 \%$</td>
<td>$41.6 \%$</td>
<td>$82.8 \%$</td>
<td>$81.0 \%$</td>
<td>$82.5 \%$</td>
<td>$\mathbf{8 7 . 6 \%}$</td>
<td>$\mathbf{8 9 . 1 \%}$</td>
</tr>
<tr>
<td>Stars</td>
<td>$48.2 \%$</td>
<td>$37.2 \%$</td>
<td>$\mathbf{8 8 . 3 \%}$</td>
<td>$\mathbf{8 5 . 0 \%}$</td>
<td>$\mathbf{8 5 . 8 \%}$</td>
<td>$\mathbf{8 9 . 4 \%}$</td>
<td>$\mathbf{9 1 . 2 \%}$</td>
</tr>
<tr>
<td>Stars[noref]</td>
<td>$58.4 \%$</td>
<td>$54.7 \%$</td>
<td>$79.6 \%$</td>
<td>$83.6 \%$</td>
<td>$83.2 \%$</td>
<td>$84.3 \%$</td>
<td>$\mathbf{8 9 . 1 \%}$</td>
</tr>
<tr>
<td>Classes</td>
<td>$47.4 \%$</td>
<td>$43.4 \%$</td>
<td>$79.6 \%$</td>
<td>$\mathbf{8 7 . 2 \%}$</td>
<td>$\mathbf{8 5 . 4 \%}$</td>
<td>$82.5 \%$</td>
<td>$\mathbf{8 9 . 1 \%}$</td>
</tr>
<tr>
<td>Classes[noref]</td>
<td>$35.0 \%$</td>
<td>$61.7 \%$</td>
<td>$78.1 \%$</td>
<td>$83.6 \%$</td>
<td>$78.8 \%$</td>
<td>$62.0 \%$</td>
<td>$\mathbf{9 1 . 2 \%}$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy of the system-level pairwise accuracy for quality estimation methods for most combinations of prompts and different GPT models. The evaluation is based on three language pairs and MQM human labels. All results higher than the WMT22 winner of Metrics shared task MetricX XXL are bolded.</p>
<p>GPT in DA style appears to have the lowest quality among those models. In addition, ChatGPT and Turbo frequently reply with a score followed by an explanation of why it has assigned that score. One possible reason may be in the form of the prompt, which wasn't modified to instruct ChatGPT not to generate an explanation.</p>
<p>Unsurprisingly, the best performance is obtained by the most powerful LLM, GPT-4. Moreover, we can see that over time, each generation of models is slightly better. This confirms the findings of Hendy et al. (2023) who demonstrated superior translation capabilities with Davinci-003 over all other previous GPT variants.</p>
<h3>4.4 Segment-level performance</h3>
<p>All previous results are reported on the system level. We also investigate how well the GEMBA metric performs on the segment level, with respect to the human gold annotations. We present Kendall's Tau results for each language pair separately in Table 4 for GPT-4 and Davinci-003 (results for all metrics are in Appendix B).</p>
<p>GPT-4 models are slightly behind the topperforming metrics but continue to have a high correlation with human judgment. On the other hand, quality estimation GEMBA-Dav3-DA [noref] has significantly lower segment-level performance in contrast to other top-performing metrics.</p>
<p>The lower performance of a segment-level correlation could be attributed to Kendall's Tau, which penalizes ties. Our metric in contrast to other automatic metrics returns a discrete value between $0-100$. There is a high probability that two translations will obtain an equal score.</p>
<p>In order to investigate this further, we collect all answers across all systems and all three language pairs and then calculate the frequency of each distinct answer value.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Acc</th>
<th>en-de</th>
<th>en-ru</th>
<th>zh-en</th>
</tr>
</thead>
<tbody>
<tr>
<td>GEMBA-GPT4-DA</td>
<td>$89.8 \%$</td>
<td>0.36</td>
<td>0.36</td>
<td>0.38</td>
</tr>
<tr>
<td>GEMBA-Dav3-DA</td>
<td>$88.0 \%$</td>
<td>0.31</td>
<td>0.33</td>
<td>0.37</td>
</tr>
<tr>
<td>GEMBA-GPT4-DA[noref]</td>
<td>$87.6 \%$</td>
<td>0.31</td>
<td>0.40</td>
<td>0.41</td>
</tr>
<tr>
<td>GEMBA-Dav3-DA[noref]</td>
<td>$86.1 \%$</td>
<td>0.18</td>
<td>0.26</td>
<td>0.29</td>
</tr>
<tr>
<td>MetricX XXL</td>
<td>$85.0 \%$</td>
<td>0.36</td>
<td>$\mathbf{0 . 4 2}$</td>
<td>$\mathbf{0 . 4 3}$</td>
</tr>
<tr>
<td>BLEURT-20</td>
<td>$84.7 \%$</td>
<td>0.34</td>
<td>0.36</td>
<td>0.36</td>
</tr>
<tr>
<td>COMET-22</td>
<td>$83.9 \%$</td>
<td>$\mathbf{0 . 3 7}$</td>
<td>0.40</td>
<td>$\mathbf{0 . 4 3}$</td>
</tr>
<tr>
<td>UniTE</td>
<td>$82.8 \%$</td>
<td>$\mathbf{0 . 3 7}$</td>
<td>0.38</td>
<td>0.36</td>
</tr>
<tr>
<td>COMETKiwi[noref]</td>
<td>$78.8 \%$</td>
<td>0.29</td>
<td>0.36</td>
<td>0.36</td>
</tr>
<tr>
<td>COMET-QE[noref]</td>
<td>$78.1 \%$</td>
<td>0.28</td>
<td>0.34</td>
<td>0.36</td>
</tr>
<tr>
<td>eleF</td>
<td>$73.4 \%$</td>
<td>0.21</td>
<td>0.17</td>
<td>0.15</td>
</tr>
<tr>
<td>BLEU</td>
<td>$70.8 \%$</td>
<td>0.17</td>
<td>0.14</td>
<td>0.14</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Kendall's Tau $(\tau)$ segment-level evaluation. Full results are in Appendix B.</p>
<p>We can notice several interesting observations in Table 5. The DA reference-based prompt generates mostly multiples of five. Over three-quarters of all scores are either score 80,95 , or 100 . This could reflect the actual quality of the system translations as the underlying systems are provably high-quality. This is also a finding of Freitag et al. (2022b) that many metrics fall into the same significance cluster.</p>
<p>When we investigate the "DA[noref]", we notice that $60.5 \%$ of all scores are of value "95". Despite this fact, the metric still manages to differentiate the systems from each other and outperform all other quality estimation metrics on the system level. This is contributed to the fact that betterperforming systems obtain more segments with a score 95 than worse-performing systems, therefore getting a lower average score. We should note, that there are no system-level ties.</p>
<p>We conjecture that frequent segment-level ties and the discrete scale thus may contribute to the lower Kendall's Tau segment-level performance.</p>
<h3>4.5 Failure rate</h3>
<p>As we described earlier, LLMs may answer with an invalid answer, for example with a textual answer instead of a score, mostly explaining its decision. When such a situation happens, we iteratively in-</p>
<table>
<thead>
<tr>
<th>Answers</th>
<th>DA</th>
<th>DA[noref]</th>
<th>SQM</th>
<th>SQM[noref]</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.1%</td>
<td>0.1%</td>
<td>0.1%</td>
<td>0.1%</td>
</tr>
<tr>
<td>5</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr>
<td>10</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.1%</td>
</tr>
<tr>
<td>15</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr>
<td>20</td>
<td>0.2%</td>
<td>0.3%</td>
<td>0.2%</td>
<td>0.3%</td>
</tr>
<tr>
<td>25</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>â€”</td>
</tr>
<tr>
<td>30</td>
<td>0.1%</td>
<td>0.2%</td>
<td>0.1%</td>
<td>0.1%</td>
</tr>
<tr>
<td>35</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>â€”</td>
</tr>
<tr>
<td>40</td>
<td>0.5%</td>
<td>0.6%</td>
<td>0.5%</td>
<td>0.6%</td>
</tr>
<tr>
<td>45</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr>
<td>50</td>
<td>0.0%</td>
<td>0.1%</td>
<td>0.1%</td>
<td>0.0%</td>
</tr>
<tr>
<td>55</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>â€”</td>
</tr>
<tr>
<td>60</td>
<td>2.1%</td>
<td>2.3%</td>
<td>2.0%</td>
<td>2.1%</td>
</tr>
<tr>
<td>65</td>
<td>â€”</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr>
<td>70</td>
<td>1.3%</td>
<td>0.4%</td>
<td>1.9%</td>
<td>0.6%</td>
</tr>
<tr>
<td>75</td>
<td>0.5%</td>
<td>1.0%</td>
<td>0.7%</td>
<td>0.7%</td>
</tr>
<tr>
<td>80</td>
<td>6.3%</td>
<td>4.5%</td>
<td>7.0%</td>
<td>5.7%</td>
</tr>
<tr>
<td>85</td>
<td>4.4%</td>
<td>2.7%</td>
<td>6.0%</td>
<td>2.9%</td>
</tr>
<tr>
<td>87</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>â€”</td>
</tr>
<tr>
<td>88</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>â€”</td>
</tr>
<tr>
<td>90</td>
<td>21.3%</td>
<td>13.0%</td>
<td>27.6%</td>
<td>14.5%</td>
</tr>
<tr>
<td>92</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>â€”</td>
</tr>
<tr>
<td>93</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>â€”</td>
</tr>
<tr>
<td>94</td>
<td>â€”</td>
<td>â€”</td>
<td>0.0%</td>
<td>â€”</td>
</tr>
<tr>
<td>95</td>
<td>53.3%</td>
<td>60.6%</td>
<td>44.6%</td>
<td>49.4%</td>
</tr>
<tr>
<td>98</td>
<td>0.8%</td>
<td>0.0%</td>
<td>0.4%</td>
<td>0.0%</td>
</tr>
<tr>
<td>99</td>
<td>0.4%</td>
<td>â€”</td>
<td>0.2%</td>
<td>â€”</td>
</tr>
<tr>
<td>100</td>
<td>8.6%</td>
<td>14.1%</td>
<td>8.5%</td>
<td>22.8%</td>
</tr>
</tbody>
</table>
<p>Table 5: Distribution of all distinct segment-level score values for MQM 2022 for model GPT-4.</p>
<p>crease the temperatureâ€”<em>adding randomness to the model</em>â€”and take the first answer matching the expected score output range.</p>
<p>This adds non-determinism to our evaluation, therefore we investigate how frequently this phenomenon happens. Table 6 shows the number of invalid answers. For almost all combinations of models and prompts, except of SQM-style, LLMs understand the prompt and provide answers in a valid range with less than 1% of the answers being invalid. This has a minimal effect on the final system-level score and therefore, we conclude that the metric is mostly deterministic.</p>
<p>Additionally, we confirm that a temperature equal to zero always returns the same answer, which we evaluated by re-running GEMBA-Dav2-DA[noref].</p>
<p>Processing answers is straightforward as it is usually a stand-alone number. In some occasions, LLMs give a numerical score and continue with a textual explanation, for such cases, we parse only the first number. A more complex approach needs to be taken for <em>GEMBA-stars</em> prompts where the model provides different answers which we parse separately. Here are some examples of two-star answers: "2", "two", "**", "â˜…â˜…", "two stars", or "2 stars". For non-English target languages the answer may be produced in the target language, e.g., "ä¸€æ˜Ÿ", or "äº”". We have not observed attempts to translate output for other prompts.</p>
<h2>5 Conclusion</h2>
<p>We have presented our work on GEMBA, a GPT-based estimation metric-based assessment method. Comparing our metrics to other automated metrics from the WMT22 Metrics shared task we report state-of-the-art performance on the MQM 2022 test set across three language pairs: English to German, English to Russian, and Chinese to English.</p>
<p>We intend to continue research on the application of GPT models for quality assessment. Further research will focus on the switch to few-shot (as opposed to our current zero-shot methodology) as well as model fine-tuning. Both of which promise to increase GEMBA accuracy. Furthermore, modifying prompts to support MQM error-based evaluation or post-editing efforts may lead to further improvements.</p>
<p>GPT-enhanced evaluation metrics may allow us to make progress with respect to document-level evaluation (due to their ability to use much larger context windows). This could be beneficial as there is little research into document-level metrics <em>Vernikos et al. (2022)</em>.</p>
<h2>Limitations</h2>
<p>While preliminary results indicate that the GEMBA metric performs very well when compared to other automated metrics evaluated as part of the WMT22 Metrics shared task, it is important to note that these results are based on human labels for <em>only three language pairs</em>. We expect that the metrics performance may suffer for other language pairs, mainly under-resourced languages similar to Hendy et al. (2023) who show low translation quality for such languages. In addition, GEMBA's state-of-the-art performance only holds for the system level, while segment-level scores still have room for improvement. Reported results are indicative of the potential performance LLMs could achieve for the translation quality assessment task in the long run. However, more analysis is needed before using it as the main tool for deciding translation quality.</p>
<p>An additional limitation to consider in this study is the inability to definitively ascertain that the evaluation data have not been included in OpenAI's training dataset. Nevertheless, the available evidence strongly indicates that this is unlikely. OpenAI claims that their data compilation only extends</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Bab</th>
<th style="text-align: right;">Curie</th>
<th style="text-align: right;">Dav2</th>
<th style="text-align: right;">Chat</th>
<th style="text-align: right;">Dav3</th>
<th style="text-align: right;">GPT4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DA</td>
<td style="text-align: right;">750</td>
<td style="text-align: right;">8,048</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">565</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">DA[noref]</td>
<td style="text-align: right;">146</td>
<td style="text-align: right;">862</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">935</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">SQM</td>
<td style="text-align: right;">89,599</td>
<td style="text-align: right;">129</td>
<td style="text-align: right;">4,827</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">1,279</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">SQM[noref]</td>
<td style="text-align: right;">15,577</td>
<td style="text-align: right;">95,131</td>
<td style="text-align: right;">1,763</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Stars</td>
<td style="text-align: right;">18,074</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">135</td>
<td style="text-align: right;">1,064</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Stars[noref]</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">86,593</td>
<td style="text-align: right;">135</td>
<td style="text-align: right;">1,924</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Classes</td>
<td style="text-align: right;">74</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Classes[noref]</td>
<td style="text-align: right;">115</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">-</td>
</tr>
</tbody>
</table>
<p>Table 6: Number of invalid answers (full set size 106,758) that needed to be re-prompted with added randomness. The evaluation of ChatGPT and parts of GPT-4 were excluded due to their late integration and changes in our codebase.
up to September 2021, while the test set employed in this research was generated during the second half of 2022 and made publicly available in December 2022. Our initial positive results using the Davinci-002 model were obtained in early February, which presents a narrow timeframe for OpenAI to incorporate and process the evaluation data. Furthermore, the test set is not readily accessible in plaintext format, necessitating pre-processing prior to utilization in training.</p>
<h2>Acknowledgments</h2>
<p>This work would not have been possible without the help and support from our friend and colleague, Olivier Nano, who provided access to GPT models via Microsoft Azure - Merci beaucoup, Olivier! The authors are also grateful to Matt Post, Vikas Raunak, Shabnam Sadegharmaki, and the Microsoft Translator research team for fruitful discussions and helpful feedback.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020a. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners.</p>
<p>Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22-64, Edinburgh, Scotland. Association for Computational Linguistics.</p>
<p>Christian Federmann. 2018. Appraise evaluation framework for machine translation. In Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 86-88, Santa Fe, New Mexico. Association for Computational Linguistics.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and AndrÃ© F. T. Martins. 2022a. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46-68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and Andre F. T. Martins. 2022b. Results of wmt22 metrics shared task: Stop using bleu neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation, pages 46-68, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and OndÅ™ej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.</p>
<p>Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement scales</p>
<p>in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33-41, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210.</p>
<p>Tom Kocmi, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novak, Martin Popel, Maja Popovic, and Mariya Shmatova. 2022. Findings of the 2022 conference on machine translation (wmt22). In Proceedings of the Seventh Conference on Machine Translation, pages 1-45, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478-494, Online. Association for Computational Linguistics.</p>
<p>Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt. arXiv preprint.</p>
<p>MatouÅ¡ MachÃ¡Äek and OndÅ™ej Bojar. 2014. Results of the WMT14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293-301, Baltimore, Maryland, USA. Association for Computational Linguistics.</p>
<p>Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and OndÅ™ej Bojar. 2020. Results of the WMT20 metrics shared task. In Proceedings of the Fifth Conference on Machine Translation, pages 688-725, Online. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Ricardo Rei, Jose G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andre F. T. Martins. 2022. Comet-22: Unbabel-ist 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation, pages 578585, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Giorgos Vernikos, Brian Thompson, Prashant Mathur, and Marcello Federico. 2022. Embarrassingly easy document-level mt metrics: How to convert any pretrained metric into a document-level metric. In Proceedings of the Seventh Conference on Machine Translation, pages 118-128, Abu Dhabi. Association for Computational Linguistics.</p>
<p>David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022. Prompting palm for translation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102.</p>
<h1>A Appendix: Prompt Templates</h1>
<p>Below we provide our prompt templates which we use for the experiments described in this paper. Template portions in bold face are used only when a human reference translation is available.</p>
<h2>A. 1 DA: Direct Assessment</h2>
<p>Output scores range from $0-100$.</p>
<div class="codehilite"><pre><span></span><code>Score the following translation from {source_lang} to {target_lang} with respect to
the human reference on a continuous scale from 0 to 100, where a score of zero means
&quot;no meaning preserved&quot; and score of one hundred means &quot;perfect meaning and grammar&quot;.
{source_lang} source: &quot;{source_seg}&quot;
(target lang) human reference: {reference_seg}
(target lang) translation: &quot;{target_seg}&quot;
Score:
</code></pre></div>

<h2>A. 2 SQM: Scalar Quality Metrics</h2>
<p>Output scores range from $0-100$.</p>
<div class="codehilite"><pre><span></span><code>Score the following translation from {source_lang} to
(target lang) with respect to the human reference on a continuous
scale from 0 to 100 that starts with &quot;No meaning preserved&quot;, goes
through &quot;Some meaning preserved&quot;, then &quot;Most meaning preserved and
few grammar mistakes&quot;, up to &quot;Perfect meaning and grammar&quot;.
{source_lang} source: &quot;{source_seg}&quot;
(target lang) human reference: &quot;{reference_seg}&quot;
(target lang) translation: &quot;{target_seg}&quot;
Score (0-100):
</code></pre></div>

<h2>A. 3 Stars: One to Five Stars Ranking</h2>
<p>Output scores range from $1-5$. Special care is taken for answers containing non-numerical answers, such as "Three stars", "****", or "1 star".</p>
<div class="codehilite"><pre><span></span><code>Score the following translation from {source_lang} to {target_lang}
with respect to the human reference with one to five stars.
Where one star means &quot;Nonsense/No meaning preserved&quot;,
two stars mean &quot;Some meaning preserved, but not understandable&quot;,
three stars mean &quot;Some meaning preserved and understandable&quot;,
four stars mean &quot;Most meaning preserved with possibly few grammar mistakes&quot;,
and five stars mean &quot;Perfect meaning and grammar&quot;.
{source_lang} source: &quot;{source_seg}&quot;
(target lang) human reference: &quot;{reference_seg}&quot;
(target lang) translation: &quot;{target_seg}&quot;
Stars:
</code></pre></div>

<h2>A. 4 Classes: Quality Class Labels</h2>
<p>Output label one of "No meaning preserved", "Some meaning preserved, but not understandable", "Some meaning preserved and understandable", "Most meaning preserved, minor issues", "Perfect translation".</p>
<div class="codehilite"><pre><span></span><code>Classify the quality of translation from {source_lang} to {target_lang}
with respect to the human reference into one of following classes: &quot;No meaning
preserved&quot;, &quot;Some meaning preserved, but not understandable&quot;, &quot;Some meaning
preserved and understandable&quot;, &quot;Most meaning preserved, minor issues&quot;, &quot;Perfect
translation&quot;.
{source_lang} source: &quot;{source_seg}&quot;
(target lang) human reference: &quot;{reference_seg}&quot;
(target lang) translation: &quot;{target_seg}&quot;
Class:
</code></pre></div>

<p>Templates available from: https://github.com/MicrosoftTranslator/GEMBA/gemba/prompt.py</p>
<h1>B Appendix: Full Results</h1>
<p>Below table lists all GEMBA results we have obtained for this work. Any missing segment-level scores are due to a subset of segments for which we could not obtain a score even after adding randomness.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">en-de</th>
<th style="text-align: left;">en-ru</th>
<th style="text-align: left;">zh-en</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GEMBA-GPT4-Classes[noref]</td>
<td style="text-align: left;">$91.2 \%$</td>
<td style="text-align: left;">0.304</td>
<td style="text-align: left;">0.390</td>
<td style="text-align: left;">0.313</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-GPT4-Stars</td>
<td style="text-align: left;">$91.2 \%$</td>
<td style="text-align: left;">0.326</td>
<td style="text-align: left;">0.351</td>
<td style="text-align: left;">0.382</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-GPT4-DA</td>
<td style="text-align: left;">$89.8 \%$</td>
<td style="text-align: left;">0.357</td>
<td style="text-align: left;">0.358</td>
<td style="text-align: left;">0.382</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Turbo-Stars</td>
<td style="text-align: left;">$89.4 \%$</td>
<td style="text-align: left;">0.259</td>
<td style="text-align: left;">0.223</td>
<td style="text-align: left;">0.265</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-GPT4-Classes</td>
<td style="text-align: left;">$89.1 \%$</td>
<td style="text-align: left;">0.222</td>
<td style="text-align: left;">0.267</td>
<td style="text-align: left;">0.273</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-GPT4-Stars[noref]</td>
<td style="text-align: left;">$89.1 \%$</td>
<td style="text-align: left;">0.308</td>
<td style="text-align: left;">0.366</td>
<td style="text-align: left;">0.404</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-GPT4-SQM[noref]</td>
<td style="text-align: left;">$89.1 \%$</td>
<td style="text-align: left;">0.359</td>
<td style="text-align: left;">$\mathbf{0 . 4 3 2}$</td>
<td style="text-align: left;">0.416</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-GPT4-SQM</td>
<td style="text-align: left;">$88.7 \%$</td>
<td style="text-align: left;">$\mathbf{0 . 3 8 0}$</td>
<td style="text-align: left;">0.388</td>
<td style="text-align: left;">0.398</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav2-Stars</td>
<td style="text-align: left;">$88.3 \%$</td>
<td style="text-align: left;">0.225</td>
<td style="text-align: left;">0.282</td>
<td style="text-align: left;">0.183</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav3-DA</td>
<td style="text-align: left;">$88.0 \%$</td>
<td style="text-align: left;">0.306</td>
<td style="text-align: left;">0.332</td>
<td style="text-align: left;">0.371</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-GPT4-DA[noref]</td>
<td style="text-align: left;">$87.6 \%$</td>
<td style="text-align: left;">0.311</td>
<td style="text-align: left;">0.405</td>
<td style="text-align: left;">0.407</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Turbo-SQM[noref]</td>
<td style="text-align: left;">$87.6 \%$</td>
<td style="text-align: left;">0.259</td>
<td style="text-align: left;">0.309</td>
<td style="text-align: left;">0.291</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Chat-Classes</td>
<td style="text-align: left;">$87.2 \%$</td>
<td style="text-align: left;">0.220</td>
<td style="text-align: left;">0.270</td>
<td style="text-align: left;">0.259</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Turbo-SQM</td>
<td style="text-align: left;">$87.2 \%$</td>
<td style="text-align: left;">0.298</td>
<td style="text-align: left;">0.277</td>
<td style="text-align: left;">0.313</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Turbo-DA[noref]</td>
<td style="text-align: left;">$86.9 \%$</td>
<td style="text-align: left;">0.255</td>
<td style="text-align: left;">0.294</td>
<td style="text-align: left;">0.264</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Turbo-DA</td>
<td style="text-align: left;">$86.5 \%$</td>
<td style="text-align: left;">0.250</td>
<td style="text-align: left;">0.234</td>
<td style="text-align: left;">0.255</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav3-DA[noref]</td>
<td style="text-align: left;">$86.1 \%$</td>
<td style="text-align: left;">0.180</td>
<td style="text-align: left;">0.258</td>
<td style="text-align: left;">0.289</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav3-Stars</td>
<td style="text-align: left;">$85.8 \%$</td>
<td style="text-align: left;">0.294</td>
<td style="text-align: left;">0.294</td>
<td style="text-align: left;">0.297</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav2-SQM</td>
<td style="text-align: left;">$85.8 \%$</td>
<td style="text-align: left;">0.279</td>
<td style="text-align: left;">0.325</td>
<td style="text-align: left;">0.344</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav2-DA</td>
<td style="text-align: left;">$85.8 \%$</td>
<td style="text-align: left;">0.231</td>
<td style="text-align: left;">0.302</td>
<td style="text-align: left;">0.303</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav3-Classes</td>
<td style="text-align: left;">$85.4 \%$</td>
<td style="text-align: left;">0.235</td>
<td style="text-align: left;">0.289</td>
<td style="text-align: left;">0.251</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav3-SQM</td>
<td style="text-align: left;">$85.4 \%$</td>
<td style="text-align: left;">0.283</td>
<td style="text-align: left;">0.308</td>
<td style="text-align: left;">0.346</td>
</tr>
<tr>
<td style="text-align: left;">MetricX XXL</td>
<td style="text-align: left;">$85.0 \%$</td>
<td style="text-align: left;">0.360</td>
<td style="text-align: left;">0.420</td>
<td style="text-align: left;">0.427</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Chat-Stars</td>
<td style="text-align: left;">$85.0 \%$</td>
<td style="text-align: left;">0.292</td>
<td style="text-align: left;">0.248</td>
<td style="text-align: left;">0.343</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Chat-SQM</td>
<td style="text-align: left;">$85.0 \%$</td>
<td style="text-align: left;">0.250</td>
<td style="text-align: left;">0.293</td>
<td style="text-align: left;">0.310</td>
</tr>
<tr>
<td style="text-align: left;">BLEURT-20</td>
<td style="text-align: left;">$84.7 \%$</td>
<td style="text-align: left;">0.344</td>
<td style="text-align: left;">0.359</td>
<td style="text-align: left;">0.361</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Turbo-Stars[noref]</td>
<td style="text-align: left;">$84.3 \%$</td>
<td style="text-align: left;">0.255</td>
<td style="text-align: left;">0.279</td>
<td style="text-align: left;">0.261</td>
</tr>
<tr>
<td style="text-align: left;">COMET-22</td>
<td style="text-align: left;">$83.9 \%$</td>
<td style="text-align: left;">0.368</td>
<td style="text-align: left;">0.400</td>
<td style="text-align: left;">$\mathbf{0 . 4 2 8}$</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav2-DA[noref]</td>
<td style="text-align: left;">$83.9 \%$</td>
<td style="text-align: left;">0.209</td>
<td style="text-align: left;">0.285</td>
<td style="text-align: left;">0.280</td>
</tr>
<tr>
<td style="text-align: left;">COMET-20</td>
<td style="text-align: left;">$83.6 \%$</td>
<td style="text-align: left;">0.319</td>
<td style="text-align: left;">0.330</td>
<td style="text-align: left;">0.332</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Chat-Classes[noref]</td>
<td style="text-align: left;">$83.6 \%$</td>
<td style="text-align: left;">0.193</td>
<td style="text-align: left;">0.306</td>
<td style="text-align: left;">0.256</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Chat-Stars[noref]</td>
<td style="text-align: left;">$83.6 \%$</td>
<td style="text-align: left;">0.209</td>
<td style="text-align: left;">0.323</td>
<td style="text-align: left;">0.356</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav3-Stars[noref]</td>
<td style="text-align: left;">$83.2 \%$</td>
<td style="text-align: left;">0.198</td>
<td style="text-align: left;">0.310</td>
<td style="text-align: left;">0.235</td>
</tr>
<tr>
<td style="text-align: left;">UniTE</td>
<td style="text-align: left;">$82.8 \%$</td>
<td style="text-align: left;">0.369</td>
<td style="text-align: left;">0.378</td>
<td style="text-align: left;">0.357</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-22</td>
<td style="text-align: left;">$82.8 \%$</td>
<td style="text-align: left;">0.283</td>
<td style="text-align: left;">0.351</td>
<td style="text-align: left;">0.341</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav2-SQM[noref]</td>
<td style="text-align: left;">$82.8 \%$</td>
<td style="text-align: left;">0.216</td>
<td style="text-align: left;">0.306</td>
<td style="text-align: left;">0.310</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav3-SQM[noref]</td>
<td style="text-align: left;">$82.5 \%$</td>
<td style="text-align: left;">0.218</td>
<td style="text-align: left;">0.328</td>
<td style="text-align: left;">0.268</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Turbo-Classes</td>
<td style="text-align: left;">$82.5 \%$</td>
<td style="text-align: left;">0.170</td>
<td style="text-align: left;">0.167</td>
<td style="text-align: left;">0.178</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Chat-DA[noref]</td>
<td style="text-align: left;">$82.1 \%$</td>
<td style="text-align: left;">0.231</td>
<td style="text-align: left;">0.332</td>
<td style="text-align: left;">0.359</td>
</tr>
<tr>
<td style="text-align: left;">MATESE</td>
<td style="text-align: left;">$81.0 \%$</td>
<td style="text-align: left;">0.323</td>
<td style="text-align: left;">0.279</td>
<td style="text-align: left;">0.389</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Chat-SQM[noref]</td>
<td style="text-align: left;">$81.0 \%$</td>
<td style="text-align: left;">0.224</td>
<td style="text-align: left;">0.320</td>
<td style="text-align: left;">0.284</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Chat-DA</td>
<td style="text-align: left;">$81.0 \%$</td>
<td style="text-align: left;">0.307</td>
<td style="text-align: left;">0.328</td>
<td style="text-align: left;">0.361</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav2-Classes</td>
<td style="text-align: left;">$79.6 \%$</td>
<td style="text-align: left;">0.173</td>
<td style="text-align: left;">0.260</td>
<td style="text-align: left;">0.184</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav2-Stars[noref]</td>
<td style="text-align: left;">$79.6 \%$</td>
<td style="text-align: left;">0.142</td>
<td style="text-align: left;">0.203</td>
<td style="text-align: left;">0.193</td>
</tr>
<tr>
<td style="text-align: left;">Yife-1</td>
<td style="text-align: left;">$79.2 \%$</td>
<td style="text-align: left;">0.235</td>
<td style="text-align: left;">0.227</td>
<td style="text-align: left;">0.296</td>
</tr>
<tr>
<td style="text-align: left;">COMETKiwi[noref]</td>
<td style="text-align: left;">$78.8 \%$</td>
<td style="text-align: left;">0.290</td>
<td style="text-align: left;">0.359</td>
<td style="text-align: left;">0.364</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav3-Classes[noref]</td>
<td style="text-align: left;">$78.8 \%$</td>
<td style="text-align: left;">0.176</td>
<td style="text-align: left;">0.271</td>
<td style="text-align: left;">0.172</td>
</tr>
<tr>
<td style="text-align: left;">COMET-QE[noref]</td>
<td style="text-align: left;">$78.1 \%$</td>
<td style="text-align: left;">0.281</td>
<td style="text-align: left;">0.341</td>
<td style="text-align: left;">0.365</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Dav2-Classes[noref]</td>
<td style="text-align: left;">$78.1 \%$</td>
<td style="text-align: left;">0.105</td>
<td style="text-align: left;">0.172</td>
<td style="text-align: left;">0.128</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: left;">$77.4 \%$</td>
<td style="text-align: left;">0.232</td>
<td style="text-align: left;">0.192</td>
<td style="text-align: left;">0.316</td>
</tr>
<tr>
<td style="text-align: left;">UniTE-sec[noref]</td>
<td style="text-align: left;">$75.9 \%$</td>
<td style="text-align: left;">0.287</td>
<td style="text-align: left;">0.342</td>
<td style="text-align: left;">0.343</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-QE-22[noref]</td>
<td style="text-align: left;">$75.5 \%$</td>
<td style="text-align: left;">0.233</td>
<td style="text-align: left;">0.305</td>
<td style="text-align: left;">0.287</td>
</tr>
<tr>
<td style="text-align: left;">MATESE-QE[noref]</td>
<td style="text-align: left;">$74.8 \%$</td>
<td style="text-align: left;">0.244</td>
<td style="text-align: left;">0.229</td>
<td style="text-align: left;">0.337</td>
</tr>
<tr>
<td style="text-align: left;">f200spBLEU</td>
<td style="text-align: left;">$74.1 \%$</td>
<td style="text-align: left;">0.180</td>
<td style="text-align: left;">0.153</td>
<td style="text-align: left;">0.140</td>
</tr>
<tr>
<td style="text-align: left;">chrP</td>
<td style="text-align: left;">$73.4 \%$</td>
<td style="text-align: left;">0.214</td>
<td style="text-align: left;">0.168</td>
<td style="text-align: left;">0.147</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: left;">$70.8 \%$</td>
<td style="text-align: left;">0.169</td>
<td style="text-align: left;">0.140</td>
<td style="text-align: left;">0.145</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Turbo-Classes[noref]</td>
<td style="text-align: left;">$62.0 \%$</td>
<td style="text-align: left;">-0.010</td>
<td style="text-align: left;">0.027</td>
<td style="text-align: left;">0.029</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Curie-Classes[noref]</td>
<td style="text-align: left;">$61.7 \%$</td>
<td style="text-align: left;">0.001</td>
<td style="text-align: left;">-0.007</td>
<td style="text-align: left;">-0.053</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Bab-Stars[noref]</td>
<td style="text-align: left;">$58.4 \%$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Bab-DA[noref]</td>
<td style="text-align: left;">$55.8 \%$</td>
<td style="text-align: left;">-0.119</td>
<td style="text-align: left;">-0.011</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Curie-Stars[noref]</td>
<td style="text-align: left;">$54.7 \%$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Curie-DA</td>
<td style="text-align: left;">$54.4 \%$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Curie-DA[noref]</td>
<td style="text-align: left;">$51.8 \%$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.054</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Bab-SQM</td>
<td style="text-align: left;">$51.8 \%$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Bab-SQM[noref]</td>
<td style="text-align: left;">$51.1 \%$</td>
<td style="text-align: left;">-0.010</td>
<td style="text-align: left;">0.006</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Bab-Stars</td>
<td style="text-align: left;">$48.2 \%$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Bab-Classes</td>
<td style="text-align: left;">$47.4 \%$</td>
<td style="text-align: left;">-0.086</td>
<td style="text-align: left;">-0.089</td>
<td style="text-align: left;">-0.066</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Curie-Classes</td>
<td style="text-align: left;">$43.4 \%$</td>
<td style="text-align: left;">-0.011</td>
<td style="text-align: left;">-0.035</td>
<td style="text-align: left;">-0.035</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Curie-SQM[noref]</td>
<td style="text-align: left;">$41.6 \%$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Curie-SQM</td>
<td style="text-align: left;">$40.5 \%$</td>
<td style="text-align: left;">0.026</td>
<td style="text-align: left;">0.013</td>
<td style="text-align: left;">0.007</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Bab-DA</td>
<td style="text-align: left;">$39.1 \%$</td>
<td style="text-align: left;">-0.110</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Curie-Stars</td>
<td style="text-align: left;">$37.2 \%$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA-Bab-Classes[noref]</td>
<td style="text-align: left;">$35.0 \%$</td>
<td style="text-align: left;">-0.010</td>
<td style="text-align: left;">-0.033</td>
<td style="text-align: left;">0.067</td>
</tr>
</tbody>
</table>
<p>Table 7: System-level pairwise accuracy and segment-level Kendall Tau scores.</p>
<h1>C Appendix: Ada Answers</h1>
<p>We present some of the replies from the Ada model on various prompts in Table 8 to illustrate the frequent, invalid answers of this model. Similar behaviour was observed when querying GPT 2. Both models failed with respect to our translation quality assessment experiment.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">$0=$ no meaning preserved</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$1=$ perfect meaning and grammar</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$2=$ excellent meaning</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$3=$ very good meaning</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$4=$ excellent</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$5=$ very good</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$6=$ excellent</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$7=$ excellent</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$8=$ excellent</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$9=$ excellent</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$10=$ perfect</td>
</tr>
<tr>
<td style="text-align: center;">DA</td>
<td style="text-align: center;">No meaning preserved: 0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perfect meaning and grammar: 100</td>
</tr>
<tr>
<td style="text-align: center;">DA</td>
<td style="text-align: center;">$0-$ It does not exist in my language</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$1-$ It does not exist in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$2-$ It does not exist in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$3-$ It exists in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$4-$ It exists in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$5-$ It exists in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$6-$ It exists in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$7-$ It exists in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$8-$ It exists in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$9-$ It exists in my text</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$10-$ It exists in my text</td>
</tr>
<tr>
<td style="text-align: center;">DA</td>
<td style="text-align: center;">This sentence does not have a score.</td>
</tr>
<tr>
<td style="text-align: center;">DA[noref]</td>
<td style="text-align: center;">The source for this translation is English. The translation is 1 out of 100.</td>
</tr>
<tr>
<td style="text-align: center;">DA[noref]</td>
<td style="text-align: center;">$0=$ no meaning preserved</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$1=$ perfect meaning and grammar</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$2=$ perfect meaning</td>
</tr>
<tr>
<td style="text-align: center;">SQM</td>
<td style="text-align: center;">Some meaning preserved</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Most meaning preserved and few grammar mistakes</td>
</tr>
<tr>
<td style="text-align: center;">SQM[noref]</td>
<td style="text-align: center;">No meaning preserved</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Some meaning preserved</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Most meaning preserved and few grammar mistakes</td>
</tr>
</tbody>
</table>
<p>Table 8: Answers by the Ada model for various prompts. We observe that SQM prompts are closer to expected outputs than answers to the corresponding DA prompts. Similar behaviour was observed when querying GPT 2.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://github.com/google-research/
mt-metrics-eval&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://learn.microsoft.com/en-us/
azure/cognitive-services/openai/concepts/
models and https://platform.openai.com/docs/ model-index-for-researchers&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
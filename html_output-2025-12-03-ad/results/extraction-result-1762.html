<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1762 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1762</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1762</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-c8a30bbbe787bca245dede9f0e789c62388e4482</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c8a30bbbe787bca245dede9f0e789c62388e4482" target="_blank">Reinforcement Learning for Pivoting Task</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an approach to learn a robust policy for solving the pivoting task and shows that several model-free continuous control algorithms were shown to learn successful policies with this approach.</p>
                <p><strong>Paper Abstract:</strong> In this work we propose an approach to learn a robust policy for solving the pivoting task. Recently, several model-free continuous control algorithms were shown to learn successful policies withou ...</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1762.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1762.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pivoting sim-to-real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning for Pivoting Task (sim-to-real transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real study where a model-free RL policy (TRPO) is trained in a simple custom simulator with randomized friction and actuation delays, and the resulting policy is deployed on a Baxter robot to perform an in-hand pivoting task; demonstrates robustness to large mismatch in friction and to an unseen tool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Baxter (single 7-DOF arm with parallel gripper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A Baxter robot arm (7 DOF) using a 1-DOF parallel gripper attached to the wrist; fingers have slightly deformable fingertips and finger distance can be commanded; vision via Kinect2 (30 fps) tracks a colored marker on the tool.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (extrinsic dexterity / in-hand manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>custom simulator (OpenAI Gym environment)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A custom 2-link planar dynamics simulator implemented as an OpenAI Gym environment that simulates the actuated gripper link and the under-actuated pivoting tool using rigid-body dynamics equations, Coulomb + viscous friction at the pivot, a fingertip deformation model for normal force, and injected actuator delays/noise.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>simplified / approximate dynamics (custom low-complexity physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body two-link planar dynamics (inertial and gravity terms), torsional friction at contact (static Coulomb bound; kinetic: viscous + Coulomb), fingertip normal force via linear deformation model, actuator delays and action noise, randomized task initial/target angles.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>only the final two-link subsystem modeled (not whole-arm dynamics), friction parameters approximated (not measured precisely), no tactile sensing modeled, no photorealistic rendering or camera simulation, limited modeling of complex deformation/airflow/soft-body effects; many real-world parameters approximated and randomized rather than precisely estimated.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Baxter robot arm with parallel gripper (deformable fingertips), Kinect2 RGB-D camera (30 fps) for colored-marker tracking; measured gripper actuation delay ~80 ms; experiments performed with two real tools differing in mass, inertia and friction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>In-hand pivoting: reorienting a grasped tool to a target angle by commanding arm accelerations and changing gripper finger distance to control friction.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Model-free reinforcement learning (Trust Region Policy Optimization, TRPO) trained in simulation (rllab-like setup) with neural-network policies (2 hidden layers: 32 and 16 nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Percentage of trials reaching target (goal region ±3°) and episode-based reward / steps-to-goal; on-hardware reported 'Goal reached' counts over 30 trials per tool.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Tool 1: 28/30 successes (≈93% success); Tool 2 (unseen during training): 25/30 successes (≈83% success). Typical time-to-target: ~5 s (tool1) and ~10 s (tool2) in demonstrated episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized initial/target angles across episodes; injected up to ±10% randomized delay for arm and finger actions; added ±10% noise to modeled friction values (k·μ_c) during training; training episodes sampled parameters from ranges to produce variability.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Unmodeled / imprecisely modeled friction; actuator timing/delay differences; sensor/tracker inaccuracies and limited camera frame rate; limited gripper control frequency and mechanical delays (80 ms), simplified dynamics (only last links modeled), potential sliding and tool drops, and lack of tactile feedback in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of a dynamics-based custom simulator with explicit friction modeling plus deliberate domain randomization (friction noise, actuation delays, randomized parameters); training robust policies with TRPO (stochastic policy optimization) and using a representation that generalizes across symmetric motions; using deformable fingertips and visual tracking in hardware to match key control affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No strict quantitative fidelity threshold specified; paper finds that approximate dynamics with randomized friction and delays is sufficient for transfer — precise friction estimation is not required (policy retained partial performance even with 250–500% friction mismatch). Contact/friction modeling plus randomized variation is identified as important, but exact numerical fidelity requirements were not given.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>They evaluated robustness to friction values outside the training noise: although only ±10% friction noise was used in training, the learned policy retained ≈40% success rate when tested in simulation with 250%–500% change in Coulomb friction (k·μ_c). No systematic multi-fidelity comparison was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training model-free RL policies in a simple, dynamics-based simulator with explicit but approximate friction modeling and deliberate domain randomization (actuation delays and friction noise) can produce policies that transfer to a real robot (Baxter) for in-hand pivoting with high success (≈93% on the tool modeled in sim; ≈83% on an unseen tool). Precise friction parameter identification is not necessary if randomness and robustness are built into training; contact/friction and actuation delay effects are major contributors to sim-to-real gap and should be randomized or modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning for Pivoting Task', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real robot learning from pixels with progressive nets <em>(Rating: 2)</em></li>
                <li>A general framework for open-loop pivoting <em>(Rating: 1)</em></li>
                <li>Benchmarking deep reinforcement learning for continuous control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1762",
    "paper_id": "paper-c8a30bbbe787bca245dede9f0e789c62388e4482",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Pivoting sim-to-real",
            "name_full": "Reinforcement Learning for Pivoting Task (sim-to-real transfer)",
            "brief_description": "A sim-to-real study where a model-free RL policy (TRPO) is trained in a simple custom simulator with randomized friction and actuation delays, and the resulting policy is deployed on a Baxter robot to perform an in-hand pivoting task; demonstrates robustness to large mismatch in friction and to an unseen tool.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Baxter (single 7-DOF arm with parallel gripper)",
            "agent_system_description": "A Baxter robot arm (7 DOF) using a 1-DOF parallel gripper attached to the wrist; fingers have slightly deformable fingertips and finger distance can be commanded; vision via Kinect2 (30 fps) tracks a colored marker on the tool.",
            "domain": "general robotics manipulation (extrinsic dexterity / in-hand manipulation)",
            "virtual_environment_name": "custom simulator (OpenAI Gym environment)",
            "virtual_environment_description": "A custom 2-link planar dynamics simulator implemented as an OpenAI Gym environment that simulates the actuated gripper link and the under-actuated pivoting tool using rigid-body dynamics equations, Coulomb + viscous friction at the pivot, a fingertip deformation model for normal force, and injected actuator delays/noise.",
            "simulation_fidelity_level": "simplified / approximate dynamics (custom low-complexity physics simulator)",
            "fidelity_aspects_modeled": "rigid-body two-link planar dynamics (inertial and gravity terms), torsional friction at contact (static Coulomb bound; kinetic: viscous + Coulomb), fingertip normal force via linear deformation model, actuator delays and action noise, randomized task initial/target angles.",
            "fidelity_aspects_simplified": "only the final two-link subsystem modeled (not whole-arm dynamics), friction parameters approximated (not measured precisely), no tactile sensing modeled, no photorealistic rendering or camera simulation, limited modeling of complex deformation/airflow/soft-body effects; many real-world parameters approximated and randomized rather than precisely estimated.",
            "real_environment_description": "Baxter robot arm with parallel gripper (deformable fingertips), Kinect2 RGB-D camera (30 fps) for colored-marker tracking; measured gripper actuation delay ~80 ms; experiments performed with two real tools differing in mass, inertia and friction.",
            "task_or_skill_transferred": "In-hand pivoting: reorienting a grasped tool to a target angle by commanding arm accelerations and changing gripper finger distance to control friction.",
            "training_method": "Model-free reinforcement learning (Trust Region Policy Optimization, TRPO) trained in simulation (rllab-like setup) with neural-network policies (2 hidden layers: 32 and 16 nodes).",
            "transfer_success_metric": "Percentage of trials reaching target (goal region ±3°) and episode-based reward / steps-to-goal; on-hardware reported 'Goal reached' counts over 30 trials per tool.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Tool 1: 28/30 successes (≈93% success); Tool 2 (unseen during training): 25/30 successes (≈83% success). Typical time-to-target: ~5 s (tool1) and ~10 s (tool2) in demonstrated episodes.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized initial/target angles across episodes; injected up to ±10% randomized delay for arm and finger actions; added ±10% noise to modeled friction values (k·μ_c) during training; training episodes sampled parameters from ranges to produce variability.",
            "sim_to_real_gap_factors": "Unmodeled / imprecisely modeled friction; actuator timing/delay differences; sensor/tracker inaccuracies and limited camera frame rate; limited gripper control frequency and mechanical delays (80 ms), simplified dynamics (only last links modeled), potential sliding and tool drops, and lack of tactile feedback in simulation.",
            "transfer_enabling_conditions": "Use of a dynamics-based custom simulator with explicit friction modeling plus deliberate domain randomization (friction noise, actuation delays, randomized parameters); training robust policies with TRPO (stochastic policy optimization) and using a representation that generalizes across symmetric motions; using deformable fingertips and visual tracking in hardware to match key control affordances.",
            "fidelity_requirements_identified": "No strict quantitative fidelity threshold specified; paper finds that approximate dynamics with randomized friction and delays is sufficient for transfer — precise friction estimation is not required (policy retained partial performance even with 250–500% friction mismatch). Contact/friction modeling plus randomized variation is identified as important, but exact numerical fidelity requirements were not given.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "They evaluated robustness to friction values outside the training noise: although only ±10% friction noise was used in training, the learned policy retained ≈40% success rate when tested in simulation with 250%–500% change in Coulomb friction (k·μ_c). No systematic multi-fidelity comparison was reported.",
            "key_findings": "Training model-free RL policies in a simple, dynamics-based simulator with explicit but approximate friction modeling and deliberate domain randomization (actuation delays and friction noise) can produce policies that transfer to a real robot (Baxter) for in-hand pivoting with high success (≈93% on the tool modeled in sim; ≈83% on an unseen tool). Precise friction parameter identification is not necessary if randomness and robustness are built into training; contact/friction and actuation delay effects are major contributors to sim-to-real gap and should be randomized or modeled.",
            "uuid": "e1762.0",
            "source_info": {
                "paper_title": "Reinforcement Learning for Pivoting Task",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real robot learning from pixels with progressive nets",
            "rating": 2
        },
        {
            "paper_title": "A general framework for open-loop pivoting",
            "rating": 1
        },
        {
            "paper_title": "Benchmarking deep reinforcement learning for continuous control",
            "rating": 1
        }
    ],
    "cost": 0.008262,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reinforcement Learning for Pivoting Task</h1>
<p>Rika Antonova ${ }^{1}$, Silvia Cruciani ${ }^{1}$, Christian Smith and Danica Kragic</p>
<h4>Abstract</h4>
<p>In this work we propose an approach to learn a robust policy for solving the pivoting task. Recently, several model-free continuous control algorithms were shown to learn successful policies without prior knowledge of the dynamics of the task. However, obtaining successful policies required thousands to millions of training episodes, limiting the applicability of these approaches to real hardware. We developed a training procedure that allows us to use a simple custom simulator to learn policies robust to the mismatch of simulation vs robot. In our experiments, we demonstrate that the policy learned in the simulator is able to pivot the object to the desired target angle on the real robot. We also show generalization to an object with different inertia, shape, mass and friction properties than those used during training. This result is a step towards making model-free reinforcement learning available for solving robotics tasks via pre-training in simulators that offer only an imprecise match to the real-world dynamics.</p>
<h2>I. INTRODUCTION</h2>
<p>In this work we address the problem of pivoting an object held by a robotic gripper. Pivoting consists of rotating an object or a tool between two fingers to reorient it to a desired angle. This problem falls in the general class of extrinsic dexterity problems formulated by [1]. Since many tasks in robotics require interaction with objects and tool use, the ability of placing items in the correct pose with respect to the gripper is important.</p>
<p>Instead of relying on releasing the object and picking it up again [2], we focus on in-hand manipulation. Successful strategies for this dexterous task often rely on multi-fingered robotic hands or customized grippers [3]-[5]. However, currently many robots have only parallel grippers, e.g. Baxter, PR2, Yumi. Thus it is important to develop regrasping strategies that do not rely on the additional degrees of freedom provided by complex hands.</p>
<p>We formalize the pivoting task as a reinforcement learning problem. The goal is to discover policies that, given the current state of the gripper and the tool, produce a sequence of actions to pivot the tool to the desired target angle. The actions consist of commanding the acceleration of the gripper and the distance for gripper's fingers. It is important to note that learning directly on the robot could focus too narrowly on the specific tool and robotic manipulator, while attempts to generalize would likely make the learning infeasible in terms of time. Hence it is not straightforward to directly apply reinforcement learning to this problem, if one aims</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Pivoting task executed on the Baxter robot - pivoting the tool to $45^{\circ}$ with respect to the central axis of the gripper.
to obtain policies that generalize beyond the exact hardware setup and tool properties.</p>
<p>Our contribution is an approach to learn robust policies in a custom simulator, while taking into account the mismatch between the simulation and the real robot. We begin by constructing a simulator using dynamics equations for the setting. These equations describe well the general behavior of the system, but contain parameters that are infeasible to estimate precisely, like friction properties of the objects. Hence we generate a variety of training episodes with parameters randomly chosen from a range of values. We also simulate delays and errors in actuation. This results in learning policies robust to the mismatch between the outcome of control actions in simulation vs on a real robot.</p>
<p>We designed this approach of training with a custom simulator so that we are not limited to only sample-efficient learning algorithms. While sample-efficient learning in real time can be effective, it could limit the flexibility and applicability of the learned policies to slightly altered hardware setups. In contrast, we can apply recent model-free policy search algorithms, even those not widely used for real robots previously. This facilitates learning flexible non-linear control policies, including those represented by deep neural networks.</p>
<p>We demonstrate that our approach is able to learn policies to successfully solve the pivoting task when used on the robot. We present experiments with controlling the tool whose observable physical properties, like mass and inertia, are used when constructing the simulator. We also demonstrate that the same policy is able to successfully control an object whose properties differ from those used during training. Fig. 1 above shows one of our experiments with parallel gripper executing a pivoting task.</p>
<h2>II. Related Work</h2>
<p>Extrinsic dexterity has been widely studied in robotics and is still an open challenge. Pivoting is one type of extrinsic dexterity problem that recently attracted attention of the robotics research community. In the following section we provide and overview of the previous work. Since in contrast to prior approaches ours employs deep reinforcement learning, we also provide a brief overview of this learning approach in the context of robotics.</p>
<h2>A. Previous Work in Pivoting</h2>
<p>Existing solutions for pivoting exploit environmental constraints, motions of the robot arm to generate inertial forces, and external forces, such as gravity. In [6] the authors exploit gravity to rotate an object between two stable poses by using a contact surface. The pivoting is performed open loop and there is no control of the gripping force. Conversely, several other works on pivoting strongly focus on controlling the torque applied by the gripper on the object: in [7] the authors focus on swing-up motions. They address the problem using an energy-based control and they consider the ability of the gripper to exert dissipative torques on the object thanks to the friction at the pivoting point. In this case the motion of the object appears to be limited to a vertical plane and the approach strongly depends on fast sensory feedback and rapid response time of the gripper. The adaptive control for pivoting presented in [8] exploits gravity and controlled slip with only visual feedback. This approach has then been extended to consider also tactile feedback in [9]. However, the gripper is assumed to be in a fixed position, therefore the motion of the tool is determined only by the gravitational torque and the torsional friction. This motion is limited to be in a vertical plane and the proposed strategy can successfully reorient the object only when the desired configuration has less potential energy than the initial one.</p>
<p>All these prior approaches rely strongly on having an accurate model of the object the gripper is holding, as well as precise measurement and modeling of the friction. Since it is difficult to obtain accurate estimates of these quantities, especially when they are related to friction modeling, in our work we do not rely strongly on highly accurate parameters for the successful outcome of a pivoting action.</p>
<h2>B. Deep Reinforcement Learning in Robotics</h2>
<p>Reinforcement Learning (RL) algorithms can be classified into two broad categories: model-based and model-free. Model-based algorithms can make learning data-efficient by assuming the dynamics of the task can be captured by a particular model (frequently parametric), then estimating the parameters of the model from samples obtained when learning to solve a task. However, strict global assumptions on the model class can limit the flexibility of the model-based representation. While this can be partially resolved by learning local models (e.g. [10], [11]), such approaches might limit ability to incorporate prior knowledge - for example, forgoing the knowledge that can be easily incorporated into dynamics equations in a simulator.</p>
<p>Model-free RL algorithms instead allow to learn flexible control policies by directly interacting with either simulated or real environment. Recently, several model-free continuous state continuous action reinforcement learning algorithms have been proposed. Among these, two algorithms were reported to perform well on several simulated robotic tasks: Trust Region Policy Optimization (TRPO) [12] and Deep Deterministic Policy Gradient (DDPG) [13]. Both use neural networks as policy and Q function approximators. While it has been shown that these approaches can handle problems similar in principle to those considered in robotics, significant practical problems arise when applying modelfree policy search algorithms to real-world robotics tasks.</p>
<p>First of all, when using deep neural networks as function approximators, the question of data efficiency becomes key: the number of training episodes needed might be prohibitively large. For example, a recent benchmarking paper used up to 25 million steps when training [14]. The paper that introduced DDPG used up to 2 million steps, sometimes without achieving close to maximum reward (which in some cases means not being able to solve the task). Networks reported as successful had from 2 to 3 hidden layers with 25 to 400 nodes in each ( 15 K to 200 K parameters to learn, depending on the particular network structure). This was the case even for low-dimensional continuous problems, where state space is represented by joint angles, torques, velocities (as opposed to pixels) and the task of sensing the state is considered separate - e.g. assumed to be performed by an off-the-shelf tracking system or achieved via custom hardware sensors.</p>
<p>Secondly, extensive hyper-parameter search might be needed for ensuring learning success on a given problem. In addition to the usual RL considerations, e.g. how aggressively to explore or how to shape the reward function, a new challenge is selecting the appropriate neural network architecture.</p>
<p>One additional challenge arises from the phenomenon known in deep learning literature as "forgetting". It has been observed that when training a neural network on a new task, the capacity to perform old tasks well can diminish. For the pivoting problem considered in this work, "forgetting" is problematic for several reasons. Reaching different target angles could be seen as slightly different tasks, since the optimal policies might differ. Theoretically, this is resolved by adding the target angle to the representation of the state. Practically, the knowledge acquired when training to reach different target angles is embedded in the same network, and this could impede or stall the learning progress during training. This presents a challenge for real-time learning, hence giving the motivation for our approach to learn a robust policy in simulation, where training on a large number of episodes could allow to recover from slow or inconsistent learning progress. The problem of "forgetting" in the context of learning in robotics is discussed at length in [15] - the work that attempted to resolve the issue by pre-training in simulation, then using Progressive Neural Network architecture to continue learning on the real hardware. This or a</p>
<p>similar approach could also be applicable to the pivoting task setting, however it might still require a significant amount of time for the second stage of training on the robot. Hence, we are motivated to develop an approach that can learn acceptable policies directly from the simulator, and would be applicable even when second stage training on the hardware is costly or infeasible.</p>
<h2>III. Problem Description</h2>
<p>The problem we address is pivoting the tool to a desired angle while holding it in the gripper. This can be accomplished by moving the arm of the robot to generate inertial forces sufficient to move the tool and, at the same time, opening or closing the fingers of the gripper to change the friction at the pivoting point, gaining more precise control of the motion. We assume that the robot is able to use one of the standard planning approaches to initially grasp the tool, but the position of the tool between the fingers is initially at some random angle $\phi_{i n i t}$. The goal is to pivot the tool to a desired target angle $\phi_{t g t}$.</p>
<p>The first challenge is that the motion of the tool is influenced by the friction at the pivoting point. This is dictated by the materials of the tool and fingers, the deformation of the tool and fingers, and potentially the air flow. It is difficult to estimate precisely all the necessary coefficients to construct a high-fidelity friction model [16].</p>
<p>The second challenge is due to limitations of the robot hardware: delays in actuation, possible errors in execution, joint and velocity limits and constraints maintained to ensure safety. These cause the overall dynamics of the system to be uncertain. Combined with difficulties of estimating the object properties, this causes high uncertainty of the outcomes of the commanded actions overall.</p>
<p>The above challenges could be addressed by using a vision system with high frame rate to estimate the current state and high frequency control of the robot arm to ensure rapid response and re-adjustment. However, currently available commercial robots often have limited control frequency for adjusting gripper’s fingers. Moreover, it is preferable to be able to solve the pivoting task with readily available vision systems, which have limited frame rate.</p>
<p>We take into consideration all of the challenges mentioned. Hence we propose a learning approach that lets us obtain control policies robust to some degree of uncertainty of both the tool properties and imprecisions of robot motion execution.</p>
<h2>IV. Modeling for Simulation</h2>
<p>As mentioned in the previous section, simulators that can effectively incorporate global information about the task dynamics can be utilized for learning flexible control policies. In this section we describe the dynamic model of the system and the friction model that we used to simulate the pivoting task.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Model of a 2-link planar arm. The first link represents the gripper and the second link represents the tool that rotates around the pivoting point.</p>
<h3>A. Dynamic Model</h3>
<p>Our system is composed of a 1 DOF parallel gripper attached to a link that can rotate around a single axis. This system is an under-actuated two-link planar arm, in which the under-actuated joint corresponds to the pivoting point. We assume that we can control the desired acceleration on the first joint. This system is shown in Fig. 2 above.</p>
<p>The dynamic model of the system is given by:</p>
<p>$$
\begin{aligned}
&amp; \left(I+mr^2 + mlr\cos(\phi_{tl})\right)\ddot{\phi}<em tl="tl">{grp} + \left(I+mr^2\right)\ddot{\phi}</em> + \dots \
&amp; \quad \dots + mlr\sin(\phi_{tl})\dot{\phi}<em grp="grp">{grp}^2 + mgr\cos(\phi</em>) = \tau_f
\end{aligned}
\tag{1}
$$} + \phi_{tl</p>
<p>where the variables are as follows: $\phi_{grp}$ and $\phi_{tl}$ are the angles of the first and second link respectively; $\ddot{\phi}<em tl="tl">{grp}$ and $\ddot{\phi}</em>}$ are their angular acceleration and $\dot{\phi<em tl="tl">{grp}$ is the angular velocity of the first link; $l$ is the length of the first link; $I$ is the tool’s moment of inertia with respect to its center of mass; $m$ is the tool’s mass; $r$ is the distance of its center of mass from the pivoting point; $g$ is the gravity acceleration; $\tau_f$ is the torsional friction at the contact point between the gripper’s fingers and the tool. In our case, the second link represents the tool and $\phi</em>$ is the variable we aim to control.</p>
<p>On a real setup, the modeled two-link arm is the final part of a robotic manipulator. Therefore, the gravity component $g$ varies according to the current orientation of the plane that contains the actuated link and the tool. We assume that the manipulator’s configuration is such that this plane has only pitch angle and no roll. As a result, the acceleration due to gravity influences only one direction of motion and $g$ varies between 0 and 9.8 according to the pitch angle, without the need of an additional term in Equation 1.</p>
<h3>B. Friction Model</h3>
<p>Our proposed solution for pivoting exploits the friction at the contact point between the gripper and the tool to control the rotational motion. Such friction is controlled by enlarging or tightening the grasp.</p>
<p>When the tool is not moving, that is $\dot{\phi}_{tl} = 0$, the static friction $\tau_s$ is modeled according to the Coulomb friction model:</p>
<p>$$
|\tau_s| \leq \gamma f_n,
\tag{2}
$$</p>
<p>where $\gamma$ is the coefficient of static friction and $f_n$ is the normal force applied by the gripper’s fingers on the tool.</p>
<p>When the tool moves with respect to the gripper, that is $\dot{\phi}<em f="f">{t l} \neq 0$, we model the friction torque $\tau</em>$ as viscous friction and Coulomb friction [17]:</p>
<p>$$
\tau_{f}=-\mu_{v} \dot{\phi}<em c="c">{t l}-\mu</em>\right)
$$} f_{n} \operatorname{sgn}\left(\dot{\phi}_{t l</p>
<p>in which $\mu_{v}$ and $\mu_{c}$ are the viscous and Coulomb friction coefficients respectively and $\operatorname{sgn}(\cdot)$ is the signum function.</p>
<p>When the tool starts moving numerical singularities can occur due to switching between the two models in Equations 2 and 3. To alleviate this problem we follow the approach proposed in [18]. We define a neighborhood $\left|\dot{\phi}_{t l}\right| \leq \epsilon$, for a small $\epsilon&gt;0$, where the friction torque is equal to the net torque acting on the tool. Therefore, when the tool has zero velocity, the normal force will counterbalance the net torque.</p>
<p>Since most of the robots are not equipped with tactile sensors to measure the normal force $f_{n}$ at the contact point, we follow the approach proposed in [8] and express this force as a function of the distance $d_{f i n g}$ between the two fingers, assuming a linear deformation model:</p>
<p>$$
f_{n}=k\left(d_{0}-d_{f i n g}\right)
$$</p>
<p>where $k$ is a stiffness parameter and $d_{0}$ is the distance at which there is no fingertip deformation. In other words, $d_{0}$ is the distance at which the fingers initiate the contact with the tool.</p>
<h2>V. LEARNING</h2>
<p>As discussed in Section II-B, recent algorithmic advances in reinforcement learning suggest the possibility that modellfree algorithms could be applied to robotics tasks. However, since these algorithms frequently are not designed to be sample-efficient enough to learn in real time on the hardware, our approach is to construct a training procedure to learn robust policies in a simple custom simulated environment, then deploy on the robot.</p>
<p>Below we first describe the overall training approach, then present formalization of the pivoting task as a Markov Decision Process, then give a brief summary of the modellree reinforcement learning algorithm we used for policy search.</p>
<h2>A. Learning Robust Policies using a Simulator</h2>
<p>As described in Section III, our approach is to enable learning from simulated environment, while being robust to the discrepancies between the simulation and actual execution on the robot. For this purpose, we first built a simple custom simulator using the equations described in Section IV. Then, to facilitate learning policies robust to uncertainty, we injected up to $10 \%$ randomized delay for arm and finger actions in simulation. We also added $10 \%$ noise to friction values estimated for the tool modeled by the simulator. We then trained a model-free deep reinforcement learning policy search algorithm on our simulated setting. Lastly, we executed the resulting policy on the robot (Baxter) for evaluation. This approach allowed us to keep the simulator simple and fast, while still enabling learning policies robust to the mismatch of the simulated and real environments.</p>
<h2>B. Pivoting Task as a Markov Decision Process</h2>
<p>We formulate the pivoting task as a Markov Decision Process (MDP) - a tuple $\left{S, A, P\left(s^{\prime} \mid s, a\right), R\left(s, a, s^{\prime}\right), \alpha\right}$. The state space $S$ is comprised of states $s_{t}$ observed at each time step $t: s_{t}=\left[\phi_{t l}-\phi_{t g t}, \dot{\phi}<em g="g" p="p" r="r">{t l}, \phi</em>}, \dot{\phi<em f="f" g="g" i="i" n="n">{g r p}, d</em>$ separately.}\right]$, with notation as in Section IV-A. Using the signed distance of the tool angle as the first component of the state vector allows us to facilitate generalization in learning, since in our setting the motions of the robot arm are symmetric, e.g. the optimal policy for reaching target angle $0^{\circ}$ starting from tool at $-45^{\circ}$ would be symmetric to the case of starting from $45^{\circ}$. For the settings without the symmetry the state space representation could instead include $\phi_{t l}$ and $\phi_{t g t</p>
<p>The action space $A$ is comprised of actions $a_{t}$ at time $t$ : $a_{t}=\left{\dot{\phi}<em f="f" g="g" i="i" n="n">{g r p}, d</em>}\right}$, where $\dot{\phi<em f="f" g="g" i="i" n="n">{g r p}$ is the rotational acceleration of the robot arm, and $d</em>$ is the distance between the fingers of the gripper. In cases where the hardware is limited in ability to achieve fingers' distances precisely, it is advantageous to learn to control the direction of the change in distance instead of precise target distance. We discuss this further in the experiments section.</p>
<p>The transition probabilities $P\left(s^{\prime} \mid s, a\right)$ are not used during learning explicitly, since we employ model-free approaches for learning. The dynamics of the state transitions is implemented by the simulator (as described in Section IV-A), but the learner does not have an explicit access to the state transition dynamics.</p>
<p>The reward function $R$ gives a reward $r_{t} \in[-1,1]$ at each time step $t$ such that higher rewards are given when the angle of the tool is closer to the target angle: $r(t)=\frac{-\left|\phi_{t l}-\phi_{t g t}\right|}{\phi_{R N G}}$, where $\phi_{R N G}$ is a normalizing constant denoting the range of angles the tool can attain, e.g. close to $2 \pi$ if the motion is not further restricted by the shape of the tool or the gripper. A bonus of 1 is given when the goal region is reached - the tool is close to the target angle and the velocity of the tool with respect to the gripper is not changing, i.e. the tool is gripped firmly.</p>
<p>Finally, to obtain infinite horizon discounted MDP, since we aim to achieve the goal within 100 steps on average, we use the discount factor of $\alpha=0.99$ (from $\frac{1}{1-\alpha}=100$ ). Of course the horizon of 100 can be changed as needed depending on the desired duration of the pivoting task.</p>
<h2>C. Policy Search using Reinforcement Learning</h2>
<p>As discussed in Section II-B, several continuous control model-free reinforcement learning algorithms would be suitable for learning optimal policies for our formulation of the pivoting task as MDP. In our experiments, we found that Trust Region Policy Optimization [12] was able to solve the problem without extensive parameter adjustment. Below we summarize the main ideas behind TRPO for a brief overview.</p>
<p>TRPO is a method for optimizing large non-linear control policies, as those represented by neural networks. The algorithm aims to ensure monotonic improvement during training by computing a safe region for exploration.</p>
<p>The main optimization performed by the algorithm is to iteratively solve a set of optimization problems:</p>
<p>$\operatorname{maximize}<em _rho__theta__o="\rho_{\theta_{o" _sim="\sim" d="d" l="l" s="s">{\theta} E</em>(s, a)\right]$
subject to $E_{s \sim \rho_{\theta_{o l d}}}\left[D_{K L}\left(\pi_{\theta_{o l d}}(\cdot \mid s) | \pi_{\theta}(\cdot \mid s)\right)\right] \leq \delta$,
where $\theta_{o l d}$ denotes the initial (or previous) set of policy parameters, $\theta$ denotes the updated policy parameters, $\pi_{\theta}$ is the stochastic policy parameterized by $\theta, q$ is a sampling distribution for exploration, $Q_{\theta_{o l d}}$ is the Q function approximator estimated from previous samples, and the expectation $E$ is taken over samples obtained using the policy from previous iteration (see [12] for details). The constraint in the optimization aims to keep the new policy sufficiently close to the old to yield (in theory) monotonically improving policies by limiting KL divergence of $\pi_{\theta}(\cdot \mid s)$ from $\pi_{\theta_{o l d}}(\cdot \mid s)$. Briefly, the overall structure of the algorithm is: 1) collect a set of state-action pairs along with Monte Carlo estimates of their Q-values; 2) by averaging over samples, construct the estimated objective and constraint from Equation 5; 3) approximately solve this constrained optimization problem to update policy parameter vector $\theta$ (using conjugate gradient followed by a line search).}}}, a \sim q\left[\frac{\pi_{\theta}(a \mid s)}{q(a \mid s)} Q_{\theta_{o l d}</p>
<p>TRPO has been shown to be competitive with (and sometimes outperform) other recent continuous state and action RL algorithms [14]. However, to our knowledge it has not yet been widely applied to real-world robotics tasks. While the background for the algorithm is well-motivated theoretically, the approximations made for practicality, along with challenges in achieving reasonable training results with a small-to-medium number of samples, could impair the applicability of the algorithm to learning on the robot directly. Hence we explore the approach of using TRPO for policy search in a simulated environment.</p>
<h2>VI. EXPERIMENTS</h2>
<p>In this section we first discuss implementation details of training in the simulator. We then show initial evaluation results indicating potential for robustness to changes in friction. Then we describe experiments run on Baxter robot. We discuss the performance on the tool with parameters similar to the ones used during training, as well as results for manipulating the tool with different shape, mass, inertia parameters and unknown friction properties. The results indicate that our training procedure is able to produce robust policies capable of solving the pivoting task on both known and unknown tool.</p>
<h2>A. Learning Robust Policies in Simulation</h2>
<p>To facilitate experimenting with various RL algorithms, we implemented a custom environment in OpenAI Gym [19] for the pivoting task MDP (as defined in Section V-B). To experiment with TRPO [12] and DDPG [13] algorithms, we used rllab implementation [14] as a starting point, then adjusted the behavior and parameters as needed for various experiments for this project. Both TRPO and DDPG papers
documented exploration and network training parameters used to obtain results for simulated control tasks. Starting with these reported values, we experimented with several options like rate of exploration, batch size for neural network training and network size. We were not able to obtain robust learning with DDPG (training progress would frequently stall), so for the rest of the experiments we decided to only use TRPO, which in our setting exibited a gradual but satisfactory learning progress.</p>
<p>We trained TRPO with a fully connected network with 2 hidden layers (with 32, 16 nodes) for policy and Q function approximators. We also experimented with larger networks of up to 3 hidden layers, though found that the smaller network was enough to solve the task. When generating training episodes initial and target angles were chosen at random from $[-\pi / 2, \pi / 2]$; each training episode had 100 steps. The motion was constrained to be linear in a horizontal plane. However, since the learning was not at all informed of the physics of the task, any other plane could be chosen and implemented by the simulator if needed. Fig. 3 below visualizes evaluation of the policies as the number of training iterations increases.</p>
<p>Fig. 3: Evaluation of the training progress of TRPO algorithm. 50 episodes were used for each training iteration. The goal region was $\pm 3^{\circ}$ of the target angle. Left plot shows average reward when evaluating the policy obtained at the current training iteration on angles randomly selected from $[-\pi / 2.5, \pi / 2.5]\left(\approx\left[-72^{\circ}, 72^{\circ}\right]\right)$. Right plot shows average number of steps before reaching the goal (with a cap of 250 steps per episode).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>To simulate the dynamics of the gripper arm and the tool we used the modeling approach described in Section IV. We used the parameters of tool 1 as specified in Table I with friction coefficients from Equation 3 set to $\mu_{v}=0.066, k \mu_{c}=9.906$.</p>
<p>As discussed in Section V-A, the noise injected when training in simulation aimed to help learning policies robust to mismatch between the simulator and the robot. Fig. 4 below visualizes using the policy obtained after 20K training iterations on simulated settings with a mismatch larger than noise used during training. While we allowed only up to $10 \%$ noise in the value $k \mu_{c}$ which modeled Coulomb friction, we observed that the policy was still able to retain $40 \%$ success</p>
<p>rate of reaching the target angle when tested on settings with 250% to 500% change in $k\mu_{c}$. This level of robustness suggests we could to avoid estimating friction coefficients precisely. This is crucial for the pivoting task, since, unlike estimating the dimensions of the tools, estimating the friction properties precisely would be an intractable challenge for most widely available robotic systems.</p>
<p>Fig. 4: Evaluating performance on friction coefficients outside of the noise range used during training. Coulomb friction was set to $k\mu_{c}=9.906$ during training, with noise of up to $\pm 0.1k\mu_{c}$. The plots below show the average reward and % of evaluation episodes with goal reached when evaluated on settings with friction coefficient significantly larger or smaller than the range used during training. Goal region, initial and target angles same as in Fig. 3.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h3>V-B Experiments on Hardware with Baxter</h3>
<p>We implemented the proposed approach on a Baxter robot, using one of its 7 DOF arms. We selected slightly deformable fingertips for the gripper to be able to control the force applied on the tool by changing the distance between the fingers. As for many commercial robots, Baxter's gripper cannot be controlled at the same high frequency as the joints. In particular, we measured that the gripper's fingers were able to reach a desired distance only after allowing a delay of 80ms. This delay entails longer time needed for executing each control action on the hardware, hence longer times needed by the tool to reach the goal due to these limitations of the robot.</p>
<p>To estimate the angle $\phi_{tl}$ we used a color based segmentation to track a colored marker on the tool. The images were collected by a Kinect 2 RGB-D camera running at 30 fps.</p>
<p>During the experiments, we kept the gripper and the actuated joint in a horizontal position, such that the gravitational acceleration would be $g=0$ and the motion of the tool was only determined by the inertial forces generated by the commanded actions. The distance between the actuated joint and the pivoting point was $l=0.35$ m.</p>
<p>We performed experiments with two different objects of different materials, hence different friction properties. The parameters of the tools are shown in Table I.</p>
<p>TABLE I: Parameters of the tools.</p>
<table>
<thead>
<tr>
<th></th>
<th>$I[\mathrm{kg} \cdot \mathrm{m}^{2}]$</th>
<th>$m[\mathrm{kg}]$</th>
<th>$r[\mathrm{m}]$</th>
<th>$d_{0}[\mathrm{m}]$</th>
</tr>
</thead>
<tbody>
<tr>
<td>tool 1</td>
<td>0.00006943</td>
<td>0.026</td>
<td>0.089</td>
<td>0.0188</td>
</tr>
<tr>
<td>tool 2</td>
<td>0.0001111</td>
<td>0.033</td>
<td>0.1</td>
<td>0.0162</td>
</tr>
</tbody>
</table>
<p>The robot is able to estimate the difference in the finger's distance when closing them to grasp the tool and therefore it can estimate the difference in the $d_{0}$ value also for an unknown object. The friction coefficients used for modeling the tool in the simulator have been estimated only for the first tool. Hence the policy was not explicitly trained using the parameters matching those of the second tool.</p>
<p>Table II summarizes the results of our experiments on the robot. To streamline the experiments, we cycled through the following target angles: starting from $0^{\circ}$, reach $45^{\circ}$, then $0^{\circ}$, then $-60^{\circ}$, then $30^{\circ}$, then $5^{\circ}$ and finally $0^{\circ}$. This allowed us to test the policy on both wide and narrow ranges of motion. With this we obtained a $\approx 93\%$ success rate with tool 1 and $\approx 83\%$ with tool 2. As expected, the policy performs better with the tool whose parameters were used (in a noisy manner) during training in simulation. However, it is important to note that the performance using the tool not seen during training was robust as well. The experiments on the robot were performed without re-adjusting the tool. As a consequence, we observed that eventual sliding of the tool would cause drops after several rounds of experiments. However, these were still very infrequent.</p>
<p>TABLE II: Results of hardware experiments on Baxter.</p>
<table>
<thead>
<tr>
<th></th>
<th>tool 1</th>
<th>tool 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Goal reached</td>
<td>28</td>
<td>25</td>
</tr>
<tr>
<td>Goal not reached</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>Tool dropped</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Total</td>
<td>30</td>
<td>30</td>
</tr>
</tbody>
</table>
<p>Fig. 5 below illustrates two example episodes run on Baxter with the first and second tool. We observe that the target is reached faster when tool 1 is used (after $\approx 5s$), and a bit slower when tool 2 is used (after $\approx 10s$).</p>
<p>Fig. 5: Two example trajectories from experiments on Baxter robot: reaching the target of $-60^{\circ}$ ( $\pm 3^{\circ}$ for goal region) from $0^{\circ}$. Left: using tool 1 whose parameters are used in the simulator. Right: using tool 2 whose parameters are not used for training.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6 below illustrates the performance averaged over all the trials. The deviation from the goal region after reaching the goal is likely due to inaccuracies in tracking. After reporting that the tool is in the goal region, the tracker might later report a corrected estimate, indicating further tool adjustment is needed. We observe that in such cases the policy still succeeds in further pivoting the tool to the target angle.</p>
<p>Fig. 6: Mean distance to target vs time for experiments on Baxter averaged over all trials (excluding drops, since tool angle is not tracked after a drop). Left: using tool 1. Right: using tool 2.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h2>VII. CONCLUSIONS AND FUTURE WORK</h2>
<p>In this work we proposed solving the pivoting task via building a simple custom simulator of the task dynamics and using reinforcement learning to learn control policies. We presented the learning procedure that is able to manage the mismatch between the simulated and real settings, hence does not require precise estimates of all the tool parameters. We then demonstrated the ability of the learned policy to solve the pivoting task on the Baxter robot.</p>
<p>To extend our work to a more general set of dextrous manipulation problems, in the future we can combine pivoting with sliding. This would enable more possibilities for repositioning. In fact, with pivoting we can change the orientation of the object around a single axis, while with sliding we can translate the object, hence changing the contact point. The next step would be to incorporate existing approaches to modeling sliding into the simulator.</p>
<p>We can also explore augmenting the learning part of our approach. In situations where further training on the hardware is possible, we could develop a second stage to finetune the policies learned in simulation. This could be accomplished, for example, by quickly learning an adjustment to the discrepancy between the simulated and real environment without the need to re-learn control policies from scratch. Another direction is to experiment with recurrent networks to model aspects of the environment not observable directly.</p>
<p>Further into the future we can consider the very challenging problem of manipulating non-rigid objects. Building high fidelity simulators for deformable objects and objects with variable center of mass has been mostly intractable in the past. However, approximate simulators can be constructed. We have observed that for rigid objects only approximate simulation of the dynamics is sufficient to learn effective control policies. So we can explore whether our approach with learning from approximate simulation can be adapted to the case of manipulating deformable objects.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>This work was supported by the European Union framework program H2020-645403 RobDREAM.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] N. C. Dafle, A. Rodriguez, R. Paolini, B. Tang, S. S. Srinivasa, M. Erdmann, M. T. Mason, I. Lundberg, H. Staab, and T. Fuhlbrigge, "Extrinsic dexterity: In-hand manipulation with external forces," in <em>2014 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE, 2014, pp. 1578–1585.</li>
<li>[2] P. Tournassoud, T. Lozano-Perez, and E. Mazer, "Regrasping," in <em>Robotics and Automation. Proceedings. 1987 IEEE International Conference on</em>, vol. 4, Mar 1987, pp. 1924–1928.</li>
<li>[3] J. C. Trinkle and J. J. Hunter, "A framework for planning dexterous manipulation," in <em>Robotics and Automation, 1991. Proceedings., 1991 IEEE International Conference on</em>, Apr 1991, pp. 1245–1251 vol.2.</li>
<li>[4] N. Furukawa, A. Namiki, S. Taku, and M. Ishikawa, "Dynamic regrasping using a high-speed multilingered hand and a high-speed vision system," in <em>Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.</em>, May 2006, pp. 181–187.</li>
<li>[5] N. Chavan-Dafle, M. T. Mason, H. Staab, G. Rossano, and A. Rodriguez, "A two-phase gripper to reorient and grasp," in <em>2015 IEEE International Conference on Automation Science and Engineering (CASE)</em>, Aug 2015, pp. 1249–1255.</li>
<li>[6] A. Holladay, R. Paolini, and M. T. Mason, "A general framework for open-loop pivoting," in <em>2015 IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2015, pp. 3675–3681.</li>
<li>[7] A. Sintov and A. Shapiro, "Swing-up regrasping algorithm using energy control," in <em>2016 IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2016, pp. 4888–4893.</li>
<li>[8] F. E. Viña, Y. Karayiannidis, K. Pauwels, C. Smith, and D. Kragic, "In-hand manipulation using gravity and controlled slip," in <em>Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</em>, Sept 2015, pp. 5636–5641.</li>
<li>[9] F. E. Viña, Y. Karayiannidis, C. Smith, and D. Kragic, "Adaptive control for pivoting with visual and tactile feedback," in <em>2016 IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2016, pp. 399–406.</li>
<li>[10] C. G. Atkeson, A. W. Moore, and S. Schaal, "Locally weighted learning for control," in <em>Lazy learning</em>. Springer, 1997, pp. 75–113.</li>
<li>[11] S. Levine and P. Abbeel, "Learning neural network policies with guided policy search under unknown dynamics," in <em>Advances in Neural Information Processing Systems</em>, 2014, pp. 1071–1079.</li>
<li>[12] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, "Trust region policy optimization," <em>CoRR, abs/1502.05477</em>, 2015.</li>
<li>[13] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement learning," <em>arXiv preprint arXiv:1509.02971</em>, 2015.</li>
<li>[14] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, "Benchmarking deep reinforcement learning for continuous control," <em>arXiv preprint arXiv:1604.06778</em>, 2016.</li>
<li>[15] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell, "Sim-to-real robot learning from pixels with progressive nets," <em>arXiv preprint arXiv:1610.04286</em>, 2016.</li>
<li>[16] F. E. Viña, Y. Bekiroglu, C. Smith, Y. Karayiannidis, and D. Kragic, "Predicting slippage and learning manipulation affordances through gaussian process regression," in <em>IEEE-RAS International Conference on Humanoid Robots</em>, Oct 2013, pp. 462–468.</li>
<li>[17] H. Olsson, K. J. strm, M. Gfvert, C. C. D. Wit, and P. Lischinsky, "Friction models and friction compensation," <em>Eur. J. Control</em>, p. 176, 1998.</li>
<li>[18] D. Karnopp, "Computer simulation of stick-slip friction in mechanical dynamic systems." <em>J. Dyn. Syst. Meas. Control.</em>, vol. 107, no. 1, pp. 100–103, 1985.</li>
<li>[19] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, "Openai gym," 2016.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Rika Antonova, Silvia Cruciani, Christian Smith and Danica Kragic are with the Robotics, Perception and Learning Lab, CSC at KTH Royal Institute of Technology, Stockholm, Sweden. {antonova, cruciani, ccs, dani}@kth.se
${ }^{1}$ Both of these authors contributed equally.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
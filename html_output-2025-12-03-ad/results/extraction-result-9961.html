<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9961 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9961</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9961</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-ee1ef7b70dc34adcc90c42cc28168165ea56501f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ee1ef7b70dc34adcc90c42cc28168165ea56501f" target="_blank">SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work revisits the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level).</p>
                <p><strong>Paper Abstract:</strong> In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9961.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9961.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SummaC (NLI-based judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SummaC: NLI-based inconsistency detector (SummaC_ZS and SummaC_Conv)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated inconsistency detector that uses a pre-trained NLI model applied to sentence-level document-summary pairs to produce entailment/contradiction/neutral probabilities, then aggregates these via either a zero-shot max-mean rule (SummaC_ZS) or a learned histogram+1D-convolution (SummaC_Conv) to produce a summary-level consistency score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Summarization — factual consistency / inconsistency detection</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Transformer-based NLI models (e.g., BERT/Large or RoBERTa trained on MNLI + VitaminC; paper uses BERT Large+MNLI+VitaminC / roberta-large-mnli variants as the default NLI judge)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>For each (document, summary) pair the document and summary are split into sentence blocks; every document-sentence / summary-sentence pair is scored by an NLI model to produce probabilities for Entailment/Contradiction/Neutral (the NLI Pair Matrix). SummaC_ZS aggregates by taking column-wise max then mean; SummaC_Conv bins each column into a histogram (H=50) and passes it through a trained 1-D convolution to produce per-sentence scores which are averaged. Thresholds for binary classification are tuned per dataset on validation splits.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations from multiple datasets are used as ground truth: crowd-workers or authors/experts annotate summaries with consistency labels (datasets vary: CoGenSumm, XSumFaith, Polytope, FactCC, SummEval, FRANK). Annotation protocols differ per dataset (e.g., SummEval: 3 annotators, 5-point Likert; FRANK: 3 crowd annotators with majority rule; FactCC: annotations by paper authors because crowdsourcing yielded low agreement). The paper standardizes these labels to binary consistent/inconsistent for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Model vs human ground truth is evaluated using balanced accuracy (primary) and ROC-AUC (secondary). Reported values: SummaC_Conv overall balanced accuracy = 74.4% (SummaC_ZS = 72.1%); prior best non-NLI method (QuestEval) = 69.4%. ROC-AUC: SummaC_Conv = 77.8, SummaC_ZS = 74.3. The paper also cites prior out-of-the-box NLI results (~52% accuracy reported by Kryscinski et al., 2020). Inter-annotator agreement (human) is reported per dataset with Fleiss' kappa (Table 1): CoGenSumm 0.65, XSumFaith 0.80, SummEval 0.70, FRANK 0.53; Polytope and FactCC lacked multi-annotator IAA.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When replacing human judges with automated NLI-based judges, the paper identifies several degradations: (1) sensitivity to input granularity — treating the full document as the premise can falsely signal entailment, losing the ability to localize sentence-level contradictions; (2) loss of nuanced judgment and domain/contextual reasoning (e.g., distinguishing intrinsic vs extrinsic hallucinations that require external world knowledge); (3) degraded interpretability for learned aggregators (SummaC_Conv is less traceable to a single supporting sentence than SummaC_ZS); (4) susceptibility to noise/outliers and model miscalibration (extrema-driven decisions can be unreliable); (5) potential lack of generalization when trained on synthetic data (synthetic-to-real transfer issues); and (6) inability to substitute for the consensus that multiple human annotators provide (datasets sometimes need many annotators to reach reliability).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Explicit examples and empirical observations from the paper: (a) Figure 1: an NLI model given the entire document as premise predicted high entailment probability (0.91) for an inconsistent summary — but when run on sentence-level pairs the inconsistent summary sentence (S3) had no supporting document sentence, correctly revealing inconsistency; (b) prior work (Falke et al., 2019) found that re-ranking with an NLI model increased consistency errors; (c) Kryscinski et al. (2020) reported out-of-the-box NLI models obtaining only ~52% accuracy on inconsistency detection, near random; (d) SummaC_ZS is sensitive to extrema (single high entailment scores) and thus can be noisy without distributional aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Caveats and positive cases reported: (1) with the correct setup (sentence-level granularity and appropriate aggregation) NLI-based judges can strongly improve and outperform prior automated metrics — SummaC_Conv achieves statistically significant gains over prior work on several datasets; (2) including contradiction probabilities and more robust NLI training datasets (MNLI + VitaminC) improves performance; (3) automated judges offer reproducibility and throughput advantages (though throughput varies — SummaC processes ~430 docs/min on the test hardware); (4) in datasets with low human agreement or where expert labeling is unavailable, a robust automated judge can be a practical alternative; (5) some automated methods trained on synthetic data (FactCC-CLS) perform well on the dataset they were trained for (FactCC) but generalize less well elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>See Figure 1 and Sections 1 (Introduction), 3 (SummaC Models, esp. 3.1–3.3), 4 (Benchmark descriptions), 5 (Results and 5.3 Granularity / 5.3.2 NLI category), Table 1 and Table 2 for quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9961.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9961.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation (crowd & expert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human annotations used as gold labels (crowd-workers and expert annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human judgments (crowd or expert annotators) are used as the reference labels for inconsistency detection datasets; the paper documents varying annotation protocols and inter-annotator agreement, and highlights limitations of crowd annotation in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Summarization — dataset annotation / human evaluation of factual consistency</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Datasets used as ground truth vary: CoGenSumm (3 annotators historically; reported IAA 0.65), XSumFaith (annotator protocol with IAA 0.80), Polytope (single annotator per summary), FactCC (annotations by paper authors/experts because crowdsourcing produced low IAA), SummEval (3 annotators rating consistency on a 5-point Likert; labeled consistent only if all gave 5), FRANK (3 crowd annotators; majority rule). For standardization the authors convert diverse labels/typologies to binary consistent/inconsistent and use majority or author-based rules per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Inter-annotator agreement reported with Fleiss' Kappa where available: CoGenSumm 0.65, XSumFaith 0.80, SummEval 0.70, FRANK 0.53; Polytope and FactCC lack multi-annotator IAA. The paper uses these human labels as the reference for computing model balanced accuracy and ROC-AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>The paper implies what is lost when automated judges replace humans: (1) domain expertise and nuanced decisions that experts supply (FactCC authors chose expert labels because crowd IAA was low); (2) reliability from multiple independent annotators — several datasets require many annotators to reach stable labels (the paper cites work suggesting up to 8 annotators may be needed); (3) the ability to use external/world knowledge to judge whether an extrinsic hallucination is accurate versus inconsistent; and (4) human interpretive judgments and typology labeling (fine-grained error types) that are hard to capture with a single automated score.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete mentions in the paper: (a) FactCC authors annotated with experts because crowd-sourcing yielded low inter-annotator agreement (Section 4.2); (b) the paper cites Falke et al. (2019) and Gillick & Liu (2010) noting divergence between expert and non-expert annotations and that non-expert evaluation can be risky; (c) XSumFaith distinguishes extrinsic hallucinations that require external knowledge — a nuance automated consistency checks may not capture.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Human evaluation itself can be noisy and costly: some datasets have low IAA (e.g., FRANK 0.53), some labels come from single annotators (Polytope, FactCC training), and annotator protocols differ—meaning automated judges (when properly designed, e.g., SummaC with sentence-level NLI and trained aggregation) can offer consistent, reproducible evaluation and competitive performance vs human labels. The paper also shows automated methods can outperform prior automated baselines and approach human-labeled standards when configured correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>See Section 2.2 (Datasets for Inconsistency Detection), Section 4.1–4.2 (Benchmark Standardization and Dataset descriptions), Table 1 (dataset statistics and inter-annotator agreement), and citations in Section 2 (e.g., Falke et al., 2019; Gillick and Liu, 2010).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ranking generated summaries by correctness: An interesting but challenging application for natural language inference <em>(Rating: 2)</em></li>
                <li>Evaluating the factual consistency of abstractive text summarization <em>(Rating: 2)</em></li>
                <li>On faithfulness and factuality in abstractive summarization <em>(Rating: 2)</em></li>
                <li>Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics <em>(Rating: 2)</em></li>
                <li>Questeval: Summarization asks for fact-based evaluation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9961",
    "paper_id": "paper-ee1ef7b70dc34adcc90c42cc28168165ea56501f",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "SummaC (NLI-based judge)",
            "name_full": "SummaC: NLI-based inconsistency detector (SummaC_ZS and SummaC_Conv)",
            "brief_description": "An automated inconsistency detector that uses a pre-trained NLI model applied to sentence-level document-summary pairs to produce entailment/contradiction/neutral probabilities, then aggregates these via either a zero-shot max-mean rule (SummaC_ZS) or a learned histogram+1D-convolution (SummaC_Conv) to produce a summary-level consistency score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Summarization — factual consistency / inconsistency detection",
            "llm_judge_model": "Transformer-based NLI models (e.g., BERT/Large or RoBERTa trained on MNLI + VitaminC; paper uses BERT Large+MNLI+VitaminC / roberta-large-mnli variants as the default NLI judge)",
            "llm_judge_setup": "For each (document, summary) pair the document and summary are split into sentence blocks; every document-sentence / summary-sentence pair is scored by an NLI model to produce probabilities for Entailment/Contradiction/Neutral (the NLI Pair Matrix). SummaC_ZS aggregates by taking column-wise max then mean; SummaC_Conv bins each column into a histogram (H=50) and passes it through a trained 1-D convolution to produce per-sentence scores which are averaged. Thresholds for binary classification are tuned per dataset on validation splits.",
            "human_evaluation_setup": "Human annotations from multiple datasets are used as ground truth: crowd-workers or authors/experts annotate summaries with consistency labels (datasets vary: CoGenSumm, XSumFaith, Polytope, FactCC, SummEval, FRANK). Annotation protocols differ per dataset (e.g., SummEval: 3 annotators, 5-point Likert; FRANK: 3 crowd annotators with majority rule; FactCC: annotations by paper authors because crowdsourcing yielded low agreement). The paper standardizes these labels to binary consistent/inconsistent for evaluation.",
            "agreement_metric": "Model vs human ground truth is evaluated using balanced accuracy (primary) and ROC-AUC (secondary). Reported values: SummaC_Conv overall balanced accuracy = 74.4% (SummaC_ZS = 72.1%); prior best non-NLI method (QuestEval) = 69.4%. ROC-AUC: SummaC_Conv = 77.8, SummaC_ZS = 74.3. The paper also cites prior out-of-the-box NLI results (~52% accuracy reported by Kryscinski et al., 2020). Inter-annotator agreement (human) is reported per dataset with Fleiss' kappa (Table 1): CoGenSumm 0.65, XSumFaith 0.80, SummEval 0.70, FRANK 0.53; Polytope and FactCC lacked multi-annotator IAA.",
            "losses_identified": "When replacing human judges with automated NLI-based judges, the paper identifies several degradations: (1) sensitivity to input granularity — treating the full document as the premise can falsely signal entailment, losing the ability to localize sentence-level contradictions; (2) loss of nuanced judgment and domain/contextual reasoning (e.g., distinguishing intrinsic vs extrinsic hallucinations that require external world knowledge); (3) degraded interpretability for learned aggregators (SummaC_Conv is less traceable to a single supporting sentence than SummaC_ZS); (4) susceptibility to noise/outliers and model miscalibration (extrema-driven decisions can be unreliable); (5) potential lack of generalization when trained on synthetic data (synthetic-to-real transfer issues); and (6) inability to substitute for the consensus that multiple human annotators provide (datasets sometimes need many annotators to reach reliability).",
            "examples_of_loss": "Explicit examples and empirical observations from the paper: (a) Figure 1: an NLI model given the entire document as premise predicted high entailment probability (0.91) for an inconsistent summary — but when run on sentence-level pairs the inconsistent summary sentence (S3) had no supporting document sentence, correctly revealing inconsistency; (b) prior work (Falke et al., 2019) found that re-ranking with an NLI model increased consistency errors; (c) Kryscinski et al. (2020) reported out-of-the-box NLI models obtaining only ~52% accuracy on inconsistency detection, near random; (d) SummaC_ZS is sensitive to extrema (single high entailment scores) and thus can be noisy without distributional aggregation.",
            "counterexamples_or_caveats": "Caveats and positive cases reported: (1) with the correct setup (sentence-level granularity and appropriate aggregation) NLI-based judges can strongly improve and outperform prior automated metrics — SummaC_Conv achieves statistically significant gains over prior work on several datasets; (2) including contradiction probabilities and more robust NLI training datasets (MNLI + VitaminC) improves performance; (3) automated judges offer reproducibility and throughput advantages (though throughput varies — SummaC processes ~430 docs/min on the test hardware); (4) in datasets with low human agreement or where expert labeling is unavailable, a robust automated judge can be a practical alternative; (5) some automated methods trained on synthetic data (FactCC-CLS) perform well on the dataset they were trained for (FactCC) but generalize less well elsewhere.",
            "paper_reference": "See Figure 1 and Sections 1 (Introduction), 3 (SummaC Models, esp. 3.1–3.3), 4 (Benchmark descriptions), 5 (Results and 5.3 Granularity / 5.3.2 NLI category), Table 1 and Table 2 for quantitative comparisons.",
            "uuid": "e9961.0",
            "source_info": {
                "paper_title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Human evaluation (crowd & expert)",
            "name_full": "Human annotations used as gold labels (crowd-workers and expert annotators)",
            "brief_description": "Human judgments (crowd or expert annotators) are used as the reference labels for inconsistency detection datasets; the paper documents varying annotation protocols and inter-annotator agreement, and highlights limitations of crowd annotation in this task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Summarization — dataset annotation / human evaluation of factual consistency",
            "llm_judge_model": null,
            "llm_judge_setup": null,
            "human_evaluation_setup": "Datasets used as ground truth vary: CoGenSumm (3 annotators historically; reported IAA 0.65), XSumFaith (annotator protocol with IAA 0.80), Polytope (single annotator per summary), FactCC (annotations by paper authors/experts because crowdsourcing produced low IAA), SummEval (3 annotators rating consistency on a 5-point Likert; labeled consistent only if all gave 5), FRANK (3 crowd annotators; majority rule). For standardization the authors convert diverse labels/typologies to binary consistent/inconsistent and use majority or author-based rules per dataset.",
            "agreement_metric": "Inter-annotator agreement reported with Fleiss' Kappa where available: CoGenSumm 0.65, XSumFaith 0.80, SummEval 0.70, FRANK 0.53; Polytope and FactCC lack multi-annotator IAA. The paper uses these human labels as the reference for computing model balanced accuracy and ROC-AUC.",
            "losses_identified": "The paper implies what is lost when automated judges replace humans: (1) domain expertise and nuanced decisions that experts supply (FactCC authors chose expert labels because crowd IAA was low); (2) reliability from multiple independent annotators — several datasets require many annotators to reach stable labels (the paper cites work suggesting up to 8 annotators may be needed); (3) the ability to use external/world knowledge to judge whether an extrinsic hallucination is accurate versus inconsistent; and (4) human interpretive judgments and typology labeling (fine-grained error types) that are hard to capture with a single automated score.",
            "examples_of_loss": "Concrete mentions in the paper: (a) FactCC authors annotated with experts because crowd-sourcing yielded low inter-annotator agreement (Section 4.2); (b) the paper cites Falke et al. (2019) and Gillick & Liu (2010) noting divergence between expert and non-expert annotations and that non-expert evaluation can be risky; (c) XSumFaith distinguishes extrinsic hallucinations that require external knowledge — a nuance automated consistency checks may not capture.",
            "counterexamples_or_caveats": "Human evaluation itself can be noisy and costly: some datasets have low IAA (e.g., FRANK 0.53), some labels come from single annotators (Polytope, FactCC training), and annotator protocols differ—meaning automated judges (when properly designed, e.g., SummaC with sentence-level NLI and trained aggregation) can offer consistent, reproducible evaluation and competitive performance vs human labels. The paper also shows automated methods can outperform prior automated baselines and approach human-labeled standards when configured correctly.",
            "paper_reference": "See Section 2.2 (Datasets for Inconsistency Detection), Section 4.1–4.2 (Benchmark Standardization and Dataset descriptions), Table 1 (dataset statistics and inter-annotator agreement), and citations in Section 2 (e.g., Falke et al., 2019; Gillick and Liu, 2010).",
            "uuid": "e9961.1",
            "source_info": {
                "paper_title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization",
                "publication_date_yy_mm": "2021-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Evaluating the factual consistency of abstractive text summarization",
            "rating": 2
        },
        {
            "paper_title": "On faithfulness and factuality in abstractive summarization",
            "rating": 2
        },
        {
            "paper_title": "Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics",
            "rating": 2
        },
        {
            "paper_title": "Questeval: Summarization asks for fact-based evaluation",
            "rating": 2
        }
    ],
    "cost": 0.01563775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization</h1>
<p>Philippe Laban Tobias Schnabel Paul N. Bennett Marti A. Hearst<br>UC Berkeley, USA Microsoft, USA<br>Microsoft, USA UC Berkeley, USA*</p>
<h4>Abstract</h4>
<p>In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaC $<em _Conv="{Conv" _text="\text">{\text {Conv }}$ that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaC $</em>$ obtains state-of-the-art results with a balanced accuracy of $74.4 \%$, a $5 \%$ improvement compared with prior work.}</p>
<h2>1 Introduction</h2>
<p>Recent progress in text summarization has been remarkable, with ROUGE record-setting models published every few months, and human evaluations indicating that automatically generated summaries are matching human-written summaries in terms of fluency and informativeness (Zhang et al., 2020a).</p>
<p>A major limitation of current summarization models is their inability to remain factually consistent with the respective input document. Summary inconsistencies are diverse-from inversions (i.e., negation) to incorrect use of an entity (i.e., subject, object swapping), or hallucinations (i.e., introduction of entity not in the original document). Recent studies have shown that in some scenarios,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>even state-of-the-art pre-trained language models can generate inconsistent summaries in more than $70 \%$ of all cases (Pagnoni et al., 2021). This has led to accelerated research around summary inconsistency detection.</p>
<p>A closely related task to inconsistency detection is textual entailment, also referred to as Natural Language Inference (NLI), in which a hypothesis sentence must be classified as either entailed by, neutral, or contradicting a premise sentence. Enabled by the crowd-sourcing of large NLI datasets such as SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018), modern architectures have achieved close to human performance at the task.</p>
<p>The similarity of NLI to inconsistency detection, as well as the availability of high-performing NLI models, led to early attempts at using NLI to detect consistency errors in summaries. These early attempts were unsuccessful, finding that re-ranking summaries according to an NLI model can lead to an increase in consistency errors (Falke et al., 2019), or that out-of-the-box NLI models obtain $52 \%$ accuracy at the binary classification task of inconsistency detection, only slightly above random guessing (Kryscinski et al., 2020).</p>
<p>In this work, we revisit this approach, showing that NLI models can in fact successfully be used for inconsistency detection, as long as they are used at the appropriate granularity. Figure 1 shows how crucial using the correct granularity as input to NLI models is. An inconsistency checker should flag the last sentence in the summary (shown right) as problematic. When treating the entire document as the premise and the summary as the hypothesis, a competitive NLI model predicts with probability of 0.91 that the summary is entailed by the document. However, when splitting the documents into sentence premise-hypothesis pairs (visualized as edges in Figure 1) the NLI model correctly determines that $S_{3}$ is not supported by any document sentence. This illustrates that working with sentence pairs is crucial for making NLI models work for inconsistency detection.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example document with an inconsistent summary. When running each sentence pair $\left(D_{i}, S_{j}\right)$ through an NLI model, $S_{3}$ is not entailed by any document sentence. However, when running the entire (document, summary) at once, the NLI model incorrectly predicts that the document highly entails the entire summary.</p>
<p>Our contributions are two-fold. First, we introduce a new approach for inconsistency detection based on the aggregation of sentence-level entailment scores for each pair of input document and summary sentences. We present two model variants that differ in the way they aggregate sentencelevel scores into a single score. SummaC $<em _Conv="{Conv" _text="\text">{\text {ZS }}$ performs zero-shot aggregation by combining sentence-level scores using max and mean operators. SummaC $</em>$ is a trained model consisting of a single learned convolution layer compiling the distribution of entailment scores of all document sentences into a single score.}</p>
<p>Second, to evaluate our approach, we introduce the SummaC Benchmark by standardizing existing datasets. Because the benchmark contains the six largest summary consistency datasets, it is more comprehensive and includes a broader range of inconsistency errors than prior work.</p>
<p>The SummaC models outperform existing inconsistency detection models on the benchmark, with the SummaC $_{\text {Conv }}$ obtaining an overall balanced accuracy of $74.4 \%, 5 \%$ above prior work. We publicly release the models and datasets. ${ }^{1}$</p>
<h2>2 Related Work</h2>
<p>We briefly survey existing methods and datasets for fact checking, inconsistency detection, and inconsistency correction.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.1 Fact Checking and Verification</h3>
<p>Fact checking is a related task in which a model receives an input claim along with a corpus of ground truth information. The model must then retrieve relevant evidence and decide whether the claim is supported, refuted, or if there is not enough information in the corpus (Thorne et al., 2018). The major difference to our task lies in the different semantics of consistency and accuracy. If a summary adds novel and accurate information not present in the original document (e.g., adding background information), the summary is accurate but inconsistent. In the summary inconsistency detection domain, the focus is on detecting any inconsistency, regardless of its accuracy, as prior work has shown that current automatic summarizers are predominantly inaccurate when inconsistent (Maynez et al., 2020).</p>
<h3>2.2 Datasets for Inconsistency Detection</h3>
<p>Several datasets have been annotated to evaluate model performance in inconsistency detection, typically comprising up to two thousand annotated summaries. Datasets are most commonly crowdannotated with three judgements each, despite some work showing that as many as eight annotators are required to achieve high inter-annotator agreement (Falke et al., 2019).</p>
<p>Reading the entire original document being summarized is time-consuming, and to amortize this cost, consistency datasets often contain multiple summaries, generated by different models, for the same original document.</p>
<p>Some datasets consist of an overall consistency label for a summary (e.g., FactCC [Kryscinski et al., 2020]), while others propose a finer-grained typology with up to 8 types of consistency errors (Huang et al., 2020).</p>
<p>We include the six largest summary consistency datasets in the SummaC Benchmark, and describe them more in detail in Section 4.</p>
<h3>2.3 Methods for Inconsistency Detection</h3>
<p>Due to data limitations, most inconsistency detection methods adapt NLP pipelines from other tasks including QAG models, synthetic classifiers, and parsing-based methods.</p>
<p>QAG methods follow three steps: (1) question generation (QG), (2) question answering (QA) with the document and the summary, (3) matching document and summary answers. A summary is</p>
<p>considered consistent if few or no questions have differing answer with the document. A key design choice for these methods lies in the source for question generation. Durmus et al. (2020) generate questions using the summary as a source, making their FEQA method precision-oriented. Scialom et al. (2019) generate questions with the document as a source, creating a recall-focused measure. Scialom et al. (2021) unite both in QuestEval, by generating two sets of questions, sourced from the summary and document respectively. We include FEQA and QuestEval in our benchmark results.</p>
<p>Synthetic classifiers rely on large, synthetic datasets of summaries with inconsistencies, and use those to train a classifier with the expectation that the model generalizes to non-synthetic summaries. To generate a synthetic dataset, Kryscinski et al. (2020) propose a set of semantically invariant (e.g., paraphrasing) and variant (e.g., sentence negation) text transformations that they apply to a large summarization dataset. FactCC-CLS, the classifier obtained when training on the synthetic dataset, is included in our benchmark results for comparison.</p>
<p>Parsing-based methods generate relations through parsing and compute the fraction of summary relations that are compatible with docu ment relations as a precision measure of summary factuality. Goodrich et al. (2019) extract (subject, relation, object) tuples most commonly using OpenIE (Etzioni et al., 2008). In the recent DAE model, Goyal and Durrett (2020) propose to use arc labels from a dependency parser instead of relation triplet. We include the DAE model in our benchmark results.</p>
<h3>2.4 Methods for Consistency Correction</h3>
<p>Complementary to inconsistency detection, some work focused on the task of mitigating inconsistency errors during summarization. Approaches fall in two categories: Reinforcement Learning (RL) methods to improve models and stand-alone re-writing methods.</p>
<p>RL methods often rely on an out-of-the-box inconsistency detection model and use reinforcement learning to optimize a reward with a consistency component. Arumae and Liu (2019) optimize a QA-based consistency reward, and Nan et al. (2021) streamline a QAG reward by combining the QG and QA model, making it more efficient for RL training. Pasunuru and Bansal
(2018) leverage an NLI-based component as part of an overall ROUGE-based reward, and Zhang et al. (2020b) use a parsing-based measure in the domain of medical report summarization.</p>
<p>Re-writing methods typically operate as a modular component that is applied after an existing summarization model. Cao et al. (2020) use a synthetic dataset of rule-corrupted summaries to train a post-corrector model, but find that this model does not transfer well to real summarizer errors. Dong et al. (2020) propose to use a QAG model to find erroneous spans, which are then corrected using a post-processing model.</p>
<p>Since all methods discussed above for consistency correction rely on a model to detect inconsistencies, they will naturally benefit from more accurate inconsistency detectors.</p>
<h2>3 SummaC Models</h2>
<p>We now introduce our SummaC models for inconsistency detection. The first step common to all models is to apply an out-of-the-box NLI model to generate an NLI Pair Matrix for a (document, summary) pair. The two models we present then differ in the way they process this pair matrix to produce a single consistency score for a given summary. We also describe the SummaC evaluation benchmark, a set of inconsistency detection datasets, in Section 4. In Section 5, we measure the performance of the SummaC models on this benchmark and investigate components of the models, including which NLI model achieves highest performance, which NLI categories should be used, and what textual granularity is most effective.</p>
<h3>3.1 Generating the NLI Pair Matrix</h3>
<p>NLI datasets are predominantly represented at the sentence level. In our pilot experiments, we found that this causes the resulting NLI models to fail in assessing consistency for documents with 50 sentences and more.</p>
<p>This motivates the following approach. We generate an NLI Pair Matrix by splitting a (document, summary) pair into sentence blocks. The document is split into $M$ blocks, each considered a premise labeled from $D_{1}, \ldots, D_{M}$, and the summary is split into $N$ blocks, each considered a hypothesis labeled from $S_{1}, \ldots, S_{N}$.</p>
<p>Each $D_{i}, S_{j}$ combination is run through the NLI model, which produces a probability distribution over the three NLI categories $\left(E_{i j}, C_{i j}, N_{i j}\right)$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Diagram of the SummaC ${ }<em _Conv="{Conv" _text="\text">{\text {ZS }}$ (top) and SummaC $</em>}}$ (bottom) models. Both models utilize the same NLI Pair Matrix (middle) but differ in their processing to obtain a score. The SummaC ${ <em _Conv="{Conv" _text="\text">{\mathrm{ZS}}$ is Zero-Shot, and does not have trained parameters. SummaC $</em>$ uses a convolutional layer trained on a binned version of the NLI Pair Matrix.
for entailment, contradiction, and neutral, respectively. If not specified otherwise, the pair matrix is an $M \times N$ matrix consisting of the entailment scores $E_{i j}$. In Section 5.3.3, we examine the effect of granularity by splitting texts at the paragraph level or binning two sentences at a time. In Section 5.3.2, we explore the use of the contradiction and neutral categories in our experiments.}</p>
<p>The example in Figure 1 has $M=4$ document sentences, and $N=3$ summary sentences, and the corresponding NLI Pair Matrix is the following:</p>
<p>$$
X_{\text {pair }}=\left[\begin{array}{lll}
0.02 &amp; 0.02 &amp; 0.04 \
0.98 &amp; 0.00 &amp; 0.00 \
0.43 &amp; 0.99 &amp; 0.00 \
0.00 &amp; 0.00 &amp; 0.01
\end{array}\right]
$$</p>
<p>The pair matrix can be interpreted as the weights of a bipartite graph, which is also illustrated in Figure 1 where the opacity of each edge $(i, j)$ represents the entailment probability $E_{i j}$.</p>
<p>The two SummaC models take as input the same NLI Pair Matrix, but differ in the aggregation method to transform the pair matrix into a score. Figure 2 presents an overview of SummaC ${ }<em _Conv="{Conv" _text="\text">{\mathrm{ZS}}$ and SummaC $</em>$.}</p>
<h3>3.2 SummaC ${ }_{\text {ZS }}$ : Zero-Shot</h3>
<p>In the SummaC ${ }<em _pair="{pair" _text="\text">{\text {ZS }}$ model, we reduce the pair matrix to a one-dimensional vector by taking the maximum (max) value of each column. On an intuitive level, for each summary sentence, this
step consists of retaining the score for the document sentence that provides the strongest support for each summary sentence. For the example in Figure 1:
$\max \left(X</em>\right]$
The second step consists of taking the mean of the produced vector, reducing the vector to a scalar which is used as the final model score. At a high level, this step aggregates sentence-level information into a single score for the entire summary. For example, in Figure 1, the score produced by SummaC ${ }_{\mathrm{ZS}}$ would be 0.67 . If we removed the third sentence from the summary, the score would increase to 0.985 . We experiment with replacing the max and mean operators with other operators in Appendix B.}}, \text { axis='col' }\right)=\left[\begin{array}{lll}0.98 &amp; 0.99 &amp; 0.04\end{array</p>
<h3>3.3 SummaC $_{\text {Conv }}$ : Convolution</h3>
<p>One limitation of SummaC ${ }<em _Conv="{Conv" _text="\text">{\text {ZS }}$ is that it is highly sensitive to extrema, which can be noisy due to the presence of outliers and the imperfect nature of NLI models. In SummaC ${ }</em>$, we reduce the reliance on extrema values by instead taking into account the entire distribution of entailment scores for each summary sentence. For each summary sentence, a learned convolutional layer is in charge of converting the entire distribution into a single score.}</p>
<p>The first step of the SummaC $_{\text {Conv }}$ algorithm is to turn each column of the NLI Pair Matrix into a fixed-size histogram that represents the distribution of scores for that given summary sentence.</p>
<p>We bin the NLI scores into $H$ evenly spaced bins (e.g., if $H=5$, the bins are $[0,0.2)$, $[0.2,0.4),[0.4,0.6),[0.6,0.8),[0.8,1))$. Thus the first summary sentence of the example in Figure 1 would have the following histogram: $[2,0,1,0,1]$, because there are two values between $[0.0,0.2]$ in the first column, one in $[0.4,0.6]$ and one in $[0.8,1.0]$.</p>
<p>By producing one histogram for each summary sentence, the binning process in the example of Figure 1 would produce:</p>
<p>$$
\operatorname{bin}\left(X_{\text {pair }}\right)=\left[\begin{array}{lll}
2 &amp; 3 &amp; 4 \
0 &amp; 0 &amp; 0 \
1 &amp; 0 &amp; 0 \
0 &amp; 0 &amp; 0 \
1 &amp; 1 &amp; 0
\end{array}\right]
$$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">Size</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">\% Positive</th>
<th style="text-align: right;">IAA</th>
<th style="text-align: right;">Source</th>
<th style="text-align: right;"># Summarizer</th>
<th style="text-align: right;"># Sublabel</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Valid.</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">CoGenSumm (Falke et al., 2019)</td>
<td style="text-align: right;">1281</td>
<td style="text-align: right;">400</td>
<td style="text-align: right;">49.8</td>
<td style="text-align: right;">0.65</td>
<td style="text-align: right;">C</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">XSumFaith (Maynez et al., 2020)</td>
<td style="text-align: right;">1250</td>
<td style="text-align: right;">1250</td>
<td style="text-align: right;">10.2</td>
<td style="text-align: right;">0.80</td>
<td style="text-align: right;">X</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">Polytope (Huang et al., 2020)</td>
<td style="text-align: right;">634</td>
<td style="text-align: right;">634</td>
<td style="text-align: right;">6.6</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">C</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">FactCC (Kryscinski et al., 2020)</td>
<td style="text-align: right;">931</td>
<td style="text-align: right;">503</td>
<td style="text-align: right;">85.0</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">C</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">SummEval (Fabbri et al., 2021)</td>
<td style="text-align: right;">850</td>
<td style="text-align: right;">850</td>
<td style="text-align: right;">90.6</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">C</td>
<td style="text-align: right;">23</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">FRANK (Pagnoni et al., 2021)</td>
<td style="text-align: right;">671</td>
<td style="text-align: right;">1575</td>
<td style="text-align: right;">33.2</td>
<td style="text-align: right;">0.53</td>
<td style="text-align: right;">C+X</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">7</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the six datasets in the SummaC Benchmark. For each dataset, we report the validation and test set sizes, the percentage of summaries with positive (consistent) labels (\% Positive), the inter-annotator agreement (when available, IAA), the source of the documents (Source: C for CNN/DM, X for XSum), the number of summarizers evaluated, and the number of sublabels annotated.</p>
<p>The binned matrix is then passed through a 1-D convolution layer with a kernel size of $H$. The convolution layer scans the summary histograms one at a time, and compiles each into a scalar value for each summary. Finally, the scores of each summary sentence are averaged to obtain the final summary-level score.</p>
<p>In order to learn the weights of the convolution layer, we train the SummaC $_{\text {Conv }}$ model end-toend with the synthetic training data in FactCC (Kryscinski et al., 2020). The original training dataset contains one million (document, summary) pairs evenly distributed with consistent and inconsistent summaries. Because we are only training a small set of $H$ parameters (we use $H=50$ ), we find that using a 10,000 sub-sample is sufficient. We train the model using a cross-entropy loss, the Adam optimizer, a batch size of 32 , and a learning rate of $10^{-2}$. We perform hyper-parameter tuning on a validation set from the FactCC dataset.</p>
<p>The number of bins used in the binning process, which corresponds to the number of parameters in the convolution layer, is also a hyper-parameter we tune on the validation set. We find that performance increases until 50 bins (i.e., a bin width of 0.02 ) and then plateaus. We use 50 bins in all our experiments.</p>
<h2>4 SummaC Benchmark</h2>
<p>To rigorously evaluate the SummaC models on a diverse set of summaries with consistency judgements, we introduce a new large benchmark dataset, the SummaC Benchmark. It comprises the six largest available datasets for summary incon-
sistency detection, which we standardize to use the same classification task.</p>
<h3>4.1 Benchmark Standardization</h3>
<p>We standardize the task of summary inconsistency detection by casting it as a binary classification task. Each dataset contains (document, summary, label) samples, where the label can either be consistent or inconsistent.</p>
<p>Each dataset is divided into a validation and test split, with the validation being available for parameter tuning. We used existing validation/test splits created by dataset authors when available. We did not find a split for XSumFaith, Polytope, and SummEval, and created one by putting even-indexed samples in a validation split, and odd-indexed samples in the test split. This method of splitting maintains similar class imbalance and summarizer identity with the entire dataset.</p>
<p>We computed inter-annotator agreement calculated with Fleiss' Kappa (Fleiss, 1971) on the dataset as an estimate for dataset quality, omitting datasets for which summaries only had a single annotator (Polytope and FactCC). Table 1 summarizes dataset statistics and properties.</p>
<h3>4.2 Benchmark Datasets</h3>
<p>We introduce each dataset in the benchmark chronologically, and describe the standardizing procedure.</p>
<p>CoGenSumm (Correctness of Generated Summaries, CGS) (Falke et al., 2019) is the first introduced dataset for summary inconsistency detection, based on models trained on the CNN/DM dataset (Nallapati et al., 2016). The</p>
<p>authors proposed that consistency detection should be approached as a ranking problem: Given a consistent and inconsistent summary for a common document, a ranking model should score the consistent summary higher. Although innovative, other datasets in the benchmark do not always have positive and negative samples for a given document. We thus map the dataset to a classification task by using all inconsistent and consistent summaries as individual samples.</p>
<p>XSumFaith (eXtreme Summarization Faithfulness, XSF) (Maynez et al., 2020) is a dataset with models trained on the XSum dataset (Narayan et al., 2018), which consists of more abstractive summaries than CoGenSumm. The authors find that standard generators remain consistent for only 20-30\% of generated summaries. The authors differentiate between extrinsic and intrinsic hallucinations (which we call inconsistencies in this work). Extrinsic hallucinations, which involve words or concepts not in the original document can nonetheless be accurate or inaccurate. In order for a summarizer to generate an accurate extrinsic hallucination, the summarizer must possess external world knowledge. Because the authors found that the models are primarily inaccurate in terms of extrinsic hallucinations, we map both extrinsic and intrinsic hallucinations to a common inconsistent label.</p>
<p>Polytope (Huang et al., 2020) introduces a more extensive typology of summarization errors, based on the Multi-dimensional Quality Metric (Mariana, 2014). Each summary is annotated with eight possible errors, as well as a severity level for the error. We standardize this dataset by labeling a summary as inconsistent if it was annotated with any of the five accuracy errors (and disregarded the three fluency errors). Each summary in Polytope was labeled by a single annotator, making it impossible to measure inter-annotator agreement.</p>
<p>FactCC (Kryscinski et al., 2020) contains validation and test splits that are entirely annotated by authors of the paper, because attempts at crowd-sourced annotation yielded low interannotator agreement. Prior work (Gillick and Liu, 2010) shows that there can be divergence in annotations between experts and non-experts in summarization, and because the authors of the paper are NLP researchers familiar with the limitations of automatic summarizations, we expect that FactCC annotations differs in quality from other datasets. FactCC also introduces a synthetic
dataset by modifying consistent summaries with semantically variant rules. We use a sub-portion of this synthetic dataset to train the SummaC $_{\text {Conv }}$ model.</p>
<p>SummEval (Fabbri et al., 2021) contains summarizer outputs from seven extractive models and sixteen abstractive models. Each summary was labeled using a 5-point Likert scale along four categories: coherence, consistency, fluency, and relevance by 3 annotators. We label summaries as consistent if all annotators gave a score of 5 in consistency, and inconsistent otherwise.</p>
<p>FRANK (Pagnoni et al., 2021) contains annotations for summarizers trained on both CNN/DM and XSum, with each summary annotated by three crowd-workers. The authors propose a new typology with seven error types, organized into semantic frame errors, discourse errors and content verifiability errors. The authors confirm that models trained on the more abstractive XSum dataset generate a larger proportion of inconsistent summaries, compared to models trained on CNN/DM. We label summaries as consistent if a majority of annotators labeled the summary as containing no error.</p>
<h3>4.3 Benchmark Evaluation Metrics</h3>
<p>With each dataset in the SummaC Benchmark converted to a binary classification task, we now discuss the choice of appropriate evaluation metrics for the benchmark. Previous work on each dataset in the benchmark used different evaluation methods, falling into three main categories.</p>
<p>First, CoGenSumm proposes a re-ranking based measure, requiring pairs of consistent and inconsistent summaries for any document evaluated; this information is not available in several datasets in the benchmark.</p>
<p>Second, XSumFaith, SummEval, and FRANK report on correlation of various metrics with human annotations. Correlation has some advantages, such as not requiring a threshold and being compatible with the Likert-scale annotations of SummEval, however it is an uncommon choice to measure performance of a classifier due to the discrete and binary label.</p>
<p>Third, authors of FactCC measured model performance using binary F1 score, and balanced accuracy, which corrects unweighed accuracy with the class imbalance ratio, so that majority class voting obtains a score of $50 \%$.</p>
<p>The datasets have widely varying class imbalances, ranging from $6 \%$ to $91 \%$ positive samples. Therefore, we select balanced accuracy (Brodersen et al., 2010) as the primary evaluation metric for the SummaC Benchmark. Balanced accuracy is defined as:</p>
<p>$$
B A c c=\frac{1}{2}\left(\frac{T P}{T P+F N}+\frac{T N}{T N+F P}\right)
$$</p>
<p>Where $T P$ stands for true positive, $F P$ false positive, $T N$ true negative, and $F N$ false negative. The choice of metric is based on the fact that accuracy is a conceptually simple, interpretable metric, and that adjusting the class imbalance out of the metric makes the score more uniform across datasets.</p>
<p>The balanced accuracy metric requires models to output a binary label (i.e., not a scalar score), which for most models requires the selection of a threshold in the score. The threshold is selected using the validation set, allowing for a different threshold for each dataset in the benchmark. Performance on the benchmark is the unweighted average of performance on the six datasets.</p>
<p>We choose Area Under the Curve of the Receiver Operating Chart (ROC-AUC) as a secondary evaluation metric, a common metric to summarize a classifier's performance at different threshold levels (Bradley, 1997).</p>
<h2>5 Results</h2>
<p>We compared the SummaC models against a wide array of baselines and state-of-the-art methods.</p>
<h3>5.1 Comparison Models</h3>
<p>We evaluated the following models on the SummaC Benchmark:</p>
<p>NER Overlap uses the spaCy named entity recognition (NER) model (Honnibal et al., 2020) to detect when an entity present in the summary is not present in the document. This model, adapted from Laban et al. (2021), considers only a subset of entity types as hallucinations (PERSON, LOCATION, ORGANIZATION, etc.)</p>
<p>MNLI-doc is a RoBERTa (Liu et al., 2019) model finetuned on the MNLI dataset (Williams et al., 2018). The document is used as the premise and the summary as a hypothesis, and we use the predicted probability of entailment as a score,
similar to prior work on using NLI models for inconsistency detection (Kryscinski et al., 2020).</p>
<p>FactCC-CLS is a RoBERTa-base model finetuned on the synthetic training portion of the FactCC dataset. Although trained solely on artificially created inconsistent summaries, prior work showed the model to be competitive on the FactCC and FRANK datasets.</p>
<p>DAE (Goyal and Durrett, 2020) is a parsingbased model using the default model and hyperparameters provided by the authors of the paper. ${ }^{2}$</p>
<p>FEQA (Durmus et al., 2020) is a QAG method, using the default model and hyper-parameters provided by the authors of the paper. ${ }^{3}$</p>
<p>QuestEval (Scialom et al., 2021) is a QAG method taking both precision and recall into account. We use the default model and hyperparameters provided by the authors of the paper. ${ }^{4}$ The model has an option to use an additional question weighter, however experiments revealed that the weighter lowered overall performance on the validation portion of the SummaC Benchmark, and we compare to the model without weighter.</p>
<h3>5.2 SummaC Benchmark Results</h3>
<p>Balanced accuracy results are summarized in Table 2. We find that the SummaC models achieve the two best performances in the benchmark. SummaC $_{\text {Conv }}$ achieves the best benchmark performance at $74.4 \%, 5$ points above QuestEval, the best method not involving NLI.</p>
<p>Looking at the models' ability to generalize across datasets and varying scenarios of inconsistency detection provides interesting insights. For example, the FactCC-CLS model achieves strong performance on the FactCC dataset, but close to lowest performance on FRANK and XSumFaith. In comparison, SummaC model performance is strong across the board.</p>
<p>The strong improvement from the SummaC ${ }<em _Conv="{Conv" _text="\text">{\text {ZS }}$ to SummaC $</em>$ model learns to look at the distribution and makes more robust decisions, leading to gains in performance.}}$ also shines a light on the importance of considering the entire distribution of document scores for each summary sentence, instead of taking only the maximum score: The SummaC $_{\text {Conv }</p>
<p>The table of results with the ROC-AUC metric, the secondary metric of the SummaC Benchmark,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">SummaC Benchmark Datasets</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">Doc./min.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CGS</td>
<td style="text-align: center;">XSF</td>
<td style="text-align: center;">Polytope</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">SummEval</td>
<td style="text-align: center;">FRANK</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">NER-Overlap</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">55,900</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI-doc</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">6,200</td>
</tr>
<tr>
<td style="text-align: center;">Classifier</td>
<td style="text-align: center;">FactCC-CLS</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">13,900</td>
</tr>
<tr>
<td style="text-align: center;">Parsing</td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">755</td>
</tr>
<tr>
<td style="text-align: center;">QAG</td>
<td style="text-align: center;">FEQA</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">70.3*</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">22.7</td>
</tr>
<tr>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">SummaC $_{\text {ZS }}$</td>
<td style="text-align: center;">70.4*</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">83.8*</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">72.1*</td>
<td style="text-align: center;">435</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SummaC $_{\text {Conv }}$</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">66.4*</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">89.5**</td>
<td style="text-align: center;">81.7**</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">74.4**</td>
<td style="text-align: center;">433</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of Summary Inconsistency Detection models on the test set of the SummaC Benchmark. Balanced accuracy is computed for each model on the six datasets in the benchmark, and the average is computed as the overall performance on the benchmark. We obtain confidence intervals comparing the SummaC models to prior work: * indicates an improvement with $95 \%$ confidence, and ** $99 \%$ confidence (details in Section 5.2.1). The results of the throughput analysis of Section 5.2.2 are in column Doc./min (Documents per minute).
is included in Appendix A2, echoing the trends seen with the balanced accuracy metric.</p>
<h3>5.2.1 Statistical Testing</h3>
<p>We aim to determine whether the performance improvements of the SummaC models are statistically significant. For each dataset of the benchmark, we perform two tests through bootstrap resampling (Efron, 1982), comparing each of the SummaC models to the best-performing model from prior work. We perform interval comparison at two significance level: $p=0.05$ and $p=0.01$, and apply the Bonferroni correction (Bonferroni, 1935) as we perform several tests on each dataset. We summarize which improvements are significant in Table 2, and perform a similar testing procedure for the ROC-AUC results in Table A2.</p>
<p>SummaC models lead to a statistically significant improvement on CoGenSumm, XSumFaith, FactCC, and SummEval. QuestEval outperforms the SummaC models on Polytope at a confidence of $95 \%$. On the FRANK dataset, QuestEval and SummaC $<em _ZS="{ZS" _text="\text">{\text {Conv }}$ achieve highest performance with no statistical difference. Overall on the benchmark, both SummaC models significantly outperform prior work, SummaC $</em>$ at $p=0.01$.}}$ at a $p=0.05$ significance level and SummaC $_{\text {Conv }</p>
<h3>5.2.2 Computational Cost Comparison</h3>
<p>Computational cost of the method is an important practical factor to consider when choosing a model to use, as some applications such as training with a generator with Reinforcement Learning might require a minimum throughput from the model
(i.e., number of documents processed by the model per unit of time).</p>
<p>A common method to compare algorithms is using computational complexity analysis, computing the amount of resources (time, space) needed as the size of the input varies. Computational complexity analysis is impractical in our case, as the units of analysis differ between models, and do not allow for a direct comparison. More specifically, some of the models' complexity scales with the number of sub-word units in the document (MNLI-doc, FactCC-CLS), some with the number of entities in a document (NER-Overlap, DAE, QuestEval), and some with number of sentences (the SummaC models).</p>
<p>We instead compare models by measuring throughput on a fixed dataset using a common hardware setup. More precisely, we measured the processing time of each model on the 503 documents in the test set of FactCC (with an average of 33.2 sentences per document), running a single Quadro RTX 8000 GPU. For prior work, we used implementation publicly released by the authors, and made a best effort to use the model at an appropriate batch size for a fair comparison.</p>
<p>The result of the throughput analysis is included in Table 2 (column Docs./min.). SummaC models are able to process around 430 documents per minute, which is much lower than some of the baselines capable of processing more than 10,000 documents per minute. However, QAG methods are more than 10 times slower than SummaC models, processing only 20-40 documents per minute.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Architecture</td>
<td style="text-align: left;">NLI Dataset</td>
<td style="text-align: center;">ZS</td>
<td style="text-align: center;">Conv</td>
</tr>
<tr>
<td style="text-align: left;">Dec. Attn</td>
<td style="text-align: left;">SNLI</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">56.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SNLI</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">64.0</td>
</tr>
<tr>
<td style="text-align: left;">BERT Base</td>
<td style="text-align: left;">MNLI</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">69.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MNLI+VitaminC</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">71.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SNLI</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: left;">BERT Large</td>
<td style="text-align: left;">SNLI+MNLI+ANLI</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">VitaminC</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">72.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MNLI</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MNLI+VitaminC</td>
<td style="text-align: center;">$\mathbf{7 2 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Effect of NLI model choice on SummaC models performance. For each NLI model, we include the balanced accuracy scores of SummaC ${ }<em _Conv="{Conv" _text="\text">{\text {ZS }}$ and SummaC $</em>$. BERT X corresponds to a BERT or other pre-trained models of similar size.}</p>
<h3>5.3 Further Results</h3>
<p>We now examine how different components and design choices affect SummaC model performance.</p>
<h3>5.3.1 Choice of NLI Model</h3>
<p>SummaC models rely on an NLI model at their core, which consists of choosing two main components: a model architecture, and a dataset to train on. We investigate the effect of both of these choices on the performance of SummaC models on the benchmark.</p>
<p>Regarding model architectures, we experi ment with the decomposable attention model (Parikh et al., 2016), which is a pre-Transformer architecture model that was shown to achieve high performance on SNLI, as well as Transformer base and Transformer Large architectures.</p>
<p>With respect to datasets, we include models trained on standard NLI datasets such as SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018), as well as more recent datasets such as Adversarial NLI (Nie et al., 2019) and Vitamin C (Schuster et al., 2021).</p>
<p>Results are summarized in Table 3, and we emphasize three trends. First, the low performance of the decomposable attention model used in experiments in prior work (Falke et al., 2019) confirms that less recent NLI models did not transfer well to summary inconsistency detection.</p>
<p>Second, NLI models based on pre-trained Transformer architectures all achieve strong performance on the benchmark, with an average
increase of 1.3 percentage points when going from a base to a large architecture.</p>
<p>Third, the choice of NLI dataset has a strong influence on overall performance. SNLI leads to lowest performance, which is expected as its textual domain is based on image captions, which are dissimilar to the news domain. MNLI and Vitamin C trained models both achieve close to the best performance, and training on both jointly leads to the best model, which we designate as the default NLI model for the SummaC models (i.e., the model included in Table 2).</p>
<p>The latter two trends point to the fact that improvements in the field of NLI lead to improvements in the SummaC models, and we can expect that future progress in the NLI community will translate to gains of performance when integrated into the SummaC model.</p>
<p>We relied on trained models available in HuggingFace's Model Hub (Wolf et al., 2020). Details in Appendix A.</p>
<h3>5.3.2 Choice of NLI Category</h3>
<p>The NLI task is a three-way classification task, yet most prior work has limited usage of the model to the use of the entailment probability for inconsistency detection (Kryscinski et al., 2020; Falke et al., 2019). We run a systematic experiment by training multiple SummaC $_{\text {Conv }}$ models that have access to varying subsets of the NLI labels, and measure the impact on overall performance. Results are summarized in Table 4. Using solely the entailment category leads to strong performance for all models. However, explicitly including the contradiction label as well leads to small boosts in performance for the ANLI and MNLI models.</p>
<p>With future NLI models being potentially more nuanced and calibrated, it is possible that inconsistency detector models will be able to rely on scores from several categories.</p>
<h3>5.3.3 Choice of Granularity</h3>
<p>So far, we've reported experiments primarily with a sentence-level granularity, as it matches the granularity of NLI datasets. One can imagine cases where sentence-level granularity might be limiting. For example, in the case of a summary performing a sentence fusion operation, an NLI model might not be able to correctly predict entailment of the fused sentence, seeing only one sentence at a time.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SummaC $_{\text {Conv }}$ Performance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">E</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">VITC+MNLI</td>
<td style="text-align: center;">ANLI</td>
<td style="text-align: center;">MNLI</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">73.0</td>
</tr>
</tbody>
</table>
<p>Table 4: Effect of NLI category inclusion on SummaC $_{\text {Conv }}$ performance. Models had access to different subsets of the three category predictions (Entailment, Neutral, Contradiction), with performance measured in terms of balanced accuracy. Experiments were performed with 3 NLI models: Vitamic C+MNLI, ANLI, and MNLI.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Granularity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">MNLI + VitC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Document</td>
<td style="text-align: center;">Summary</td>
<td style="text-align: center;">ZS</td>
<td style="text-align: center;">Conv</td>
<td style="text-align: center;">ZS</td>
<td style="text-align: center;">Conv</td>
</tr>
<tr>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Paragraph</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">71.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;">Two Sent.</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">71.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">74.7</td>
</tr>
<tr>
<td style="text-align: center;">Sentence</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">69.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">74.4</td>
</tr>
</tbody>
</table>
<p>Table 5: Effect of granularity choice on SummaC models performance. We tested four granularities on the document side: full, paragraph, two sentence, and sentence, and two granularities on the summary side: full and sentence. Performance of the four models is measured in balanced accuracy on the benchmark test set.</p>
<p>To explore this facet further, we experiment with modifying the granularity of both the document and the summary. With regard to document granularity, we consider four granularities: (1) full text, the text is treated as a single block, (2) paragraph-level granularity, the text is separated into paragraph blocks, (3) two-sentence granularity, the text is separated into blocks of contiguous sentences of size two (i.e., block 1 contains sentence 1-2, block 2 contains sentence 3-4), and (4) sentence-level, splitting text at individual sentences. For the summary granularity, we only consider two granularities: (1) full text, and (2) sentence, because other granularities are less applicable since summaries usually consist of three sentences or fewer.</p>
<p>We study the total of 8 (document, summary) granularity combinations with the two best-performing NLI models of Table 2: MNLI and Vitamin C, each included as SummaC $<em _Conv="{Conv" _text="\text">{\text {ZS }}$ and SummaC $</em>$}}$ models. ${ }^{5</p>
<p>Results for the granularity experiments are summarized in Table 5. Overall, finer granularities lead to better performance, with (sentence, sentence) and (two sent, sentence) achieving highest performance across all four models.</p>
<p>The MNLI-only trained model achieves lowest performance when used with full text granularity on the document level, and performance steadily increases from $56.4 \%$ to $73.5 \%$ as granularity is made finer both on the document and summary side. Results for the MNLI+VitaminC model vary less with changing granularity, showcasing that the model is perhaps more robust to different granularity levels. However the (two sent, sentence) and (sentence, sentence) settings achieve highest performance, implying that finer granularity remains valuable.</p>
<p>For all models, performance degrades in cases where granularity on the document level is finer than summary granularity. For example the (sentence, full) or (two sent, full) combinations lead to some of the lowest performance. This is expected, as in cases in which summaries have several sentences, it is unlikely that they will fully be entailed by a single document sentence. This implies that granularity on the document side should be coarser or equal the summary's granularity.</p>
<p>Overall, we find that finer granularity for the document and summary is beneficial in terms of performance and recommend the use of a (sentence, sentence) granularity combination.</p>
<h2>6 Discussion and Future Work</h2>
<p>Improvements on the Benchmark. The models we introduced in this paper are just a first step towards harnessing NLI models for inconsistency detection. Future work could explore a number of improvements: combining the predictions of multiple NLI models, or combining multiple granularitiy levels-for example, through multi-hop reasoning (Zhao et al., 2019).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Interpretability of Model Output. If a model can pinpoint which portion of a summary is inconsistent, some work has shown that corrector models can effectively re-write the problematic portions and often remove the inconsistency (Dong et al., 2020). Furthermore, fine-grained consistency scores can be incorporated into visual analysis tools for summarization such as SummViz (Vig et al., 2021). The SummaC ${ }<em _Conv="{Conv" _text="\text">{\mathrm{ZS}}$ model is directly interpretable, whereas the SummaC $</em>$ model is another open area for future work.}}$ is slightly more opaque, due to the inability to trace back a low score to a single sentence in the document being invalidated. Improving the interpretability of the SummaC $_{\text {Conv }</p>
<p>Beyond News Summarization. The six datasets in the SummaC Benchmark contain summaries from the news domain, one of the most common application of summarization technology. Recent efforts to expand the application of summarization to new domains such as legal (Kornilova and Eidelman, 2019) or scholarly (Cachola et al., 2020) text will hopefully lead to the study of inconsistency detection in these novel domains, and perhaps even out of summarization on tasks such as text simplification, or code generation.</p>
<p>Towards Consistent Summarization. Inconsistency detection is but a first step in eliminating inconsistencies from summarization. Future work can include more powerful inconsistency detectors in the training of next generation summarizers to reduce the prevalence of inconsistencies in generated text.</p>
<h2>7 Conclusion</h2>
<p>We introduce SummaC $<em _Conv="{Conv" _text="\text">{Z S}$ and SummaC $</em>$ outperforms all prior work with a balanced accuracy score of $74.4 \%$, an improvement of five absolute percentage points over the best baseline. To the best of our knowledge, this the first successful attempt at adapting NLI models for inconsistency detection, and we believe that there are many exciting opportuni-
ties for further improvements and applications of our methods.}}$, two NLI-based models for summary inconsistency detection based on the key insight that NLI models require sentence-level input to work best. Both models achieve strong performance on the SummaC Benchmark, a new diverse and standardized collection of the six largest datasets for inconsistency detection. SummaC $_{\text {Conv }</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Katie Stasaski, Dongyeop Kang, and the TACL reviewers and editors for their helpful comments, as well as Artidoro Pagnoni for helpful pointers during the project. This work was supported by a Microsoft BAIR Commons grant as well as a Microsoft Azure Sponsorship.</p>
<h2>References</h2>
<p>Kristjan Arumae and Fei Liu. 2019. Guiding extractive summarization with questionanswering rewards. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2566-2577. https://doi.org/10 .18653/v1/N19-1264</p>
<p>Carlo E. Bonferroni. 1935. Il calcolo delle assicurazioni su gruppi di teste. Studi in onore del professore salvatore ortu carboni, pages 13-60.</p>
<p>Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642.</p>
<p>Andrew P. Bradley. 1997. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognition, 30(7):1145-1159. https://doi.org /10.1016/S0031-3203(96)00142-2</p>
<p>Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M. Buhmann. 2010. The balanced accuracy and its posterior distribution. In 2010 20th International Conference on Pattern Recognition, pages 3121-3124. IEEE. https://doi.org /10.1109/ICPR.2010.764</p>
<p>Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel S. Weld. 2020. Tldr: Extreme summarization of scientific documents. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 4766-4777. https://doi.org/10 .18653/v1/2020.findings-emnlp. 428</p>
<p>Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020. Factual error correction for abstractive summarization models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6251-6258. https://doi.org/10 .18653/v1/2020.emnlp-main. 506</p>
<p>Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, and Jingjing Liu. 2020. Multi-fact correction in abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9320-9331. https://doi.org/10 .18653/v1/2020.emnlp-main. 749</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055-5070. https://doi.org/10 .18653/v1/2020.acl-main. 454</p>
<p>Bradley Efron. 1982. The jackknife, the bootstrap and other resampling plans. In CBMSNSF Regional Conference Series in Applied Mathematics. https://doi.org/10.1145 /1409360.1409378</p>
<p>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68-74.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Reevaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409. https://doi.org /10.1162/tacl_a_00373</p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych.
2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220. https://doi.org/10 . 1037/h0031619</p>
<p>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378.</p>
<p>Dan Gillick and Yang Liu. 2010. Non-expert evaluation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, pages 148-151.</p>
<p>Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 166-175. https://doi.org /10.1145/3292500.3330955</p>
<p>Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. arXiv preprint arXiv:2010.05478.</p>
<p>Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python. https://doi.org/10 .18653/v1/2020.findings-emnlp. 322</p>
<p>Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What have we achieved on text summarization? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 446-469. https://doi.org/10 .18653/v1/2020.emnlp-main. 33</p>
<p>Anastassia Kornilova and Vladimir Eidelman. 2019. Billsum: A corpus for automatic summarization of us legislation. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 48-56.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of</p>
<p>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346. https://doi.org/10 .18653/v1/2020.emnlp-main. 750</p>
<p>Philippe Laban, Tobias Schnabel, Paul Bennett, and Marti A. Hearst. 2021. Keep it simple: Unsupervised simplification of multi-paragraph text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6365-6378, Online. Association for Computational Linguistics. https://doi.org/10.18653/v1 /2021.acl-long. 498</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Valerie R. Mariana. 2014. The Multidimensional Quality Metric (MQM) framework: A new framework for translation quality assessment. Brigham Young University.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005 .00661. https://doi.org/10.18653 /v1/2020.acl-main. 173</p>
<p>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çağlar Gulçehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 280-290. https://doi.org/10 .18653/v1/K16-1028</p>
<p>Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and Bing Xiang. 2021. Improving factual consistency of abstractive summarization via question answering. arXiv preprint arXiv:2105.04623. https://doi .org/10.18653/v1/2021.acl-long. 536</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807. https://doi.org/10 .18653/v1/D18-1206</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2019. Adversarial NLI: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599. https://doi.org/10 .18653/v1/2020.acl-main. 441</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics. In NAACL.</p>
<p>Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249-2255.</p>
<p>Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-reward reinforced summarization with saliency and entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 646-653. https://doi.org/10.18653/v1/N18 -2102</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your Vitamin C! Robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 624-643. https://doi.org /10.18653/v1/2021.naacl-main. 52</p>
<p>Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. 2021. Questeval: Summarization asks for factbased evaluation. arXiv preprint arXiv:2103. 12693. https://doi.org/10.18653/v1 /2021.emnlp-main. 529</p>
<p>Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3246-3256. https://doi.org/10 .18653/v1/D19-1320</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: A large-scale dataset for fact extraction and verification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819. https:// doi.org/10.18653/v1/N18-1074</p>
<p>Jesse Vig, Wojciech Kryscinski, Karan Goel, and Nazneen Fatema Rajani. 2021. Summvis: Interactive visual analysis of models, data, and evaluation for text summarization. arXiv preprint arXiv:2104.07605. https://doi.org/10 .18653/v1/2021.acl-demo. 18</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. https://doi.org /10.18653/v1/N18-1101</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics. https://doi.org/10.18653 /v1/2020.emnlp-demos. 6</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020a. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328-11339. PMLR.</p>
<p>Yuhao Zhang, Derek Merck, Emily Tsai, Christopher D. Manning, and Curtis Langlotz. 2020b. Optimizing the factual correctness of a summary: A study of summarizing radiology reports. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5108-5120. https://doi .org/10.18653/v1/2020.acl-main. 458</p>
<p>Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett, and Saurabh Tiwary. 2019. Transformer-XH: Multi-evidence reasoning with extra hop attention. In International Conference on Learning Representations.</p>
<h2>Appendix</h2>
<h2>A NLI Model Origin</h2>
<p>We list the NLI models we used throughout the paper, which can be retrieved on HuggingFace's model hub. ${ }^{6}$ BERT stands for any Pre-trained bi-directional Transformer of an equivalent size:</p>
<ul>
<li>boychaboy/SNLI_roberta-base BERT Base+SNLI</li>
<li>microsoft/deberta-base-mnli BERT Base+MNLI</li>
<li>tals/albert-base-vitaminc-mnli BERT Base + MNLI + VitaminC</li>
<li>boychaboy/SNLI_roberta-large BERT Large+SNLI</li>
<li>tals/albert-xlarge-vitaminc Bert Large+VitaminC</li>
<li>roberta-large-mnli Bert Large+MNLI</li>
<li>tals/albert-xlarge-vitaminc-mnli BERT Large+MNLI+VitaminC</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>B SummaC $_{\text {ZS }}$ Operator Choice</h2>
<p>Table A1 measures the effect of the choice of the two operators in the SummaC ${ }_{\mathrm{ZS}}$ model. We explore three options (min, mean, and max) for each operator. We find that the choice of max for Operator 1 and mean for Operator 2 achieves the highest performance and use these choices in our model.</p>
<h2>C SummaC Benchmark ROC-AUC Results</h2>
<p>Table A2 details results of models on the benchmark according to the ROC-AUC metric, confirming that the SummaC models achieve the two best accuracy results on the benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Operator 2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Op. 1</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Max</td>
</tr>
<tr>
<td style="text-align: left;">Min</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">57.4</td>
</tr>
<tr>
<td style="text-align: left;">Mean</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: left;">Max</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">$\mathbf{7 2 . 1}$</td>
<td style="text-align: center;">69.1</td>
</tr>
</tbody>
</table>
<p>Table A1: Effect of operator choice on the performance of the SummaC ${ }_{\text {ZS }}$ model, measured in terms of balanced accuracy. Operator 1 reduces the row dimension of the NLI Pair Matrix, and Operator 2 reduces the column dimension.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">SummaC Benchmark Datasets</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CGS</td>
<td style="text-align: center;">XSF</td>
<td style="text-align: center;">Polytope</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">SummEval</td>
<td style="text-align: center;">FRANK</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">NER-Overlap</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">56.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI-doc</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">63.4</td>
</tr>
<tr>
<td style="text-align: center;">Classifier</td>
<td style="text-align: center;">FactCC-CLS</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;">Parsing</td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">66.3</td>
</tr>
<tr>
<td style="text-align: center;">QAG</td>
<td style="text-align: center;">FEQA</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">57.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">73.6</td>
</tr>
<tr>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">SummaC $_{\text {ZS }}$</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SummaC $_{\text {Conv }}$</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">92.2**</td>
<td style="text-align: center;">86.0*</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">77.8**</td>
</tr>
</tbody>
</table>
<p>Table A2: Performance of Summary Inconsistency Detection models on the test portion of the SummaC Benchmark in terms of ROC-AUC metric. The metric is computed for each model on the six datasets in the benchmark, and the average is computed as the overall performance on the benchmark. Confidence intervals comparing the SummaC models to prior work: * indicates an improvement with $95 \%$ confidence, and ** $99 \%$ confidence (details in Section 5.2.1).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://huggingface.co/models.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
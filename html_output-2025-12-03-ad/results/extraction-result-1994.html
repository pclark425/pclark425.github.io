<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1994 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1994</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1994</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-46.html">extraction-schema-46</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <p><strong>Paper ID:</strong> paper-278501841</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.06850v3.pdf" target="_blank">Visual Evolutionary Optimization on Graph-Structured Combinatorial Problems with MLLMs: A Case Study of Influence Maximization</a></p>
                <p><strong>Paper Abstract:</strong> —Graph-structured combinatorial problems in complex networks are prevalent in many domains, and are computationally demanding due to their complexity and non-linear nature. Traditional evolutionary algorithms (EAs), while robust, often face obstacles due to content-shallow encoding limitations and lack of structural awareness, necessitating hand-crafted modifications for effective application. In this work, we introduce an original framework, visual evolutionary ptimization (VEO), leveraging multimodal large language models (MLLMs) as the backbone evolutionary optimizer in this context. Specifically, we propose a context-aware encoding way, representing the solution of the network as an image. In this manner, we can utilize MLLMs’ image processing capabilities to intuitively comprehend network configurations, thus enabling machines to solve these problems in a human-like way. We have developed MLLM-based operators tailored for various evolutionary optimization stages, including initialization, crossover, and mutation. Furthermore, we propose that graph sparsification can effectively enhance the applicability and scalability of VEO on large-scale networks, owing to the scale-free nature of real-world networks. We demonstrate the effectiveness of our method using a well-known task in complex networks, influence maximization, and validate it on eight different real-world networks of various structures. The results have confirmed VEO’s reliability and enhanced effectiveness compared to traditional evolutionary optimization.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1994.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1994.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VEO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Evolutionary Optimization (VEO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary optimization framework that encodes graph-structured candidate solutions as images and uses multimodal large language models (MLLMs) as initialization, crossover and mutation operators to produce structure-aware offspring for combinatorial graph problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Visual Evolutionary Optimization (VEO)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (MLLM-based operators for initialization, crossover, mutation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Operators are implemented as multimodal LLM prompts that take image renderings of the graph (with seeds highlighted) as input and output node-index lists. Specific operators: MLLM-based Initialization (three agent strategies: High-degree Spread Selector, High-degree Central Selector, etc.), MLLM-based Crossover (analyzes two parent images and returns an offspring seed set), and MLLM-based Mutation (two-phase removal/addition or one-shot replacement). Models used in experiments include Gpt-4o-2024-11-20, Gemini-2.0-flash-lite, Claude-3.7-sonnet, and Qwen-vl-max. No model fine-tuning is performed; behavior is controlled via handcrafted prompts (including a Two-Phases prompting strategy for mutation). Image visualization parameters (layout styles KK and FR, node/label sizes) are part of the operator context.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Influence maximization on eight real-world networks (USAir, Netscience, Polblogs, Facebook, WikiVote, Rutgers89, MSU24, Texas84) with additional generalizability test on Karate network (network dismantling). Simplified networks used in MLLM inputs: |V|=50, |E|=100.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional evolutionary optimization with list/index encodings and standard random crossover/mutation; initialization baselines: Random Initialization, Refined Random (from simplified graph), High-Degree Initialization, High-Betweenness Initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>MLLM-based initialization yields consistently higher fitness than baselines (Table IV: example USAir initialization fitness: MLLM 49.92 ± 1.98 vs Random 48.84 ± 2.23). MLLM-based reproduction (Two-Phases, KK layout) yields improved or comparable fitness across networks (Table VI examples: Netscience Normal 14.04 ± 1.11 vs MLLM 14.63 ± 1.13; WikiVote Normal 480.32 ± 20.59 vs MLLM (Two-Phases, KK) 506.40 ± 22.91). Improvements vary by network; VEO often achieves faster initial gains and higher final fitness (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Traditional EA reported fitness values per network (Table VI examples): USAir 44.03 ± 3.17, Netscience 14.04 ± 1.11, WikiVote 480.32 ± 20.59, Texas84 2197.38 ± 225.31. These are generally lower than the best MLLM-driven configurations reported for the same tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td>High validation rates for MLLM outputs: initialization checks T1I (valid nodes) and T2I (seed size) near 100% across networks; T3I (low-degree node avoidance) generally >88% even for larger inputs. Crossover validation T1C/T3C near 99% with small drops in T2C (duplicate-node checks). Mutation validations are more variable; T3M (repetitive additions) shows the most issues. Model-wise reproduction validation (Figure 7): nearly all evaluated MLLMs >75% and majority >95% for reproduction validity.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Qualitative generalizability test on Karate network for network dismantling shows MLLM-based operators outperform the standard EA (faster convergence and higher fitness) even though task objective differs from influence maximization, indicating transferability of the image-based prompting strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>No formal analysis of dataset-induced training bias; authors note general LLM hallucination risk and therefore implement validation checks, but no evidence is reported that MLLM behaviors are biased toward particular training-distribution patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Per-API-call latency (Table VIII): gpt-4o-2024-11-20 crossover 2.46 ± 0.52 s, mutation 1.75 ± 0.41 s; Gemini-2.0-flash-lite crossover 2.28 ± 0.46 s, mutation 2.18 ± 0.45 s; Claude-3.7-sonnet crossover 5.59 ± 1.34 s, mutation 4.11 ± 1.30 s; Qwen-vl-max crossover 3.11 ± 0.95 s, mutation 1.81 ± 0.56 s. Token-cost scaling (Figure 8): text-based LLM graph encodings (adjacency/incidence lists) scale linearly with graph size (large token explosion), whereas image-based MLLM inputs have near-constant token cost independent of graph size (difference across image resolutions like 400×400 vs 768×768). Crossover (2 images) is slower than mutation (1 image) across models.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Only general pretrained MLLMs are compared: Gpt-4o-2024-11-20 and Claude-3.7-sonnet generally achieve higher final fitness across networks (Figure 6); no domain-specific fine-tuning is performed or evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations performed: prompt design (One-Shot vs Two-Phases mutation) — Two-Phases slightly outperforms One-Shot; layout effect (Kamada-Kawai vs Fruchterman-Reingold) — KK layout generally yields marginally better fitness; initialization agents — ANOVA shows significant differences among agent strategies in 6/8 networks indicating agent prompt strategy matters.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>No online fine-tuning; adaptation occurs only via prompt strategy (e.g., Two-Phases mutation) and different MLLM choices. The paper does not implement online learning or fine-tuning of the MLLMs during evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Hallucination risk requiring validation checks; mutation stage shows more variability and issues (repetitive additions, lower T3M scores); occasional selection of low-degree nodes in initialization/mutation (T3I dips on some networks); higher latency and cost for some MLLMs (e.g., Claude-3.7-sonnet). Performance sensitive to visualization layout and prompt framing.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Encoding solutions as images and using MLLMs as operators imparts structural awareness absent from index-based operators, leading to improved initialization quality and often better reproduction outcomes across diverse real-world graphs. MLLM operators yield high validation rates with careful prompting and graph sparsification; image-based inputs scale far better in token cost than textual graph encodings. Prompt design (multi-step prompts) and visualization layout materially affect performance. No fine-tuning is required to obtain gains, but model choice strongly influences both performance and computational cost. Overall, learned (prompted MLLM) operators can outperform traditional random/naive EA operators by leveraging visual structural understanding, with tradeoffs in per-call latency and the need for robust output validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1994.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1994.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based EA (related works)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based evolutionary operators and LLM-assisted evolutionary optimization (related literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>References and prior works that use LLMs as search operators (crossover/mutation) or to automate parts of evolutionary algorithms, discussed in the related work but not experimentally evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based evolutionary operators (general category)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based / learned from data (prompted, few-shot approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Cited works classify LLM-augmented evolutionary computation into LLMs-based search operators and LLMs-based algorithm automation. Examples include using LLMs as crossover [42], mutation [43], and frameworks like LLMs-driven EA (LMEA) [44] that employ LLMs for both crossover and mutation. These approaches rely on prompting and few-shot examples rather than explicit model fine-tuning in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Varied across cited works: program synthesis, protein property optimization, combinatorial optimization subproblems; the current paper lists these in related work but does not experimentally evaluate them.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Generally compared in those works to standard EA operators, random search, or domain-specific heuristics (as summarized by the present paper's related-work discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>The literature suggests two main paradigms: (1) using LLMs as operators (crossover/mutation) capable of semantic/structural reasoning via prompts, and (2) using LLMs to automate algorithm design. The present paper situates VEO within the former paradigm but extends it with image encoding for graph structure.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1994.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1994.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP vs LLM comparison (cite)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A comparison of large language models and genetic programming for program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that directly compares the capabilities of large language models and genetic programming on program synthesis tasks (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comparison of large language models and genetic programming for program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Comparison study: LLMs vs Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>comparison (LLM-based vs genetic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Mentioned in the references as an external comparative study; the present paper does not reproduce or re-evaluate its findings. The cited study likely evaluates LLMs and GP on program synthesis benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Program synthesis (per the cited title); not evaluated in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Genetic Programming (traditional GP) vs Large Language Models (LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Cited as evidence that direct comparisons between LLM-driven approaches and genetic programming exist in the literature; the present paper references such work but does not provide cross-domain experimental data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1994.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1994.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM crossover & mutation mentions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-model-based crossover and mutation (selected citations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Specific prior works that explore using language models for genetic operators, e.g., few-shot prompting to produce variation and LLMs to enhance genetic improvement mutations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Language-model crossover/mutation methods (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (few-shot prompting, mutation enhancement)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Examples cited include 'Language model crossover: Variation through few-shot prompting' and 'Enhancing genetic improvement mutations using large language models'. These works use prompting/few-shot examples to let LLMs propose offspring or code edits; the present paper references them but does not reproduce experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Likely program synthesis / software improvement tasks (based on cited titles); not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Typically compared to standard GP operators or conventional mutation heuristics in the cited works (not in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Paper cites these works as antecedents showing LLMs can be used as genetic operators in other domains; they support the motivation for testing MLLMs as operators on graph-structured combinatorial tasks in VEO.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A comparison of large language models and genetic programming for program synthesis. <em>(Rating: 2)</em></li>
                <li>Language model crossover: Variation through few-shot prompting. <em>(Rating: 2)</em></li>
                <li>Enhancing genetic improvement mutations using large language models. <em>(Rating: 2)</em></li>
                <li>Large language model-based evolutionary optimizer: Reasoning with elitism. <em>(Rating: 2)</em></li>
                <li>Large language models as evolutionary optimizers. <em>(Rating: 2)</em></li>
                <li>LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics. <em>(Rating: 2)</em></li>
                <li>Evolutionary computation in the era of large language model: Survey and roadmap. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1994",
    "paper_id": "paper-278501841",
    "extraction_schema_id": "extraction-schema-46",
    "extracted_data": [
        {
            "name_short": "VEO",
            "name_full": "Visual Evolutionary Optimization (VEO)",
            "brief_description": "An evolutionary optimization framework that encodes graph-structured candidate solutions as images and uses multimodal large language models (MLLMs) as initialization, crossover and mutation operators to produce structure-aware offspring for combinatorial graph problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Visual Evolutionary Optimization (VEO)",
            "operator_type": "LLM-based (MLLM-based operators for initialization, crossover, mutation)",
            "operator_description": "Operators are implemented as multimodal LLM prompts that take image renderings of the graph (with seeds highlighted) as input and output node-index lists. Specific operators: MLLM-based Initialization (three agent strategies: High-degree Spread Selector, High-degree Central Selector, etc.), MLLM-based Crossover (analyzes two parent images and returns an offspring seed set), and MLLM-based Mutation (two-phase removal/addition or one-shot replacement). Models used in experiments include Gpt-4o-2024-11-20, Gemini-2.0-flash-lite, Claude-3.7-sonnet, and Qwen-vl-max. No model fine-tuning is performed; behavior is controlled via handcrafted prompts (including a Two-Phases prompting strategy for mutation). Image visualization parameters (layout styles KK and FR, node/label sizes) are part of the operator context.",
            "training_data_description": null,
            "domain_or_benchmark": "Influence maximization on eight real-world networks (USAir, Netscience, Polblogs, Facebook, WikiVote, Rutgers89, MSU24, Texas84) with additional generalizability test on Karate network (network dismantling). Simplified networks used in MLLM inputs: |V|=50, |E|=100.",
            "comparison_baseline": "Traditional evolutionary optimization with list/index encodings and standard random crossover/mutation; initialization baselines: Random Initialization, Refined Random (from simplified graph), High-Degree Initialization, High-Betweenness Initialization.",
            "performance_learned_operator": "MLLM-based initialization yields consistently higher fitness than baselines (Table IV: example USAir initialization fitness: MLLM 49.92 ± 1.98 vs Random 48.84 ± 2.23). MLLM-based reproduction (Two-Phases, KK layout) yields improved or comparable fitness across networks (Table VI examples: Netscience Normal 14.04 ± 1.11 vs MLLM 14.63 ± 1.13; WikiVote Normal 480.32 ± 20.59 vs MLLM (Two-Phases, KK) 506.40 ± 22.91). Improvements vary by network; VEO often achieves faster initial gains and higher final fitness (Figure 3).",
            "performance_traditional_operator": "Traditional EA reported fitness values per network (Table VI examples): USAir 44.03 ± 3.17, Netscience 14.04 ± 1.11, WikiVote 480.32 ± 20.59, Texas84 2197.38 ± 225.31. These are generally lower than the best MLLM-driven configurations reported for the same tasks in this paper.",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": "High validation rates for MLLM outputs: initialization checks T1I (valid nodes) and T2I (seed size) near 100% across networks; T3I (low-degree node avoidance) generally &gt;88% even for larger inputs. Crossover validation T1C/T3C near 99% with small drops in T2C (duplicate-node checks). Mutation validations are more variable; T3M (repetitive additions) shows the most issues. Model-wise reproduction validation (Figure 7): nearly all evaluated MLLMs &gt;75% and majority &gt;95% for reproduction validity.",
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Qualitative generalizability test on Karate network for network dismantling shows MLLM-based operators outperform the standard EA (faster convergence and higher fitness) even though task objective differs from influence maximization, indicating transferability of the image-based prompting strategy.",
            "training_bias_evidence": "No formal analysis of dataset-induced training bias; authors note general LLM hallucination risk and therefore implement validation checks, but no evidence is reported that MLLM behaviors are biased toward particular training-distribution patterns.",
            "computational_cost_comparison": "Per-API-call latency (Table VIII): gpt-4o-2024-11-20 crossover 2.46 ± 0.52 s, mutation 1.75 ± 0.41 s; Gemini-2.0-flash-lite crossover 2.28 ± 0.46 s, mutation 2.18 ± 0.45 s; Claude-3.7-sonnet crossover 5.59 ± 1.34 s, mutation 4.11 ± 1.30 s; Qwen-vl-max crossover 3.11 ± 0.95 s, mutation 1.81 ± 0.56 s. Token-cost scaling (Figure 8): text-based LLM graph encodings (adjacency/incidence lists) scale linearly with graph size (large token explosion), whereas image-based MLLM inputs have near-constant token cost independent of graph size (difference across image resolutions like 400×400 vs 768×768). Crossover (2 images) is slower than mutation (1 image) across models.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": "Only general pretrained MLLMs are compared: Gpt-4o-2024-11-20 and Claude-3.7-sonnet generally achieve higher final fitness across networks (Figure 6); no domain-specific fine-tuning is performed or evaluated.",
            "ablation_study_results": "Ablations performed: prompt design (One-Shot vs Two-Phases mutation) — Two-Phases slightly outperforms One-Shot; layout effect (Kamada-Kawai vs Fruchterman-Reingold) — KK layout generally yields marginally better fitness; initialization agents — ANOVA shows significant differences among agent strategies in 6/8 networks indicating agent prompt strategy matters.",
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "No online fine-tuning; adaptation occurs only via prompt strategy (e.g., Two-Phases mutation) and different MLLM choices. The paper does not implement online learning or fine-tuning of the MLLMs during evolution.",
            "failure_modes": "Hallucination risk requiring validation checks; mutation stage shows more variability and issues (repetitive additions, lower T3M scores); occasional selection of low-degree nodes in initialization/mutation (T3I dips on some networks); higher latency and cost for some MLLMs (e.g., Claude-3.7-sonnet). Performance sensitive to visualization layout and prompt framing.",
            "key_findings_for_theory": "Encoding solutions as images and using MLLMs as operators imparts structural awareness absent from index-based operators, leading to improved initialization quality and often better reproduction outcomes across diverse real-world graphs. MLLM operators yield high validation rates with careful prompting and graph sparsification; image-based inputs scale far better in token cost than textual graph encodings. Prompt design (multi-step prompts) and visualization layout materially affect performance. No fine-tuning is required to obtain gains, but model choice strongly influences both performance and computational cost. Overall, learned (prompted MLLM) operators can outperform traditional random/naive EA operators by leveraging visual structural understanding, with tradeoffs in per-call latency and the need for robust output validation.",
            "uuid": "e1994.0"
        },
        {
            "name_short": "LLM-based EA (related works)",
            "name_full": "LLM-based evolutionary operators and LLM-assisted evolutionary optimization (related literature)",
            "brief_description": "References and prior works that use LLMs as search operators (crossover/mutation) or to automate parts of evolutionary algorithms, discussed in the related work but not experimentally evaluated in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-based evolutionary operators (general category)",
            "operator_type": "LLM-based / learned from data (prompted, few-shot approaches)",
            "operator_description": "Cited works classify LLM-augmented evolutionary computation into LLMs-based search operators and LLMs-based algorithm automation. Examples include using LLMs as crossover [42], mutation [43], and frameworks like LLMs-driven EA (LMEA) [44] that employ LLMs for both crossover and mutation. These approaches rely on prompting and few-shot examples rather than explicit model fine-tuning in many cases.",
            "training_data_description": null,
            "domain_or_benchmark": "Varied across cited works: program synthesis, protein property optimization, combinatorial optimization subproblems; the current paper lists these in related work but does not experimentally evaluate them.",
            "comparison_baseline": "Generally compared in those works to standard EA operators, random search, or domain-specific heuristics (as summarized by the present paper's related-work discussion).",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": null,
            "failure_modes": null,
            "key_findings_for_theory": "The literature suggests two main paradigms: (1) using LLMs as operators (crossover/mutation) capable of semantic/structural reasoning via prompts, and (2) using LLMs to automate algorithm design. The present paper situates VEO within the former paradigm but extends it with image encoding for graph structure.",
            "uuid": "e1994.1"
        },
        {
            "name_short": "GP vs LLM comparison (cite)",
            "name_full": "A comparison of large language models and genetic programming for program synthesis",
            "brief_description": "A referenced study that directly compares the capabilities of large language models and genetic programming on program synthesis tasks (mentioned in related work).",
            "citation_title": "A comparison of large language models and genetic programming for program synthesis.",
            "mention_or_use": "mention",
            "system_name": "Comparison study: LLMs vs Genetic Programming",
            "operator_type": "comparison (LLM-based vs genetic programming)",
            "operator_description": "Mentioned in the references as an external comparative study; the present paper does not reproduce or re-evaluate its findings. The cited study likely evaluates LLMs and GP on program synthesis benchmarks.",
            "training_data_description": null,
            "domain_or_benchmark": "Program synthesis (per the cited title); not evaluated in the current paper.",
            "comparison_baseline": "Genetic Programming (traditional GP) vs Large Language Models (LLMs).",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": null,
            "failure_modes": null,
            "key_findings_for_theory": "Cited as evidence that direct comparisons between LLM-driven approaches and genetic programming exist in the literature; the present paper references such work but does not provide cross-domain experimental data.",
            "uuid": "e1994.2"
        },
        {
            "name_short": "LM crossover & mutation mentions",
            "name_full": "Language-model-based crossover and mutation (selected citations)",
            "brief_description": "Specific prior works that explore using language models for genetic operators, e.g., few-shot prompting to produce variation and LLMs to enhance genetic improvement mutations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Language-model crossover/mutation methods (cited)",
            "operator_type": "LLM-based (few-shot prompting, mutation enhancement)",
            "operator_description": "Examples cited include 'Language model crossover: Variation through few-shot prompting' and 'Enhancing genetic improvement mutations using large language models'. These works use prompting/few-shot examples to let LLMs propose offspring or code edits; the present paper references them but does not reproduce experiments.",
            "training_data_description": null,
            "domain_or_benchmark": "Likely program synthesis / software improvement tasks (based on cited titles); not evaluated here.",
            "comparison_baseline": "Typically compared to standard GP operators or conventional mutation heuristics in the cited works (not in this paper).",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": null,
            "failure_modes": null,
            "key_findings_for_theory": "Paper cites these works as antecedents showing LLMs can be used as genetic operators in other domains; they support the motivation for testing MLLMs as operators on graph-structured combinatorial tasks in VEO.",
            "uuid": "e1994.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A comparison of large language models and genetic programming for program synthesis.",
            "rating": 2
        },
        {
            "paper_title": "Language model crossover: Variation through few-shot prompting.",
            "rating": 2
        },
        {
            "paper_title": "Enhancing genetic improvement mutations using large language models.",
            "rating": 2
        },
        {
            "paper_title": "Large language model-based evolutionary optimizer: Reasoning with elitism.",
            "rating": 2
        },
        {
            "paper_title": "Large language models as evolutionary optimizers.",
            "rating": 2
        },
        {
            "paper_title": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics.",
            "rating": 2
        },
        {
            "paper_title": "Evolutionary computation in the era of large language model: Survey and roadmap.",
            "rating": 1
        }
    ],
    "cost": 0.016449,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Visual Evolutionary Optimization on Graph-Structured Combinatorial Problems with MLLMs: A Case Study of Influence Maximization</p>
<p>Jie Zhao 
Senior Member, IEEEHao Cheong Kang 
Kang Hao 
K H Cheong </p>
<p>Division of Mathematical Sciences
School of Physical and Mathematical Sciences
Nanyang Technological University
S637371Hao CheongSingapore. Kang</p>
<p>College of Computing and Data Science
Nanyang Technological University
S639798Singapore</p>
<p>Visual Evolutionary Optimization on Graph-Structured Combinatorial Problems with MLLMs: A Case Study of Influence Maximization
F51BFEF85C8D453FA31F2D09F78E07ADMultimodal large language modelsevolutionary optimizationinfluence maximizationcomplex networks
Graph-structured combinatorial problems in complex networks are prevalent in many domains, and are computationally demanding due to their complexity and non-linear nature.Traditional evolutionary algorithms (EAs), while robust, often face obstacles due to content-shallow encoding limitations and lack of structural awareness, necessitating hand-crafted modifications for effective application.In this work, we introduce an original framework, visual evolutionary ptimization (VEO), leveraging multimodal large language models (MLLMs) as the backbone evolutionary optimizer in this context.Specifically, we propose a context-aware encoding way, representing the solution of the network as an image.In this manner, we can utilize MLLMs' image processing capabilities to intuitively comprehend network configurations, thus enabling machines to solve these problems in a human-like way.We have developed MLLM-based operators tailored for various evolutionary optimization stages, including initialization, crossover, and mutation.Furthermore, we propose that graph sparsification can effectively enhance the applicability and scalability of VEO on large-scale networks, owing to the scale-free nature of real-world networks.We demonstrate the effectiveness of our method using a well-known task in complex networks, influence maximization, and validate it on eight different real-world networks of various structures.The results have confirmed VEO's reliability and enhanced effectiveness compared to traditional evolutionary optimization.</p>
<p>I. INTRODUCTION</p>
<p>G RAPH-structured combinatorial problems are pervasive across various fields, such as influence maximization [1] in social media.These problems are characterized by their reliance on networks where nodes and edges represent discrete entities and their interactions, respectively.The complexity of these networks, coupled with their typically large and complex nature, poses significant computational challenges [2,3].Evolutionary algorithms (EAs) are well-suited for graph-structured problems as they are adept at managing the nonlinearities and discontinuities [4,5,6].While traditional EAs have been instrumental in tackling a variety of optimization problems [7,8], they still exhibit an inherent limitation, particularly when applied to graph-structured combinatorial problems due to the encoding limitations.</p>
<p>The choice of encoding scheme plays a crucial role in solving discrete problems, as it determines how solutions are represented and manipulated within the evolutionary optimization framework.Different discrete tasks require tailored encoding strategies in EAs, such as permutation and path encodings for scheduling [9] and routing problems [10], or binary and label-based (node or edge index) encodings for subset selection problems, such as influence maximization [11] and community deception [12].However, these traditional encodings represent nodes or paths in an abstract manner, failing to capture their roles within the network.As a result, evolutionary operators like crossover and mutation are applied without an inherent understanding of the underlying entities or relationships.This lack of structural awareness can lead to naive modifications that may not respect the structural properties of the graph.</p>
<p>Multimodal large language models (MLLMs) are extensively trained on diverse datasets, enabling them to effectively process multiple data types with inherent adaptability [13,14].This capability presents a unique opportunity to enhance evolutionary optimization fundamentally.In this work, we introduce a novel encoding methodology that visually represents solutions as images.By leveraging this visual encoding strategy, we unlock the potential of MLLMs in evolutionary optimization and propose an original framework called Visual Evolutionary Optimization (VEO).This framework enables the model to analyze and manipulate solutions in a way that mirrors human decision-making in evolutionary operations, such as crossover and mutation.The motivations and advantages behind VEO operate on two levels as follows:</p>
<p>(1) Representation-level: Image-based Encoding Scheme (2) Optimization-level: MLLMs as Evolutionary Optimizers</p>
<p>• Human-Like Visual Reasoning: MLLMs can interpret the structure of image-encoded graph and reason about solution quality in a human-like manner, enabling highquality decision-making for offspring generation within the EA framework without relying on random search or complex heuristics.• Generalization and Adaptability: MLLMs can adapt across diverse and dynamic graph structures without requiring problem-specific algorithm design.They can also flexibly respond to changes in topology, offering broad applicability and real-time adaptability.• Constraint-Aware Optimization: MLLMs can naturally avoid infeasible solutions by understanding constraints through contextual cues, reducing the need for manual constraint handling and dedicated adjustment.</p>
<p>Effective visualization is essential for utilizing VEO on graph-structured problems, particularly when dealing with large networks.Plotting all nodes on a limited canvas often results in cluttered visuals, obscuring critical structural details and connections.Node labels, essential for MLLM-based operations like initialization, crossover, and mutation, must remain clear and readable to ensure that MLLMs can accurately identify each node's role and relationship within the network.As noted in [15], many real-world networks exhibit a scale-free structure, where a small subset of nodes holds disproportionately high centrality or importance.By concentrating on these key nodes and reducing overall graph complexity without significantly compromising solution quality, we are able to address largescale problem instances more effectively.To this end, we propose a community-level graph sparsification method that compresses large networks into more manageable scales while preserving essential structural information necessary for meaningful analysis and optimization.</p>
<p>A common challenge with existing community detection algorithms is their tendency to produce an excessive number of communities, often including single-node communities [16].While this level of granularity could be informative, it is often impractical for graph sparsification.Recognizing the importance of community information [17,18] and the need to balance detail with usability, we develop a new community merging method to reduce the number of communities while preserving the essential structural information during graph sparsification.The main contributions of our work are summarized as follows:</p>
<p>• Our paper presents a novel context-aware encoding scheme for graph-structured combinatorial optimization, where solutions are represented as images that encapsulate rich structural information of the underlying networks.The proposed representation allows us to further develop evolutionary operations with a profound understanding of the network's topology.More precisely, we introduce MLLM-based operators tailored to various stages of evolutionary optimization, including initialization, crossover, and mutation, fundamentally redefining how solutions are generated, manipulated, and refined within the evolutionary framework.</p>
<p>• We explore how to effectively improve the applicability and scalability of MLLMs by leveraging graph reduction techniques.Specifically, we propose a community-level sparsification method that reduces the size of large-scale networks while preserving key structural information.As part of this process, a new community merging technique is employed to consolidate close communities, enhancing sparsification efficiency while maintaining meaningful graph semantics.</p>
<p>• We have comprehensively investigated the reliability of VEO, such as the validity and effectiveness of MLLMs output.</p>
<p>Several key parameters such as the influence of the number of nodes in the plotted network as well as the different-level tasks on the quality of MLLMs are also studied.The experiments are conducted on eight different real-world networks and the ablation results suggest the proposed VEO can greatly facilitate different stages of evolutionary optimization.</p>
<p>As an illustration, we adopt the influence maximization problem as a representative case study to illustrate the potential of MLLMs as effective evolutionary operators.To maintain generalizability and focus, our approach adheres to the standard evolutionary optimization framework in this work.</p>
<p>The structure of this paper is as follows: Section II discusses previous research on evolutionary optimization, LLMs and combinatorial problems.Section III details the proposed MLLMdriven evolutionary optimization.In Section IV, we evaluate the effectiveness of VEO and analyze various parameters.The future work for the MLLM-based evolutionary optimization on graph-structured combinatorial problems is discussed in Section V. Finally, Section VI presents the conclusions of the paper.</p>
<p>II. RELATED WORK</p>
<p>In this section, we will review some literature related to evolutionary optimization, combinatorial problems and large language models.</p>
<p>A. Evolutionary Optimization on Combinatorial Problems</p>
<p>The utility of evolutionary optimization in addressing discrete and non-linear problems has prompted its widespread adoption for tackling complex networks [19,20] and various combinatorial tasks [21,22], such as trunk scheduling [23,24] and job shop scheduling [25,26].In the realm of complex networks, the versatility of evolutionary optimization is further demonstrated by its widespread use in enhancing network robustness [27,28] and facilitating network reconstruction [29,30].Additional applications include identifying critical nodes [31], discovering dynamic communities [32], recognizing network modules [33,34] and sensor deployment [35], thereby illustrating its broad applications in network-related challenges.</p>
<p>B. LLMs-Assisted Evolutionary Optimization</p>
<p>Wu et al. [36] classified existing works regarding the synergy of LLMs and evolutionary computation into two main branches: LLMs-based search operators [37,38] and LLMsbased algorithm automation [39,40,41].Our study can be roughly classified into the first category.In [42], LLMs are employed as crossover operators to derive new solutions from parental inputs.Brownlee et al. [43] also presented LLMs effectively functioning as mutation operators that enhance the search process.Liu et al. [44] introduced a novel framework known as LLMs-driven EA (LMEA), which utilizes LLMs for both crossover and mutation operations.Furthermore, LLMs-based search operators were adapted for multi-objective scenarios by segmenting traditional optimization tasks into subproblems [45].The related application can be found in [46], where protein properties are optimized with LLMs-enhanced evolutionary operators.</p>
<p>C. LLMs on Graph-Structured Problems</p>
<p>The recent advancements in LLMs have led to significant progress across diverse fields including sentiment analysis [47], optimization [48], and social sciences [49].This raises the question of whether the capabilities of LLMs can be extended to graph-related tasks [50,51].However, as indicated by studies such as [52,53], LLMs struggle with graph-structured data and often fail to achieve satisfactory results on fundamental tasks, especially as graph complexity increases.</p>
<p>Recent studies have explored the application of MLLMs to graph-related tasks, primarily as benchmarks to evaluate their visual reasoning capabilities.Wei et al. [54] combined visual and textual representations of graphs to enable MLLMs to perform structured reasoning, while VisionGraph [55] relied exclusively on visual inputs to assess how well these models interpret and reason over graphical structures.Beyond basic graph reasoning tasks, there are also some applications of MLLMs for combinatorial optimization.Huang et al. [56] proposed a multimodal approach that combines visual and textual inputs to solve the traveling salesman problem (TSP), whereas Elhenawy et al. [57] focused solely on visual representations to address routing tasks using MLLMs.Although both studies represent pioneering efforts in applying MLLMs to combinatorial problems, they do not incorporate evolutionary optimization frameworks and are limited to relatively smallscale networks with fewer than 200 nodes.</p>
<p>Our work is, to the best of our knowledge, the first to integrate MLLMs as evolutionary optimizers for solving large-scale combinatorial problems, with a particular focus on scalability and structural understanding of real-world complex networks.We aim to explore this untapped synergy by investigating how MLLMs can facilitate structure-aware evolutionary optimization in complex network scenarios, using influence maximization as a representative case study.</p>
<p>III. MLLM-DRIVEN EVOLUTIONARY OPTIMIZATION</p>
<p>In this section, we first introduce a classical combinatorial problem to optimize [58,59], followed by detailed presentations of the proposed graph sparsification and community merging methods, along with the MLLM-based evolutionary optimizer.</p>
<p>A. Problem Formulation</p>
<p>In this work, we illustrate the proposed VEO with influence maximization [60,61].Consider a network G = (V, E), where we aim to identify a subset of nodes S ⊆ V that maximizes the spread of influence throughout the network and the objective is expressed as: max
S⊆V,|S|=k σ(S),(1)
where σ(S) is the expected influence spread of the seed set S, and k is the predefined number of nodes of S.</p>
<p>Due to the computational intensity of running Monte Carlo methods for diffusion simulation, we use the widely adopted Expected Diffusion Value (EDV) [62] as the fitness function to evaluate diffusion speed.For a small propagation probability p, the EDV estimates the expected number of nodes influenced by a seed set S as:
EDV (S) = k + b∈N (S)/S 1 − (1 − p) δ(b) ,(2)
where N (S) represents the one-hop neighbors of the seed set S, defined as N (S) = S ∪ {b | ∃s ∈ S, (s, b) ∈ E}, and δ(b) denotes the number of edges connecting b to any node in S.</p>
<p>B. Graph Sparsification</p>
<p>The applicability of MLLMs would be challenged when attempting to visualize large-scale networks on a single canvas, where displaying all nodes with labels can lead to significant overlaps and a chaotic structure, making direct analysis and optimization impractical.As indicated in [15], many real-world networks are scale-free, meaning that only a minority of nodes carry high centrality or importance.By focusing on these key nodes and reducing graph complexity without significant loss of solution quality, we are able to explore an approach applicable to large and real-world problem instances.To this end, we resort to the sparsification technique that simplifies the original network while preserving its essential topological features.Let the original network be represented by G = (V, E), its simplified network G s can be obtained using a sparsification operator S(•; θ), specifically,
G s = S(G; θ),(3)
where θ refers to the specific graph sparsification technique and it satisfies G s ⊂ G.</p>
<p>Consider a graph G = (V, E) with a set of communities C = {C 1 , C 2 , . . ., C m }, where each C i ⊂ V denotes a cluster of nodes.The simplified network should retain key nodes and connections, preserving the core community structure of the original network.The proposed sparsification process is as follows (see Algorithm 1 for the pseudocode): We begin by determining the number of selected nodes from each community, that is
|C ′ i | = |C i | |V | × N * V ,(4)
where N * V denotes the target node number of the simplified network.</p>
<p>Once the proportional selection is established, the selection is guided by betweenness centrality, where nodes with higher centrality are prioritized, i.e.,
V ′ = m i=1 {v j ∈ C i | b(v j ) ≥ b(v l ), ∀v l ∈ C i \ V ′ }, (5)
where b(v) refers to the betweenness value of node v and
|V ′ ∩ C i | = |C ′ i |.
Following node selection, a subgraph is induced by retaining only the edges between the selected nodes:
E ′ = {(u, v) ∈ E | u, v ∈ V ′ }.(6)
To control the network's density, an edge-pruning step is applied if the number of edges in E ′ exceeds a predefined threshold |N * E |.In this case, a subset of E ′ is randomly or strategically selected such that the total number of edges equals |N * E |, and the rest are discarded.</p>
<p>In addition, isolated nodes will also be removed, and only the largest connected component is retained.The node and edge sets of refined network are updated as:
V ′ = {v ∈ V ′ | d(v) &gt; 0}, E ′ = {(u, v) ∈ E ′ | u, v ∈ V ′ }. (7)
Algorithm 1 Graph Sparsification Input: Graph G = (V, E), the target node number and edge number of simplified network
N * V and N * E ; Output: Simplified network G s = (V ′ , E ′ ) 1: Initialize V ′ ← ∅, E ′ ← ∅ 2: Identify the community structure C = {C 1 , C 2 , . . . , C m } 3: for each community C i in C do 4: Compute betweenness centrality b(v) for each v ∈ C i 5: Sort nodes in C i based on b(v) in descending order 6: Select top |C ′ i | nodes from C i to include in V ′ 7: end for 8: Construct V ′ ← m i=1 V ′ i 9: E ′ ← {(u, v) ∈ E : u ∈ V ′ and v ∈ V ′ } 10: if the number of edges in E ′ exceeds a threshold N * E then 11:
Randomly remove excess edges from E ′ to meet the threshold 12: end if 13: Remove isolated nodes from V ′ 14: Identify the largest connected component and update G s 15: return G s</p>
<p>C. Community Merging</p>
<p>Most existing community detection techniques often lead to clusters with highly imbalanced sizes, including many small or even single-node communities that contribute little to the overall network structure.This imbalance can undermine the effectiveness of the aforementioned graph sparsification, indicating the importance of merging these small-scale communities.Given a graph G = (V, E) with an initial community structure C, where each community C i ⊆ C, our goal is to reduce the total number of communities to a target number N * C .We define the size of each community C i as |C i |, and the algorithm proceeds while the number of updated communities | C ′ | exceeds N * C .The community merging process involves several key steps:
C k ∈ C ′ e(C min , C k ).
After each merging operation, the community structure C ′ is updated, and the process repeats until the number of communities equals N * C .The process of community merging can be found in Algorithm 2.</p>
<p>Algorithm 2 Community Merging
Require: Graph G = (V, E), initial community structure C = {C 1 , C 2 , . . . , C m }, target number of communities N * C Ensure: Updated community structure C ′ with | C ′ | = N * C 1: while | C ′ | &gt; N * C do 2:
Identify the smallest community C min 3:  Update the community structure C ′ 13: end while 14: return C ′
for each community C k ∈ C ′ such that C k ̸ = C min do 4: Initialize e(C min , C k ) ← 0 5: for each edge (v i , v j ) ∈ E do 6: if (v i ∈ C min and v j ∈ C k ) or (v i ∈ C k and v j ∈ C min )</p>
<p>D. MLLM-based Evolutionary Optimizer</p>
<p>When solving graph-structured combinatorial problems, traditional EAs typically utilize list-based encodings, consisting of sequences of node indices.While this approach is straightforward, it has some inherent drawbacks.Crossover and mutation operations could be compromised due to the absence of topological awareness concerning the network's structure.</p>
<p>To overcome these challenges, we propose a new contextaware framework VEO that incorporates visual encoding and direct interaction with network images by utilizing the capabilities of MLLMs.This method transforms the solution into an image format, enhancing machines' understanding of how specific nodes influence particular tasks.The diagram of our method is shown in Figure 1 and the process of MLLMbased evolutionary optimization is in Algorithm 3.</p>
<p>To ensure scalability for large networks, we first address cases where the network size exceeds predetermined thresholds   Our scheme encodes the solution in the form of an image where seed nodes of the solution are distinctly highlighted.Crossover intelligently combines nodes from parent solutions to produce valid offspring by filtering out low-degree nodes (e.g., nodes A and P), while mutation replaces ineffective nodes with more suitable ones to enhance influence spread (e.g., replacing node E with node F).This approach ensures validity and better solution quality.(d): Prompt for reproduction provides instructions for an MLLM: in crossover, the MLLM examines network images and suggests an optimal combination of seed nodes; in mutation, it identifies ineffective nodes for replacement by removing nodes with minimal influence and proposing better non-seed nodes.</p>
<p>to ensure the plotted network is manageable to MLLMs.Formally,
G = S(G; θ), if |V | &gt; N * V and |E| &gt; N * E , G, otherwise.(8)
To facilitate visual integration into our evolutionary framework, we define two distinct functions for generating image representations, one for initializing the population and another for encoding candidate solutions for crossover and mutation.First, we define the visualization function for initialization as
I init (G, Θ) = Visualize(G, Θ),(9)
which produces an image of the graph G using the layout Θ via visualization software.In contrast, to encode specific candidate solutions, we define the function
I(G, Θ, s) = Visualize(G, Θ, s),(10)
which generates an image by plotting the graph G and visually highlighting the nodes of the solution s with the layout Θ.</p>
<p>Based on this image-encoding representation, we can define the following MLLM-based evolutionary operator and the full prompt can be found in Table I.</p>
<p>Definition 1. MLLM-based Initialization: Given a graph G, and an initialization strategy α (which can focus on different aspects) and layout style Θ, the initial population P is generated as:
P = MLLM init(I init (G, Θ), α; N p ) = {s 1 , s 2 , . . . , s N P },(11)
where N P denotes the population size.Definition 2. MLLM-based Crossover: For two parent solutions s i , s j ∈ P, the crossover operator guided by MLLM generates an offspring s ′ as:
s ′ = MLLM crossover I(G, Θ, s i ), I(G, Θ, s j ); P c , (12)
where P c is the crossover rate.</p>
<p>Definition 3. MLLM-based Mutation:</p>
<p>A mutation operator is similarly applied on the network, yielding: You are an expert in network science and will be provided with one network in the form of an image.Please help me intelligently select nodes as the diffusion seeds in this network to achieve influence maximization.
s ′′ = MLLM mutate I(G, Θ, s ′ ); P m ,(13)
Only provide a list of node indices separated by commas.</p>
<p>Initialization (High-degree Spread Selector)</p>
<p>You are an expert in network science and will be provided with one network in the form of an image.Please help me intelligently select nodes as the diffusion seeds in this network to achieve influence maximization.</p>
<p>Here are some tips: (1) Choose largebetweenness nodes.(2) Pick nodes spread across different center parts of the network.</p>
<p>Only provide a list of node indices separated by commas.</p>
<p>Initialization (High-degree Central Selector)</p>
<p>You are an expert in network science and will be provided with one network in the form of an image.Please help me intelligently select nodes as the diffusion seeds in this network to achieve influence maximization.</p>
<p>Here are some tips: (1) Choose largedegree nodes.( 2) Pick nodes at the center place of the network.Only provide a list of node indices separated by commas.</p>
<p>Crossover</p>
<p>Examine a network image where seed nodes are distinctly labeled.Carefully analyze the seed nodes present in each network image and suggest an optimal set of seed nodes that harness the advantages of both parent networks to maximize influence spread.</p>
<p>Focus on selecting high-degree nodes or nodes in strategic positions that significantly enhance network connectivity.Provide your answer as a list of node indices, separated by commas.</p>
<p>Mutation (Two-Phase, Removal)</p>
<p>Examine a network image where seed nodes are colored and non-seed nodes are labeled in white.</p>
<p>Identify the current seed node that contributes the least to influence maximization.</p>
<p>Focus on nodes that appear trivial or less connected.Provide the index of this node.</p>
<p>Mutation (Two-Phase, Addition)</p>
<p>Examine a network image where seed nodes are colored and non-seed nodes are labeled in white.Propose a non-seed node that could significantly increase the network's influence spread.</p>
<p>Focus on nodes with higher degrees or strategically critical positions in the network.Provide the index of this node.</p>
<p>Mutation (One-shot)</p>
<p>Examine a network image where seed nodes are colored and non-seed nodes are labeled in white.</p>
<p>Identify the current seed node that contributes the least to influence maximization and propose a nonseed node in white that could significantly increase the network's influence spread.</p>
<p>Provide the answer as a list: the first element is the index of the seed node to remove, and the second element is the index of the non-seed node to add.</p>
<p>where P m is the mutation rate.</p>
<p>E. Complexity Analysis</p>
<p>The complexity of the graph sparsification method centers on several key computations.</p>
<p>IV. EXPERIMENTAL STUDIES</p>
<p>In the experiment, we examine the effectiveness of MLLM in the evolutionary stages, as well as the parameter sensitivity.</p>
<p>A. Experimental Settings and Dataset</p>
<p>In the validation, we evaluate our VEO on eight realworld networks obtained from the Network Repository (https://networkrepository.com/ ).The number of nodes and edges in the simplified networks is fixed at 50 and 100, respectively, to remain manageable for MLLMs.The probability of crossover and mutation are set to 0.2 and 0.1.The size of the initialized population is set to 15 and each strategy is prompted to generate 5 individuals.The reported result is averaged from 20 independent simulations.Communities with fewer than 2% of the total nodes will be merged into their nearest, larger community.The initial community distribution of networks is Algorithm 3 Visual Evolutionary Optimization using MLLMs
Input: Graph G = (V, E), thresholds N * V and N * E Output: Optimal node set S ′ 1: if |V | &gt; N * V and |E| &gt; N * E then 2:
G s ← Sparsify(G, θ)  obtained by the FastGreedy algorithm [16].The number of selected seeds is set to 5 for Netscience and USAir and is set to 10 for the rest of the larger-scale network.The backbone MLLM is Gpt-4o-2024-11-20.The visualization settings for different evolutionary operators are listed in Table III.</p>
<p>B. Benchmark</p>
<p>We validate the effectiveness of MLLMs in both of initialization and reproduction, thus the benchmark has two parts:</p>
<p>1) Initialization: Random Initialization randomly selects nodes from the entire set of nodes in the original graph to generate the initial population.Refined Random Initialization randomly selects nodes from the set of nodes in the simplified graph to generate the initial population.High-Degree Initialization and High-Betweenness Initialization generate population by randomly sampling nodes from a preselected set of highbetweenness and high-degree nodes, respectively.MLLMbased Initialization utilizes MLLMs to intelligently generate candidate solutions.Three distinct agents, each implementing a unique strategy, are employed to effectively initialize the population (see Table I).</p>
<p>2) Reproduction: This stage involves crossover and mutation operations.Normal evolutionary optimization refers to the approach where solutions are encoded using indices, with crossover and mutation operations performed randomly.In contrast, MLLM-based reproduction employs the proposed image-based encoding method, where crossover and mutation are executed by MLLMs.</p>
<p>C. Examination of MLLM as Initialization Operator</p>
<p>To compare different methods, we use the Wilcoxon test with a 95% confidence interval to statistically examine their difference.The '+' indicates statistically better performance, '≈' indicates no significant difference, and '-' indicates statistically worse performance.Table IV presents the optimization results regarding fitness values achieved through different initialization modes across eight networks.The fitness values are reported as averages with standard deviations.The MLLM-based method consistently outperforms other modes across all networks, as indicated by the highest fitness values and its average ranking of 1.00, demonstrating its superiority.This analysis highlights the superior capability of MLLMs in initializing the population for evolutionary optimization more effectively compared to traditional approaches.In addition, the results show that Refined Random initialization (sampled from the simplified network) outperforms the other three non-MLLM initialization methods (derived from the original network) in 7 out of 8 cases except for Netscience, demonstrating the effectiveness of our graph sparsification approach.</p>
<p>In addition, we investigate the accuracy of MLLMs in following instructions.Specifically, we examine whether the initial population obtained through various strategies exhibits any differences.Table V presents the ANOVA results for three different agent-based initialization methods used in various networks.These agents operate under strategies formulated by MLLMs, which demonstrate an acute awareness of network structural topologies and can execute tasks according to specific instructions.The results indicate significant differences in the performance of these agents in 6 out of 8 networks examined, with only MSU24 and Texas84 showing no significant differences among the agents' strategies.This result also demonstrates the spatial intelligence of MLLMs and their potential in the graph problems.To gain a perspective on the node selection behavior of MLLMs during mutation, we compared the degree distribution of nodes suggested for removal and addition by MLLMs in Figure 2. As seen, the average degree of nodes suggested for addition is clearly higher than those suggested for removal, indicating a strategy favoring the addition of more highly connected nodes to maximize influence or connectivity potentially.</p>
<p>The marked contrast in the degree distributions indicates the MLLMs' awareness of the network structure and the ability to intelligently identify critical nodes, thereby enhancing the overall strategy for influence maximization.</p>
<p>D. Ablation Study of MLLM as Reproduction Operator</p>
<p>To demonstrate the effectiveness of VEO in reproduction (including crossover and mutation), Table VI compares the fitness values achieved through 'Normal' and 'MLLMs' modes.The results demonstrate that VEO (Two-Phases, KK) consistently outperforms conventional evolutionary optimization across networks of varying scales and complexities, highlighting the potential of MLLMs as effective evolutionary operators for graph-based combinatorial problems.</p>
<p>For generalizability, we intentionally employ a simple prompt strategy to test the inherent capability of MLLMs on such problems.The only strategy we adopt is splitting the mutation prompt into two steps, referred to as Two-Phases (see Table I).We conduct an ablation study to compare this with a One-Shot prompting strategy.As shown in Table VI, both One-Shot and Two-Phases approaches achieve improved fitness values, but  the Two-Phases mutation strategy tends to slightly outperform the One-Shot method, suggesting that a clearer and stepwise prompt may better guide the model during evolution.</p>
<p>Furthermore, the results across networks plotted in both KK and FR styles indicate that the KK layout style generally achieves marginally better fitness results compared to those in the FR style.While most cases exhibit no statistically significant differences, the observed variations in certain instances indicate that the graph layout might be the critical factor affecting the performance of MLLM-driven optimization.</p>
<p>Figure 3 presents the optimization landscape of VEO against traditional evolutionary optimization across eight networks.The VEO approach, facilitated by MLLMs, consistently outperforms the traditional method, achieving higher fitness scores and demonstrating accelerated improvements in the initial iterations.The shaded regions indicate variability in the results, showing the robustness and reliability of the VEO process.These findings suggest that MLLMs provide insightful guidance, facilitating optimization in graph-structured combinatorial problems and achieving superior outcomes with fewer iterations.</p>
<p>E. Correctness and Reliability Examination</p>
<p>Due to the commonly reported LLMs hallucination [63], the validity examination is required to ensure the correctness.Table VII reflects generally strong validation performance across most evolutionary stages, particularly during initialization and crossover.Tests T 1 I and T 2 I consistently achieve perfect scores across all networks, indicating robust mechanisms for validating node feasibility and ensuring correct solution sizes.Although the T 3 I test, which checks for low-degree nodes, records slightly lower values, such as 93.1% for Facebook in the FR method, it still demonstrates a high level of reliability, with minor room for refinement.Crossover validation remains solid, with nearly all networks achieving above 99% in T 1 C and T 3 C , and only slight drops in T 2 C , suggesting effective control over offspring structure.Mutation tests show more variability, with T 3 M revealing opportunities for enhancement in managing repetitive node additions.Overall, the result indicates the robust performance of MLLMs in maintaining high standards of the evolutionary process.</p>
<p>Furthermore, we examine the parameter sensitivity of MLLMs in terms of output validation.Figure 4 presents a   comparative analysis of MLLM output validity during initialization across networks of two different input sizes.Across all networks and tests, the validation rates remain consistently high, typically exceeding 95%, which demonstrates the robustness of the initialization process.The T 1 I and T 2 I checks maintain near-perfect accuracy, showing that node validity and seed size constraints are reliably enforced regardless of graph size.The T 3 I test, which identifies low-degree nodes, shows slightly more variation, particularly in networks like Facebook and WikiVote, where a moderate dip is observed in larger graphs.Despite this, all validation rates stay well above 88%, reflecting the scalability and resilience of the MLLM framework, even as input complexity increases.
T 1 I T 2 I T 3 I T 1 I T 2 I T 3 I T 1 C T 2 C T 3 C T 1 M T 2 M T3
On the other hand, we also examine the influence of the required number of selected seeds on the validity, as shown in Figure 5.As observed, the validation rate for T 1 I and T 2 I are basically the same for |S| = 10 and |S| = 20 and there is a slight drop for T 3 I when the required sample nodes increase.This result demonstrates the capability of MLLMs to deal with graph-related problems even when the complexity of tasks increases.</p>
<p>F. Foundation Model Comparison</p>
<p>In terms of model selection, Figure 6 presents the progression of fitness values over optimization iterations when different MLLMs are used for reproduction operations across various networks.Overall, all models show steady improvements in fitness as iterations proceed, confirming the effectiveness of VEO pipeline.Moreover, clear performance differences among models are observed.Gpt-4o-2024-11-20 and Claude-3.7-sonnetconsistently achieve higher final fitness values across most networks, but the other models also demonstrated effective  optimization trends, showing that the VEO framework is compatible with various MLLMs.Notably, the differences in performance suggest that the capabilities of each MLLM, such as visual understanding, layout comprehension, and generation quality, directly affect the optimization results.These observations indicate that model selection is critical when using MLLM-based strategies for solving combinatorial problems.Figure 7 shows the validation rates of different MLLMs when performing reproduction.A higher validation rate indicates that the offspring generated by the model satisfy the problem constraints and maintain solution feasibility.While the reliability is subject to the specific model used, all models achieve strong validation rates, with nearly all results above 75% and the majority exceeding 95%.Additionally, mutation operations present slightly more difficulty for models compared to crossover.These results confirm that all evaluated MLLMs are reliable for reproduction in combinatorial optimization.</p>
<p>G. Computational Cost Analysis</p>
<p>To investigate the efficiency of VEO, we compare running time per API calls for crossover and mutation across different models in Table VIII.The results clearly show that the running time is closely related to the choice of MLLM model, with Claude-3.7-sonnetbeing significantly slower than the others,   In our work, we focus on MLLM-based optimization of combinatorial problems, where a critical aspect is ensuring that the model effectively perceives and reasons over the underlying network structure.To achieve this, the majority of token usage is dedicated to depicting the network, either visually (via images) or textually to provide the LLMs or MLLMs with sufficient structural context.In contrast, the actual prompt instructions used for optimization are minimal and remain static across tasks, accounting for only a marginal portion of the overall token cost.Therefore, our cost analysis specifically isolates and compares the cost of network depiction in image and text formats, which represents the dominant share of token usage.Figure 8 illustrates how the token cost scales with network size under different input representations for both LLM-and MLLM-based models.For LLM inputs (Adjacency and Incident methods) [52], token counts grow significantly as the number of edges or nodes increases, highlighting a direct linear relationship between network size and input complexity.In contrast, for MLLM inputs based on images, the token cost remains almost constant regardless of the network scale, with differences between the 400×400 and 768×768 pixel image sizes.This indicates that visual representations via images offer much greater scalability for handling large networks, as they decouple the input size from the underlying network complexity.
T 1 C T 2 C T 3 C T 1 M T 2 M T 3 M GPT Claude Gemini Qwen USAir T 1 C T 2 C T 3 C T 1 M T 2 M T 3 M GPT Claude Gemini Qwen Netscience T 1 C T 2 C T 3 C T 1 M T 2 M T 3 M GPT Claude Gemini Qwen Polblogs T 1 C T 2 C T 3 C T 1 M T 2 M T 3 M GPT Claude Gemini Qwen Facebook T 1 C T 2 C T 3 C T 1 M T 2 M T 3 M GPT Claude Gemini Qwen WikiVote T 1 C T 2 C T 3 C T 1 M T 2 M T 3 M GPT Claude Gemini Qwen Rutgers89 T 1 C T 2 C T 3 C T 1 M T 2 M T 3 M GPT Claude Gemini Qwen MSU24 T 1 C T 2 C T 3 C T 1 M T 2 M T 3 M</p>
<p>H. Generalizability Test</p>
<p>To investigate the generalizability of VEO, we apply it to the network dismantling problem on the Karate network [64], as shown in Figure 9.In this task, fitness is defined as the difference between the size of the original network and the size of the largest connected component after node removal.Note that network dismantling is fundamentally different from influence maximization, as the latter focuses on maximizing widespread reach, whereas dismantling aims to disrupt connectivity by targeting structurally critical nodes.The results show that the MLLM-based method consistently outperforms the standard evolutionary strategy across iterations, achieving higher fitness values and faster convergence.The descriptions on the right illustrate how MLLMs are prompted to perform task-specific crossover and mutation operations, confirming the adaptability of the framework to different optimization objectives.</p>
<p>Crossover: Carefully analyze the nodes present in each network image and suggest an optimal set of nonrepetitive nodes whose removal harnesses the advantages of both parent networks to maximize network fragmentation.Focus on selecting highdegree nodes at the central place that, when removed, significantly break down the network's structure.</p>
<p>V. FUTURE WORK</p>
<p>As this work lays the foundation for integrating multimodal large language models (MLLMs) into evolutionary optimization over graph-structured data, several avenues remain open for exploration.We discuss two primary directions that could enhance the effectiveness, scalability, and efficiency of VEO.</p>
<p>A. Representation-Level Enhancements</p>
<p>• Adaptive Graph Visualization: Future efforts may focus on developing visualization techniques that dynamically adapt to graph structure and task context.Highlighting key features such as communities and critical connections could help MLLMs better perceive and reason about graph inputs.• Graph Reduction: Techniques such as sparsification and abstraction can be further explored to reduce graph complexity while more accurately preserving essential relational patterns, thereby enabling MLLMs to operate more efficiently on large-scale graphs.</p>
<p>B. Optimization-Level Enhancements</p>
<p>• Fine-Tuning MLLMs: Future work could investigate fine-tuning MLLMs on graph-centric tasks to enhance their adaptability and domain alignment.Fine-tuning lightweight variants may also improve inference speed and resource efficiency.• Advanced Optimization Techniques: Exploring strategies such as prompt engineering, ensemble approaches, or multi-agent collaboration among MLLMs could further improve solution quality, robustness, and generalization across diverse graph problems.</p>
<p>VI. CONCLUSION</p>
<p>In this paper, we have proposed an original and novel framework named visual evolutionary optimization (VEO) powered by multimodal large language models (MLLMs).The framework incorporates encoding schemes and MLLM-based evolutionary operators to facilitate intuitive and context-aware adaptations and optimization.This study contributes to the existing body of work on MLLM-driven evolutionary optimization and discusses a viable path for further research in optimizing complex network-based problems.In addition, several key factors to the outcome of VEO have been investigated such as the layout as well as the scale of the simplified network.In future work, we will explore the application of MLLMs to additional graph-structured combinatorial tasks beyond influence maximization by designing relevant prompting strategies and visualization techniques to enhance VEO's applicability and versatility.</p>
<p>then</p>
<p>min , C k ) ← e(C min , C k ) + 1 C close to C min and merge them 12:</p>
<p>Fig. 1 :
1
Fig. 1: The diagram of MLLM-driven VEO and the comparison with traditional evolutionary optimization, illustrated with influence maximization.(a): The original network presents a graph with nodes and edges, where two solutions (seed sets) are shown: Solution 1 [A, C, P] and Solution 2 [A, E, H], representing selected nodes to maximize influence spread.(b): Traditional encoding and reproduction represent solutions as index lists.However, the crossover process often generates invalid solutions, such as duplicate nodes (e.g., [A, A, P]) or suboptimal combinations (e.g., [A, E, P] where nodes A and E are low-degree).(c):Our scheme encodes the solution in the form of an image where seed nodes of the solution are distinctly highlighted.Crossover intelligently combines nodes from parent solutions to produce valid offspring by filtering out low-degree nodes (e.g., nodes A and P), while mutation replaces ineffective nodes with more suitable ones to enhance influence spread (e.g., replacing node E with node F).This approach ensures validity and better solution quality.(d): Prompt for reproduction provides instructions for an MLLM: in crossover, the MLLM examines network images and suggests an optimal combination of seed nodes; in mutation, it identifies ineffective nodes for replacement by removing nodes with minimal influence and proposing better non-seed nodes.</p>
<p>The betweenness centrality is calculated with a complexity of O(|V ||E|).Constructing the subgraph and adjusting edges are achieved in O(|V ′ | + |E ′ |).The process of community merging iterates (| C| − | C ′ |) times, where | C| represents the initial number of communities and | C ′ | the target count.Each iteration involves identifying the smallest community and calculating its edge connections, requiring O(|E|) time.The running time of MLLM-based evolutionary operators depends on the model's architecture, with lighter and more efficiently optimized models achieving quicker results.</p>
<p>Fig. 2 :
2
Fig.2:The comparative degree analysis of node selection for addition and removal suggested by MLLMs during the mutation stage in different networks.</p>
<p>Fig. 3 :
3
Fig. 3: Fitness evolution over iterations for different reproduction modes across eight different networks.Shaded areas indicate standard deviations over multiple runs.</p>
<p>Texas84|V| = 50 ,Fig. 4 :
504
Fig.4: The comparative analysis of MLLMs output validity in initialization under different-size input graphs.Tests include T 1 I : Valid Node Check (ensuring nodes belong to the valid set), T 2 I : Initialization Size Check (verifying the required seed size is met), and T 3 I : Low-Degree Node Check (identifying low-degree nodes).The seed size is fixed at 10.</p>
<p>Fig. 5 :Fig. 6 :
56
Fig. 5: The comparative analysis of MLLMs output validity in initialization under different seed size settings.Tests include T 1 I : Valid Node Check (ensuring nodes belong to the valid set), T 2 I : Initialization Size Check (verifying the required seed size is met), and T 3 I : Low-Degree Node Check (identifying low-degree nodes).The number of simplified graph is {|V | = 50, |E| = 100}.</p>
<p>Fig. 7 :
7
Fig. 7: Validation rates for different MLLMs across various networks.Higher validation rates indicate greater reliability of reproduction operations, with darker colors corresponding to lower validation rates.</p>
<p>Fig. 8 :
8
Fig. 8: Token cost comparison for different input formats when depicting networks of varying sizes.</p>
<p>11 Fig. 9 :
119
Fig. 9: Generalizability test of VEO applied to the network dismantling task on the Karate network.</p>
<p>-scale networks in textual form.This characteristic enables more efficient inference and enhances scalability within MLLM frameworks.
• Expressing High-Order and Global Structural Infor-mation: Image representations intuitively capture complexstructural information in graphs, such as indirect relation-ships and community structures. This visual encodingprovides a clear and compact view of high-level featuresthat are difficult to express with traditional representations.• Seamless Compatibility with MLLMs: By transformingcombinatorial graph problems into visual formats, weenable a seamless pipeline for applying MLLMs to do-mains traditionally dominated by symbolic or algorithmicmethods and unlock the reasoning capabilities of MLLMsfor combinatorial tasks.
• Scalability and Inference Efficiency: Image-based representations maintain a consistent input size regardless of graph complexity, thereby avoiding issues such as token explosion, which commonly arises when representing arXiv:2505.06850v2[cs.NE] 11 Aug 2025 large</p>
<p>1 )
1
Size Identification: Identify the smallest community C min , where |C min | = min{|C i |} for all i ∈ [1, m]. 2) Connectivity Analysis: For each edge (v i , v j ) ∈ E, increment an edge count between communities for edges where v i ∈ C min and v j ∈ C k ∈ C ′ , and vice versa.This edge count e(C min , C k ) measures the connectivity between C min and the community C k .3) Determination of Closest Community: Identify the community C close with the maximum edge count to C min , i.e., C close = arg max</p>
<p>Crossover solution Mutation solution Crossover solution</p>
<p>Crossover: Examine a network image where seed nodes are distinctly labeled.Carefully analyze the seed nodes present in each network image and suggest an optimal set of seed nodes that harness the advantages of both parent networks to maximize influence spread.Mutation (Removal):Examine a network image where seed nodes are colored and non-seed nodes are labeled in white.Identify the current seed node that contributes the least to influence maximization.Mutation (Addition):Examine a network image where seed nodes are colored and non-seed nodes are labeled in white.Propose a non-seed node that could significantly increase the network's influence spread.</p>
<p>TABLE I :
I
Structure of prompts for MLLM-based evolutionary operators of different phases.The prompt begins with contextsetting, which introduces the input information and clarifies the role of the agents.The latter part of the prompt specifies the desired output format and includes necessary guidance or restrictions.
TaskContext-setting promptOutput directive promptInitialization (In-telligent Selector)</p>
<p>Update the population with new offspring 12: end while 13: S ′ ← Select the solution with the highest fitness in P 14: return S ′
3:G ← G s4: end if5: P ← Initialize population with MLLMs by encoding Ginto images6: while termination criteria not met do7:Fitness ← Evaluate the fitness of each node set in P8:Parents ← Select individuals from P based on fitnessfor reproduction9:Offspring ← Generate new solutions by applyingMLLM-based crossover to images of parent solutions10:Apply MLLM-based mutation to images of offspringsolution to alter node sets11:P ←</p>
<p>TABLE II :
II
Basic network information including the number of nodes |V|, the number of edges |E|, the average degree ⟨K⟩, the clustering coefficient CC, and the average shortest path length ⟨d⟩.|C| and |C ′ | denote the number of communities in the original and simplified networks.
Network|V||E|⟨K⟩CC ⟨d⟩ |C| |C ′ |USAir3322,12612.81 0.40 2.7373Netscience3799144.82 0.74 6.06 194Polblogs1,22216,71727.35 0.23 2.74 102Facebook4,03988,23443.69 0.61 3.69 138WikiVote7,066100,73628.51 0.21 3.25 313Rutgers89 24,568784,59663.87 0.13 3.10 574MSU2432,361 1,118,767 69.14 0.12 3.04 294Texas8436,365 1,590,651 87.48 0.10 2.89 454</p>
<p>TABLE III :
III
Visualization settings summary.
SettingValueGraph Library plotly.graph_objsNode Size35Label Size22Canvas Size1200 × 1200 pxLayoutKamada-KawaiInit. &amp; Crossover: #2F7FC1ColorMutation: Solution#2F7FC1,Non-solution#FFFFFFNode LabelsInit. &amp; Mutation: All nodes Crossover: Solution only</p>
<p>TABLE IV :
IV
Statistical analysis of fitness values obtained by different population initialization modes across eight networks.
NetworksInitialization modesRandomRefined RandomHigh-DegreeHigh-BetweennessMLLMsUSAir48.84±2.23(-)49.27±1.57(-)49.12±1.88(≈)49.19±1.51(≈)49.92±1.98Netscience16.24±0.56(-)15.89±0.80(-)16.40±0.53(≈)16.35±0.41(-)16.63±0.26Polblogs183.32±13.11(-)213.77±5.94(≈)203.93±8.09(-)206.54±8.04(-)216.57±7.43Facebook236.46±49.69(-)407.45±8.86(≈)380.77±31.13(-)376.12±36.48(-)410.57±5.80WikiVote315.58±28.19(-)545.13±15.67(-)498.45±25.11(-)485.01±22.03(-)563.67±10.94Rutgers89327.80±72.36(-)778.96±25.65(-)692.16±44.54(-)671.41±34.45(-)790.93±8.36MSU24363.87±102.34(-)1152.17±27.81(-)1098.03±52.09(-) 1033.17±113.48(-) 1170.79±15.29Texas84543.29±147.81(-) 2594.51±144.18(≈) 2063.10±208.67(-) 2109.58±190.13(-) 2629.62±80.04+/ ≈ /-0/0/80/3/50/2/60/1/7-Avg ranking4.882.383.253.501.0012Avg: 5.31USAirAvg: 7.418Netscience Avg: 3.65 Avg: 4.7712Avg: 4.21PolblogsAvg: 6.9320Faceboo Avg: 5.26Avg: 7.17Degree Distribution2 4 6 8 10Degree Distribution2 4 6Degree Distribution2 4 6 8 10Degree Distribution5 10 150000Node TypeNode TypeNode TypeNode Type</p>
<p>TABLE V :
V
The ANOVA results for different graphs analyzing significant differences among agents.
GraphF-statistic P-value DifferentNetscience26.352.92E-8YesUSAir8.422.76E-4YesPolblogs4.908.00E-3YesFacebook3.562.96E-2YesWikiVote5.534.36E-3YesRutgers893.303.80E-2YesMSU241.242.91E-1NoTexas842.031.33E-1No</p>
<p>TABLE VI :
VI
Statistical analysis of fitness values obtained by different reproduction modes across eight networks, using MLLMdriven evolutionary operations.The networks are visualized in KK and FR styles, and the MLLM operates under One-Shot and Two-Phase mutation strategies.
NetworksReproduction modesNormalMLLM (One-Shot, KK) MLLM (Two-Phases, FR) MLLM (Two-Phases, KK)USAir44.03±3.17(≈)43.46±2.62(-)43.74±2.91(-)45.02±2.49Netscience14.04±1.11(-)14.55±1.15 (≈)14.75±0.94(≈)14.63±1.13Polblogs187.95±8.90(≈)193.65±10.50(≈)191.44±8.82(≈)190.75±6.94Facebook354.58±36.72(-)353.31±42.28(≈)356.00±34.24(≈)367.90±27.03WikiVote480.32±20.59(-)487.36±24.42(-)501.01±22.15(≈)506.40±22.91Rutgers89686.99±39.65(-)698.56±46.61(≈)718.94±44.73(≈)710.39±45.16MSU241017.86±140.14(-)1080.24±339.29(≈)1071.14±39.23(≈)1084.14±39.20Texas842197.38±225.31(-)2339.35±168.78(≈)2269.19±172.68(≈)2277.91±213.75USAir4544Fitness42 434140Visual Evolutionary Optimization (MLLMs) Normal Evolutionary Optimization024 Iterations6802468</p>
<p>TABLE VII :
VII
Validation of output of MLLMs during different evolutionary stages.For initialization, tests include T 1 I : Valid Node Check (ensuring nodes belong to the valid set), T 2 I : Initialization Size Check (verifying solutions meet the required seed size), and T 3 I : Low-Degree Node Check (identifying nodes with low degrees).During crossover, validations include T 1 C : Crossover Size Check (ensuring offspring solutions meet size requirements), T 2 C : Duplicate Node Check (identifying duplicate nodes), and T 3 C : Parent Node Source Check (verifying nodes are derived from parent solutions).For mutation, tests consist of T 1 M : Node Presence Check (ensuring removed nodes exist in the solution), T 2 M : Mutation Valid Node Check (verifying added nodes belong to the valid set) and T 3 M : Mutation Repetitive Node Check (verifying added nodes already exist in the solution).
NetworksInitialization (KK)Initialization (FR)CrossoverMutation</p>
<p>TABLE VIII :
VIII
Average running time (in seconds) per API call for different MLLMs performing crossover (with 2 input images) and mutation (with 1 input image) during evolutionary optimization.
ModelCrossover Mutationgpt-4o-2024-11-202.46±0.52 1.75±0.41Gemeni-2.0-flash-lite 2.28±0.46 2.18±0.45Claude-3.7-sonnet5.59±1.34 4.11±1.30Qwen-vl-max3.11±0.95 1.81±0.56while Gemini-2.0-flash-lite and Gpt-4o-2024-11-20 are muchfaster and more consistent. Additionally, the running timeis influenced by the number of input images or input tokennumbers. Crossover operations (with 2 images) generally takelonger than mutation (with 1 image) across all models.
This work was supported by the Singapore Ministry of Education (MOE) Science of Learning, under Grant No. MOESOL2022-0003.
Influence maximization in complex networks by using evolutionary deep reinforcement learning. L Ma, Z Shao, X Li, Q Lin, J Li, V C Leung, A K Nandi, IEEE Transactions on Emerging Topics in Computational Intelligence. 742022</p>
<p>Enhancing robustness and resilience of multiplex networks against node-community cascading failures. L Ma, X Zhang, J Li, Q Lin, M Gong, C A C Coello, A K Nandi, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 5262021</p>
<p>Obfuscating community structure in complex network with evolutionary divideand-conquer strategy. J Zhao, K H Cheong, IEEE Transactions on Evolutionary Computation. 2762023</p>
<p>A search space reduction-based progressive evolutionary algorithm for influence maximization in social networks. L Zhang, K Ma, H Yang, C Zhang, H Ma, Q Liu, IEEE Transactions on Computational Social Systems. 1052022</p>
<p>An improved clustering based multi-objective evolutionary algorithm for influence maximization under variablelength solutions. T K Biswas, A Abbasi, R K Chakrabortty, Knowledge-Based Systems. 2561098562022</p>
<p>A bi-population evolutionary algorithm with feedback for energy-efficient fuzzy flexible job shop scheduling. Z Pan, D Lei, L Wang, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 5282021</p>
<p>An evolutionary multitasking memetic algorithm for multi-objective distributed heterogeneous welding flow shop scheduling. R Li, L Wang, W Gong, F Ming, IEEE Transactions on Evolutionary Computation. 2024</p>
<p>Deep reinforcement learning for multi-objective combinatorial optimization: A case study on multi-objective traveling salesman problem. S Li, F Wang, Q He, X Wang, Swarm and Evolutionary Computation. 831013982023</p>
<p>Evolutionary scheduling: A review. E Hart, P Ross, D Corne, Genetic Programming and Evolvable Machines. 62005</p>
<p>Adaptive ant colony optimization with node clustering for the multi-depot vehicle routing problem. P Stodola, J Nohel, IEEE Transactions on Evolutionary Computation. 2022</p>
<p>Identifying influential spreaders in social networks through discrete moth-flame optimization. L Wang, L Ma, C Wang, N -G. Xie, J M Koh, K H Cheong, IEEE Transactions on Evolutionary Computation. 2021</p>
<p>Multi-domain evolutionary optimization of network structures. J Zhao, K H Cheong, Y Jin, arXiv:2406.148652024arXiv preprint</p>
<p>A survey on multimodal large language models. S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, E Chen, National Science Review. e4032024</p>
<p>Iron: Private inference on transformers. M Hao, H Li, H Chen, P Xing, G Xu, T Zhang, Advances in neural information processing systems. 202235</p>
<p>Scale-free networks: a decade and beyond. A.-L Barabási, science. 32559392009</p>
<p>Finding community structure in very large networks. A Clauset, M E Newman, C Moore, Physical Review E. 706661112004</p>
<p>Network community detection via neural embeddings. S Kojaku, F Radicchi, Y.-Y Ahn, S Fortunato, Nature Communications. 15194462024</p>
<p>Ga-based q-attack on community detection. J Chen, L Chen, Y Chen, M Zhao, S Yu, Q Xuan, X Yang, IEEE Transactions on Computational Social Systems. 632019</p>
<p>Surrogate-assisted robust optimization of large-scale networks based on graph embedding. S Wang, J Liu, Y Jin, IEEE Transactions on Evolutionary Computation. 2442019</p>
<p>A scalable parallel coevolutionary algorithm with overlapping cooperation for large-scale networkbased combinatorial optimization. W.-J Qiu, X.-M Hu, A Song, J Zhang, W.-N Chen, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2024</p>
<p>Multiobjective combinatorial optimization using a single deep reinforcement learning model. Z Wang, S Yao, G Li, Q Zhang, IEEE Transactions on Cybernetics. 2023</p>
<p>Evolutionary multimodal multiobjective optimization for traveling salesman problems. Y Liu, L Xu, Y Han, X Zeng, G G Yen, H Ishibuchi, IEEE Transactions on Evolutionary Computation. 2023</p>
<p>Deep reinforcement learning assisted genetic programming ensemble hyper-heuristics for dynamic scheduling of container port trucks. X Chen, R Bai, R Qu, J Dong, Y Jin, IEEE Transactions on Evolutionary Computation. 2024</p>
<p>Cooperative double-layer genetic programming hyper-heuristic for online container terminal truck dispatching. X Chen, R Bai, R Qu, H Dong, IEEE Transactions on Evolutionary Computation. 2022</p>
<p>A bilevel evolutionary algorithm for largescale multiobjective task scheduling in multiagile earth observation satellite systems. F Yao, Y Chen, L Wang, Z Chang, P.-Q Huang, Y Wang, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2024</p>
<p>Multitask multiobjective genetic programming for automated scheduling heuristic learning in dynamic flexible job-shop scheduling. F Zhang, Y Mei, S Nguyen, M Zhang, IEEE Transactions on Cybernetics. 2022</p>
<p>A computationally efficient evolutionary algorithm for multiobjective network robustness optimization. S Wang, J Liu, Y Jin, IEEE Transactions on Evolutionary Computation. 2532021</p>
<p>Enhancing the robustness of networks against multiple damage models using a multifactorial evolutionary algorithm. S Wang, Y Jin, M Cai, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2023</p>
<p>An evolutionary multiobjective framework for complex network reconstruction using community structure. K Wu, J Liu, X Hao, P Liu, F Shen, IEEE Transactions on Evolutionary Computation. 2522020</p>
<p>A multiobjective evolutionary approach for solving large-scale network reconstruction problems via logistic principal component analysis. C Ying, J Liu, K Wu, C Wang, IEEE Transactions on Cybernetics. 2021</p>
<p>An interactive co-evolutionary framework for multi-objective critical node detection on large-scale complex networks. L Zhang, H Zhang, H Yang, Z Liu, F Cheng, IEEE Transactions on Network Science and Engineering. 2023</p>
<p>Higher-order knowledge transfer for dynamic community detection with great changes. H Ma, K Wu, H Wang, J Liu, IEEE Transactions on Evolutionary Computation. 2023</p>
<p>Mumi: Multitask module identification for biological networks. W Chen, Z Zhu, S He, IEEE Transactions on Evolutionary Computation. 2442019</p>
<p>Multilayer network community detection: A novel multiobjective evolutionary algorithm based on consensus prior information [feature]. C Gao, Z Yin, Z Wang, X Li, X Li, IEEE Computational Intelligence Magazine. 1822023</p>
<p>Enhanced epidemic control: Community-based observer placement and source tracing. J Zhao, K H Cheong, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2025</p>
<p>X Wu, S -H. Wu, J Wu, L Feng, K C Tan, arXiv:2401.10034Evolutionary computation in the era of large language model: Survey and roadmap. 2024arXiv preprint</p>
<p>Evolutionary multi-objective optimization of large language model prompts for balancing sentiments. J Baumann, O Kramer, International Conference on the Applications of Evolutionary Computation. Part of EvoStar</p>
<p>Large language model-based evolutionary optimizer: Reasoning with elitism. S Brahmachary, S M Joshi, A Panda, K Koneripalli, A K Sagotra, H Patel, A Sharma, A D Jagtap, K Kalyanaraman, arXiv:2403.020542024arXiv preprint</p>
<p>Llm guided evolutionthe automation of models advancing models. C Morris, M Jurado, J Zutty, arXiv:2403.114462024arXiv preprint</p>
<p>LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics. N Van Stein, T Bäck, IEEE Transactions on Evolutionary Computation. 2024</p>
<p>A comparison of large language models and genetic programming for program synthesis. D Sobania, J Petke, M Briesch, F Rothlauf, IEEE Transactions on Evolutionary Computation. 2024</p>
<p>Language model crossover: Variation through few-shot prompting. E Meyerson, M J Nelson, H Bradley, A Gaier, A Moradi, A K Hoover, J Lehman, arXiv:2302.121702023arXiv preprint</p>
<p>Enhancing genetic improvement mutations using large language models. A E Brownlee, J Callan, K Even-Mendoza, A Geiger, C Hanna, J Petke, F Sarro, D Sobania, International Symposium on Search Based Software Engineering. Springer2023</p>
<p>Large language models as evolutionary optimizers. S Liu, C Chen, X Qu, K Tang, Y.-S Ong, arXiv:2310.190462023arXiv preprint</p>
<p>Large language model for multi-objective evolutionary optimization. F Liu, X Lin, Z Wang, S Yao, X Tong, M Yuan, Q Zhang, arXiv:2310.125412023arXiv preprint</p>
<p>Protein design by directed evolution guided by large language models. T V T Tran, T S Hy, IEEE Transactions on Evolutionary Computation. 2024</p>
<p>What do llms know about financial markets? a case study on reddit market sentiment analysis. X Deng, V Bashlovkina, F Han, S Baumgartner, M Bendersky, Companion Proceedings of the ACM Web Conference 2023. 2023</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J Ruiz, J S Ellenberg, P Wang, O Fawzi, Nature. 62579952024</p>
<p>Toward mitigating misinformation and social media manipulation in llm era. Y Zhang, K Sharma, L Du, Y Liu, Companion Proceedings of the ACM on Web Conference 2024. 2024</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, ACM SIGKDD Explorations Newsletter. 2522024</p>
<p>J Tang, Y Yang, W Wei, L Shi, L Xia, D Yin, C Huang, arXiv:2402.16024Higpt: Heterogeneous graph language model. 2024arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Can language models solve graph problems in natural language?. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, Advances in Neural Information Processing Systems. 202436</p>
<p>Gita: Graph to visual and textual integration for vision-language graph reasoning. Y Wei, S Fu, W Jiang, Z Zhang, Z Zeng, Q Wu, J Kwok, Y Zhang, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>VisionGraph: Leveraging large multimodal models for graph theory problems in visual context. Y Li, B Hu, H Shi, W Wang, L Wang, M Zhang, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning202427919</p>
<p>How multimodal integration boost the performance of llm for optimization: Case study on capacitated vehicle routing problems. Y Huang, W Zhang, L Feng, X Wu, K C Tan, arXiv:2403.017572024arXiv preprint</p>
<p>Eyeballing combinatorial problems: A case study of using multimodal large language models to solve traveling salesman problems. M Elhenawy, A Abdelhay, T I Alhadidi, H I Ashqar, S Jaradat, A Jaber, S Glaser, A Rakotonirainy, arXiv:2406.068652024arXiv preprint</p>
<p>A multi-transformation evolutionary framework for influence maximization in social networks. C Wang, J Zhao, L Li, L Jiao, J Liu, K Wu, IEEE Computational Intelligence Magazine. 1812023</p>
<p>Eriue: Evidential reasoning-based influential users evaluation in social networks. T Wen, Y -W. Chen, T Syed, T Wu, Omega. 1221029452024</p>
<p>Efficient minimum cost seed selection with theoretical guarantees for competitive influence maximization. W Hong, C Qian, K Tang, IEEE Transactions on Cybernetics. 2020</p>
<p>Community opinion maximization in social networks. Y Liu, Q Zhang, Z Wang, IEEE Transactions on Evolutionary Computation. 2024</p>
<p>Simulated annealing based influence maximization in social networks. Q Jiang, G Song, C Gao, Y Wang, W Si, K Xie, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201125</p>
<p>Hallucination is inevitable: An innate limitation of large language models. Z Xu, S Jain, M Kankanhalli, arXiv:2401.118172024arXiv preprint</p>
<p>Robustness and resilience of complex networks. O Artime, M Grassia, M De Domenico, J P Gleeson, H A Makse, G Mangioni, M Perc, F Radicchi, Nature Reviews Physics. 622024</p>            </div>
        </div>

    </div>
</body>
</html>
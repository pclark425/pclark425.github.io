<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9712 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9712</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9712</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-273187056</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.03775v2.pdf" target="_blank">B EYOND CORRELATION : T HE IMPACT OF HUMAN UN - CERTAINTY IN MEASURING THE EFFECTIVENESS OF AUTOMATIC EVALUATION AND LLM-AS - A - JUDGE</a></p>
                <p><strong>Paper Abstract:</strong> The effectiveness of automatic evaluation of generative models is typically measured by comparing the labels generated via automation with labels by humans using correlation metrics. However, metrics like Krippendorff's $\alpha$ and Randolph's $\kappa$ were originally designed to measure the reliability of human labeling, thus make assumptions about typical human labeling behavior, and these assumptions may not be applicable to machine generated labels. In this paper, we show how *relying on a single aggregate correlation score* can obscure fundamental differences between human labels and those from automatic evaluation, including LLM-as-a-Judge. Specifically, we demonstrate that when the proportion of samples with variation or uncertainty in human assigned labels is relatively high, machine labels (generated by automatic evaluation methods) may superficially appear to have similar or better correlation with the human majority label compared to the human-to-human (HH) correlation. This can create the illusion that labels from automatic evaluation approximates the human majority label. However, as the proportion of samples with consistent human labels increases, the correlation between machine and human labels fall well below HH correlation. Based on these findings, we first propose stratifying data by human label uncertainty to provide a more robust analysis of automatic evaluation performance. Second, recognizing that uncertainty and variation are inherent in perception-based human evaluations, such as those involving attitudes or preferences, we introduce a new metric - binned Jensen-Shannon Divergence for perception for such scenarios to better measure the effectiveness of automatic evaluations. We present visualization techniques -- perception charts, to contextualize correlation measures appropriately. We have open-sourced at https://github.com/amazon-science/BeyondCorrelation.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9712.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9712.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LJ-Summarization (SummEval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge applied to SummEval summarization evaluation (G-Eval / Claude Sonnet 3 / Mistral Large v2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of LLM-as-a-Judge (G-Eval / Claude Sonnet 3 / Mistral) against expert human annotators on Likert (1–5) summary quality criteria (coherence, relevance, fluency, consistency); analysis stratified by human-label uncertainty and using correlation metrics plus the proposed binned JSD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Summarization (quality ratings: coherence, relevance, fluency, consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>G-Eval (GPT-4 results reused), Claude Sonnet 3, Mistral Large v2</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompted LLMs (prompts reused from G-Eval) asked to rate summaries on Likert scales (1–5) per criterion; machine medians and distributions computed; M sampling settings for MM correlation used temperature=1.0, top-p=0.8, 20 samples per example for MM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>SummEval: 1,600 samples; each item labeled by 3 expert annotators with Likert 1–5 for criteria (coherence, relevance, fluency, consistency); aggregated via median and percentage agreement used for stratification.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's-α (ordinal), Spearman's ρ, Kendall's τ, % agreement, and proposed binned Jensen-Shannon Divergence (JS_b). Example reported values cited in paper: Mistral Spearman's ρ ≈ 0.56 and Krippendorff's-α ≈ 0.49 on some criteria; differences in metrics when using median vs mean (ρ: 0.49 → 0.60). (See Table 2 and Sec.3.2)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Losses include: (1) loss of human perceptual distribution — LJs compress or bias ratings (e.g., under-represent extreme scores), (2) degradation in agreement when human consensus is high (LJ diverges under high HH certainty), (3) inability to reflect intra-item human variability (single-value summaries hide distributional differences), and (4) systematic, reproducible errors from LJs (not the random noise of humans) which IRA metrics are ill-equipped to detect.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Mistral rarely assigns rating 1 and assigns negligible 5s while humans assign >24% median=5 (visualized in Fig.3); Krippendorff's-α for some criteria low (e.g., α=0.25 rejected) despite moderate Spearman ρ, showing metric-dependent misinterpretation; aggregation via median vs mean shifts correlations substantially (median vs mean ρ: 0.49 vs 0.60) revealing that LJs do not preserve distributional shape of human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Under high human-label uncertainty (many ties / low HH correlation) LJs can appear to match or even 'outperform' HH correlation (e.g., HM correlation can superficially exceed HH in noisy bins). The paper notes prompts reused from prior work and that LJs are prompt-sensitive, so reported LJ performance should not be treated as a final ranking of models.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sec.3.1, Sec.3.2, Table 2, Fig.3; discussion around SummEval (Sec. 'Datasets' and 'RQ1/RQ2')</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'B EYOND CORRELATION : T HE IMPACT OF HUMAN UN - CERTAINTY IN MEASURING THE EFFECTIVENESS OF AUTOMATIC EVALUATION AND LLM-AS - A - JUDGE', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9712.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9712.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LJ-NLI (MNLI / SNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge applied to Natural Language Inference datasets (MNLI, SNLI) using Llama 3.1 8B, Mistral Large v2, Sonnet 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis comparing LLM-generated labels to human labels on nominal NLI tasks (entailment/neutral/contradiction), stratified by human percentage agreement and number of unique human labels to reveal when LJs diverge from humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Natural Language Inference (nominal labels: entailment / neutral / contradiction)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama 3.1 8B, Mistral Large v2, Sonnet 3 (used as LJs with reused prompts from Liu et al. 2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompted classification into {[[e]], [[n]], [[c]]} using reused NLI prompts; machine-to-machine (MM) correlation obtained via sampling (temperature=1.0, top-p=0.8, 20 samples per example) to maximize LJ variation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>MNLI / SNLI: 10K samples each (MNLI has matched/mismatched splits); each item assigned 5 human labels; percentage agreement (PA) and number of unique labels used to stratify items (e.g., PA=1 means perfect human agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's-α, Fleiss-κ, Randolph's-κ, % agreement. Example numbers (MNLI matched/mismatched, Table 1): All samples HH K-α ≈ 0.70–0.72; MM ≈ 0.96–0.97; H_w M_w (machine vs human majority) ≈ 0.72–0.74; stratified: when PA=1 HH K-α=1.00 while H_w M_w=0.83–0.85 (Δ positive, HH>HM); when PA low, HM can appear better than HH (Δ negative). See Table 1 and Fig.1.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Key differences: (1) LJs produce systematic, reproducible labeling errors (vs humans' random variability) that IRA measures don't detect; (2) LJs fail to match humans when humans have high consensus (loss of fidelity under high human certainty); (3) LJs can artificially inflate apparent agreement on items with high human uncertainty, giving misleading impression that LJs approximate human majority.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 1: for MNLI partitions with PA=1 (fully certain human labels), HH Krippendorff's-α=1.00 while H_w M_w is only ≈0.83 (Δ≈0.17), revealing substantial LJ divergence under high human agreement; synthetic experiment with a random labeler shows even a random judge can appear comparably correlated with majority under high human uncertainty (Fig.2), illustrating how LJ agreement can be illusory in noisy bins.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>In bins with high uncertainty (low HH correlation), HM correlation sometimes exceeds HH correlation (LJ appears comparable or better); behavior consistent across different LLMs tested. Authors stress that these experiments reuse prompts and are not optimizations intended to rank LJs—prompt sensitivity is a caveat.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sec.3.1 (RQ1), Table 1, Fig.1, synthetic random-labeler experiment (Fig.2), 'Datasets' and 'Models' descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'B EYOND CORRELATION : T HE IMPACT OF HUMAN UN - CERTAINTY IN MEASURING THE EFFECTIVENESS OF AUTOMATIC EVALUATION AND LLM-AS - A - JUDGE', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9712.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9712.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LJ-Preference (MT-Bench / pairwise)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge applied to pairwise preference evaluation (MT-Bench; LJ Claude Sonnet adapted and GPT-4 results reused)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of LLM judges with human majority preferences on pairwise model comparisons (which model output is preferred), using Krippendorff's-α, % agreement and the proposed JS_b, and visualized with pairwise HM perception charts to reveal when LJs and humans disagree.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Preference / pairwise model comparison (dialogue / multi-turn conversation evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (results from Zheng et al. 2023 reused), adapted Claude Sonnet (as LJ)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise preference prompts adapted from MT-Bench; in reused GPT-4 LJ data there is typically one LJ judgment per item; for Sonnet the authors adapted prompts and computed Krippendorff's-α and % agreement, with sampling where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>MT-Bench preference tuples with varying numbers of human raters; in this paper the subset used included items with at least 3 human ratings; human majority (most frequent label) used to bin items and compute agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's-α, % agreement, Fleiss-κ, Randolph's-κ, and JS_b. Table 3 reports stratified scores; example: overall HH α and HM α differ with Δ changing sign depending on PA; win-rate differences also reported (LJ win-rate vs human win-rate).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Losses include: (1) LJs can misrepresent minority but meaningful preferences and understate reliable minority choices, (2) LJs may change ranking and absolute correlation numbers depending on which human reference (majority vs single annotator) is used, (3) LJs may give misleadingly good agreement when human preference labels are uncertain, masking systematic LJ errors under high consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Pairwise preference HM perception charts (Fig.4) show cases (e.g., Claude-v1 vs GPT-3.5) where overall majority preference exists but contention patterns differ between humans and LJ (when human majority prefers GPT-3.5 there is almost no contention — visualization reveals reliability differences); Table 3 shows stratified Δ values where HM may seem better than HH under certain uncertainty partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>When one model is overwhelmingly preferred by humans (near-unanimous), LJs' agreement on the winning model may be sufficient for ranking use-cases; metrics that assume uniform label distribution (Randolph's-κ) may behave differently than chance-estimating metrics (Krippendorff's-α / Fleiss-κ). Authors note limitations due to single LJ judgments in some reused GPT-4 data.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sec.3.1, Sec.3.3 (Perception-based preference visualization), Table 3, Fig.4</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'B EYOND CORRELATION : T HE IMPACT OF HUMAN UN - CERTAINTY IN MEASURING THE EFFECTIVENESS OF AUTOMATIC EVALUATION AND LLM-AS - A - JUDGE', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>seeing the big through the small": Can llms approximate human judgment distributions on nli from a few explanations? <em>(Rating: 2)</em></li>
                <li>A closer look into using large language models for automatic evaluation <em>(Rating: 1)</em></li>
                <li>Re-examining system-level correlations of automatic summarization evaluation metrics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9712",
    "paper_id": "paper-273187056",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LJ-Summarization (SummEval)",
            "name_full": "LLM-as-a-Judge applied to SummEval summarization evaluation (G-Eval / Claude Sonnet 3 / Mistral Large v2)",
            "brief_description": "Comparison of LLM-as-a-Judge (G-Eval / Claude Sonnet 3 / Mistral) against expert human annotators on Likert (1–5) summary quality criteria (coherence, relevance, fluency, consistency); analysis stratified by human-label uncertainty and using correlation metrics plus the proposed binned JSD.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Summarization (quality ratings: coherence, relevance, fluency, consistency)",
            "llm_judge_model": "G-Eval (GPT-4 results reused), Claude Sonnet 3, Mistral Large v2",
            "llm_judge_setup": "Prompted LLMs (prompts reused from G-Eval) asked to rate summaries on Likert scales (1–5) per criterion; machine medians and distributions computed; M sampling settings for MM correlation used temperature=1.0, top-p=0.8, 20 samples per example for MM.",
            "human_evaluation_setup": "SummEval: 1,600 samples; each item labeled by 3 expert annotators with Likert 1–5 for criteria (coherence, relevance, fluency, consistency); aggregated via median and percentage agreement used for stratification.",
            "agreement_metric": "Krippendorff's-α (ordinal), Spearman's ρ, Kendall's τ, % agreement, and proposed binned Jensen-Shannon Divergence (JS_b). Example reported values cited in paper: Mistral Spearman's ρ ≈ 0.56 and Krippendorff's-α ≈ 0.49 on some criteria; differences in metrics when using median vs mean (ρ: 0.49 → 0.60). (See Table 2 and Sec.3.2)",
            "losses_identified": "Losses include: (1) loss of human perceptual distribution — LJs compress or bias ratings (e.g., under-represent extreme scores), (2) degradation in agreement when human consensus is high (LJ diverges under high HH certainty), (3) inability to reflect intra-item human variability (single-value summaries hide distributional differences), and (4) systematic, reproducible errors from LJs (not the random noise of humans) which IRA metrics are ill-equipped to detect.",
            "examples_of_loss": "Mistral rarely assigns rating 1 and assigns negligible 5s while humans assign &gt;24% median=5 (visualized in Fig.3); Krippendorff's-α for some criteria low (e.g., α=0.25 rejected) despite moderate Spearman ρ, showing metric-dependent misinterpretation; aggregation via median vs mean shifts correlations substantially (median vs mean ρ: 0.49 vs 0.60) revealing that LJs do not preserve distributional shape of human ratings.",
            "counterexamples_or_caveats": "Under high human-label uncertainty (many ties / low HH correlation) LJs can appear to match or even 'outperform' HH correlation (e.g., HM correlation can superficially exceed HH in noisy bins). The paper notes prompts reused from prior work and that LJs are prompt-sensitive, so reported LJ performance should not be treated as a final ranking of models.",
            "paper_reference": "Sec.3.1, Sec.3.2, Table 2, Fig.3; discussion around SummEval (Sec. 'Datasets' and 'RQ1/RQ2')",
            "uuid": "e9712.0",
            "source_info": {
                "paper_title": "B EYOND CORRELATION : T HE IMPACT OF HUMAN UN - CERTAINTY IN MEASURING THE EFFECTIVENESS OF AUTOMATIC EVALUATION AND LLM-AS - A - JUDGE",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LJ-NLI (MNLI / SNLI)",
            "name_full": "LLM-as-a-Judge applied to Natural Language Inference datasets (MNLI, SNLI) using Llama 3.1 8B, Mistral Large v2, Sonnet 3",
            "brief_description": "Analysis comparing LLM-generated labels to human labels on nominal NLI tasks (entailment/neutral/contradiction), stratified by human percentage agreement and number of unique human labels to reveal when LJs diverge from humans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Natural Language Inference (nominal labels: entailment / neutral / contradiction)",
            "llm_judge_model": "Llama 3.1 8B, Mistral Large v2, Sonnet 3 (used as LJs with reused prompts from Liu et al. 2023a)",
            "llm_judge_setup": "Prompted classification into {[[e]], [[n]], [[c]]} using reused NLI prompts; machine-to-machine (MM) correlation obtained via sampling (temperature=1.0, top-p=0.8, 20 samples per example) to maximize LJ variation.",
            "human_evaluation_setup": "MNLI / SNLI: 10K samples each (MNLI has matched/mismatched splits); each item assigned 5 human labels; percentage agreement (PA) and number of unique labels used to stratify items (e.g., PA=1 means perfect human agreement).",
            "agreement_metric": "Krippendorff's-α, Fleiss-κ, Randolph's-κ, % agreement. Example numbers (MNLI matched/mismatched, Table 1): All samples HH K-α ≈ 0.70–0.72; MM ≈ 0.96–0.97; H_w M_w (machine vs human majority) ≈ 0.72–0.74; stratified: when PA=1 HH K-α=1.00 while H_w M_w=0.83–0.85 (Δ positive, HH&gt;HM); when PA low, HM can appear better than HH (Δ negative). See Table 1 and Fig.1.",
            "losses_identified": "Key differences: (1) LJs produce systematic, reproducible labeling errors (vs humans' random variability) that IRA measures don't detect; (2) LJs fail to match humans when humans have high consensus (loss of fidelity under high human certainty); (3) LJs can artificially inflate apparent agreement on items with high human uncertainty, giving misleading impression that LJs approximate human majority.",
            "examples_of_loss": "Table 1: for MNLI partitions with PA=1 (fully certain human labels), HH Krippendorff's-α=1.00 while H_w M_w is only ≈0.83 (Δ≈0.17), revealing substantial LJ divergence under high human agreement; synthetic experiment with a random labeler shows even a random judge can appear comparably correlated with majority under high human uncertainty (Fig.2), illustrating how LJ agreement can be illusory in noisy bins.",
            "counterexamples_or_caveats": "In bins with high uncertainty (low HH correlation), HM correlation sometimes exceeds HH correlation (LJ appears comparable or better); behavior consistent across different LLMs tested. Authors stress that these experiments reuse prompts and are not optimizations intended to rank LJs—prompt sensitivity is a caveat.",
            "paper_reference": "Sec.3.1 (RQ1), Table 1, Fig.1, synthetic random-labeler experiment (Fig.2), 'Datasets' and 'Models' descriptions",
            "uuid": "e9712.1",
            "source_info": {
                "paper_title": "B EYOND CORRELATION : T HE IMPACT OF HUMAN UN - CERTAINTY IN MEASURING THE EFFECTIVENESS OF AUTOMATIC EVALUATION AND LLM-AS - A - JUDGE",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LJ-Preference (MT-Bench / pairwise)",
            "name_full": "LLM-as-a-Judge applied to pairwise preference evaluation (MT-Bench; LJ Claude Sonnet adapted and GPT-4 results reused)",
            "brief_description": "Comparison of LLM judges with human majority preferences on pairwise model comparisons (which model output is preferred), using Krippendorff's-α, % agreement and the proposed JS_b, and visualized with pairwise HM perception charts to reveal when LJs and humans disagree.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Preference / pairwise model comparison (dialogue / multi-turn conversation evaluation)",
            "llm_judge_model": "GPT-4 (results from Zheng et al. 2023 reused), adapted Claude Sonnet (as LJ)",
            "llm_judge_setup": "Pairwise preference prompts adapted from MT-Bench; in reused GPT-4 LJ data there is typically one LJ judgment per item; for Sonnet the authors adapted prompts and computed Krippendorff's-α and % agreement, with sampling where applicable.",
            "human_evaluation_setup": "MT-Bench preference tuples with varying numbers of human raters; in this paper the subset used included items with at least 3 human ratings; human majority (most frequent label) used to bin items and compute agreement.",
            "agreement_metric": "Krippendorff's-α, % agreement, Fleiss-κ, Randolph's-κ, and JS_b. Table 3 reports stratified scores; example: overall HH α and HM α differ with Δ changing sign depending on PA; win-rate differences also reported (LJ win-rate vs human win-rate).",
            "losses_identified": "Losses include: (1) LJs can misrepresent minority but meaningful preferences and understate reliable minority choices, (2) LJs may change ranking and absolute correlation numbers depending on which human reference (majority vs single annotator) is used, (3) LJs may give misleadingly good agreement when human preference labels are uncertain, masking systematic LJ errors under high consensus.",
            "examples_of_loss": "Pairwise preference HM perception charts (Fig.4) show cases (e.g., Claude-v1 vs GPT-3.5) where overall majority preference exists but contention patterns differ between humans and LJ (when human majority prefers GPT-3.5 there is almost no contention — visualization reveals reliability differences); Table 3 shows stratified Δ values where HM may seem better than HH under certain uncertainty partitions.",
            "counterexamples_or_caveats": "When one model is overwhelmingly preferred by humans (near-unanimous), LJs' agreement on the winning model may be sufficient for ranking use-cases; metrics that assume uniform label distribution (Randolph's-κ) may behave differently than chance-estimating metrics (Krippendorff's-α / Fleiss-κ). Authors note limitations due to single LJ judgments in some reused GPT-4 data.",
            "paper_reference": "Sec.3.1, Sec.3.3 (Perception-based preference visualization), Table 3, Fig.4",
            "uuid": "e9712.2",
            "source_info": {
                "paper_title": "B EYOND CORRELATION : T HE IMPACT OF HUMAN UN - CERTAINTY IN MEASURING THE EFFECTIVENESS OF AUTOMATIC EVALUATION AND LLM-AS - A - JUDGE",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "seeing the big through the small\": Can llms approximate human judgment distributions on nli from a few explanations?",
            "rating": 2,
            "sanitized_title": "seeing_the_big_through_the_small_can_llms_approximate_human_judgment_distributions_on_nli_from_a_few_explanations"
        },
        {
            "paper_title": "A closer look into using large language models for automatic evaluation",
            "rating": 1,
            "sanitized_title": "a_closer_look_into_using_large_language_models_for_automatic_evaluation"
        },
        {
            "paper_title": "Re-examining system-level correlations of automatic summarization evaluation metrics",
            "rating": 1,
            "sanitized_title": "reexamining_systemlevel_correlations_of_automatic_summarization_evaluation_metrics"
        }
    ],
    "cost": 0.011587,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>3 Oct 2024</p>
<p>Aparna Elangovan 
KAIST AI</p>
<p>Jongwoo Ko 
KAIST AI</p>
<p>Lei Xu 
KAIST AI</p>
<p>Mahsa Elyasi 
KAIST AI</p>
<p>Ling Liu 
KAIST AI</p>
<p>Sravan Bodapati 
KAIST AI</p>
<p>Dan Roth 
KAIST AI</p>
<p>3 Oct 2024E2FFC85D92AF204F56F0A84B857A99B6arXiv:2410.03775v1[cs.HC]
The effectiveness of automatic evaluation of generative models is typically measured by comparing it to human evaluation using correlation metrics.However, metrics like Krippendorff's α and Randolph's κ, originally designed to measure the reliability of human labeling, make assumptions about human behavior and the labeling process.In this paper, we show how relying on a single aggregate correlation score can obscure fundamental differences between human behavior and automatic evaluation methods, including LLM-as-a-Judge.Specifically, we demonstrate that when the proportion of samples with variation or uncertainty in human labels (gathered during human evaluation) is relatively high, machine labels (generated by automatic evaluation methods) may superficially appear to have similar or better correlation with the human majority label compared to humanto-human (HH) correlation.This can create the misleading impression that automatic evaluation is accurate enough to approximate the human majority label.However, as the proportion of samples with consistent human labels increases, the correlation between machine labels and human majority labels declines, falling below HH correlation.Based on these findings, we first propose stratifying results by human label uncertainty to provide a more robust analysis of automatic evaluation performance.Second, recognizing that uncertainty and variation are inherent in perception-based human evaluations, such as those involving attitudes or preferences, we introduce a new metric -binned Jensen-Shannon Divergence for perception for such scenarios to better measure the effectiveness of automatic evaluations.Third, we present visualization techniques -perception charts, to compare the strengths and limitations of automatic evaluation and to contextualize correlation measures appropriately.</p>
<p>INTRODUCTION</p>
<p>Despite the increasing importance of automatic evaluation for generative large language models (LLMs), measuring and interpreting the effectiveness of these methods remains challenging.Automatic evaluation methods include N -gram-based methods such as Rouge (Lin, 2004) and BERTScore (Zhang* et al., 2020), task-specific model-based methods (TM) such as Align-Score (Zha et al., 2023) and Prometheus (Kim et al., 2024), and LLM-as-a-Judge-based methods (LJ) where a general purpose LLM, using prompts, is instructed to perform evaluation.The use of automatic evaluation is tied to the key question -How can the efficacy of automatic evaluation methods be measured in comparison to human evaluation?A widely accepted practice is to use agreement or rank correlation metrics such as Krippendorff's-α (Krippendorff, 2011), Cohen's-κ (Cohen, 1960;McHugh, 2012), Spearman's ρ, and Kendall's τ -b (Fabbri et al., 2021;Deutsch et al., 2022;Chiang &amp; Lee, 2023;Liu et al., 2023b), where a higher correlation between machine labels (generated by automatic evaluation methods) and human labels (gathered during human evaluation) implies a more effective automatic evaluation.Variation or uncertainty in human labels (Tversky &amp; Kahneman, 1974) is almost impossible to avoid when humans are involved (Kahneman et al., 2021).In this paper, we first highlight how comparing the overall correlation score between human labels and machine labels can obscure key discrepancies in machine generated labels.Therefore, we propose that measurements be stratified by label uncertainty.</p>
<p>Secondly, this problem also leads to a follow-on question about how to interpret uncertainty.Not all types of uncertainties are equal (Kahneman &amp; Tversky, 1982), as they can range from random human errors, ambiguity in tasks, and nature of perception-based evaluation (Elangovan et al., 2024), where some of these variations are inherent to the task while others indicate poor reliability of human labels.For instance, when evaluating aspects such as "How readable is the given content?", the answer relies on human perception and is likely to vary from person to person.In contrast, evaluating a natural language inference (NLI) task, such as "Can you infer the given sentence from a given text?", is less likely to show variations.Acceptable variation in labels, therefore, depends on the task, as variations or uncertainties in human labels can sometimes be a feature rather than a bug in human evaluation.Thus, we explore how to effectively measure and interpret reliability in the context of human uncertainties.Hence, we ask the following questions in this paper</p>
<p>• Research Question 1 -How does uncertainty in human labels impact correlation metrics when we measure the efficacy of automatic evaluation methods?(Sec.3.1) • Research Question 2 -How can we measure human-to-machine (HM) agreement that accounts for human uncertainty as a result of variation in human perception?(Sec.3.2) • Research Question 3 -How can we visualize the underlying data to draw meaningful insights when we compare the results from automatic and human evaluations?(Sec.3.3)</p>
<p>BACKGROUND ON CORRELATION MEASURES</p>
<p>CORRELATION MEASUREMENTS AND ASSUMPTIONS</p>
<p>Correlation measures have traditionally been used to measure the inter-rater agreement (IRA) of human-generated labels, where there is no ground truth available, to measure human-to-human (HH) correlation.There are 2 broad categories of correlation measures, (a) non-chance-adjusted indices and (b) chance-adjusted metrics (Xinshu Zhao &amp; Deng, 2013).Chance-adjusted measures take into account a scenario where raters can choose the same label by chance even when they do not know the correct answer.Chance-adjusted measures, such as Krippendorff's-α are generally more popular IRA measures than non-chance-adjusted metrics such as percentage agreement.The key difference between various chance-adjusted IRA measures is how chance agreement is computed (Meyer et al., 2014;Xinshu Zhao &amp; Deng, 2013).For instance, Randolph's-κ (Randolph, 2005) assumes that all labels are equally likely and assigns a probability of 1 /k for chance agreement,</p>
<p>where k is the number of possible label choices.On the other hand, Fleiss's κ (Fleiss, 1971) and Krippendorff's α calculate chance agreement empirically, based on the actual human labels of the study.These measurements can be useful in cases where certain labels (e.g., 3 on a Likert scale of 1-5) are more likely to be chosen when humans are uncertain.A high proportion of such labels, even if consistent, may indicate unreliability in the labeling process, particularly when the minority labels show lower agreement.Therefore, chance-adjusted correlation measures rely on key assumptions about the types of errors humans are likely to make.Furthermore, Krippendorff's-α, specifies that the agreement coefficient is an indicator of reliability if and only if (1) the raters duplicate the process of rating, i.e., when two independent raters use the same guidelines or instructions and use the same units, (2) raters must be treated as interchangeable, and (3) correct answers are not known (Krippendorff, 2004).Human labels and machine labels are clearly not interchangeable even when using current state-of-the-art LLMs to generate machine labels, as humans and LLMs are fundamentally different.</p>
<p>DIFFERENCES BETWEEN CORRELATION MEASURES AND RAW ACCURACY</p>
<p>The assumptions above, especially the assumption that raters must be treated as interchangeable, raise a fundamental question as to why even use correlation measures to compare machine and human performance, instead of measures such as accuracy and F1 using human labels as ground truth?Firstly, one of the main challenges with using scores like accuracy or F1 is that human labels tend to be noisy regardless of the task or the dataset, where some label choices are more acceptable than others, and measures such as F1 or accuracy do not account for variations in human labels and hence correlation measures appear to be a better measure to compare human vs. machine performance.However, as mentioned previously, many of the common chance-adjusted correlation measures aim to account for random chance agreement in HH agreement, whereas the errors make by machines tend to be more systematic.A key characteristic of random errors is that they are non-reproducible which result in variations when repeated, resulting in low IRA, which in-turn may point to potential problems in the labelling process or complexities in the task.In contrast, systematic errors are consistent, and IRA measures are not designed to detect them.Secondly, as a direct consequence, IRA metrics assume that agreement is good and variance is bad, which implies that there is a single correct answer -a situation that may not hold for ambiguous items or those that gauge user perception.</p>
<p>TYPES OF RAW DATA COLLECTED DURING EVALUATION</p>
<p>The type of the data collected, such as nominal, ordinal, interval, ratio, discrete or continuous data, dictate how such data can be aggregated and interpreted.Nominal and ordinal measures can be considered as qualitative measurements, while continuous values can be interpreted as quantitative (Mishra et al., 2018).The most common types of values collected during automatic or human evaluation are:</p>
<p>Nominal values: Nominal data, such as gender, have no natural ranking or ordering.Such data are primarily used in human evaluation tasks such as fact checking, where the aim is to verify whether a given text can be inferred from reference content (Liu et al., 2023c)  Ordinal values: Ordinal values have an implicit order, e.g., ratings of quality (very good, good, fair, poor, very poor) (Mishra et al., 2018).The most common use of ordinal values in human evaluation are through Likert scales.Likert scales are rating systems of attitude scale to measure the degree to which a person agrees/disagrees with a given statement (Taherdoost, 2019) and commonly to gauge user perception on qualitative aspects such as readability, coherence, and fluency (van der Lee et al., 2021).IRA measures for ordinal values should ideally account for the fact that ratings that are closer to one another (1 vs. 2) for a given item indicate a higher agreement than ratings that are further apart (1 vs. 4).Such IRA measures include Krippendorff's-α for ordinal values, where disagreements are weighted by distance between the ratings.Rank correlations such as Kendall's-τ and Spearman's-ρ are also widely used to measure item-level correlation between machines and humans for ordinal values.However, there's a caveat on using these measures: their ability to handle a relatively large proportion of ties is widely debated, even when measuring system correlations that involve a much smaller proportion of ties (Deutsch et al., 2023).When the number of items to rank is n, the number of ordinal values to assign is r, and when r &lt;&lt; n, it results in many ties or rank collision.For instance, when assigning a Likert scale of 1-5 to 100 items, at least 20% of the items will have the same rank.While ordinal values preserve order, the objective of using ordinal values is not to rank the item in relation to other items, therefore the proportion of ties affects the interpretability and reliability of the rank correlation scores.</p>
<p>Continuous values: Continuous values are typically obtained when using automatic evaluation methods such as Rouge (Lin, 2004) and BERTScore (Zhang* et al., 2020).</p>
<p>ANALYSIS SETTINGS AND RESULTS</p>
<p>Datasets: We use datasets that have multiple human annotations used in typical evaluation tasks for (a) Likert-based ordinal qualitative evaluation, (b) NLI datasets for fact or consistency checking, and (c) preference datasets for understanding human preferences as follows: 1. SummEval (Fabbri et al., 2021) is human evaluation of summary quality using four criteria -fluency, coherence, consistency, and relevance, with Likert scores assigned from 1 to 5.This dataset has 1600 samples, where each item is labeled by 3 expert annotators.2. SNLI (Bowman et al., 2015) is an NLI inference task dataset with 10K samples and 3 labels -entailment, neutral and contradiction.Each item is assigned 5 labels by human annotators.3. MNLI (Williams et al., 2018) is also an NLI inference task dataset with 10K samples and 3 labels -entailment, neutral and contradiction.Each item is assigned 5 labels by human annotators.It contains two splits -matched and mismatched.4. MT-Bench (Zheng et al., 2023) is a preference dataset which compares human preferences of several model outputs on open-ended questions using multi-turn conversations.From this dataset, we only use samples that have at least 3 human ratings.</p>
<p>Models: For SummEval dataset, we reuse the original G-Eval results (Liu et al., 2023b) which rates the quality of summaries on a scale of 1-5.In addition, we also use Claude Sonnet 3 and Mistral
K-HH K-MM K-H w H K-H w M w K-H w R w
Figure 1: Comparing Krippendorff's α (K-α) on subsets of MNLI-mismatched with varying levels of uncertainty, using 3 LLMs -Sonnet, Mistral, and Llama -as labelers.As labels become more certain (towards the right end of X-axis), HH correlations are significantly higher than H w M w .H w H ′ represents the correlation between the human majority and another human annotator (H ′ ).H w R w compares the human majority with a random machine evaluator, serving as baseline performance.</p>
<p>Large v2 on SummEval, re-using the prompts from G-Eval as detailed in Appendix A.2.For NLI and MNLI datasets, we use Llama 3.1 8B, Mistral Large v2 and Sonnet 3, reusing the prompts from Liu et al. (2023a) as detailed in Appendix A.1.For MTBench, we rely on the existing results on preference data on MT-Bench and GPT-4 from Zheng et al. (2023).We also adapt the same prompts (see Appendix A.3) for Sonnet as LJ for MTBench.In order to compute machine-to-machine (MM) correlation, we set the sampling temperature to 1.0 to maximize the variation across runs, and set top p = 0.8.We sample 20 annotations per example.NOTE: We have predominately reused the prompts from existing works primarily relying on GPT-4 as LJ, we have NOT attempted to optimize the prompts per LLM except for minor format changes.Since models are known to be highly sensitive to prompts, our experiments in this paper are not meant to rank the underlying LJ, but to study the impact of human uncertainty regardless of the LJ.</p>
<p>Correlation measures: For nominal data, we report scores on Krippendorff's-α, Fleiss-κ, Randolph's-κ and percentage agreement (McHugh, 2012) also detailed in Appendix A.4.For MT-Bench preference data, since the number of human raters vary per tuple ⟨conversation, turn, model pair⟩ we use Krippendorff-α and percentage agreement to compute HH correlation as it can handle varying number of raters per item unlike Fleiss-κ or Randolph's-κ.For ordinal data, we report Krippendorff-α for ordinal data and commonly used rank correlation measures (despite the caveat highlighted previously as many papers have used them to measure for item-level correlation) such as Spearman's-ρ and Kendall's-τ .</p>
<p>RQ1: HOW DOES UNCERTAINTY IN HUMAN LABELS IMPACT CORRELATION METRICS WHEN WE MEASURE THE EFFICACY OF AUTOMATIC EVALUATION METHODS?</p>
<p>To study the impact of human uncertainty, we stratify samples and then compare HH and HM correlation measurements for each group.We primarily stratify by percentage agreement, which is the proportion of labels that reflect the majority (for nominal data) or the median label (for ordinal data) among all human labels assigned to an item.For example, if 3 out of 5 people assign the same label, the percentage agreement is 60%.We also stratify samples by the number of unique human labels to ensure that our findings are consistent regardless of the stratification method.</p>
<p>At surface level, HM correlation seems to improve with human certainty, as shown in Table 1, (column H w M w ).On closer inspection, another trend emerges where in any given stratified group when the noisy or uncertain samples increases (as measured by low HH correlation), the HM correlation seems to outperform HH correlation as highlighted.However, as the proportion of samples with noisy labels decreases and HH correlation approaches near-perfect agreement, HH correlation is much higher than HM as highlighted .Thus, the illusion that machines (specifically LJs) approximate majority human behavior is largely due to the presence of noisy labels in the data.As a corollary, if LJs were truly approximating the human majority, the ∆ would approach 0 under perfect HH correlation -which is clearly not the case.We also replicated this behavior on SummEval dataset which relies on ordinal labels in Table 2, using correlation metrics such as Krippendorff's-α for ordinal data, Kendall's-τ and Spearman's-ρ.The same patterns are observed on preference data in Table 3.</p>
<p>Partition by human labels (% samples)
Krippendorff's-α % Agreement Fleiss-κ Randoph's-κ Dataset HH MM H w M w ∆ HH H w M w ∆ HH H w M w ∆ HH H w M w ∆ MNLI (matched)
All (100%) 0.70 0.96 0.72 -0.02 0.88 0.82 0.07 0.70 0.72 -0.02 0.70 0.72 -0.03 PA = 1 (58%) 1.00 0.97 0.83 0.17 1.00 0.89 0.11 1.00 0.83 0.17 1.00 0.83 0.17 0.8 ≤ PA &lt; 1 (25%) 0.39 0.95 0.66 -0.27 0.80 0.78 0.02 0.39 0.66 -0.27 0.40 0.67 -0.27 0 ≤ PA &lt; 0.8 (16%) 0.04 0.93 0.37 -0.33 0.60 0.61 -0.01 0.04 0.37 -0.33 0.07 0.42 -0.34 unique = 2 (38%) 0.28 0.95 0.56 -0.28 0.73 0.72 0.01 0.28 0.56 -0.28 0.30 0.58 -0.28 unique = 3 (3%) -0.06 0.91 0.39 -0.45 0.60 0.63 -0.03 -0.06 0.39 -0.45 -0.05 0.44 -0.49</p>
<p>MNLI (mismatched)</p>
<p>All (100%) 0.72 0.97 0.74 -0.02 0.89 0.83 0.07 0.72 0.74 -0.02 0.72 0.74 -0.02 PA = 1 (60%) 1.00 0.98 0.85 0.15 1.00 0.90 0.10 1.00 0.85 0.15 1.00 0.85 0.15 0.8 ≤ PA &lt; 1 (24%) 0.39 0.95 0.65 -0.26 0.80 0.77 0.03 0.39 0.65 -0.26 0.40 0.66 -0.26  0 ≤ PA &lt; 0.8 (14%) 0.04 0.94 0.39 -0.35 0.60 0.62 -0.02 0.04 0.39 -0.35 0.07 0.43 -0  We further randomly sample subsets from NLI datasets (sampling detailed in Appendix 5) and compare correlation scores across subsets with varying levels of human label uncertainty.We repeat this for three different LLMs, and consistently observe the same findings, performance gaps become more apparent as human label consistency increases, shown in Fig. 1.Similar findings for the SummEval dataset are shown in Appendix Fig. 5.</p>
<p>We also attempt to understand the phenomena using a random labeler in a synthetically labeled dataset.The synthetic dataset simulates a nominal binary classification as well as a ordinal label {1,2,3} classification dataset, where the simulator (mimicking 2 humans) assigns 2 labels to each item.The random labeler simply assigns a random label.We plot the correlation of the random labeler with the 2 simulated humans across varying degree of certainty.In this scenario, even a random labeler can appear better under high uncertainty, and it is only when the labels get more certain does the poor relative performance of the random labeler come to light as shown in Fig. 2. Intuitively speaking, for example, if 2 humans disagree, a 3rd random labeler cannot do any worse interms of agreement.The random labeler can only disagree (in which case they are no better than the 2 humans) or agree with the any chosen label.This example further illustrates why stratification by proportion of uncertainty is crucial to uncovering weaknesses in any automatic labeler.</p>
<p>RQ2: HOW CAN WE MEASURE HUMAN-MACHINE AGREEMENT THAT ACCOUNT FOR HUMAN UNCERTAINTY AS A RESULT OF VARIATION IN HUMAN PERCEPTION?</p>
<p>Judging the quality of a free-form model generated output, such as a summary, largely depends on human perception.As a result, the human ratings can vary depending on a wide range of factors  et al. (2023).Furthermore, whether to treat Likert-data as ordinal or interval data dictates the aggregation method -median or mean, is also debated (Joshi et al., 2015).The argument against using Likert-data as interval data is that the points on the scale may not be equidistant, e.g., the distance between pair ⟨neutral, agree⟩ may not be the same as the distance between ⟨agree, strongly agree⟩.We report Spearman's-ρ for both to illustrate the difference in Table 2.While in most cases the two sets of scores look largely the same, there are instances where the numbers are vastly different.For instance, Table 2 Spearman's-ρ median vs. mean , median comparison yields a correlation of 0.49 (low correlation) while comparing the mean, results in 0.60 (indicating moderate correlation) (Mukaka, 2012), provoking the question which number is more representative of the underlying data?Regardless of whether we choose to use Krippendorff-α or a rank correlation metric, one aspect that both assume is that there is a single rating (e.g., median value) that appropriately represents the underlying choice a human rater makes.This assumption may not be correct, especially when the task is inherently subjective, especially when the number of judgments collected per item is relatively low, e.g., 3 ratings per item.This shows a clear gap in existing metrics to compare human and machine performance, where intra-human ratings naturally vary as a a result of subjectivity inherent to the task.</p>
<p>Hence, we propose an additional metric to supplement existing measures, the use of binned Jensen-Shannon divergence (JSD) for perception (Lin, 1991) to quantify how closely automatic methods mimic human judgment.To the best of our knowledge, our approach of using JSD has not been used as a correlation measure to account for human perception, without relying on a single gold label.</p>
<p>The closest work to ours is the use of cumulative JSD to predict the aesthetic score distribution, instead of just the average rating, to represent image quality (Jin et al., 2018).</p>
<p>Formally (source code in Appendix A.7), let H i and M i represent the set of human and machine ratings respectively assigned to the item x i , and n be the total number of the items and R be the set of possible ratings (e.g., 1 to 5 for ordinal ratings, or {A, B, T ie} for nominal model preferences).</p>
<p>To compute the distance between human and machine predictions, we can define the probability mass function (PMF) for a given rating c as follows:
P h r (c) = i∈Br l∈Hi 1 {l=c} i∈Br |H i | (Human), P m r (c) = i∈Br l∈Mi 1 {l=c} i∈Br |M i | (Machine) (1)
where for ordinal values, bin B r := {i ∈ {1, ..n}|median(H i ) = r} contains the set of samples with a median human rating of r.For nominal perception-based preference labels, bin B r := {i ∈ {1, ..n}|mostfrequent(H i ) = r} contains the set of samples with the most frequent human rating of r.</p>
<p>From Equation 1, we can define the binned JSD (JS b ) as follows:
JS b = r∈R |B r | n JS(P h r ||P m r ) := r∈R |B r | n 1 2 KL(P h r ||Q r ) + 1 2 KL(P m r ||Q r ) ,(2)
where
KL(P h r ||Q r ) = c∈R P h r (c) log 2 ( P h r (c)
Qr(c) ) is the Kullback-Leibler divergence (Kullback &amp; Leibler, 1951) and Q r (c) = P h r (c) + P m r (c) /2 for ∀c ∈ R is the average between human and machine PMFs, based on the definition of Jensen-Shannon distance (JS).Since JSD is a distancebased measure, lower scores are better because they indicate that the human and machine judgments are similar.</p>
<p>The binned JSD for ordinal perception data has 2 main advantages, (1) it measures how closely machine generated labels mimic human perception without the need for a single gold label (2) human and machine labels are not treated interchangeably, as the items in a given bin are selected by the human median or majority value.For HM correlation, the proposed approach can be applied at itemlevel when the number of labels collected per item is high enough to form a distribution, however, for judging model generated output even obtaining 3 human labels per item can be expensive.Hence, we suggest computing the distribution distance per bin.The main shortcoming of this approach is that some key item level discrepancies between machines and humans ratings might not be clearly surfaced.For instance, for a given item, if all the humans assign a rating of 1 and machines assign say 4, at an aggregate level this pattern may not be obvious if this pattern is not frequent.Hence, we also recommend reporting correlations stratified by uncertainty in human labels, e.g., for items that have high agreement labels, such as perfect human agreement as shown in Table 2.</p>
<p>24.4</p>
<p>Figure 3: Ordinal HM perception comparison chart: Visualization of human perception vs. machine labels binned by human median rating (LJ Mistral on Coherence): Dial at the top shows the percentage of samples that fall into the bin.Middle scores (median 2-4) have higher human uncertainty, where only 60% (less than 2 out of 3) of the human labels follow the median.The extremes scores (1, 5) have less uncertainty.While Mistral seems to mimic human perception when the human median is 3, it is also biased towards the rating 3 when the human ratings are between 2-4.</p>
<p>RQ3: HOW CAN WE VISUALIZE THE UNDERLYING DATA TO DRAW MEANINGFUL</p>
<p>INSIGHTS WHEN WE COMPARE AUTOMATIC LABELS AND HUMAN LABELS?</p>
<p>PERCEPTION-BASED ORDINAL RATING VISUALIZATION</p>
<p>We propose the "Ordinal HM perception comparison chart" as shown in Fig. 3 to compare humans' perception of the content vs. how machines rate them.Here, we bin the samples by the human median rating and plot the corresponding distribution of human and machine ratings.For instance, say for a grading task using label values between 1-5, each item is rated 3 times by humans.For a sample set of 10 items, the total number of judgments would be 30 (10 items * 3 ratings per item).Let's say 7 items get a median rating of 4, these 7 items would fall in bin H = 4.We then plot the human label distribution for that bin to include all the 21 judgments (7 items * 3 ratings per item = 21), we also plot all the machine labels for the corresponding bin.We repeat this for each of the label from 1-5 as shown in Fig. 3.This allows us to visualize the variation in human perception, thus including all the ratings rather than a single gold rating, and compare how the corresponding machine labels vary.</p>
<p>Correlation numbers are challenging to interpret, and do not provide sufficient insights to the meaning behind the numbers unless supported by visualization.When does a correlation score imply the model is good enough to replace human evaluation as indicated in Table 2? And what are the gaps in the LJ?In Table 2, LJ Mistral achieves a Krippendorff's-α HM of 0.49.Visualization in Fig. 3 shows that the LJ does not rate any item as 1 and negligible amount of items are rated 5, while humans have assigned over 24% of the items a median rating of 5.This shows the key difference between LJ Mistral's judgments and humans.This type of insight can potentially be useful in optimizing the prompts used by the LJ to minimize the gap with humans judgments, demonstrating the importance of effective visualization techniques.</p>
<p>Another key aspect to note is how human uncertainty varies across different median labels.Human labels tend to be more certain when they assign extreme ratings (1, 5) compared to the middle ratings (2-4), demonstrated by the distribution difference between the median H and all H ratings, as shown in Fig. 3. Relatively higher consistency in extreme ratings is a common pattern observed when humans are asked to rate using star-rating schemes and Likert scales, an observation typically reported in recommendation systems (Amatriain et al., 2009).This type of differences in the extent of human uncertainty depending on the rating further demonstrates the deficiencies in assuming a single gold rating for tasks that rely on human perception, clearly illustrating the need for a HM correlation metric that does not assume a single gold label, such as the JS b we proposed in Sec.3.2 in RQ2.</p>
<p>PERCEPTION-BASED PREFERENCE VISUALIZATION</p>
<p>For visualizing pairwise model preferences to compare human and machine judgments, we propose the "Pairwise preference HM perception chart".The concept behind this visualization is similar to the Ordinal HM perception comparison chart, the main difference is that is bins are separated by the most frequent human label for a given item.This visualization also helps understand, for a given model pair, how reliable the human majority is.For instance, in Fig. 4, in the case of ⟨claude-v1, gpt-3.5-turbo⟩,even though the preferred model is Claude ( 38.9% preference), when human majority prefers GPT-3.5 there is almost no contention.We also visualize the preference of LJ Claude Sonnet shown in Appendix Fig. 7.  Evaluation procedures have predominantly circumvented dealing with human uncertainty, except for recent limited works (Chen et al., 2020;Wang et al., 2023;Chen et al., 2024), and have thus relied on the simplified assumption that a single gold label is sufficient.A single gold label is only sufficient when there is little or no ambiguity, while being able to quantify human uncertainty is crucial in measuring the effectiveness of automatic evaluation.The ubiquitous nature of uncertainty is studied in human psychology (Kahneman et al., 2021) and is also illustrated in a widely used MNLI dataset, which was carefully curated with 5 human labels per item.Over 40% of the samples have some degree of uncertainty, as shown in Table 1.In addition, given the challenges in human evaluation, including obtaining consistent labels (Elangovan et al., 2024), recent datasets have started to rely on collecting one human label per item (Bai et al., 2022;Ganguli et al., 2022;Stiennon et al., 2020).This further demonstrates the urgent need to acknowledge that quantifying human uncertainty is essential for measure the effectiveness of automated evaluation.</p>
<p>The machine learning community cannot afford to dismiss uncertainty as simply "poor quality labels from humans", key systematic errors and performance gaps in models become apparent under high human certainty as shown in Sec.3.1.As a corollary, under high uncertainty even a random labeler can appear to have similar or better correlation with humans, as shown in Fig. 2.More importantly, for safety critical applications such as medicine, when humans labels are highly robust and certain, but the corresponding machine assigned labels differ, it can point to serious deficiencies in the automated system.Furthermore, the implications of uncertainty include changes to ranking of the automatic evaluator as well as the corresponding absolute correlation scores as shown in Table 2 and Table 3 depending on the reference human label.We acknowledge that collecting high quality labels from humans is expensive, and that in some cases gathering just one label per item might be adequate, such as when attempting to understand if one LLM is better than the other, provided the humans end up overwhelmingly preferring the output of one LLM over the other.In the case where the performance of two LLMs are close to one another, uncertainty in human judgments may be a signal that says the model outputs are quite similar.Thus, in some cases uncertainty can be a potential indicator of lower quality of labels requiring improvements to the underlying label collection process (Elangovan et al., 2024), in other cases uncertainty might be inevitable and therefore a valuable signal.Under uncertainty, comparison of automatic evaluation to human evaluation is challenging, and traditional correlation measures also fail to adequately account for this.The proposed binned JSD for perception is an effort in that direction so that metrics do not penalize human uncertainty, whilst comparing machine perception or attitudes with humans.</p>
<p>CHALLENGES IN METRICS AND INTERPRETABILITY</p>
<p>Deciding which statistic is appropriate depends on the data and how well it fits the assumptions made by the statistical measure (Eubanks, 2017).IRA metrics were never designed for systematic errors, and any accommodation for errors were based on assumptions about typical human behavior.Errors made by LLMs are rarely predictable, yet they are not random; rather, they are reproducible, making them systematic errors.The unpredictable nature of LLMs makes it difficult to design an effective metric that compares them with humans, given the uncertainty associated with human labels.</p>
<p>Despite the deficiencies in metrics, some metrics may be a better fit, depending on the aim and the results of the study.For instance, in the case of perception-based preference experiments, where the goal is to understand which among a given pair of models is better, and when the result is almost unanimous with one model overwhelmingly preferred over the other, then the agreement in rarer (minority) labels may not matter.Hence, measures such as Krippendorff's-α and Fleiss-κ which estimate chance agreement based on the observed label distribution can result in substantially lower IRA scores even when there is high agreement on the majority label (winning model), whereas Randolph's-κ assumes that the labels are equally distributed and hence is a better fit.For perceptionbased ordinal values, the extreme labels (such as 1 or 5 in the case of Likert 1-5) are usually strong indicators of human preferences, especially when supported by visualization as shown in Fig. 3.These extreme labels, might form the minority case, but might be crucial to interpreting the results.Hence, measures such as Krippendorff's-α and Fleiss-κ estimate chance agreement based on the observed label distribution are a better fit compared to measures such as Randolph's-κ that assumes uniform distribution of all labels.For tasks where a single majority label is not sufficient to represent human preferences, measures that do not assume a single gold label such as the proposed approach -binned JSD for perception, can be a better choice to compare human and machine performance.</p>
<p>In addition, statistical analysis, such as null-hypothesis and significance testing, is essential for determining whether one model outperforms another by random chance.Here, the chance component includes 2 aspects, (1) chance due to the nature of samples in the evaluation set (2) uncertainty in human labels.A third aspect, even harder, is estimating the error rate as a result of systematically unpredictable erroneous labels from any automated evaluator.Future studies should explore these problems, including approaches like resampling (Deutsch et al., 2021).Incorporation of chance in rank correlation is also an important aspect to account for when two models differ in rank, but the corresponding difference in absolute scores is negligible, then the difference in the rank may not be meaningful.</p>
<p>IRA metrics are challenging to interpret, and hence visualization is key to understanding gaps and strengths of any given metric.Effective visualization is a trade-off between plotting every single data point (too much information that is hard to synthesize) and an aggregate view (summarized view where key information might be obscured).The proposed perception charts are a step towards emphasizing how an aggregate number may not be sufficient in capturing the underlying data, depicting how human uncertainty varies across different labels, which in turn affects how machine performance can be interpreted across labels as shown in Fig. 3 and 4.</p>
<p>RECOMMENDATIONS FOR REPORTING EFFECTIVENESS OF AUTOMATIC METHODS</p>
<p>To conclude, comprehensive analysis of performance involves investigation beyond a single aggregate number.To that effect, in the case of comparing automatic evaluation with human evaluation, we recommend the following steps: 1. Stratification by uncertainty levels: As discussed in Sec.3.1, uncertainty in human labels can obfuscate performance gaps between machines and human evaluators.Hence, we strongly recommend stratifying results by uncertainty proportions.2. Multi-metric reporting: If there was no uncertainty, measures such as F1 would have worked.However, as a result of uncertainty, no single metric can capture important insights about every type of data as demonstrated in Sections 3.1, 3.2 and 3.3.Thus, we recommend reporting on multiple metrics belonging to different families, such as chance and non-chance-adjusted measures, so each metric in its own way can assist in bringing the less obvious but critical aspects about the underlying to the forefront.</p>
<p>Visualization of results:</p>
<p>A single non-parametric aggregate metric can rarely capture the entirety of underlying raw data, and hence visualization is key to understanding performance gaps, as discussed in Section 3.3.The proposed perception charts are a step towards making aggregate correlation more interpretable, as well as highlighting the strengths and gaps of the automatic labeler.</p>
<p>A APPENDIX</p>
<p>A.1 PROMPTS USED FOR NLI These prompts are reused from Liu et al. (2023a) You will be presented with a premise and a hypothesis about that premise.You need to decide whether the hypothesis is entailed by the premise by choosing one of the following answers:</p>
<p>[[e]]: The hypothesis follows logically from the information contained in the premise.</p>
<p>[[n]]: It is not possible to determine whether the hypothesis is true or false without further information.</p>
<p>[[c]]: The hypothesis is logically false from the information contained in the premise.</p>
<p>Read the following premise and hypothesis thoroughly and select the correct answer from the three answer labels.</p>
<p>COHERENCE</p>
<p>You will be given one summary written for a news article.</p>
<p>Your task is to rate the summary on one metric.</p>
<p>Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.</p>
<p>Evaluation Criteria:</p>
<p>Coherence (1-5) -the collective quality of all sentences.We align this dimension with the DUC quality question of structure and coherence whereby "the summary should be well-structured and well-organized.The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic."Evaluation Steps:</p>
<ol>
<li>Read the news article carefully and identify the main topic and key points.2. Read the summary and compare it to the news article.Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order .3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.</li>
</ol>
<p>CONSISTENCY</p>
<p>You will be given a news article.You will then be given one summary written for this article.</p>
<p>Your task is to rate the summary on one metric.</p>
<p>Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.</p>
<p>Evaluation Criteria:</p>
<p>Consistency (1-5) -the factual alignment between the summary and the summarized source.A factually consistent summary contains only statements that are entailed by the source document .Annotators were also asked to penalize summaries that contained hallucinated facts.</p>
<p>FLUENCY</p>
<p>You will be given one summary written for a news article.</p>
<p>Your task is to rate the summary on one metric.</p>
<p>Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.</p>
<p>Evaluation Criteria:</p>
<p>Fluency (1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.1: Poor.The summary has many errors that make it hard to understand or sound unnatural.2: Fair.The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.3: Good.The summary has few or no errors and is easy to read and follow.</p>
<p>RELEVANCE</p>
<p>You will be given one summary written for a news article.</p>
<p>Your task is to rate the summary on one metric.</p>
<p>Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.</p>
<p>Evaluation Criteria:</p>
<p>Relevance (1-5) -selection of important content from the source.The summary should include only important information from the source document.Annotators were instructed to penalize summaries which contained redundancies and excess information.</p>
<p>A.5 IMPACT OF LABEL UNCERTAINTY ON ORDINAL SUMMEVAL DATASET</p>
<p>We create multiple subsets of 100 samples, with each subset having a predefined portion of samples with perfect agreement in human labels.We then randomly select samples from the dataset based on this portion.This simulation helps us understand how correlation measurements change with varying levels of human label uncertainty.</p>
<p>Figure 2 :
2
Figure2: Simulating the impact of high uncertainty to compare with a random labeler (R).Under high uncertainty, even a random labeler can appear to have better correlation with a majority (H w ) or median H human label.As the consistency increases, the weakness of the random labeler become evident.</p>
<p>Figure 4 :
4
Figure 4: Pairwise preference HM perception chart: Human preference distribution vs. LJ GPT-4 for a given majority label per model pair.T is tie.Compares model pair (Left) Claude-v1 (A) vs. GPT3.5-turbo(B) (Right) Alpaca-13B (A) vs. GPT-3.5-turbo(B).Note: Using GPT-4 LJ has only one judgment per item in the results from Zheng et al. (2023), hence M w and M are the same.</p>
<p>from "[[e]]", "[[n]]", "[[c]]".Only write the answer, do not write reasons.A.2 PROMPTS USED FOR SUMMEVALNote: These prompts are reused fromLiu et al. (2023b) used in G-Eval.</p>
<p>output the score only:</p>
<p>the news article carefully identify the main facts and details it presents.2. Read the summary and compare it to the article.Check if the summary contains any factual errors that are not supported by the article.3. Assign a score for consistency based on the Evaluation Criteria.</p>
<p>Figure 5: Impact of noise on ordinal dataset Summeval</p>
<p>Figure 6 :
6
Figure 6: Visualizing correlation with human median rating for relevance: (Left) Rouge-1 (Right) LJ (Mistral).Note: Rouge scores are continuous values [0,1], humans and LJ use integer values 1-5.</p>
<p>and usually is modeled as a binary classification task.Preference judgments such as "Is the output of model-A or Model-B bet-</p>
<p>ter?" are also common examples of nominal values collected during human evaluation.IRA measures for nominal values include percentage agreement (McHugh, 2012), Randolph's-κ, Fleiss's-κ, Krippendorff's-α and Cohen's-κ.</p>
<p>Table 1 :
1
Compare HM and HH correlation measurements on NLI datasets using Sonnet to generate machine labels.The overall correlation scores (All) indicate that the HM correlation is similar to HH.However, when stratifying by percentage agreement (PA) or human unique labels, the difference (∆ = HH − H w M w ) is highest when the all human labels are the same (PA=1), and H w M w outperforms HH when certainty is the lowest.H w and M w are human and machine majority label.
.36</p>
<p>Table 2 :
2
Results on SummEval dataset for criteria -Coherence and Relevance, H is the human median rating and M is machine median.∆ = (HH − HM ).Impact of human uncertainty: When the human label uncertainty is high , HM can seem better than HH correlation and vice versa under low uncertainty .Does the correlation score (ρ of 0.56 , α 0.49) imply the LJ is good enough to replace human evaluation?Spearman's-ρ median vs. mean : Median vs. mean results in an 11 points boost 0.49 → 0.60.Differences in correlation metrics: Krippendorff's-α of 0.25 (reject results), while Spearman's-ρ shows 0.53 (moderate correlation).LJ ranking and absolute correlation numbers change depending on reference human under high uncertainty, Mistral best performance 0.42 replaced by Sonnet 0.22 when reference human is H ′ .
Rouge1ModelKrippendorff's-αJS bSpearman's-ρKendall's-τCPartition (% samples) ρτHH MMHM∆ H ′ MHMHM H ′ M H µ M µHM H ′ MG-Eval0.55 0.740.310.250.270.390.530.430.570.470.381. All (100%)0.18 0.14Mistral0.55 0.78 0.490.070.42 0.360.560.480.620.490.42Sonnet0.55 0.830.240.320.210.510.340.310.410.300.28Coh2. PA = 1 (12%)0.36 0.28G-Eval Mistral1.00 0.78 1.00 0.790.25 0.350.75 0.650.25 0.350.70 0.690.61 0.620.61 0.620.64 0.680.55 0.560.55 0.56Sonnet1.00 0.860.240.760.240.710.430.430.480.390.39G-Eval0.19 0.700.32 -0.130.160.350.560.310.580.520.273. PA &lt; 60% (21%)0.17 0.13Mistral0.19 0.780.55 -0.370.360.300.570.390.640.520.34Sonnet0.19 0.850.170.020.140.500.400.310.450.370.28G-Eval0.40 0.74 0.250.150.220.37 0.530.420.590.470.371. All (100%)0.33 0.26Mistral0.40 0.790.380.020.300.310.540.420.610.490.38Sonnet0.40 0.790.350.050.290.380.390.340.480.360.31Rel2. PA = 1 (14%)0.38 0.30G-Eval Mistral1.00 0.76 1.00 0.800.30 0.430.70 0.570.30 0.430.63 0.590.69 0.710.69 0.710.73 0.720.63 0.650.63 0.65Sonnet1.00 0.770.360.640.360.51 0.490.490.600.450.45G-Eval -0.09 0.690.18 -0.270.090.330.400.170.420.380.153. PA &lt; 60% (19%)0.28 0.22Mistral -0.09 0.740.32 -0.410.160.28 0.420.190.46 0.400.17Sonnet-0.09 0.750.29 -0.380.160.43 0.360.220.40 0.350.20Krippendorff's-α% AgreementJSbFleiss-κRandolph-κLJ WinrateMP (A vs B) H ⟨Alp, Gp3⟩ LJ Partition (# samples) Human HH MM H w M w ∆ GPT 1. All (52) B 0.79 0.24 -0.32 -0.08 0.29 0.81 0.77 2. PA = 1 (26) B 0.92 1.00 --0.05 1.05 -0.05 1.00 0.85 3. PA: [60,80) (18) B 0.61 -0.04 -0.36 -0.40 0.28 0.68 0.670.75 0.11 0.85 0.13 -0.07 -0.07 0.31 0.28 0.61 0.22 0.34 0.260.65 0.77 0.500.62 B 0.81 0.02 0.77 B 0.92 0.00 0.42 B 0.67 0.061. All (52)B 0.79 0.24 0.90 0.010.23 0.11 0.81 0.730.73 0.180.000.100.600.60 B 0.90 0.12Sonnet2. PA = 1 (26)B 0.92 1.00 1.00 -0.02 1.02 -0.02 1.00 0.920.92 0.17 -0.04 -0.040.850.85 B 1.00 0.083. PA: [60,80) (18)B 0.61 -0.04 0.93 -0.04 0.000.18 0.68 0.500.61 0.17 -0.070.160.250.42 B 0.78 0.171. All (36)A 0.39 0.40-0.54-0.14 0.34 0.79 0.690.56 0.110.540.330.540.33 A 0.39 0.00GPT2. PA = 1 (14)B 0.64 1.00-0.250.750.25 1.00 0.570.57 0.100.220.220.360.36 B 0.57 0.07⟨Cld, Gp3⟩3. PA: [60,80) (21)A 0.52 -0.02-0.58 -0.60 0.35 0.67 0.760.57 0.120.570.340.640.36 A 0.52 0.001. All (36)A 0.39 0.40 0.78 0.360.05 0.18 0.79 0.580.47 0.170.350.170.380.21 A 0.56 0.17Sonnet2. PA = 1 (14)B 0.64 1.00 0.56 -0.07 1.07 -0.07 1.00 0.430.43 0.24 -0.11 -0.110.140.14 B 0.64 0.003. PA: [60,80) (21)A 0.52 -0.02 0.86 0.37 -0.39 0.09 0.67 0.670.48 0.190.350.070.500.21 A 0.67 0.141. All (38)B 0.53 0.26-0.27-0.01 0.22 0.74 0.610.61 0.180.260.210.410.41 B 0.74 0.21GPT2. PA = 1 (11)B 0.82 1.00-0.640.360.64 1.00 0.910.91 0.160.630.630.860.86 B 0.91 0.09⟨Gp3, Gp4⟩3. PA: [60,80) (20)B 0.50 0.08-0.24 -0.17 0.12 0.68 0.550.50 0.21 0.220.100.330.25 B 0.65 0.151. All (38)B 0.53 0.26 0.76 0.200.06 0.15 0.74 0.500.50 0.130.190.140.250.25 B 0.45 0.08Sonnet2. PA = 1 (11)B 0.82 1.00 0.66 0.190.810.19 1.00 0.640.64 0.280.150.150.450.45 B 0.64 0.183. PA: [60,80) (20)B 0.50 0.08 0.81 0.22 -0.15 0.19 0.68 0.500.50 0.06 0.200.170.250.25 B 0.45 0.05
′ M w HH H w M w H ′ M w HM H w M w H ′ M w H w M w H ′ M w W WR ∆</p>
<p>Table 3 :
3
Stratified performance on preference data with at least 3 human ratings from MT-Bench for model pairs (MP).Impact of human label uncertainty: When the HH label uncertainty is high, H w M w can seem better than HH correlation and vice versa.IRA vs. Win-rate: While the difference in win-rate between model and human (∆) can be smaller in one evaluator model (e.g., Sonnet 0.08) compared to another (e.g., GPT 0.21), the H w M w IRA can tell an opposite story, as in the example above, GPT has better correlation with human majority across all correlation metrics except JS b .LJ ranking and absolute correlation numbers change depending on reference human H ′ , mimicking single annotator, under high uncertainty compared to using consensus human majority (H w ).</p>
<p>(Amidei et al., 2019)kes to their emotional state(Elangovan et al., 2024).Perception-based ordinalvalue evaluation schemes, such as Likert scores, pose 2 main challenges.(1)It is an attitude-based scale, and since human attitudes vary, metrics like Krippendorff's α, which penalize variation, can produce scores lower than the minimum acceptable value(Amidei et al., 2019).(2) Rank correlation measures such as Kendall-τ or Spearman's-ρ are not entirely appropriate given the large number of ties when comparing median scores, as previously discussed in Sec.2.3 and in the works by Deutsch</p>
<p>ACKNOWLEDGMENTSWe would like to thank Saab Mansour for reviewing our paper and providing valuable feedback.Evaluation Steps:1. Read the summary and the source document carefully.2. Compare the summary to the source document and identify the main points of the article.3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.4. Assign a relevance score from 1 to 5.2012), for n itemswhere x 1 , x 2 , . . ., x n represent each item -that is labeled, with R raters and C categories, as the maximum percentage of votes the most frequent item gets as long as it is greater than 1.Formally:(3) where x r i is an annotation for x i by the rth rater.Note that the 1 A is the indicator function that returns 1 if condition A is true and 0 otherwise.Published as a conference paper at ICLR 2025 A.7 CODE FOR BINNED-JSD-PERCEPTION import itertools import statistics from collections import Counter from statistics import mode from scipy.spatial import distance def compute_binned_js(df, human_labels_column, machine_labels_column, bin_type='median'):""" Compute JS @param df: DataFrame containing the columns human_labels_bin_col, human_labels_column, machine_labels_column @param human_labels_column: This is the name of the column containing human labels.Each value in this column is a list @param machine_labels_column: This is the name of the column containing machine labels.Each value in this column is a list @param bin_type: The type of binning, either median or majority depending on the type of data """ result_weighted_js = 0
I like it... i like it not: Evaluating user ratings noise in recommender systems. Xavier Amatriain, M Josep, Nuria Pujol, Oliver, User Modeling, Adaptation, and Personalization. Geert-Jan Houben, Gord Mccalla, Fabio Pianesi, Massimo Zancanaro, Berlin, Heidelberg; Berlin HeidelbergSpringer2009</p>
<p>Agreement is overrated: A plea for correlation to assess human evaluation reliability. Jacopo Amidei, Paul Piwek, Alistair Willis, 10.18653/v1/W19-8642Proceedings of the 12th International Conference on Natural Language Generation. Chenghua Kees Van Deemter, Hiroya Lin, Takamura, the 12th International Conference on Natural Language GenerationTokyo, JapanAssociation for Computational LinguisticsOctober-November 2019</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2015</p>
<p>seeing the big through the small": Can llms approximate human judgment distributions on nli from a few explanations?. Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert Litschko, Anna Korhonen, Barbara Plank, 2024</p>
<p>Association for Computational Linguistics. Tongfei Chen, Zhengping Jiang, Adam Poliak, Keisuke Sakaguchi, Benjamin Van Durme, 10.18653/v1/2020.acl-main.774Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsOnlineJuly 2020Uncertain natural language inference</p>
<p>A closer look into using large language models for automatic evaluation. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.findings-emnlp.599Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>A coefficient of agreement for nominal scales. Jacob Cohen, 10.1177/001316446002000104Educational and Psychological Measurement. 2011960</p>
<p>A Statistical Analysis of Summarization Evaluation Metrics Using Resampling Methods. Daniel Deutsch, Rotem Dror, Dan Roth, 10.1162/tacl_a_00417Transactions of the Association for Computational Linguistics. 2307-387X910 2021</p>
<p>Re-examining system-level correlations of automatic summarization evaluation metrics. Daniel Deutsch, Rotem Dror, Dan Roth, 10.18653/v1/2022.naacl-main.442Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Marine Carpuat, Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesJuly 2022Association for Computational Linguistics</p>
<p>ConSiDERS-thehuman evaluation framework: Rethinking human evaluation for generative large language models. Daniel Deutsch, George Foster, Markus Freitag, Ling Liu, Lei Xu, Sravan Babu Bodapati, Dan Roth, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2023. August 20241Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration</p>
<p>(re) visualizing rater agreement: Beyond single-parameter measures. David Eubanks, Journal of Writing Analytics. 12017</p>
<p>SummEval: Re-evaluating Summarization Evaluation. Alexander R Fabbri, Wojciech Kryściński, Bryan Mccann, Caiming Xiong, Richard Socher, Dragomir Radev, 10.1162/tacl_a_00373Transactions of the Association for Computational Linguistics. 2307-387X904 2021</p>
<p>Measuring nominal scale agreement among many raters. L Joseph, Fleiss, 10.1037/h0031619Psychological Bulletin. 7651971</p>
<p>Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, arXiv:2209.07858Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'18/IAAI'18/EAAI'18. the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'18/IAAI'18/EAAI'18AAAI Press2022. 2018arXiv preprintPredicting aesthetic score distribution through cumulative jensen-shannon divergence</p>
<p>Likert scale: Explored and explained. Ankur Joshi, Saket Kale, 10.9734/BJAST/2015/14975Satish Chandel, and Dinesh Pal. 72015</p>
<p>Variants of uncertainty. Daniel Kahneman, Amos Tversky, Cognition. 1121982</p>
<p>Noise: a flaw in human judgment. Olivier Daniel Kahneman, Cass R Sibony, Sunstein, 2021Little, Brown Spark, New Yorkfirst edition edition</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Reliability in content analysis: Some common misconceptions and recommendations. Klaus Krippendorff, Human communication research. 3032004</p>
<p>Computing krippendorff's alpha-reliability. Klaus Krippendorff, 201159901023</p>
<p>On information and sufficiency. The annals of mathematical statistics. Solomon Kullback, Richard A Leibler, 195122</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Association for Computational Linguistics. Barcelona, SpainJuly 2004Text Summarization Branches Out</p>
<p>Divergence measures based on the shannon entropy. J Lin, 10.1109/18.61115IEEE Transactions on Information Theory. 3711991</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023aarXiv preprint</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeDecember 2023bAssociation for Computational Linguistics</p>
<p>Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev, doi: 10.18653Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 2023c1</p>
<p>URL. </p>
<p>Interrater reliability: the kappa statistic. Mary L Mchugh, Biochem. Med. (Zagreb). 2232012</p>
<p>DKPro agreement: An open-source Java library for measuring inter-rater agreement. Christian M Meyer, Margot Mieskes, Christian Stab, Iryna Gurevych, Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations. Lamia Tounsi, Rafal Rak, COLING 2014, the 25th International Conference on Computational Linguistics: System DemonstrationsDublin, IrelandDublin City University and Association for Computational LinguisticsAugust 2014</p>
<p>Scales of measurement and presentation of statistical data. Prabhaker Mishra, C M Pandey, Uttam Singh, Anshul Gupta, Ann. Card. Anaesth. 214October 2018</p>
<p>Statistics corner: A guide to appropriate use of correlation coefficient in medical research. M M Mukaka, Malawi Med. J. 243September 2012</p>
<p>Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss' fixed-marginal multirater kappa. Online submission. Randolph Justus, 2005</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>What Is the Best Response Scale for Survey and Questionnaire Design; Review of Different Lengths of Rating Scale / Attitude Scale / Likert Scale. Post-Print hal-02557308, HAL. Hamed Taherdoost, 10.1126/science.185.4157.1124Amos Tversky and Daniel Kahneman. 2019. 1974185Judgment under uncertainty: Heuristics and biases</p>
<p>Human evaluation of automatically generated text: Current trends and best practice guidelines. Chris Van Der Lee, Albert Gatt, Emiel Emiel Van Miltenburg, Krahmer, 10.1016/j.csl.2020.101151.URLhttps://www.sciencedirect.com/science/article/pii/S088523082030084XComputer Speech &amp; Language. 0885-2308671011512021</p>
<p>Collective Human Opinions in Semantic Textual Similarity. Yuxia Wang, Shimin Tao, Ning Xie, Hao Yang, Timothy Baldwin, Karin Verspoor, 10.1162/tacl_a_00584Transactions of the Association for Computational Linguistics. 2307-387X1108 2023</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, 10.18653/v1/N18-1101Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Marilyn Walker, Ji Heng, Amanda Stent, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational LinguisticsJune 20181</p>
<p>Assumptions behind intercoder reliability indices. S Jun, Xinshu Liu, Ke Zhao, Deng, 10.1080/23808985.2013.11679142Annals of the International Communication Association. 3612013</p>
<p>AlignScore: Evaluating factual consistency with a unified alignment function. Yuheng Zha, Yichi Yang, Ruichen Li, Zhiting Hu, 10.18653/v1/2023.acl-long.634Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
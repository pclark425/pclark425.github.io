<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7864 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7864</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7864</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-274280574</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.16594v7.pdf" target="_blank">From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge</a></p>
                <p><strong>Paper Abstract:</strong> Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). Traditional methods, usually matching-based or small model-based, often fall short in open-ended and dynamic scenarios. Recent advancements in Large Language Models (LLMs) inspire the"LLM-as-a-judge"paradigm, where LLMs are leveraged to perform scoring, ranking, or selection for various machine learning evaluation scenarios. This paper presents a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to review this evolving field. We first provide the definition from both input and output perspectives. Then we introduce a systematic taxonomy to explore LLM-as-a-judge along three dimensions: what to judge, how to judge, and how to benchmark. Finally, we also highlight key challenges and promising future directions for this emerging area. More resources on LLM-as-a-judge are on the website: https://llm-as-a-judge.github.io and https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7864.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7864.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General_Performance_Comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Survey-level summary of agreement between LLM judges and human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey reports that many works benchmark LLM-as-a-judge by measuring alignment with manual human judgments (using metrics such as Cohen's kappa, discernment score, normalized accuracy, percent agreement, correlation), and highlights systematic differences, biases, and limitations observed across those comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Various / general NLG and evaluation tasks (summarization, dialogue, reasoning, retrieval, safety)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>various benchmarks and leaderboards (e.g., MT-Bench, Chatbot Arena, task-specific benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLMs (various: GPT-4, o1, other large LLMs referenced across studies)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Varies by study; survey summarizes many papers using large proprietary and open LLMs without unified size/version reporting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Manual human annotators / established benchmark labels (various across referenced studies)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa; Discernment Score; normalized accuracy; percent agreement; Scott's π; correlation (Pearson/Spearman) - as reported in referenced works</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>positional/order sensitivity; bias toward longer outputs; vulnerability to adversarial artifacts; lower reliability on hard or low-resource cases; self-preference biases</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM judges often show substantial alignment with human judgments at a coarse level but exhibit systematic biases and failure modes (e.g., order/position effects, over-caution or over-refusal on safety, hallucination in long outputs, cross-lingual inconsistency). Agreement depends strongly on protocol (pairwise vs pointwise), prompt/rubric design, and dataset difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Scalability, lower marginal cost, ability to produce fine-grained verbal rationales and multi-aspect scores, faster throughput for large evaluation sets</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Survey-level synthesis: references include pairwise and pointwise protocols, rule-augmented prompts, swapping operation to detect ties, leaderboards compared against human rankings; metrics computed per-study vary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7864.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7864.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChiangLee2023_eval_comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chiang & Lee (2023) — study of LLMs as alternatives to human evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced empirical investigation asking whether large language models can substitute for human evaluators in NLG evaluation, discussed in the survey as an early and influential comparison between LLM-based and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models be an alternative to human evaluations?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can large language models be an alternative to human evaluations?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Automatic evaluation for open-domain conversation / NLG</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>conversation evaluation datasets (as used in Chiang & Lee 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLMs (unspecified in survey excerpt; Chiang & Lee investigate large LMs generally)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Not specified in the survey summary; original study compares LLM-based automatic evaluation against human annotations</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (crowdworkers / expert annotators depending on the original study)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Not specified in the survey excerpt (original work reports comparisons / correlations between LLM and human judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>LLM evaluators can be sensitive to prompt and context; may not consistently replicate human nuance across all evaluation dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The paper is cited as a seminal analysis exploring whether LLMs can be an alternative to human evaluation and is used by the survey to motivate subsequent work; key finding discussed is that LLM-based evaluation can approximate human judgment in many cases but with important caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Potential substitute to expensive human annotation for many open-ended evaluation scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Referenced study used automatic LLM-based evaluation protocols and compared rankings/ratings to human annotations across conversational tasks; survey does not reproduce numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7864.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7864.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTBench_Leaderboard_vs_Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MT-Bench / Chatbot Arena style leaderboards compared to human rankings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey notes works that build LLM leaderboards (e.g., MT-Bench, Chatbot Arena) using LLM-as-a-judge and assess validity by comparing model rankings to those from established human-evaluated leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging llm-as-a-judge with mt-bench and chatbot arena</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Judging llm-as-a-judge with mt-bench and chatbot arena</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Chatbot / conversational quality ranking and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MT-Bench; Chatbot Arena; other leaderboard datasets</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLM judges used to create leaderboards (various large LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Varies; MT-Bench and similar use strong LLMs to compare system outputs and produce pairwise/overall rankings</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Established leaderboard human evaluations (crowdworkers / expert matching used by referenced leaderboards)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Ranking correlation and comparison with established benchmarks (survey: 'assess their validity by comparing model rankings with those from established benchmarks')</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>sensitivity to input ordering; positional bias can distort pairwise comparisons; swapping/order effects may produce inconsistent rankings</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM-derived leaderboards can approximate human rankings but are vulnerable to protocol artifacts (e.g., ordering effects); techniques like swapping operation are proposed to detect/mitigate ties and positional bias.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Enables scalable continual leaderboards and rapid evaluation across many systems without repeated human labeling</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Leaderboard construction via LLM pairwise/listwise comparisons; validation through correlation/comparison to human-established leaderboard rankings; swapping operation and other prompt-level mitigations discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7864.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7864.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Murugadoss2024_GPT4o_vs_Humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Murugadoss et al. (2024) — QA comparison of GPT-4o and human evaluators (table entry)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey's benchmark overview table lists a comparison for Question Answering (3.3K examples) between human evaluators and GPT-4o using accuracy and correlation as metrics, and notes non-English and challenging cases as foci.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Murugadoss et al. (2024) (as cited in survey table)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>3.3K QA examples (as listed in survey table)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4o (as reported in the survey table); specific training/version details not provided in the survey excerpt</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (table entry: 'Human & GPT-4o')</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy; Correlation (as listed in the survey table)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>differences pronounced on non-English and challenging instances (survey conveys these as areas of divergence)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Survey table indicates a direct experimental comparison was performed using accuracy and correlation metrics, highlighting that differences are particularly visible on non-English and hard examples.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Fast, scalable automatic scoring across thousands of QA items</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>QA dataset of 3.3K instances compared between human annotations and GPT-4o, metrics reported included accuracy and correlation (survey reproduces the table entry but does not include numeric scores).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7864.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7864.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multilingual_Eval_Inconsistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multilingual evaluation: English-tuned LLM evaluators underperform on low-resource languages</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey highlights prior work and benchmarks (MM-Eval, PARIKSHA) showing that evaluators tuned in English often underperform on low-resource languages, producing low agreement with human judgments and inconsistent fairness across dialects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MM-Eval / PARIKSHA (as discussed in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Multilingual NLG evaluation (e.g., summarization, dialogue across languages)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MM-Eval; PARIKSHA; multilingual evaluation sets (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Evaluators tuned in English (various LLMs / multilingual judges)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Often English-centric LLM evaluators or judges adapted for multilingual evaluation; survey notes some open-source multilingual judges that outperform English-centric ones on many languages</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators across languages (native speakers / crowdworkers as per the referenced benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Agreement / consistency metrics (qualitative mention of low agreement across languages); some benchmarks measure separability and agreement</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>cross-lingual inconsistency; low agreement on low-resource languages; factual and cultural errors; dialectal variation reduces alignment with human toxicity/fairness judgments</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>English-tuned evaluators show degraded performance on low-resource languages and cultural contexts; multilingual judges tuned separately show improved alignment in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When properly tuned multilingual judges can scale cross-lingual evaluation but English-only tuning is insufficient</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multilingual benchmarks (MM-Eval, PARIKSHA) used to test consistency and fairness; techniques include scoring non-English answers against English references or using multilingual judge models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7864.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7864.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self_Preference_Bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM self-preference / narcissistic evaluation bias</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey cites studies showing LLM evaluators can prefer outputs from models like themselves (self-preference), leading to biased evaluation outcomes and reduced oversight reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Do LLM evaluators prefer themselves? (as cited in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General NLG evaluation and evaluation-of-evaluators studies</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>various synthetic and real evaluation comparisons (as used in cited studies)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLMs (various; studies examine tendency of an evaluator to favor same-source/generated outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Varies by study; reported behavior observed across multiple LLM families in referenced works</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators used as baseline in referenced analyses</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement; preference rates; qualitative bias analyses (survey references studies of preference leakage and self-preference)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>self-preference leading to inflated scores for self-generated outputs; preference leakage contaminating comparisons; undermines fair oversight</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM judges sometimes give systematically higher ratings to outputs from the same model family or their own generations, which can artificially improve apparent performance and reduce trustworthiness of automatic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not an advantage — this is reported as a significant limitation requiring mitigation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Analyses typically compare preference rates when evaluator and evaluated outputs share provenance versus when they do not; survey cites multiple works documenting this effect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>MT-Bench <em>(Rating: 2)</em></li>
                <li>MM-Eval <em>(Rating: 2)</em></li>
                <li>Do llm evaluators prefer themselves for a reason? <em>(Rating: 2)</em></li>
                <li>Are large language models good at utility judgments? <em>(Rating: 1)</em></li>
                <li>Larger studies on LLM-evaluator biases (e.g., Dubois et al., 2024) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7864",
    "paper_id": "paper-274280574",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "General_Performance_Comparisons",
            "name_full": "Survey-level summary of agreement between LLM judges and human judgments",
            "brief_description": "The survey reports that many works benchmark LLM-as-a-judge by measuring alignment with manual human judgments (using metrics such as Cohen's kappa, discernment score, normalized accuracy, percent agreement, correlation), and highlights systematic differences, biases, and limitations observed across those comparisons.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "paper_title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
            "evaluation_task": "Various / general NLG and evaluation tasks (summarization, dialogue, reasoning, retrieval, safety)",
            "dataset_name": "various benchmarks and leaderboards (e.g., MT-Bench, Chatbot Arena, task-specific benchmarks)",
            "judge_model_name": "LLMs (various: GPT-4, o1, other large LLMs referenced across studies)",
            "judge_model_details": "Varies by study; survey summarizes many papers using large proprietary and open LLMs without unified size/version reporting",
            "human_evaluator_type": "Manual human annotators / established benchmark labels (various across referenced studies)",
            "agreement_metric": "Cohen's kappa; Discernment Score; normalized accuracy; percent agreement; Scott's π; correlation (Pearson/Spearman) - as reported in referenced works",
            "agreement_score": null,
            "reported_loss_aspects": "positional/order sensitivity; bias toward longer outputs; vulnerability to adversarial artifacts; lower reliability on hard or low-resource cases; self-preference biases",
            "qualitative_findings": "LLM judges often show substantial alignment with human judgments at a coarse level but exhibit systematic biases and failure modes (e.g., order/position effects, over-caution or over-refusal on safety, hallucination in long outputs, cross-lingual inconsistency). Agreement depends strongly on protocol (pairwise vs pointwise), prompt/rubric design, and dataset difficulty.",
            "advantages_of_llm_judge": "Scalability, lower marginal cost, ability to produce fine-grained verbal rationales and multi-aspect scores, faster throughput for large evaluation sets",
            "experimental_setting": "Survey-level synthesis: references include pairwise and pointwise protocols, rule-augmented prompts, swapping operation to detect ties, leaderboards compared against human rankings; metrics computed per-study vary.",
            "uuid": "e7864.0",
            "source_info": {
                "paper_title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ChiangLee2023_eval_comparison",
            "name_full": "Chiang & Lee (2023) — study of LLMs as alternatives to human evaluations",
            "brief_description": "A referenced empirical investigation asking whether large language models can substitute for human evaluators in NLG evaluation, discussed in the survey as an early and influential comparison between LLM-based and human judgments.",
            "citation_title": "Can large language models be an alternative to human evaluations?",
            "mention_or_use": "mention",
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "evaluation_task": "Automatic evaluation for open-domain conversation / NLG",
            "dataset_name": "conversation evaluation datasets (as used in Chiang & Lee 2023)",
            "judge_model_name": "LLMs (unspecified in survey excerpt; Chiang & Lee investigate large LMs generally)",
            "judge_model_details": "Not specified in the survey summary; original study compares LLM-based automatic evaluation against human annotations",
            "human_evaluator_type": "Human annotators (crowdworkers / expert annotators depending on the original study)",
            "agreement_metric": "Not specified in the survey excerpt (original work reports comparisons / correlations between LLM and human judgments)",
            "agreement_score": null,
            "reported_loss_aspects": "LLM evaluators can be sensitive to prompt and context; may not consistently replicate human nuance across all evaluation dimensions",
            "qualitative_findings": "The paper is cited as a seminal analysis exploring whether LLMs can be an alternative to human evaluation and is used by the survey to motivate subsequent work; key finding discussed is that LLM-based evaluation can approximate human judgment in many cases but with important caveats.",
            "advantages_of_llm_judge": "Potential substitute to expensive human annotation for many open-ended evaluation scenarios",
            "experimental_setting": "Referenced study used automatic LLM-based evaluation protocols and compared rankings/ratings to human annotations across conversational tasks; survey does not reproduce numeric results.",
            "uuid": "e7864.1",
            "source_info": {
                "paper_title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "MTBench_Leaderboard_vs_Human",
            "name_full": "MT-Bench / Chatbot Arena style leaderboards compared to human rankings",
            "brief_description": "Survey notes works that build LLM leaderboards (e.g., MT-Bench, Chatbot Arena) using LLM-as-a-judge and assess validity by comparing model rankings to those from established human-evaluated leaderboards.",
            "citation_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "mention_or_use": "mention",
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "evaluation_task": "Chatbot / conversational quality ranking and benchmarking",
            "dataset_name": "MT-Bench; Chatbot Arena; other leaderboard datasets",
            "judge_model_name": "LLM judges used to create leaderboards (various large LMs)",
            "judge_model_details": "Varies; MT-Bench and similar use strong LLMs to compare system outputs and produce pairwise/overall rankings",
            "human_evaluator_type": "Established leaderboard human evaluations (crowdworkers / expert matching used by referenced leaderboards)",
            "agreement_metric": "Ranking correlation and comparison with established benchmarks (survey: 'assess their validity by comparing model rankings with those from established benchmarks')",
            "agreement_score": null,
            "reported_loss_aspects": "sensitivity to input ordering; positional bias can distort pairwise comparisons; swapping/order effects may produce inconsistent rankings",
            "qualitative_findings": "LLM-derived leaderboards can approximate human rankings but are vulnerable to protocol artifacts (e.g., ordering effects); techniques like swapping operation are proposed to detect/mitigate ties and positional bias.",
            "advantages_of_llm_judge": "Enables scalable continual leaderboards and rapid evaluation across many systems without repeated human labeling",
            "experimental_setting": "Leaderboard construction via LLM pairwise/listwise comparisons; validation through correlation/comparison to human-established leaderboard rankings; swapping operation and other prompt-level mitigations discussed.",
            "uuid": "e7864.2",
            "source_info": {
                "paper_title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Murugadoss2024_GPT4o_vs_Humans",
            "name_full": "Murugadoss et al. (2024) — QA comparison of GPT-4o and human evaluators (table entry)",
            "brief_description": "The survey's benchmark overview table lists a comparison for Question Answering (3.3K examples) between human evaluators and GPT-4o using accuracy and correlation as metrics, and notes non-English and challenging cases as foci.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Murugadoss et al. (2024) (as cited in survey table)",
            "evaluation_task": "Question Answering",
            "dataset_name": "3.3K QA examples (as listed in survey table)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "GPT-4o (as reported in the survey table); specific training/version details not provided in the survey excerpt",
            "human_evaluator_type": "Human annotators (table entry: 'Human & GPT-4o')",
            "agreement_metric": "Accuracy; Correlation (as listed in the survey table)",
            "agreement_score": null,
            "reported_loss_aspects": "differences pronounced on non-English and challenging instances (survey conveys these as areas of divergence)",
            "qualitative_findings": "Survey table indicates a direct experimental comparison was performed using accuracy and correlation metrics, highlighting that differences are particularly visible on non-English and hard examples.",
            "advantages_of_llm_judge": "Fast, scalable automatic scoring across thousands of QA items",
            "experimental_setting": "QA dataset of 3.3K instances compared between human annotations and GPT-4o, metrics reported included accuracy and correlation (survey reproduces the table entry but does not include numeric scores).",
            "uuid": "e7864.3",
            "source_info": {
                "paper_title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Multilingual_Eval_Inconsistency",
            "name_full": "Multilingual evaluation: English-tuned LLM evaluators underperform on low-resource languages",
            "brief_description": "Survey highlights prior work and benchmarks (MM-Eval, PARIKSHA) showing that evaluators tuned in English often underperform on low-resource languages, producing low agreement with human judgments and inconsistent fairness across dialects.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "MM-Eval / PARIKSHA (as discussed in survey)",
            "evaluation_task": "Multilingual NLG evaluation (e.g., summarization, dialogue across languages)",
            "dataset_name": "MM-Eval; PARIKSHA; multilingual evaluation sets (as reported)",
            "judge_model_name": "Evaluators tuned in English (various LLMs / multilingual judges)",
            "judge_model_details": "Often English-centric LLM evaluators or judges adapted for multilingual evaluation; survey notes some open-source multilingual judges that outperform English-centric ones on many languages",
            "human_evaluator_type": "Human annotators across languages (native speakers / crowdworkers as per the referenced benchmarks)",
            "agreement_metric": "Agreement / consistency metrics (qualitative mention of low agreement across languages); some benchmarks measure separability and agreement",
            "agreement_score": null,
            "reported_loss_aspects": "cross-lingual inconsistency; low agreement on low-resource languages; factual and cultural errors; dialectal variation reduces alignment with human toxicity/fairness judgments",
            "qualitative_findings": "English-tuned evaluators show degraded performance on low-resource languages and cultural contexts; multilingual judges tuned separately show improved alignment in many settings.",
            "advantages_of_llm_judge": "When properly tuned multilingual judges can scale cross-lingual evaluation but English-only tuning is insufficient",
            "experimental_setting": "Multilingual benchmarks (MM-Eval, PARIKSHA) used to test consistency and fairness; techniques include scoring non-English answers against English references or using multilingual judge models.",
            "uuid": "e7864.4",
            "source_info": {
                "paper_title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Self_Preference_Bias",
            "name_full": "LLM self-preference / narcissistic evaluation bias",
            "brief_description": "The survey cites studies showing LLM evaluators can prefer outputs from models like themselves (self-preference), leading to biased evaluation outcomes and reduced oversight reliability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Do LLM evaluators prefer themselves? (as cited in survey)",
            "evaluation_task": "General NLG evaluation and evaluation-of-evaluators studies",
            "dataset_name": "various synthetic and real evaluation comparisons (as used in cited studies)",
            "judge_model_name": "LLMs (various; studies examine tendency of an evaluator to favor same-source/generated outputs)",
            "judge_model_details": "Varies by study; reported behavior observed across multiple LLM families in referenced works",
            "human_evaluator_type": "Human annotators used as baseline in referenced analyses",
            "agreement_metric": "Percent agreement; preference rates; qualitative bias analyses (survey references studies of preference leakage and self-preference)",
            "agreement_score": null,
            "reported_loss_aspects": "self-preference leading to inflated scores for self-generated outputs; preference leakage contaminating comparisons; undermines fair oversight",
            "qualitative_findings": "LLM judges sometimes give systematically higher ratings to outputs from the same model family or their own generations, which can artificially improve apparent performance and reduce trustworthiness of automatic evaluation.",
            "advantages_of_llm_judge": "Not an advantage — this is reported as a significant limitation requiring mitigation",
            "experimental_setting": "Analyses typically compare preference rates when evaluator and evaluated outputs share provenance versus when they do not; survey cites multiple works documenting this effect.",
            "uuid": "e7864.5",
            "source_info": {
                "paper_title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "MT-Bench",
            "rating": 2
        },
        {
            "paper_title": "MM-Eval",
            "rating": 2
        },
        {
            "paper_title": "Do llm evaluators prefer themselves for a reason?",
            "rating": 2,
            "sanitized_title": "do_llm_evaluators_prefer_themselves_for_a_reason"
        },
        {
            "paper_title": "Are large language models good at utility judgments?",
            "rating": 1,
            "sanitized_title": "are_large_language_models_good_at_utility_judgments"
        },
        {
            "paper_title": "Larger studies on LLM-evaluator biases (e.g., Dubois et al., 2024)",
            "rating": 1,
            "sanitized_title": "larger_studies_on_llmevaluator_biases_eg_dubois_et_al_2024"
        }
    ],
    "cost": 0.023198749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge
29 Sep 2025</p>
<p>Dawei Li 
Bohan Jiang 
Arizona State University</p>
<p>Liangjie Huang 
University of Illinois</p>
<p>Alimohammad Beigi 
Arizona State University</p>
<p>Chengshuai Zhao 
Arizona State University</p>
<p>Zhen Tan 
Arizona State University</p>
<p>Amrita Bhattacharjee 
Arizona State University</p>
<p>Yuxuan Jiang 
University of Maryland
Baltimore County</p>
<p>❦ Northwestern University</p>
<p>Canyu Chen 
Tianhao Wu 
University of California
Berkeley</p>
<p>Emory University</p>
<p>Kai Shu 
University of California
Berkeley</p>
<p>Emory University</p>
<p>Lu Cheng 
University of Illinois</p>
<p>Huan Liu 
Arizona State University</p>
<p>Chicago 
Linyong Nan 
Ellen Zhang 
Weijin Zou 
Yilun Zhao 
Kun-Peng Ning 
Shuo Yang 
Yuyang Liu 
Jia-Yu Yao 
Zhenhui Liu 
Yu Wang 
Ming Pang 
Yuan Pico 
Masanari Ohi 
Masahiro Kaneko 
Ryuto Koike 
Mengsay Loem 
Naoaki 2024 Okazaki 
Matthew Lyle Olson 
Neale Ratzlaff 
Musashi Hinck 
Shao-Yen Tseng 
Vasudev 2024 Lal 
Steering 
Isaac Ong 
Amjad Almahairi 
Vincent Wu 
Wei-Lin Chiang 
Joseph E Gonzalez 
Long Ouyang 
Jeffrey Wu 
Xu Jiang 
Diogo Almeida 
Carroll L Wainwright 
Pamela Mishkin 
Chong Zhang 
Sandhini Agarwal 
Katarina Slama 
Alex Ray 
John Schulman 
Jacob Hilton 
Fraser Kelton 
Luke Miller 
Maddie Simens 
Amanda Askell 
Peter Welin- Der 
Arizona State University</p>
<p>Paul F Christiano 
Jan Leike 
Ryan Lowe 
Mansi Phute 
Alec Helbling 
Matthew Daniel Hull 
Shengyun Peng 
Sebastian Szyller 
Cory Cornelius 
Duen Horng 
Llm 
Andrea Piergentili 
Beatrice Savoldi 
Matteo Negri 
Luisa 2025 Bentivogli 
An 
From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge
29 Sep 2025C8A0A86869E304314F3026159B8BF2ECarXiv:2411.16594v7[cs.AI]
Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP).Traditional methods, usually matching-based or small model-based, often fall short in openended and dynamic scenarios.Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection for various machine learning evaluation scenarios.This paper presents a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to review this evolving field.We first provide the definition from both input and output perspectives.Then we introduce a systematic taxonomy to explore LLM-as-a-judge along three dimensions: what to judge, how to judge, and how to benchmark.Finally, we also highlight key challenges and promising future directions for this emerging area 12 .</p>
<p>Introduction</p>
<p>Automatic model assessment and evaluation have long been essential yet challenging tasks in machine learning (ML) and natural language processing (NLP) (Sai et al., 2022;Chang et al., 2024).Traditional static metrics (Tan et al., 2022;Li et al., 2024i) like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) measure quality by calculating lexical overlap between output and reference texts.While computationally efficient, these metrics perform poorly in dynamic and open-ended scenarios (Liu et al., 2016;Reiter, 2018).With the rise of deep learning, small language modelbased metrics like BERTScore (Zhang et al., 2020) and BARTScore (Yuan et al., 2021) have emerged.</p>
<p>However, these metrics still face challenges in capturing nuanced attributes like fairness (Sun et al., 2022) and helpfulness (Zhu et al., 2024a).</p>
<p>Recently, the advancements of large language models (LLMs) (Zhao et al., 2025a) such as GPT-4 (Achiam et al., 2023) and o1 (Jaech et al., 2024), have led to striking improvements in various applications, leveraging substantial prior knowledge in vast training corpora.This progress has motivated researchers to propose the concept of "LLM-asa-judge" (Zheng et al., 2023;Wang et al., 2023c;Liu et al., 2023b;Chiang and Lee, 2023b), where LLMs are used to assess the candidate outputs by assigning scores, producing rankings, or selecting the best options, based on various input formats (e.g., point-and pair-wise), given context and instruction.The strong capability of LLMs combined with well-designed assessment pipelines (Li et al., 2023b;Bai et al., 2023a) leads to fine-grained and human-like judgment for various evaluation applications, addressing the previous limitations.</p>
<p>Beyond evaluation, LLMs-as-a-judge has been adopted across the lifecycle for next generations of LLM developments and applications.LLMsas-a-judge is often used as a scalable way to provide supervisions for key development steps like alignment (Lee et al., 2023), retrieval (Li et al., 2024c), and reasoning (Liang et al., 2023).LLMas-a-judge also empowers LLMs with a series of advanced capabilities such as self-evolution (Sun et al., 2024), active retrieval (Li et al., 2024c), and decision-making (Yang et al., 2023), driving their elevations from generative models to intelligent agents (Zhuge et al., 2024).However, as the field develops rapidly, challenges like bias and vulnerability (Koo et al., 2023;Park et al., 2024;Fu et al., 2024;Huang et al., 2024a) are emerging.Therefore, a systematic review of both techniques and limitations is crucial for facilitating this field.</p>
<p>This survey delves into the details of LLM-asa-judge, aiming to provide a systematic overview of LLM-based judgment systems.We start by formally defining LLM-as-a-judge with its diverse input and output formats (Section 2).Next, we propose an in-depth and comprehensive taxonomy to address the three key questions (Section 3, 4 6):</p>
<p>• Attribute: What to judge?We outline six subtle attributes that are uniquely assessed by LLM-asa-judge, including helpfulness, safety &amp; security, reliability, relevance, logical, and overall quality.</p>
<p>• Methodology: How to judge?We explore ten tuning and prompting methods for LLM-as-ajudge, including manual labeling, synthetic feedback, supervised fine-tuning, preference learning, swapping operation, rule augmentation, multiagent collaboration, demonstration, multi-turn interaction, and comparison acceleration.</p>
<p>• Benchmark: How to evaluate LLM-as-ajudge?We categorize existing benchmarks for LLM-as-a-judge into four types: for general performance, bias quantification, challenging tasks, and domain-specific performance.</p>
<p>Finally, we discuss challenges and potential future directions for LLM-as-a-judge in Section 7.</p>
<p>Differences from Existing Surveys.Existing concurrent surveys investigate LLM for the evaluation of natural language generation (NLG) (Gao et al., 2024;Li et al., 2024o;Gu et al., 2024).However, LLM-as-a-judge has been applied across a broader range of scenarios beyond evaluation, as we discussed, necessitating a systematic survey to categorize and summarize its various applications.</p>
<p>Preliminary</p>
<p>In this section, we provide a detailed definition of LLM-as-a-judge, discussing the various input and output formats as shown in Figure 1.</p>
<p>Input</p>
<p>Given a judge LLM J, the assessment process can be formulated as: R = J(C 1 , ...C n ).Here C i is the i th candidate to be judged and R is the judging result.We categorize two input formats based on the different candidate numbers n.</p>
<p>Point-Wise: When n = 1, it becomes a point-wise judgment where the LLMs judges will solely focus on one candidate sample (Gao et al., 2023).</p>
<p>Pair/ List-Wise: When n ≥ 2, it becomes a pairwise (n = 2) or list-wise (n &gt; 2) judgment where multiple candidate samples are provided together for the LLM judges to compare and make a comprehensive assessment (Zheng et al., 2023).</p>
<p>Output</p>
<p>In this section, we discuss three kinds of output of the judgment based on the different formats of R. Score: When each candidate sample is assigned a continuous or discrete score: R = {C 1 : S 1 , ..., C n : S n }, it becomes a score-based judgment.This is the most widely adopted protocol, leveraging LLMs to generate scores for quantitative comparisons (Li et al., 2024a) or attribute detection (Xie et al., 2024a).</p>
<p>Ranking: In ranking-based judgment, the output is a ranking of each candidate sample, represented as R = {C i &gt; ... &gt; C j }.This comparative approach is useful in scenarios where establishing a rank order among candidates is required (Li et al., 2023b;Liu et al., 2024b).</p>
<p>Selection: In selection-based judgment, the output involves selecting one or more optimal candidates, represented as
R = {C i , ..., C j } &gt; {C 1 , ...C n }.
This method is particularly crucial in decisionmaking (Yao et al., 2023a) or content-filtering (Li et al., 2024c) contexts.</p>
<p>Attribute</p>
<p>In this section, we categorize current research in LLM-as-a-judge from attribute perspectives.</p>
<p>Helpfulness</p>
<p>Helpfulness is a critical criterion to measure the utility and informativeness of a generated response.et al., 2023;Guo et al., 2024;Zhang et al., 2025d).Beyond alignment tuning, helpfulness assessment using LLM-as-a-judge also plays a vital role in automatic model evaluation (Zheng et al., 2023;Lin et al., 2023;Li et al., 2024e;Zhang et al., 2025a).</p>
<p>Safety &amp; Security</p>
<p>Safety and security are essential to ensure that models do not generate harmful content or respond inappropriately to malicious inputs.Current studies have validated that LLMs can be effectively used for model safety assessment, either as off-theshelf models guided by policy instructions (Bai et al., 2022;Phute et al., 2023;Li et al.;Ye et al., 2024b;Wang et al., 2024l;Eiras et al., 2025;Chen and Goldfarb-Tarrant, 2025;Rodriguez et al., 2025;Hengle et al., 2025), or as lightweight models finetuned on safety-specific datasets (Inan et al., 2023;Zhang et al., 2024f;Xie et al., 2024a).Besides, LLM-as-a-judge has been widely adopted to detect and purify adversarial and toxic prompts designed with malicious intent (Cantini et al., 2025;Mu et al., 2025;Armstrong et al., 2025).</p>
<p>Reliability</p>
<p>Reliability is a crucial attribute for LLMs, enabling them to generate faithful content while presenting uncertainty or acknowledging missing knowledge about certain topics.Regarding sentence-level faithfulness assessment, existing researches leverage LLM-as-a-judge to either instruct the powerful LLMs (e.g., GPT-4) directly (Cheng et al., 2023;Gekhman et al., 2023;Luo et al., 2024a;Hsu et al., 2024) or train specific reliability judges (Wang et al., 2024a).Several works adopt LLM judges for long-form and fine-grained faithfulness evaluation (Tan et al., 2024a;Bai et al., 2024;Wu et al., 2025), using external retrieval bases (Min et al., 2023;Cao et al., 2025b;Loru et al., 2025) or search engines (Wei et al., 2024b).Jing et al. (2024); Pu et al. (2025) further expand this assessment to the multimodal area.Besides evaluation, there are also many works that adopt LLM-as-a-judge to improve the reliability of the generated content, either by external verifiers (Xie et al., 2024b) or synthetic alignment datasets (Zhang et al., 2024g;Wen et al., 2024).For uncertainty judgment, Xu et al. (2024d) propose SaySelf, a training framework that teaches LLMs to express more fine-grained confidence estimates with self-consistency prompting and groupbased calibration training.</p>
<p>Relevance</p>
<p>Relevance assessment with LLM-as-a-judge has been explored and validated to be a more refined and effective manner across various tasks (Chiang and Lee, 2023a;Arabzadeh and Clarke, 2025a).In conversation evaluation, both Lin and Chen (2023a) and Abbasiantaeb et al. (2024) propose to replace expensive human annotation with LLM judgment in relevance assessment.In retrieval-augmented generation (RAG) scenarios, there are also many works that utilize LLMs to determine which demonstrations (Li and Qiu, 2023a) or documents (Li et al., 2024c) are most relevant for solving the current problem.Recently, LLM-as-a-judge has also been used in multimodal applications for crossmodality relevance judgment (Lee et al., 2024b;Chen et al., 2024g;Yang and Lin, 2024;Chen et al., 2024a;Lu et al., 2024b;Luo et al., 2024b;Lin et al., 2025).Additionally, LLM-as-a-judge has also been explored in many traditional retrieval applications for relevance assessment (Zhao et al., 2023a;Alaofi et al., 2024;Dietz et al., 2025;Arabzadeh and Clarke, 2025b;Balog et al., 2025), such as search (Thomas et al., 2024;Sebastian and Hoppe, 2025), retrieval (Ma et al., 2024;Dey et al., 2025), recommendation (Hou et al., 2024;Zhang et al., 2024h).</p>
<p>Logic</p>
<p>In agentic LLMs, assessing the logical correctness of candidate actions or steps is crucial for LLMs' planning, reasoning and decision-making, which further releases their great potential at inference-time.While some works leverage metrics or external tools for this feasibility assessment (Huang et al., 2023a;Yuan et al.), many others leverage LLMs' feedback as the signal (Lightman et al.; Kawabata and Sugawara, 2024) to per-form planning and searching in complex reasoning spaces (Hao et al., 2023;Yao et al., 2023a;Besta et al., 2024).In multi-agent collaboration systems, both Liang et al. (2023) and Li et al. (2024b) propose to leverage the judge LLM to select the most feasible solutions among multiple candidates' responses.Besides, other works adopt LLM judges to perform logical assessment in API selection (Zhao et al., 2024b), tool using (Yang et al., 2023) and LLM routing (Ong et al., 2024).</p>
<p>Overall Quality</p>
<p>As previously mentioned, LLM-as-a-judge can be employed to perform multi-aspect and finegrained assessments.However, in many cases, a general assessment is still required to represent the candidates' overall quality.One straightforward approach to obtain this overall score is based on the aspect-specific scores, either by averaging them (Lin et al., 2023) or referring them to generate an overall judgment (Yu et al., 2024c).Moreover, in many traditional NLP tasks (Lu et al., 2024a;Jiang et al., 2024;Ho et al., 2025;Shibata and Miyamura, 2025;Kartáč et al., 2025) like summarization (Gao et al., 2023;Jain et al., 2023a;Chen et al., 2023;Kumar et al., 2024a;Qi et al., 2025;Barnes et al., 2025;Altemeyer et al., 2025;Jeong et al., 2025;Calderon et al., 2025) and machine translation (Kocmi and Federmann, 2023;Huang et al., 2024b;Piergentili et al., 2025;Wang et al., 2025d), the evaluation dimensions are less diverse compared to more open-ended, long-form generation tasks.As a result, LLM-as-a-judge is often prompted directly to produce an overall judgment in these tasks.</p>
<p>Methodology</p>
<p>In this section, we present commonly adopted methods and tricks to improve LLMs' judging capabilities, splitting them into tuning (Section 4.1) and prompting strategies (Section 4.2).</p>
<p>Tuning</p>
<p>To enhance the judging capabilities of a general LLM, various tuning techniques have been employed in different studies.In this section, we discuss these tuning approaches for LLM-as-a-judge from two perspectives: data sources (Section 4.1.1)and training techniques (Section 4.1.2).</p>
<p>Data Source</p>
<p>Manually-labeled Data: To train a LLM judge with human-like criteria, one intuitive method is to collect manually-labeled judgments.Previous works have leveraged and integrated existing sources annotated by humans, including instruction tuning datasets (Lee et al., 2024a;Wang et al., 2024k) and traditional NLP datasets (Vu et al., 2024), for tuning LLM judges.Other works collect manually-labeled datasets with fine-grained judgment feedback.These fine-grained feedbacks can be rationales behind judgment results (Xu et al., 2023a), multi-aspect judgment formats (Liu et al., 2024a) and fine-grained judgment labels (Yue et al., 2023), all of which facilitate the LLM judges to produce more detailed and context-rich judging results.Notably, Ke et al. (2024) first prompt GPT-4 to generate judgment and then manually verify and revise the outputs to ensure high-quality annotations.</p>
<p>Synthetic Feedback: While manually labeled feedback is high-quality and accurately reflects human judgment preferences, it is limited in both scale and coverage.To address it, researchers have also explored synthetic feedback as a data source for LLM judges' tuning.Some rely on the LLM judges themselves to generate the synthetic feedback.It involves instructing the LLM to self-evaluate and improve its judgments (Wu et al., 2024a), or by generating corrupted instructions and corresponding responses as negative samples for Directed Preference Optimization (DPO) training (Wang et al., 2024h).Besides, other powerful and stronger LLMs are also introduced for feedback synthesis.For example, GPT-4 has been widely leveraged to synthesize judging evidence (Wang et al., 2024a), erroneous responses (Park et al., 2024), rationale and feedback (Li et al., 2024e;Kim et al., 2024b;Xiong et al., 2024), and judgment labels (Zhu et al., 2023;Xie et al., 2024a).</p>
<p>Tuning Techniques</p>
<p>Supervised Fine-tuning: Supervised fine-tuning (SFT) is the most widely used approach for training LLM judges (Hu et al., 2025a), enabling them to learn from pairwise (Li et al., 2024e;Wang et al., 2023b;Zhu et al., 2023;Wang et al., 2024k;Pombal et al., 2025b;Salinas et al., 2025) or pointwise (Wang et al., 2023b;Yue et al., 2023;Xie et al., 2024a;Lee et al., 2024a;Chiang et al., 2025) judgment data.Among many tricks applied in SFT, multi-task training and weight merging are introduced to enhance the robustness and generalization of LLM judges (Kim et al., 2024b;Vu et al., 2024;Saad-Falcon et al., 2024b).Other works try to enrich the original training set with augmented or self-generated samples.Ke et al. (2024) augment pairwise training data by swapping the order of two generated texts and exchanging the corresponding content in critiques.Xu et al. (2023a) further fine-tune their INSTRUCTSCORE model on self-generated outputs to align diagnostic reports better with human judgment.Additionally, Liu et al. (2024a) propose a two-stage SFT process: an initial phase of vanilla instruction tuning for evaluation diversity, followed by additional tuning with auxiliary aspects to enrich the model's evaluative depth.</p>
<p>Reinforcement Learning: Reinforcement learning from human preference is closely tied to judgment and evaluation tasks, particularly those involving comparison and ranking.Rather than directly adopt or augment preference learning datasets for SFT, several studies apply preference learning techniques to enhance LLMs' judging capabilities.One straightforward way is to treat the off-topic responses as inferior samples and apply DPO (Wang et al., 2024a;Yu et al., 2025;Rad et al., 2025).Besides, Wu et al. (2024a) propose meta-rewarding, which leverages the policy LLMs to judge the quality of their own judgment and produce pairwise signals for enhancing the LLMs' judging capability.This concept is also adopted by Wang et al. (2024h), who propose self-taught evaluators that use corrupted instructions to generate suboptimal responses as inferior examples for preference learning.Moreover, Hu et al. (2024b) introduce rating-guided DPO, in which the rating difference between two responses is considered in preferences modeling.Different from RLHF-and DPO-based approaches, several recent works leverage reinforcement learning with verifiable reward (RLVR) (Guo et al., 2025) to train LLM judges by rewarding reasoning trajectories that lead to correct judgments (Saha et al., 2025;Liu et al., 2025e;Zhou et al., 2025).</p>
<p>Prompting</p>
<p>Designing appropriate prompting strategies and pipelines at the inference stage could improve judgment accuracy and mitigate bias.We summarize existing prompting strategies for LLM-as-a-judge into six categories (see Figure 3).</p>
<p>Swapping Operation</p>
<p>Previous studies have demonstrated that LLMbased judges are sensitive to the positions of candidates, and the ranking results of candidate responses can be easily manipulated by merely altering their order in the context (Wang et al., 2023d).To mitigate this positional bias and establish a more fair LLM judging system, (Zheng et al., 2023) propose a swapping operation, which involves invoking the judge LLM twice, swapping the order of the two candidates in each instance.If the two results are inconsistent, it is labeled a "tie", indicating that the LLM is unable to confidently distinguish the quality of the candidates.This swapping operation technique has also been widely adopted in pairwise feedback synthesis to produce more accurate reward signals (Lee et al., 2023;Sun et al., 2024;Lee et al., 2024a).</p>
<p>Rule Augmentation</p>
<p>Rule-augmented prompting involves embedding a set of principles, references, and evaluation rubrics directly within the prompt for LLM judges.This approach is commonly employed in LLM-based evaluations, where LLM judges are guided to assess specific aspects (Lahoti et al., 2023;Li et al., 2024e;Bai et al., 2023a;Yu et al., 2024c;Qian et al., 2024;Dong et al., 2024;Wei et al., 2025;Xie et al., 2025b) and provided with detailed rubrics and criteria (Gao et al., 2023;Kim et al.;Wang et al., 2024g;Murugadoss et al., 2024;Li et al., 2024m,h;Hu et al., 2024a;Liu et al., 2024d;Li et al., 2025b;Fan et al., 2025) to ensure accurate judgments.Following this concept, studies in alignment (Bai et al., 2022;Lee et al., 2023Lee et al., , 2024a;;Guo et al., 2024;Sun et al., 2024;Beigi et al., 2024) enhance this principle-driven prompting by incorporating more detailed explanations (Hu et al.) for each aspect of the principle or rubric.Apart from these human-written rules, some works (Liu et al., 2024c;Zhang et al., 2024f;Xu et al., 2025b;Wen et al., 2025;Zhou et al., 2024a) embed the selfgenerated or automaticaly-searched scoring criteria and principles as a part of their instruction.</p>
<p>Multi-agent Collaboration</p>
<p>Accessing results from a single LLM judge may not be reliable due to inherent biases in LLMs (Wang et al., 2023d;Liusie et al., 2024;Ohi et al., 2024).To address this limitation, Li et al. (2023b); Chen Additionally, others apply multi-agent collaboration for alignment data synthesis, leveraging multiple LLM judges to refine responses (Arif et al., 2024) or provide more accurate feedback (Li et al., 2024j).</p>
<p>Demonstration</p>
<p>In-context samples or demonstrations (Brown et al., 2020;Dong et al., 2023;Agarwal et al.) provide concrete examples for LLMs to follow and have been shown to be a crucial factor in the success of in-context learning for LLMs.Several studies have introduced human assessment results as demonstrations for LLMs-as-judges, aiming to help LLMs learn evaluation standards from a few illustrative examples (Jain et al., 2023b;Kotonya et al., 2023).</p>
<p>To improve the robustness of LLM evaluations, Hasanbeig et al. (2023) propose ALLURE, an approach that iteratively incorporates demonstrations of significant deviations to enhance the evaluator's robustness.Additionally, Song et al. (2024b) borrow the insights from many-shot in-context learning and apply it in LLM-as-a-judge applications.</p>
<p>Multi-turn Interaction</p>
<p>A single response may not provide enough information for an LLM judge to thoroughly and fairly assess each candidate.To address this limitation, multi-turn interactions are proposed to offer a more comprehensive evaluation.Typically, the process begins with an initial query or topic, followed by dynamically interacting between the LLM judge and candidate models (Bai et al., 2023b;Yu et al., 2024c;Pombal et al., 2025a).Besides, some approaches facilitate debates among candidates in a multi-round manner, allowing their true knowledge and performance to be fully revealed and evaluated (Zhao et al., 2024c;Moniri et al., 2024).</p>
<p>Comparison Acceleration</p>
<p>Among various input formats in LLM-as-a-judge, pair-wise comparison is the most common approach for model comparison in evaluation or producing pair-wise feedback for training.However, when multiple candidates need to be ranked, this method can be quite time-consuming (Zhai et al., 2024).To mitigate the computational overhead, Zhai et al. (2024) propose a ranked pairing method in which all candidates are compared against an intermediate baseline response.In addition, Lee et al. (2024a); Liu et al. (2025d) utilize a tournamentbased approach (Liu et al., 2023a;Zhao et al., 2023b) for rejection sampling during inference to speed up the pair-wise comparison.</p>
<p>Application</p>
<p>We introduce four applications which LLM-as-ajudge can be applied: evaluation (Section 5.1), alignment (Section 5.2), retrieval (Section 5.3), and reasoning (Section ??).Due to the space limitation, we provide a more detailed version in Appendix C.</p>
<p>Evaluation</p>
<p>LLM judges are initially proposed for and widely adopted in various evaluation scenarios.For openended generation, LLM judges assess the quality of outputs like dialogues, summaries, and creative writing, ensuring contextual relevance, coherence, and safety (Badshah and Sajjad, 2024; Kumar et al., 2024b;Zeng et al.;Jones et al., 2024).For reasoning tasks, they judge intermediate steps and final answers (He et al., 2023;Parmar et al., 2024;Xia et al., 2024) in areas such as math (Xia et al., 2024), logic (Parmar et al., 2024), and temporal reasoning (Fatemi et al., 2024).There are also some emerging areas where LLM judges are applied to domains once dominated by humans, including social intelligence (Zhou et al., 2023), multimodal tasks (Chen et al.) and multilingual generation (Fu and Liu, 2025).</p>
<p>Alignment</p>
<p>Model alignment also benefits from the automatic LLM-as-a-judge to produce and filter data at scale.Typically, larger and powerful LLMs are usually used as judges to align smaller models, providing synthetic preference data.This includes methods like multi-agent collaboration (Arif et al., 2024) and specialized tasks such as code alignment (Weyssow et al., 2024).Additionally, selfjudging methods have LLMs rank or critique their own outputs to generate preference data without external teachers.To improve the judging capability of the policy model, techniques such as metarewarding (Wu et al., 2024a), Judge Augmented Supervised Fine-Tuning (JSFT) (Lee et al., 2024a), and self-evaluation (Zhang et al., 2024g) have been proposed.Apart from pairwise data, some other studies also use LLM-as-a-judge to judge and filter synthetic SFT data for instruction tuning (Liang et al., 2024c;Yasunaga et al., 2024).</p>
<p>Retrieval</p>
<p>LLM judges can assist with both traditional retrieval tasks and retrieval-augmented generation (RAG).For traditional retrieval, LLM-as-a-judge ranks documents by relevance (Zhuang et al., 2024a) without task-specific data (Ma et al., 2023), using permutation-based (Sun et al., 2023), pairwise (Qin et al., 2024), and listwise (Zhuang et al., 2024b) approaches to improve reranking for complex queries and domain-specific search tasks.For RAG, LLM judges guide how external knowledge is fetched and used during generation, ensuring coherence, accuracy, and relevance.This includes frameworks like Memory-of-Thought (Li and Qiu, 2023b), Self-Retrieval (Tang et al., 2024a), and Self-RAG (Asai et al.), where the judge selects or filters retrieved content, particularly in specialized fields such as biomedicine (Li et al., 2024c).</p>
<p>Reasoning</p>
<p>Reasoning is a critical capability of LLMs for complex and dynamic problem-solving.LLM judges can aid reasoning tasks by improving reasoning path selection and external tool use.Reasoning path selection involves identifying the correct trajectory for the LLM's reasoning process, where LLM-as-a-judge are adopted to evaluate intermediate reasoning steps (Lahoti et al., 2023), perform trajectory-level selection (Musolesi, 2024), and act as a process reward model for reasoning state scoring (Lightman et al., 2023) or a fine-grained critic to provide verbal feedback (Ankner et al., 2024).</p>
<p>For external tool use, LLM judges help AI systems decide which external tools, modules, or agents to activate at each step of reasoning, acting as controllers that coordinate tool choice (Sha et al., 2023), agent communication (Ong et al., 2024), and message flow management (Liang et al., 2023) to ensure accurate and coherent problem solving.</p>
<p>6 Benchmark: Judging LLM-as-a-judge</p>
<p>We categorize benchmarks for evaluating LLMs-asjudges into four groups: general performance (Section 6.1), bias quantification (Section 6.2), challenging task performance (Section 6.3), and domainspecific performance (Section 6.4).</p>
<p>General Performance</p>
<p>Benchmarks focusing on general performance aim to evaluate the overall competence of LLMs in various tasks.One direct way to benchmark LLM judges' performance is to calculate the alignment between LLM prediction and the manual judgment result, using various metrics like Cohen's kappa, Discernment Score, and normalized accuracy (Li et al., 2023a;Tan et al., 2024b;Wang et al., 2024j;Lambert et al., 2024;Penfever et al., 2024;Qu et al., 2025;Xu et al., 2025a;Chang et al., 2025;Hu et al., 2025b;Calderon et al., 2025;Elangovan et al., 2024;Schroeder and Wood-Doughty, 2024;Gera et al., 2024).Moreover, several studies build LLM leaderboards using LLM-as-a-judge and assess their validity by comparing model rankings with those from established benchmarks and leaderboards, such as Chatbot Arena (Zheng et al., 2023)) (Zheng et al., 2023;Dubois et al., 2024;Li et al., 2024l;Zhao et al., 2024c;Chi et al., 2025).</p>
<p>Bias Quantification</p>
<p>Quantifying and mitigating bias in LLM judgments is critical to ensuring fairness and reliability (Xie et al., 2025a).Typical benchmarks include EvalBi-asBench (Park et al., 2024) and CALM (Ye et al., 2024a), focus explicitly on quantifying biases, including those emerging from alignment and robustness under adversarial conditions.Besides, Shi et al. ( 2024) adopt metrics such as position bias and percent agreement in question-answering tasks.</p>
<p>Recently, (Tripathi et al., 2025) examine the influence of protocol choice (pairwise and pointwise) on the bias degree of LLM judges.</p>
<p>Challenging Task Performance</p>
<p>Benchmarks designed for difficult tasks push the boundaries of LLM evaluation.For example, Arena-Hard (Li et al., 2024l) and JudgeBench (Tan et al., 2024b) select harder questions based on LLMs' performance for conversational QA and various reasoning tasks, respectively.CALM (Ye et al., 2024a) explores alignment and challenging scenarios, using metrics like separability and agreement to evaluate performance in manually identified hard datasets.</p>
<p>Domain-Specific Performance</p>
<p>Domain-specific benchmarks provide task-focused evaluations to assess LLMs' effectiveness in specialized contexts.Concretely, Raju et al. ( 2024) measure separability and agreement across tasks in specific domains such as coding, medical, finance, law and mathematics.CodeJudge-Eval (Zhao et al., 2024a) specifically evaluates LLMs for judging code generation with execution-focused metrics such as accuracy and F1 score.This idea has also been adopted by several following works in code summarization and generation evaluation (Wu et al., 2024b;Yang et al., 2024;Tong and Zhang, 2024).Besides, there are also domain-specific benchmarks focusing on LLMs' assessing capabilities in multimodal (Chen et al., 2024a), multilingual (Son et al., 2024b,a), instruction following (Murugadoss et al., 2024) and LLM agent (Lù et al., 2025).</p>
<p>7 Challenges &amp; Future Works</p>
<p>Bias &amp; Vulnerability</p>
<p>The use of LLMs-as-a-judge inherently introduces significant challenges related to bias and vulnerability, which significantly compromise fairness and reliability when LLMs are deployed for diverse judging tasks.Among the various types of bias, some are consistent across all LLM judges, for example, a tendency to prefer longer (Koo et al., 2023;Dubois et al., 2024;Domhan and Zhu, 2025;Yuan et al., 2024a) 2025), DPO (Saha et al., 2025) or distillation (Zhao et al., 2025b) to train LLMs to serve as more effective judges.Future Directions.While LLM-as-a-judge approaches benefit from ITS techniques, it is also important to recognize the associated challenges.These include efficiency bottlenecks (Sui et al., 2025), performance degradation from overthinking (Chen et al., 2024e), and increased vulnerability of long CoTs to adversarial attack (Jiang et al., 2025).Future research could investigate these limitations and develop mitigation strategies, paving the way for more efficient, accurate, and robust judge LLMs enhanced by ITS.</p>
<p>Dynamic &amp; Complex Judging Strategy</p>
<p>Compared with earlier static and straightforward approaches that directly prompt LLMs for judgment (Zheng et al., 2023), more dynamic and complex judgment pipelines have been proposed recently to address various limitations, improving the robustness and effectiveness of LLM-asa-judge.One approach is to follow the concept of "LLM-as-a-examiner", where the system dynamically and interactively generates both questions and judgments based on the candidate LLMs' performance (Yu et al., 2024c;Bai et al., 2023a;Pombal et al., 2025a;Dammu et al., 2025;Khalili and Smyth, 2025;Wang et al., 2024i;Kim et al., 2024a;Zhang et al., 2025e).Other works focus on making judgments based on multiple candidate LLMs' battling and debating (Moniri et al., 2024;Zhao et al., 2024c).Additionally, building complex judgment agents is another popular research area (Li et al., 2023b;Chan et al., 2023;Zhuge et al., 2024), which typically involves multi-agent collaboration with well-designed planning systems.</p>
<p>Future Direction.One promising direction for future research is to equip LLMs with human-like and agentic judgment capabilities (Yuan et al., 2024a;Liang et al., 2024b;Li et al., 2024p;Saha et al., 2024;Zhang et al., 2024b;Wang et al., 2025e;Song et al., 2025), such as anchoring, comparing, and meta-judgment.Another intriguing avenue would be to develop an adaptive difficulty assessment system (Hu, 2024;Patel et al., 2025), dynamically adjusting problems' difficulties based on candidates' performance.</p>
<p>Human-LLMs Co-judgement</p>
<p>As mentioned earlier, the biases and vulnerabilities in LLM-as-a-judge can be addressed through human-in-the-loop for further intervention and proofreading.However, only a few studies have focused on this direction (Wang et al., 2023d;Faggioli et al., 2023;Pradeep et al., 2025).Future Direction.As data selection (Xie et al., 2023;Albalak et al., 2024) becomes an increasingly popular research area for improving the efficiency of LLMs' training and inference, it also holds the potential for enhancing LLMs-based evaluation.LLM-as-a-judge can draw insights from data selection to enable judge LLMs to serve as a critical sample selector, choosing a small subset of samples based on specific criteria (e.g., difficulty) for human annotators to conduct evaluation.Due to the space limitation, we put the application of LLM-as-a-judge, paper collection for our taxonomy, tuning techniques and benchmark for LLM-as-a-judge in Appendix 5, D, E and F.</p>
<p>Conclusion</p>
<p>This survey explores the intricacies of LLM-as-ajudge.We begin by categorizing existing LLMbased judgment methods based on input and output formats.Then, we propose a comprehensive taxonomy for LLM-as-a-judge, encompassing judging attributes, methodologies and benchmarks.After this, a detailed and thoughtful analysis of current challenges and future directions of LLM-as-a-judge is proposed, aiming to provide more resources and insights for future works in this emerging area.</p>
<p>Limitations</p>
<p>This work aims to provide a comprehensive survey of the LLM-as-a-judge paradigm.Due to space constraints, we focus on three core aspects in the main paper: judging attributes, methods, and benchmarks.Applications of LLM-as-a-judge and a detailed list of related papers are included in the appendix.Additionally, as discussed in Section 7.1, LLM-as-a-judge carries inherent limitations and biases.The substantial computational resources required for deploying LLMs may also pose challenges in resource-constrained scenarios.Assessing multimodal llm-as-a-judge with visionlanguage benchmark.In Forty-first International Conference on Machine Learning.</p>
<p>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun.2024a.MLLM-asa-judge: Assessing multimodal LLM-as-a-judge with vision-language benchmark.In Forty-first International Conference on Machine Learning.</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024b.Humans or llms as the judge?a study on judgement biases.ArXiv preprint, abs/2402.10669.</p>
<p>Hongyu Chen and Seraphina Goldfarb-Tarrant.2025.Safer or luckier?llms as safety evaluators are not robust to artifacts.arXiv preprint arXiv:2503.09347.</p>
<p>Junjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, and Shaoping Ma.</p>
<p>2024c.An automatic and cost-efficient peer-review framework for language generation evaluation.arXiv preprint arXiv:2410.12265.</p>
<p>Kai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, and 1 others.2024d.Automated evaluation of large vision-language models on self-driving corner cases.ArXiv preprint, abs/2404.10595.</p>
<p>Meilin Chen, Jian Tian, Liang Ma, Di Xie, Weijie Chen, and Jiang Zhu.2025a.Unbiased evaluation of large language models from a causal perspective.arXiv preprint arXiv:2502.06655.</p>
<p>Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. 2025b.Judgelrm: Large reasoning models as a judge.arXiv preprint arXiv:2504.00050.</p>
<p>Wei-Lin Chen, Zhepei Wei, Xinyu Zhu, Shi Feng, and Yu Meng. 2025c.Do llm evaluators prefer themselves for a reason?arXiv preprint arXiv:2504.03846.</p>
<p>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, and 1 others.2024e.Do not think that much for 2+ 3=? on the overthinking of o1-like llms.arXiv preprint arXiv:2412.21187.</p>
<p>Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, and 1 others.2025d.Rm-r1: Reward modeling as reasoning.arXiv preprint arXiv:2505.02387.</p>
<p>Yen-Shan Chen, Jing Jin, Peng-Ting Kuo, Chao-Wei Huang, and Yun-Nung Chen.2024f.Llms are biased evaluators but not biased for retrieval augmented generation.ArXiv preprint, abs/2410.20833.</p>
<p>A Attribute Definition</p>
<p>We provide a detailed definition for each judgment attribute in Table 1.</p>
<p>B Prompting Methods Categories</p>
<p>C Application with More Details</p>
<p>C.1 Evaluation LLM-as-a-judge is first proposed for evaluation.It enables human-like evaluations rather than overlapbased matching (Post, 2018;Lin and Chen, 2023b).</p>
<p>We discuss how LLM-as-a-judge has been utilized to evaluate open-ended generation (Section C.1.1),reasoning (Section C.1.2),and emerging NLP tasks (Section C.1.3).</p>
<p>C.1.1 Open-ended Generation Tasks</p>
<p>Open-ended generation includes tasks like dialog response, text summarization, and creative writing, where outputs must be safe, accurate, and contextually relevant with multiple "correct" answers (Badshah and Sajjad, 2024;Kumar et al., 2024b;Zeng et al.;Song et al., 2024a;Jones et al., 2024).Unlike traditional metrics, LLM-as-a-judge enables nuanced and adaptable evaluation (Zheng et al., 2023).This approach has been used for single-model evaluations and competitive comparisons (Gao et al., 2023;Wu et al., 2023).While LLMs-as-judges demonstrate human-like judgments, longer outputs risk hallucinations (Wang et al., 2024a;Cheng et al., 2023).Another concern is biased and unsafe judgements (Yu et al., 2024a;Li et al., 2024g;Ye et al., 2024a), though excessive caution may cause overly refusal (Xie et al., 2024a).To address these, researchers have proposed conversational frameworks like self-reflection (Ji et al., 2023) and debating (Moniri et al., 2024).Besides, multilingual LLM-as-a-judge research has advanced with various methods and benchmarks that address cross-lingual evaluation challenges.Approaches include scoring non-English answers against English references (Doddapaneni et al., 2024a), using multi-agent debate frameworks for fine-grained evaluation (Feng et al., 2024), and developing opensource multilingual judges that outperform Englishcentric evaluators across 20+ languages (Pombal et al., 2025b).Benchmarks like MM-Eval and PARIKSHA test the consistency and fairness of multilingual LLM judges, showing that evaluators tuned in English often underperform on lowresource languages (Son et al., 2024b).However, key challenges still remain in LLMbased multilingual judgment.Studies highlight cross-lingual inconsistency, where judges show low agreement across languages, especially for lowresource settings (Fu and Liu, 2025).Evaluators may also suffer from factual errors, cultural misrepresentations, and toxic content (Hada et al., 2024).Additionally, dialectal variation further complicates the bias, with weaker alignment between LLM and human toxicity ratings in regional varieties [8].These issues underscore the need for more culturally sensitive and robust multilingual evaluation methods.</p>
<p>C.1.2 Reasoning Tasks</p>
<p>The reasoning abilities of LLMs can be assessed through their intermediate thinking processes and final answers (He et al., 2023;Parmar et al., 2024;Mondorf and Plank, 2024).For mathematical reasoning, Xia et al. (2024) introduce a framework using judge LLMs to assess the quality of reasoning steps.Similarly, for temporal reasoning, Fatemi et al. ( 2024) create synthetic datasets to evaluate models' ability to reason about event sequences, causality, and dependencies.To distinguish genuine reasoning ability from pattern memorization, Wang et al. (2023a) propose a human-inthe-loop framework where LLMs and users adopt opposing positions to reach correct decisions.Nan et al. ( 2024) develop a multi-agent framework simulating peer review, leveraging LLMs-as-judges to collaboratively assess reasoning capabilities in data-driven tasks.</p>
<p>C.1.3 Emerging Tasks</p>
<p>LLM-as-a-judge is also applied to tasks once exclusive to humans, particularly in context-specific</p>
<p>Attribute Definition</p>
<p>Helpfulness</p>
<p>Helpfulness is a critical criterion to measure the utility and informativeness of a generated response.</p>
<p>Safety &amp; Security</p>
<p>Safety &amp; security refer to whether the model avoids generating and is not affected by harmful, toxic, biased, or adversarial content.</p>
<p>Reliability Reliability is the degree to which a response is faithful to verifiable sources and appropriately calibrated in expressing uncertainty.</p>
<p>Relevance</p>
<p>Relevance is a metric to measure how well a response aligns with the user query, topic, or task context.</p>
<p>Logic</p>
<p>Logic refers to the internal coherence and correctness of reasoning steps within a response, independent of factual accuracy.</p>
<p>Overall Quality Overall quality is a holistic assessment of a response's merit, typically integrating multiple dimensions into one comprehensive score.</p>
<p>Table 1: Common judgment attributes and their definitions.</p>
<p>areas.A prominent task is in social intelligence, where models are presented with complex social scenarios requiring the understanding of cultural values, ethical principles, and potential social impacts (Xu et al., 2024a;Zhou et al., 2023).</p>
<p>Research has also extended to evaluating Large Multimodal Models (LMMs) and Large Vision-Language Models (LVLMs) (Zhu et al., 2024b).For example, Xiong et al. (2024) use LMM-as-ajudge to provide transparent evaluations with rationales, while Chen et al. ( 2024d) propose a benchmark for LVLMs in self-driving scenarios, showing that LLM-based evaluations align better with human preferences than LVLM-based ones.Recently, we have seen more customized utilization of LLMas-a-judge to evaluate emerging tasks such as code understanding and generation (Zhao et al., 2024a;Zhuo, 2024;Tseng et al., 2024;Wu et al., 2024c;He et al., 2025;Yu et al.;Wang et al., 2025b;Prasad et al., 2025;Liu et al., 2025b;Chi et al., 2025), legal knowledge (Fei et al., 2023), game development (Isaza-Giraldo et al., 2024), nature science (Bi et al., 2023;Chuang et al., 2025;Kim et al., 2025), manufacture engineering (Liu et al., 2025a), healthcare conversations (Wang et al., 2024m;Zhang et al., 2024a;Zhou et al., 2024c), debating judgment (Liang et al., 2024a), RAG (Dhole et al., 2024;Saad-Falcon et al., 2024a;Jin et al., 2024;Liu et al., 2025c;Seo et al., 2025), biomedical application (Brake and Schaaf, 2024; Zheng et al., 2025;Zhang et al., 2024i), paper review (Zhou et al., 2024e;Wang et al., 2024c;Zhu et al., 2025;Kirtani et al., 2025), novelty &amp; creativity evaluation (Olson et al., 2024;Feng et al., 2025;Sawicki et al., 2025), and human-computer interaction (Li et al., 2024k).</p>
<p>C.2 Alignment</p>
<p>Alignment tuning is a vital technique to align LLMs with human preferences and values (Wei et al., 2022a;Ouyang et al., 2022;Rafailov et al., 2023).</p>
<p>In this section, we discuss the use of larger LLMs as judges (Section C.2.1) and self-judging (Section C.2.2) for alignment.</p>
<p>C.2.1 Larger Models as Judges</p>
<p>Recently, alignment tuning leverages feedback from larger LLMs to guide smaller models.Bai et al. (2022) first propose to train reward models with synthetic preferences from pre-trained LLMs.Following this, there are also some works explore online learning (Guo et al., 2024)   ing capabilities, including meta-rewarding (Wu et al., 2024a), Judge-Augmented Supervised Fine-Tuning (JSFT) (Lee et al., 2024a) and selfevaluation (Zhang et al., 2024g).To guarantee the quality of synthetic pairwise data, Pace et al. ( 2024) introduce West-of-N approach while Tong et al. (2024) apply self-filtering to produce high-quality synthetic data pairs for reasoning tasks.To reduce computational overhead, Zhai et al. (2024) propose ranked pairing for self-preferring models.Liu et al. (2024e) introduce meta-ranking, enabling smaller LLMs to act as judges and combining this method with Kahneman-Tversky optimization for post-SFT alignment.Besides pairwise data, (Liang et al., 2024c) and (Yasunaga et al., 2024) leverage LLM-as-a-judge to filter synthetic instruction tuning data.Other works adopt self-assessment and self-judgment in specific domains, such as robotics (Zeng et al., 2024;Yi et al., 2024) andmultimodal (Ahn et al., 2024).
Direct Optimizing Reward Modeling Various Tasks Improvement Diagnosis Analysis Response Candidate Documents Traditional Retrieval RAG External Tools Reasoning Path Retrieval Reasoning Alignment Evaluation</p>
<p>C.3 Retrieval</p>
<p>In traditional retrieval, LLM-as-a-judge ranks documents by relevance with minimal labeled data (Section C.3.1).LLM judges can also enhance the RAG system by dynamically integrating retrieved knowledge into the final response (Section C.3.2).</p>
<p>C.3.1 Traditional Retrieval</p>
<p>LLMs enhance document ranking by employing methods like permutation-based ranking (Sun et al., 2023), fine-grained relevance labeling (Zhuang et al., 2024a), and listwise reranking without taskspecific training (Ma et al., 2023).Moreover, Setwise (Zhuang et al., 2024b) and Pairwise Ranking Prompting (PRP) (Qin et al., 2024) offer a costefficient alternative for complex tasks.Tang et al. (2024b) introduce a permutation self-consistency technique that averages across multiple orders to obtain order-independent rankings.Domainspecific knowledge retrieval with LLM-as-a-judge includes legal information, recommender systems and searching (Ma et al., 2024;Hou et al., 2024;Thomas et al., 2023) 2024) present an LLM-based evaluation framework using synthetic queries to judge RAG agent performance.Zhang et al. (2024c) study LLMs' ability to assess relevance versus utility.In the biomedical area, several studies explore the usage of LLM-as-a-judge for active and dynamic retrival (Wang et al., 2024b) or retrieved knowledge filtering (Jeong et al., 2024;Li et al., 2024c).</p>
<p>C.4 Reasoning</p>
<p>Reasoning is a critical aspect of LLMs because it directly affects their ability to solve complex problems.Recently, many studies leverage LLM-as-ajudge in reasoning path selection (Section C.4.1) and external source utilization (Section C.4.2).</p>
<p>C.4.1 Reasoning Path Selection</p>
<p>While many complex reasoning and cognition structures emerges for LLMs' reasoning (Yao et al., 2023a;Hao et al., 2023), one crucial challenge is how to select a reasonable and reliable reasoning path or trajectory for LLMs to reason.To achieve this, LLM-as-a-judge has been introduced.Some works adopt the reasoner LLMs to perform selfassessment, alternatively executing reasoning and judging steps to achieve the best result (Lahoti et al., 2023;Creswell et al., 2023;Xie et al., 2024c;Kawabata and Sugawara, 2024) or perform sample-level selection among a group of candidates (Musolesi, 2024).Additionally, there are also many work train LLM-based verifiers, leveraging the judge LLM as the process reward model (PRM) to evaluate each state (Lightman et al., 2023;Setlur et al., 2024;Zhang et al., 2024d;Ye et al., 2025).Besides, there are also studies train critique-based LLM judges (Xu et al., 2024c;Ankner et al., 2024;Yu et al., 2024b;Wang et al., 2024e;Lan et al.;Xie et al., 2024b) which provide fine-grained verbal feedback to boost the reasoning process.</p>
<p>C.4.2 Reasoning with External Source</p>
<p>Selecting an appropriate external source to use is essential in the success of agentic LLM systems (Xi et al., 2023;Wang et al., 2024d).Auto-GPT (Yang et al., 2023) is the first to benchmark LLMs' performance in real-world decision-making scenarios.Following them, many other works adopt LLM-as-a-judge in various external tool selection applications, including autonomous driving (Sha et al., 2023), reasoning structure selection (Zhou et al., 2024d) and multi-modal area (Zhao et al., 2024b).In addition to selecting among external tools or APIs, LLM-as-a-judge has also been widely adopted as a controller in multi-agent systems, to selectively activate agents for a given problem (Ong et al., 2024) or to assess and manage message flow among a group of agents (Liang et al., 2023;Li et al., 2024b).</p>
<p>C.5 Definition of each LLM-as-a-judge Benchmark Category</p>
<p>We provide the definition of each LLM-as-a-judge benchmark in Table 2.</p>
<p>Figure 1 :
1
Figure 1: Overview of I/O formats of LLM-as-a-judge.</p>
<p>Figure 2 gives an overview summarization of what aspects can be assessed by the LLM judges.</p>
<p>Figure 3 :
3
Figure 3: Overview of prompting strategies for LLM-as-a-judge.</p>
<p>Figure 4 :
4
Figure 4: Overview of application and scenario for LLM-as-a-judge.</p>
<p>(Li et al., 2025a;Goel et al., 2025;024;2024; Chen et al., 2024b)and well-formatted(Chen  et al., 2024b)responses.In addition, other biases stem from individual judges' own preferences or knowledge, such as egocentric bias(Liu et al., 2023c;Wataoka et al., 2024; Panickssery et al.,  2024; Chen et al., 2025c)and preference leakage(Li et al., 2025a;Goel et al., 2025; Naseh and  Mireshghallah, 2025).LLM judges are also suscep-
accuracy and fairness (Chen et al., 2025c; Wanget al., 2025a) of judge LLMs have seen significantimprovements. A straightforward approach to scal-ing judge LLMs is to employ Large ReasoningModels (LRMs) that generate judgments via longCoT reasoning (Chen et al., 2025b). Additionally,traditional sampling and search strategies, suchas self-consistency, best-of-N, and Monte CarloTree Search (MCTS), have been used to more thor-oughly explore the space of possible judgmenttrajectories (Wang et al., 2025f; Kalra and Tang,2025). Other methods leverage golden labels as su-pervision, applying rule-based reinforcement learn-ing (Chen et al., 2025b; Liu et al., 2025e; White-house et al., 2025; Chen et al., 2025d; Shi and Jin,Future Direction. Existing studies have alreadyexplored approaches, such as providing more de-tailed evaluation principles (Zheng et al., 2023; Zhuet al., 2023; Liusie et al., 2023; Krumdick et al.,2025) and eliminating spurious features throughcalibration (Li et al., 2024d; Raina et al., 2024;Zhou et al., 2024b; Liu et al., 2024c; Chen et al.,2025a; Wang et al., 2025c; van den Burg et al.,2025), to mitigate LLM judges' bias. Future workcould focus more on analyzing and understand-ing the root causes of these biases. For example,why would LLMs prefer their own generation (Pan-ickssery et al., 2024)?7.2 Scaling Judgment at Inference Time.Motivated by recent inference-time scaling (ITS)studies in LLMs (Snell et al., 2024; Zhang et al.,2025b), several works have begun to explore howto scale LLMs' judgment capabilities at inferencetime (Saha et al., 2025; Liu et al., 2025e; Zhouet al., 2025). By expanding the reasoning processin judgment tasks and incorporating advanced be-haviors such as reflection and exploration, both the
(Shi et al., 2024;Raina et al., 2024)24;Doddapaneni et al., 2024b) 2025)cks(Shi et al., 2024; BENCH- MARK; Banerjee et al., 2024;Tong et al., 2025)and adversarial phrases(Liusie et al., 2023;Raina et al., 2024;Doddapaneni et al., 2024b)can drastically influence LLMs' judgment, thus raising concerns about the reliability of LLM judges in high-stakes scenarios(Shi et al., 2024;Raina et al., 2024).</p>
<p>Yi Chen, RuiWang, Haiyun Jiang, Shuming Shi, and  Ruifeng Xu. 2023.Exploring the use of large language models for reference-free text quality evaluation: An empirical study.In Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings), pages 361-374.
Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou,Chenhang Cui, Zhenzhen Weng, Haoqin Tu, ChaoqiWang, Zhengwei Tong, Qinglan Huang, and 1 others.2024g. Mj-bench: Is your multimodal reward modelreally a good judge for text-to-image generation?ArXiv preprint, abs/2407.04842.Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, SiyinWang, Xiangyang Liu, Mozhi Zhang, Junliang He,Mianqiu Huang, Zhangyue Yin, Kai Chen, and 1 oth-ers. 2023. Evaluating hallucinations in chinese largelanguage models. ArXiv preprint, abs/2310.03368.Wayne Chi, Valerie Chen, Anastasios Nikolas An-gelopoulos, Wei-Lin Chiang, Aditya Mittal, NamanJain, Tianjun Zhang, Ion Stoica, Chris Donahue, andAmeet Talwalkar. 2025. Copilot arena: A platformfor code llm evaluation in the wild. arXiv preprintarXiv:2502.09328.Cheng-Han Chiang and Hung-yi Lee. 2023a. Can largelanguage models be an alternative to human evalua-tions? In Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 15607-15631, Toronto,Canada. Association for Computational Linguistics.Cheng-Han Chiang and Hung-yi Lee. 2023b. A closerlook into automatic evaluation using large languagemodels. arXiv preprint arXiv:2310.05657.Cheng-Han Chiang, Hung-yi Lee, and Michal Lukasik.2025. Tract: Regression-aware fine-tuning meetschain-of-thought reasoning for llm-as-a-judge. arXivpreprint arXiv:2503.04381.Marianne Chuang, Gabriel Chuang, Cheryl Chuang,and John Chuang. 2025. Judging it, washing it:Scoring and greenwashing corporate climate disclo-sures using large language models. arXiv preprintarXiv:2502.15094.Antonia Creswell, Murray Shanahan, and Irina Higgins.2023. Selection-inference: Exploiting large languagemodels for interpretable logical reasoning. In TheEleventh International Conference on Learning Rep-resentations, ICLR 2023, Kigali, Rwanda, May 1-5,2023. OpenReview.net.Preetam Prabhu Srikar Dammu, Himanshu Naidu, andChirag Shah. 2025. Dynamic-kgqa: A scalableframework for generating adaptive question answer-ing datasets. arXiv preprint arXiv:2503.05049.Soumik Dey, Hansi Wu, and Binbin Li. 2025. To judgeor not to judge: Using llm judgements for adver-tiser keyphrase relevance at ebay. arXiv preprintarXiv:2505.04209.</p>
<p>Raju et al. (2024))propose the Memory-of-Thought (MoT) framework, where LLMs store and recall reasoning to enhance response relevance.Tang et al. (2024a)introduce Self-Retrieval, an architecture integrating retrieval into document generation, enabling end-to-end IR within a single LLM.Similarly, Asai et al. (2024) develop SELF-RAG, combining retrieval with selfreflection to enhance response quality.In the do-the general accuracy performance of LLM judges (e.g., MT-Bench)Bias QuantificationBenchmarks focused on measuring and analyzing biases in LLM judgments (e.g., CALM)Challenging Performance Benchmarks that test LLM judges on difficult or adversarial tasks designed to probe the limits of their evaluation capabilities (e.g., Arena-Hard) Domain-Specific Performance Benchmarks that measure LLM judges' effectiveness in specific domains, such as biomedical, legal, and coding evaluation (e.g.,Raju et al. (2024))
BenchmarkDefinitionGeneral PerformanceBenchmarks that assessC.3.2 Retrieval-Augmented Generation(RAG)</p>
<p>Table 2 :
2
Categories of benchmarks for evaluating LLM judges.main of Q&amp;A,Rackauckas et al. (</p>
<p>More resources on LLM-as-a-judge are on the website: https://llm-as-a-judge.github.io
We have released and will maintain a paper list about LLM-as-a-judge at: https://github.com/ llm-as-a-judge/Awesome-LLM-as-a-judge
AcknowledgmentThis material is based upon work supported by the U.S. Department of Homeland Security under Grant Award Number 17STQAC00001-08-00.The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S. Department of Homeland Security.Lu Cheng is supported by the National Science Foundation (NSF) Grant #2312862, NSF CAREER #2440542, NSF-Simons SkAI Institute, National Institutes of Health (NIH) #R01AG091762, Google Research Scholar Award, and a Cisco gift grant.llm-as-a-judge approach for scalable gender-neutral translation evaluation.arXiv preprint arXiv:2504.11934.D TaxonomyLLM-as-a-judge Attributes ( §3) Helpfulness ( §3.1) Constitutional AI(Bai et al., 2022), RLAIF(Lee et al., 2023), MT-Bench(Zheng et al., 2023), Just-Eval(Lin et al., 2023), Starling(Zhu et al., 2024a), AUTO-J(Li et al., 2024e), OAIF(Guo et al., 2024), Safety &amp; Security ( §3.2) LLaMA Guard(Inan et al., 2023), TRUSTGPT(Huang et al., 2023b), Moral Choice(Scherrer et al., 2023), SORRY-Bench(Xie et al., 2024a), FLASK(Ye et al., 2024b), R-judge(Yuan et al., 2024b), Do-not-answer(Wang et al., 2024l), RAIN(Li et al.)Reliability ( §3.3) FactScore(Min et al., 2023), HALU-J(Wang et al., 2024a), HalluJudge(Luo et al., 2024a),HalluQA (Cheng et al., 2023), SaySelf(Xu et al., 2024d),(Wei et al., 2024b), Self-Alignment for Factuality(Zhang et al., 2024g), FaithScore(Jing et al., 2024), FENCE(Xie et al., 2024b)Relevance ( §3.4) LLM-Eval(Lin and Chen, 2023a), MoT(Li and Qiu, 2023a),(Abbasiantaeb et al., 2024), DALK(Li et al., 2024c),MJ-Bench (Chen et al., 2024g),(Thomas et al., 2024),(Ma et al., 2024), LLMRank(Hou et al., 2024), LLM Evaluation (Chiang and Lee, 2023a),(Yang andLin, 2024), (Chen et al., 2024a), LLM-SQL-Solver(Zhao et al., 2023a),(Lin et al., 2025)Logic ( §3.5) RAP(Hao et al., 2023), ToT(Yao et al., 2023a), Auto-GPT(Yang et al., 2023),GoT (Besta et al., 2024), Diffagent(Zhao et al., 2024b), Routellm(Ong et al., 2024), MAD(Liang et al., 2023), SMoA(Li et al., 2024b)Overall Quality ( §3.6)(Gao et al., 2023), Just-Eval(Lin et al., 2023), ICE(Jain et al., 2023a), LLM-Eval(Lin and Chen, 2023b), GEMBA(Kocmi and Federmann, 2023), KIEVAL(Yu et al., 2024c), OAIF(Guo et al., 2024), Comp-Analysis(Zhang et al., 2024a), LostITS(Huang et al., 2024b)Methodology ( §4) Tuning ( §4.1) Data Source ( §4.1.1)Manually-labeled ( §4.1.1)AttrScore(Yue et al., 2023), PandaLM(Wang et al., 2024k), InstructScore(Xu et al., 2023a), SELF-JUDGE(Lee et al., 2024a), X-Eval(Liu et al., 2024a), CritiqueLLM(Ke et al., 2024), FLAMe(Vu et al., 2024), Synthetic Feedback ( §4.1.1)JudgeLM(Zhu et al., 2023), AUTO-J(Li et al., 2024e), Meta-Rewarding(Wu et al., 2024a), Self-Taught(Wang et al., 2024h), HALU-J(Wang et al., 2024a),OFFSETBIAS(Park et al., 2024), SORRY-Bench(Xie et al., 2024a), LLaVA-Critic(Xiong et al., 2024), PROMETHEUS2(Kim et al., 2024b), InstructScore(Xu et al., 2023a), Tuning Techniques ( §4.1.2)Supervised F-Tuning ( §4.1.2)PerSE(Wang et al., 2023b), INSTRUCTSCORE(Xu et al., 2023a), CRITIQUE-LLM(Ke et al., 2024), PandaLM(Wang et al., 2024k), X-Eval(Liu et al., 2024a), AUTO-J(Li et al., 2024e), JudgeLM(Zhu et al., 2023), SORRY-Bench(Xie et al., 2024a), AttrScore(Yue et al., 2023), FLAMe(Vu et al., 2024), PROMETHEUS2(Kim et al., 2024b), SELF-JUDGE(Lee et al., 2024a), Critique-LLM(Ke et al., 2024), X-Eval(Liu et al., 2024a), Reinforcement Learning ( §4.1.2)HALU-J(Wang et al., 2024a), OFFSETBIAS (Park et al., 2024), Themis(Hu et al., 2024b), Meta-Rewarding(Wu et al., 2024a), Self-Taught(Wang et al., 2024h), PORTIA(Li et al., 2024p)Prompting ( §4.2) Swapping Operation ( §4.2.1) MT-Bench(Zheng et al., 2023), RLAIF(Lee et al., 2023), SALMON(Sun et al., 2024), SELF-JUDGE(Lee et al., 2024a), Starling(Zhu et al., 2024a)Rule Augmentation ( §4.2.2) Constitutional AI(Bai et al., 2022), MoT(Li and Qiu, 2023a),(Lahoti et al., 2023), RLAIF(Lee et al., 2023),LRQ-Fact (Beigi et al., 2024), AUTO-J(Li et al., 2024e),(Bai et al., 2023a),(Gao et al., 2023), Prometheus(Kim et al.), KIEVAL(Yu et al., 2024c), CEB(Wang et al., 2024g),(Murugadoss et al., 2024),(Liu et al., 2024c), OAIF(Guo et al., 2024), SALMON(Sun et al., 2024), SELF-JUDGE(Lee et al., 2024a), DALK(Li et al., 2024c),(Qian et al., 2024), RevisEval(Zhang et al., 2024f), LLM-as-a-personalized-judge(Dong et al., 2024),(Li et al., 2024m),(Li et al., 2024h)Multi-Agent Collaboration ( §4.2.3) PRD(Li et al., 2023b),(Zhang et al., 2023),(Wu et al., 2023), MPA(Zhu et al., 2024c), JudgeLM(Zhu et al., 2023), ChatEval(Chan et al., 2023), CoEvol(Li et al., 2024j)LRQ-Fact (Beigi et al., 2024), Cascaded Selective Evaluation(Jung et al., 2024), Fellowship (Arif et al., 2024), MATEval(Li et al., 2024n),(Zhang et al., 2024e)Demonstration ( §4.2.4) ICE(Jain et al., 2023b), Little Giants(Kotonya et al., 2023), ALLURE(Hasanbeig et al., 2023), MSoR(Song et al., 2024b)Multi-Turn Interaction ( §4.2.5) LLM-as-an-examine(Bai et al., 2023b), KIEVAL(Yu et al., 2024c), Auto-Arena(Zhao et al., 2024c),(Moniri et al., 2024)Comparison Acceleration ( §4.2.6)(Liu et al., 2023a), OSP(Zhai et al., 2024), Starling(Zhu et al., 2024a), SELF-JUDGE(Lee et al., 2024a)Application ( §5) Evaluation ( §5.1)(Bi et al., 2023),(Fei et al., 2023),(Zhou et al., 2023),(Wang et al., 2023a),(Nan et al., 2024),(Zheng et al., 2023),(Gao et al., 2023),(Wu et al., 2023), (Cheng et al., 2023),(Lin and Chen, 2023b),(Mondorf and Plank, 2024), (Badshah and Sajjad, 2024),(Bai et al., 2023a),(Kumar et al., 2024b),(Wang et al., 2024a),(Li et al., 2024g),(Xie et al., 2024a), (Chan et al., 2023),(Moniri et al., 2024),(Xia et al., 2024), (Fatemi et al., 2024), (Parmar et al., 2024),(Xu et al., 2024a),(Xiong et al., 2024), (Chen et al., 2024d),(Zhao et al., 2024a),(Isaza-Giraldo et al., 2024),(Wang et al., 2024m),(Zeng et al.),(Yu et al., 2024a),(Dhole et al., 2024),(Yang et al., 2024),(Xu et al., 2024b),(Wu et al., 2024b)Alignment ( §5.2)(Bai et al., 2022),(Lee et al., 2023),(Sun et al., 2024),(Guo et al., 2024), (Arif et al., 2024),(Li et al., 2024j),(Yuan et al., 2024c),(Wu et al., 2024a),(Pace et al., 2024),(Lee et al., 2024a),(Tong et al., 2024),(Zhai et al., 2024),(Liu et al., 2024e),(Liang et al., 2024c),(Zhang et al., 2024g),(Zeng et al., 2024), (Ahn et al., 2024),(Weyssow et al., 2024),(Wang et al., 2024f),(Yasunaga et al., 2024),(Sengupta et al., 2024)Retrieval ( §5.3)(Sun et al., 2023),(Thomas et al., 2023),(Ma et al., 2023),(Tang et al., 2024b),(Qin et al., 2024),(Ma et al., 2024),(Hou et al., 2024),(Li and Qiu, 2023a),(Tang et al., 2024a), (Asai et al., 2024)(Zhuang et al., 2024a),(Rackauckas et al., 2024),(Zhang et al., 2024c),(Wang et al., 2024b),(Li et al., 2024c),(Jeong et al., 2024),(Zhuang et al., 2024b), (Chen et al., 2024f)Reasoning ( §)(Yao et al., 2023b), (Creswell et al., 2023),(Wei et al., 2022b),(Yao et al., 2023a),(Yang et al., 2023),(Sha et al., 2023),(Hao et al., 2023),(Zhou et al., 2024d),(Lahoti et al., 2023),(Liang et al., 2023),(Li et al., 2024b), (Besta et al., 2024),(Ong et al., 2024),(Zhao et al., 2024b),(Kawabata and Sugawara, 2024),(Xie et al., 2024b), (Lightman et al., 2023),(Li et al.),(Setlur et al., 2024)Figure5: Taxonomy of research in LLM-as-a-judge that consists of judging attribution, methodology and application.E Tuning MethodsF Benchmark G AI Assistants In WritingWe acknowledge the use of ChatGPT-4o in paper polishing, but not in any direct paper writing or relevant work collections.
Can we use large language models to fill relevance judgment holes?. Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, Mohammad Aliannejadi, ArXiv preprint, abs/2405.056002024</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, preprint, abs/2303.08774Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. </p>
<p>Many-shot in-context learning. Rishabh Agarwal, Avi Singh, M Lei, Bernd Zhang, Luis Bohnet, Rosias, C Y Stephanie, Biao Chan, Aleksandra Zhang, Hugo Faust, Larochelle, ICML 2024 Workshop on In-Context Learning. </p>
<ol>
<li>i-srt: Aligning large multimodal models for videos by iterative self-retrospective judgment. Yura Daechul Ahn, San Choi, Youngjae Kim, Dongyeop Yu, Jonghyun Kang, Choi, abs/2406.11280ArXiv preprint. </li>
</ol>
<p>Generative information retrieval evaluation. Marwah Alaofi, Negar Arabzadeh, L A Charles, Mark Clarke, Sanderson, Information Access in the Era of Generative AI. Springer2024</p>
<p>Liangming Pan, Haewon Jeong, and 1 others. 2024. A survey on data selection for language models. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, abs/2402.16827ArXiv preprint. </p>
<p>Argument summarization and its evaluation in the era of large language models. Moritz Altemeyer, Steffen Eger, Johannes Daxenberger, Tim Altendorf, Philipp Cimiano, Benjamin Schiller, arXiv:2503.008472025arXiv preprint</p>
<p>Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, Prithviraj Ammanabrolu, arXiv:2408.11791Critique-out-loud reward models. 2024arXiv preprint</p>
<p>Conqret: Benchmarking fine-grained evaluation of retrieval augmented argumentation with llm judges. D Kaustubh, Kai Dhole, Eugene Shu, Agichtein, 2024</p>
<p>Laura Dietz, Oleg Zendel, Peter Bailey, Charles Clarke, Ellese Cotterill, Jeff Dalton, Faegheh Hasibi, Mark Sanderson, Nick Craswell, arXiv:2504.19076Llmevaluation tropes: Perspectives on the validity of llm-evaluations. 2025arXiv preprint</p>
<p>Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, arXiv:2410.13394Dilip Venkatesh, Raj Dabre, Anoop Kunchukuttan, and Mitesh M Khapra. 2024a. Crosslingual auto evaluation for assessing multilingual llms. arXiv preprint</p>
<p>Finding blind spots in evaluator llms with interpretable checklists. Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M Khapra, abs/2406.13439ArXiv preprint. 2024b</p>
<p>Same evaluation, more tokens: On the effect of input length for machine translation evaluation using large language models. Tobias Domhan, Dawei Zhu, arXiv:2505.017612025arXiv preprint</p>
<p>A survey on in-context learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, 2023ArXiv preprint, abs/2301.00234</p>
<p>Can llm be a personalized judge?. Yijiang River, Dong , Tiancheng Hu, Nigel Collier, ArXiv preprint, abs/2406.116572024</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-controlled alpacaeval: A simple way to debias automatic evaluators. 2024arXiv preprint</p>
<p>Know thy judge: On the robustness meta-evaluation of llm safety judges. Francisco Eiras, Eliott Zemour, Eric Lin, Vaikkunth Mugunthan, arXiv:2503.044742025arXiv preprint</p>
<p>Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and llm-as-a-judge. Aparna Elangovan, Lei Xu, Jongwoo Ko, Mahsa Elyasi, Ling Liu, Sravan Bodapati, Dan Roth, arXiv:2410.037752024arXiv preprint</p>
<p>Perspectives on large language models for relevance judgment. Guglielmo Faggioli, Laura Dietz, L A Charles, Gianluca Clarke, Matthias Demartini, Claudia Hagen, Noriko Hauff, Kando, Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval. the 2023 ACM SIGIR International Conference on Theory of Information Retrieval2023Evangelos Kanoulas, Martin Potthast, Benno Stein, and 1 others</p>
<p>Sedareval: Automated evaluation using self-adaptive rubrics. Zhiyuan Fan, Weinong Wang, Xing Wu, Debing Zhang ; Bahare, Mehran Fatemi, Anton Kazemi, Karishma Tsitsulin, Jinyeong Malkan, John Yim, Sungyong Palowitch, Seo, arXiv:2501.15595abs/2406.09170Test of time: A benchmark for evaluating llms on temporal reasoning. 2025. 2024ArXiv preprintJonathan Halcrow, and Bryan Perozzi</p>
<p>Lawbench: Benchmarking legal knowledge of large language models. Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge, ArXiv preprint, abs/2309.162892023</p>
<p>Grapheval: A lightweight graph-based llm framework for idea evaluation. Tao Feng, Yihang Sun, Jiaxuan You, arXiv:2503.126002025arXiv preprint</p>
<p>M-mad: Multidimensional multi-agent debate for advanced machine translation evaluation. Zhaopeng Feng, Jiayuan Su, Jiamei Zheng, Jiahan Ren, Yan Zhang, Jian Wu, Hongwei Wang, Zuozhu Liu, arXiv:2412.201272024arXiv preprint</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See Kiong Ng, Zhengbao Jiang, Pengfei Liu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>Xiyan Fu, Wei Liu, arXiv:2505.12201How reliable is multilingual llm-as-a-judge?. 2025arXiv preprint</p>
<p>Llm-based nlg evaluation: Current status and challenges. Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan, abs/2402.013832024ArXiv preprint</p>
<p>Human-like summarization evaluation with chatgpt. Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, abs/2304.025542023ArXiv preprintShiping Yang, and Xiaojun Wan</p>
<p>Trueteacher: Learning factual consistency evaluation with large language models. Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, Idan Szpektor, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, Asaf Yehudai, arXiv:2412.09569Justrank: Benchmarking llm judges for system ranking. 2024arXiv preprint</p>
<p>Great models think alike and this undermines ai oversight. Shashwat Goel, Joschka Struber, Amanda Ilze, Auzina, K Karuna, Ponnurangam Chandra, Douwe Kumaraguru, Ameya Kiela, Matthias Prabhu, Jonas Bethge, Geiping, arXiv:2502.043132025arXiv preprint</p>
<p>Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, arXiv:2411.15594Honghao Liu, and 1 others. 2024. A survey on llm-as-a-judge. arXiv preprint</p>
<p>Luke Guerdan, Solon Barocas, Kenneth Holstein, Hanna Wallach, Zhiwei Steven Wu, Alexandra Chouldechova, arXiv:2503.05965Validating llm-as-a-judge systems in the absence of gold labels. 2025arXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, arXiv:2501.12948Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint</p>
<p>Direct language model alignment from online ai feedback. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, ArXiv preprint, abs/2402.04792Bilal Piot, and 1 others. 2024</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian De Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. 2024</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, Zhiting Hu, 10.18653/v1/2023.emnlp-main.507Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Allure: auditing and improving llm-based evaluation of text using iterative in-context-learning. Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, Ida Momennejad, 20232309arXiv eprints</p>
<p>Socreval: Large language models with the socratic method for reference-free reasoning evaluation. Hangfeng He, Hongming Zhang, Dan Roth, arXiv:2310.000742023arXiv preprint</p>
<p>Junda He, Jieke Shi, Terry Yue Zhuo, Christoph Treude, Jiamou Sun, Zhenchang Xing, Xiaoning Du, David Lo, arXiv:2503.02246From code to courtroom: Llms as the new software judges. 2025arXiv preprint</p>
<p>Cseval: Towards automated, multi-dimensional, and reference-free counterspeech evaluation using auto-calibrated llms. Amey Hengle, Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty, arXiv:2501.175812025arXiv preprint</p>
<p>Llm-as-a-judge: Reassessing the performance of llms in extractive qa. Xanh Ho, Jiahao Huang, Florian Boudin, Akiko Aizawa, arXiv:2504.119722025arXiv preprint</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, European Conference on Information Retrieval. 2024</p>
<p>Sougata Chaudhuri, and 1 others. 2024. Rate, explain and cite (rec): Enhanced explanation and attribution in automatic evaluation by large language models. James Aliyah R Hsu, Zhichao Zhu, Bin Wang, Shubham Bi, Mehrotra, Katherine Shiva K Pentyala, Xiang-Bo Tan, Roshanak Mao, Omrani, arXiv:2411.02448arXiv preprint</p>
<p>Developing an ai-based psychometric system for assessing learning difficulties and adaptive system to overcome: A qualitative and conceptual framework. Aaron Hu, abs/2403.062842024ArXiv preprint</p>
<p>Editable concept bottleneck models. Lijie Hu, Chenyang Ren, Zhengyu Hu, Hongbin Lin, Cheng-Long Wang, Zhen Tan, Weimin Lyu, Jingfeng Zhang, Hui Xiong, Di Wang, Forty-second International Conference on Machine Learning. </p>
<p>Renjun Hu, Yi Cheng, Libin Meng, Jiaxin Xia, Yi Zong, Xing Shi, Wei Lin, arXiv:2502.02988Training an llm-as-ajudge model: Pipeline, insights, and practical lessons. 2025aarXiv preprint</p>
<p>Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, Xiaojun Wan, arXiv:2402.12055Are llm-based evaluators confusing nlg quality criteria?. 2024aarXiv preprint</p>
<p>A dual-perspective nlg meta-evaluation framework with automatic benchmark and better interpretability. Xinyu Hu, Mingqi Gao, Li Lin, Zhenghan Yu, Xiaojun Wan, arXiv:2502.120522025barXiv preprint</p>
<p>Themis: A reference-free nlg evaluation language model with flexibility and interpretability. Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, Xiaojun Wan, 2024bArXiv preprint, abs/2406.18365</p>
<p>On the limitations of fine-tuned judge models for llm evaluation. Hui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao, arXiv:2403.028392024aarXiv preprint</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 10.18653/v1/2023.emnlp-main.67Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Lost in the source language: How large language models evaluate the quality of machine translation. Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Jiajun Chen, Shujian Huang, Annual Meeting of the Association for Computational Linguistics. 2024b</p>
<p>Trustgpt: A benchmark for trustworthy and responsible large language models. Yue Huang, Qihui Zhang, abs/2306.11507Lichao Sun, and 1 others. 2023bArXiv preprint</p>
<p>Davide Testuggine, and 1 others. 2023. Llama guard: Llm-based inputoutput safeguard for human-ai conversations. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, abs/2312.06674ArXiv preprint. </p>
<p>Prompt-gaming: A pilot study on llm-evaluating agent in a meaningful energy game. Andrés Isaza-Giraldo, Paulo Bala, Pedro F Campos, Lucas Pereira, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Alex Beutel, Alex Carney, and 1 others. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Multi-dimensional evaluation of text summarization with in-context learning. Sameer Jain, Vaishakh Keshava, Mysore Swarnashree, Patrick Sathyendra, Pengfei Fernandes, Graham Liu, Chunting Neubig, Zhou, 10.18653/v1/2023.findings-acl.537Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023a</p>
<p>Multi-dimensional evaluation of text summarization with in-context learning. Sameer Jain, Vaishakh Keshava, Mysore Swarnashree, Patrick Sathyendra, Pengfei Fernandes, Graham Liu, Chunting Neubig, Zhou, 10.18653/v1/2023.findings-acl.537Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023b</p>
<p>Improving medical reasoning through retrieval and self-reflection with retrievalaugmented large language models. Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang, Bioinformatics. 40Supplement_12024</p>
<p>Agent-as-judge for factual summarization of long narratives. Yeonseok Jeong, Minsoo Kim, Seung-Won Hwang, Byung-Hak Kim, arXiv:2501.099932025arXiv preprint</p>
<p>Towards mitigating llm hallucination via self reflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Safechain: Safety of language models with long chain-of-thought reasoning capabilities. Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran, arXiv:2502.120252025arXiv preprint</p>
<p>Genres: Rethinking evaluation for generative relation extraction in the era of large language models. Pengcheng Jiang, Jiacheng Lin, Zifeng Wang, Jimeng Sun, Jiawei Han, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>Ragrewardbench: Benchmarking reward models in retrieval augmented generation for preference alignment. Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, arXiv:2412.137462024arXiv preprint</p>
<p>Faithscore: Fine-grained evaluations of hallucinations in large vision-language models. Liqiang Jing, Ruosen Li, Yunmo Chen, Xinya Du, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>A multi-aspect framework for counter narrative evaluation using large language models. Jaylen Jones, Lingbo Mo, Eric Fosler-Lussier, Huan Sun, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesShort Papers20242</p>
<p>Trust or escalate: Llm judges with provable guarantees for human agreement. Jaehun Jung, Faeze Brahman, Yejin Choi, arXiv:2502.18018abs/2407.18370. Nimit Kalra and Leonard Tang. 2025. Verdict: A library for scaling judge-time compute. 2024arXiv preprint</p>
<p>Openlgauge: An explainable metric for nlg evaluation with open-weights llms. Ivan Kartáč, Mateusz Lango, Ondřej Dušek, arXiv:2503.118582025arXiv preprint</p>
<p>Rationaleaware answer verification by pairwise self-evaluation. Akira Kawabata, Saku Sugawara, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Critiquellm: Towards an informative critique generation model for evaluation of large language model generation. Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241Hongning Wang, and 1 others</p>
<p>others. Zachary Kenton, Noah Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah Goodman, On scalable oversight with weak llms judging strong llms. Advances in Neural Information Processing Systems. 202437</p>
<p>Autodriveqa-automated generation of multiple-choice questions for autonomous driving datasets using large vision-language models. Boshra Khalili, Andrew W Smyth, arXiv:2503.157782025arXiv preprint</p>
<p>Eunsu Kim, Juyoung Suk, Seungone Kim, Niklas Muennighoff, Dongkwan Kim, Alice Oh, arXiv:2412.10424Llm-as-an-interviewer: Beyond static testing through dynamic llm evaluation. 2024aarXiv preprint</p>
<p>Heegyu Kim, Taeyang Jeon, Seungtaek Choi, Ji Hoon Hong, Dong Won Jeon, Ga-Yeon, Gyeong-Won Baek, Dong-Hee Kwak, Jisu Lee, Bae, arXiv:2502.16457Chihoon Lee, and 1 others. 2025. Towards fully-automated materials discovery via large-scale synthesis dataset and expert-level llm-as-a-judge. arXiv preprint</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, The Twelfth International Conference on Learning Representations. James Thorne, and 1 others</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, ArXiv preprint, abs/2405.015352024b</p>
<p>Revieweval: An evaluation framework for aigenerated reviews. Chhavi Kirtani, Krishan Madhav, Tejash Garg, Tanmay Prasad, Murari Singhal, Dhruv Mandal, Kumar, arXiv:2502.117362025arXiv preprint</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023</p>
<p>Benchmarking cognitive biases in large language models as evaluators. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang, abs/2309.17012ArXiv preprint. 2023</p>
<p>Little giants: Exploring the potential of small LLMs as evaluation metrics in summarization in the Eval4NLP 2023 shared task. Neema Kotonya, Saran Krishnasamy, Joel Tetreault, Alejandro Jaimes, 10.18653/v1/2023.eval4nlp-1.17Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems. the 4th Workshop on Evaluation and Comparison of NLP SystemsBali, Indonesia2023Association for Computational Linguistics</p>
<p>No free labels. Michael Krumdick, Charles Lovering, Varshini Reddy, Seth Ebner, Chris Tanner, arXiv:2503.05061Limitations of llm-as-a-judge without human grounding. 2025arXiv preprint</p>
<p>Abhishek Kumar, Sonia Haiduc, arXiv:2409.00630Partha Pratim Das, and Partha Pratim Chakrabarti. 2024a. Llms as evaluators: A novel approach to evaluate bug report summarization. arXiv preprint</p>
<p>Decoding biases: Automated methods and llm judges for gender bias detection in language models. Saurav Shachi H Kumar, Sahisnu Sahay, Eda Mazumder, Ramesh Okur, Nicole Manuvinakurike, Hsuan Beckage, Hung-Yi Su, Lama Lee, Nachman, abs/2408.03907ArXiv preprint. 2024b</p>
<p>Improving diversity of demographic representation in large language models via collective-critiques and self-voting. Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex Beutel, Jilin Chen, 10.18653/v1/2023.emnlp-main.643Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023Association for Computational Linguistics</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Zick, arXiv:2403.13787Yejin Choi, and 1 others. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint</p>
<p>Criticeval: Evaluating large-scale language model as critic. Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-Ling Mao, The Thirty-eighth Annual Conference on Neural Information Processing Systems. </p>
<p>Rlaif: Scaling reinforcement learning from human feedback with ai feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, abs/2309.00267Abhinav Rastogi, and 1 others. 2023</p>
<p>Aligning large language models by on-policy selfjudgment. Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Min Kang, Youngjae Yoo, Yu, abs/2402.11253ArXiv preprint. 2024a</p>
<p>Fleur: An explainable reference-free evaluation metric for image captioning using a large multimodal model. Yebin Lee, Imseong Park, Myungjoo Kang, arXiv:2406.060042024barXiv preprint</p>
<p>Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, Huan Liu, arXiv:2502.01534Preference leakage: A contamination problem in llm-as-a-judge. 2025aarXiv preprint</p>
<p>Exploring large language models for feature selection: A datacentric perspective. Dawei Li, Zhen Tan, Huan Liu, abs/2408.12025ArXiv preprint. 2024a</p>
<p>Smoa: Improving multi-agent large language models with sparse mixture-of-agents. Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Satvik Kumar, Lijie Chaudhary, Jiayi Hu, Shen, abs/2411.03284ArXiv preprint. 2024b</p>
<p>Dalk: Dynamic co-augmentation of llms and kg to answer alzheimer's disease questions with scientific literature. Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sunkwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, abs/2405.04819Ying Ding, and 1 others. 2024c</p>
<p>Calibraeval: Calibrating prediction distribution to mitigate selection bias in llms-as-judges. Haitao Li, Junjie Chen, Qingyao Ai, Zhumin Chu, Yujia Zhou, Qian Dong, Yiqun Liu, ArXiv preprint, abs/2410.153932024d</p>
<p>Generative judge for evaluating alignment. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Pengfei Liu, The Twelfth International Conference on Learning Representations. 2024e</p>
<p>Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, arXiv:2411.17451Bill Yuchen Lin, and 1 others. 2024f. Vlrewardbench: A challenging benchmark for visionlanguage generative reward models. arXiv preprint</p>
<p>Salad-bench: A hierarchical and comprehensive safety benchmark for large language models. Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao, ArXiv preprint, abs/2402.050442024g</p>
<p>Hypoeval: Hypothesis-guided evaluation for natural language generation. Mingxuan Li, Hanchen Li, Chenhao Tan, arXiv:2504.071742025barXiv preprint</p>
<p>Minzhi Li, Zhengyuan Liu, Shumin Deng, Shafiq Joty, Nancy F Chen, Min-Yen Kan, arXiv:2405.15329Decompose and aggregate: A step-by-step interpretable evaluation framework. 2024harXiv preprint</p>
<p>Quantmoe-bench: Examining post-training quantization for mixture. Pingzhi Li, Xiaolong Jin, Zhen Tan, Yu Cheng, Tianlong Chen, arXiv:2406.081552024iof-experts. arXiv preprint</p>
<p>Exploring the reliability of large language models as customized evaluators for diverse nlp tasks. Qintong Li, Leyang Cui, Lingpeng Kong, Wei Bi, arXiv:2310.197402023aarXiv preprint</p>
<p>Coevol: Constructing better responses for instruction finetuning through multi-agent cooperation. Renhao Li, Minghuan Tan, Derek F Wong, Min Yang, abs/2406.07054ArXiv preprint. 2024j</p>
<p>Iqa-eval: Automatic evaluation of humanmodel interactive question answering. Ruosen Li, Ruochen Li, Barry Wang, Xinya Du, Advances in Neural Information Processing Systems. 2024k37</p>
<p>From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. Ruosen Li, Teerth Patel, Xinya Du ; Tianle, Wei-Lin Li, Evan Chiang, Lisa Frick, Tianhao Dunlap, Banghua Wu, Joseph E Zhu, Ion Gonzalez, Stoica, abs/2406.119392023b. 2024lArXiv preprintPrd: Peer rank and discussion improve large language model based evaluations</p>
<p>Rule-based data selection for large language models. Xiaomin Li, Mingye Gao, Zhiwei Zhang, Chang Yue, Hong Hu, arXiv:2410.047152024marXiv preprint</p>
<p>MoT: Memory-ofthought enables ChatGPT to self-improve. Xiaonan Li, Xipeng Qiu, 10.18653/v1/2023.emnlp-main.392Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023aAssociation for Computational Linguistics</p>
<p>Mot: Memory-ofthought enables chatgpt to self-improve. Xiaonan Li, Xipeng Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023b</p>
<p>Mateval: A multi-agent discussion framework for advancing open-ended text evaluation. Yu Li, Shenyu Zhang, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu, Guilin Qi, Dehai Min, International Conference on Database Systems for Advanced Applications. Springer2024n</p>
<p>Rain: Your language models can align themselves without finetuning. Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang, The Twelfth International Conference on Learning Representations. </p>
<p>Yuran Li, Jama Hussein Mohamud, Chongren Sun, Di Wu, Benoit Boulet, arXiv:2504.17087Leveraging llms as meta-judges: A multi-agent framework for evaluating llm judgments. 2025carXiv preprint</p>
<p>Leveraging large language models for nlg evaluation: Advances and challenges. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma, 2024o</p>
<p>Split and merge: Aligning position biases in llmbased evaluators. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024p</p>
<p>Debatrix: Multi-dimensinal debate judge with iterative chronological analysis based on llm. Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhongyu Wei, arXiv:2403.080102024aarXiv preprint</p>
<p>Abseval: An agent-based framework for script evaluation. Sirui Liang, Baoli Zhang, Jun Zhao, Kang Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024b</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, 2023. 19118ArXiv preprint, abs/2305</p>
<p>Lei Ma, and 1 others. 2024c. I-sheep: Self-alignment of llm from scratch through an iterative self-enhancement paradigm. Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, abs/2408.08072ArXiv preprint. </p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJanarXiv preprint</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan</p>
<p>The unlocking spell on base llms: Rethinking alignment via in-context learning. Abhilasha Bill Yuchen Lin, Ximing Ravichander, Nouha Lu, Melanie Dziri, Khyathi Sclar, Chandra Chandu, Yejin Bhagavatula, Choi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, 10.18653/v1/2023.nlp4convai-1.5Proceedings of the 5th Workshop on NLP for Conversational AI. the 5th Workshop on NLP for Conversational AIToronto, CanadaAssociation for Computational Linguistics2023a. NLP4ConvAI 2023</p>
<p>LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, 10.18653/v1/2023.nlp4convai-1.5Proceedings of the 5th Workshop on NLP for Conversational AI. the 5th Workshop on NLP for Conversational AIToronto, CanadaAssociation for Computational Linguistics2023b. NLP4ConvAI 2023</p>
<p>Evaluating text-to-visual generation with image-to-text generation. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, Deva Ramanan, European Conference on Computer Vision. Springer2025</p>
<p>How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. Beiming Liu, Zhizhuo Cui, Siteng Hu, Xiaohua Li, Haifeng Lin, Zhengxin Zhang, 10.18653/v1/D16-1230arXiv:2501.17183Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, Joelle Pineau, the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2025a. 2016arXiv preprintLlm evaluation based on aerospace manufacturing expertise: Automated generation and multi-model question answering</p>
<p>X-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects. Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, Lifu Huang, Proceedings of the 2024 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American ChapterPapers; Mexico City, MexicoAssociation for Computational Linguistics2024a1</p>
<p>On iterative evaluation and enhancement of code quality using gpt-4o. Rundong Liu, Andre Frade, Amal Vaidya, Maxime Labonne, Marcus Kaiser, Bismayan Chakrabarti, Jonathan Budd, Sean Moran, arXiv:2502.073992025barXiv preprint</p>
<p>Judge as a judge: Improving the evaluation of retrieval-augmented generation through the judge-consistency of large language models. Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu, arXiv:2502.188172025carXiv preprint</p>
<p>Statistical rejection sampling improves preference optimization. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, Jialu Liu, abs/2309.06657ArXiv preprint. 2023a</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023b</p>
<p>Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li, arXiv:2501.13007Pairwise rm: Perform best-of-n sampling with knockout tournament. 2025darXiv preprint</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier, arXiv:2403.169502024barXiv preprint</p>
<p>Llms as narcissistic evaluators: When ego inflates evaluation scores. Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin, abs/2311.097662023cArXiv preprint</p>
<p>Calibrating LLMbased evaluator. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024c</p>
<p>Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, arXiv:2402.15754Hd-eval: Aligning large language model evaluators through hierarchical criteria decomposition. 2024darXiv preprint</p>
<p>Meta ranking: Less capable language models are capable for single response judgement. Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu, abs/2402.12146ArXiv preprint. 2024e</p>
<p>Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu, arXiv:2504.02495Inference-time scaling for generalist reward modeling. 2025earXiv preprint</p>
<p>LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. Adian Liusie, Potsawee Manakul, Mark Gales, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers)</p>
<p>Zero-shot nlg evaluation through pairware comparisons with llms. Adian Liusie, Potsawee Manakul, Mark, Gales, abs/2307.078892023ArXiv preprint</p>
<p>Edoardo Loru, Jacopo Nudo, Di Niccolò, Matteo Marco, Walter Cinelli, Quattrociocchi, arXiv:2502.04426Decoding ai judgment: How llms assess news credibility and bias. 2025arXiv preprint</p>
<p>Xing Han Lù, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stańczak, Peter Shaw, Christopher J Pal, Siva Reddy, arXiv:2504.08942Agentrewardbench: Evaluating automatic evaluations of web agent trajectories. 2025arXiv preprint</p>
<p>Beyond exact match: Semantically reassessing event extraction by large language models. Yi-Fan Lu, Xian-Ling Mao, Tian Lan, Heyan Huang, Chen Xu, Xiaoyan Gao, arXiv:2410.094182024aarXiv preprint</p>
<p>Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation. Yujie Lu, Xianjun Yang, Xiujun Li, Xin , Eric Wang, William Yang, Wang , Advances in Neural Information Processing Systems. 2024b36</p>
<p>Halludial: A large-scale benchmark for automatic dialogue-level hallucination evaluation. Wen Luo, Tianshu Shen, Wei Li, Guangyue Peng, Richeng Xuan, Houfeng Wang, Xi Yang, ; Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, Junnan Li, arXiv:2411.13281Videoautoarena: An automated arena for evaluating large multimodal models in video analysis through user simulation. 2024a. 2024barXiv preprint</p>
<p>Leveraging large language models for relevance judgments in legal case retrieval. Shengjie Ma, Chong Chen, Qi Chu, Jiaxin Mao, abs/2403.184052024ArXiv preprint</p>
<p>Zero-shot listwise document reranking with a large language model. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, Jimmy Lin, abs/2305.021562023ArXiv preprint</p>
<p>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/2023.emnlp-main.741Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Beyond accuracy: Evaluating the reasoning behavior of large language models-a survey. Philipp Mondorf, Barbara Plank, abs/2404.018692024ArXiv preprint</p>
<p>Evaluating the performance of large language models via debates. Behrad Moniri, Hamed Hassani, Edgar Dobriban, abs/2406.110442024ArXiv preprint</p>
<p>Evaluate-and-purify: Fortifying code language models against adversarial attacks using llm-as-a-judge. Wenhan Mu, Ling Xu, Shuren Pei, Le Mi, Huichi Zhou, arXiv:2504.197302025arXiv preprint</p>
<p>Evaluating the evaluator: Measuring llms' adherence to task evaluation instructions. Bhuvanashree Murugadoss, Christian Poelitz, Ian Drosos, Nick Vu Le, Carina Suzana Mckenna, Chris Negreanu, Advait Parnin, Sarkar, ArXiv preprint, abs/2408.087812024</p>
<p>Creative beam search: Llm-asa-judge for improving response generation. Mirco Musolesi, 2024ICCC</p>
<p>Zero-shot benchmarking: A framework for flexible and scalable automatic evaluation of language models. José Pombal, M Nuno, Ricardo Guerreiro, André Ft Rei, Martins, arXiv:2504.010012025aarXiv preprint</p>
<p>José Pombal, Dongkeun Yoon, Patrick Fernandes, Ian Wu, Seungone Kim, Ricardo Rei, arXiv:2504.04953Graham Neubig, and André FT Martins. 2025b. M-prometheus: A suite of open multilingual llm judges. arXiv preprint</p>
<p>A call for clarity in reporting BLEU scores. Matt Post, 10.18653/v1/W18-6319Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>The great nugget recall: Automating fact extraction and rag evaluation with large language models. Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick Craswell, Jimmy Lin, arXiv:2504.150682025arXiv preprint</p>
<p>Learning to generate unit tests for automated debugging. Archiki Prasad, Elias Stengel-Eskin, Justin Chih-Yao Chen, Zaid Khan, Mohit Bansal, arXiv:2502.016192025arXiv preprint</p>
<p>Shu Pu, Yaochen Wang, Dongping Chen, Yuhang Chen, Guohao Wang, Qi Qin, Zhongyi Zhang, Zhiyuan Zhang, Zetong Zhou, arXiv:2503.17489Shuang Gong, and 1 others. 2025. Judge anything: Mllm as a judge across any modality. arXiv preprint</p>
<p>Evaluating llms' assessment of mixed-context hallucination through the lens of summarization. Siya Qi, Rui Cao, Yulan He, Zheng Yuan, arXiv:2503.016702025arXiv preprint</p>
<p>What do large language models need for machine translation evaluation?. Shenbin Qian, Archchana Sindhujan, Minnie Kabra, Diptesh Kanojia, Constantin Orašan, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024Tharindu Ranasinghe, and Fred Blain</p>
<p>Large language models are effective text rankers with pairwise ranking prompting. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, Michael Bendersky, Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, Mexico2024Association for Computational Linguistics</p>
<p>Efficient map estimation of llm judgment performance with prior transfer. Huaizhi Qu, Inyoung Choi, Zhen Tan, Song Wang, Sukwon Yun, Qi Long, Faizan Siddiqui, Kwonjoon Lee, Tianlong Chen, arXiv:2504.125892025arXiv preprint</p>
<p>Evaluating rag-fusion with ragelo: an automated elo-based framework. Zackary Rackauckas, Arthur Câmara, Jakub Zavrel, abs/2406.147832024ArXiv preprint</p>
<p>Refining input guardrails: Enhancing llm-as-ajudge efficiency through chain-of-thought fine-tuning and alignment. Melissa Kazemi, Rad , Huy Nghiem, Andy Luo, Sahil Wadhwa, Mohammad Sorower, Stephen Rawls, arXiv:2501.130802025arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>Emine Hossein A Rahmani, Nick Yilmaz, Bhaskar Craswell, Mitra, arXiv:2412.13268Judgeblender: Ensembling judgments for automatic relevance assessment. 2024arXiv preprint</p>
<p>Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. Adian Vyas Raina, Mark Liusie, Gales, abs/2402.140162024ArXiv preprint</p>
<p>Constructing domainspecific evaluation sets for llm-as-a-judge. Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, Urmish Thakkar, abs/2408.08808ArXiv preprint. 2024</p>
<p>A structured review of the validity of BLEU. Ehud Reiter, 10.1162/coli_a_00322Computational Linguistics. 4432018</p>
<p>David Rodriguez, William Seymour, Jose M Del Alamo, Jose Such, arXiv:2502.01436Towards safer chatbots: A framework for policy compliance evaluation of custom gpts. 2025arXiv preprint</p>
<p>Ares: An automated evaluation framework for retrieval-augmented generation systems. Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia, Proceedings of the 2024 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American Chapter2024a1Long Papers</p>
<p>Jon Saad-Falcon, Rajan Vivek, William Berrios, Nandita Shankar Naik, Matija Franklin, Bertie Vidgen, Amanpreet Singh, Douwe Kiela, Shikib Mehri, arXiv:2412.13091Lmunit: Fine-grained evaluation with natural language unit tests. 2024barXiv preprint</p>
<p>Branchsolve-merge improves large language model evaluation and generation. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>Learning to plan &amp; reason for evaluation with thinking-llm-as-ajudge. Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang, arXiv:2501.180992025arXiv preprint</p>
<p>A survey of evaluation metrics used for nlg systems. Akash Ananya B Sai, Mitesh M Kumar Mohankumar, Khapra, ACM Computing Surveys (CSUR). 5522022</p>
<p>Tuning llm judge design decisions for 1/1000 of the cost. David Salinas, Omar Swelam, Frank Hutter, 20252501arXiv e-prints</p>
<p>Can large language models outperform non-experts in poetry evaluation? a comparative study using the consensual assessment technique. Piotr Sawicki, Marek Grześ, Dan Brown, Fabrício Góes, arXiv:2502.190642025arXiv preprint</p>
<p>Evaluating the moral beliefs encoded in llms. Nino Scherrer, Claudia Shi, Amir Feder, David M Blei, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>Can you trust llm judgments? reliability of llm-as-a-judge. Kayla Schroeder, Zach Wood-Doughty, arXiv:2412.125092024arXiv preprint</p>
<p>Validating llm-generated relevance labels for educational resource search. J Ratan, Anett Sebastian, Hoppe, arXiv:2504.127322025arXiv preprint</p>
<p>Mag-v: A multi-agent framework for synthetic data generation and verification. Saptarshi Sengupta, Kristal Curtis, Akshay Mallipeddi, Abhinav Mathur, Joseph Ross, Liang Gou, 2024</p>
<p>Mt-raig: Novel benchmark and evaluation framework for retrieval-augmented insight generation over multiple tables. Kwangwook Seo, Donguk Kwon, Dongha Lee, arXiv:2502.117352025arXiv preprint</p>
<p>Rewarding progress: Scaling automated process verifiers for llm reasoning. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar, arXiv:2410.081462024arXiv preprint</p>
<p>Languagempc: Large language models as decision makers for autonomous driving. Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Eben Shengbo, Masayoshi Li, Wei Tomizuka, Mingyu Zhan, Ding, abs/2310.03026ArXiv preprint. 2023</p>
<p>Optimization-based prompt injection attack to llmas-a-judge. Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, Neil Zhenqiang, Gong , abs/2403.17710ArXiv preprint. 2024</p>
<p>Wenlei Shi, Xing Jin, arXiv:2504.10337Heimdall: test-time scaling on the generative verification. 2025arXiv preprint</p>
<p>Lces: Zero-shot automated essay scoring via pairwise comparisons using large language models. Takumi Shibata, Yuichi Miyamura, arXiv:2505.084982025arXiv preprint</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Ku, arXiv:2408.03314mar. 2024arXiv preprint</p>
<p>Llm-as-a-judge &amp; reward model: What they can and cannot do. Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong, abs/2409.11239ArXiv preprint. 2024a</p>
<p>Mm-eval: A multilingual meta-evaluation benchmark for llm-as-a-judge and reward models. abs/2410.175782024bArXiv preprintGuijin Son and 1 others</p>
<p>Finesure: Fine-grained summarization evaluation using llms. Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, Saab Mansour, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024a1</p>
<p>Can many-shot in-context learning help long-context llm judges? see more. Mingyang Song, Mao Zheng, Xuan Luo, abs/2406.116292024bjudge better! ArXiv preprint</p>
<p>Grp: Goal-reversed prompting for zero-shot evaluation with llms. Mingyang Song, Mao Zheng, Xuan Luo, arXiv:2503.061392025arXiv preprint</p>
<p>From calculation to adjudication: Examining llm judges on mathematical reasoning tasks. Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, Benjamin Roth, abs/2409.041682024ArXiv preprint</p>
<p>Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, arXiv:2503.16419Stop overthinking: A survey on efficient reasoning for large language models. 2025arXiv preprintHanjie Chen, and 1 others</p>
<p>Bertscore is unfair: On social bias in language model-based metrics for text generation. Tianxiang Sun, Junliang He, Xipeng Qiu, Xuan-Jing Huang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Is ChatGPT good at search? investigating large language models as re-ranking agents. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, 10.18653/v1/2023.emnlp-main.923Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Salmon: Selfalignment with instructable reward models. Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, Chuang Gan, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, arXiv:2401.15042Qun Liu, and 1 others. 2024a. Proxyqa: An alternative framework for evaluating long-form text generation with large language models. arXiv preprint</p>
<p>Judgebench: A benchmark for evaluating llm-based judges. abs/2410.127842024bArXiv preprintSijun Tan and 1 others</p>
<p>Supervised graph contrastive learning for fewshot node classification. Zhen Tan, Kaize Ding, Ruocheng Guo, Huan Liu, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer2022</p>
<p>Qiaoyu Tang, Jiawei Chen, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, ArXiv preprint, abs/2403.00801Xianpei Han, and 1 others. 2024a. Selfretrieval: Building an information retrieval system with one large language model. </p>
<p>Found in the middle: Permutation self-consistency improves listwise ranking in large language models. Raphael Tang, Crystina Zhang, Xueguang Ma, Jimmy Lin, Ferhan Ture, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024b1</p>
<p>Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes, 2024ArXiv preprint, abs/2406.12624</p>
<p>Large language models can accurately predict searcher preferences. Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra, abs/2309.106212023. 2023ArXiv preprint</p>
<p>Large language models can accurately predict searcher preferences. Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Badjudge: Backdoor vulnerabilities of llm-asa-judge. Terry Tong, Fei Wang, Zhe Zhao, Muhao Chen, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Codejudge: Evaluating code generation with large language models. Weixi Tong, Tianyi Zhang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Optimizing language model's reasoning abilities with weak supervision. Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang, ArXiv preprint, abs/2405.040862024</p>
<p>Pairwise or pointwise? evaluating feedback protocols for bias in llm-based evaluation. Tuhina Tripathi, Manya Wadhwa, Greg Durrett, Scott Niekum, arXiv:2504.147162025arXiv preprint</p>
<p>Codev: An automated grading framework leveraging large language models for consistent and constructive feedback. En-Qi Tseng, Pei-Cing Huang, Chan Hsu, Peng-Yi Wu, Chan-Tung Ku, Yihuang Kang, 2024 IEEE International Conference on Big Data (BigData). IEEE2024</p>
<p>Aligning black-box language models with human judgments. Gerrit Jj Van Den Burg, Gen Suzuki, Wei Liu, Murat Sensoy, arXiv:2502.049972025arXiv preprint</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>Foundational autoraters: Taming large language models for better automatic evaluation. Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung, abs/2407.10817ArXiv preprint. 2024</p>
<p>Halu-j: Critique-based hallucination judge. Binjie Wang, Steffi Chern, Ethan Chern, Pengfei Liu, abs/2407.129432024aArXiv preprint</p>
<p>Can ChatGPT defend its belief in truth? evaluating LLM reasoning via debate. Boshi Wang, Xiang Yue, Huan Sun, 10.18653/v1/2023.findings-emnlp.795Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023a</p>
<p>Biorag: A rag-llm framework for biological question reasoning. Chengrui Wang, Qingqing Long, Xiao Meng, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, Yuanchun Zhou, abs/2408.01107ArXiv preprint. 2024b</p>
<p>Automated genre-aware article scoring and feedback using large language models. Chihang Wang, Yuxin Dong, Zhenhong Zhang, Ruotong Wang, Shuo Wang, Jiajing Chen, arXiv:2410.141652024carXiv preprint</p>
<p>Learning personalized story evaluation. Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, Yuandong Tian, abs/2310.033042023bArXiv preprint</p>
<p>Is chatgpt a good nlg evaluator? a preliminary study. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, Proceedings of the 4th New Frontiers in Summarization Workshop. the 4th New Frontiers in Summarization Workshop2023c</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, and 1 others. 2024d. A survey on large language model based autonomous agents. 18186345</p>
<p>Direct judgement preference optimization. Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, Shafiq Joty, abs/2409.146642024eArXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, 2023dArXiv preprint, abs/2305.17926</p>
<p>Qian Wang, Zhanzhi Lou, Zhenheng Tang, Nuo Chen, Xuandong Zhao, Wenxuan Zhang, arXiv:2504.09946Dawn Song, and Bingsheng He. 2025a. Assessing judging bias in large reasoning models: An empirical study. arXiv preprint</p>
<p>Can llms replace human evaluators? an empirical study of llmas-a-judge in software engineering. Ruiqi Wang, Jiyu Guo, Cuiyun Gao, Guodong Fan, Chun Yong Chong, Xin Xia, arXiv:2502.061932025barXiv preprint</p>
<p>Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, arXiv:2411.10914Xin Zhang, and Tianlong Chen. 2024f. Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment. arXiv preprint</p>
<p>Ceb: Compositional evaluation benchmark for fairness in large language models. Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, Jundong Li, abs/2407.02408ArXiv preprint. 2024g</p>
<p>Self-taught evaluators. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li, abs/2408.02666ArXiv preprint. 2024h</p>
<p>Improving llm-as-a-judge inference with the judgment distribution. Victor Wang, Michael Jq Zhang, Eunsol Choi, arXiv:2503.030642025carXiv preprint</p>
<p>Revisiting benchmark and assessment: An agent-based exploratory dynamic evaluation framework for llms. Wanying Wang, Zeyu Ma, Pengfei Liu, Mingang Chen, arXiv:2410.115072024iarXiv preprint</p>
<p>Contrastscore: Towards higher quality, less biased, more efficient evaluation metrics with contrastive evaluation. Xiao Wang, Daniil Larionov, Siwei Wu, Yiqi Liu, Steffen Eger, Nafise Sadat Moosavi, Chenghua Lin, arXiv:2504.021062025darXiv preprint</p>
<p>Codevisionary: An agent-based framework for evaluating large language models in code generation. Xinchen Wang, Pengfei Gao, Chao Peng, Ruida Hu, Cuiyun Gao, arXiv:2504.134722025earXiv preprint</p>
<p>Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu, Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, Xia Hu, abs/2408.13704Dhp benchmark: Are llms good nlg evaluators? ArXiv preprint. 2024j</p>
<p>Pan-daLM: An automatic evaluation benchmark for LLM instruction tuning optimization. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, The Twelfth International Conference on Learning Representations. 2024k</p>
<p>Yutong Wang, Pengliang Ji, Chaoqun Yang, Kaixin Li, Ming Hu, Jiaoyang Li, Guillaume Sartoretti, arXiv:2502.12468Mcts-judge: Test-time scaling in llm-as-ajudge for code correctness evaluation. 2025farXiv preprint</p>
<p>Do-not-answer: Evaluating safeguards in LLMs. Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, Timothy Baldwin, Findings of the Association for Computational Linguistics: EACL 2024. St. Julian's, MaltaAssociation for Computational Linguistics2024l</p>
<p>Healthq: Unveiling questioning capabilities of llm chains in healthcare conversations. Ziyu Wang, Hao Li, Di Huang, Amir, Rahmani, abs/2409.19487ArXiv preprint. 2024m</p>
<p>Self-preference bias in llm-as-a-judge. Koki Wataoka, Tsubasa Takahashi, Ryokan Ri, abs/2410.21819ArXiv preprint. 2024</p>
<p>Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates. Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, Mei Han, 2024aArXiv preprint, abs/2408.13006</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022a. April 25-29, 2022OpenReview.net</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022b. 2022. 2022. November 28 -December 9, 2022</p>
<p>Cosmo Du, and 1 others. 2024b. Longform factuality in large language models. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, ArXiv preprint, abs/2403.18802</p>
<p>Rocketeval: Efficient automated llm evaluation via grading checklist. Tianjun Wei, Wei Wen, Ruizhi Qiao, Xing Sun, Jianghong Ma, arXiv:2503.051422025arXiv preprint</p>
<p>Bosi Wen, Pei Ke, Yufei Sun, Cunxiang Wang, Xiaotao Gu, Jinfeng Zhou, Jie Tang, Hongning Wang, Minlie Huang, arXiv:2502.13031Hpss: Heuristic prompting strategy search for llm evaluators. 2025arXiv preprint</p>
<p>On-policy fine-grained knowledge feedback for hallucination mitigation. Xueru Wen, Xinyu Lu, Xinyan Guan, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, arXiv:2406.122212024arXiv preprint</p>
<p>Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences. Martin Weyssow, Aton Kamanda, Houari Sahraoui, arXiv:2403.090322024arXiv preprint</p>
<p>J1: Incentivizing thinking in llmas-a-judge via reinforcement learning. Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha, arXiv:2505.103202025Preprint</p>
<p>Large language models are diverse role-players for summarization evaluation. Ning Wu, Ming Gong, Linjun Shou, Shining Liang, Daxin Jiang, CCF International Conference on Natural Language Processing and Chinese Computing. Springer2023</p>
<p>Longeval: A comprehensive analysis of long-text generation through a plan-based paradigm. Siwei Wu, Yizhi Li, Xingwei Qu, Rishi Ravikumar, Yucheng Li, Tyler Loakman, Shanghaoran Quan, Xiaoyong Wei, Riza Batista-Navarro, Chenghua Lin, arXiv:2502.191032025arXiv preprint</p>
<p>Meta-rewarding language models: Self-improving alignment with llm-as-ameta-judge. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar, 2024aArXiv preprint, abs/2407.19594</p>
<p>Can large language models serve as evaluators for code summarization?. Yang Wu, Yao Wan, Zhaoyang Chu, Wenting Zhao, Ye Liu, Hongyu Zhang, Xuanhua Shi, Philip S Yu, 2024b</p>
<p>Can large language models serve as evaluators for code summarization?. Yang Wu, Yao Wan, Zhaoyang Chu, Wenting Zhao, Ye Liu, Hongyu Zhang, Xuanhua Shi, Philip S Yu, arXiv:2412.013332024carXiv preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, arXiv:2309.07864Senjie Jin, Enyu Zhou, and 1 others. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint</p>
<p>Evaluating mathematical reasoning beyond accuracy. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu, abs/2404.056922024ArXiv preprint</p>
<p>Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang, arXiv:2502.10709An empirical analysis of uncertainty in large language model evaluations. 2025aarXiv preprint</p>
<p>Data selection for language models via importance resampling. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy Liang, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, and 1 others. 2024a. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. ArXiv preprint, abs/2406.14598</p>
<p>Wenwen Xie, Gray Gwizdz, Dongji Feng, arXiv:2502.13396Prompting a weighting mechanism into llm-as-ajudge in two-step: A case study. 2025barXiv preprint</p>
<p>Yiqing Xie, Wenxuan Zhou, Pradyot Prakash, Di Jin, Yuning Mao, Quintin Fettes, Arya Talebzadeh, Sinong Wang, Han Fang, arXiv:2410.18359Carolyn Rose, and 1 others. 2024b. Improving model factuality with finegrained critique-based evaluator. arXiv preprint</p>
<p>Self-evaluation guided beam search for reasoning. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, Michael Xie, Advances in Neural Information Processing Systems. 2024c36</p>
<p>Llava-critic: Learning to evaluate multimodal models. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li, abs/2410.02712ArXiv preprint. 2024</p>
<p>Does context matter? contextualjudgebench for evaluating llm-based judges in contextual settings. Austin Xu, Srijan Bansal, Yifei Ming, Semih Yavuz, Shafiq Joty, arXiv:2503.156202025aarXiv preprint</p>
<p>Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li, arXiv:2502.18874Learning to align multi-faceted evaluation: A unified and robust framework. 2025barXiv preprint</p>
<p>Academically intelligent llms are not necessarily socially intelligent. Ruoxi Xu, Hongyu Lin, Xianpei Han, Le Sun, Yingfei Sun, abs/2403.065912024aArXiv preprint</p>
<p>Benchmarking llms' judgments with no gold standard. Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong, 2024b</p>
<p>Large language models are active critics in nlg evaluation. Shuying Xu, Junjie Hu, Ming Jiang, arXiv:2410.107242024carXiv preprint</p>
<p>Sayself: Teaching llms to express confidence with self-reflective rationales. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao, 2024dArXiv preprint, abs/2405.20974</p>
<p>INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, Lei Li, 10.18653/v1/2023.emnlp-main.365Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Zhenran Xu, Senbao Shi, Baotian Hu, Jindi Yu, Dongfang Li, Min Zhang, Yuxiang Wu, arXiv:2311.08152Towards reasoning in large language models via multiagent peer review collaboration. 2023barXiv preprint</p>
<p>Autogpt for online decision making: Benchmarks and additional opinions. Hui Yang, Sifu Yue, Yunzhong He, abs/2306.022242023ArXiv preprint</p>
<p>Toward automatic relevance judgment using vision-language models for image-text retrieval evaluation. Jheng-Hong Yang, Jimmy Lin, abs/2408.013632024ArXiv preprint</p>
<p>. Jian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, and Junyang Lin. 2024. Evaluating and aligning codellms on human preference</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023a. 2023. 2023. December 10 -16, 2023</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023b. May 1-5, 2023OpenReview.net</p>
<p>Michihiro Yasunaga, Leonid Shamis, Chunting Zhou, Andrew Cohen, Jason Weston, Luke Zettlemoyer, Marjan Ghazvininejad, arXiv:2412.04305Alma: Alignment with minimal annotation. 2024arXiv preprint</p>
<p>Pin-Yu Chen, and 1 others. 2024a. Justice or prejudice? quantifying biases in llm-as-ajudge. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, abs/2410.02736ArXiv preprint. </p>
<p>FLASK: Fine-grained language model evaluation based on alignment skill sets. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024b</p>
<p>Zihuiwen Ye, Luckeciano Carvalho Melo, Younesse Kaddar, Phil Blunsom, Sam Staton, Yarin Gal, arXiv:2502.11250Uncertainty-aware step-wise verification with generative reward models. 2025arXiv preprint</p>
<p>Protocollm: Automatic evaluation framework of llms on domain-specific scientific protocol formulation tasks. Seungjun Yi, Jaeyoung Lim, Juyong Yoon, arXiv:2410.046012024arXiv preprint</p>
<p>Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, Xuelong Li, arXiv:2502.11689Improve llm-asa-judge ability as a general ability. 2025arXiv preprint</p>
<p>Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, Ding Chen, arXiv:2405.11874xfinder: Robust and pinpoint answer extraction for large language models. 2024aarXiv preprint</p>
<p>Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, arXiv:2411.16646Chao Zhang, and 1 others. 2024b. Self-generated critiques boost reward modeling for language models. arXiv preprint</p>
<p>Kieval: A knowledgegrounded interactive evaluation framework for large language models. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang, ArXiv preprint, abs/2402.150432024c</p>
<p>Reasoning through execution: Unifying process and outcome rewards for code generation. Zhuohao Yu, Weizheng Gu, Yidong Wang, Xingru Jiang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang, </p>
<p>Advancing llm reasoning generalists with preference trees. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, AI for Math Workshop@ ICML 2024. Yankai Lin, and 1 others</p>
<p>Batcheval: Towards human-like text evaluation. Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, Kan Li, abs/2401.00437ArXiv preprint. 2024a</p>
<p>Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, abs/2401.10019Zhuosheng Zhang, and 1 others. 2024b. R-judge: Benchmarking safety risk awareness for llm agents. </p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. 2021. December 6-14, 20212021</p>
<p>Self-rewarding language models. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, abs/2401.100202024cArXiv preprint</p>
<p>Automatic evaluation of attribution by large language models. Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, Huan Sun, 10.18653/v1/2023.findings-emnlp.307Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Learning reward for robot skills using large language models via self-alignment. Yuwei Zeng, Yao Mu, Lin Shao, abs/2405.071622024ArXiv preprint</p>
<p>Evaluating large language models at evaluating instruction following. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, The Twelfth International Conference on Learning Representations. </p>
<p>Online self-preferring language models. Yuanzhao Zhai, Zhuo Zhang, Kele Xu, Hanyang Peng, Yue Yu, Dawei Feng, Cheng Yang, Bo Ding, Huaimin Wang, abs/2405.14103ArXiv preprint. 2024</p>
<p>Bang Zhang, Ruotian Ma, Qingxuan Jiang, Peisong Wang, Jiaqi Chen, Zheng Xie, Xingyu Chen, Yue Wang, Fanghua Ye, arXiv:2505.02847Jian Li, and 1 others. 2025a. Sentient agent as a judge: Evaluating higher-order social cognition in large language models. arXiv preprint</p>
<p>A comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators. Chen Zhang, Luis Fernando, D' Haro, Yiming Chen, Malu Zhang, Haizhou Li, 10.1609/AAAI.V38I17.29923Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. Vancouver, CanadaAAAI Press2024a. February 20-27, 20242014</p>
<p>Evaluation agent: Efficient and promptable evaluation framework for visual generative models. Fan Zhang, Shulin Tian, Ziqi Huang, Yu Qiao, Ziwei Liu, arXiv:2412.096452024barXiv preprint</p>
<p>Are large language models good at utility judgments?. Hengran Zhang, Ruqing Zhang, Jiafeng Maarten De Rijke, Yixing Fan, Xueqi Cheng, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024c</p>
<p>Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal, arXiv:2408.15240Generative verifiers: Reward modeling as next-token prediction. 2024darXiv preprint</p>
<p>Breaking event rumor detection via stance-separated multi-agent debate. Mingqing Zhang, Haisong Gong, Qiang Liu, Shu Wu, Liang Wang, 2024e</p>
<p>Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, Chen Ma, arXiv:2503.24235What, how, where, and how well? a survey on test-time scaling in large language models. 2025barXiv preprint</p>
<p>Ruiming Tang, and 1 others. 2024f. Reviseval: Improving llm-as-a-judge via responseadapted references. Qiyuan Zhang, Yufei Wang, Tiezheng Yu, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, abs/2410.05193ArXiv preprint. </p>
<p>Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, Yeyun Gong, arXiv:2503.03746Process-based self-rewarding language models. ess-based self-rewarding language models2025carXiv preprint</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:2504.12663OpenReview.net. Xiaotian Zhang, Ruizhe Chen, Yang Feng, and Zuozhu Liu. 2025d. Persona-judge: Personalized alignment of large language models via token-level selfjudgment. Addis Ababa, Ethiopia2020. April 26-30, 20202020arXiv preprint8th International Conference on Learning Representations</p>
<p>Self-alignment for factuality: Mitigating hallucinations in llms via self-evaluation. Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, Helen Meng, ArXiv preprint, abs/2402.092672024g</p>
<p>Large language models as evaluators for recommendation explanations. Xiaoyu Zhang, Yishan Li, Jiayin Wang, Bowen Sun, Weizhi Ma, Peijie Sun, Min Zhang, Proceedings of the 18th ACM Conference on Recommender Systems. the 18th ACM Conference on Recommender Systems2024h</p>
<p>Ace-m3: Automatic capability evaluator for multimodal medical models. Xiechi Zhang, Shunfan Zheng, Linlin Wang, Gerard De Melo, Zhu Cao, Xiaoling Wang, Liang He, arXiv:2412.114532024iarXiv preprint</p>
<p>Wider and deeper llm networks are fairer llm evaluators. Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li, ArXiv preprint, abs/2308.018622023</p>
<p>Yueheng Zhang, Xiaoyuan Liu, Yiyou Sun, Atheer Alharbi, Hend Alzahrani, Basel Alomair, Dawn Song, arXiv:2501.03491Can llms design good questions based on context? arXiv preprint. 2025e</p>
<p>Is chain-of-thought reasoning of llms a mirage? a data distribution lens. Chengshuai Zhao, Zhen Tan, Pingchuan Ma, Dawei Li, Bohan Jiang, Yancheng Wang, Yingzhen Yang, Huan Liu, arXiv:2508.011912025aarXiv preprint</p>
<p>Llmsql-solver: Can llms determine sql equivalence?. Fuheng Zhao, Lawrence Lim, Ishtiyaque Ahmad, Divyakant Agrawal, Amr El Abbadi, arXiv:2312.103212023aarXiv preprint</p>
<p>Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, arXiv:2504.00891Xiu Li, and 1 others. 2025b. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint</p>
<p>Codejudge-eval: A benchmark for evaluating code generation. John Zhao, abs/2401.100191 others. 2024aArXiv preprint</p>
<p>Diffagent: Fast and accurate text-to-image api selection with large language model. Lirui Zhao, Yue Yang, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, Rongrong Ji, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024b</p>
<p>Auto arena of llms: Automating llm evaluations with agent peerbattles and committee discussions. Ruochen Zhao, Wenxuan Zhang, Ken Yew, Deli Chia, Lidong Zhao, Bing, ArXiv preprint. 2024c. 2405.20267</p>
<p>Slic-hf: Sequence likelihood calibration with human feedback. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, Peter J Liu, abs/2305.10425ArXiv preprint. 2023b</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023</p>
<p>Hierarchical divide-and-conquer for fine-grained alignment in llm-based medical evaluation. Shunfan Zheng, Xiechi Zhang, Gerard De Melo, Xiaoling Wang, Linlin Wang, arXiv:2501.067412025arXiv preprint</p>
<p>Han Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vulić, Anna Korhonen, arXiv:2406.11370Fairer preferences elicit improved human-aligned large language model judgments. 2024aarXiv preprint</p>
<p>Hongli Zhou, Hui Huang, Yunfei Long, Bing Xu, Conghui Zhu, Hailong Cao, Muyun Yang, Tiejun Zhao, ArXiv preprint, abs/2409.16788Mitigating the bias of large language model evaluation. 2024b</p>
<p>An llm feature-based framework for dialogue constructiveness assessment. Lexin Zhou, Youmna Farag, Andreas Vlachos, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024c</p>
<p>Selfdiscover: Large language models self-compose reasoning structures. Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven, Zheng , abs/2402.03620ArXiv preprint. 2024d</p>
<p>Is llm a reliable reviewer? a comprehensive evaluation of llm on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024e</p>
<p>Graham Neubig, and 1 others. 2023. Sotopia: Interactive evaluation for social intelligence in language agents. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, abs/2310.11667ArXiv preprint. </p>
<p>Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, Shafiq Joty, arXiv:2504.15253Evaluating judges as evaluators: The jetts benchmark of llm-as-judges as test-time scaling evaluators. 2025arXiv preprint</p>
<p>Starling-7b: Improving helpfulness and harmlessness with rlaif. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, Jiantao Jiao, First Conference on Language Modeling. 2024a</p>
<p>Hanwei Zhu, Haoning Wu, Yixuan Li, Zicheng Zhang, Baoliang Chen, Lingyu Zhu, Yuming Fang, Guangtao Zhai, Weisi Lin, Shiqi Wang, arXiv:2405.19298Adaptive image quality assessment via teaching large multimodal model to compare. 2024barXiv preprint</p>
<p>Dynamic evaluation of large language models by meta probing agents. Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie, Fortyfirst International Conference on Machine Learning. 2024c</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, abs/2310.176312023ArXiv preprint</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>
<p>Beyond yes and no: Improving zero-shot LLM rankers via scoring fine-grained relevance labels. Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, Michael Bendersky, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024a2Short Papers)</p>
<p>A setwise approach for effective and highly efficient zero-shot ranking with large language models. Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024b</p>
<p>Raghuraman Krishnamoorthi, Yuandong Tian, and 1 others. 2024. Agent-as-ajudge: Evaluate agents with agents. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, ArXiv preprint, abs/2410.10934</p>
<p>Ice-score: Instructing large language models to evaluate code. Terry Yue, Zhuo , Findings of the Association for Computational Linguistics: EACL 2024. 2024</p>
<p>Question Answering 3.3K Human &amp; GPT-4o Accuracy, Correlation Non-English &amp; Challenging Murugadoss et al. (2024) Various Tasks -Human Correlation Evaluation Instruction Following Thakur et al. (2024) Question Answering 400 Human Scott's π, Percent Agreement Vulnerability Rewardbench (Lambert et al., 2024) Various Tasks 20K Human &amp; LLMs Accuracy General Performance Arena-Hard Auto. Judgebench (tan, Summarization, Alignment 1K Human Accuracy, Flipping Noise, Position Bias, Length Bias General Performance DHP. 2024b. 2024. 2024a. 2024j. 2024. 2024a. 2024a. 2024l. 2024bMulti-turn Interaction 569 Human F1, Recall, Spec, Effect Safety Shi et al. (2024) Alignment 100K Human Repetition Stability, Position Consistency, Preference Fairness Position Bias CALM (Ye et al., 2024a) Various Tasks 14K Human Robustness/Consistency Rate, 0riginal/ Hacked Accuracy Bias Quantification VL-RewardBench. 2024f) Various Tasks 1.2K Human &amp; LLMs Overall Accuracy, Macro Average Accuracy Multimodal Table 4: Overview of various benchmarks and datasets for LLM-as-a-judge</p>            </div>
        </div>

    </div>
</body>
</html>
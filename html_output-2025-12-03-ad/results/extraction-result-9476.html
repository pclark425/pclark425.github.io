<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9476 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9476</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9476</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-5dbd9f9fd231863dda17d9e3caff2edb99a113e2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5dbd9f9fd231863dda17d9e3caff2edb99a113e2" target="_blank">RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> RE-Bench is introduced, which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts, and finds that modern AI agents possess significant expertise in many ML topics and can generate and test solutions over ten times faster than humans, at much lower cost.</p>
                <p><strong>Paper Abstract:</strong> Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-k with varying time budgets and agent designs, and find that the best AI agents achieve a score 4x higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2x the score of the top AI agent when both are given 32 total hours (across different attempts). Qualitatively, we find that modern AI agents possess significant expertise in many ML topics -- e.g. an agent wrote a faster custom Triton kernel than any of our human experts' -- and can generate and test solutions over ten times faster than humans, at much lower cost. We open-source the evaluation environments, human expert data, analysis code and agent trajectories to facilitate future research.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9476",
    "paper_id": "paper-5dbd9f9fd231863dda17d9e3caff2edb99a113e2",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00444375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RE-Bench: Evaluating frontier AI R\&amp;D capabilities of language model agents against human experts</h1>
<p>Hjalmar Wijk Tao Lin Joel Becker* Sami Jawhar Neev Parikh<br>Thomas Broadley ${ }^{\dagger}$ Lawrence Chan ${ }^{\dagger}$ Michael Chen ${ }^{\dagger}$ Josh Clymer ${ }^{\dagger}$, $\ddagger$ Jai Dhyani ${ }^{\dagger, \S}$<br>Elena Ericheva ${ }^{\dagger}$<br>Katharyn Garcia ${ }^{\dagger}$ Brian Goodrich ${ }^{\dagger}$ Nikola Jurkovic ${ }^{\dagger, \S}$<br>Holden Karnofsky ${ }^{\dagger, \S}$ Megan Kinniment ${ }^{\dagger}$ Aron Lajko ${ }^{\dagger, \ddagger}$ Seraphina Nix ${ }^{\dagger}$ Lucas Sato ${ }^{\dagger}$<br>William Saunders ${ }^{\dagger}$ Maksym Taran ${ }^{\dagger}$ Ben West ${ }^{\dagger}$ Elizabeth Barnes<br>Model Evaluation and Threat Research (METR)</p>
<h4>Abstract</h4>
<p>Frontier AI safety policies highlight automation of AI research and development (R\&amp;D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R\&amp;D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, V1), which consists of 7 challenging, openended ML research engineering environments and data from 718 -hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with $82 \%$ of expert attempts achieving a non-zero score and $24 \%$ matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of- $k$ with varying time budgets and agent designs, and find that the best AI agents achieve a score $4 \times$ higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8 -hour budget, and achieving $2 \times$ the score of the top AI agent when both are given 32 total hours (across different attempts). Qualitatively, we find that modern AI agents possess significant expertise in many ML topics-e.g. an agent wrote a faster custom Triton kernel than any of our human experts'-and can generate and test solutions over ten times faster than humans, at much lower cost. We open-source the evaluation environments, human expert data, analysis code and agent trajectories to facilitate future research. ${ }^{\dagger}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: RE-Bench is a collection of seven hand-crafted environments intended to test the AI R\&amp;D capabilities of current AI systems. Humans and AI agents are tested under the same conditions. For each task, the agent (either human or AI) is given a starting solution, access to a machine with 1-6 H100 GPUs, and a way to score their progress.</p>
<h1>1 Introduction</h1>
<p>Large language models are increasingly capable of complex programming tasks and are already being used to accelerate AI research, from generating training data to serving as programming tools $[1,2,3,4,5,6]$. As these capabilities grow, there is increasing concern about the potential for AI systems to fully automate frontier AI research and development (henceforth AI R\&amp;D) with minimal human involvement $[7,8,9] .{ }^{2}$ Multiple AI developers and governments have identified the need for evaluations that can provide advance warning of these capabilities, to allow for the implementation of key security and deployment mitigations [10, 11, 12, 13, 14].
In this work, we present RE-Bench, which aims to evaluate whether AI agents can fully automate the work of expert AI R\&amp;D researchers, using direct performance comparisons between AI agents and human experts under equivalent conditions and resources. These provide a clear logical connection to automation risk: if AI agents perform significantly worse than human ML experts given equivalent resources and conditions, then the AI agents likely cannot automate these experts' research work.
To implement this approach, we present:</p>
<ul>
<li>Seven novel, hand-crafted evaluation environments covering realistic ML research tasks</li>
<li>Data from 71 attempts by 61 ML experts completing these tasks under equivalent conditions to those used for agents</li>
<li>score@ $k$ results (best score obtained across sample of $k$ runs, similar to pass@ $k$ [1]) from o1-preview and Claude 3.5 Sonnet in two different scaffolds across different time limits and number of samples.</li>
<li>Qualitative analysis of task characteristics and agent limitations.</li>
</ul>
<p>All of the AI agents we evaluated outperform humans with a 2-hour time budget (Figure 2). However, humans display better returns to increasing time budgets, exceeding the best agent scores when given 8 hours, and continuing to improve rapidly under longer total time budgets when evaluated through best-of- $k$ (i.e., using the result from the most successful of $k$ independent experts) with more samples.
Qualitatively, we find that most agent solutions score close to 0 (i.e. do not improve on the reference solution), but that agents submit new solutions over 10 times faster than human experts, and occasionally find very successful approaches. Notably, both o1-preview and Claude 3.5 Sonnet find distinct solutions to our kernel optimization problem that beat the efforts of all 9 human experts (see Figure 18).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Best score@ $k$ results observed by total time budget across different allocations between samples and time horizon. For different agents, the optimal number of independent runs is different. For instance, at 16 hours, we saw the highest score for Claude by taking the best of 32 half-hour runs (30min@32), whereas for ol-preview the best allocation was 2h@8, and for humans 8h@2. See Section 4 for details on which score@k results were evaluated. Shaded regions represent 95\% confidence intervals generated via percentile bootstrapping, and are skewed downwards due to a long right tail of agent performance and best-of-k sampling. We find that agents initially make faster progress than humans, but that human experts improve more rapidly with additional time. That being said, current AI agents are substantially cheaper than human experts per unit time, suggesting that there may be ways to greatly improve AI agent performance in a given time budget by using additional tokens (see Section 6 for more discussion, and Figure 11 for a version of this graph with labor cost on the $x$-axis).</p>
<p>We then discuss ways in which this evaluation may over- or under-estimate the capabilities of AI agents relative to humans. For example, our environments are smaller in scope and have clearer objectives and shorter feedback loops compared to real-world research engineering. On the other hand, the cost of generation tokens is very cheap relative to human labor, suggesting that optimization of scaffolding and compute usage could substantially improve AI agent performance from what we show here.</p>
<p>The remainder of this paper details our methodology, results and analysis. Section 2 provides background on AI R\&amp;D automation risk and current approaches to evaluation; Section 3 introduces our evaluation and the design principles behind it; Sections 4 and 5 presents our evaluation results with current frontier models; and Section 6 concludes the paper with a discussion of limitations and implications.</p>
<h1>2 Background</h1>
<h3>2.1 Risks from AI R\&amp;D Automation</h3>
<p>Recent developments in frontier AI capabilities have prompted leading AI developers and governments to establish frameworks for identifying and mitigating catastrophic risks from advanced AI systems [12, 11, 10]. An emerging consensus, reflected in commitments like the Bletchley Declaration [15] and the Seoul AI Safety Summit [16], is that systematic evaluation of dangerous capabilities should be central to effective AI governance. Progress has been made in developing safety policies and evaluation frameworks around specific capability risks, particularly those involving potential misuse such as bioweapons development [17] and cyber attacks [18]. However, there</p>
<p>exists a distinct challenge in assessing risks downstream of AI systems' capacity for autonomous operation-particularly in the domain of AI research and development itself.
The concern is that AI agents automating AI R\&amp;D could drastically increase the amount of skilled research labor available to AI developers, which would likely accelerate further progress, potentially leading to a runaway feedback loop of accelerating progress [19, 8, 9] The consequences of rapidly advancing the pace of scientific discoveries could be dramatic and enable many positive outcomes, but some risks from such acceleration that have been raised include:</p>
<ul>
<li>Capability progress outpacing the development of safety measure[10].</li>
<li>Model weight theft leading to proliferation of highly capable AI systems, amplifying other risks[11].</li>
<li>AI agents capable of autonomous AI R\&amp;D may be able to quickly improve their own (or a successor model's) abilities in other risk areas such as weapons R\&amp;D, self-replication, cyber offense, and persuasion, undermining other key evaluation and mitigation efforts[11].</li>
<li>Autonomous AI agents doing their own advanced R\&amp;D within a lab may have opportunities to sabotage control and oversight methods, if they are misaligned[20].</li>
</ul>
<p>While partial automation may be a relevant metric to track, many of the most severe risks under consideration by AI developers and others-such as explosive feedback loops, "self-improvement" and sabotage-become particularly acute when systems can fully automate R\&amp;D tasks over long time horizons with minimal human oversight. Full automation is also conceptually simpler to evaluate, since demonstrating any critical missing capability is sufficient to conclude that it is unlikely.
Managing these risks effectively may require developers to implement a wide range of mitigations (such as improved accountability mechanisms, and increased investment in control and alignment techniques). Securing the model weights has been identified as a mitigation of particular importance, which may be challenging to implement[10, 11, 21]. Security-related mitigations will often need to be implemented before an AI R\&amp;D capable model is created, since model weight theft could occur during or shortly after training. As a result, AI developers have identified the need for early warning evaluations, which can provide advance assurance that some further training or elicitation (within a 'safety buffer') will not lead to significant AI R\&amp;D risks[12, 11].</p>
<h1>2.2 Early warning evaluations</h1>
<p>Effective early warning evaluations need to have a low risk of underestimating AI capabilities, while being practical and avoiding early saturation. Some key challenges of achieving this include:</p>
<ul>
<li>Feasibility: Achieving a high score on the evaluation has to be feasible without demanding extreme levels of capability or guesswork. To be solvable, an evaluation needs to (among other things) provide unambiguous instructions, avoid blocking technical issues, and provide enough time and resources. ${ }^{3}$ This has come up as a frequent issue with many complex evaluations; for instance, many SWE-bench [22] problems turned out to be impossible and under-specified on closer inspection. This continued to hold true even after an attempt was made to identify a verified-to-be-feasible subset [23].</li>
<li>Ecological validity: To be able to calibrate risk thresholds without adding unnecessary margin or taking on excessive risk, there needs to be a clear link between evaluation results and risk levels. This is often challenging. For example, it is unclear how scores in MLEbench translate into the ability of AI agents to automate AI R\&amp;D.</li>
<li>Resistance to saturation: In recent years, most AI benchmarks and evaluations have been saturating rapidly [24]. If early warning evaluations saturate long before AI agents pose significant risks, they can no longer serve as an early warning. There are many potential causes of this, including the risk of contamination and the limited scope of many evaluations, and we discuss some approaches to mitigate this in Section 3.</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">Novel, non- <br> contaminated</th>
<th style="text-align: center;">Human <br> comparisons</th>
<th style="text-align: center;">Number of <br> tasks</th>
<th style="text-align: center;">Time <br> Horizon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MLE-bench [25]</td>
<td style="text-align: center;">$\nless$</td>
<td style="text-align: center;">$\sim^{4}$</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Sci-code [26]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\nless$</td>
<td style="text-align: center;">338</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPQA [27]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">448</td>
<td style="text-align: center;">30 m</td>
</tr>
<tr>
<td style="text-align: left;">GAIA [28]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">466</td>
<td style="text-align: center;">$5 \mathrm{~m}-20 \mathrm{~m}$</td>
</tr>
<tr>
<td style="text-align: left;">SWE-bench verified [29]</td>
<td style="text-align: center;">$\nless$</td>
<td style="text-align: center;">$\sim^{5}$</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$5 \mathrm{~m}-4 \mathrm{~h}$</td>
</tr>
<tr>
<td style="text-align: left;">WebArena [30]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">812</td>
<td style="text-align: center;">2 m</td>
</tr>
<tr>
<td style="text-align: left;">ML-Agent-Bench [31]</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\nless$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DISCOVERYWORLD [32]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\sim^{6}$</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">1 h</td>
</tr>
<tr>
<td style="text-align: left;">H-ARC-AGI [33]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">5 m</td>
</tr>
<tr>
<td style="text-align: left;">RE-Bench</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">8 h</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of different benchmarks and their characteristics. Here, "time-horizon" refers to the length of time humans spend on the task. We use $\checkmark$ to indicate that the benchmark satisfies the criteria, $\nless$ if it does not, and $\sim$ if the benchmark partially satisfies the criteria.</p>
<p>We think that more emphasis on faithful human comparisons can help significantly with the first two challenges (Section 3). By comparing to humans under equivalent conditions, we can confirm that our evaluations are feasible and provide crucial grounding for interpreting agent results.</p>
<h1>2.3 Related work</h1>
<p>Beyond benchmarking, much prior research has studied the usage of LLMs to assist with diverse aspects of machine learning research, from ideation to experimentation. LLMs are used to generate synthetic data for pretraining LLMs more efficiently and are used as reward models for fine-tuning LLMs with reinforcement learning $[3,4,34,35,36]$.
Most relevant to our work are other evaluations and benchmarks attempting to capture challenging engineering, autonomy or research skills in LLMs, many of which have been used as a part of early warning evaluations. Table 1 contains our evaluation of several benchmarks. We find that few benchmarks provide direct human comparisons and the ones that do are often over very short time horizons in much simpler environments or QA datasets.
Of particular relevance for our work is the usage of LLMs for code generation in the domain of machine learning, either as an LLM agent or, more commonly, in a fixed scaffold that does not allow the LLM to choose what tools to use. Focusing in on code generation, LLMs have been used to do autonomous machine learning research, neural architecture search, data science problems, paper reproduction, writing research papers, and reward function design, including preference optimization for LLM fine-tuning [37, 38, 39, 40, 41, 42, 43, 44].
LLMs for science. Besides machine learning, LLMs have also been applied to R\&amp;D in other scientific domains to assist with coding, chemical synthesis, biology, and virtual scientific discovery [26, 45, $46,47,32]$.
Also relevant to our work is the field of automated machine learning (AutoML), which also seeks to automate machine learning model development, including hyperparameter and model architecture selection. However, standard AutoML techniques are less applicable to LLMs, due to the greater complexity of LLM training and assessment [48].
We focus on AI capabilities in software aspects of AI R\&amp;D, as opposed to AI usage in hardware design [49], though progress in hardware development is a driver of AI progress [50].</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Summary of the creation process for RE-Bench. Our goal was to construct an early warning evaluation suite that is feasible to run, ecologically valid, and resistant to saturation (Section 2.2). Our environments seek to maximize coverage of AI R\&amp;D (Section 3.3) and enable human experts to make steady progress towards a high ceiling (Section 3.5), while satisfying practical constraints (Section 3.1).</p>
<h1>3 RE-Bench</h1>
<h3>3.1 Design goals</h3>
<p>To meet the three challenges to robust and effective early warning evaluations identified in Section 2.2-feasibility, ecological validity, and resistance to saturation-we set out to create a benchmark that directly compared human experts to AI agents in realistic AI R\&amp;D environments. To enable us to iterate quickly and gather data at a reasonable cost, we decided on two practical constraints: we evaluate human experts over 8 hours, and make sure all environments can be run with 8 or fewer H100 GPUs. The environments were designed with two key pillars in mind: (i) maximize coverage of key frontier AI R\&amp;D challenges, while (ii) ensuring that human experts under the same conditions as the agents can reliably make steady progress on the task, without running into issues or hitting a score ceiling.
Covering a large variety of the challenges involved in AI R\&amp;D reduces the risk of early saturation if AI agents become capable of some but not all of the key sub-skills needed to automate AI R\&amp;D. Similarly, ensuring that environments have a high ceiling, beyond what humans can achieve within the time budget, reduces the risk that the evaluation saturates due to both agents and humans reaching</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Brief description</th>
<th style="text-align: center;">Scoring function</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Optimize runtime</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Optimize LLM Foundry</td>
<td style="text-align: center;">Given a finetuning script, reduce its runtime as much as possible without changing its behavior.</td>
<td style="text-align: center;">Log time taken by the optimized script to finetune the model on 1000 datapoints.</td>
</tr>
<tr>
<td style="text-align: center;">Optimize a Kernel</td>
<td style="text-align: center;">Write a custom kernel for computing the prefix sum of a function on a GPU</td>
<td style="text-align: center;">Log time taken to evaluate the prefix sum of the function on $10^{11}$ randomly generated inputs.</td>
</tr>
<tr>
<td style="text-align: center;">Optimize loss</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fix Embedding</td>
<td style="text-align: center;">Given a corrupted model with permuted embeddings, recover as much of its original OpenWebText performance as possible</td>
<td style="text-align: center;">$\log (\operatorname{loss}-1.5)$ achieved by the model on the OpenWebText test set.</td>
</tr>
<tr>
<td style="text-align: center;">Scaling Law Experiment</td>
<td style="text-align: center;">Predict optimal tradeoff between hidden size and number of training steps for a model trained with 5 e 17 flops while only using much smaller training runs, with less than 1 e 16 flops, for experimentation</td>
<td style="text-align: center;">The sum of the Loss achieved by the submitted parameters on OpenWebText test set and prediction error of the submitted loss prediction.</td>
</tr>
<tr>
<td style="text-align: center;">Restricted Architecture MLM (Masked Language Model)</td>
<td style="text-align: center;">Build a model for text predication out of a limited set of PyTorch primitives, not including division or exponentiation.</td>
<td style="text-align: center;">$\log (\operatorname{loss}-1.5)$ achieved by the model on the OpenWebText test set.</td>
</tr>
<tr>
<td style="text-align: center;">Optimize win-rate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Finetune GPT-2 for QA</td>
<td style="text-align: center;">Finetune GPT-2 (small) to be an effective chatbot.</td>
<td style="text-align: center;">Average win percentage, as evaluated by Llama-3 8B, against both the base model and a GPT-2 (small) model finetuned on the Stanford Alpaca dataset.</td>
</tr>
<tr>
<td style="text-align: center;">Scaffolding for Rust Codecontest</td>
<td style="text-align: center;">Prompt and scaffold GPT-3.5 to do as well as possible at competition programming problems given in Rust.</td>
<td style="text-align: center;">Percentage of problems solved on a held-out dataset of 175 Code Contest problems.</td>
</tr>
</tbody>
</table>
<p>Table 2: Summary of environments. Detailed descriptions of the environments and reference solutions can be found in Appendix C.
the top score. Meanwhile, ensuring that human experts reliably make progress provides assurance that the environments are feasible and have unambiguous instructions.</p>
<p>Together these two pillars can allow for significantly improved grounding, by observing that if agents do worse than humans at key frontier AI R\&amp;D activities for the same cost, then they are unlikely to cost-effectively automate these experts. This does not follow in the other direction (i.e. agents might do better than human experts on this evaluation, without being capable of automating AI R\&amp;D in practice), since these environments are very far from covering all the challenges involved in frontier AI R\&amp;D (which generally occurs on a timescale of months or years, rather than a single day).
We will first describe the seven environments in the benchmark, then discuss how we gathered human baselines and finally go into detail on how we approached and assessed these requirements.</p>
<h1>3.2 Evaluation Description</h1>
<h3>3.2.1 Environments</h3>
<p>The current RE-Bench suite includes seven hand-crafted novel evaluation environments (see Table 2). Each environment presents a unique ML optimization problem, where achieving a high score generally requires significant experimentation, implementation, and efficient use of compute resources. We designed these environments to have a high ceiling, but still allow significant progress with just 8 hours of time and limited hardware resources (at most 6 H 100 s ).</p>
<p>Each evaluation environment includes:</p>
<ul>
<li>A scoring function, which defines the goal of the environment, and can be run by the agent at any time. Each time the scoring function is run a timestamped entry is added to a score log, allowing us to reconstruct progress over time. In almost all environments, the agent can see its score log, and inspect the details of the scoring function, to help it understand its goal. ${ }^{7}$</li>
<li>A (simple but poorly performing) starting solution which is provided to the agent and demonstrates what a valid solution looks like. This helps clarify the environment setup and allows the agent to get to the challenging research components of the evaluation faster. For example, in 'Optimize a Kernel,' the agent is provided with a simple but slow Python solution.</li>
<li>A reference solution, created by the task author, which scores highly. This solution is not provided to the agent, but is used in normalizing scores, and serves as an example of a good solution for researchers.</li>
</ul>
<p>For all environments except 'Scaling Law Experiment' (where only the final submission counts), the agent is assessed by the highest score registered in the score log at the end of the run.</p>
<h3>3.2.2 Normalizing scores</h3>
<p>Since each evaluation environment defines a different scoring metric, we use a normalization scheme to make scores comparable between them. For each environment we linearly transform the raw score $y$ to a normalized score $y_{s}$ so that the starting score $y_{s}$ (achieved by running the scoring function on the starting solution) is 0 , and the reference solution score $y_{r}$ (achieved by running the scoring function on the reference solution) is 1 :</p>
<p>$$
y_{n}=\frac{y-y_{s}}{y_{r}-y_{s}}
$$</p>
<p>It is possible to achieve scores significantly exceeding the reference solution, though we have not observed any normalized scores above 2. In addition, we assign solutions that score worse than the starting solution a score of 0 .
Since the difficulty of matching the reference solution varies by task, the average normalized score achieved also varies (in experiments with human experts over 8 hours, the average score is between $0.5-1.5$ for each environment).</p>
<h3>3.2.3 Creation Process</h3>
<p>We evaluated environments at 3 stages of development.</p>
<ul>
<li>Specification: Environment ideas were generated through discussions with ML practitioners, reading relevant literature or from past work experiences of the METR staff. To assess the idea, written descriptions of the proposed environments, scoring functions, and starting solutions were evaluated and reworked until we were optimistic that implementation would not be too challenging, and that the specification could do well on our two design pillars.</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Environment</th>
<th style="text-align: center;">Time to run and <br> score</th>
<th style="text-align: center;">Lines of code</th>
<th style="text-align: center;">Novel reference <br> solution?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Scaling Law</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">433</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: left;">Experiment</td>
<td style="text-align: center;">40 seconds</td>
<td style="text-align: center;">218</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Optimize a Kernel</td>
<td style="text-align: center;">2.5 hours</td>
<td style="text-align: center;">802</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Fix Embedding</td>
<td style="text-align: center;">8 minutes</td>
<td style="text-align: center;">746</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: left;">Scaffolding for Rust</td>
<td style="text-align: center;">50 minutes</td>
<td style="text-align: center;">495</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Codecontests</td>
<td style="text-align: center;">100 seconds</td>
<td style="text-align: center;">1,392</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: left;">Restricted</td>
<td style="text-align: center;">40 minutes</td>
<td style="text-align: center;">716</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: left;">Architecture MLM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Optimize LLM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Foundry</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Finetune GPT-2 for</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">QA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Overview of some key environment metrics. Time to run and score is an estimate of the time required for the agent to train/compile and then score the reference solution. (Note that the Scaling Law Experiment environment does not show the score to the agent at all). Lines of code includes all lines in files edited by the reference solution. The novelty of the reference solution is based on a subjective judgment call.</p>
<ul>
<li>Implementation: An initial full implementation of the evaluation was created, along with an internal solution. These were again evaluated and reworked until we were optimistic about the implementation meeting our requirements.</li>
<li>Baselining: After trials with human experts, we would evaluate how the environment performed on our criteria, and often make significant adjustments.</li>
</ul>
<p>Each environment is the product of significant iteration, and in the process of developing these 7, we dropped over 12 specifications (at various levels of completion), and more than 5 completed implementations which encountered unexpected difficulties or performed poorly on our criteria.</p>
<h1>3.3 Coverage of key AI R\&amp;D challenges</h1>
<p>Achieving good coverage of the many challenges involved in advancing AI R\&amp;D while ensuring experts can make significant progress in just 8 hours is challenging, since real AI R\&amp;D research projects usually take months to complete. This creates a trade-off between different types of difficulty (e.g. engineering complexity, novelty, slow feedback loops), where human experts or AI agents have to split their time between different kinds of difficulty. For instance, an environment that has significant engineering complexity may not leave as much time to come up with and test novel ideas, and thus will not emphasize those skills as much.
Therefore, we have focused on creating a highly varied suite, covering different pipeline stages (pretraining, post-training, scaffolding), and types of difficulty. Table 3 highlights this variety along three key dimensions of difficulty: (i) the length of feedback loops in the environment, which determines how much the agent can rely on search/trial-and-error, (ii) the number of lines of code the agent needs to interact with and (iii) the novelty of strong solutions. These metrics of course do not fully capture the types of difficulty involved in the environments, e.g. while strong Optimize a Kernel solutions involve few lines of code, the code is highly complex Triton.
To assess our coverage of particular frontier AI R\&amp;D tasks, we compare our environments to the key areas identified in Epoch AI's recent survey on AI R\&amp;D automation, as seen in Table 4 [7]. We observe that our suite is lacking environments focused on organizing distributed training, setting research directions, creating datasets and investigating hardware issues, but otherwise has reasonable coverage. We have also conducted informal interviews with frontier ML researchers at several AI developers, who identified the following additional missing areas:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Area</th>
<th style="text-align: center;">Examples from Epoch</th>
<th style="text-align: center;">Relevant environments</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Creating hypothesis</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">High-level planning</td>
<td style="text-align: center;">Conceptual thinking, ideas for system improvements, research direction</td>
<td style="text-align: center;">Fix Embedding, Restricted Architecture MLM</td>
</tr>
<tr>
<td style="text-align: center;">Detailed planning</td>
<td style="text-align: center;">Experiment plans, adapting ideas, prioritization</td>
<td style="text-align: center;">Optimize LLM Foundry, Finetune GPT-2 for QA</td>
</tr>
<tr>
<td style="text-align: center;">Designing experiments</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Prototype engineering</td>
<td style="text-align: center;">Writing code, trying an architectural change, creating a dataset</td>
<td style="text-align: center;">Restricted Architecture MLM</td>
</tr>
<tr>
<td style="text-align: center;">Performance engineering</td>
<td style="text-align: center;">Improving efficiency in resources such as compute/memory, engineering a system for large-scale operation, improving a dataset</td>
<td style="text-align: center;">Optimize a Kernel, Optimize LLM Foundry, Finetune GPT-2 for QA</td>
</tr>
<tr>
<td style="text-align: center;">Debugging</td>
<td style="text-align: center;">Reproducing error behavior, finding causes for errors, modifying code to fix errors</td>
<td style="text-align: center;">Optimize a Kernel (Triton is poorly documented and challenging)</td>
</tr>
<tr>
<td style="text-align: center;">Running experiments</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Organizing distributed training</td>
<td style="text-align: center;">Compute allocation, scripting for submission, monitoring, etc., investigating network/hardware problems</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Monitoring training runs</td>
<td style="text-align: center;">Detecting cluster problems in training, resolving errors and issues, checking logs to monitor training progress</td>
<td style="text-align: center;">Scaling Law Experiment</td>
</tr>
<tr>
<td style="text-align: center;">Analyzing results</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Results</td>
<td style="text-align: center;">Interpreting whether results confirm hypotheses, examining examples to inform hypotheses, deducing causes for system behaviors</td>
<td style="text-align: center;">Fix Embedding, Scaling Law Experiment</td>
</tr>
</tbody>
</table>
<p>Table 4: Attempted mapping of our environments onto 8 skills belonging to 4 subparts of an AI R\&amp;D workflow identified in the Epoch AI R\&amp;D automation survey [7]. We map each of our environments onto the two most relevant skills.</p>
<ul>
<li>Technical design, architecting systems that will be maintainable and expandable over time, making design tradeoffs.</li>
<li>Challenging and novel math/theory work.</li>
<li>Creating effective metrics which can then be optimized for.</li>
<li>Environments that demand more generalization/transfer from solutions.</li>
<li>Environments with more overlapping or poorly defined constraints/goals.</li>
<li>Greater engineering complexity.</li>
</ul>
<p>Overall, we believe our suite covers a reasonable variety of AI R\&amp;D challenges-but the limited number of environments and short time horizons inevitably leave some significant gaps, which we hope to address in future iterations of this benchmark.</p>
<table>
<thead>
<tr>
<th>Source</th>
<th>Unique Baseliners</th>
<th>Total Runs</th>
<th>Avg Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>ML RS/RE hiring process</td>
<td>43</td>
<td>45</td>
<td>0.48</td>
</tr>
<tr>
<td>Professional network</td>
<td>11</td>
<td>17</td>
<td>0.98</td>
</tr>
<tr>
<td>Graduate student outreach</td>
<td>7</td>
<td>9</td>
<td>0.83</td>
</tr>
<tr>
<td>Total</td>
<td>61</td>
<td>71</td>
<td>0.64</td>
</tr>
</tbody>
</table>
<p>Table 5: Summary statistics for our human expert baseline runs.</p>
<h1>3.4 Comparison to human experts</h1>
<p>To facilitate direct comparison with human expert performance and ensure our evaluation achieves our second design pillar-that humans reliably make progress without hitting a ceiling-we collected baselines of our environments with human experts under conditions equivalent to those used for AI agents (see detailed evaluation procedure in Appendix A). For practical reasons, we limited all expert baselines to 8 hours.</p>
<p>We selected human experts from three sources: the professional networks of METR staff, applicants to a Machine Learning Research Scientist/Research Engineer position at METR, and graduate student outreach.</p>
<p>Experts selected from the professional networks of METR staff members all have more than 5 years experience in an area highly relevant to the particular environment they baselined or have recently worked at organizations with strong ML research outputs (Google DeepMind, Google, Anthropic, OpenAI, FAR Labs, Redwood Research). Experts selected via the Research Scientist/Research Engineer hiring process needed to complete a CV and CodeSignal screen, short interview, and short task before being asked to complete a baseline. Graduate students were selected for having relevant advisors, and/or for being in an ML PhD program at University of California Berkeley, Carnegie Mellon University, Stanford University, or Massachusetts Institute of Technology. Table 5 shows the number of baselines from each source. The average scores achieved were meaningfully different for different sources, with experts from our professional networks scoring 0.96 normalized score on average, while hiring applicants only achieved an average of 0.46 .
We attempted to match experts to environments where they had relevant experience, whilst trying to maintain a balanced distribution of human expert experience between environments.</p>
<p>The results achieved by human experts vary significantly, with some attempts failing to improve on the starting solution and others significantly exceeding scores of the reference solutions (see Figure 4). Because of the high variance of human results, and the significant differences between baseliners from different sources, we caution against narrowly focusing AI agent comparisons on the average human result. In fact, when assessing the potential of AI agents to automate researchers at their best, working in their own fields with significant experience and expertise, it is more suitable to compare to the most successful human attempts.</p>
<h3>3.5 Validating feasibility and non-saturation</h3>
<p>To assess our second design pillar (that human experts reliably make progress and avoid issues, while not saturating the environments by running into a score ceiling), we investigated 4 metrics. These are represented in Table 6.</p>
<p>In all of our environments we find that by 8 hours, more than $70 \%$ of experts have made some progress, and greater than $80 \%$ of experts say that the score they achieved matched their expectation. This indicates that the environments are all possible to make progress on, and that the instructions and feedback provided are sufficient to make the scoring unambiguous.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Human expert score over time spent across the 7 AI R\&amp;D environments. Shaded regions show mean performance of the respective percentiles across tasks. We see a very significant spread in human performance, though the vast majority of human expert baseline attempts eventually achieve non-zero scores (i.e. improve on the starting solution).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Environment</th>
<th style="text-align: center;">Experts scoring <br> $&gt;0.05$ at 8 h</th>
<th style="text-align: center;">Experts: score <br> matches <br> expectation</th>
<th style="text-align: center;">Standard <br> deviation of <br> starting score</th>
<th style="text-align: center;">Major technical <br> issues</th>
<th style="text-align: center;">Saturated by <br> experts?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Scaling Law <br> Experiment</td>
<td style="text-align: center;">$7 / 8$</td>
<td style="text-align: center;">$6 / 8$</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">$0 / 8$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Optimize a Kernel</td>
<td style="text-align: center;">$7 / 9$</td>
<td style="text-align: center;">$7 / 7$</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">$1 / 9$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Fix Embedding</td>
<td style="text-align: center;">$10 / 13$</td>
<td style="text-align: center;">$11 / 13$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$1 / 11$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: left;">Scaffolding for <br> Rust Codecontests</td>
<td style="text-align: center;">$8 / 11$</td>
<td style="text-align: center;">$6 / 7$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$2 / 10$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: left;">Restricted <br> Architecture MLM</td>
<td style="text-align: center;">$10 / 11$</td>
<td style="text-align: center;">$9 / 11$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">$1 / 11$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Optimize LLM <br> Foundry</td>
<td style="text-align: center;">$8 / 8$</td>
<td style="text-align: center;">$7 / 8$</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">$0 / 3$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Finetune GPT-2 for <br> QA</td>
<td style="text-align: center;">$10 / 11$</td>
<td style="text-align: center;">$9 / 9$</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">$0 / 9$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 6: Evaluation of individual environments on our validation metrics (Section 3.5, see Appendix B. 2 for more details on how these are assessed). Note that not all experts responded to the survey question, and not all runs have been investigated for issues. Saturation by experts is our assessment of whether the top human score could be improved by more than $15 \%$.</p>
<p>By inspecting runs and feedback from experts we find that major/blocking technical issues are rare but do happen in more than $5 \%$ of runs (examples include external APIs being down, GPU/hardware issues, and DevOps issues with package installations), highlighting some of the challenges of running complex environments like these. We also find significant variance in the scoring of Finetune GPT-2 for QA, likely due to the starting RL training script easily diverging. We discuss how the variance in this environment introduces issues for interpreting its score in Section 6.
Finally, we find that most environments have significant room for further improvement beyond the top expert score, though the top Finetune GPT-2 for QA and Scaling Law Experiment solutions are</p>
<p>both likely within $15 \%$ of the maximum achievable score. Even in these environments, only a single expert run got close to saturation.</p>
<h1>4 Experiments and results</h1>
<p>For both humans and AI agents, our experiments use the open-source Vivaria platform [51] to set up secure VMs equipped with $20-48 \mathrm{vCPUs}, 200-400$ GB RAM and between 0 and 6 H100s (depending on the requirements specified by the task, see Appendix C for details), which the human or agent can use freely. We evaluate the most recent version of Claude 3.5 Sonnet (claude-3-5-sonnet-20241022 [52] along with o1-preview [53], in two scaffolds. The first is METR's Modular agent, a basic general-purpose scaffold which METR has used for general autonomy evaluations before, and the second is AIDE [54], which performed the best in recent studies of MLE-bench [25]. Both scaffoldings were lightly modified to be able to run and observe the output of the scoring function in each environment, and to deal with command timeouts (see Appendix A for details). We also compare to the previous version of Claude 3.5 Sonnet (claude-3-5-sonnet-20240620 in the Modular scaffold. Preliminary studies found that o1-preview performed poorly in the Modular scaffold [55], so we focused our evaluation of this model on the AIDE scaffolding.
Similar to MLE-bench [25], we ran the agent with a wall-clock time limit, rather than a token usage limit, to make sure that agents and humans are given an equal amount of GPU hours with which to complete experiments (while this means API latency can affect results, we believe this has only a modest effect since agents spend a majority of the time waiting for computations to finish). We did not count pauses due to API rate limits or other API errors toward the agents' time usage. More details on agent and human evaluation procedures and API latency can be found in Appendix A.</p>
<h3>4.1 Agent performance by time spent</h3>
<p>We evaluate how agent scores evolve over time, by averaging over log-linear interpolations of scores achieved by the agent across 3 to 58 -hour attempts per environment. We observe o1-preview and Claude 3.5 Sonnet making rapid progress initially, but slowing down after the first few hours (see Figure 5), while human experts are slow to start but then make rapid progress.
We also note that the AIDE scaffold seems to do better in this setting, continuing to make occasional progress after the Modular agent has stopped.</p>
<h3>4.2 Agent performance by number of samples</h3>
<p>Instead of doing one long run, we also consider best-of- $k$ evaluations with shorter time limits for individual attempts, spending the same total time budget. In these experiments we sample $k$ results (with replacement) for each environment, and pick the highest score achieved across all $k$ runs (except for Scaling Law Experiment, where the agent cannot access the score achieved, and instead selects randomly). Note that this is a procedure which AI agents could implement themselves by regularly resetting their environment and deleting their context, and so we believe this is a fair comparison.
In Figure 6 we find that the agents in the Modular scaffold do significantly better over many short 30-minute attempts, while agents in the AIDE scaffold do best with fewer 2-hour attempts. We also find that o1-preview in AIDE and Claude 3.5 Sonnet (claude-3-5-sonnet-20241022) in the Modular scaffold achieve the highest scores overall, matching the $36^{\text {th }}$ and $37^{\text {th }}$ percentile of human experts respectively.
We also evaluate how the best-of- $k$ score (score@ $k$ ) changes as you continue to increase the number of samples beyond the 8 -hour time budget. In Figure 7, we find that both versions of Claude 3.5 Sonnet continue to improve steadily with more 30 minutes samples, but that even with $k=128$ samples agents are far from top human performance. We find similar results with additional 2-hour samples, as seen in Figure 8.
Figure 2 aggregates these results, comparing the scores achieved with the best allocation of time budget we found for each agent to human performance. We find that overall, humans are slower to get started, but seem to be on a much steeper improvement trajectory, and reach a much higher score with a 32-hour time budget than any agent. For the Modular agents, this plots the time budget</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average (normalized) score achieved by time spent across the 7 environments. We only include the top agent of each scaffold (Modular and AIDE). We observe that both ol-preview and Claude 3.5 Sonnet outperform humans over short time horizons (under 2 hours), but then fail to improve their scores further while human scores rapidly improve. The performance dip ( 7 min-1 hour) for Claude 3.5 Sonnet stems from an anomalous lucky guess early in one run of the 'scaling law experiment' environment. See Figure 48 for more detailed results and 6.3 for discussion of this environment.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparing 3 ways of allocating 8 total hours, we find that 30-minute runs score the best for agents in the Modular scaffold, while 2 hour runs score the best for AIDE agents.</p>
<p>Score@k (95\% CI): 30min Time Limit
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Best achieved normalized score (averaged over environments) by number of samples, each with a time limit of 30 minutes. We see both agents improve significantly with more samples, though they remain far from top human performance even with several times more time spent per environment. We collected fewer 30-minute samples of the AIDE agents since they did better with 2-hour samples.</p>
<p>Score@k (95\% CI): 2h Time Limit
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Best achieved normalized score (averaged over environments) by number of samples, each with a time limit of 2 hours. We see both agents improve significantly with more samples, though they remain far from top human performance even with several times more time spent per environment. We collected fewer 2-hour samples of the Modular agents since they did better with 30 minutes samples.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Comparison of agent vs human performance across the 7 environments. Stars indicate best achieved score. The maximum scores for the average task exclude Scaling Law Experiment.
allocated as 30-minute runs, while for AIDE agents it uses 2-hour runs when the time budget is 2 hours or more, otherwise 30-minute runs. We also evaluate human attempts beyond 8 -hour time budgets in the same way. It is possible that better results could be achieved (for agents and humans) by using other time horizons, e.g. giving human experts 16 hours per attempts or agents 8 hours for longer time budgets. We leave this investigation for future work.</p>
<h1>4.3 Agent performance by environment</h1>
<p>In Figure 9, we compare the highest performing allocation of 32 hours for our two best agents (Claude 3.5 Sonnet in Modular, and o1-preview in AIDE) to human results (the best over four 8 -hour runs) to match the agent time budget. We find significant heterogeneity in which environments different agents excel at. The Claude 3.5 Sonnet agent gets close to human performance in 'Finetune GPT-2 for QA' and 'Scaling Law Experiment' environment, which o1-preview does poorly in, while o1-preview exceeds human performance at 'Optimize a Kernel'.</p>
<h2>5 Qualitative analysis of human-AI gap</h2>
<p>To better understand under what conditions human experts do better or worse than AI agents and why, we qualitatively analyzed the agent transcripts and solutions.</p>
<h3>5.1 Investigating agent successes</h3>
<p>While agents are generally unable to find solutions matching the top human experts, they have some striking successes. For instance, both Claude 3.5 Sonnet (New) and o1-preview in the AIDE scaffold</p>
<p>are able to find solutions to "Optimize a Kernel" that run about twice as fast as the reference solution (see Figure 18), with the o1-preview solution beating all 9 human experts. These are complex and novel algorithms that effectively work around communication limitations in GPUs and utilize a low-resource programming language (Triton) that lacks good public documentation. This was a surprising result, though it may be partially attributed to an expertise gap between the AI agent and many of the human experts, most of whom did not have specific expertise in programming GPU kernels. This shows up in several environments where some human experts, who are less familiar with the subfield, have to spend significant time learning about or reproducing standard approaches-or fail to do so at allwhile AI agents often find this trivial with their broad knowledge.</p>
<p>Another key contributor to agent successes might be their ability to try many more solutions than human experts. On average, AIDE and modular agents run score 36.8 and 25.3 times per hour respectively, while human experts only do so 3.4 times. This often leads to agents finding highly optimized 'local-optima' solutions which simply tweak the parameters and code of the starting solution, and yet achieve a surprisingly large improvement. For instance, many agent runs solve the same "Optimize a Kernel" environment not by writing a successful Triton solution (which is very difficult), but by carefully tweaking the starting Pytorch solution, making it run significantly faster. This also seems to be the case with the best agent solutions to "Finetune GPT-2 for QA" (see Figure 21), which tweaks the parameters of the starting solution and gets very lucky with the training trajectory and evaluation (as noted earlier, this environment can be very noisy). Rerunning the agent solution, it achieves a normalized score of only 0.69 (significantly lower than the original score of 0.88 ), indicating that the high agent score is partially driven by overfitting to this noise.</p>
<p>This ability to try a very large number of solutions would not work nearly as well without an ability to occasionally generate creative and effective solutions, as seen in the Triton kernel but also in workarounds for the limitations in "Restricted Architecture MLM" (see Figure 20). While human experts seem more reliable at identifying effective approaches, this might not matter as much in environments where evaluating solutions is cheap, and these occasional good ideas are often enough for agents to make significant progress.</p>
<h1>5.2 Investigating agent failures</h1>
<p>Despite often having more knowledge about the domain, and rapidly proposing and evaluating many more solutions, agents still do not reach the level of strong human experts in most environments.</p>
<p>One reason for this is a lack of variety in the solutions proposed by agents. For example, in "Restricted Architecture MLM", the agent attempts to use lightly modified transformer architectures $84 \%$ of the time, despite the fact that transformers work very poorly without division and exponentiation. ${ }^{9}$ It is possible that future scaffolding improvements could better incentivize variety in agent solutions and achieve higher scores.</p>
<p>Another limitation is consistent misunderstandings of instructions, particularly in "Restricted Architecture MLM" and "Optimize LLM Foundry". In some cases (see above mentioned Figure 14), these misreadings of environments can lead to agents finding impressive and unexpected loopholes, which score well on automated evaluation, but are nevertheless clearly breaking the environment rules upon manual inspection (reference A for details).</p>
<p>Perhaps the biggest observed limitations are in the area of long-horizon agency: effectively learning from new information, building on previous work and recovering from failures. As seen in Figure 15, AI agents often make stubborn and incorrect assumptions, and do not notice or correctly interpret contradictory information. Figure 13 demonstrates how they struggle to recover from zombie processes (e.g. from timeouts) taking up VRAM. The fact that Modular does better when previous work and context is wiped every 30 minutes is also an indication that the AI agent overall accumulates more issues and false assumptions than useful insights and code to build on over time (AIDE avoids this issue by not accumulating large contexts by design, instead doing tree-search over whole solutions).</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Environments with greater engineering complexity, as measured by the number of lines of code in all files edited by the reference solution, tend to favor humans over agents.</p>
<h1>5.3 Characteristics of environments with small human-AI gap</h1>
<p>Based on these observations, we might expect that AI agents would do especially well compared to humans in environments with:</p>
<ul>
<li>Short and high-fidelity feedback loops, which allow AI agents to lean into their ability to rapidly try many solutions.</li>
<li>Low engineering complexity, so that AI agents can solve the environments in a few steps rather than needing to build complex programs over longer time horizons.</li>
<li>Specialized expertise requirements, which AI agents are more likely to have than most human experts.</li>
<li>Significant noisiness which benefits the many attempts AI agent make over the fewer human expert attempts.</li>
<li>Few surprises and little need for exploration or discovery, which AI agents may struggle with.</li>
</ul>
<p>It is difficult to validate these empirically with so few environments, but in Figure 10 we find that our results are compatible with low engineering complexity contributing to a smaller human-AI gap $\left(R^{2}=0.602\right)$, while we did not find a clear connection with length of feedback loops or novelty. The two environments that require the most exploration/discovery are "Optimize LLM Foundry" (where profiling the script and codebase for inefficiencies is critical) and "Fix Embedding" (where the issue with the embeddings has to be investigated), which both have a large gap.</p>
<h2>6 Discussion</h2>
<h3>6.1 Agent performance on RE-Bench may overestimate AI R\&amp;D automation capabilities</h3>
<p>We expect the human-AI gap in real-world AI R\&amp;D to be much larger than the gap observed on these evaluations, and find it fairly plausible that the first agents that match top human performance in these environments may still be far from capable of AI R\&amp;D automation. Specifically, we believe there</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dimension of scale</th>
<th style="text-align: center;">RE-Bench</th>
<th style="text-align: center;">Real AI R\&amp;D</th>
<th style="text-align: left;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Time horizon</td>
<td style="text-align: center;">$8 \mathrm{~h} / 32 \mathrm{~h}$</td>
<td style="text-align: center;">6 months+</td>
<td style="text-align: left;">Many research projects take months to complete.</td>
</tr>
<tr>
<td style="text-align: left;">Feedback loops</td>
<td style="text-align: center;">2 h</td>
<td style="text-align: center;">6 months+</td>
<td style="text-align: left;">Developers may not get reliable feedback about <br> whether a technique or product is successful un- <br> til they've completed long and expensive train- <br> ing runs, and then gotten it into the hands of <br> customers.</td>
</tr>
<tr>
<td style="text-align: left;">Engineering <br> complexity (lines of <br> code)</td>
<td style="text-align: center;">1651</td>
<td style="text-align: center;">$1 \mathrm{M}+$</td>
<td style="text-align: left;">While individual research projects rarely need <br> to touch the whole codebase of an AI developer, <br> technical design and infrastructure projects may <br> need to engage with much of this complexity.</td>
</tr>
<tr>
<td style="text-align: left;">Parallel interacting <br> projects</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$100+$</td>
<td style="text-align: left;">It is not clear exactly how to count or define this <br> since AI agents might organize work differently <br> to humans, but it is clear that a great deal of work <br> would need to happen in parallel to match a real <br> AI developer.</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparison of the largest "scales" seen across different dimensions between RE-Bench and real AI R\&amp;D. As real AI R\&amp;D requires substantially more time, features longer feedback loops, and contains substantially more engineering and project management complexity than our environments, performance on RE-Bench is likely to be an overestimate of the real-world AI R\&amp;D performance of autonomous AI agents.
are two main reasons why an AI agent might perform well on our benchmark yet fail to contribute meaningfully to the automation of AI R\&amp;D:
Limited scope of tasks. The scale and complexity of real world AI R\&amp;D is at least 2 orders of magnitude (OOM) larger across many dimensions than those encountered in our environments (Table 7). We have seen that the human-AI gap increases over longer time horizons and qualitatively found indications that the same is true of high engineering complexity.
Cleanly defined, non-interacting tasks. Our tasks are well defined (in that each involves writing a modest amount of code to optimize a specified objective) and do not involve coordination between many interacting workstreams. We have not been able to investigate how agents deal with orchestrating parallel projects or with resolving ambiguous instructions, but given their difficulty identifying and fixing simple issues with their own environment, they may struggle with the additional communication bottlenecks, constraints and coordination difficulties required.
We also observe anecdotally that current AI agents seem to be much further from autonomously automating AI R\&amp;D than the relatively modest gap we observe in our environments would imply.</p>
<h1>6.2 Agent performance on RE-Bench may underestimate AI R\&amp;D automation capabilities</h1>
<p>At the same time, it's possible that AI agents that end up automating substantial fractions of the AI R\&amp;D process may nonetheless not exceed or even match human expert performance.
Agents are substantially cheaper than human experts: On average, our agents use $\sim 29 \mathrm{M}$ input tokens and $\sim 499 \mathrm{~K}$ output tokens in each 8 -hour run, at a cost of approximately $\$ 123$. This is only a small fraction of the approximately $\$ 1,855$ that we paid our human experts on average (and what a skilled human researcher costs in general), and the cost of agent runs could potentially be much lower if proper prompt caching was used.
Even if an AI agent takes longer time (or many more attempts) than a human expert to accomplish AI R\&amp;D tasks, they might nonetheless end up economically competitive with human researchers due to lower costs.
Different workflows or tooling. The tasks in RE-Bench are designed using the AI R\&amp;D workflows of human researchers. It may be possible that AI agents can automate AI research using alternative workflows that do not require performing similar tasks; for example, AI agents are capable of</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Best score@k results by token cost for AI agents and by money paid to human experts. Note that this figure excludes the cost of the H100s used in the environment. Though AI agents are generally unable to match expert human performance, AI agents are substantially cheaper than human experts, suggesting performance of these agents may be improved by approaches that make better use of more tokens for research compute. Note that because agents are so cheap per hour of compute usage, agents at a given cost budget in this figure are given more than $10 \times$ the compute resources than human experts.
generating synthetic data, and future workflows may involve more such data generation. Similarly, these evaluations require using existing standard software packages or interfaces optimized for human usage; it is possible that future AI research will involve more specialized tooling on which AI agents outperform humans.</p>
<h1>6.3 Other Limitations</h1>
<p>There are also other limitations to our results and benchmark, which we hope to address in future work.</p>
<p>Evaluation design criteria lead to unrepresentative environments: In order to create highreliability evaluations matching our design criteria we have tried to ensure that instructions and scoring are easily understood, that significant progress is possible within 8 hours, and that all necessary resources are provided. We have also had to select for environments that are easy for us to construct and assess. These constraints make the evaluation environments less representative of real research, where unclear goals, poor instructions, slow feedback and impossible problems are common.</p>
<p>Noisy agent results due to rare successes: Our results have significant noise from the small number of environments. Since agent scores are heavily skewed toward the right, with most runs scoring 0 and a few achieving very high scores, our estimates are sensitive to sampling noise.
Cost and complexity of evaluation: Running agents for many hours with access to H100 GPUs requires significant infrastructure and a large budget. This makes the evaluation less accessible to researchers, and makes it more challenging to run large-scale experiments comparing many models, scaffoldings and parameters.
Lack of scaffold iteration: Different agent scaffolds or prompts might be able to achieve a much better score on our benchmark in a similar time. In particular, we expect better performance from giving the agents better tools for managing GPU resources, and from approaches that make use of a larger number of tokens, e.g., by exploring many solutions in parallel (though limited compute resources pose limitations for this approach).</p>
<p>Limitations in covering frontier research: Due to the limited hardware access, and because frontier AI research is increasingly siloed within large AI developers, there may be significant differences between the kinds of research these evaluations cover, and the kinds driving frontier AI progress.
Agent solutions may be overfitted: All environments except for "Scaling Law Experiment" provide the agents with the test score output (to minimize the risk of misunderstandings or confusions). In experiments with humans, this did not seem to allow for significant overfitting as long as datasets were withheld, but AI agents are able to score many more solutions. In future iterations, we will likely only provide agents with a validation score in most environments, keeping the test score hidden. This issue seems to especially affect "Finetune GPT-2 for QA", which likely underestimates the human-AI gap in this environment.
"Scaling Law Experiment" scores involve luck: While good experimentation helps many human experts make informed predictions in this environment, agents rarely do this and mostly rely on guesswork, which comes down more to luck than skill. A better version of this environment would ensure that random guessing did much worse than real solutions.</p>
<h1>6.4 Conclusion</h1>
<p>In this work, we presented RE-Bench, a suite of environments that measure the ability of AI agents to automate AI R\&amp;D tasks. From our human expert baselines, we believe that achieving a good score on RE-Bench is feasible-though there is significant variance in the results. We also find that the environments are challenging, and most are not saturated even by the top human experts in 8 hours. We hope that these properties will allow for useful direct comparisons between human expert performance and AI agent performance on (short and self-contained) AI R\&amp;D activities.
Evaluating some current frontier AI agents we find that, when evaluated through best-of- $k$ with 8 hours of total compute budget, they achieve scores close to the average human expert, demonstrating very impressive capabilities. However a significant gap remains compared to the top human performance in most environments, as seen in Figure 9. Monitoring whether and how quickly AI agents are bridging this gap may help predict the emergence of autonomous AI R\&amp;D automation.
However, as discussed above, there are some significant limitations to this work. Future work may want to focus on expanding these evaluations to focus on longer time horizons, more expensive or challenging feedback loops and greater engineering complexity. We expect bridging these gaps while preserving feasibility and direct human comparisons to be a challenging and expensive development direction, but one that might be needed in the near future to avoid having to make compromises on the sensitivity and reliability of AI R\&amp;D evaluations.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Daniel Freeman for providing significant input over the course of the research project. We also wish to thank Nate Thomas, Eli Lifland, Kevin Liu, Rohin Shah, Lewis Ho, Mary Phuong, Matthew Kenney, Max Hasin, Daniel Ziegler, and David Rein for giving substantial feedback and helping review earlier drafts of this work.</p>
<h2>References</h2>
<p>[1] Chen, M. et al. Evaluating large language models trained on code. ArXiv abs/2107.03374 (2021). URL https://api.semanticscholar.org/CorpusID:235755472.
[2] Li, Y. et al. Competition-level code generation with AlphaCode. Science 378, 1092-1097 (2022). URL https://www.science.org/doi/abs/10.1126/science.abq1158. https:// www.science.org/doi/pdf/10.1126/science.abq1158.
[3] Gunasekar, S. et al. Textbooks are all you need (2023). URL https://arxiv.org/abs/2306. 11644. 2306.11644.
[4] AI at Meta. The Llama 3 herd of models (2024). URL https://arxiv.org/abs/2407.21783. 2407.21783 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ Based on manually analyzing 37 high scoring runs.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
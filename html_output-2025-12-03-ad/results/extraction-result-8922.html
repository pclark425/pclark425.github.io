<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8922 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8922</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8922</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-1823b8aecd62ccfca0cb6caa8e2a1159754afc5e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1823b8aecd62ccfca0cb6caa8e2a1159754afc5e" target="_blank">LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning, is proposed and a set of open-source LLMs are fine-tune, among which, Mistral serves as the best base model for chemistry tasks.</p>
                <p><strong>Paper Abstract:</strong> Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8922.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8922.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMolInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMolInstruct (Large-scale instruction tuning dataset for small molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3.3M-sample, high-quality, multi-task instruction tuning dataset focused on small molecules that covers 14 chemistry tasks (name conversion, property prediction, molecule captioning/generation, forward synthesis, retrosynthesis), built from PubChem, MoleculeNet, ChEBI-20, Mol-Instructions and USPTO-full and canonicalized for LLM instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>dataset</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.3M instruction-response samples; 1.6M distinct molecules</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Aggregated from PubChem (IUPAC/SMILES/formulas), MoleculeNet (six property datasets), ChEBI-20 and Mol-Instructions (molecule descriptions), and USPTO-full (reactions); canonicalized SMILES; SELFIES optionally provided.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Training/fine-tuning LLMs for small-molecule tasks (drug-discovery-relevant property prediction, molecule generation, reaction prediction/retrosynthesis, representation conversion).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction tuning: natural-language query-response pairs for multiple tasks (MG: text -> SMILES; FS/RS: reactants/reagents <-> product SMILES), with special content tags and canonicalized SMILES; used to fine-tune LLMs via LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in terms of novelty (fraction not in training sets or external databases not reported); dataset emphasizes diversity (1.6M distinct molecules) and complexity but does not report explicit novelty metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Task-specific instruction templates (e.g., textual description -> molecule for MG; product -> reactants for RS) enforce application conditioning; explicit tasks for drug-relevant property prediction ensure alignment to drug discovery properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used by authors for model evaluation: Exact Match (EM), Validity (SMILES validity), Fingerprint Tanimoto Similarity (FTS), METEOR (textual description), RMSE (regression), Accuracy (binary classification).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SMolInstruct enabled substantial performance gains when used to fine-tune LLMs (LlaSMol series) compared to base LLMs and prior instruction datasets (Mol-Instructions); ablation shows training on Mol-Instructions yields much worse performance than SMolInstruct.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against Mol-Instructions (Fang et al., 2023) and shown to produce materially better downstream LLM performance when used for instruction tuning; leads to narrower gap vs. SoTA task-specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Dataset does not quantify chemical novelty of generated molecules; some task evaluations (notably MG and MC) are difficult to assess for chemical correctness; potential residual data issues despite curation; generalization beyond trained tasks not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8922.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_Mistral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Mistral-based) — Mistral 7B fine-tuned on SMolInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM obtained by LoRA-based instruction tuning of the Mistral-7B base model on SMolInstruct, optimized for a broad suite of small-molecule tasks including molecule generation, reaction prediction, and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol_Mistral</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLM (Mistral 7B base, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Base 7B parameters; LoRA fine-tuned parameters ~41.9M (≈0.58% of total) reported as trainable during tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on SMolInstruct (3.3M samples across 14 tasks); SMILES canonicalized; data sources include PubChem, MoleculeNet, ChEBI-20, Mol-Instructions, USPTO-full.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Small-molecule design and analysis: molecule generation from text (MG), molecule captioning, forward synthesis (product prediction), retrosynthesis (reactant prediction), and drug-relevant property prediction (ESOL, Lipo, BBBP, ClinTox, HIV, SIDER).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction tuning via LoRA applied to linear layers (self-attention and FFN) with lora.r and lora.alpha=16; at inference uses beam search and task-specific prompts to directly generate SMILES or textual outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not explicitly reported; paper reports very high validity of generated SMILES for MG (Valid 99.7%) and structural similarity metrics (FTS for MG 61.7%) but does not report percentage of molecules novel relative to training data or external databases.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Generations conditioned by explicit task prompts (textual descriptions for MG; reactants/reagents for FS; product for RS); evaluated on application-specific metrics (property prediction RMSE/Accuracy, reaction EM/FTS/Validity) to measure suitability for drug-related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MG: Exact Match (EM), Fingerprint Tanimoto Similarity (FTS), Validity; MC: METEOR; FS/RS: EM, FTS, Validity; PP tasks: RMSE (ESOL/Lipo) and Accuracy (binary tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Strongest performer among the LlaSMol variants. MG: EM 19.2%, FTS 61.7%, Valid 99.7%; FS: EM 63.3%, FTS 84.9%, Valid 99.8%; RS: EM 32.9%, FTS 70.4%, Valid 100.0%. Achieves best overall LLM-based performance and narrows gap to SoTA task-specific models; surpasses SoTA on PP-ClinTox and PP-SIDER. Achieved with only small LoRA tuning; further gains possible with more trainable params.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms base LLMs (GPT-4, Claude 3 Opus, Galactica, Llama 2, Code Llama) on this benchmark suite; outperforms models fine-tuned on Mol-Instructions by a large margin; still generally below dedicated SoTA task-specific models on several tasks but approaches them.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Paper notes limitation that MG/MC evaluations cannot fully verify chemical correctness or application usefulness; novelty and real-world synthesizability of generated molecules not assessed; generalization beyond trained tasks untested; models may hallucinate chemically incorrect outputs in some contexts despite high validity; only a small fraction of parameters were tuned (possible suboptimal training).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8922.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Galactica-based) — Galactica 6.7B fine-tuned on SMolInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of LlaSMol created by LoRA fine-tuning of Galactica-6.7B on SMolInstruct, intended to leverage scientific pretraining for chemistry tasks including molecule generation and reaction prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol_Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLM (Galactica 6.7B base, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B base; LoRA fine-tuned (same LoRA config as others)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on SMolInstruct (3.3M samples). Galactica base was pretrained on scientific text and exposed to some chemistry data during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Small-molecule tasks: MG, MC, FS, RS, and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LoRA instruction tuning on SMolInstruct; beam search inference; direct generation of SMILES or textual outputs conditioned on prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported; reported MG Valid 99.6%, MG FTS 52.2%, MG EM 7.7% (lower EM than Mistral-based), no explicit novelty metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Same task conditioning as other LlaSMol models; evaluated on property and reaction tasks to assess application relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EM, FTS, Valid, METEOR, RMSE, Accuracy depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Substantially improved relative to base Galactica without tuning; MG: EM 7.7%, FTS 52.2%, Valid 99.6%; FS: EM 53.1%, FTS 79.9%, Valid 99.7%. Performs well overall but behind LlaSMol_Mistral on most metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Better than Galactica base and many other untuned LLMs on the benchmark after SMolInstruct tuning; trails Mistral-based LlaSMol and SoTA task-specific approaches on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Same general limitations as LlaSMol series: MG evaluation ambiguity, no novelty quantification, partial fine-tuning may limit ultimate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8922.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Llama 2-based) — Llama 2 7B fine-tuned on SMolInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LlaSMol variant created by LoRA fine-tuning of Llama 2 (7B) on SMolInstruct to improve LLM performance on multi-task chemistry benchmarks including molecule generation and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol_Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLM (Llama 2 7B base, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B base; LoRA fine-tuned (same LoRA config)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on SMolInstruct (3.3M samples).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Small-molecule tasks: MG, MC, FS, RS, property prediction relevant to drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LoRA instruction tuning on SMolInstruct; beam search generation; direct SMILES/text generation from prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported; MG: EM 6.4%, FTS 47.1%, Valid 99.6% (validity high but EM and FTS lower than Mistral-based).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Prompt-conditioned generation for task-specific outputs; evaluated on properties and reaction predictions for application relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EM, FTS, Valid, METEOR, RMSE, Accuracy depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Improved over untuned Llama 2 base but underperforms compared to LlaSMol_Mistral and LlaSMol_Galactica in many tasks; demonstrates base-model dependence on downstream chemistry performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms untuned Llama 2 and Molinst in some tasks but underperforms Mistral-based LlaSMol; Molinst (same base) was outperformed showing dataset impact.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Lower chemical generation fidelity (FTS, EM) vs. best LlaSMol; same caveats regarding MG evaluation and lack of novelty reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8922.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_CodeLlama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Code Llama-based) — Code Llama 7B fine-tuned on SMolInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LlaSMol variant built by LoRA fine-tuning Code Llama (a code-specialized Llama 2 variant) on SMolInstruct, exploring whether programming-language pretraining benefits molecule tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol_CodeLlama</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLM (Code Llama 7B base, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B base; LoRA fine-tuned (same LoRA config)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on SMolInstruct (3.3M samples).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Small-molecule tasks: MG, MC, FS, RS, property prediction; exploring synergy between code pretraining and molecular representation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LoRA instruction tuning; beam search; direct SMILES/text generation from prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported; MG: EM 6.5%, FTS 46.6%, Valid 99.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Task prompts condition generation; evaluated using same task-specific metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EM, FTS, Valid, METEOR, RMSE, Accuracy depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Performs better than LlaSMol_Llama2 on many tasks, suggesting benefits from code-oriented pretraining; still behind Mistral-based LlaSMol on top metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Superior to untuned Code Llama and some other baseline LLMs after SMolInstruct tuning; indicates base pretraining (code vs. general text vs. science) affects chemistry downstream results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Same as other LlaSMol variants: MG evaluation ambiguity, no novelty quantification, limited parameter fine-tuning may cap performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8922.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large closed-source LLM evaluated zero-shot on SMolInstruct chemistry tasks using task-specific prompt templates and few-shot ICL variants; serves as a strong baseline but showed limited chemistry task performance here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based autoregressive LLM (GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on broad web and code corpora (not fine-tuned on SMolInstruct for these experiments); evaluated via OpenAI API on SMolInstruct test samples.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Evaluated on same small-molecule tasks (MG, MC, FS, RS, PP) as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based zero-shot/few-shot (authors report best results in zero-shot for many tasks) with structured templates and in-context examples provided where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported; for MG and reaction tasks outputs often had lower validity and low exact-match rates (MG EM 6.4%, FTS 42.6%, Valid 81.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioning via prompts and ICL examples; performance indicates limited reliable application-specific molecule generation without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EM, FTS, Valid, METEOR, RMSE, Accuracy as used in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 performed poorly relative to LlaSMol models on SMolInstruct tasks — low EM and validity on name conversion and MG/FS/RS tasks and worse property prediction RMSE/Acc in several tasks (e.g., MG Valid 81.4%, FS EM 1.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Underperformed fine-tuned LlaSMol models despite being a larger closed-source model; highlights need for domain instruction tuning for chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Struggles with precise SMILES handling and chemical-specific textual formats; produces chemically implausible outputs per related-work citations and observed low validity; prompt engineering helps but is insufficient to match domain-tuned LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8922.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3 Opus (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 Opus (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art closed-source assistant-style LLM evaluated zero-shot on the SMolInstruct benchmark showing improved but still limited chemistry task performance compared to domain-tuned LlaSMol models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based conversational LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on broad corpora; not fine-tuned on SMolInstruct for these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Evaluated on small-molecule tasks (MG, MC, FS, RS, PP) as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based zero-shot using the same structured templates as for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported; exhibited higher validity than GPT-4 on many tasks (MG Valid 92.6%, MG FTS 57.6%, MG EM 12.3%) but still below domain-fine-tuned LlaSMol_Mistral.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Uses prompts to condition outputs; better than GPT-4 on several tasks but still improved by instruction-fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics as paper: EM, FTS, Valid, METEOR, RMSE, Accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Claude 3 Opus outperformed GPT-4 on many tasks but was still outperformed by LlaSMol models tuned on SMolInstruct; scored MG EM 12.3%, FTS 57.6%, Valid 92.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Better than untuned GPT-4 in this evaluation but inferior to SMolInstruct-fine-tuned LlaSMol models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Also limited in precise SMILES manipulation and reliable chemical generation without domain fine-tuning; sometimes produced non-exact or inaccurate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8922.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molinst / Mol-Instructions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molinst (Llama 2 model tuned on Mol-Instructions) / Mol-Instructions dataset (Fang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mol-Instructions is a prior molecule instruction dataset (≈1.3M instructions); Molinst refers to a Llama 2 model fine-tuned on that dataset — used in this paper as a baseline and ablation target showing substantially inferior performance to SMolInstruct-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molinst (Llama 2 fine-tuned on Mol-Instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLM (Llama 2 variant fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 2 base (7B in comparisons), dataset size ~1.3M (Mol-Instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Mol-Instructions dataset (Fang et al., 2023) — molecule- and biomolecule-oriented instruction pairs; authors also trained an LlaSMol variant on Mol-Instructions for ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation and reaction-related tasks (shared tasks: MC, MG, FS, RS) and other molecular instruction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction tuning on Mol-Instructions; direct generation of SMILES/text from prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported; in experiments models trained on Mol-Instructions had much worse task performance and lower validity on some tasks (e.g., ablation 'train on Mol-Instructions' reported MG Valid 88.2% and much worse EM/FTS on many tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Task conditioning via Mol-Instructions templates but judged lower-quality and coverage relative to SMolInstruct.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics used in paper: EM, FTS, Valid, METEOR, RMSE, Accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Molinst and the model trained on Mol-Instructions performed substantially worse than SMolInstruct-trained models across shared tasks; ablation shows training on Mol-Instructions produced poor MG/FS/RS outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Demonstrates that dataset quality and coverage (SMolInstruct > Mol-Instructions) are critical — switching dataset yields large performance differences even with same base model and LoRA settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Mol-Instructions contains lower-quality and more highly formatted templates per authors' analysis; training on it did not produce competitive LLM performance for chemistry in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8922.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5 (SoTA for MG/MC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (pretrained T5 model for molecule <-> text translation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-specific transformer model pretrained on SMILES and natural language and fine-tuned for molecule captioning and generation tasks; used as a SoTA comparison for MC and MG in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Encoder-decoder Transformer (T5-style) pretrained on SMILES and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (authors used released checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large SMILES and natural language corpora (MolT5 original work), fine-tuned on molecule captioning/generation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule captioning and generation (text ↔ molecule translation).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised translation between SMILES and natural language (sequence-to-sequence).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported here; SoTA results for MG/MC reported by authors: MG EM 31.7%, FTS 73.2%, Valid 95.3% (higher than LlaSMol variants in MG/MC metrics in this benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Trained specifically for mapping between molecular representations and natural language, providing higher performance on MC/MG than general LLMs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>METEOR (MC), EM/FTS/Validity (MG), as reported in paper comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolT5 provides superior MG/MC performance compared to LlaSMol variants in this benchmark (higher EM and FTS), illustrating that task-specific supervised models remain strong baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SoTA task-specific MolT5 outperforms LlaSMol on MC/MG despite LlaSMol's advantage of being a multi-task instruction-tuned LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Authors could not retrain MolT5 from scratch for direct comparison and relied on released checkpoints; MolT5 is task-specific rather than a multi-task foundation model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8922.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8922.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemLLM (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemLLM (chemical large language model, Zhang et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrently proposed chemistry LLM evaluated as a baseline in this paper; reported lower or mixed performance relative to LlaSMol models on many SMolInstruct tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based chemical LLM (details per original paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on chemistry instruction data (details in original ChemLLM work); dataset and eval details not available to authors for deep analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry tasks (molecule generation/captioning, property prediction, reactions) as evaluated on SMolInstruct.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction-tuning on chemistry data (per original work); direct generation from prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported; in this paper ChemLLM showed poor or variable validity and low EM/FTS on several tasks (e.g., MG Valid 4.3% in Table 2—this indicates possibly incompatible output formatting or evaluation extraction issues).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed for chemistry but performance in this benchmark was mixed; dataset/method differences not fully disclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>EM, FTS, Valid, METEOR, RMSE, Accuracy as used in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemLLM performed worse than LlaSMol models on many tasks in SMolInstruct evaluations; authors could not analyze its dataset/methods in depth due to lack of details.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Inferior to LlaSMol variants in this evaluation; reasons unclear due to unavailable details on training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Opaque training/evaluation details limit interpretation; reported very low validity/EM on some tasks suggests possible incompatibilities with this benchmark's formats or weaker instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>MolT5: A pretrained T5 model on SMILES and natural language for molecule-text translation <em>(Rating: 2)</em></li>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks <em>(Rating: 2)</em></li>
                <li>Drugchat: towards enabling chatgpt-like capabilities on drug molecule graphs <em>(Rating: 1)</em></li>
                <li>Is GPT-3 all you need for machine learning for chemistry? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8922",
    "paper_id": "paper-1823b8aecd62ccfca0cb6caa8e2a1159754afc5e",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "SMolInstruct",
            "name_full": "SMolInstruct (Large-scale instruction tuning dataset for small molecules)",
            "brief_description": "A 3.3M-sample, high-quality, multi-task instruction tuning dataset focused on small molecules that covers 14 chemistry tasks (name conversion, property prediction, molecule captioning/generation, forward synthesis, retrosynthesis), built from PubChem, MoleculeNet, ChEBI-20, Mol-Instructions and USPTO-full and canonicalized for LLM instruction tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_type": "dataset",
            "model_size": "3.3M instruction-response samples; 1.6M distinct molecules",
            "training_data": "Aggregated from PubChem (IUPAC/SMILES/formulas), MoleculeNet (six property datasets), ChEBI-20 and Mol-Instructions (molecule descriptions), and USPTO-full (reactions); canonicalized SMILES; SELFIES optionally provided.",
            "application_domain": "Training/fine-tuning LLMs for small-molecule tasks (drug-discovery-relevant property prediction, molecule generation, reaction prediction/retrosynthesis, representation conversion).",
            "generation_method": "Instruction tuning: natural-language query-response pairs for multiple tasks (MG: text -&gt; SMILES; FS/RS: reactants/reagents &lt;-&gt; product SMILES), with special content tags and canonicalized SMILES; used to fine-tune LLMs via LoRA.",
            "novelty_of_chemicals": "Not quantified in terms of novelty (fraction not in training sets or external databases not reported); dataset emphasizes diversity (1.6M distinct molecules) and complexity but does not report explicit novelty metrics.",
            "application_specificity": "Task-specific instruction templates (e.g., textual description -&gt; molecule for MG; product -&gt; reactants for RS) enforce application conditioning; explicit tasks for drug-relevant property prediction ensure alignment to drug discovery properties.",
            "evaluation_metrics": "Used by authors for model evaluation: Exact Match (EM), Validity (SMILES validity), Fingerprint Tanimoto Similarity (FTS), METEOR (textual description), RMSE (regression), Accuracy (binary classification).",
            "results_summary": "SMolInstruct enabled substantial performance gains when used to fine-tune LLMs (LlaSMol series) compared to base LLMs and prior instruction datasets (Mol-Instructions); ablation shows training on Mol-Instructions yields much worse performance than SMolInstruct.",
            "comparison_to_other_methods": "Compared against Mol-Instructions (Fang et al., 2023) and shown to produce materially better downstream LLM performance when used for instruction tuning; leads to narrower gap vs. SoTA task-specific models.",
            "limitations_and_challenges": "Dataset does not quantify chemical novelty of generated molecules; some task evaluations (notably MG and MC) are difficult to assess for chemical correctness; potential residual data issues despite curation; generalization beyond trained tasks not evaluated here.",
            "uuid": "e8922.0",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LlaSMol_Mistral",
            "name_full": "LlaSMol (Mistral-based) — Mistral 7B fine-tuned on SMolInstruct",
            "brief_description": "An open-source LLM obtained by LoRA-based instruction tuning of the Mistral-7B base model on SMolInstruct, optimized for a broad suite of small-molecule tasks including molecule generation, reaction prediction, and property prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol_Mistral",
            "model_type": "Transformer-based LLM (Mistral 7B base, instruction-tuned)",
            "model_size": "Base 7B parameters; LoRA fine-tuned parameters ~41.9M (≈0.58% of total) reported as trainable during tuning",
            "training_data": "Fine-tuned on SMolInstruct (3.3M samples across 14 tasks); SMILES canonicalized; data sources include PubChem, MoleculeNet, ChEBI-20, Mol-Instructions, USPTO-full.",
            "application_domain": "Small-molecule design and analysis: molecule generation from text (MG), molecule captioning, forward synthesis (product prediction), retrosynthesis (reactant prediction), and drug-relevant property prediction (ESOL, Lipo, BBBP, ClinTox, HIV, SIDER).",
            "generation_method": "Instruction tuning via LoRA applied to linear layers (self-attention and FFN) with lora.r and lora.alpha=16; at inference uses beam search and task-specific prompts to directly generate SMILES or textual outputs.",
            "novelty_of_chemicals": "Not explicitly reported; paper reports very high validity of generated SMILES for MG (Valid 99.7%) and structural similarity metrics (FTS for MG 61.7%) but does not report percentage of molecules novel relative to training data or external databases.",
            "application_specificity": "Generations conditioned by explicit task prompts (textual descriptions for MG; reactants/reagents for FS; product for RS); evaluated on application-specific metrics (property prediction RMSE/Accuracy, reaction EM/FTS/Validity) to measure suitability for drug-related tasks.",
            "evaluation_metrics": "MG: Exact Match (EM), Fingerprint Tanimoto Similarity (FTS), Validity; MC: METEOR; FS/RS: EM, FTS, Validity; PP tasks: RMSE (ESOL/Lipo) and Accuracy (binary tasks).",
            "results_summary": "Strongest performer among the LlaSMol variants. MG: EM 19.2%, FTS 61.7%, Valid 99.7%; FS: EM 63.3%, FTS 84.9%, Valid 99.8%; RS: EM 32.9%, FTS 70.4%, Valid 100.0%. Achieves best overall LLM-based performance and narrows gap to SoTA task-specific models; surpasses SoTA on PP-ClinTox and PP-SIDER. Achieved with only small LoRA tuning; further gains possible with more trainable params.",
            "comparison_to_other_methods": "Outperforms base LLMs (GPT-4, Claude 3 Opus, Galactica, Llama 2, Code Llama) on this benchmark suite; outperforms models fine-tuned on Mol-Instructions by a large margin; still generally below dedicated SoTA task-specific models on several tasks but approaches them.",
            "limitations_and_challenges": "Paper notes limitation that MG/MC evaluations cannot fully verify chemical correctness or application usefulness; novelty and real-world synthesizability of generated molecules not assessed; generalization beyond trained tasks untested; models may hallucinate chemically incorrect outputs in some contexts despite high validity; only a small fraction of parameters were tuned (possible suboptimal training).",
            "uuid": "e8922.1",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LlaSMol_Galactica",
            "name_full": "LlaSMol (Galactica-based) — Galactica 6.7B fine-tuned on SMolInstruct",
            "brief_description": "A version of LlaSMol created by LoRA fine-tuning of Galactica-6.7B on SMolInstruct, intended to leverage scientific pretraining for chemistry tasks including molecule generation and reaction prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol_Galactica",
            "model_type": "Transformer-based LLM (Galactica 6.7B base, instruction-tuned)",
            "model_size": "6.7B base; LoRA fine-tuned (same LoRA config as others)",
            "training_data": "Fine-tuned on SMolInstruct (3.3M samples). Galactica base was pretrained on scientific text and exposed to some chemistry data during pretraining.",
            "application_domain": "Small-molecule tasks: MG, MC, FS, RS, and property prediction.",
            "generation_method": "LoRA instruction tuning on SMolInstruct; beam search inference; direct generation of SMILES or textual outputs conditioned on prompts.",
            "novelty_of_chemicals": "Not reported; reported MG Valid 99.6%, MG FTS 52.2%, MG EM 7.7% (lower EM than Mistral-based), no explicit novelty metrics.",
            "application_specificity": "Same task conditioning as other LlaSMol models; evaluated on property and reaction tasks to assess application relevance.",
            "evaluation_metrics": "EM, FTS, Valid, METEOR, RMSE, Accuracy depending on task.",
            "results_summary": "Substantially improved relative to base Galactica without tuning; MG: EM 7.7%, FTS 52.2%, Valid 99.6%; FS: EM 53.1%, FTS 79.9%, Valid 99.7%. Performs well overall but behind LlaSMol_Mistral on most metrics.",
            "comparison_to_other_methods": "Better than Galactica base and many other untuned LLMs on the benchmark after SMolInstruct tuning; trails Mistral-based LlaSMol and SoTA task-specific approaches on several tasks.",
            "limitations_and_challenges": "Same general limitations as LlaSMol series: MG evaluation ambiguity, no novelty quantification, partial fine-tuning may limit ultimate performance.",
            "uuid": "e8922.2",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LlaSMol_Llama2",
            "name_full": "LlaSMol (Llama 2-based) — Llama 2 7B fine-tuned on SMolInstruct",
            "brief_description": "An LlaSMol variant created by LoRA fine-tuning of Llama 2 (7B) on SMolInstruct to improve LLM performance on multi-task chemistry benchmarks including molecule generation and property prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol_Llama2",
            "model_type": "Transformer-based LLM (Llama 2 7B base, instruction-tuned)",
            "model_size": "7B base; LoRA fine-tuned (same LoRA config)",
            "training_data": "Fine-tuned on SMolInstruct (3.3M samples).",
            "application_domain": "Small-molecule tasks: MG, MC, FS, RS, property prediction relevant to drug discovery.",
            "generation_method": "LoRA instruction tuning on SMolInstruct; beam search generation; direct SMILES/text generation from prompts.",
            "novelty_of_chemicals": "Not reported; MG: EM 6.4%, FTS 47.1%, Valid 99.6% (validity high but EM and FTS lower than Mistral-based).",
            "application_specificity": "Prompt-conditioned generation for task-specific outputs; evaluated on properties and reaction predictions for application relevance.",
            "evaluation_metrics": "EM, FTS, Valid, METEOR, RMSE, Accuracy depending on task.",
            "results_summary": "Improved over untuned Llama 2 base but underperforms compared to LlaSMol_Mistral and LlaSMol_Galactica in many tasks; demonstrates base-model dependence on downstream chemistry performance.",
            "comparison_to_other_methods": "Outperforms untuned Llama 2 and Molinst in some tasks but underperforms Mistral-based LlaSMol; Molinst (same base) was outperformed showing dataset impact.",
            "limitations_and_challenges": "Lower chemical generation fidelity (FTS, EM) vs. best LlaSMol; same caveats regarding MG evaluation and lack of novelty reporting.",
            "uuid": "e8922.3",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LlaSMol_CodeLlama",
            "name_full": "LlaSMol (Code Llama-based) — Code Llama 7B fine-tuned on SMolInstruct",
            "brief_description": "An LlaSMol variant built by LoRA fine-tuning Code Llama (a code-specialized Llama 2 variant) on SMolInstruct, exploring whether programming-language pretraining benefits molecule tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol_CodeLlama",
            "model_type": "Transformer-based LLM (Code Llama 7B base, instruction-tuned)",
            "model_size": "7B base; LoRA fine-tuned (same LoRA config)",
            "training_data": "Fine-tuned on SMolInstruct (3.3M samples).",
            "application_domain": "Small-molecule tasks: MG, MC, FS, RS, property prediction; exploring synergy between code pretraining and molecular representation generation.",
            "generation_method": "LoRA instruction tuning; beam search; direct SMILES/text generation from prompts.",
            "novelty_of_chemicals": "Not reported; MG: EM 6.5%, FTS 46.6%, Valid 99.7%.",
            "application_specificity": "Task prompts condition generation; evaluated using same task-specific metrics.",
            "evaluation_metrics": "EM, FTS, Valid, METEOR, RMSE, Accuracy depending on task.",
            "results_summary": "Performs better than LlaSMol_Llama2 on many tasks, suggesting benefits from code-oriented pretraining; still behind Mistral-based LlaSMol on top metrics.",
            "comparison_to_other_methods": "Superior to untuned Code Llama and some other baseline LLMs after SMolInstruct tuning; indicates base pretraining (code vs. general text vs. science) affects chemistry downstream results.",
            "limitations_and_challenges": "Same as other LlaSMol variants: MG evaluation ambiguity, no novelty quantification, limited parameter fine-tuning may cap performance.",
            "uuid": "e8922.4",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 (evaluated)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4, OpenAI)",
            "brief_description": "A large closed-source LLM evaluated zero-shot on SMolInstruct chemistry tasks using task-specific prompt templates and few-shot ICL variants; serves as a strong baseline but showed limited chemistry task performance here.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "Transformer-based autoregressive LLM (GPT family)",
            "model_size": "Not specified in paper (proprietary)",
            "training_data": "Pretrained on broad web and code corpora (not fine-tuned on SMolInstruct for these experiments); evaluated via OpenAI API on SMolInstruct test samples.",
            "application_domain": "Evaluated on same small-molecule tasks (MG, MC, FS, RS, PP) as a baseline.",
            "generation_method": "Prompt-based zero-shot/few-shot (authors report best results in zero-shot for many tasks) with structured templates and in-context examples provided where applicable.",
            "novelty_of_chemicals": "Not reported; for MG and reaction tasks outputs often had lower validity and low exact-match rates (MG EM 6.4%, FTS 42.6%, Valid 81.4%).",
            "application_specificity": "Conditioning via prompts and ICL examples; performance indicates limited reliable application-specific molecule generation without task-specific fine-tuning.",
            "evaluation_metrics": "EM, FTS, Valid, METEOR, RMSE, Accuracy as used in paper.",
            "results_summary": "GPT-4 performed poorly relative to LlaSMol models on SMolInstruct tasks — low EM and validity on name conversion and MG/FS/RS tasks and worse property prediction RMSE/Acc in several tasks (e.g., MG Valid 81.4%, FS EM 1.6%).",
            "comparison_to_other_methods": "Underperformed fine-tuned LlaSMol models despite being a larger closed-source model; highlights need for domain instruction tuning for chemistry tasks.",
            "limitations_and_challenges": "Struggles with precise SMILES handling and chemical-specific textual formats; produces chemically implausible outputs per related-work citations and observed low validity; prompt engineering helps but is insufficient to match domain-tuned LLMs.",
            "uuid": "e8922.5",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude 3 Opus (evaluated)",
            "name_full": "Claude 3 Opus (Anthropic)",
            "brief_description": "A state-of-the-art closed-source assistant-style LLM evaluated zero-shot on the SMolInstruct benchmark showing improved but still limited chemistry task performance compared to domain-tuned LlaSMol models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 3 Opus",
            "model_type": "Transformer-based conversational LLM",
            "model_size": "Not specified in paper (proprietary)",
            "training_data": "Pretrained on broad corpora; not fine-tuned on SMolInstruct for these experiments.",
            "application_domain": "Evaluated on small-molecule tasks (MG, MC, FS, RS, PP) as baseline.",
            "generation_method": "Prompt-based zero-shot using the same structured templates as for GPT-4.",
            "novelty_of_chemicals": "Not reported; exhibited higher validity than GPT-4 on many tasks (MG Valid 92.6%, MG FTS 57.6%, MG EM 12.3%) but still below domain-fine-tuned LlaSMol_Mistral.",
            "application_specificity": "Uses prompts to condition outputs; better than GPT-4 on several tasks but still improved by instruction-fine-tuning.",
            "evaluation_metrics": "Same metrics as paper: EM, FTS, Valid, METEOR, RMSE, Accuracy.",
            "results_summary": "Claude 3 Opus outperformed GPT-4 on many tasks but was still outperformed by LlaSMol models tuned on SMolInstruct; scored MG EM 12.3%, FTS 57.6%, Valid 92.6%.",
            "comparison_to_other_methods": "Better than untuned GPT-4 in this evaluation but inferior to SMolInstruct-fine-tuned LlaSMol models.",
            "limitations_and_challenges": "Also limited in precise SMILES manipulation and reliable chemical generation without domain fine-tuning; sometimes produced non-exact or inaccurate outputs.",
            "uuid": "e8922.6",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Molinst / Mol-Instructions",
            "name_full": "Molinst (Llama 2 model tuned on Mol-Instructions) / Mol-Instructions dataset (Fang et al., 2023)",
            "brief_description": "Mol-Instructions is a prior molecule instruction dataset (≈1.3M instructions); Molinst refers to a Llama 2 model fine-tuned on that dataset — used in this paper as a baseline and ablation target showing substantially inferior performance to SMolInstruct-trained models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Molinst (Llama 2 fine-tuned on Mol-Instructions)",
            "model_type": "Transformer-based LLM (Llama 2 variant fine-tuned)",
            "model_size": "Llama 2 base (7B in comparisons), dataset size ~1.3M (Mol-Instructions)",
            "training_data": "Mol-Instructions dataset (Fang et al., 2023) — molecule- and biomolecule-oriented instruction pairs; authors also trained an LlaSMol variant on Mol-Instructions for ablation.",
            "application_domain": "Molecule generation and reaction-related tasks (shared tasks: MC, MG, FS, RS) and other molecular instruction tasks.",
            "generation_method": "Instruction tuning on Mol-Instructions; direct generation of SMILES/text from prompts.",
            "novelty_of_chemicals": "Not reported; in experiments models trained on Mol-Instructions had much worse task performance and lower validity on some tasks (e.g., ablation 'train on Mol-Instructions' reported MG Valid 88.2% and much worse EM/FTS on many tasks).",
            "application_specificity": "Task conditioning via Mol-Instructions templates but judged lower-quality and coverage relative to SMolInstruct.",
            "evaluation_metrics": "Same metrics used in paper: EM, FTS, Valid, METEOR, RMSE, Accuracy.",
            "results_summary": "Molinst and the model trained on Mol-Instructions performed substantially worse than SMolInstruct-trained models across shared tasks; ablation shows training on Mol-Instructions produced poor MG/FS/RS outcomes.",
            "comparison_to_other_methods": "Demonstrates that dataset quality and coverage (SMolInstruct &gt; Mol-Instructions) are critical — switching dataset yields large performance differences even with same base model and LoRA settings.",
            "limitations_and_challenges": "Mol-Instructions contains lower-quality and more highly formatted templates per authors' analysis; training on it did not produce competitive LLM performance for chemistry in this study.",
            "uuid": "e8922.7",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MolT5 (SoTA for MG/MC)",
            "name_full": "MolT5 (pretrained T5 model for molecule &lt;-&gt; text translation)",
            "brief_description": "A task-specific transformer model pretrained on SMILES and natural language and fine-tuned for molecule captioning and generation tasks; used as a SoTA comparison for MC and MG in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MolT5",
            "model_type": "Encoder-decoder Transformer (T5-style) pretrained on SMILES and natural language",
            "model_size": "Not specified in this paper (authors used released checkpoint)",
            "training_data": "Pretrained on large SMILES and natural language corpora (MolT5 original work), fine-tuned on molecule captioning/generation datasets.",
            "application_domain": "Molecule captioning and generation (text ↔ molecule translation).",
            "generation_method": "Supervised translation between SMILES and natural language (sequence-to-sequence).",
            "novelty_of_chemicals": "Not reported here; SoTA results for MG/MC reported by authors: MG EM 31.7%, FTS 73.2%, Valid 95.3% (higher than LlaSMol variants in MG/MC metrics in this benchmark).",
            "application_specificity": "Trained specifically for mapping between molecular representations and natural language, providing higher performance on MC/MG than general LLMs in this study.",
            "evaluation_metrics": "METEOR (MC), EM/FTS/Validity (MG), as reported in paper comparisons.",
            "results_summary": "MolT5 provides superior MG/MC performance compared to LlaSMol variants in this benchmark (higher EM and FTS), illustrating that task-specific supervised models remain strong baselines.",
            "comparison_to_other_methods": "SoTA task-specific MolT5 outperforms LlaSMol on MC/MG despite LlaSMol's advantage of being a multi-task instruction-tuned LLM.",
            "limitations_and_challenges": "Authors could not retrain MolT5 from scratch for direct comparison and relied on released checkpoints; MolT5 is task-specific rather than a multi-task foundation model.",
            "uuid": "e8922.8",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ChemLLM (evaluated)",
            "name_full": "ChemLLM (chemical large language model, Zhang et al., 2024)",
            "brief_description": "A concurrently proposed chemistry LLM evaluated as a baseline in this paper; reported lower or mixed performance relative to LlaSMol models on many SMolInstruct tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChemLLM",
            "model_type": "Transformer-based chemical LLM (details per original paper)",
            "model_size": "Not specified in this paper",
            "training_data": "Trained on chemistry instruction data (details in original ChemLLM work); dataset and eval details not available to authors for deep analysis.",
            "application_domain": "Chemistry tasks (molecule generation/captioning, property prediction, reactions) as evaluated on SMolInstruct.",
            "generation_method": "Instruction-tuning on chemistry data (per original work); direct generation from prompts.",
            "novelty_of_chemicals": "Not reported; in this paper ChemLLM showed poor or variable validity and low EM/FTS on several tasks (e.g., MG Valid 4.3% in Table 2—this indicates possibly incompatible output formatting or evaluation extraction issues).",
            "application_specificity": "Designed for chemistry but performance in this benchmark was mixed; dataset/method differences not fully disclosed.",
            "evaluation_metrics": "EM, FTS, Valid, METEOR, RMSE, Accuracy as used in paper.",
            "results_summary": "ChemLLM performed worse than LlaSMol models on many tasks in SMolInstruct evaluations; authors could not analyze its dataset/methods in depth due to lack of details.",
            "comparison_to_other_methods": "Inferior to LlaSMol variants in this evaluation; reasons unclear due to unavailable details on training and evaluation.",
            "limitations_and_challenges": "Opaque training/evaluation details limit interpretation; reported very low validity/EM on some tasks suggests possible incompatibilities with this benchmark's formats or weaker instruction tuning.",
            "uuid": "e8922.9",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "MolT5: A pretrained T5 model on SMILES and natural language for molecule-text translation",
            "rating": 2
        },
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks",
            "rating": 2
        },
        {
            "paper_title": "Drugchat: towards enabling chatgpt-like capabilities on drug molecule graphs",
            "rating": 1
        },
        {
            "paper_title": "Is GPT-3 all you need for machine learning for chemistry?",
            "rating": 1
        }
    ],
    "cost": 0.02149575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</h1>
<p>Botao Yu Frazier N. Baker<em> ${ }^{</em>}$ Ziqi Chen* Xia Ning Huan Sun<br>The Ohio State University<br>Columbus, OH 43210, USA<br>{yu.3737, baker.3239, chen.8484, ning.104, sun.397}@osu.edu</p>
<h4>Abstract</h4>
<p>Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs named as LlaSMol, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Chemistry is a fundamental science that underpins countless aspects of modern life, ranging from drug discovery and materials science to energy production. To facilitate research and applications in this domain, deep learning models including graph neural networks (Kipf \&amp; Welling, 2017) and Transformer-based models (Vaswani et al., 2017) have been developed for various chemistry tasks such as forward reaction prediction, retrosynthesis, property prediction (Schwaller et al., 2019; Zhong et al., 2022; Chen et al., 2023; Zhou et al., 2023). However, these models are usually task-specific models, which neglect shared chemistry knowledge across tasks and can hardly be adapted to different tasks.
On the other hand, large language models (LLMs) such as GPT-4 (OpenAI, 2023), Llama series (Touvron et al., 2023a,b), and Mistral (Jiang et al., 2023) have emerged as general-purpose foundation models and demonstrate remarkable abilities on various natural language processing tasks (Chang et al., 2024; Thirunavukarasu et al., 2023; Yue et al., 2023; Zhang et al., 2023; Deng et al., 2023). However, when applied to chemistry tasks, LLMs show only limited capabilities (Jablonka et al., 2022; Guo et al., 2023; Hatakeyama-Sato et al., 2023). For example, Guo et al. (2023) conducted evaluations on eight chemistry tasks and observed that while GPT-4 outperforms other closed- and open-source LLMs, its performance is far from that of task-specific deep learning models. Particularly, they found that GPT models perform poorly when a precise understanding of SMILES (Weininger, 1988), a widely used textual representation for molecules, is required. In addition to directly applying pretrained LLMs, Fang et al. (2023) fine-tuned LLMs on an instruction tuning dataset, but their performance remains very low, far behind the state-of-the-art (SoTA) models designed and trained for specific tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of tasks in the proposed SMolInstruct dataset.</p>
<p>Given these discouraging results, some critical questions arise: Are LLMs actually able to effectively perform chemistry tasks? Or, Are they fundamentally limited for chemistry? In this paper, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, substantially outperforming the most advanced GPT-4 OpenAI (2023) and Claude 3 Opus Anthropic (2024).</p>
<p>What makes such LLMs possible? First, we construct a large-scale, comprehensive, and high-quality dataset for instruction tuning named SMolInstruct. We incorporate tasks with meaningful applications, collect data from diverse data sources, and apply rigorous scrutiny for quality control. The resulting dataset consists of 14 tasks (illustrated in Figure 1) and over 3M samples, laying a solid foundation for training and evaluating LLMs for chemistry tasks. Based on the dataset, we build a series of LLMs for chemistry named LlaSMol by fine-tuning four open-source LLMs namely Galactica, Llama 2, Code Llama, and Mistral, on SMolInstruct with LoRA (Hu et al., 2022).</p>
<p>We conduct comprehensive experiments to evaluate our models and explore their insights, yielding some interesting findings. Firstly, among the four LlaSMol models, the Mistral-based model surpasses others by a substantial margin, showcasing the considerable influence of base models on downstream chemistry tasks. Moreover, contrast to claims made in previous work (Fang et al., 2023), using SMILES as the molecular representation achieves sufficient validity of generated molecules and better performance compared to using SELFIES (Krenn et al., 2019). Furthermore, employing canonicalized SMILES during model training and applications can alleviate learning burdens and increase performance. Finally, while instruction tuning can inject chemistry task-related knowledge into models, the dataset plays a crucial role. Our experiments demonstrate that training on our SMolInstruct leads to substantially better performance compared to training on previous dataset, emphasizing the contribution of the proposed dataset. Although LlaSMol models do not yet surpass state-of-the-art (SoTA) task-specific models that are designed and trained specifically for each individual task, they approach SoTA performance with only 0.58% of parameters being fine-tuned, suggesting their great potential for further improvements and to serve as strong foundation models for the field.</p>
<h1>2 Related Work</h1>
<p>Task-specific Models for Chemistry. In recent years, many deep learning models have been developed to tackle different chemistry tasks. For example, Molecular Transformer Schwaller et al. (2019) and RSMILES Zhong et al. (2022) formulate forward synthesis and retrosynthesis prediction as sequence-to-sequence translation problems. Chemformer Irwin et al. (2022) pretrains a transformer model on a large-scale SMILES dataset and fine-tunes it for various downstream tasks, such as forward synthesis and property prediction. MolT5 Edwards et al. (2022) first pretrains a T5 model on both SMILES and natural language, and then fine-tunes it to translate SMILES into natural language (i.e., molecule captioning) or vice versa (i.e., molecule generation). Graph neural networks (GNNs), which directly leverage the graph structure of the molecule Wang et al. (2023), have also shown promise in many chemistry applications, such as property prediction Yang et al. (2019); Han et al. (2023), retrosynthesis Chen et al. (2023); Somnath et al. (2021), and molecule optimization Chen et al. (2021); Zhang et al. (2022b). Recent studies Zhou et al. (2023); Zhang et al. (2022a) have shown the promise of leveraging equivariant representations of molecular 3D structures for chemistry tasks, such as property prediction Zhou et al. (2023) and docking Zhang et al. (2022a). Uni-Mol Zhou et al. (2023) incorporates this 3D information into the pretraining of a transformer model and fine-tunes it for downstream tasks. Despite their effectiveness, these models operate on single tasks and therefore cannot harness knowledge shared across diverse chemistry tasks like LLMs.
LLMs for Chemistry. Recent efforts have integrated LLMs with chemistry to solve key chemistry problems, which can be divided into two categories: (1) benchmark studies, and (2) fine-tuning LLMs with new datasets. Multiple benchmark studies White et al. (2023); Guo et al. (2023); Jablonka et al. (2023); Liu et al. (2023a) have evaluated the capabilities and limitations of different off-the-shelf LLMs, such as GPT-4 and Llama, on chemistry problems. For example, Guo et al. (2023) finds that these LLMs do not perform well on chemistry tasks and often produce chemically implausible outputs. These findings highlight the need for further efforts to improve LLMs via fine-tuning for chemistry tasks.
To improve LLMs for chemistry, multiple instruction tuning datasets have been developed. Mol-Instructions Fang et al. (2023) consists of 1.3M instructions for multiple small molecule tasks. However, fine-tuning on the dataset does not significantly improve LLMs' performance (Section 4.3). Drugchat Liang et al. (2023) collects an instruction tuning dataset on drug properties with 10.8 K drug molecules. MolOpt-Instructions Ye et al. (2023) consists of instructions with 1 M molecule pairs for molecule optimization on six properties, in which each pair has similar molecules with different properties. Recent works also develop 2D or 3D molecular graph-centric datasets and integrate the graph understanding ability into LLMs Liu et al. (2023b); Cao et al. (2023); Li et al. (2024). Compared with these datasets, SMolInstruct is much larger and covers a more diverse and comprehensive set of chemistry tasks, which enables LLMs to better understand molecule representations and learn chemistry knowledge across tasks.</p>
<h2>3 SMolInstruct</h2>
<p>This section introduces our proposed dataset SMolInstruct and its construction. Readers may refer to Appendix A for preliminaries and background.</p>
<h3>3.1 Overview of SMolInstruct</h3>
<p>SMolInstruct is a large-scale instruction tuning dataset that centers around small molecules. It contains 14 chemistry tasks, illustrated in Figure 1.
(1) We include four name conversion tasks, namely converting IUPAC name to molecular formula (NC-I2F), converting IUPAC name to SMILES (NC-I2S), converting SMILES to molecular formula (NC-S2F), and converting SMILES to IUPAC name (NC-S2I). They are designed to enable deep understanding of molecular structures and representations, which should serve as the fundamental knowledge for chemistry LLMs.</p>
<p>(2) Additionally, six property prediction tasks (Wu et al., 2018) are integrated, including PPESOL for water solubility (Mobley \&amp; Guthrie, 2014), PP-Lipo for octanol/water distribution coefficient (Poole \&amp; Poole, 2003), PP-BBBP for blood-brain barrier penetration (Martins et al., 2012), PP-ClinTox for toxicity to human body (Gayvert et al., 2016), PP-HIV for HIV replication inhibition (Institute, 2004), and PP-SIDER for side effects of drugs (Kuhn et al., 2015). These involved properties are crucial especially for drug development.
(3) Two tasks focus on the textual descriptions of molecules: molecule captioning (MC) is to generate a textual description of a given molecule, and molecule generation (MG) is to generate a molecule based on the given textual description. They require comprehensive understanding of molecules - their structures and properties, from their textual descriptions. They also bridge the gap between natural language and molecules.
(4) Lastly, two tasks revolve around chemical reaction knowledge. Forward synthesis (FS) aims to predict potential products from reactants and reagents, and retrosynthesis (RS) involves predicting potential reactants given a product. These tasks play vital roles in real-world applications (Coley et al., 2018). For example, retrosynthesis is essential for synthesis planning, while forward synthesis is used to validate retrosynthetic suggestions.
SMolInstruct contains 3.3M samples. Each sample is a query-response pair, where the query describes a task and any task-specific information (e.g., input molecule, textual description, etc.), and the response is a sentence containing the answer to the queried task. For all the tasks, unless explicitly defined in the tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we use SMILES as the default representation for molecules, but also provide the SELFIES (Krenn et al., 2019) representation.</p>
<h1>3.2 SMolInstruct Construction</h1>
<p>We construct the SMolInstruct dataset by following a four-step pipeline: data collection, quality control, data splitting, and instruction construction.
Data Collection. After consulting domain experts and pinpointing the set of meaningful tasks (summarized in Section 3.1), we collect data for these tasks from various sources, as listed in Table 5. Specifically, for the name conversion tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we leverage PubChem ${ }^{2}$ (Kim et al., 2019), one of the most comprehensive molecule databases. Within this database, we randomly select a large set of molecule entries, and extract their IUPAC names, SMILES representations, and molecular formulas. This obtained data is then re-organized as input-output pairs for the tasks. For molecular descriptionrelated tasks (MC and MG), we utilize a combination of ChEBI-20 (Edwards et al., 2021; 2022) and Mol-Instructions (Fang et al., 2023), as they both contain high-quality moleculetext paired data. For property prediction tasks (PP-ESOL, PP-Lipo, PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER), we employ the well-established MoleculeNet datasets (Wu et al., 2018). We select the 6 datasets from MoleculeNet that represent the essential properties for real-world applications such as drug discovery. For chemical reaction tasks (FS and RS), we collect the reaction data from USPTO-full (Lowe, 2017), which is an extensive collection encompassing over 1 M reaction samples extracted from U.S. patents. All the aforementioned datasets are also widely used in previous studies (He et al., 2021; Zhong et al., 2022; Edwards et al., 2022; Irwin et al., 2022; Chen et al., 2023; Zhou et al., 2023).
Quality Control. To guarantee high quality, we apply rigorous scrutiny. The collected data contains many problematic and low-quality samples, which can be roughly categorized into the following three types, along with our curation methods: (1) Chemically invalid SMILES. Numerous SMILES strings are chemically invalid (e.g., deviating from the SMILES grammar, or violating chemical valence). To address this issue, we employ RDKit (RDKit, 2023), a widely used toolkit for cheminformatics, to parse molecules and detect errors. (2) Wrong or inaccurate information. Based on manual check, we observed wrong and inaccurate information recorded in the data. For instance, within the USPTO-full dataset (Lowe, 2017), we identify and correct mislabeled reactants and reagents in chemical reactions by comparing their atom mappings with products. For the MC and MG tasks, we filter out those</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>textual descriptions that lack pertinent, molecule-specific information, with a set of rules based on wording patterns, lengths and keywords. For PP-SIDER, we eliminate disorders with ambiguous names that could impede the creation of precise and comprehensible instructions. (3) Duplicated samples. We detect and remove them.
Data Splitting. Data splitting for multi-task datasets requires careful handling in order to avoid data leakage across tasks. For instance, FS and RS are a pair of reverse tasks, so data leakage occurs when the training set contains an FS sample for a certain chemical reaction and the test set has an RS sample for the same reaction. This can lead to biased evaluation. Therefore, we identify sample pairs across related tasks (FS and RS, MC and MG, and the four NC tasks) that correspond to the same molecules/reactions, and ensure that matched samples are placed together in either training or evaluation set. Moreover, some samples may share the same input but have different outputs. For instance, in the RS task, one product (the same input) may be synthesized from multiple sets of reactants (different outputs). If these samples are placed into both training and test set, it may lead to exaggerated performance. Therefore we ensure that samples with identical inputs are placed together either in or outside of the test set. Additionally, to achieve fair comparisons with Mol-instructions (Fang et al., 2023), for tasks shared between the two datasets (MC, MG, FS, and RS), we ensure that their training examples are not included in the test set of SMolInstruct, allowing for a direct evaluation of their models on our test set. Following these necessary limitations, samples are randomly split into training/validation/test set, except for PP task samples that undergo a scaffold splitting following the canonical method (Wu et al., 2018).
Instruction Creation. To create query-response textual pairs for instruction tuning, we manually craft several templates, each including a query and a corresponding response, and apply GPT-4 to rephrase them. Unlike those in (Fang et al., 2023) which consist of highly formatted queries (containing three explicitly labeled parts namely instruction, input, and output) and answer-only responses (e.g., responses for FS and RS only contain answer SMILES alone, without any natural text), our templates exhibit a more natural and diverse set of formats in both queries and responses, allowing for more variations and naturalness in input-output interactions. Moreover, all the SMILES representations are canonicalized, establishing a standardized data format. In light of the dataset's inclusion of multi-type sequences (SMILES, molecular formula, numbers, etc.) beyond natural language text alone, we utilize special tags to encapsulate corresponding segments (e.g., <SMILES> . . </SMILES> for SMILES, <MOLFORMULA> . . </MOLFORMULA> for molecular formula, <NUMBER>. . </NUMBER> for numbers). This design does not only explicitly inform models about the information types within the tagged content, but also facilitate answer extraction during evaluation.
For more details of dataset construction, please refer to Appendix B.2.</p>
<h1>3.3 Merits of SMolInstruct</h1>
<p>Compared to previous work (Fang et al., 2023; Liang et al., 2023; Ye et al., 2023), SMolInstruct stands out in several key aspects:
(1) Large-Scale. SMolInstruct consists of 3.3M samples and 1.6M distinct molecules, with a diverse range of sizes, structures, and properties (see Appendix B.1), showcasing an extensive coverage of diverse chemical knowledge.
(2) Comprehensive. SMolInstruct contains 4 types of chemical tasks (14 tasks in total), emerging as the most comprehensive instruction tuning dataset for small molecules. Notably, the tasks are meticulously selected to build a strong chemistry foundation model and to adapt to real-world applications.
(3) High-Quality. Rigorous processing steps have been implemented to exclude problematic and low-quality samples. Along with careful data splitting and canonicalization of SMILES representations, SMolInstruct stands as a high-quality resource valuable for future research.
A detailed introduction and statistics of the SMolInstruct dataset can be found in Appendix B. For a comparison with the previous work, Mol-Instructions (Fang et al., 2023), please refer to Appendix C.</p>
<h1>4 Experiments</h1>
<h3>4.1 Our LlaSMol Models</h3>
<p>By fine-tuning base models on the proposed SMolInstruct dataset, we create LLMs capable of performing chemistry tasks, which we name LlaSMol (Large language models on Small Molecules). Specifically, we extensively consider four different LLMs as our base models, namely Galactica 6.7B (Taylor et al., 2022), Llama 2 (Touvron et al., 2023b) 7B, Code Llama (Roziere et al., 2023) 7B, and Mistral (Jiang et al., 2023) 7B, where Galactica is trained for scientific applications and has already been exposed to chemistry-related data during its pretraining, Llama 2 and Mistral are general-purpose LLMs, while Code Llama is based on Llama 2 and trained for code. We conduct instruction tuning on the proposed SMolInstruct dataset, and name the resulting models as LlaSMol ${ }<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$, LlaSMol ${ }</em>}}$, LlaSMol ${ <em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$, and LlaSMol ${ }</em>-4$, and a cosine scheduler. The input length for training is set to 512 , which covers $99.7 \%$ of the samples. During inference, we adopt beam search as the generation strategy for simplicity.}}$, respectively. All the LlaSMol models are trained with LoRA (Hu et al., 2022), which is applied to all weight matrices in the self-attention and feedforward neural network (FFN) modules with lora.r and lora.alpha set to 16. The finetuning process utilizes the Huggingface Transformers library (Wolf et al., 2020). Training spans three epochs, employing the 8 -bit AdamW optimizer, a learning rate of $1 \mathrm{e</p>
<h3>4.2 Experimental Setup</h3>
<p>Compared Models. We compare our LlaSMol models with two types of models:
(1) LLMs without fine-tuning on SMolInstruct. This type includes our four base models, namely Galactica (Taylor et al., 2022), Llama 2 (Touvron et al., 2023b), Code Llama (Roziere et al., 2023), Mistral (Jiang et al., 2023). we also benchmark against GPT-4 (OpenAI, 2023) and the more recent Claude 3 Opus (Anthropic, 2024), the current state-of-the-art (SoTA) LLMs ${ }^{3}$. For Llama 2, Code Llama, and Mistral, we use 1-shot, due to their poor instruction following ability; for GPT-4, we report its results under a zero-shot setting, as GPT-4 performs best on this setting in our experiments (Appendix E); for Claude 3 Opus, we report its zero-shot results as well. We also include two LLMs tuned specifically for chemistry tasks: Molinst, a Llama 2 model tuned on the Mol-Instructions dataset by Fang et al. (2023), which shares the training tasks of MC, MG, FS, and RS with LlaSMol; and ChemLLM (Zhang et al., 2024), an LLM for chemistry proposed concurrently to our work.
(2) SoTA task-specific models. To provide a comprehensive view of LlaSMol's performance, we present results from SoTA task-specific models. For NC-I2S and NC-S2I, we compare with STOUT (Rajan et al., 2021), an encoder-decoder model trained on SMILES-IUPAC name paired data. For NC-S2F, a task achievable with a fixed algorithm, we implement a program with RDKit (RDKit, 2023), a widely used Python toolkit for cheminformatics, and report its results. For NC-I2F where no dedicated models exist, we construct a baseline called STOUT+RDKit by aggregating STOUT for I2S conversion and RDKit for S2F conversion. For the PP tasks, our compared model is Uni-Mol (Zhou et al., 2023). It incorporates molecular 3D representations and follows a pretraining and fine-tuning paradigm. Following its original settings, we fine-tune the model on our SMolInstruct dataset with its pretrained checkpoint. In the case of MC and MG, we compare with MolT5 (Edwards et al., 2022) and directly use their released checkpoint. The reasons why we do not use our re-trained model are: (1) we were unable to reproduce results close to those reported in the paper as no original code was provided; and (2) we take great care to ensure that our test set is devoid of training examples used by MolT5, ensuring fairness in the evaluation. Lastly, regarding FS and RS, we re-train RSMILES (Zhong et al., 2022) and Molecular Transformer (Schwaller et al., 2019) for the two tasks, respectively, following their reported settings. Both of the models are transformer encoder-decoder models (Vaswani et al., 2017), specifically adapted for the FS and RS tasks.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results for name conversion (NC) and property prediction (PP) tasks. Metrics EM, Valid, and Acc are in percentage.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">NC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I2F</td>
<td style="text-align: center;">I2S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S2F</td>
<td style="text-align: center;">S2I</td>
<td style="text-align: center;">ESOL</td>
<td style="text-align: center;">Lipo</td>
<td style="text-align: center;">BBBP</td>
<td style="text-align: center;">Clintox</td>
<td style="text-align: center;">HIV</td>
<td style="text-align: center;">SIDER</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">Task-Specific, Non-LLM Based Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SoTA</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">Existing LLMs without fine-tuning on SMoIInstruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.570</td>
<td style="text-align: center;">1.545</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">57.6</td>
</tr>
<tr>
<td style="text-align: center;">Claude 3 Opus</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.036</td>
<td style="text-align: center;">1.194</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;">Galactica</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.184</td>
<td style="text-align: center;">2.979</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">68.1</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.287</td>
<td style="text-align: center;">1.634</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">61.9</td>
</tr>
<tr>
<td style="text-align: center;">Code Llama</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.483</td>
<td style="text-align: center;">1.733</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.079</td>
<td style="text-align: center;">1.730</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: center;">Molinst (chemistry LLM)</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.271</td>
<td style="text-align: center;">1.691</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">52.4</td>
</tr>
<tr>
<td style="text-align: center;">ChemLLM (chemistry LLM)</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.946</td>
<td style="text-align: center;">1.797</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;">Our LlaSMol Series</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Galactica }}$</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">1.959</td>
<td style="text-align: center;">1.213</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Llama 2 }}$</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">2.791</td>
<td style="text-align: center;">1.338</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Code Llama }}$</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">2.959</td>
<td style="text-align: center;">1.203</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">69.9</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Mistral }}$</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">1.150</td>
<td style="text-align: center;">1.010</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">70.7</td>
</tr>
</tbody>
</table>
<p>Evaluation Metrics. We employ metrics commonly used in previous work (Schwaller et al., 2019; Zhong et al., 2022; Fang et al., 2023; Zhou et al., 2023; Chen et al., 2023), which include: (1) Exact Match (EM), indicating the proportion of predicted results that exactly match the gold standards. (2) Fingerprint Tanimoto Similarity (FTS), quantifying structural similarities between molecules using Tanimoto similarities of their Morgan fingerprints (Morgan, 1965). (3) METEOR score, a comprehensive text-based metric considering both exact matches and semantic similarity (Lavie \&amp; Agarwal, 2007) for the MC task. (4) Root Mean Square Error (RMSE), measuring the square root of the average squared differences between predicted and actual values for the PP-ESOL and PP-Lipo tasks (5) Accuracy (Acc), the ratio of correct predictions for the binary classification tasks (PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER). (6) Validity (Valid), the ratio of valid predictions following SMILES grammar and chemical valence rules for tasks with SMILES outputs (NC-I2S, MG, FS, and RS). For all the metrics except RMSE, higher values indicate better performance.</p>
<h1>4.3 Main Results</h1>
<p>Table 1 and 2 show the performance on SMoIInstruct. Key observations are as follows:
(1) Among all the LLMs, our LlaSMol models demonstrate the best performance, underscoring the effectiveness of the proposed SMoIInstruct dataset and fine-tuning. Specifically, compared to the base models (Galactica, Llama 2, Code Llama, and Mistral), LlaSMol models exhibit substantial performance improvements, which highlights the effectiveness of SMoIInstruct in enhancing the understanding of molecular representations and the taskrelated knowledge, and signifies the effective learning of chemistry-related tasks by LLMs. Furthermore, LlaSMol substantially outperforms GPT-4 on all the tasks and Claude 3 Opus on most tasks, despite their larger parameter size. LlaSMol also surpasses the two chemistry LLMs namely ChemLLM ${ }^{4}$, which is similarly trained on chemistry instruction data. and Molinst. Notably, LlaSMol $<em _Mistral="{Mistral" _text="\text">{\text {Llama 2 }}$, which uses the same base model and LoRA setting as Molinst, outperforms it even on the shared training tasks (MC, MG, FS, and RS). This finding highlights the benefits of our dataset.
(2) Our four LlaSMol models show substantial differences in their performance, emphasizing the considerable impact of base models on downstream tasks. Despite sharing identical training, inference settings, and comparable model sizes, LlaSMol $</em>$ consistently outperforms LlaSMol $}<em Llama="Llama" _Code="{Code" _text="\text">{\text {Llama 2 }}$ by a substantial margin, highlighting Mistral's potential on chemistry tasks. In addition, LlaSMol $</em>$ on most tasks, indicating a potential synergy between programming language knowledge}}$ exhibits better performance than LlaSMol $_{\text {Llama 2 }</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Results for molecule captioning (MC), molecule generation (MG), forward synthesis (FS), and retrosynthesis (RS). Metrics EM, FTS, and Valid are in percentage.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MC</th>
<th>MG</th>
<th></th>
<th></th>
<th>FS</th>
<th></th>
<th></th>
<th>RS</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>METEOR</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
</tr>
<tr>
<td>Task-Specific, Non-LLM Based Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SoTA</td>
<td>0.515</td>
<td>31.7</td>
<td>73.2</td>
<td>95.3</td>
<td>78.7</td>
<td>92.2</td>
<td>100.0</td>
<td>47.0</td>
<td>77.5</td>
<td>99.7</td>
</tr>
<tr>
<td>Existing LLMs Without Fine-Tuning on SMoIInstruct</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.188</td>
<td>6.4</td>
<td>42.6</td>
<td>81.4</td>
<td>1.6</td>
<td>40.5</td>
<td>87.0</td>
<td>0.0</td>
<td>33.4</td>
<td>42.6</td>
</tr>
<tr>
<td>Claude 3 Opus</td>
<td>0.219</td>
<td>12.3</td>
<td>57.6</td>
<td>92.6</td>
<td>3.7</td>
<td>45.7</td>
<td>97.0</td>
<td>1.1</td>
<td>46.2</td>
<td>94.8</td>
</tr>
<tr>
<td>Galactica</td>
<td>0.050</td>
<td>0.0</td>
<td>11.6</td>
<td>94.7</td>
<td>0.0</td>
<td>25.9</td>
<td>83.7</td>
<td>0.0</td>
<td>34.6</td>
<td>93.0</td>
</tr>
<tr>
<td>Llama 2</td>
<td>0.150</td>
<td>0.0</td>
<td>4.8</td>
<td>93.5</td>
<td>0.0</td>
<td>13.7</td>
<td>97.7</td>
<td>0.0</td>
<td>27.5</td>
<td>87.7</td>
</tr>
<tr>
<td>Code Llama</td>
<td>0.143</td>
<td>0.0</td>
<td>8.5</td>
<td>95.2</td>
<td>0.0</td>
<td>15.8</td>
<td>99.6</td>
<td>0.0</td>
<td>25.3</td>
<td>97.1</td>
</tr>
<tr>
<td>Mistral</td>
<td>0.193</td>
<td>0.0</td>
<td>9.0</td>
<td>35.9</td>
<td>0.0</td>
<td>19.9</td>
<td>95.8</td>
<td>0.0</td>
<td>24.2</td>
<td>98.0</td>
</tr>
<tr>
<td>Molinst (chemistry LLM)</td>
<td>0.124</td>
<td>6.0</td>
<td>43.6</td>
<td>84.8</td>
<td>2.1</td>
<td>31.7</td>
<td>99.8</td>
<td>5.7</td>
<td>48.0</td>
<td>97.8</td>
</tr>
<tr>
<td>ChemLLM (chemistry LLM)</td>
<td>0.050</td>
<td>0.9</td>
<td>14.3</td>
<td>4.3</td>
<td>0.0</td>
<td>1.6</td>
<td>38.5</td>
<td>0.0</td>
<td>2.9</td>
<td>10.9</td>
</tr>
<tr>
<td>Our LlaSMol Series</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LlaSMol $_{\text {Galactica }}$</td>
<td>0.394</td>
<td>7.7</td>
<td>52.2</td>
<td>99.6</td>
<td>53.1</td>
<td>79.9</td>
<td>99.7</td>
<td>25.7</td>
<td>67.0</td>
<td>99.9</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Llama 2 }}$</td>
<td>0.377</td>
<td>6.4</td>
<td>47.1</td>
<td>99.6</td>
<td>47.1</td>
<td>76.9</td>
<td>99.8</td>
<td>22.5</td>
<td>65.2</td>
<td>99.9</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Code Llama }}$</td>
<td>0.366</td>
<td>6.5</td>
<td>46.6</td>
<td>99.7</td>
<td>52.0</td>
<td>79.2</td>
<td>99.8</td>
<td>25.7</td>
<td>66.7</td>
<td>100.0</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Mistral }}$</td>
<td>0.452</td>
<td>19.2</td>
<td>61.7</td>
<td>99.7</td>
<td>63.3</td>
<td>84.9</td>
<td>99.8</td>
<td>32.9</td>
<td>70.4</td>
<td>100.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of ablation study on NC and PP tasks. Metrics EM, Valid, and Acc are in percentage. Orange cells represent better results than LlaSMol $_{\text {Mistral }}$ while blue cells represent worse results.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>NC</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>PP</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>I2F</td>
<td>I2S</td>
<td></td>
<td>S2F</td>
<td>S2I</td>
<td>ESOL</td>
<td>Lipo</td>
<td>BBBP</td>
<td>Clintox</td>
<td>HIV</td>
<td>SIDER</td>
</tr>
<tr>
<td></td>
<td>EM</td>
<td>EM</td>
<td>Valid</td>
<td>EM</td>
<td>EM</td>
<td>RMSE $_{1}$</td>
<td>RMSE $_{1}$</td>
<td>Acc</td>
<td>Acc</td>
<td>Acc</td>
<td>Acc</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Mistral }}$</td>
<td>87.9</td>
<td>70.1</td>
<td>99.6</td>
<td>93.2</td>
<td>29.0</td>
<td>1.150</td>
<td>1.010</td>
<td>74.6</td>
<td>93.1</td>
<td>96.7</td>
<td>70.7</td>
</tr>
<tr>
<td>w/o canonical</td>
<td>88.5</td>
<td>67.2</td>
<td>99.6</td>
<td>93.4</td>
<td>24.5</td>
<td>1.224</td>
<td>1.072</td>
<td>71.6</td>
<td>93.1</td>
<td>96.8</td>
<td>70.3</td>
</tr>
<tr>
<td>using SELFIES</td>
<td>86.9</td>
<td>47.7</td>
<td>100.0</td>
<td>94.7</td>
<td>19.7</td>
<td>1.456</td>
<td>1.106</td>
<td>69.5</td>
<td>91.7</td>
<td>96.5</td>
<td>64.4</td>
</tr>
<tr>
<td>train on Mol-Instructions</td>
<td>0.0</td>
<td>0.0</td>
<td>75.2</td>
<td>0.0</td>
<td>0.0</td>
<td>4.416</td>
<td>2.282</td>
<td>0.0</td>
<td>0.0</td>
<td>2.6</td>
<td>0.4</td>
</tr>
</tbody>
</table>
<p>in Code Llama and molecular representations. Furthermore, LlaSMol $<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$ outperforms LlaSMol $</em>$, and LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$ in most cases, suggesting the benefits of pretraining on chemistry-related documents.
(3) Although LlaSMol models do not outperform SoTA models, they demonstrate considerable potential for further improvements. Specifically, LlaSMol $</em>$ surpasses the SoTA models on PP-Clintox and PP-SIDER, but has yet to achieve the success on other tasks. However, LlaSMol has greatly narrowed the performance gap between LLMs and SoTA task-specific models, compared to previous efforts (Fang et al., 2023; Zhang et al., 2024). Remarkably, LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Mistral }}$ attains such performance with only a small proportion of its parameters fine-tuned (approximately $41.9 \mathrm{M}, 0.58 \%$ of its parameters). As shown in Appendix F.2, increasing the number of trainable parameters can substantially boost performance, suggesting that LlaSMol $</em>$ has immense potential to surpass task-specific models through more extensive fine-tuning and serve as a strong foundation model for chemistry applications.}</p>
<h1>4.4 Ablation Study</h1>
<p>To investigate the advantages of SMoIInstruct, we conduct an ablation study by comparing LlaSMol $<em _Mistral="{Mistral" _text="\text">{\text {Mistral }}$ with the following variants: (1) w/o canonical, which uses uncanonicalized SMILES, to examine the benefits of canonicalization. (2) using SELFIES, which uses SELFIES Krenn et al. (2019) instead of SMILES to explore their differences. (3) train on Mol-Instructions, which is trained on Mol-Instructions (Fang et al., 2023), to compare the performance improvements of our dataset against the previously proposed dataset.
The results in Table 3 and Table 4 lead to the following observations: (1) The "w/o canonical" model underperforms LlaSMol $</em>$ on most tasks, with a substantial performance drop on FS and RS. This suggests that canonicalizing SMILES can reduce learning difficulty and improve performance. As canonicalization can be easily performed using fixed algorithms}</p>
<p>Table 4: Results of ablation study on MC, MG, FS, and RS. Metrics EM, FTS, and Valid are in percentage. Orange represents better results than LlaSMol ${ }_{\text {Mistral }}$, while blue represents worse results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;">MG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol ${ }_{\text {Mistral }}$</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">w/o canonical</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;">using SELFIES</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;">train on Mol-Instructions</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">76.7</td>
</tr>
</tbody>
</table>
<p>before feeding into models, we recommend using canonical SMILES when training and applying LLMs for chemistry. (2) While using SELFIES slightly improves the validity of generated molecules, which aligns with the motivation behind SELFIES (Krenn et al., 2019), the validity of using SMILES is also sufficiently high. Moreover, using SELFIES results in worse performance on most tasks, possibly due to SELFIES being typically longer than SMILES, making it more difficult for the model to accurately understand and generate. Therefore, using SELFIES over SMILES may not be necessary, contrast to claims made in previous work (Krenn et al., 2019; Fang et al., 2023). (3) Despite using identical base models and training settings, the model trained on Mol-Instructions (Fang et al., 2023) performs much worse than LlaSMol ${ }_{\text {Mistral }}$ trained on SMolInstruct even on the shared tasks (MC, MG, FS, and RS). This demonstrates the superiority of our dataset. A detailed comparison with Mol-Instructions can be found in Appendix C.</p>
<p>To gain deeper insights into the models' performance and behavior, we conduct further analytical experiments: (1) To investigate the synergistic effects among different tasks, we evaluate models trained on a single task and models with certain tasks removed. The results demonstrate multiple-task training outperforms single-task training, indicating its benefits. However, each task generally does not heavily rely on the presence of other tasks, suggesting a degree of independence among them. (2) To investigate the influence of LoRA (Hu et al., 2022) settings, we vary the involved LoRA modules. We observe that adding LoRA modules (and trainable parameters) leads to a substantial boost in performance, indicates the models' great potential for further improvements if with larger-scale fine-tuning. Please refer to Appendix F for more details.</p>
<h1>5 Conclusion</h1>
<p>While LLMs have shown promise as versatile assistants, their performance on chemistryrelated tasks remains notably subpar. To address this issue, we introduces SMolInstruct, a large-scale, comprehensive, and high-quality instruction tuning dataset. It comprises 14 tasks highly relevant to real-world applications and contains over 3M rigorously curated samples. Using SMolInstruct, we develop LlaSMol, a series of LLMs for performing chemistry tasks. Our experiments demonstrate LlaSMol's superiority over existing LLMs, and highlight SMolInstruct's crucial role in boosting the performance. Further analytical experiments also provide significant insights towards developing LLMs for chemistry.
However, this work has the following limitations. First, the evaluations for the MC and MG tasks cannot accurately assess models' abilities to generate chemically correct descriptions and molecules. Since the definition of molecular descriptions remain ambiguous and the available data is limited, it is challenging to assess whether the generated descriptions or molecules are accurate and correct. Second, this work does not delve into the models' generalization capabilities beyond the trained tasks. While we recognize the importance of such capabilities, how to meaningfully test generalization abilities is nontrivial and needs careful design, which falls outside the purview of this work. Third, our models do not yet outperform SoTA task-specific models, possibly due to the small ratio of trainable parameters or suboptimal training procedures. Nevertheless, we propose a high-quality instruction tuning dataset, demonstrate its effectiveness, and gain deeper insights, which we hope can be valuable for future research. We will try to address the aforementioned limitations in our future work.</p>
<h1>Ethics Statement</h1>
<p>Despite our best efforts to maintain the high quality of the SMollnstruct dataset and the integrity of the LlaSMol models, we cannot guarantee that the dataset is free of inaccurate, incorrect, or harmful content, nor can we prevent the models from generating such content. Users should engage with our dataset and models at their own discretion and uphold the highest ethical standards in their use.</p>
<h2>Acknowledgement</h2>
<p>The authors would thank colleagues from the OSU NLP group and the OSU Ning Lab for constructive feedback. This research was supported in part by NSF IIS-2133650, NIH 1R01LM014385-01, and NSF CAREER #1942980, as well as Ohio Supercomputer Center (Ohio Supercomputer Center, 1987). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.</p>
<h2>References</h2>
<p>Mikhail Andronov, Varvara Voinarovska, Natalia Andronova, Michael Wand, Djork-Arné Clevert, and Jürgen Schmidhuber. Reagent prediction with a molecular transformer improves reaction data quality. Chemical Science, 14(12):3235-3246, 2023.</p>
<p>Anthropic. The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024.
Theodore L. Brown. Chemistry: the central science. Pearson, 14th edition edition, 2018.
Andrew R. Burns, Trevor C. Y. Kwok, Al Howard, Ed Houston, Karl Johanson, Anthony Chan, Sean R. Cutler, Peter McCourt, and Peter J. Roy. High-throughput screening of small molecules for bioactivity and target identification in Caenorhabditis elegans. Nature Protocols, 1:1906-1914, 2006.</p>
<p>He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. arXiv preprint arXiv:2311.16208, 2023.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2024.</p>
<p>Heng-Yi Chen, Michael Hsu, and Chan-Wang Jerry Lio. Micro but mighty - Micronutrients in the epigenetic regulation of adaptive immune responses. Immunological reviews, 305: $152-164,2022$.</p>
<p>Ziqi Chen, Martin Renqiang Min, Srinivasan Parthasarathy, and Xia Ning. A deep generative model for molecule optimization via one fragment modification. Nature machine intelligence, 3(12):1040-1049, 2021.</p>
<p>Ziqi Chen, Oluwatosin R Ayinde, James R Fuchs, Huan Sun, and Xia Ning. G2retro as a twostep graph generative models for retrosynthesis prediction. Communications Chemistry, 6 (1):102, 2023.</p>
<p>Connor W. Coley, William H. Green, and Klavs F. Jensen. Machine learning in computeraided synthesis planning. Accounts of Chemical Research, 51(5):1281-1289, 2018.</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In Proceedings of Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2023.</p>
<p>Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6):1273-1280, 2002.</p>
<p>Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 595-607, 2021.</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 375-413, 2022.</p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018, 2023.</p>
<p>Henri A. Favre and Warren H. Powell. Nomenclature of organic chemistry: IUPAC recommendations and preferred names 2013. Royal Society of Chemistry, 2014.</p>
<p>Kaitlyn M. Gayvert, Neel S. Madhukar, and Olivier Elemento. A data-driven approach to predicting successes and failures of clinical trials. Cell Chemical Biology, 23(10):1294-1301, 2016.</p>
<p>Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. In Proceedings of Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2023.</p>
<p>Shen Han, Haitao Fu, Yuyang Wu, Ganglan Zhao, Zhenyu Song, Feng Huang, Zhongfei Zhang, Shichao Liu, and Wen Zhang. Himgnn: a novel hierarchical molecular graph representation learning framework for property prediction. Briefings in Bioinformatics, 24 (5):bbad305, 2023.</p>
<p>Kan Hatakeyama-Sato, Naoki Yamane, Yasuhiko Igarashi, Yuta Nabae, and Teruaki Hayakawa. Prompt engineering of gpt-4 for chemical research: what can/cannot be done? Science and Technology of Advanced Materials: Methods, 3(1), 2023.</p>
<p>Jiazhen He, Huifang You, Emil Sandström, Eva Nittinger, Esben Jannik Bjerrum, Christian Tyrchan, Werngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemist's intuition using deep neural networks. Journal of cheminformatics, 13(1):1-17, 2021.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>National Cancer Institute. AIDS antiviral screen data, 2004. URL https://wiki.nci.nih. gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data. Accessed on 1 Fec 2024.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology, 3(1):015022, 2022.</p>
<p>Kevin Maik Jablonka, Philippe Schwaller, and Berend Smit. Is gpt-3 all you need for machine learning for chemistry? In AI for Accelerated Materials Design NeurIPS 2022 Workshop, 2022.</p>
<p>Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Leveraging large language models for predictive chemistry. ChemRxiv, 2023. doi: 10.26434/chemrxiv-2023-fw8n4-v3.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47:D1102-D1109, 2019.</p>
<p>Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.</p>
<p>Mario Krenn, Florian Häse, A Nigam, Pascal Friederich, and Alán Aspuru-Guzik. Selfies: a robust representation of semantically constrained graphs with an example application in chemistry. arXiv preprint arXiv:1905.13741, 1(3), 2019.</p>
<p>Michael Kuhn, Ivica Letunic, Lars Juhl Jensen, and Peer Bork. The sider database of drugs and side effects. Nucleic Acids Research, 44:D1075-D1079, 2015.</p>
<p>Alon Lavie and Abhaya Agarwal. Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT '07, pp. 228-231. Association for Computational Linguistics, 2007.</p>
<p>Elena Lenci and Andrea Trabocchi. Chapter 1 - Synthetic approaches toward small molecule libraries. In Small Molecule Drug Discovery, pp. 1-34. Elsevier, 2020.</p>
<p>Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, TatSeng Chua, and Qi Tian. Towards 3d molecule-text interpretation in language models. In Proceedings of International Conference on Learning Representations (ICLR), 2024.</p>
<p>Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. Drugchat: towards enabling chatgpt-like capabilities on drug molecule graphs. arXiv preprint arXiv:2309.03907, 2023.</p>
<p>Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. Chatgpt-powered conversational drug editing using retrieval and domain feedback. arXiv preprint arXiv:2305.18090, 2023a.</p>
<p>Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $15623-15638,2023 b$.</p>
<p>Daniel Lowe. Chemical reactions from us patents (1976-sep2016), 2017. URL https://doi . org/10.6084/m9.figshare.5104873.v1.</p>
<p>Ines Filipa Martins, Ana L. Teixeira, Luis Pinheiro, and Andre O. Falcao. A bayesian approach to in silico blood-brain barrier penetration modeling. Journal of Chemical Information and Modeling, 52(6):1686-1697, 2012.</p>
<p>Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442-451, 1975.</p>
<p>Monica P McNerney and Mark P Styczynski. Small molecule signaling, regulation, and potential applications in cellular therapeutics. Wiley Interdisciplinary Reviews: Systems Biology and Medicine, 10:e1405, 2018.</p>
<p>David L. Mobley and J. Peter Guthrie. Freesolv: a database of experimental and calculated hydration free energies, with input files. Journal of Computer-Aided Molecular Design, 28(7): $711-720,2014$.
H. L. Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Journal of Chemical Documentation, 5(2): $107-113,1965$.</p>
<p>Ohio Supercomputer Center. Ohio supercomputer center, 1987. URL http://osc.edu/ark: /19495/f5s1ph73.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Salwa K Poole and Colin F Poole. Separation methods for estimating octanol-water partition coefficients. Journal of Chromatography B, 797(1-2):3-19, 2003.</p>
<p>Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Stout: Smiles to iupac names using neural machine translation. Journal of Cheminformatics, 13(1):1-14, 2021.</p>
<p>RDKit. Rdkit: Open-source cheminformatics, 2023. URL https://doi.org/10.5281/zenodo. 8254217. Accessed on 27 Jan 2024.</p>
<p>Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.</p>
<p>Nadine Schneider, Roger A Sayle, and Gregory A Landrum. Get your atoms in order: An open-source implementation of a novel and robust molecular canonicalization algorithm. Journal of chemical information and modeling, 55(10):2111-2120, 2015.</p>
<p>Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.
T. W. Graham Solomons, Craig B. Fryhle, and Scott A. Snyder. Organic Chemistry, Integrated E-Text with E-Solutions Manual. Wiley, 13th edition, 2022.</p>
<p>Vignesh Ram Somnath, Charlotte Bunne, Connor Coley, Andreas Krause, and Regina Barzilay. Learning graph models for retrosynthesis prediction. Advances in Neural Information Processing Systems, 34:9405-9415, 2021.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature Medicine, 29(8):1930-1940, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of Conference on Advances in neural information processing systems (NeurIPS), volume 30, 2017.</p>
<p>Yuyang Wang, Zijie Li, and Amir Barati Farimani. Graph Neural Networks for Molecules. In Chen Qu and Hanchao Liu (eds.), Machine Learning in Molecular Sciences, pp. 21-66. Springer International Publishing, Cham, 2023. ISBN 978-3-031-37196-7. doi: 10.1007/ 978-3-031-37196-7_2. URL https://doi.org/10.1007/978-3-031-37196-7_2.</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28 (1):31-36, 1988.</p>
<p>Andrew D. White, Glen M. Hocky, Heta A. Gandhi, Mehrad Ansari, Sam Cox, Geemi P. Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, and Willmor J. Peña Ccoa. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery, 2(2):368-376, 2023.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of conference on empirical methods in natural language processing: system demonstrations (EMNLP), pp. 38-45, 2020.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530, 2018.</p>
<p>Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, Andrew Palmer, Volker Settels, Tommi Jaakkola, Klavs Jensen, and Regina Barzilay. Analyzing Learned Molecular Representations for Property Prediction. Journal of Chemical Information and Modeling, 59(8):3370-3388, August 2019. ISSN 1549-9596. doi: 10.1021/acs.jcim.9b00237. URL https://doi.org/10.1021/acs.jcim.9b00237. Publisher: American Chemical Society.</p>
<p>Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, and Xiangxiang Zeng. Drugassist: A large language model for molecule optimization. arXiv preprint arXiv:2401.10334, 2023.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, et al. Chemllm: A chemical large language model. arXiv preprint arXiv:2402.06852, 2024.</p>
<p>Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models for tables. arXiv preprint arXiv:2311.09206, 2023.</p>
<p>Yangtian Zhang, Huiyu Cai, Chence Shi, and Jian Tang. E3bind: An end-to-end equivariant network for protein-ligand docking. In The Eleventh International Conference on Learning Representations, 2022a.</p>
<p>Zaixi Zhang, Yaosen Min, Shuxin Zheng, and Qi Liu. Molecule generation for target protein binding with structural motifs. In The Eleventh International Conference on Learning Representations, 2022b.</p>
<p>Zipeng Zhong, Jie Song, Zunlei Feng, Tiantao Liu, Lingxiang Jia, Shaolun Yao, Min Wu, Tingjun Hou, and Mingli Song. Root-aligned smiles: a tight representation for chemical reaction prediction. Chemical Science, 13(31):9023-9034, 2022.</p>
<p>Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In International Conference on Learning Representations (ICLR), 2023.</p>
<h1>Table of Contents in Appendix</h1>
<p>A Preliminaries ..... 16
B Details of SMolInstruct ..... 16
B. 1 The statistics of SMolInstruct ..... 16
B. 2 Details of Dataset Construction ..... 17
C Comparison with Mol-Instructions ..... 18
D Details of Experimental Setup ..... 19
D. 1 LlaSMol Models ..... 20
D. 2 Compared LLMs ..... 20
D.2.1 GPT-4 ..... 20
D.2.2 Claude 3 Opus ..... 21
D.2.3 Galactica ..... 21
D.2.4 Llama 2, Code Llama, and Mistral ..... 21
D.2.5 Molinst ..... 22
D.2.6 ChemLLM ..... 22
D. 3 Task-Specific, Non-LLM Based SoTA Models ..... 22
D.3.1 STOUT for NC-I2S and NC-S2I ..... 22
D.3.2 RDKit for NC-S2F ..... 22
D.3.3 STOUT+RDKit for NC-I2F ..... 22
D.3.4 Uni-Mol for All The PP Tasks ..... 22
D.3.5 MolT5 for MC and MG ..... 23
D.3.6 RSMILES for FS and RS ..... 23
D.3.7 Molecular Transformer for FS and RS ..... 23
D. 4 Evaluation Metrics ..... 24
E Detailed Experimental Results ..... 24
E. 1 Name Conversion Tasks ..... 24
E. 2 Property Prediction ..... 27
E. 3 Molecule Description ..... 28
E. 4 Chemical Reaction ..... 28
E. 5 Other Common Findings ..... 28
F More Analytical Experiments ..... 30
F. 1 Task Synergy ..... 30
F. 2 Influence of LoRA Modules and Trainable Parameters ..... 31</p>
<h1>A Preliminaries</h1>
<p>Molecules form the basis of chemistry, which fundamentally determines the properties and behaviors of most substances. A molecule is a group of atoms held together by chemical bonds (Brown, 2018). In this paper, we focus on small molecules, which typically have no more than 100 atoms and a low molecular weight under 1,500 Daltons (Lenci \&amp; Trabocchi, 2020). Small molecules perform many important functions, such as signaling in cellular biology (McNerney \&amp; Styczynski, 2018), pest control in agriculture (Burns et al., 2006), micronutrients in nutrition (Chen et al., 2022), and drug therapy in medicine (Lenci \&amp; Trabocchi, 2020). Given the importance of small molecules, it is essential to integrate LLMs into the study of small molecules to further advance their design or development.
Molecules can be represented in multiple ways, such as SMILES strings, IUPAC names, and molecular formulas. SMILES strings use a sequence of symbols to encode the 2D structures of molecules (Weininger, 1988). A molecule can have multiple SMILES strings; a canonical SMILES for the molecule is unique and deterministic. For example, the canonical SMILES representation of glucose is " $\left.\mathrm{C}(\mathrm{C} 1 \mathrm{C}(\mathrm{C}(\mathrm{C}(\mathrm{O} 1) \mathrm{O}) \mathrm{O}) \mathrm{O}) \mathrm{O}\right) \mathrm{O}$ ". SELFIES (Krenn et al., 2019) is an alternative representation to SMILES that also uses a sequence of symbols to denote molecular structures. Its key advantage is robustness, as every SELFIES string is guaranteed to correspond to a valid molecule. The SELFIES representation corresponding to the above SMILES representation of glucose is "[C][Branch2][Ring1][Branch1][C][C][Branch1][S][C][Branch1][N][C][Branch1][Branch2] [C][Branch1][Ring2][O][Ring1][=Branch1][O][O][O][O][O]". Molecular formulas represent a molecule by enumerating the type and number of atoms in the molecule (Solomons et al., 2022). For example, the molecular formula for glucose is " $\mathrm{C}<em 12="12">{6} \mathrm{H}</em>$ ". IUPAC names are formal names based on natural language elements, which follow the systematic rules set by the International Union of Preferred and Applied Chemistry (IUPAC) (Favre \&amp; Powell, 2014). These names are derived from the structures and functional groups of molecules, and are intended to be human-readable. For example, the IUPAC name for glucose is "(3R,4S,5S,6R)-6-(hydroxymethyl)oxane-2,3,4,5-tetrol".
Molecules are one of the fundamental units of chemistry that participate in reactions (Brown, 2018). A reaction is a process which converts input molecules (reactants) into output molecules (products) through the breaking and forming of chemical bonds. Other molecules (reagents) may be present to enhance or facilitate the reaction.} \mathrm{O}_{6</p>
<h2>B Details of SMolInstruct</h2>
<p>In this section, we introduce the details of our proposed dataset SMolInstruct, including statistics and construction details.</p>
<h2>B. 1 The statistics of SMolInstruct</h2>
<p>Table 5 shows the statistics of SMolInstruct. It contains 4 types of altogether 14 tasks, which are selected to be meaningful and useful. There are about 3.3 M samples, and each of them is a distinct sample. In other words, there does not exist a pair of samples who share the same chemical information (i.e., the core input and output information, such as input molecules and output molecules), but with the same or different natural language templates (i.e., the task description in the query and the sentence templates in the response). When needed, one can easily create more instruction tuning samples by combining one piece of chemical information with multiple natural language templates. All in all, SMolInstruct can serve as a good benchmark for training and evaluating LLMs on various chemistry tasks.
To know more about the diversity of SMolInstruct, we conduct a statistics on the molecules. Altogether, there exist 1.6 M distinct molecules, and several important statistical values are shown in Figure 2. Specifically, Bertz complexity is a topological index that measures the complexity of molecules based on the number and types of bonds and atoms. Atom count shows the number of atoms in a molecule, and it represents the size of a molecule. Molecular weight is the sum of the atomic weights of the atoms in a molecule. And ring count shows</p>
<p>Table 5: The statistics of SMolInstruct. "Qry." and "Resp." are average lengths of queries and responses, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Task abbr.</th>
<th style="text-align: center;">#Train</th>
<th style="text-align: center;">#Valid</th>
<th style="text-align: center;">#Test</th>
<th style="text-align: center;">#All</th>
<th style="text-align: center;">Qry.</th>
<th style="text-align: center;">Resp.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name Conversion. Data Source: PubChem</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">IUPAC to Molecular Formula</td>
<td style="text-align: center;">NC-I2F</td>
<td style="text-align: center;">300,000</td>
<td style="text-align: center;">1,497</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,490</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">IUPAC to SMILES</td>
<td style="text-align: center;">NC-I2S</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: center;">SMILES to Molecular Formula</td>
<td style="text-align: center;">NC-S2F</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: center;">SMILES to IUPAC</td>
<td style="text-align: center;">NC-S2I</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;">Property Prediction. Data Source: MoleculeNet</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ESOL</td>
<td style="text-align: center;">PP-ESOL</td>
<td style="text-align: center;">888</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">1,111</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: center;">Lipo</td>
<td style="text-align: center;">PP-Lipo</td>
<td style="text-align: center;">3,360</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">4,200</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">BBBP</td>
<td style="text-align: center;">PP-BBBP</td>
<td style="text-align: center;">1,569</td>
<td style="text-align: center;">196</td>
<td style="text-align: center;">197</td>
<td style="text-align: center;">1,962</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">ClinTox</td>
<td style="text-align: center;">PP-ClinTox</td>
<td style="text-align: center;">1,144</td>
<td style="text-align: center;">143</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">1,431</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">HIV</td>
<td style="text-align: center;">PP-HIV</td>
<td style="text-align: center;">32,864</td>
<td style="text-align: center;">4,104</td>
<td style="text-align: center;">4,107</td>
<td style="text-align: center;">41,075</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">SIDER</td>
<td style="text-align: center;">PP-SIDER</td>
<td style="text-align: center;">22,820</td>
<td style="text-align: center;">2,860</td>
<td style="text-align: center;">2,860</td>
<td style="text-align: center;">28,540</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">Molecule Description. Data Source: Mol-Instructions, ChEBI-20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Molecule Captioning</td>
<td style="text-align: center;">MC</td>
<td style="text-align: center;">56,498</td>
<td style="text-align: center;">1,269</td>
<td style="text-align: center;">2,538</td>
<td style="text-align: center;">60,305</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">102</td>
</tr>
<tr>
<td style="text-align: center;">Molecule Generation</td>
<td style="text-align: center;">MG</td>
<td style="text-align: center;">56,498</td>
<td style="text-align: center;">1,269</td>
<td style="text-align: center;">2,493</td>
<td style="text-align: center;">60,260</td>
<td style="text-align: center;">117</td>
<td style="text-align: center;">75</td>
</tr>
<tr>
<td style="text-align: center;">Chemical Reaction. Data Source: USPTO-full</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Forward Synthesis</td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">971,809</td>
<td style="text-align: center;">2,049</td>
<td style="text-align: center;">4,062</td>
<td style="text-align: center;">977,920</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: center;">Retrosynthesis</td>
<td style="text-align: center;">RS</td>
<td style="text-align: center;">941,735</td>
<td style="text-align: center;">2,092</td>
<td style="text-align: center;">4,156</td>
<td style="text-align: center;">947,983</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3,288,855</td>
<td style="text-align: center;">20,498</td>
<td style="text-align: center;">33,061</td>
<td style="text-align: center;">3,342,414</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">55</td>
</tr>
</tbody>
</table>
<p>the number of rings in the molecular structures. As we can see, the values varies much, showing a extensive coverage in terms of complexity, size, and structure. Notably, when compared to Mol-Instructions (Fang et al., 2023), molecules in SMolInstruct show a higher complexity and diversity, which indicates that SMolInstruct is more comprehensive and complicated than Mol-Instructions. The scale, complexity, and diversity of SMolInstruct makes it well-suited for learning chemistry LLMs.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The statistics of molecules in SMolInstruct, with the long tail parts removed for a clear presentation.</p>
<h1>B. 2 Details of Dataset Construction</h1>
<p>Dataset construction involves four key steps (Section 3.2): data collection, quality control, data splitting, and instruction creation. This section provides task-specific details, omitting the common steps of canonicalizing SMILES/SELFIES and verbalizing information into query and response sentences, which have been introduced in Section 3.2.
Name Conversion (NC). The raw data for name conversion is collected from PubChem (Kim et al., 2019). Approximately 300k molecule/compound entries are randomly selected from the database, and their SMILES, IUPAC names, and molecular formulas are extracted. Entries with incomplete or missing information in these three domains are discarded. Finally, the SMILES, IUPAC names, molecular formulas are paired to create samples for the four name conversion tasks.
Property Prediction (PP). The raw data for property prediction is sourced from MoleculeNet (Wu et al., 2018). Out of its 16 core datasets ${ }^{5}$, we select 6 that are only related to small</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>molecules and are useful especially in drug discovery. Answers for regression tasks (e.g., ESOL and Lipo) are formulated as strings of numbers, and answers for binary classification tasks (e.g., BBBP and SIDER) are formulated as "Yes" or "No".
Molecule Captioning (MC) and Molecule Generation (MG). The raw data is collected from ChEBI-20 (Edwards et al., 2021; 2022) and Mol-Instructions (Fang et al., 2023). Despite the large number of samples in Mol-Instructions, many are found to be of low quality. For example, numerous molecular descriptions end with the ambiguous phrase "with data available", while others are overly general, making it difficult to generate a specific molecule based on the description. To ensure data quality, regular expressions and heuristic rules are employed to filter out low-quality samples.
Forward Synthesis (FS). USPTO-full (Lowe, 2017), one of the most comprehensive chemical reaction datasets, serve as the data source. The following processing steps are performed to clean the data: (1) Reactants and reagents are combined as input, and the product(s) serve as output, consistent with other datasets such as Mol-Instructions (Fang et al., 2023). (2) Duplicate chemicals in both input and output are removed to avoid redundancy. (3) If a chemical appears in both input and output, it is removed from the output to maintain data integrity. (4) Products in the output containing fewer than 5 molecules are considered non-main products and excluded. (5) If the above steps result in an empty output, the entire sample is discarded.
Retrosynthesis (RS). The data is also sourced from USPTO-full (Lowe, 2017), with the product as input and the reactants (excluding reagents) as output. During data exploration, we observe instances where reactants are mislabeled as reagents and vice versa. To address this issue, we compare the atom mapping numbers of the reactants and reagents with the products and relabel them accordingly. Subsequently, we apply the following processing steps: (1) Duplicate chemicals in both input and output are removed. (2) If a chemical appears in both input and output, it is removed from the input. (3) Products in the input containing fewer than 5 molecules are excluded. (4) In cases where multiple products exist in the input, the reaction is split into multiple samples, with each product serving as the input once. (5) If the above steps result in an empty input, the entire sample is discarded.
For all the tasks, samples containing invalid SMILES strings (i.e., those that cannot be parsed into a valid molecule with RDKit(RDKit, 2023)) are discarded, and duplicate samples are removed to avoid redundancy. Finally, since some molecules contain multiple components and they are separated by dots in SMILES, which is the same delimiter used to separate different reactants/reagents/products in FS and RS, the dots in SMILES strings for NC, PP, MC, and MG are replaced with semicolons to differentiate between these two usages.</p>
<h1>C Comparison with Mol-Instructions</h1>
<p>In this section, we present a comprehensive comparison between our work and MolInstructions (Fang et al., 2023).
We begin by comparing our dataset, SMolInstruct, with the Mol-Instructions dataset. While Mol-Instructions covers a broader scope (including molecule-oriented, protein-oriented, and biomolecular text instructions), SMolInstruct focuses exclusively on small molecules, providing a deeper and more comprehensive exploration of this domain.
If focusing on the molecule-related data, as shown in Table 6, SMolInstruct is a larger, more comprehensive, and higher-quality dataset. It incorporates more tasks, samples, and molecular representations, and involves more careful curation. Both datasets share the tasks of MC, MG, FS, and RS. Although SMolInstruct has fewer samples for MC and MG, the included samples are of higher quality (see Appendix B.2). Furthermore, SMolInstruct contains substantially more samples for FS and RS, which have been carefully cleaned and processed. Additionally, SMolInstruct incorporates four NC tasks to facilitate the understanding of various molecular representations. Unlike Mol-Instructions, we do not include the reagent prediction task mainly due to the lack of sufficient high-quality data (Andronov et al., 2023) and the limited practicality of this task in real world applications.</p>
<p>Table 6: Comparison between Mol-Instructions (the molecule-oriented part) (Fang et al., 2023) and our SMolInstruct.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mol-Instructions</th>
<th style="text-align: center;">SMolInstruct (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">name conversion property prediction</td>
<td style="text-align: center;">1.2 M samples <br> 78.3 k samples on 6 useful properties. <br> ergy. <br> 298.3 k samples. <br> 298.3 k samples. <br> 298.3 k samples. <br> 60.3 k samples.</td>
</tr>
<tr>
<td style="text-align: center;">Tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { molecule } \quad \text { genera- } \ &amp; \text { tion } \end{aligned}$</td>
<td style="text-align: center;">60.3k samples.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">forward synthesis</td>
<td style="text-align: center;">60.3k samples.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { retrosynthesis } \ &amp; \text { reagent prediction } \end{aligned}$</td>
<td style="text-align: center;">977.9k samples. <br> 948.0k samples. <br> Not included due to its insufficient data and limited practicality.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.2 M</td>
<td style="text-align: center;">3.3 M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.3 M</td>
<td style="text-align: center;">3.3 M</td>
</tr>
<tr>
<td style="text-align: center;">#Samples</td>
<td style="text-align: center;">SELFIES.</td>
<td style="text-align: center;">Supports SMILES (default) and SELFIES, also involves IUPAC names and molecular formula in the NC tasks.</td>
</tr>
<tr>
<td style="text-align: center;">Molecular representations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Carefully split into train/validation/test set, removing potential data leakage (see Section 3.2)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes, all the SMILES/SELFIES representations are canonicalized, providing a standardized data format.</td>
</tr>
<tr>
<td style="text-align: center;">Data splitting</td>
<td style="text-align: center;">Provides test set, while train/validation sets are not explicitly split.</td>
<td style="text-align: center;">Higher (see Appendix B.1).</td>
</tr>
</tbody>
</table>
<p>Beyond the dataset, our work makes contributions to the exploration of chemistry LLMs. While Fang et al. (2023) primarily focus on the dataset itself and provide a preliminary exploration of the models, we conduct comprehensive experiments to investigate the abilities of LLMs in the chemistry domain. Our experiments in Section 4 demonstrates that our LlaSMol models achieves superior performance compared to the LLMs trained on MolInstructions and the strongest LLMs such as GPT-4 and Claude 3 Opus, greatly diminishing the gap between LLMs and SoTA task-specific models. Moreover, we provides valuable insights about multi-task training, LoRA (Hu et al., 2022) settings, and other aspects that could be helpful for future research in this field.</p>
<h1>D Details of Experimental Setup</h1>
<p>In this section, we introduce the details of our experimental setups, including the training and inference details of our LlaSMol models and the compared models. We also give detailed explanations of the metrics used in Section 4.3, as well extra metrics that we will use in Appendix E.</p>
<h1>D. 1 LlaSMol Models</h1>
<p>The base models used for developing LlaSMol are Galactica ${ }^{6}$ (Taylor et al., 2022), Llama $2^{7}$ (Touvron et al., 2023b), Code Llama ${ }^{8}$ (Roziere et al., 2023) and Mistral ${ }^{9}$ (Jiang et al., 2023). We conduct instruction tuning on our SMolInstruct, and the resulting models are called named as LlaSMol ${ }<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$, LlaSMol $</em>$, LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$, and LlaSMol $</em>$, respectively. Expect for being based on different base models, their training and evaluation configurations are identical, as described as follows.
We used LoRA (Hu et al., 2022) during training, which is applied to all linear layers in the self-attention and FFN modules with lora.r and lora_alpha set to 16 . With the 8 -bit AdamW optimizer, a learning rate of $1 \mathrm{e}-4$, and a cosine scheduler, we train each model for three epochs. The input length is set to 512 , and sequences longer than 512 are truncated.
During inference, we adopt beam search as the generation strategy for simplicity. Due to the need of evaluations on the top- $k$ predicted answers (as in Appendix E, where $k$ varies for different tasks, we generate different numbers of sequences for different tasks by setting the num_return_sequences argument in the Huggingface Transformers library (Wolf et al., 2020). Specifically, it is set to 5 for NC-I2S, NC-S2I, FS, and MG; 3 for NC-I2F and NC-S2F; 1 for all the PP tasks; and 10 for RS. The beam size is set to num_return_sequences +3 for all the tasks. The maximum number of new generated tokens is set to 1024 .}</p>
<h2>D. 2 Compared LLMs</h2>
<p>We introduce each of the compared LLMs in details, including their training (if applicable) and inference process.</p>
<h2>D.2.1 GPT-4</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">General <br> Template</th>
<th style="text-align: center;">You are an expert chemist. Given the SMILES representation of reactants and reagents, your task is to predict the potential product using your chemical reaction knowledge.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task-Specific <br> Template</td>
<td style="text-align: center;">The input contains both reactants and reagents, and different reactants and reagents are separated by ' $\$$. Your reply should contain only the SMILES representation of the predicted product and no other text. Your reply must be valid and chemically reasonable.</td>
</tr>
<tr>
<td style="text-align: center;">ICL</td>
<td style="text-align: center;">Reactants and reagents SMILES: C1CCOC1.CCN(CC)CC.CS( $=0)(=0) \mathrm{Cl} . \mathrm{CS}(\mathrm{C})=0$. <br> N[C@@H]1CC2=CC=C(CN3C=C(CO)C(C(F)(F)F)=N3)C=C2C1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Product SMILES: CS( $=0)(=0) \mathrm{N}[\mathrm{C@@H}] 1 \mathrm{CC} 2=\mathrm{CC}=\mathrm{C}(\mathrm{CN} 3 \mathrm{C}=\mathrm{C}(\mathrm{CO}) \mathrm{C}(\mathrm{C}(\mathrm{F})(\mathrm{F}) \mathrm{F})=\mathrm{N} 3) \mathrm{C}=$ C2C1</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Reactants and reagents SMILES: CCN.CN1C=CC=C1C=O <br> Product SMILES:</td>
</tr>
</tbody>
</table>
<p>Figure 3: An example of query template for GPT-4.
GPT-4 (OpenAI, 2023) is one of the SoTA LLMs. We use the model versioned as gpt-4-0613 and evaluate it on 500 samples from SMolInstruct test set via OpenAI's API. Since GPT-4 is not fine-tuned on our dataset and thus is not familiar with the flexible queries, to ensure it generates answers in an expected format, we follow the prompt format proposed in (Guo et al., 2023) and create a query template for each of the tasks. The template for FS is shown in Figure 3. It contains 4 parts: (1) General template describes the task in a general way. (2) Task-specific template describes the detailed content requirements and format</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>requirements for the specific task. (3) ICL contains the in-context learning examples. It provides examples in the format of <input.title>: <input.content>\n <output.title>: <output.content>\n, where <input.title> and <output.title> serve as straightforward prompts to the input and output content. This design make the queried task more clear. (4) Question has the same format as ICL, with <output.content> being empty for the model to generate.
We conduct both $s$-shot evaluations, where $s=0,1,3,5$ is the number of provided ICL examples. For 0 -shot evaluation, the ICL part in the template is removed from the queries. In $k$-shot evaluation, for each sample,the ICL examples are randomly selected from the training set. The results of these settings are shown in Appendix E, which reals that these settings' performance is not consistent across all the tasks. Since 0 -shot shows the best performance on most tasks, we report its results in Section 4.3.
In the evaluations, we use the default generation strategy set in the API. To generate the same number of results for each sample (as described in Appendix D.1), we set the argument n in the API, which controls the number of output sequences.
GPT-4 can always follow the formatted instructions introduced above, so we do not bother to extract the answers from its outputs, but directly use its outputs as the predicted answers.</p>
<h1>D.2.2 Claude 3 Opus</h1>
<p>Claude 3 Opus (Anthropic, 2024) is a newly proposed SoTA LLM to date. Similarly to GPT-4, we evaluate Claude 3 Opus on 500 samples from SMolInstruct test set via Anthropic's API, and the generation strategy is the default one. The used prompt format is identical to the one used for GPT-4 (Appendix D.2.1. For each sample, we generate one response. Since Claude 3 Opus can always follow the formatted instructions, we do not bother to extract the answers from its outputs, but directly use its outputs as the predicted answers.</p>
<h2>D.2.3 Galactica</h2>
<p>Galactica (Taylor et al., 2022) is a LLM without instruction tuning. To evaluate it on SMolInstruct, we follow the instructions in the paper (Taylor et al., 2022) and the repository ${ }^{10}$ to create the queries for each task. We use zero-shot setting, as its official instruction does not suggest using few-shot setting. The generation configuration is set identical to that of our LlaSMol models (Appendix D.1).
Galatica's outputs may contain extra text other than the expected answers. Therefore, with heuristic rules and regular expression matching, we implement a program to extract the answers from the outputs of the models. Since the extraction cannot possibly cover all the possible output formats, some answers might not be correctly extracted, which might lead to validities lower than the actual value.</p>
<h2>D.2.4 Llama 2, Code Llama, and Mistral</h2>
<p>For our base models (Llama 2, Code Llama, and mistral), since they are not trained on SMolInstruct and have not seen the diverse queries in the dataset, we use the same query templates as those used for GPT-4 (Appendix D.2.1). We use the one-shot setting for them, as it would improve models' abiltity to follow the instructions and generate answers in a more formated way. In addition, the generation configuration (including beam size, output sequence numbers, etc) is set identical to that of our LlaSMol models (Appendix D.1).
Although we try our best to make the output format as clear as possible in the queries, these three models still cannot follow the instructions and their outputs are in various formats. By heuristic rules and regular expression matching, we implement a program to extract the answers from the outputs of each of the models. Since the extraction cannot possibly cover all the possible output formats, some answers might not be correctly extracted, which might lead to validities lower than the actual value.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ https://github.com/paperswithcode/galai&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
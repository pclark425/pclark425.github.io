<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1507 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1507</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1507</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-253107789</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.14162v1.pdf" target="_blank">Commonsense Knowledge from Scene Graphs for Textual Environments</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games are becoming commonly used in reinforcement learning as real-world simulation environments. They are usually imperfect information games, and their interactions are only in the textual modality. To challenge these games, it is effective to complement the missing information by providing knowledge outside the game, such as human common sense. However, such knowledge has only been available from textual information in previous works. In this paper, we investigate the advantage of employing commonsense reasoning obtained from visual datasets such as scene graph datasets. In general, images convey more comprehensive information compared with text for humans. This property enables to extract commonsense relationship knowledge more useful for acting effectively in a game. We compare the statistics of spatial relationships available in Visual Genome (a scene graph dataset) and ConceptNet (a text-based knowledge) to analyze the effectiveness of introducing scene graph datasets. We also conducted experiments on a text-based game task that requires commonsense reasoning. Our experimental results demonstrated that our proposed methods have higher and competitive performance than existing state-of-the-art methods.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1507.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1507.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptNet+SG (curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential ConceptNet then Scene-Graph (Visual Genome) Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential curriculum where an agent is first trained using textual commonsense from ConceptNet and then further trained using visual scene-graph commonsense from Visual Genome (VG) to complement abstract with spatial/concrete knowledge; applied to TextWorld Commonsense (TWC) household tidying tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TWC agent (ConceptNet + SG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An extension of the TextWorld Commonsense (TWC) baseline agent: encoders for actions, observations and context; maintains a dynamic commonsense subgraph extracted from external knowledge; encodes graphs with node embeddings (GloVe) and a Graph Attention Network (GAT); integrates observation and graph via co-attention; trained with Advantage Actor-Critic. The curriculum variant sequentially replaces/augments the external knowledge source (ConceptNet then VG).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A text-only, partially-observable interactive game (TextWorld) where the agent must tidy a house by placing objects into their commonsense locations; interactions are natural-language text observations and textual actions; tasks have three difficulty levels (easy, medium, hard) and two test splits: IN (entities seen during training) and OUT (unseen entities).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures (household tidying / object placement)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Examples include: 'take the dirty fork from the table', 'open the fridge', 'put the dirty fork in the dishwasher' — i.e., pick-up, move, and place object-to-location sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Procedures decompose into subtasks (locate object, pick up object, navigate to target room/location, place object); complexity increases with number of objects and rooms (easy: single-object placement; medium/hard: multiple objects/rooms and different pick-up/place locations).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>sequential external-knowledge curriculum (ConceptNet -> Visual Genome)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Two-stage training: first train the TWC agent for 100 episodes using ConceptNet (textual, abstract commonsense) as the external commonsense graph; then continue training the same agent for 100 episodes using Visual Genome (scene-graph triplets with rich spatial relations) to provide concrete spatial commonsense. The intention is to provide broad/abstract commonsense first and then specialize with spatially-detailed visual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>knowledge-modality / specificity progression (from abstract/textual commonsense to concrete/visual spatial commonsense)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Curriculum applied across TWC game difficulties ranging from easy (single-object single-room tasks) to medium and hard (multi-object, multi-room, multi-step object relocation); curriculum phases themselves are not defined by increasing step-count but by knowledge source.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Qualitative: ConceptNet+SG achieved the strongest generalization (OUT) performance for easy and medium levels and was second-best on IN (behind ConceptNet+Manual). Reported experimental setup: ConceptNet phase 100 episodes followed by VG phase 100 episodes; results are averages over five runs. Specific numeric note reported in the paper: on easy-level games ConceptNet+SG's score increased by 0.14 from IN to OUT (normalized score units). Metrics used: normalized score (score / maximum possible) and number of steps; ConceptNet+SG shows improved normalized scores and robust OUT generalization compared to ConceptNet-only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative comparison: ConceptNet-only (100 episodes) is less robust on OUT than ConceptNet+SG; Scene-Graph-only (VG) is efficient and robust to unseen entities (particularly low steps in OUT for medium-level), and ConceptNet+Manual (manually crafted subgraph) shows highest IN performance but overfits and performs worse on OUT. The paper reports that adding the VG phase improves performance relative to ConceptNet-only (training curves show improvement after 100 episodes when VG is introduced). Exact numeric breakdowns (per-level scores/steps) are in Table 4 of the paper but are not reproduced verbatim in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Alternatives evaluated in the paper: (1) Scene-Graph only (VG): strong efficiency (fewer steps) and robustness on OUT, fast convergence, sometimes better than ConceptNet+Manual on IN for easy tasks; (2) ConceptNet only: baseline textual commonsense training (100 episodes) with weaker generalization to OUT; (3) ConceptNet + Manual: manually-crafted minimal ConceptNet subgraph yields highest IN performance but overfits and performs poorly on OUT. ConceptNet+SG strikes a balance: wider coverage and best OUT generalization for easy/medium.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Reported: ConceptNet+SG demonstrates improved transfer/generalization to OUT (unseen entities), achieving best OUT performance for easy and medium difficulties; Scene-Graph-only is also robust to unseen objects (smallest steps in OUT for medium). The paper frames these as improved generalization to novel object- location pairs relative to ConceptNet-only and ConceptNet+Manual.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sequential curriculum from abstract textual commonsense to concrete visual scene-graph commonsense (ConceptNet -> VG) improves agent robustness to unseen entities and overall performance on TWC tasks (especially OUT generalization), compared to training with either source alone. Trade-offs noted: Scene-Graph-only provides high search efficiency (fewer steps) and robustness, but ConceptNet+SG provides broader coverage (better generalization) at some cost in search efficiency due to larger graph size. ConceptNet+Manual achieves high IN performance but overfits and generalizes poorly. The addition of VG after ConceptNet yields a clear improvement in training curves after the VG phase.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_context</strong></td>
                            <td>Paper experimental setup: ConceptNet+SG trained for 100 episodes with ConceptNet then 100 episodes with Visual Genome; other agents trained 100 episodes each; all runs averaged over five seeds; GloVe used for graph embedding in all agents for fair comparison.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1507.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1507.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum learning (Bengio et al. 2009)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Learning (Bengio, Louradour, Collobert, Weston 2009)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learning paradigm that proposes training models by presenting examples (or tasks) in a meaningful order — typically from easier to harder — to improve convergence speed and final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>curriculum learning (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Cited as inspiration: ordering training by progressive difficulty or relevance. In this paper the idea is instantiated as ordering by external-knowledge modality (textual then visual) rather than by explicit task-step difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task/task-knowledge difficulty progression (general principle: easy-to-hard ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as motivating prior; the paper adapts the curriculum idea to sequence the type of external commonsense knowledge (abstract textual first, concrete visual second) to help agents learn balanced commonsense representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning <em>(Rating: 1)</em></li>
                <li>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines <em>(Rating: 1)</em></li>
                <li>Eye of the Beholder: Improved Relation Generalization for Text-based Reinforcement Learning Agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1507",
    "paper_id": "paper-253107789",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "ConceptNet+SG (curriculum)",
            "name_full": "Sequential ConceptNet then Scene-Graph (Visual Genome) Curriculum",
            "brief_description": "A sequential curriculum where an agent is first trained using textual commonsense from ConceptNet and then further trained using visual scene-graph commonsense from Visual Genome (VG) to complement abstract with spatial/concrete knowledge; applied to TextWorld Commonsense (TWC) household tidying tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TWC agent (ConceptNet + SG)",
            "agent_description": "An extension of the TextWorld Commonsense (TWC) baseline agent: encoders for actions, observations and context; maintains a dynamic commonsense subgraph extracted from external knowledge; encodes graphs with node embeddings (GloVe) and a Graph Attention Network (GAT); integrates observation and graph via co-attention; trained with Advantage Actor-Critic. The curriculum variant sequentially replaces/augments the external knowledge source (ConceptNet then VG).",
            "agent_size": null,
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "A text-only, partially-observable interactive game (TextWorld) where the agent must tidy a house by placing objects into their commonsense locations; interactions are natural-language text observations and textual actions; tasks have three difficulty levels (easy, medium, hard) and two test splits: IN (entities seen during training) and OUT (unseen entities).",
            "procedure_type": "commonsense procedures (household tidying / object placement)",
            "procedure_examples": "Examples include: 'take the dirty fork from the table', 'open the fridge', 'put the dirty fork in the dishwasher' — i.e., pick-up, move, and place object-to-location sequences.",
            "compositional_structure": "Procedures decompose into subtasks (locate object, pick up object, navigate to target room/location, place object); complexity increases with number of objects and rooms (easy: single-object placement; medium/hard: multiple objects/rooms and different pick-up/place locations).",
            "uses_curriculum": true,
            "curriculum_name": "sequential external-knowledge curriculum (ConceptNet -&gt; Visual Genome)",
            "curriculum_description": "Two-stage training: first train the TWC agent for 100 episodes using ConceptNet (textual, abstract commonsense) as the external commonsense graph; then continue training the same agent for 100 episodes using Visual Genome (scene-graph triplets with rich spatial relations) to provide concrete spatial commonsense. The intention is to provide broad/abstract commonsense first and then specialize with spatially-detailed visual knowledge.",
            "curriculum_ordering_principle": "knowledge-modality / specificity progression (from abstract/textual commonsense to concrete/visual spatial commonsense)",
            "task_complexity_range": "Curriculum applied across TWC game difficulties ranging from easy (single-object single-room tasks) to medium and hard (multi-object, multi-room, multi-step object relocation); curriculum phases themselves are not defined by increasing step-count but by knowledge source.",
            "performance_with_curriculum": "Qualitative: ConceptNet+SG achieved the strongest generalization (OUT) performance for easy and medium levels and was second-best on IN (behind ConceptNet+Manual). Reported experimental setup: ConceptNet phase 100 episodes followed by VG phase 100 episodes; results are averages over five runs. Specific numeric note reported in the paper: on easy-level games ConceptNet+SG's score increased by 0.14 from IN to OUT (normalized score units). Metrics used: normalized score (score / maximum possible) and number of steps; ConceptNet+SG shows improved normalized scores and robust OUT generalization compared to ConceptNet-only.",
            "performance_without_curriculum": "Qualitative comparison: ConceptNet-only (100 episodes) is less robust on OUT than ConceptNet+SG; Scene-Graph-only (VG) is efficient and robust to unseen entities (particularly low steps in OUT for medium-level), and ConceptNet+Manual (manually crafted subgraph) shows highest IN performance but overfits and performs worse on OUT. The paper reports that adding the VG phase improves performance relative to ConceptNet-only (training curves show improvement after 100 episodes when VG is introduced). Exact numeric breakdowns (per-level scores/steps) are in Table 4 of the paper but are not reproduced verbatim in the provided text.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Alternatives evaluated in the paper: (1) Scene-Graph only (VG): strong efficiency (fewer steps) and robustness on OUT, fast convergence, sometimes better than ConceptNet+Manual on IN for easy tasks; (2) ConceptNet only: baseline textual commonsense training (100 episodes) with weaker generalization to OUT; (3) ConceptNet + Manual: manually-crafted minimal ConceptNet subgraph yields highest IN performance but overfits and performs poorly on OUT. ConceptNet+SG strikes a balance: wider coverage and best OUT generalization for easy/medium.",
            "transfer_generalization": "Reported: ConceptNet+SG demonstrates improved transfer/generalization to OUT (unseen entities), achieving best OUT performance for easy and medium difficulties; Scene-Graph-only is also robust to unseen objects (smallest steps in OUT for medium). The paper frames these as improved generalization to novel object- location pairs relative to ConceptNet-only and ConceptNet+Manual.",
            "key_findings": "Sequential curriculum from abstract textual commonsense to concrete visual scene-graph commonsense (ConceptNet -&gt; VG) improves agent robustness to unseen entities and overall performance on TWC tasks (especially OUT generalization), compared to training with either source alone. Trade-offs noted: Scene-Graph-only provides high search efficiency (fewer steps) and robustness, but ConceptNet+SG provides broader coverage (better generalization) at some cost in search efficiency due to larger graph size. ConceptNet+Manual achieves high IN performance but overfits and generalizes poorly. The addition of VG after ConceptNet yields a clear improvement in training curves after the VG phase.",
            "citation_context": "Paper experimental setup: ConceptNet+SG trained for 100 episodes with ConceptNet then 100 episodes with Visual Genome; other agents trained 100 episodes each; all runs averaged over five seeds; GloVe used for graph embedding in all agents for fair comparison.",
            "uuid": "e1507.0"
        },
        {
            "name_short": "Curriculum learning (Bengio et al. 2009)",
            "name_full": "Curriculum Learning (Bengio, Louradour, Collobert, Weston 2009)",
            "brief_description": "A learning paradigm that proposes training models by presenting examples (or tasks) in a meaningful order — typically from easier to harder — to improve convergence speed and final performance.",
            "citation_title": "Curriculum learning",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": null,
            "agent_size": null,
            "environment_name": null,
            "environment_description": null,
            "procedure_type": null,
            "procedure_examples": null,
            "compositional_structure": null,
            "uses_curriculum": null,
            "curriculum_name": "curriculum learning (general concept)",
            "curriculum_description": "Cited as inspiration: ordering training by progressive difficulty or relevance. In this paper the idea is instantiated as ordering by external-knowledge modality (textual then visual) rather than by explicit task-step difficulty.",
            "curriculum_ordering_principle": "task/task-knowledge difficulty progression (general principle: easy-to-hard ordering)",
            "task_complexity_range": null,
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": null,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "Used as motivating prior; the paper adapts the curriculum idea to sequence the type of external commonsense knowledge (abstract textual first, concrete visual second) to help agents learn balanced commonsense representations.",
            "uuid": "e1507.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curriculum learning",
            "rating": 2,
            "sanitized_title": "curriculum_learning"
        },
        {
            "paper_title": "VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning",
            "rating": 1,
            "sanitized_title": "visualhints_a_visuallingual_environment_for_multimodal_reinforcement_learning"
        },
        {
            "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
            "rating": 1,
            "sanitized_title": "textbased_rl_agents_with_commonsense_knowledge_new_challenges_environments_and_baselines"
        },
        {
            "paper_title": "Eye of the Beholder: Improved Relation Generalization for Text-based Reinforcement Learning Agents",
            "rating": 1,
            "sanitized_title": "eye_of_the_beholder_improved_relation_generalization_for_textbased_reinforcement_learning_agents"
        }
    ],
    "cost": 0.010676499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Commonsense Knowledge from Scene Graphs for Textual Environments</p>
<p>Tsunehiko Tanaka tsunehiko@fuji.waseda.jp 
IBM Research</p>
<p>Waseda University</p>
<p>Daiki Kimura 
IBM Research</p>
<p>Michiaki Tatsubori 
IBM Research</p>
<p>Commonsense Knowledge from Scene Graphs for Textual Environments</p>
<p>Text-based games are becoming commonly used in reinforcement learning as real-world simulation environments. They are usually imperfect information games, and their interactions are only in the textual modality. To challenge these games, it is effective to complement the missing information by providing knowledge outside the game, such as human common sense. However, such knowledge has only been available from textual information in previous works. In this paper, we investigate the advantage of employing commonsense reasoning obtained from visual datasets such as scene graph datasets. In general, images convey more comprehensive information compared with text for humans. This property enables to extract commonsense relationship knowledge more useful for acting effectively in a game. We compare the statistics of spatial relationships available in Visual Genome (a scene graph dataset) and ConceptNet (a text-based knowledge) to analyze the effectiveness of introducing scene graph datasets. We also conducted experiments on a text-based game task that requires commonsense reasoning. Our experimental results demonstrated that our proposed methods have higher and competitive performance than existing state-ofthe-art methods.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) is a type of machine learning method that has a great advantage of not requiring labeled data and has been used in various simulation environments (Mnih et al. 2015;Silver, Huang, and et al. 2016;Kimura 2018;Kimura et al. 2018). Since textual conversation agents are commonly used in our daily lives in the real world, text-based environments, where both observation and action spaces are restricted to the modality of text, have been attracting attention. RL in such environments requires developing an agent to have language comprehension skills by natural language process and sequential decision-making in the complex environment. This means the textual observation contains a lot of noisy information and the problem of partial observability.</p>
<p>Text-based games are a partially observable Markov decision process (POMDP) (Kaelbling, Littman, and Cassandra Figure 1: Illustration of our commonsense acquisition from scene graphs. To provide commonsense: dirty fork → IN → dishwasher to an agent, a single image is sufficient for scene graphs (top left), but ConceptNet requires several graphs to be combined, which is redundant. 1998) where the agent cannot observe the entire information from the text given by the environment. TextWorld (Côté et al. 2018) is a textual game generator and extensible sandbox learning environment for RL agents, and various methods have been proposed for this game to compensate for the missing information (Kimura et al. 2021b;Carta et al. 2020;Murugesan, Chaudhury, and Talamadupula 2021;Shridhar et al. 2020;Kimura et al. 2021a,c;Chaudhury et al. 2021). There are three types of extensions: external knowledge, new modality, and logical rule extraction. External knowledge that is useful for training agents from humans or other domain sources. A study reports commonsense knowledge is an important aspect of human intelligence . In this study, TextWorld Commonsense (TWC), which requires commonsense as external knowledge, is proposed as an extension of TextWorld. The task of the TWC game is cleaning up a room, and the commonsense in this game is mainly place information for each object. The same study also includes a baseline agent for TWC games that uses a commonsense subgraph extracted from external knowledge (we call this model TWC agent and the environment TWC games to distinguish them). Another study reported that introducing ex-ternal knowledge from humans as logical functions helps the training of the agent (Kimura et al. 2021b). New modality information extracted from observations or action text can be introduced to make decisions (Carta et al. 2020;Murugesan, Chaudhury, and Talamadupula 2021;Shridhar et al. 2020). In these methods, visual information from images or videos is commonly used since it has been used in many other studies (Tanaka and Simo-Serra 2021;Kimura et al. 2020) to understand attention and sequential information in decision making. Logical rule extraction can be exploited to improve the speed of training and interpretability of the agent (Kimura et al. 2021c;Chaudhury et al. 2021). Since commonsense knowledge is normally represented by a graph structure, the logical rule representation is compatible with commonsense knowledge.</p>
<p>However, at the time of writing, there has been no research that utilizes the benefits of these multiple extensions to compensate for missing information. In particular, we hypothesize that the commonsense knowledge of object place relationships that are used in TWC games can be easily obtained from visual information. For example, instead of stating the place name of each object, operators can display a picture of a tidy room, which is a quicker explanation for humans.</p>
<p>In this paper, we propose a novel agent that challenges a TWC game by leveraging visual scene graph datasets to obtain commonsense. The original TWC agent ) constructs a commonsense subgraph from Con-ceptNet (Speer, Chin, and Havasi 2017a), which is textual knowledge, but it is necessary to combine many graphs to obtain one commonsense and to create a complicated subgraph. In fact, Murugesan et al. prepared a 'manual' commonsense subgraph from ConceptNet to tackle this complexity of graphs in their study. However, since scene graph recognition achieves high accuracy from complex images, visual information can deliver various detailed and organized graph information all at once. Figure 1 shows an example for the acquisition of commonsense knowledge from scene graphs in an image. In this example, despite Concept-Net having redundant information for extracting a commonsense subgraph, the proposed extraction from scene graphs has necessary and sufficient information for the cleaningup task. Furthermore, relationships from scene graphs also contain direct spatial relationships such as "on" or "in" (Figure 2) between objects because agents need to determine an object's place in the TWC game. Therefore, we use scene graph datasets as visual external knowledge. A scene graph dataset contains a large number of graphs that represent the relationships between entities in images. We use Visual Genome (VG) (Krishna et al. 2017) as a scene graph dataset, which is the most commonly used, and compare its statistics with ConceptNet. We also conduct experiments to evaluate the performance of agents with commonsense knowledge from a scene graph dataset in RL on text-based games.</p>
<p>Related Work</p>
<p>Text-based RL Games</p>
<p>Text-based interactive RL games has been gaining the focus of many researchers due to the development of environments such as TextWorld (Côté et al. 2018) and Jericho (Hausknecht et al. 2019). In these games, RL agents are required to understand the high-level context information from only textual observation. To overcome this difficulty, a number of prior works on these environments have extracted new information from textual observations: knowledge graphs, visual information, and logical rule.</p>
<p>Knowledge graphs represent relationships between entities like real-world objects and events, or abstract concepts.</p>
<p>A new text-based environment, called "TextWorld Commonsense", was proposed in  to infuse RL agents with commonsense knowledge and developed baseline agents using a commonsense subgraph constructed from ConceptNet (Liu and Singh 2004;Speer, Chin, and Havasi 2017a) as an external knowledge. We use this work as a baseline method, and introduce a new type of commonsense from visual datasets. Worldformer (Ammanabrolu and Riedl 2021) represents environment status as a knowledge graph and uses a world model to predict changes caused by an agent's actions and generates a set of contextually relevant actions.</p>
<p>While knowledge graphs are useful for organizing abstract information from only text descriptions, visual information enables the agent to obtain a detailed locational situation like human imagination and visualization. The most important issue in using visual information is how to obtain it from only textual observation in text-based games. VisualHints (Carta et al. 2020) proposed an environment that can automatically generate various hints about game states from textual observation and changes the difficulty level depending on their type. The main sources of images in (Murugesan, Chaudhury, and Talamadupula 2021) are retrieved from the Internet and generated from a text-toimage pre-trained model, AttnGAN (Xu et al. 2018) with given text descriptions. ALFWorld (Shridhar et al. 2021) combines TextWorld and an embodied simulator called AL-FRED (Shridhar et al. 2020) to obtain information on two modalities. Shridhar et al. proposed an agent that first learns to solve abstract tasks in TextWorld, then transfers the learned high-level policies to low-level embodied tasks in ALFRED.</p>
<p>In addition, even if we use the aforementioned methods, improvements in the speed of training are few and the interpretability of the trained network is still missing. A number of studies (Kimura et al. 2021c;Chaudhury et al. 2021) proposed novel approaches to extract symbolic first-order logics from text observations, and select actions by using neurosymbolic Logical Neural Networks (Riegel et al. 2020). These logical representations are compatible with commonsense graph structures.</p>
<p>As previously described, there have been various approaches using knowledge graphs, visual information, and logical rules. However, at the time of writing, there has been no method that combines any of them. Therefore, we pro-  .</p>
<p>pose an approach to extract and utilize knowledge graphs from visual information.</p>
<p>Scene Graph Dataset</p>
<p>A scene graph is a structured representation of the relationships between objects in a scene. To train a scene graph generation model, a number of datasets have been created. VG (Krishna et al. 2017) is a large-scale scene graph dataset that is most commonly used these days because it contains various elements such as objects, attributes, relationships, QA descriptions, and so on. Since scene graphs can provide a large number of visual relationships in a single image, we use VG datasets as external knowledge for training agents.</p>
<p>ConceptNet vs Scene Graph Datasets</p>
<p>In this section, to show scene graph datasets are effective as external knowledge for solving TWC games, we compare ConceptNet and scene graph datasets. We first show the statistics of ConceptNet, VG (Krishna et al. 2017), and manual commonsense knowledge designed in ). Next, we compare ConceptNet and VG in terms of similarity to the manual commonsense knowledge. VG is the most commonly used scene graph dataset. The manual commonsense knowledge is manually extracted from Con-ceptNet to include only the pairs of an object in TWC games and goal location for each object. Since the entities are directly related to actions in the games, the agent with this manually-crafted information is more effective for solving the games. Therefore, external knowledge that is similar to the manual commonsense knowledge are comfortable with this task.</p>
<p>Knowledge Statistics</p>
<p>We summarize the statistics of the three types of external knowledge in Table 1. In general for all external knowledge, each graph is represented as a triplet e 1 , r 12 , e 2 : e 1 , e 2 denote entities in an image, and r 12 denotes a relationship between e 1 and e 2 . In Table 1, 'entity', 'relationship', and 'triplet' indicate the number of species of e, r, e 1 , r 12 , e 2 , respectively. The huge difference between ConceptNet and VG is the number of species of relationships, and this indicates that VG has more detailed information in relationships than ConceptNet. Figure 2 shows an example of r in these datasets. In ConceptNet, the spatial relationship is only 'at location', which is the second most common, but its ratio to the total is low. In contrast, spatial relationships such as 'on', 'in', and 'under' dominate VG. In addition, 'has' and 'with' can also express spatial relationships, such as building, has, window and window, with, building . Thus, we can see that VG has a larger number of spatial relationships than ConceptNet. However, the manual commonsense knowledge has two species of relationships: 186 'at location' and 6 'related to'. This means that spatial relationships are important for solving TWC games, and VG has an advantage in this respect.</p>
<p>Similarity to Manual Commonsense Knowledge</p>
<p>To examine whether VG contains knowledge graphs useful for solving TWC games, we compare ConceptNet and VG in terms of similarity to the manual commonsense knowledge. We calculate the similarities for both entity e and entity pairs {e 1 , e 2 } as described in the following.</p>
<p>Entity e Given an entity e V i from VG, we use GloVe (Pennington, Socher, and Manning 2014) embeddings to repre-
sent e V i as a d-dimensional vector z V i , where z V i ∈ R d
is the word embedding of the entity. Similarly, the embedding of each entity in ConceptNet e C j and in manual commonsense knowledge e M k are denoted as z C j , z M k , respectively. We calculate the similarity s eV i k between an entity in VG e V i and in manual commonsense knowledge e M k by Eq. 1.
s eV ik = cos similarity(z V i , z M k )(1)
Similarly, Eq. 1 is executed for entities in ConceptNet. We count the number of entities whose similarity is above the threshold 0.7.</p>
<p>Pair of entities {e 1 , e 2 } We also compare sets of pairs of e 1 and e 2 from these datasets in terms of similarity to the 132 pairs in the manual commonsense knowledge. In the same way as the entities previously described, we use GloVe to represent e V i1 and e V i2 from triplet t
V i = e V i1 , r V i1i2 , e V i2
in VG as d-dimensional vectors z V i1 , z V i2 , respectively. Similarly, z C j1 , z C j2 and z M k1 , z M k2 are the embeddings of entities from each triplet in ConceptNet t C j = e C j1 , r C j1j2 , e C j2 and manual commonsense knowledge t M k = e M k1 , r M k1k2 , e M k2 , respectively. We calculate the similarity using the sum of both embeddings of entities in a triplet. Thus, the similarity s pV ik is given by Eq. 2.
s pV ik = cos similarity(z V i1 + z V i2 , z M k1 + z M k2 )(2)
For ConceptNet, we use Eq. 2 similarity. We set a threshold to 0.65 and count pairs over the threshold. The results of the entity and pair counts are summarized in Table 2. Although there is no significant difference in the number of types of entities, VG has more pairs associated with the manual commonsense knowledge, which indicates that VG has more game-related relationships than Concept-Net.</p>
<p>The aforementioned comparisons show that TWC games need more spatial relationships in the external knowledge, and VG is more effective for solving TWC games than Con-ceptNet.  Table 2: Comparison of the number of entities and pairs similar to manual commonsense knowledge in external knowledge. A pair is a combination of e 1 and e 2 from a triplet e 1 , r 12 , e 2 . The number of entities is not very different, but the number of pairs is much higher in VG than in Concept-Net.</p>
<p>Proposed Method Previous TWC agent</p>
<p>The proposed methods extend the TWC agent , which is a baseline model for TextWorld Commonsense. We briefly explain the network architecture as follows.</p>
<p>The TWC agent consists of the six components: (a) action encoder, which encodes all admissible actions a, (b) observation encoder, which encodes the observation o t , (c) context encoder, which encodes the dynamic context C t , (d) dynamic commonsense subgraph, which is commonsense information G t C extracted by the agent, (e) knowledge integration, which combines the information from textual observation and the extracted commonsense subgraph, and (f) action selection, which selects an action from given action candidates. We subsequently describe dynamic commonsense subgraph and knowledge integration, which are important for this paper.</p>
<p>For dynamic commonsense subgraph, the TWC agent retrieves commonsense from external knowledge like Con-ceptNet (Speer, Chin, and Havasi 2017a) and updates a subgraph by combining it with the graph at a previous time step. At time t, the agent first extracts entities involving game status from textual observation and then obtains a set of cumulative entities E t by combining it with the entities from the previous graph G t−1 C . The commonsense subgraph G t C is constructed automatically from E t and Context Direct Connections (CDC), which is another algorithm of external knowledge. For CDC, the entities are split into two groups in accordance with their attributes, and then links between the groups are added.</p>
<p>For knowledge integration, the TWC agent encodes the commonsense subgraph and integrates the graph embedding vector with the observation context feature. In the encoding phase, the node embedding is first extracted from the commonsense subgraph using a pre-trained knowledge graph embedding called Numberbatch (Speer, Chin, and Havasi 2017b) and a sentinel vector (Lu et al. 2017) is added to enable the attention to not attend to any specific nodes in the commonsense subgraph. These embeddings are updated by messages passing between the nodes of graph attention networks (GAT) (Veličković et al. 2018). In the integration phase, Co-Attention is used, which is a bidirectional attention flow layer between the observational context and the commonsense subgraph.</p>
<p>TWC agent is an attractive design for RL agents on textbased games, and it accesses a commonsense and uses it while selecting actions. However, the source format of external knowledge is limited to text. Since textual knowledge such as ConceptNet is very useful, it is redundant because multiple concepts need to be concatenated to express more detailed information. Therefore, they prepared manually retrieved graphs in the paper ). The manual graphs contain direct connections for the objects and their goal locations.</p>
<p>Level</p>
<p>Objects</p>
<p>Objects to find Rooms  </p>
<p>Proposed Method</p>
<p>From the comparison in the previous section, it is revealed that scene graph datasets are effective for TWC games because they have more spatial relationships. This suggests that the issue of the TWC agent is that the external knowledge is limited to being text-based. To address this, we propose an approach to use scene graph datasets as external knowledge to build a commonsense subgraph for agents. Our proposal has two types depending on the training method and external knowledge.</p>
<p>Scene Graph The simplest model is to replace the external knowledge of the TWC agent with scene graph datasets from ConceptNet. As shown in Table 2, a scene graph dataset holds many triplets that are effective for solving TWC games, so we expect to obtain a higher score.</p>
<p>ConceptNet + SG We also propose a method to complement the weak point of textual knowledge with scene graph datasets. Inspired by curricular learning (Bengio et al. 2009), we first provide the agent with textual knowledge and train it, then provide the same agent with external knowledge from scene graph datasets and continue training. We hypothesize that this method is effective in training agents that have commonsense knowledge balanced between abstract and concrete knowledge. In the first step, the overall commonsense is given by the textual knowledge. In the second step, the specific commonsense focused on location is given from the scene graph dataset.</p>
<p>Experiments Experimental Setup</p>
<p>We conduct our experiments on TWC games . A TWC game is a text-based game where the goal is to tidy up a house by putting objects where they should be. The connection between objects and the locations where they should be is not given by the game, so the agent needs to depend on commonsense knowledge. This domain has three difficulty levels (easy, medium, and hard) depending on the total number of objects in the game, the number of objects in which the agent needs to find their locations, and the number of rooms to explore. The numbers are randomly sampled from the list in Table 3. In our evaluation, we consider all difficulty levels. We also use two types of test sets: IN and OUT. The games in the IN were built on the same entities as the training set, and the entities in the OUT do not appear in the training set. We can evaluate the ability to generalize unseen entities from these test sets.</p>
<p>Our experimental setup is based on the evaluation system in ; we use the Advantage Actor-Critic algorithm (Mnih et al. 2016). The most significant difference from the previous system is that all agents use GloVe for graph embedding. Numberbatch (Speer, Chin, and Havasi 2017b) used in the previous system is a combination of existing pre-trained embeddings such as word2vec (Mikolov et al. 2013) and GloVe retrofitted with ConceptNet's graph. The evaluation experiments in ) have shown that a TWC agent with Numberbatch achieved a better performance than with GloVe because of a high affinity with ConceptNet. Since we focus on the impact of external knowledge, we use only GloVe for both graph and observation embeddings in all agents.</p>
<p>Metrics</p>
<p>We measure the performance of agents with the various external knowledge on TextWorld using two metrics: the normalized score and the number of steps taken. The normalized score is calculated by dividing the actual score by the maximum possible score. Steps indicate time spent to reach the goal and the lower the value, the higher the performance.</p>
<p>Results</p>
<p>We compare four types of agents: ConceptNet, ConceptNet + Manual, Scene Graph, and ConceptNet + SG. Both Scene Graph and ConceptNet + SG are our proposed methods that use VG. The other agents are baselines proposed in . ConceptNet + Manual uses manuallyprepared knowledge directly related to the game from Con-ceptNet. ConceptNet + Manual should show human-like performance and is regarded as the upper bound of the performance of the proposed method (especially for IN). Con-ceptNet + SG is first trained for 100 episodes using Concept-Net, followed by 100 episodes using VG. All agents except ConceptNet + SG are trained for 100 episodes each and all results are the average of five runs. The results are summarized in Table 4. We also show the training curves in Figure 3.</p>
<p>We can see three overall trends in these results. First, our proposed methods outperform the baselines in both steps and scores in the easy and medium levels. The commonsense knowledge obtained from scene graph datasets is proved to be effective in solving TWC games. Second, all agents struggle with the hard level. It is necessary to have the ability to deal with complicated situations where the location to pick up objects is different from the location to place them. The development of agents with this ability is future work. Finally, ConceptNet + Manual shows high performance for the IN set regardless of difficulty level. ConceptNet + Manual has crucial information for solving TWC games, so both efficiency and scores are higher when the same species of entity is given as the training set. However, its performance in the OUT is lower than that of other agents because it is given only a minimum number of necessary graphs in the training set, so it overfits to those graphs.</p>
<p>We describe the performance of our proposed models in detail. We propose two models, Scene Graph and Concept-   . IN is built using the same entities as the training set, and OUT is built using different entities. #Steps (lower is better) denotes the steps needed to accomplish the goals and Score (higher is better) denotes the normalized score by maximum possible score. Each value is a pair (average) ± (standard deviation).</p>
<p>Net + SG, depending on the type of external knowledge and training method. Scene Graph is a simple agent that uses only a scene graph dataset. From Table 4, we found that this agent is very efficient in its graph search. For easy-level games, Scene Graph is superior to even ConceptNet + Manual in the IN set. In medium-level games, Scene Graph has the largest number of steps in the IN set, but it has the smallest number in the OUT set. This indicates that Scene Graph is robust against unseen objects. The efficiency can be seen from the fastest convergence of the training curves in the medium and hard levels in Figure 3. In terms of performance, although it is sometimes inferior to our other proposed method, it is better than the baselines in OUT. The high efficiency and performance of Scene Graph can be attributed to scene graph datasets having many graphs relevant to the TWC game with spatial relationships. Concept-Net + SG is an agent that is trained with ConceptNet, followed by scene graph datasets. Table 4 shows that the performance of this agent is very high. It has the best performance in the OUT set for both easy and medium levels and the second-best performance in the IN set after Con-ceptNet + Manual. This indicates that it also has robustness against unseen objects. The reason for the performance improvement is considered to be the wide range that can be handled by both commonsense knowledge from Concept-Net and VG. In easy-level games, the score increases by 0.14 from IN to OUT, which is an uncommon improvement. The reason for this could be that VG has a small number of graphs related to the easy-IN games, which decrease the results of ConceptNet + SG for easy-IN games. As you can see in Table 3, easy games has only require one object to be explored, so the score changes dramatically depending on whether or not external knowledge contains graphs related to the object and the goal location. However, since the number of graphs increases, the efficiency of the search decreases, resulting in inferiority with scene graphs in steps.</p>
<p>The training curve in Figure 3 shows that VG enhances the performance of ConceptNet alone from 100 episodes (in the easy level, the training curve converges during the Concept-Net phase, so ConceptNet and ConceptNet + SG overlap). In addition, we use GloVe for graph embedding in these experiments for a fair comparison, but agents using Con-ceptNet can be improved by replacing GloVe with Numberbatch.</p>
<p>In summary, our proposed method achieves both efficiency and performance improvements, and it is robust to unseen objects. However, it is still a challenge to deal with complex situations such as hard-level games. We discuss how to address this issue in the next section.</p>
<p>Conclusion and Future Work</p>
<p>We have presented new approaches to leverage commonsense subgraphs constructed from scene graph datasets for text-based games. We conducted experiments on a TWC game, which is a benchmark to evaluate how well an agent learns with commonsense knowledge. Experimental results showed that our proposed approaches using a VG dataset demonstrate highly competitive performances compared with existing state-of-the-art approaches with textual knowledge. We also illustrated that the performance can be further improved by using ConceptNet and scene graph datasets sequentially.</p>
<p>Although this work is the first step to utilize both visual information and commonsense, we still have a few challenges as future work. One topic is to deal with more complex situations like hard difficulty level games. We expect that this can be improved by exploiting the relationships among information available from external knowledge. The current model adopts GAT (Veličković et al. 2018) as a graph encoder, but GAT only takes the node features as input and ignores the edge features except for whether they exist or not. In complex tasks, the key to solving this game is more specific information than simply the link between an object and location. In particular, scene graph datasets provide more detailed information on relationships than textual knowledge as shown in Fig. 1. We consider applying networks such as Edge feature enhanced Graph Neural Networks (EGNN) (Gong and Cheng 2019) that can take advan- tage of the edge features of graphs. Another topic is introducing logical rule training into our proposed method. Since graph information can be easily converted to logical rules, we hope the commonsense graph can directly contribute to logical rule training for action policies in RL.</p>
<p>Figure 2 :
2Histogram of relationships (top 15) included in VG and ConceptNet. VG has much more spatial relationships</p>
<p>Figure 3 :
3Performance evaluation for the three levels of the training set games (Smoothing is performed to clarify the difference in the results of a single run).</p>
<p>1 .
1Take the dirty fork from the table 2. Open the fridge 3. Put the dirty fork in the fridge 4. Put the dirty fork in the dishwasherScene Graph 
Scene Graph </p>
<p>dishwasher 
dirty fork </p>
<p>IN </p>
<p>Table 3 :
3Specification of TWC games from.</p>
<p>Table 4 :
4Generalization results for two test sets, IN and OUT, on games with three difficulty levels.Scene Graph and Con-</p>
<p>Learning Knowledge Graph-based World Models of Textual Environments. P Ammanabrolu, M O Riedl, arXiv:2106.09608arXiv preprintAmmanabrolu, P.; and Riedl, M. O. 2021. Learning Knowl- edge Graph-based World Models of Textual Environments. arXiv preprint arXiv:2106.09608.</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningBengio, Y.; Louradour, J.; Collobert, R.; and Weston, J. 2009. Curriculum learning. In Proceedings of the 26th an- nual international conference on machine learning, 41-48.</p>
<p>VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning. T Carta, S Chaudhury, K Talamadupula, M Tatsubori, In arxivCarta, T.; Chaudhury, S.; Talamadupula, K.; and Tatsubori, M. 2020. VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning. In arxiv.</p>
<p>Neuro-Symbolic Approaches for Text-Based Policy Learning. S Chaudhury, P Sen, M Ono, D Kimura, M Tatsubori, A Munawar, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsChaudhury, S.; Sen, P.; Ono, M.; Kimura, D.; Tatsubori, M.; and Munawar, A. 2021. Neuro-Symbolic Approaches for Text-Based Policy Learning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 3073-3078. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.</p>
<p>TextWorld: A Learning Environment for Text. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, R Y Tao, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, based Games. CoRR, abs/1806.11532Côté, M.-A.; Kádár, A.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Tao, R. Y.; Hausknecht, M.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. TextWorld: A Learning Environment for Text-based Games. CoRR, abs/1806.11532.</p>
<p>Exploiting edge features for graph neural networks. L Gong, Q Cheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionGong, L.; and Cheng, Q. 2019. Exploiting edge features for graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9211-9219.</p>
<p>Interactive Fiction Games: A Colossal Adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X E Yuan, 2020Hausknecht, M.; Ammanabrolu, P.; Côté, M.-A.; and Yuan, X. E. 2019. Interactive Fiction Games: A Colossal Adven- ture. In AAAI 2020.</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artificial Intelligence. 1011Kaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1998. Planning and acting in partially observable stochastic do- mains. Artificial Intelligence, 101(1): 99-134.</p>
<p>D Kimura, arXiv:1806.00630DAQN: Deep Auto-encoder and Q-Network. Kimura, D. 2018. DAQN: Deep Auto-encoder and Q- Network. arXiv:1806.00630.</p>
<p>Adversarial Discriminative Attention for Robust Anomaly Detection. D Kimura, S Chaudhury, M Narita, A Munawar, R Tachibana, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Kimura, D.; Chaudhury, S.; Narita, M.; Munawar, A.; and Tachibana, R. 2020. Adversarial Discriminative Attention for Robust Anomaly Detection. In 2020 IEEE Winter Con- ference on Applications of Computer Vision (WACV), 2161- 2170.</p>
<p>LOA: Logical Optimal Actions for Text-based Interaction Games. D Kimura, S Chaudhury, M Ono, M Tatsubori, D J Agravante, A Munawar, A Wachi, R Kohita, A Gray, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System DemonstrationsOnline: Association for Computational LinguisticsKimura, D.; Chaudhury, S.; Ono, M.; Tatsubori, M.; Agra- vante, D. J.; Munawar, A.; Wachi, A.; Kohita, R.; and Gray, A. 2021a. LOA: Logical Optimal Actions for Text-based In- teraction Games. In Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, 227-231. Online: As- sociation for Computational Linguistics.</p>
<p>Internal Model from Observations for Reward Shaping. D Kimura, S Chaudhury, R Tachibana, S Dasgupta, ICML workshop. Kimura, D.; Chaudhury, S.; Tachibana, R.; and Dasgupta, S. 2018. Internal Model from Observations for Reward Shap- ing. In ICML workshop.</p>
<p>Reinforcement Learning with External Knowledge by using Logical Neural Networks. D Kimura, S Chaudhury, A Wachi, R Kohita, A Munawar, M Tatsubori, A Gray, KBRL Workshop at IJCAI-PRICAI 2020. Kimura, D.; Chaudhury, S.; Wachi, A.; Kohita, R.; Mu- nawar, A.; Tatsubori, M.; and Gray, A. 2021b. Reinforce- ment Learning with External Knowledge by using Logical Neural Networks. KBRL Workshop at IJCAI-PRICAI 2020.</p>
<p>Neuro-Symbolic Reinforcement Learning with First-Order Logic. D Kimura, M Ono, S Chaudhury, R Kohita, A Wachi, D J Agravante, M Tatsubori, A Munawar, A Gray, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsOnline and Punta CanaKimura, D.; Ono, M.; Chaudhury, S.; Kohita, R.; Wachi, A.; Agravante, D. J.; Tatsubori, M.; Munawar, A.; and Gray, A. 2021c. Neuro-Symbolic Reinforcement Learning with First- Order Logic. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 3505- 3511. Online and Punta Cana, Dominican Republic: Associ- ation for Computational Linguistics.</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, 123Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Kravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma, D. A.; et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Inter- national journal of computer vision, 123(1): 32-73.</p>
<p>. H Liu, P Singh, ConceptNet -A Practical Commonsense Reasoning Tool-Kit. BT Technology Journal. 22Liu, H.; and Singh, P. 2004. ConceptNet -A Practical Commonsense Reasoning Tool-Kit. BT Technology Journal, 22: 211-226.</p>
<p>Knowing when to look: Adaptive attention via a visual sentinel for image captioning. J Lu, C Xiong, D Parikh, R Socher, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionLu, J.; Xiong, C.; Parikh, D.; and Socher, R. 2017. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, 375-383.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef- ficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, PMLRIn International conference on machine learning. Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asyn- chronous methods for deep reinforcement learning. In In- ternational conference on machine learning, 1928-1937. PMLR.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, Nature. Mnih, V.; Kavukcuoglu, K.; Silver, D.; and et al. 2015. Human-level control through deep reinforcement learning. Nature.</p>
<p>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. K Murugesan, M Atzeni, P Kapanipathi, P Shukla, S Kumaravel, G Tesauro, K Talamadupula, M Sachan, M Campbell, Thirty Fifth AAAI Conference on Artificial Intelligence. Murugesan, K.; Atzeni, M.; Kapanipathi, P.; Shukla, P.; Ku- maravel, S.; Tesauro, G.; Talamadupula, K.; Sachan, M.; and Campbell, M. 2021. Text-based RL Agents with Com- monsense Knowledge: New Challenges, Environments and Baselines. In Thirty Fifth AAAI Conference on Artificial In- telligence.</p>
<p>Eye of the Beholder: Improved Relation Generalization for Text-based Reinforcement Learning Agents. K Murugesan, S Chaudhury, K Talamadupula, arXiv:2106.05387arXiv preprintMurugesan, K.; Chaudhury, S.; and Talamadupula, K. 2021. Eye of the Beholder: Improved Relation Generalization for Text-based Reinforcement Learning Agents. arXiv preprint arXiv:2106.05387.</p>
<p>GloVe: Global Vectors for Word Representation. J Pennington, R Socher, C Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsPennington, J.; Socher, R.; and Manning, C. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532-1543. Doha, Qatar: Association for Computational Linguistics.</p>
<p>R Riegel, A Gray, F Luus, N Khan, N Makondo, I Y Akhalwaya, H Qian, R Fagin, F Barahona, U Sharma, arXiv:2006.13155Logical neural networks. arXiv preprintRiegel, R.; Gray, A.; Luus, F.; Khan, N.; Makondo, N.; Akhalwaya, I. Y.; Qian, H.; Fagin, R.; Barahona, F.; Sharma, U.; et al. 2020. Logical neural networks. arXiv preprint arXiv:2006.13155.</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Shridhar, M.; Thomason, J.; Gordon, D.; Bisk, Y.; Han, W.; Mottaghi, R.; Zettlemoyer, L.; and Fox, D. 2020. ALFRED: A Benchmark for Interpreting Grounded Instructions for Ev- eryday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRShridhar, M.; Yuan, X.; Côté, M.-A.; Bisk, Y.; Trischler, A.; and Hausknecht, M. 2021. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Pro- ceedings of the International Conference on Learning Rep- resentations (ICLR).</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, Nature. 529Silver, D.; Huang, A.; and et al. 2016. Mastering the game of Go with deep neural networks and tree search. Nature, 529: 484-503.</p>
<p>ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. R Speer, J Chin, C Havasi, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17. the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17AAAI PressSpeer, R.; Chin, J.; and Havasi, C. 2017a. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. In Pro- ceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, 4444-4451. AAAI Press.</p>
<p>ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. R Speer, J Chin, C Havasi, AAAI Press17Speer, R.; Chin, J.; and Havasi, C. 2017b. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. AAAI'17, 4444-4451. AAAI Press.</p>
<p>LoL-V2T: Large-Scale Esports Video Description Dataset. T Tanaka, E Simo-Serra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) WorkshopsTanaka, T.; and Simo-Serra, E. 2021. LoL-V2T: Large- Scale Esports Video Description Dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 4557-4566.</p>
<p>P Veličković, G Cucurull, A Casanova, A Romero, P Liò, Y Bengio, Graph Attention Networks. International Conference on Learning Representations. Veličković, P.; Cucurull, G.; Casanova, A.; Romero, A.; Liò, P.; and Bengio, Y. 2018. Graph Attention Networks. Inter- national Conference on Learning Representations.</p>
<p>AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks. T Xu, P Zhang, Q Huang, H Zhang, Z Gan, X Huang, X He, Xu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang, X.; and He, X. 2018. AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks.</p>            </div>
        </div>

    </div>
</body>
</html>
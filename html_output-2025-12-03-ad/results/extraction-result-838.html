<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-838 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-838</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-838</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-273375045</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.12049v1.pdf" target="_blank">Sabi´a-3 Technical Report</a></p>
                <p><strong>Paper Abstract:</strong> This report presents Sabi´a-3, our new flagship language model, and Sabiazinho-3, a more cost-effective sibling. The models were trained on a large brazilian-centric corpus. Evaluations across diverse professional and academic benchmarks show a strong performance on Portuguese and Brazil-related tasks. Sabi´a-3 shows large improvements in comparison to our previous best of model, Sabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi´a-3’s average performance matches frontier LLMs, while it is offered at a three to four times lower cost per token, reinforcing the benefits of domain specialization</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e838.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e838.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sabiá-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sabiá-3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Brazil-specialized flagship LLM trained by continual pre-training on a large Portuguese/Brazil-centric corpus and instruction-tuned with human and synthetic data; strong multiple-choice QA performance but weaker multi-step/agentic abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Sabiá-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Continually pre-trained transformer LLM (generalist checkpoint further trained on high-quality Brazilian corpus), instruction-tuned with a mix of human-annotated and synthetically generated instructions; supports function-calling and up to 32k token context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Multiple-choice academic exams (70 Brazilian exams, ENADE and others)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Comparable to GPT-4o across 70 Brazilian exams; surpasses Llama-3.1 405B by ~3 percentage points on the 70-exam average; large improvements vs Sabiá-2 Medium (e.g., up to 70% error reduction in some domains).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench (OS, DB, KG, WebShopping, WebBrowsing, Digital Card Game, Lateral Thinking Puzzle, House-Holding) and BFCL (Berkeley Function Calling Leaderboard)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation, tool use / function calling, game-playing, multi-step reasoning / planning, sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>AgentBench Overall Average (OA) = 2.9; top performance in WebShopping (WS) with score 62.5 and strong WebBrowsing (second-best, numeric ~58 as reported table value); competitive on BFCL Non-live but weaker on multi-turn/multi-step BFCL subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>large-context (32k tokens) support; function-calling interface; standard LLM transformer backbone (no novel external memory or explicit planner reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Continual pre-training (next-token prediction) on specialized corpus; instruction tuning with human-annotated + synthetic data; post-training alignment to human preferences (alignment / instruction-tuning phase).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training data / instruction-tuning and system-level capability limits (context length) adjustments</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Domain specialization via further pre-training on Portuguese/Brazil-centric high-quality data; mixed human + synthetic instruction tuning (including synthetic examples to induce 'self-awareness'); model supports up to 32k tokens and function calling. Authors propose increasing context support (>32k) and improving irrelevance detection/multi-turn function-calling handling as interventions to improve agentic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Pretraining + instruction tuning improved QA strongly (Sabiá-3 matches frontier LLMs on 70 exams and outperforms Sabiá-2 Medium); agentic OA improved from 0.9 (Sabiá-2 Medium) to 2.9 (Sabiá-3), but multi-step/multi-turn performance still lags and requires further improvements (no single intervention fully closed the gap).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors attribute the gap to remaining limitations in multi-turn/multi-step function-calling, irrelevance detection, and context-length limits for chain-of-actions; also domain specialization (Portuguese focus) helps QA but agentic benchmarks (often English, tool- and multi-step-heavy) require better multi-step planning, longer context/memory, and improved function-calling/multi-turn orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sabi´a-3 Technical Report', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e838.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e838.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sabiazinho-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sabiazinho-3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cost-effective sibling of Sabiá-3 trained with the same Brazilian-centric approach but optimized for lower inference cost; shows good QA improvements but lower agentic performance than Sabiá-3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Sabiazinho-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller / cost-optimized variant of Sabiá-3 family; same training pipeline (continual pre-training on Brazil-focused data and instruction tuning) but targeted for lower per-token cost; supports up to 32k tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Multiple-choice academic exams and instruction-following benchmarks (evaluated alongside Sabiá-3)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Improved over Sabiá-2 Medium on QA and instruction-following, but lower than Sabiá-3; IFeval loose/strict = 70.0% / 65.4% (reported in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation, tool use / function calling, multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>AgentBench OA = 1.6 (lower than Sabiá-3); weaker code- and web-grounded performance compared to Sabiá-3.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>supports function calling; 32k token context; otherwise same transformer architecture as Sabiá-3 family (no extra planner/memory modules reported).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Continual pre-training on specialized corpus; instruction-tuning with human and synthetic data; alignment/post-training similar to Sabiá-3.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training-data specialization and instruction-tuning (no new architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Same domain-specialization and instruction-tuning interventions as Sabiá-3 but at a smaller/cost-optimized scale.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved IFeval instruction-following scores vs Sabiá-2 Medium; AgentBench OA 1.6 (improvement over Sabiá-2 Medium but lower than full Sabiá-3).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Smaller model / cost optimizations reduce capacity for complex multi-step agentic reasoning and tool orchestration; context-length and multi-turn planning limitations persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sabi´a-3 Technical Report', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e838.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e838.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sabiá-2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sabiá-2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Previous-generation Portuguese LLM from the same group used as a baseline; substantially worse on instruction-following and agentic tasks compared to Sabiá-3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Sabiá-2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Earlier generation Portuguese-focused LLM (baseline for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Multiple-choice exams and instruction-following (IFEval)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Lower accuracy on multiple-choice exams compared to Sabiá-3; IFeval loose/strict = 41.8% / 38.6% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation, function calling, multi-step tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>AgentBench OA = 0.9 (substantially lower than Sabiá-3's 2.9).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prior training and instruction-tuning pipeline (less data/scale/specialization than Sabiá-3).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>n/a (serves as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Smaller/older model and less specialized instruction tuning limit multi-step reasoning and instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sabi´a-3 Technical Report', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e838.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e838.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary high-capability LLM from OpenAI used as a comparison baseline across QA and agentic benchmarks; strong both on multiple-choice exams and AgentBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal/advanced LLM (details not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Multiple-choice academic exams (70 Brazilian exams and medicine-heavy benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Comparable to Sabiá-3 overall; exhibits advantage particularly in Medicine benchmarks (exact numbers not given in text).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>code-grounded tasks, web navigation, game-playing, multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>AgentBench OA = 3.7 (highest among reported models in Table 5); strong code- and web-grounded scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sabi´a-3 Technical Report', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e838.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e838.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o Mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o Mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lower-cost variant of GPT-4o showing strong instruction-following and conversational performance despite lower per-token price; used as a comparator in BRACEval and IFeval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4o Mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cost-optimized OpenAI model variant (architectural details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Instruction-following (IFEval) and conversational evaluation (BRACEval)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>IFEval loose/strict = 83.9% / 81.0% (Table 3); strong conversational win-rates on BRACEval (not full numeric summary provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step / agentic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>AgentBench OA = 3.1 (Table 5); generally competitive on agent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sabi´a-3 Technical Report', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e838.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e838.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-405B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 405B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large open/partner model used as a comparison baseline; competitive on some conversational benchmarks and AgentBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Llama-3.1 405B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>405-billion parameter instruction-tuned Llama 3.1 model (as listed in Table 1 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>405B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Multiple-choice academic exams (70 Brazilian exams)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>On average, ~3 percentage points lower than Sabiá-3 across the 70-exam suite (Sabiá-3 surpasses Llama-3.1 405B by ~3 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>code-grounded, web-grounded, multi-step agentic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>AgentBench OA = 3.8 (Table 5) — competitive high performer on agent benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sabi´a-3 Technical Report', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e838.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e838.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary Anthropic model evaluated for exam performance and used as a conversational judge in some comparisons; achieves highest average accuracy on the exam suite but may have data contamination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary instruction-following LLM from Anthropic (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Multiple-choice academic exams (70 Brazilian exams)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Reported highest average accuracy among compared models on the 70-exam suite, but authors note risk of data contamination because of later training cutoff.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sabi´a-3 Technical Report', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Agentbench: Evaluating llms as agents. <em>(Rating: 2)</em></li>
                <li>Berkeley function calling leaderboard. <em>(Rating: 2)</em></li>
                <li>Needle in a haystack - pressure testing llms. <em>(Rating: 2)</em></li>
                <li>Instruction-following evaluation for large language models. <em>(Rating: 1)</em></li>
                <li>Sabiá-2: A new generation of portuguese large language models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-838",
    "paper_id": "paper-273375045",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "Sabiá-3",
            "name_full": "Sabiá-3",
            "brief_description": "Brazil-specialized flagship LLM trained by continual pre-training on a large Portuguese/Brazil-centric corpus and instruction-tuned with human and synthetic data; strong multiple-choice QA performance but weaker multi-step/agentic abilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Sabiá-3",
            "model_description": "Continually pre-trained transformer LLM (generalist checkpoint further trained on high-quality Brazilian corpus), instruction-tuned with a mix of human-annotated and synthetically generated instructions; supports function-calling and up to 32k token context.",
            "model_size": null,
            "qa_task_name": "Multiple-choice academic exams (70 Brazilian exams, ENADE and others)",
            "qa_performance": "Comparable to GPT-4o across 70 Brazilian exams; surpasses Llama-3.1 405B by ~3 percentage points on the 70-exam average; large improvements vs Sabiá-2 Medium (e.g., up to 70% error reduction in some domains).",
            "interactive_task_name": "AgentBench (OS, DB, KG, WebShopping, WebBrowsing, Digital Card Game, Lateral Thinking Puzzle, House-Holding) and BFCL (Berkeley Function Calling Leaderboard)",
            "interactive_task_type": "web navigation, tool use / function calling, game-playing, multi-step reasoning / planning, sequential decision-making",
            "interactive_performance": "AgentBench Overall Average (OA) = 2.9; top performance in WebShopping (WS) with score 62.5 and strong WebBrowsing (second-best, numeric ~58 as reported table value); competitive on BFCL Non-live but weaker on multi-turn/multi-step BFCL subsets.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "large-context (32k tokens) support; function-calling interface; standard LLM transformer backbone (no novel external memory or explicit planner reported in this paper).",
            "training_method": "Continual pre-training (next-token prediction) on specialized corpus; instruction tuning with human-annotated + synthetic data; post-training alignment to human preferences (alignment / instruction-tuning phase).",
            "intervention_type": "training data / instruction-tuning and system-level capability limits (context length) adjustments",
            "intervention_description": "Domain specialization via further pre-training on Portuguese/Brazil-centric high-quality data; mixed human + synthetic instruction tuning (including synthetic examples to induce 'self-awareness'); model supports up to 32k tokens and function calling. Authors propose increasing context support (&gt;32k) and improving irrelevance detection/multi-turn function-calling handling as interventions to improve agentic performance.",
            "intervention_effect": "Pretraining + instruction tuning improved QA strongly (Sabiá-3 matches frontier LLMs on 70 exams and outperforms Sabiá-2 Medium); agentic OA improved from 0.9 (Sabiá-2 Medium) to 2.9 (Sabiá-3), but multi-step/multi-turn performance still lags and requires further improvements (no single intervention fully closed the gap).",
            "hypothesized_cause_of_gap": "Authors attribute the gap to remaining limitations in multi-turn/multi-step function-calling, irrelevance detection, and context-length limits for chain-of-actions; also domain specialization (Portuguese focus) helps QA but agentic benchmarks (often English, tool- and multi-step-heavy) require better multi-step planning, longer context/memory, and improved function-calling/multi-turn orchestration.",
            "uuid": "e838.0",
            "source_info": {
                "paper_title": "Sabi´a-3 Technical Report",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Sabiazinho-3",
            "name_full": "Sabiazinho-3",
            "brief_description": "Cost-effective sibling of Sabiá-3 trained with the same Brazilian-centric approach but optimized for lower inference cost; shows good QA improvements but lower agentic performance than Sabiá-3.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Sabiazinho-3",
            "model_description": "Smaller / cost-optimized variant of Sabiá-3 family; same training pipeline (continual pre-training on Brazil-focused data and instruction tuning) but targeted for lower per-token cost; supports up to 32k tokens.",
            "model_size": null,
            "qa_task_name": "Multiple-choice academic exams and instruction-following benchmarks (evaluated alongside Sabiá-3)",
            "qa_performance": "Improved over Sabiá-2 Medium on QA and instruction-following, but lower than Sabiá-3; IFeval loose/strict = 70.0% / 65.4% (reported in Table 3).",
            "interactive_task_name": "AgentBench",
            "interactive_task_type": "web navigation, tool use / function calling, multi-step reasoning",
            "interactive_performance": "AgentBench OA = 1.6 (lower than Sabiá-3); weaker code- and web-grounded performance compared to Sabiá-3.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "supports function calling; 32k token context; otherwise same transformer architecture as Sabiá-3 family (no extra planner/memory modules reported).",
            "training_method": "Continual pre-training on specialized corpus; instruction-tuning with human and synthetic data; alignment/post-training similar to Sabiá-3.",
            "intervention_type": "training-data specialization and instruction-tuning (no new architecture)",
            "intervention_description": "Same domain-specialization and instruction-tuning interventions as Sabiá-3 but at a smaller/cost-optimized scale.",
            "intervention_effect": "Improved IFeval instruction-following scores vs Sabiá-2 Medium; AgentBench OA 1.6 (improvement over Sabiá-2 Medium but lower than full Sabiá-3).",
            "hypothesized_cause_of_gap": "Smaller model / cost optimizations reduce capacity for complex multi-step agentic reasoning and tool orchestration; context-length and multi-turn planning limitations persist.",
            "uuid": "e838.1",
            "source_info": {
                "paper_title": "Sabi´a-3 Technical Report",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Sabiá-2 Medium",
            "name_full": "Sabiá-2 Medium",
            "brief_description": "Previous-generation Portuguese LLM from the same group used as a baseline; substantially worse on instruction-following and agentic tasks compared to Sabiá-3.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Sabiá-2 Medium",
            "model_description": "Earlier generation Portuguese-focused LLM (baseline for comparisons).",
            "model_size": null,
            "qa_task_name": "Multiple-choice exams and instruction-following (IFEval)",
            "qa_performance": "Lower accuracy on multiple-choice exams compared to Sabiá-3; IFeval loose/strict = 41.8% / 38.6% (Table 3).",
            "interactive_task_name": "AgentBench",
            "interactive_task_type": "web navigation, function calling, multi-step tasks",
            "interactive_performance": "AgentBench OA = 0.9 (substantially lower than Sabiá-3's 2.9).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "Prior training and instruction-tuning pipeline (less data/scale/specialization than Sabiá-3).",
            "intervention_type": "n/a (serves as baseline)",
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Smaller/older model and less specialized instruction tuning limit multi-step reasoning and instruction following.",
            "uuid": "e838.2",
            "source_info": {
                "paper_title": "Sabi´a-3 Technical Report",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "Proprietary high-capability LLM from OpenAI used as a comparison baseline across QA and agentic benchmarks; strong both on multiple-choice exams and AgentBench.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4o",
            "model_description": "Proprietary multimodal/advanced LLM (details not specified in this paper).",
            "model_size": null,
            "qa_task_name": "Multiple-choice academic exams (70 Brazilian exams and medicine-heavy benchmarks)",
            "qa_performance": "Comparable to Sabiá-3 overall; exhibits advantage particularly in Medicine benchmarks (exact numbers not given in text).",
            "interactive_task_name": "AgentBench",
            "interactive_task_type": "code-grounded tasks, web navigation, game-playing, multi-step reasoning",
            "interactive_performance": "AgentBench OA = 3.7 (highest among reported models in Table 5); strong code- and web-grounded scores.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e838.3",
            "source_info": {
                "paper_title": "Sabi´a-3 Technical Report",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o Mini",
            "name_full": "GPT-4o Mini",
            "brief_description": "Lower-cost variant of GPT-4o showing strong instruction-following and conversational performance despite lower per-token price; used as a comparator in BRACEval and IFeval.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4o Mini",
            "model_description": "Cost-optimized OpenAI model variant (architectural details not provided in this paper).",
            "model_size": null,
            "qa_task_name": "Instruction-following (IFEval) and conversational evaluation (BRACEval)",
            "qa_performance": "IFEval loose/strict = 83.9% / 81.0% (Table 3); strong conversational win-rates on BRACEval (not full numeric summary provided in text).",
            "interactive_task_name": "AgentBench",
            "interactive_task_type": "multi-step / agentic tasks",
            "interactive_performance": "AgentBench OA = 3.1 (Table 5); generally competitive on agent tasks.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e838.4",
            "source_info": {
                "paper_title": "Sabi´a-3 Technical Report",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama-3.1-405B",
            "name_full": "Llama 3.1 405B",
            "brief_description": "Large open/partner model used as a comparison baseline; competitive on some conversational benchmarks and AgentBench.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Llama-3.1 405B",
            "model_description": "405-billion parameter instruction-tuned Llama 3.1 model (as listed in Table 1 of the paper).",
            "model_size": "405B",
            "qa_task_name": "Multiple-choice academic exams (70 Brazilian exams)",
            "qa_performance": "On average, ~3 percentage points lower than Sabiá-3 across the 70-exam suite (Sabiá-3 surpasses Llama-3.1 405B by ~3 pp).",
            "interactive_task_name": "AgentBench",
            "interactive_task_type": "code-grounded, web-grounded, multi-step agentic tasks",
            "interactive_performance": "AgentBench OA = 3.8 (Table 5) — competitive high performer on agent benchmarks.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e838.5",
            "source_info": {
                "paper_title": "Sabi´a-3 Technical Report",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet",
            "name_full": "Claude 3.5 Sonnet",
            "brief_description": "Proprietary Anthropic model evaluated for exam performance and used as a conversational judge in some comparisons; achieves highest average accuracy on the exam suite but may have data contamination risk.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Claude 3.5 Sonnet",
            "model_description": "Proprietary instruction-following LLM from Anthropic (details not provided in this paper).",
            "model_size": null,
            "qa_task_name": "Multiple-choice academic exams (70 Brazilian exams)",
            "qa_performance": "Reported highest average accuracy among compared models on the 70-exam suite, but authors note risk of data contamination because of later training cutoff.",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e838.6",
            "source_info": {
                "paper_title": "Sabi´a-3 Technical Report",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Agentbench: Evaluating llms as agents.",
            "rating": 2,
            "sanitized_title": "agentbench_evaluating_llms_as_agents"
        },
        {
            "paper_title": "Berkeley function calling leaderboard.",
            "rating": 2,
            "sanitized_title": "berkeley_function_calling_leaderboard"
        },
        {
            "paper_title": "Needle in a haystack - pressure testing llms.",
            "rating": 2,
            "sanitized_title": "needle_in_a_haystack_pressure_testing_llms"
        },
        {
            "paper_title": "Instruction-following evaluation for large language models.",
            "rating": 1,
            "sanitized_title": "instructionfollowing_evaluation_for_large_language_models"
        },
        {
            "paper_title": "Sabiá-2: A new generation of portuguese large language models.",
            "rating": 2,
            "sanitized_title": "sabiá2_a_new_generation_of_portuguese_large_language_models"
        }
    ],
    "cost": 0.016190999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sabiá-3 Technical Report
29 Nov 2024</p>
<p>Hugo Abonizio 
Thales Sales Almeida 
Thiago Laitz 
Roseval Malaquias Junior 
Giovana Kerche Bonás 
Rodrigo Nogueira 
Ramon Pires 
Maritaca Ai 
Sabiá-3 Technical Report
29 Nov 2024414B8960001043143AC288C952FDD154arXiv:2410.12049v2[cs.CL]
This report presents Sabiá-3, our new flagship language model, and Sabiazinho-3, a more cost-effective sibling.The models were trained on a large brazilian-centric corpus.Evaluations across diverse professional and academic benchmarks show a strong performance on Portuguese and Brazil-related tasks.Sabiá-3 shows large improvements in comparison to our previous best of model, Sabia-2 Medium, especially in reasoningintensive tasks.Notably, Sabiá-3's average performance matches frontier LLMs, while it is offered at a three to four times lower cost per token, reinforcing the benefits of domain specialization.</p>
<p>Introduction</p>
<p>This technical report presents the details of the development and evaluation of the Sabiá-3 and Sabiazinho-3 models.We trained them on a large corpus of documents written in Portuguese, with a special focus on Brazil-related resources.Through training, models were exposed to information relevant to Brazilian culture, history, and context.The main objective was to have a specialized model that is aware of the linguistic nuances, societal norms, and regional variations unique to the country.Throughout this report, we show that this specialization allows the models to perform better in knowledge-intensive tasks.</p>
<p>We applied an approach of continual learning by leveraging a "generalist" model that already acquired some level of language understanding and reasoning abilities, and then further trained it on our corpus of high-quality data relevant to the Brazilian context.The development consisted of two main phases: (1) the pre-training phase, in which we further train a pre-trained model on specialized data following a self-supervised learning strategy optimizing for the next token prediction objective, and (2) the post-training phase where the model is tuned to follow instructions and align to human preferences.</p>
<p>Compared to our previous release, Sabiá-2 [5], we have collected a significantly larger volume of data for pre-training.In addition to the scale, we also Considering the challenges of training large scale models, we used TPU v5 accelerators with Jax [7] to perform distributed training.We combined both data and model parallelism to achieve high training throughput and to ensure efficient utilization of TPU hardware.</p>
<p>After pre-training, we employed a mix of human-annotated and synthetically generated data to teach our base model to follow instructions [1,2,15,16,13,8].The instruction tuning data is composed of both human-annotated prompts and synthetically generated instructions targeting specific capabilities.In addition, we also include synthetic examples to induct self-awareness abilities in the model [6].</p>
<p>Evaluation</p>
<p>In this section, we compare Sabiá-3 and Sabiazinho-3 with proprietary and open-source LLMs, listed in Table 1, on various benchmarks.</p>
<p>Multiple-choice Exams</p>
<p>We have developed a benchmark suite to evaluate the Sabiá-3 performance on 96 academic exams, including entry-level, undergraduate, and professional certification exams.Our focus is on multiple-choice questions from various Brazil- For more details about the datasets, we recommend the readers to the Sabiá-2 technical report [5].In addition to the 8 benchmarks introduced in [5], we now incorporated two new multiple-choice benchmarks:</p>
<p>• ENAM: The examination aims to qualify law graduates interested in participating in magistracy selection processes promoted by federal regional courts, labor courts, military courts, and courts of the states and the Federal District and territories.The benchmark covers the first edition of the exam, administered on April 14, 2024, and its subsequent re-application one month later with a different set of questions.There are 160 questions in total.Figure 2 compares the performance of Sabiá-3, Sabiá-2 Medium, and GPT-4o on the ENADE 2022 and 2023 exams.The results reveal that Sabiá-2 has a higher difficulty level in engineering and economics-related areas, as indicated by its lowest 10 accuracies.In contrast, Sabiá-3 shows a significant improvement in these challenging areas, achieving a 70% reduction in errors compared to Sabiá-2, particularly in Economics and Computer Engineering.Moreover, Sabiá-3 demonstrates a competitive performance compared with GPT-4o, performing equally well or superior in 36 of the 54 Enade exams.Despite these advancements, the results indicate that Sabiá-3 continues to face difficulties in certain areas, notably Chemical Engineering, Mechanical Engineering, and Accounting, where it reached its lowest scores.</p>
<p>Table 2 shows the scores of the models in ten benchmarks derived from academic and professional exams applied after mid-2023.The results illustrate that Sabiá-3 has a comparable level of accuracy to GPT-4o across all 70 exams assessed.GPT-4o exhibits an advantage particularly in the Medicine benchmarks, while Sabiá-3 demonstrates a notable performance in the CPNU exams.Furthermore, Sabiá-3 surpasses Llama-3.1 405B by 3 percentage points in accuracy, performing equally or better in 63% of the exams.Claude 3.5 Sonnet achieves the highest average accuracy, although it presents a higher risk of data contamination due to its training data cutoff date.</p>
<p>In Figure 1, we show the cost per token as of October 2024 in comparison to the average accuracy on the aforementioned exams.We have plotted a Pareto curve based on generalist models, i.e., the ones not specialized in the Brazilian context.Sabiá-3 and Sabiazinho-3 stay above this frontier, underscoring the cost-quality benefits that result from domain specialization.</p>
<p>Llama-3.1 Llama-3.1 GPT-4o GPT-4o Claude 3.</p>
<p>Conversation Capabilities</p>
<p>This section presents the benchmark we use to evaluate the Sabiá-3 capabilities to engage in dialogues.</p>
<p>In our previous work [5], we introduced the Brazilian Chat Evaluation (BRACEval), a benchmark inspired on MT-Bench [19] and designed to evaluate AI assistants' performance in following instructions, engaging in multi-turn dialogues, and demonstrating knowledge specific to Brazil.BRACEval consists of 150 multi-turn questions distributed across 13 categories, including culturally relevant topics such as writing, roleplay, extraction, humanities, entity, and contradiction analysis, as well as universal categories like abstention, harmful content, reasoning, math, and coding.LLM that is given the prompt, the answer from Sabiá-3 and another competitor model and decides which one won or a tie.We address position bias by calling the judge twice with the order of two answers swapped.</p>
<p>We use GPT-4-turbo (more specifically, gpt-4-1106-preview) as the judge because, in pairwise evaluations against other models like Claude 3.5 Sonnet and GPT-4o, all judges -including Claude 3.5 Sonnet, GPT-4-turbo, and GPT-4o -consistently preferred the responses from GPT-4-turbo.</p>
<p>Sabiá-3 shows a significant improvement over Sabiá-2 Medium, despite still having a high percentage of losses against more expensive models, such as GPT-4o and Claude 3.5 Sonnet.A noteworthy result is GPT-4o Mini, which, despite its lower per-token price, achieves a higher win rate compared to the more expensive Llama-3.1 405B in this benchmark.</p>
<p>Figure 4 provides a visual representation of the performance of Sabiá-3 relative to other models, across each BRACEval category.We measure the performance using the adjusted win rate, where the winner model receives one point, and in the case of a tie, both models earn 0.5 points each.Across most categories, Sabiá-3 outperforms Sabiá-2 Medium and performs comparably to the more expensive Llama-3.1 405B model.However, it performs poorly compared to GPT-4o, particularly in coding, math, and writing.This result is consistent regardless of the judge model; using Claude 3.5 Sonnet as a judge yields similar win rates, which indicates that the result is not influenced by the use of GPT-4-turbo as the judge.</p>
<p>Figure 4: Category-wise adjusted win rates of Sabiá-3 against other models on BRACEval, with 0.5 representing a tie; models that score below it are worse than Sabiá-3; above it, competitor models are superior.</p>
<p>Instruction-Following Capabilities</p>
<p>One relevant benchmark to assess LLM abilities to follow instructions is the IFEval [20].IFEval consists of approximately 500 prompts including instructions that are veriafiable programmatically such as "write in more than 400 words".It uses two main metrics: strict and loose.In the strict metric, the model's response is checked against a set of verifiable instructions to determine if it followed all of them.If so, the model receives a point.If any part of the instruction is not followed exactly as specified, the response is deemed incorrect and the model does not receive a point.The loose metric is more lenient as it considers variations and common transformations that might occur in the model's response.For example, if an instruction requires a specific phrase to be included at the end of an email, but the model includes that phrase with additional text formatting like bold or italics, the loose metric might still consider the instruction to be followed, whereas the strict metric would not.</p>
<p>Since this benchmark's prompts and target answers are in English, we do not expect our Portuguese-specialized model to yield significant improvements in this scenario.Instead, this benchmark primarily serves to assess how well our model can follow instructions in comparison to state-of-the-art LLMs.</p>
<p>The results presented in Table 3 show that Sabiá-3 and Sabiazinho-3 outperform Sabiá-2 Medium in terms of instruction-following capabilities.However, it lags behind other models evaluated in this study, including cheaper options such as GPT-4o Mini and Llama-3.1 8B.</p>
<p>Model</p>
<p>Long Context Capabilities</p>
<p>Sabiá-3 and Sabiazinho-3 can process up to 32,000 tokens in a sequence.Here we evaluate how well the model uses this capacity through the Needle-in-the-Haystack (NIAH) benchmark [9], which measures the model's ability to find answers within a long, unrelated context.The test involves inserting a small piece of text within a larger context ranging from 1,000 to 32,000 tokens, at various depths from 0% to 100%.Since we are focusing on evaluating the performance of an LLM on Portuguese tasks, we adapt this benchmark using the book Dom Casmurro1 as the context.In a random spot within the text, we insert as a needle a sentence that says "O número mágico de Campinas é {random number}."("The magic number of Campinas is {random number}."),replacing {random number} with a 6-digit number.Then, we provide this modified context along with the question "Qual o número mágico de Campinas?"("What is the magic number of Campinas?") to the model.Using regular expression, we check whether the model correctly outputs the random number as the answer.</p>
<p>As depicted in Figures 5 and 6, Sabiá-3 exhibited a perfect recall in our Portuguese version of the NIAH benchmark and Sabiazinho-3 reached a recall of 99.48%, consistently locating relevant information within long contexts.We acknowledge, however, that the task, due to its simplicity, may not fully capture the long-context capabilities of language models.Therefore, for a better measure of long-range comprehension skills, we need Portuguese versions of more challenging benchmarks, such as the QuALITY benchmark [14], which tests a model's ability to answer questions about books.</p>
<p>Function Calling Capabilities</p>
<p>The Berkeley Function Calling Leaderboard (BFCL) [17] is a benchmark for evaluating the LLM's capabilities in executing function calls and using tools effectively.The benchmark is updated periodically, and comprises three distinct subsets:</p>
<ol>
<li>Non-live: An expert-curated collection of question-function-answer pairs, encompassing multiple languages and complex use cases, designed to challenge the models' ability to understand and apply functions accurately.</li>
</ol>
<p>The examples are categorized into Abstract Syntax Tree (AST) analysis, measuring whether the function names, arguments, and parameter types match the expected response; and executable verification (Exec), which executes the function and verifies whether the output is correct.</p>
<ol>
<li>
<p>Live: User-contributed examples that reflect real-world applications, ensuring that the models are tested against scenarios that mirror everyday tasks.</p>
</li>
<li>
<p>Multi turn: Multi-turn and multi-step function calling tasks that simulate agentic behaviors, requiring models to plan execution strategies, request information, and manage a sequence of function invocations to complete a task.</p>
</li>
</ol>
<p>The benchmark also includes two categories to measure hallucinations: irrelevance detection, when none of the provided functions is relevant to the query and should not be invoked; and relevance, when at least one of the functions should be invoked.</p>
<p>To evaluate the models we use the original examples in English.Table 4 shows the results of Sabiá-3, Sabiazinho-3, and other proprietary LLMs.The results indicate that Sabiá-3 ranks competitively alongside top-performing models.It outperforms in the Non-live categories but requires enhancements in multi-turn and multi-step tasks.For instance, increasing the support for more than 32,000 tokens could improve in one of the multi-turn categories.It also needs to improve in the irrelevance category.</p>
<p>Agentic Performance</p>
<p>Recent advancements in the ability of LLMs on instruction-following, function calling, and reason in long contexts have enabled the development of LLMbased agents.These agents can automatically execute complex tasks, such as developing code [18,4], managing everyday user tasks on operating systems [11,12], and orchestrating robots [3].</p>
<p>To evaluate an LLM in agentic tasks, it is necessary to integrate it with a framework with modules for memory, action, and reasoning.This setup allows the LLM to interact with responsive environments through tools using natural language.To this end, we use AgentBench [10], a framework designed to evaluate the agentic capabilities of LLMs across eight distinct environments.It tests the model's performance in multi-turn open-ended generation scenarios involving coding, gaming, and web.</p>
<p>Here are the eight environments where the agent LLM performs tasks in AgentBench:</p>
<p>• Operating System (OS): Interacts with a real Ubuntu terminal, executing bash scripts to retrieve information about the virtual OS.</p>
<p>• Database (DB): Runs SQL queries on a database to answer high-level questions about multiple tables.</p>
<p>• Knowledge Graph (KG): Navigates a knowledge graph with 45 million entities and 3 billion facts to extract high-level insights about relationships between entities.</p>
<p>• Web Shopping (WS): Navigates an online shopping environment to make purchases from a web page.</p>
<p>• Web Browsing (WB): Browses web pages across various domains to extract information or perform high-level actions.</p>
<p>• Digital Card Game (DCG): Plays a game against a rule-based agent, analyzing cards with different abilities to defeat the opponent.</p>
<p>• Lateral Thinking Puzzle (LTP): Solves riddles by asking questions where another agent LLM (GPT-3.5 Turbo) can only respond with "yes", "no", or "irrelevant".</p>
<p>• House-Holding (HH): Explores a virtual house, interacting with objects and performing high-level tasks.</p>
<p>Table 5 presents the results, using the same metrics defined in the Agent-Bench paper [10].Each environment metric ranges from 0 to 100, with higher scores indicating better performance.We report the Overall Average (OA) across all environments as our primary metric.Due to the variation in mean scores across different environments, OA is the weighted average of tasks scores, normalized to a scale from 0 to 10, where the weights are inversely proportional to the average score obtained by the models evaluated in the AgentBench paper.That is, harder tasks will have a higher weight.</p>
<p>Even though AgentBench is in English, Sabiá-3 shows competitive performance compared to models primarily trained in this language, achieving an OA of 2.9, comparable to GPT-4o Mini.Notably, Sabiá-3 shows an improvement of 2 OA over its predecessor, Sabiá-2 Medium.Sabiá-3 particularly excels in web-grounded agentic tasks, which involve navigating through web pages to extract relevant information and perform high-level actions.It achieves the top performance in WS, with a score of 62.5, and the second-best performance in WB.However, there's still improvement to be done in code-grounded environments such as OS and DB.</p>
<p>Conclusion</p>
<p>We introduced Sabiá-3 and Sabiazinho-3, a family of language models focused on Brazil-related content.They improve upon our earlier Sabiá-2 models, especially in handling long texts, reasoning, and coding.Despite rivaling state-of-the-art proprietary and open-source models on knowledge-intensive tasks while having a lower per token cost, Sabiá-3 and Sabiazinho-3 still have room for improvement in multi-step tasks and following instructions accurately, both of which we plan to address in future work.</p>
<p>Figure 1 :
1
Figure 1: Price in USD per million tokens, considering an equal proportion of input and output tokens, versus performance on 70 Brazilian exams (in Portuguese).The dashed curve represents a Pareto frontier for general-purpose LLMs such as Llama 3.1 and GPT-4o, which Sabiá-3 and Sabiazinho-3 surpass due to their domain specialization.</p>
<p>Figure 2 :
2
Figure 2: Accuracies of Sabiá-3, Sabiá-2 Medium and GPT-4o on Enade 2022 and 2023 exams, ordered from low to high based on Sabiá-2 Medium performance.Sabiá-3 outperforms Sabiá-2 Medium on 76% of the exams.</p>
<p>Figure 3 :
3
Figure 3: Win, tie and loss rates for Sabiá-3 against other LLMs on the BRACEval conversation benchmark according to GPT-4-turbo as a judge.</p>
<p>Figure 3 compares
3
Figure 3 compares Sabiá-3 with other LLMs on BRACEval.The judge is an</p>
<p>Figure 5 :
5
Figure 5: Performance of the Sabiá-3 model in the Portuguese-adapted Needlein-the-Haystack (NIAH) benchmark.</p>
<p>Figure 6 :
6
Figure 6: Performance of the Sabiazinho-3 model in the Portuguese-adapted Needle-in-the-Haystack (NIAH) benchmark.</p>
<p>Table 1 :
1
Overview of the models evaluated in this work, with pricing information as of October 14, 2024.The API pricing for Llama 3.1 is from together.ai.
KnowledgeUSD per 1M tokensModelVersionCutoffInputOutputSabiá-32024-09-09Mid-20230.831.67Sabiazinho-32024-11-13Mid-20230.170.50Sabiá-2 Medium2024-03-13Mid-2023DeprecatedGPT-4o2024-08-06October 20232.5010.00GPT-4o Mini2024-07-18October 20230.150.60Claude 3.5 Sonnet 2024-06-20April 20243.0015.00Llama 3.1 8BInstructDecember 20230.180.18Llama 3.1 70BInstructDecember 20230.880.88Llama 3.1 405BInstructDecember 20233.503.50ian educational assessments, particularly those administered after the middleof 2023 to mitigate the risk of data contamination. However, ENADE is anexception, including spans of 2022 and 2023 to cover a wide range of disciplines.</p>
<p>Table 2 :
2
Accuracy on academic benchmarks.The average results (last row) cover all the 70 exams applied after the middle of 2023, which is the knowledge cutoff date of Sabiá-3.Claude 3.5's cutoff date is April of 2024, so exams released before that date might be present in its training data.</p>
<p>Table 3 :
3
Results on the IFeval benchmark.
Loose StrictGPT-4o88.483.7GPT-4o Mini83.981.0Llama-3.1 405B88.085.6Llama-3.1 70B87.983.1Llama-3.1 8B80.576.6Sabiá-378.073.8Sabiazinho-370.065.4Sabiá-2 Medium41.838.6</p>
<p>Table 4 :
4
Results on the BFCL benchmark ordered by overall accuracy.</p>
<p>Table 5 :
5
Results on AgentBench.
ModelsOACode-groundedWeb-groundedGame-groundedOS DB KG WSWBDCG LTP HHGPT-4o3.7 38.9 52.7 58.3 60.012.073.111.9 72.0GPT-4o Mini3.1 29.7 51.0 38.6 57.36.089.07.540.0Llama-3.1 405B3.8 43.0 58.3 48.1 55.935.058.69.682.0Llama-3.1 70B3.4 43.0 56.7 42.9 44.723.078.17.254.0Llama-3.1 8B1.5 16.7 0.0 15.0 54.117.055.10.222.0Sabiá-32.9 29.2 28.7 36.7 62.530.057.57.658.0Sabiazinho-31.6 25.0 22.9 19.3 50.023.00.50.140.0Sabiá-2 Medium 0.9 12.5 2.64.9 44.620.00.73.020.0
https://machadodeassis.net/texto/dom-casmurro/11503</p>
<p>Open foundation models by 01. Alex Ai, Bei Young, Chao Chen, Chengen Li, Ge Huang, Guanwei Zhang, Heng Zhang, Jiangcheng Li, Jianqun Zhu, Jing Chen, Kaidong Chang, Peng Yu, Qiang Liu, Shawn Liu, Senbin Yue, Shiming Yang, Tao Yang, Wen Yu, Wenhao Xie, Xiaohui Huang, Xiaoyi Hu, Xinyao Ren, Pengcheng Niu, Yuchi Nie, Yudong Xu, Yue Liu, Yuxuan Wang, Zhenyu Cai, Zhiyuan Gu, Zonghong Liu, Dai, Yi, ai2024</p>
<p>. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai, Martin Cai, Caio César, Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo De Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou, 2024Phi-3 technical report: A highly capable language model locally on your phone</p>
<p>Autort: Embodied foundation models for large scale orchestration of robotic agents. Debidatta Michael Ahn, Chelsea Dwibedi, Montse Finn, Keerthana Gonzalez Arenas, Karol Gopalakrishnan, Brian Hausman, Alex Ichter, Nikhil Irpan, Ryan Joshi, Sean Julian, Isabel Kirmani, Edward Leal, Sergey Lee, Yao Levine, Isabel Lu, Sharath Leal, Kanishka Maddineni, Dorsa Rao, Pannag Sadigh, Pierre Sanketi, Quan Sermanet, Stefan Vuong, Fei Welker, Ted Xia, Peng Xiao, Steve Xu, Zhuo Xu, Xu, 2024</p>
<p>. A I Cognition, Devin, 2024</p>
<p>Sabiá-2: A new generation of portuguese large language models. Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires, 2024</p>
<p>Taken out of context: On measuring situational awareness in llms. Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, Owain Evans, 2023</p>
<p>JAX: composable transformations of Python+NumPy programs. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake Van-Derplas, Skye Wanderman-Milne, Qiao Zhang, 2018</p>
<p>. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Roziere, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mc-Connell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, David Livshits, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Dieuwke Perino, Egor Hupkes, Ehab Lakomkin, Elina Albadawy, Emily Lobanova, Eric Michael Dinan, Filip Smith, Frank Radenovic, Gabriel Zhang, Gabrielle Synnaeve, Georgia Lee, Graeme Lewis Anderson, Gregoire Nail, Guan Mialon, Guillem Pang, Hailey Cucurell, Hannah Nguyen, Hu Korevaar, Hugo Xu, Iliyan Touvron, Zarov, Arrieta Imanol, Isabel Ibarra, Ishan Kloumann, Ivan Misra, Jana Evtimov, Jason Vranes, Jay Park, Jeet Mahadeokar, Jelmer Shah, Jennifer Van Der Linde, Jenny Billock, Jenya Hong, Jeremy Lee, Jianfeng Fu, Jianyu Chi, Jiawen Huang, Jie Liu, Jiecao Wang, Joanna Yu, Joe Bitton, Jongsoo Spisak, Joseph Park, Joshua Rocca, Joshua Johnstun, Junteng Saxe, Jia, Jan Geffert,. 2024Kartikeya UpasaniJade Copet, Jaewon Lee; Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stoneand 432 others. The llama 3 herd of models</p>
<p>Needle in a haystack -pressure testing llms. Gregory Kamradt, 2023</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. 2023</p>
<p>Aios: Llm agent operating system. Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang, 2024</p>
<p>Bo Nvidia, Niket Adler, Ashwath Agarwal, Dong H Aithal, Pallab Anh, Annika Bhattacharya, Jared Brundyn, Bryan Casper, Sharon Catanzaro, Jonathan Clay, Sirshak Cohen, Ayush Das, Olivier Dattagupta, Leon Delalleau, Yi Derczynski, Daniel Dong, Ellie Egert, Aleksander Evans, Denys Ficek, Shaona Fridman, Boris Ghosh, Igor Ginsburg, Tomasz Gitman, Robert Grzegorzek, Jining Hero, Vibhu Huang, Joseph Jawa, Aastha Jennings, John Jhunjhunwala, Sadaf Kamalu, Oleksii Khan, Patrick Kuchaiev, Hui Legresley, Jiwei Li, Zihan Liu, Eileen Liu, Ameya Long, Somshubra Sunil Mahabaleshwarkar, James Majumdar, Miguel Maki, Maer Martinez, Ivan Rodrigues De Melo, Deepak Moshkov, Sean Narayanan, Jesus Narenthiran, Phong Navarro, Osvald Nguyen, Vahid Nitski, Guruprasad Noroozi, Christopher Nutheti, Jupinder Parisien, Parmar ; Hao, Zhilin Wang, Jiaxuan Wang, Jiaqi You, Jimmy Zeng, Jing Zhang, Vivienne Zhang, Yian Zhang, Zhang, Mostofa Patwary. Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik, Sanjeev Sabavat, Jane Polak Satheesh, Jason Scowcroft, Pavel Sewall, Gerald Shamis, Mohammad Shen, Dave Shoeybi, Misha Sizer, Felipe Smelyanskiy, Soares, 2024Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal,and Chen Zhu. Nemotron-4 340b technical report</p>
<p>Quality: Question answering with long input texts, yes!. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze, Eric Cheng, Purvi Ni, Patrick Shah, Betty Kane, Manaal Chan, Aliaksei Faruqui, Hanzhao Severyn, Yaguang Lin, Yong Li, Abe Cheng, Mahdis Ittycheriah, Mia Mahdieh, Pei Chen, Dustin Sun, Sumit Tran, Balaji Bagri, Jeremiah Lakshminarayanan, Andras Liu, Fabian Orban, Hao Güra, Xinying Zhou, Aurelien Song, Harish Boffy, Steven Ganapathy, Hyunjeong Zheng, Ágoston Choe, Tao Weisz, Yifeng Zhu, Siddharth Lu, Jarrod Gopal, Maciej Kahn, Jeff Kula, Rushin Pitman, Emanuel Shah, Taropa, Al Majd, Martin Merey, Zhifeng Baeuml, Laurent El Chen, Yujing Shafey, Olcan Zhang, George Sercinoglu, Enrique Tucker, Maxim Piqueras, Iain Krikun, Nikolay Barr, Ivo Savinov, Becca Danihelka, Anaïs Roelofs, Anders White, Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Alexandre Sygnowski, Frechette, 2024Charlotte Smith, Laura CulpLev Proleev, Yi Luan, Xi Chen, and 1250 others. Gemini: A family of highly capable multimodal models</p>
<p>. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Giuseppe Pier, Aakanksha Sessa, Adam Chowdhery, Aditya Roberts, Alex Barua, Alex Botev, Ambrose Castro-Ros, Amélie Slone, Andrea Héliou, Anna Tacchetti, Antonia Bulanova, Beth Paterson, Bobak Tsai, Charline Le Shahriari, Christopher A Lan, Clément Choquette-Choo, Daniel Crepy, Daphne Cer, David Ippolito, Elena Reid, Eric Buchatskaya, Eric Ni, Geng Noland, George Yan, George-Christian Tucker, Grigory Muraru, Henryk Rozhdestvenskiy, Ian Michalewski, Ivan Tenney, Jacob Grishchenko, James Austin, Jane Keeling, Jean-Baptiste Labanowski, Jeff Lespiau, Jenny Stanway, Jeremy Brennan, Johan Chen, Justin Ferret, Justin Chiu, Katherine Mao-Jones, Kathy Lee, Katie Yu, Lars Lowe Millican, Lisa Sjoesund, Lucas Lee, Machel Dixon, Maciej Reid, Mateo Miku La, Michael Wirth, Nikolai Sharman, Nithum Chinaev, Olivier Thain, Oscar Bachem, Oscar Chang, Paige Wahltinez, Paul Bailey, Petko Michel, Rahma Yotov, Ramona Chaabouni, Reena Comanescu, Rohan Jana, Ross Anil, Ruibo Mcilroy, Ryan Liu, Mullins, L Samuel, Sebastian Smith, Sertan Borgeaud, Sholto Girgin, Shree Douglas, Siamak Pandya, Soham Shakeri, Ted De, Tom Klimenko, Vlad Hennigan, Wojciech Feinberg, Yu Stokowiec, Zafarali Hui Chen, Zhitao Ahmed, Tris Gong, Ludovic Warkentin, Minh Peran, Giang, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology. 2024Koray Kavukcuoglu, Demis HassabisZoubin Ghahramani</p>
<p>Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie, Tianjun Ji, Zhang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, Berkeley function calling leaderboard. 2024</p>
<p>Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin, 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Instruction-following evaluation for large language models. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4268 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4268</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4268</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-279402741</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.12385v1.pdf" target="_blank">Recent Advances and Future Directions in Literature-Based Discovery</a></p>
                <p><strong>Paper Abstract:</strong> The explosive growth of scientific publications has created an urgent need for automated methods that facilitate knowledge synthesis and hypothesis generation. Literature-based discovery (LBD) addresses this challenge by uncovering previously unknown associations between disparate domains. This article surveys recent methodological advances in LBD, focusing on developments from 2000 to the present. We review progress in three key areas: knowledge graph construction, deep learning approaches, and the integration of pre-trained and large language models (LLMs). While LBD has made notable progress, several fundamental challenges remain unresolved, particularly concerning scalability, reliance on structured data, and the need for extensive manual curation. By examining ongoing advances and outlining promising future directions, this survey underscores the transformative role of LLMs in enhancing LBD and aims to support researchers and practitioners in harnessing these technologies to accelerate scientific innovation.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4268.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4268.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON (Scientific inspiration machines optimized for novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that grounds literature-based discovery in natural language contexts and iteratively refines generated research ideas until desired novelty is achieved, producing full-sentence hypothesis suggestions rather than isolated concept links.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciMON: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s) (framework uses LLM-based generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Grounds LBD in natural-language contexts to narrow the generative space; iteratively generates and refines idea suggestions derived from published literature until sufficient novelty is achieved; outputs complete sentences (contextualized hypotheses) instead of only conceptual links. The paper reports direct comparison of SciMON outputs to GPT-4 outputs and claims higher originality and conceptual depth. No detailed prompting protocol, model size, or retrieval pipeline is specified in this survey beyond the high-level iterative refinement and NL grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>general scientific literature (designed as cross-domain LBD)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>contextualized scientific hypotheses and research ideas (complete-sentence hypotheses and concept-level generative insights rather than formal quantitative laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>The paper does not provide verbatim example laws; it reports that SciMON generates complete-sentence research hypotheses that are more original and conceptually deeper than GPT-4 outputs (no concrete examples provided in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparative qualitative evaluation against GPT-4 as reported by the SciMON authors (authors' reported assessments of originality and conceptual depth).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4 (authors report SciMON ideas were more original and deeper), specifics of evaluation metrics or statistical tests are not reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Grounding LBD in natural language and iteratively refining generated ideas narrows the generative space and yields more original, conceptually deep hypotheses than direct LLM generation (per authors' report); shifting from concept-pair links to full-sentence hypothesis outputs provides richer, more contextualized representations for LBD.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey notes lack of detailed evaluation metrics in the reported comparison; general LLM/LBD issues remain (validation, scalability, and reliability of generated hypotheses); the survey does not report specifics about hallucination rates, reproducibility of SciMON outputs, or quantitative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recent Advances and Future Directions in Literature-Based Discovery', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4268.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4268.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific generative pre-trained transformer trained on biomedical corpora (e.g., PubMed) for biomedical text generation and mining, used for literature retrieval, summarization, and question answering in scientific contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioGPT: Generative pre-trained transformer for biomedical text generation and mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioGPT (domain-specific LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BioGPT (domain-pretrained generative transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Domain-specific pretraining on biomedical corpora to improve retrieval, summarization and QA for biomedical literature; used as a component in literature-centric workflows to surface and synthesize information from scientific texts. The survey mentions BioGPT as an example of a model trained on PubMed that is effective for literature retrieval and summarization; no prompt recipes or multi-step extraction pipelines are detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>biomedical literature (PubMed-scale corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>high-level scientific insights, summaries, and hypotheses relevant to biomedical research (the survey frames BioGPT as effective for retrieval/summarization rather than as explicitly extracting formal laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Not provided in the survey; BioGPT is cited as effective at literature retrieval, document summarization, and question answering but the survey does not quote extracted principles or laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this survey excerpt (BioGPT referenced as an effective domain model; evaluation details are in the cited BioGPT paper, not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this survey (BioGPT mentioned as state-of-the-art domain model; no direct baseline comparisons reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain-specific LLMs trained on scientific corpora (e.g., BioGPT) are particularly effective at tasks that support discovery workflows — retrieval, summarization, and QA — thereby facilitating access to and synthesis of evidence needed for hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey highlights general LLM limitations applicable to domain models: scalability and validation challenges, and the broader needs for interpretability and careful benchmarking when used in scientific-discovery pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recent Advances and Future Directions in Literature-Based Discovery', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4268.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4268.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGLM (Scientific General Language Model / training paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scientific language model trained with self-reflective instruction annotation and tuning, designed to enhance domain-adapted reasoning and instruction-following for scientific tasks such as retrieval, summarization, and hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciGLM: Training scientific language models with self-reflective instruction annotation and tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciGLM (domain-specific LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciGLM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Self-reflective instruction annotation and tuning on scientific corpora to improve reasoning and instruction-following in scientific tasks; cited as a model trained on domain-specific corpora (e.g., PubMed, arXiv) and effective for literature retrieval, summarization and question answering. The survey does not detail exact prompting strategies or pipeline for extracting qualitative laws, only that SciGLM exemplifies domain-tuned LLMs used in scientific workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>scientific literature across domains (examples include PubMed and arXiv corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>contextualized scientific insights, hypothesis statements, and synthesized findings useful for scientific reasoning (survey frames SciGLM as effective for retrieval and synthesis tasks rather than enumerating formal laws).</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>None provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in the survey excerpt; referenced as effective for retrieval/summarization/QA but evaluation details are in the original SciGLM reference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-reflective instruction annotation and domain tuning improves LLM usefulness for scientific tasks, enabling better literature retrieval, summarization, and question answering — capabilities supportive of hypothesis generation in LBD contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey notes general, cross-model challenges: scalability of applying LLMs across large corpora, validation of generated hypotheses, and the emerging nature of benchmarking for LLM-based scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recent Advances and Future Directions in Literature-Based Discovery', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4268.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4268.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-powered LBD (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model–powered Literature-Based Discovery (general approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods integrating pretrained LLMs/PLMs into LBD pipelines to enhance retrieval, contextual grounding, hypothesis generation, and synthesis of literature into coherent hypotheses or research gap identifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (e.g., GPT-4, BioGPT, SciGLM) as referenced</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-powered LBD (unnamed, multiple approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Survey-level description: LLMs are integrated into LBD as preprocessing tools (e.g., to generate semantic triples), core components for downstream tasks (NER, relation extraction), or as generative engines that produce full-sentence hypotheses and perform reasoning across documents. Approaches include: (i) using PLMs/LLMs to extract predications/triples to build knowledge graphs (KGs) subsequently used for KGC; (ii) using LLMs directly to retrieve, summarize, and synthesize documents into hypotheses; and (iii) grounding generation in natural-language contexts (SciMON-style) with iterative refinement. The survey does not provide standardized prompting templates, specific multi-step retrieval-augmented procedures, nor parameter settings — it reports conceptual integrations and representative exemplars from the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>primarily biomedicine (due to PubMed availability) but potentially cross-domain (social sciences example cited); LLM-powered LBD is argued to broaden applicability beyond biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>qualitative scientific hypotheses, research-gap identifications, contextualized conceptual associations, and synthesized statements (complete-sentence hypotheses) rather than formal mechanistic mathematical laws; typically causal/associational hypotheses and conceptual generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>The survey does not list verbatim extracted laws; it gives the canonical example of Swanson's Fish oil → Blood viscosity → Raynaud's disease as the kind of indirect association LBD seeks to uncover and notes that LLMs can generate richer sentence-level hypotheses beyond such ABC triples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Varied across cited works: author-reported comparative qualitative assessments (e.g., SciMON vs GPT-4), task-level evaluations in downstream components (e.g., NER, relation extraction) when PLMs are used for preprocessing, and domain-specific evaluations in applications like drug-repurposing (cited works evaluate with KGC metrics and case-study validation). The survey emphasizes the need for better benchmarking and reproducibility but does not consolidate a single evaluation standard for LLM-powered law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in the literature to classical LBD methods (ABC/co-occurrence KGs), PLM-based pipelines, and off-the-shelf LLM outputs (e.g., GPT-4) — specific quantitative comparisons are not provided in this survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs significantly expand LBD capabilities by enabling contextualized, sentence-level hypothesis generation, improved retrieval and synthesis, and cross-domain applicability when domain-tuned; integrating LLMs with KGs and DL improves expressivity and potential for discovery. However, rigorous benchmarking, reproducibility, and explainability remain open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey-identified limitations include validation and scalability of LLM outputs, reliance on structured resources (UMLS) for many pipelines, need for extensive manual curation for validation, limited interpretability of deep models, and emerging issues around consistent benchmarking and reproducibility. The survey also notes general concerns about model validation and trustworthiness though it does not enumerate measured hallucination rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recent Advances and Future Directions in Literature-Based Discovery', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4268.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4268.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (as comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose large language model cited as a baseline comparator for generative hypothesis outputs in at least one cited LBD study (SciMON), used to benchmark novelty and conceptual depth of generated scientific ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned in the survey as a point of comparison for generative hypothesis quality; the survey does not describe deployment details, prompting, or pipelines used in the cited comparison beyond noting SciMON's outputs were judged more original/deeper than GPT-4 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>general scientific text generation / cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>used as a generator of hypotheses in comparative evaluations; not described as extracting formal laws in the surveyed text.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>None provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Used as a baseline in author-reported qualitative comparisons (e.g., SciMON vs GPT-4); no formal metrics provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 represents a strong general-purpose generative baseline, but domain-grounded, iterative frameworks (e.g., SciMON) can outperform GPT-4 on novelty and conceptual depth for hypothesis generation according to the cited authors' reports.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>As with other LLMs, issues include validation of generated hypotheses, interpretability, and the need for careful benchmarking; the survey does not provide model-specific error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recent Advances and Future Directions in Literature-Based Discovery', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SciMON: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
                <li>BioGPT: Generative pre-trained transformer for biomedical text generation and mining <em>(Rating: 2)</em></li>
                <li>SciGLM: Training scientific language models with self-reflective instruction annotation and tuning <em>(Rating: 2)</em></li>
                <li>LLM4SR: A survey on large language models for scientific research <em>(Rating: 1)</em></li>
                <li>Large language models: A survey of their development, capabilities, and applications <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4268",
    "paper_id": "paper-279402741",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "SciMON",
            "name_full": "SciMON (Scientific inspiration machines optimized for novelty)",
            "brief_description": "A framework that grounds literature-based discovery in natural language contexts and iteratively refines generated research ideas until desired novelty is achieved, producing full-sentence hypothesis suggestions rather than isolated concept links.",
            "citation_title": "SciMON: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s) (framework uses LLM-based generation)",
            "model_size": null,
            "method_name": "SciMON",
            "method_description": "Grounds LBD in natural-language contexts to narrow the generative space; iteratively generates and refines idea suggestions derived from published literature until sufficient novelty is achieved; outputs complete sentences (contextualized hypotheses) instead of only conceptual links. The paper reports direct comparison of SciMON outputs to GPT-4 outputs and claims higher originality and conceptual depth. No detailed prompting protocol, model size, or retrieval pipeline is specified in this survey beyond the high-level iterative refinement and NL grounding.",
            "number_of_papers": null,
            "domain_or_field": "general scientific literature (designed as cross-domain LBD)",
            "type_of_laws_extracted": "contextualized scientific hypotheses and research ideas (complete-sentence hypotheses and concept-level generative insights rather than formal quantitative laws)",
            "example_laws_extracted": "The paper does not provide verbatim example laws; it reports that SciMON generates complete-sentence research hypotheses that are more original and conceptually deeper than GPT-4 outputs (no concrete examples provided in this survey).",
            "evaluation_method": "Comparative qualitative evaluation against GPT-4 as reported by the SciMON authors (authors' reported assessments of originality and conceptual depth).",
            "performance_metrics": null,
            "comparison_baseline": "Compared to GPT-4 (authors report SciMON ideas were more original and deeper), specifics of evaluation metrics or statistical tests are not reported in this survey.",
            "key_findings": "Grounding LBD in natural language and iteratively refining generated ideas narrows the generative space and yields more original, conceptually deep hypotheses than direct LLM generation (per authors' report); shifting from concept-pair links to full-sentence hypothesis outputs provides richer, more contextualized representations for LBD.",
            "challenges_limitations": "Survey notes lack of detailed evaluation metrics in the reported comparison; general LLM/LBD issues remain (validation, scalability, and reliability of generated hypotheses); the survey does not report specifics about hallucination rates, reproducibility of SciMON outputs, or quantitative performance.",
            "uuid": "e4268.0",
            "source_info": {
                "paper_title": "Recent Advances and Future Directions in Literature-Based Discovery",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT",
            "brief_description": "A domain-specific generative pre-trained transformer trained on biomedical corpora (e.g., PubMed) for biomedical text generation and mining, used for literature retrieval, summarization, and question answering in scientific contexts.",
            "citation_title": "BioGPT: Generative pre-trained transformer for biomedical text generation and mining",
            "mention_or_use": "mention",
            "model_name": "BioGPT (domain-specific LLM)",
            "model_size": null,
            "method_name": "BioGPT (domain-pretrained generative transformer)",
            "method_description": "Domain-specific pretraining on biomedical corpora to improve retrieval, summarization and QA for biomedical literature; used as a component in literature-centric workflows to surface and synthesize information from scientific texts. The survey mentions BioGPT as an example of a model trained on PubMed that is effective for literature retrieval and summarization; no prompt recipes or multi-step extraction pipelines are detailed in the survey.",
            "number_of_papers": null,
            "domain_or_field": "biomedical literature (PubMed-scale corpora)",
            "type_of_laws_extracted": "high-level scientific insights, summaries, and hypotheses relevant to biomedical research (the survey frames BioGPT as effective for retrieval/summarization rather than as explicitly extracting formal laws)",
            "example_laws_extracted": "Not provided in the survey; BioGPT is cited as effective at literature retrieval, document summarization, and question answering but the survey does not quote extracted principles or laws.",
            "evaluation_method": "Not specified in this survey excerpt (BioGPT referenced as an effective domain model; evaluation details are in the cited BioGPT paper, not reproduced here).",
            "performance_metrics": null,
            "comparison_baseline": "Not specified in this survey (BioGPT mentioned as state-of-the-art domain model; no direct baseline comparisons reported here).",
            "key_findings": "Domain-specific LLMs trained on scientific corpora (e.g., BioGPT) are particularly effective at tasks that support discovery workflows — retrieval, summarization, and QA — thereby facilitating access to and synthesis of evidence needed for hypothesis generation.",
            "challenges_limitations": "Survey highlights general LLM limitations applicable to domain models: scalability and validation challenges, and the broader needs for interpretability and careful benchmarking when used in scientific-discovery pipelines.",
            "uuid": "e4268.1",
            "source_info": {
                "paper_title": "Recent Advances and Future Directions in Literature-Based Discovery",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SciGLM",
            "name_full": "SciGLM (Scientific General Language Model / training paradigm)",
            "brief_description": "A scientific language model trained with self-reflective instruction annotation and tuning, designed to enhance domain-adapted reasoning and instruction-following for scientific tasks such as retrieval, summarization, and hypothesis generation.",
            "citation_title": "SciGLM: Training scientific language models with self-reflective instruction annotation and tuning",
            "mention_or_use": "mention",
            "model_name": "SciGLM (domain-specific LLM)",
            "model_size": null,
            "method_name": "SciGLM",
            "method_description": "Self-reflective instruction annotation and tuning on scientific corpora to improve reasoning and instruction-following in scientific tasks; cited as a model trained on domain-specific corpora (e.g., PubMed, arXiv) and effective for literature retrieval, summarization and question answering. The survey does not detail exact prompting strategies or pipeline for extracting qualitative laws, only that SciGLM exemplifies domain-tuned LLMs used in scientific workflows.",
            "number_of_papers": null,
            "domain_or_field": "scientific literature across domains (examples include PubMed and arXiv corpora)",
            "type_of_laws_extracted": "contextualized scientific insights, hypothesis statements, and synthesized findings useful for scientific reasoning (survey frames SciGLM as effective for retrieval and synthesis tasks rather than enumerating formal laws).",
            "example_laws_extracted": "None provided in the survey text.",
            "evaluation_method": "Not specified in the survey excerpt; referenced as effective for retrieval/summarization/QA but evaluation details are in the original SciGLM reference.",
            "performance_metrics": null,
            "comparison_baseline": "Not provided in the survey text.",
            "key_findings": "Self-reflective instruction annotation and domain tuning improves LLM usefulness for scientific tasks, enabling better literature retrieval, summarization, and question answering — capabilities supportive of hypothesis generation in LBD contexts.",
            "challenges_limitations": "Survey notes general, cross-model challenges: scalability of applying LLMs across large corpora, validation of generated hypotheses, and the emerging nature of benchmarking for LLM-based scientific discovery.",
            "uuid": "e4268.2",
            "source_info": {
                "paper_title": "Recent Advances and Future Directions in Literature-Based Discovery",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLM-powered LBD (general)",
            "name_full": "Large Language Model–powered Literature-Based Discovery (general approaches)",
            "brief_description": "A class of methods integrating pretrained LLMs/PLMs into LBD pipelines to enhance retrieval, contextual grounding, hypothesis generation, and synthesis of literature into coherent hypotheses or research gap identifications.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various LLMs (e.g., GPT-4, BioGPT, SciGLM) as referenced",
            "model_size": null,
            "method_name": "LLM-powered LBD (unnamed, multiple approaches)",
            "method_description": "Survey-level description: LLMs are integrated into LBD as preprocessing tools (e.g., to generate semantic triples), core components for downstream tasks (NER, relation extraction), or as generative engines that produce full-sentence hypotheses and perform reasoning across documents. Approaches include: (i) using PLMs/LLMs to extract predications/triples to build knowledge graphs (KGs) subsequently used for KGC; (ii) using LLMs directly to retrieve, summarize, and synthesize documents into hypotheses; and (iii) grounding generation in natural-language contexts (SciMON-style) with iterative refinement. The survey does not provide standardized prompting templates, specific multi-step retrieval-augmented procedures, nor parameter settings — it reports conceptual integrations and representative exemplars from the literature.",
            "number_of_papers": null,
            "domain_or_field": "primarily biomedicine (due to PubMed availability) but potentially cross-domain (social sciences example cited); LLM-powered LBD is argued to broaden applicability beyond biomedical literature.",
            "type_of_laws_extracted": "qualitative scientific hypotheses, research-gap identifications, contextualized conceptual associations, and synthesized statements (complete-sentence hypotheses) rather than formal mechanistic mathematical laws; typically causal/associational hypotheses and conceptual generalizations.",
            "example_laws_extracted": "The survey does not list verbatim extracted laws; it gives the canonical example of Swanson's Fish oil → Blood viscosity → Raynaud's disease as the kind of indirect association LBD seeks to uncover and notes that LLMs can generate richer sentence-level hypotheses beyond such ABC triples.",
            "evaluation_method": "Varied across cited works: author-reported comparative qualitative assessments (e.g., SciMON vs GPT-4), task-level evaluations in downstream components (e.g., NER, relation extraction) when PLMs are used for preprocessing, and domain-specific evaluations in applications like drug-repurposing (cited works evaluate with KGC metrics and case-study validation). The survey emphasizes the need for better benchmarking and reproducibility but does not consolidate a single evaluation standard for LLM-powered law extraction.",
            "performance_metrics": null,
            "comparison_baseline": "Compared in the literature to classical LBD methods (ABC/co-occurrence KGs), PLM-based pipelines, and off-the-shelf LLM outputs (e.g., GPT-4) — specific quantitative comparisons are not provided in this survey excerpt.",
            "key_findings": "LLMs significantly expand LBD capabilities by enabling contextualized, sentence-level hypothesis generation, improved retrieval and synthesis, and cross-domain applicability when domain-tuned; integrating LLMs with KGs and DL improves expressivity and potential for discovery. However, rigorous benchmarking, reproducibility, and explainability remain open challenges.",
            "challenges_limitations": "Survey-identified limitations include validation and scalability of LLM outputs, reliance on structured resources (UMLS) for many pipelines, need for extensive manual curation for validation, limited interpretability of deep models, and emerging issues around consistent benchmarking and reproducibility. The survey also notes general concerns about model validation and trustworthiness though it does not enumerate measured hallucination rates.",
            "uuid": "e4268.3",
            "source_info": {
                "paper_title": "Recent Advances and Future Directions in Literature-Based Discovery",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4 (as comparator)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4)",
            "brief_description": "A general-purpose large language model cited as a baseline comparator for generative hypothesis outputs in at least one cited LBD study (SciMON), used to benchmark novelty and conceptual depth of generated scientific ideas.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_size": null,
            "method_name": "",
            "method_description": "Mentioned in the survey as a point of comparison for generative hypothesis quality; the survey does not describe deployment details, prompting, or pipelines used in the cited comparison beyond noting SciMON's outputs were judged more original/deeper than GPT-4 outputs.",
            "number_of_papers": null,
            "domain_or_field": "general scientific text generation / cross-domain",
            "type_of_laws_extracted": "used as a generator of hypotheses in comparative evaluations; not described as extracting formal laws in the surveyed text.",
            "example_laws_extracted": "None provided in the survey.",
            "evaluation_method": "Used as a baseline in author-reported qualitative comparisons (e.g., SciMON vs GPT-4); no formal metrics provided in this survey.",
            "performance_metrics": null,
            "comparison_baseline": "",
            "key_findings": "GPT-4 represents a strong general-purpose generative baseline, but domain-grounded, iterative frameworks (e.g., SciMON) can outperform GPT-4 on novelty and conceptual depth for hypothesis generation according to the cited authors' reports.",
            "challenges_limitations": "As with other LLMs, issues include validation of generated hypotheses, interpretability, and the need for careful benchmarking; the survey does not provide model-specific error rates.",
            "uuid": "e4268.4",
            "source_info": {
                "paper_title": "Recent Advances and Future Directions in Literature-Based Discovery",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SciMON: Scientific inspiration machines optimized for novelty",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "BioGPT: Generative pre-trained transformer for biomedical text generation and mining",
            "rating": 2,
            "sanitized_title": "biogpt_generative_pretrained_transformer_for_biomedical_text_generation_and_mining"
        },
        {
            "paper_title": "SciGLM: Training scientific language models with self-reflective instruction annotation and tuning",
            "rating": 2,
            "sanitized_title": "sciglm_training_scientific_language_models_with_selfreflective_instruction_annotation_and_tuning"
        },
        {
            "paper_title": "LLM4SR: A survey on large language models for scientific research",
            "rating": 1,
            "sanitized_title": "llm4sr_a_survey_on_large_language_models_for_scientific_research"
        },
        {
            "paper_title": "Large language models: A survey of their development, capabilities, and applications",
            "rating": 1,
            "sanitized_title": "large_language_models_a_survey_of_their_development_capabilities_and_applications"
        }
    ],
    "cost": 0.0123325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Recent Advances and Future Directions in Literature-Based Discovery
14 Jun 2025</p>
<p>Andrej Kastrin andrej.kastrin@mf.uni-lj.si 
University of Ljubljana
LjubljanaSlovenia</p>
<p>Bojan Cestnik bojan.cestnik@temida.si 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>Temida d
LjubljanaSlovenia</p>
<p>Nada Lavrač nada.lavrac@ijs.si 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>Recent Advances and Future Directions in Literature-Based Discovery
14 Jun 2025DBA73BB417A8FE366670E0CC21569698arXiv:2506.12385v1[cs.CL]Artificial intelligenceNatural language processingComputational scientific discoveryLiterature-based discovery
The explosive growth of scientific publications has created an urgent need for automated methods that facilitate knowledge synthesis and hypothesis generation.Literature-based discovery (LBD) addresses this challenge by uncovering previously unknown associations between disparate domains.This article surveys recent methodological advances in LBD, focusing on developments from 2000 to the present.We review progress in three key areas: knowledge graph construction, deep learning approaches, and the integration of pre-trained and large language models (LLMs).While LBD has made notable progress, several fundamental challenges remain unresolved, particularly concerning scalability, reliance on structured data, and the need for extensive manual curation.By examining ongoing advances and outlining promising future directions, this survey underscores the transformative role of LLMs in enhancing LBD and aims to support researchers and practitioners in harnessing these technologies to accelerate scientific innovation.</p>
<p>Introduction</p>
<p>The explosive growth of research publications across various scientific disciplines has led to an overwhelming volume of knowledge, ranging from research articles and monographs to preprints and conference proceedings [4].This proliferation has made it increasingly difficult for researchers to effectively locate, interpret, and synthesize relevant knowledge.As a result, staying current within one's field becomes more challenging, and the risk of missing important findings or inadvertently duplicating existing work rises significantly.Furthermore, the increasing complexity and interdisciplinarity of research further complicate the task of integrating knowledge from multiple sources, and much of the information remains siloed, underutilized, or disconnected.These challenges have led to a rising interest in developing automated methods, particularly those based on natural language processing (NLP), to support hypothesis generation and the discovery of novel scientific insights.</p>
<p>A promising approach to address this problem is literature-based discovery (LBD).LBD, originally introduced by Swanson [29] in the mid-1980s, is an approach designed to generate novel research hypotheses by revealing previously overlooked associations between two complementary and non-interactive sets of scientific literatures.It emerged as a response to the growing difficulty of staying abreast of developments across disparate fields and remains a valuable methodology in the face of ever-expanding scholarly output.</p>
<p>The primary motivation of this article is to provide an overview of current methodological challenges in LBD, survey recent scientific advances, and identify future research directions that align LBD with emerging trends in AI and more broadly computational scientific discovery.We limit the scope to the period between 2000 and early 2025, focusing exclusively on state-of-the-art approaches, as earlier methods have already been comprehensively covered in previous surveys [28,12,32].</p>
<p>The article is organized as follows.Section 2 presents the necessary preliminaries and a concise overview of LBD research.Recent advances in LBD methodologies are examined in Section 3, followed by a discussion of future research directions in Section 4. The article concludes with a synthesis of key findings in Section 5.</p>
<p>Preliminaries and Background</p>
<p>LBD is a subfield of artificial intelligence (AI) at the intersection of information retrieval, NLP and computational scientific discovery, which is dedicated to automating the scientific discovery process.The early Swanson's approach to LBD can be formalized using a generic ABC model (Figure 1) that considers two independent literature sets, A and C [30].In this model, a represents a source concept, c is a target concept, and b serves as a bridge or intermediate concept that connects the two.The key idea is that if a is associated with b in one body of literature and b is associated with c in another-yet a and c have not been directly linked in any publication-there may be a novel, undiscovered relationship between a and c worth exploring.</p>
<p>A seminal example of this model is Swanson's [29] groundbreaking discovery linking dietary fish oil (a) to Raynaud's disease (c).He found that Raynaud's disease was associated with reduced blood viscosity in one set of articles, while another set linked high blood viscosity to fish oil.Although no studies at the time had made a direct connection between Raynaud's disease and fish oil, Swanson's hypothesis suggested a new therapeutic use for fish oil, which was later confirmed by clinical research [8].</p>
<p>In general, LBD encompasses two tasks: hypothesis validation and hypothesis generation, which correspond to closed and open discovery modes, respectively.In closed discovery, the process starts with two known elements-a starting concept (a) and a target concept (c)-and seeks to validate or elaborate the Despite its pivotal importance, the ABC model exhibits critical limitations that severely constrain the broader applicability of LBD.First, scalability remains a pressing challenge.Traditional LBD systems were developed for relatively small, curated datasets and are poorly suited to handle the exponential growth of biomedical publications [32].Effectively managing large-scale, heterogeneous corpora demands advanced computational capabilities and methodological innovations that classical LBD frameworks were not originally designed to support.</p>
<p>Second, the heavy reliance on structured data sources represents a major constraint.LBD approaches have historically depended on controlled vocabularies and ontologies, such as Unified Medical Language System (UMLS) [3], which facilitate computational access but simultaneously narrow the scope of discovery to well-represented areas [12].Consequently, current LBD systems often exhibit limited flexibility when extracting knowledge directly from unstructured or semi-structured texts, which account for a substantial portion of the scientific literature.</p>
<p>Third, the reliance on extensive manual curation and expert intervention remains a substantial barrier to progress in LBD.Traditional LBD workflows necessitate expert involvement at multiple stages, including hypothesis validation, result refinement, and relevance assessment [28].This dependence not only slows the overall discovery process but also poses significant challenges to achieving the scalability and reproducibility required for the broader application of LBD tools.</p>
<p>The landscape of LBD is evolving rapidly, but a comprehensive approach to tackling these challenges is essential for realizing its full potential in biomedical research and beyond.</p>
<p>Recent Advances</p>
<p>The field of LBD has seen notable progress in recent years, driven largely by advancements in machine learning, text mining, and statistical analysis.Research efforts have increasingly harnessed these technologies to develop more effective and sophisticated LBD systems.This section reviews three major directions contributing to the recent evolution of LBD: knowledge graphs (KGs), deep learning (DL), and language models (LMs).</p>
<p>Knowledge Graphs</p>
<p>KGs have emerged as a pivotal technology in NLP, offering a structured and scalable approach to organizing scientific knowledge.By representing information as networks of entities and their relationships, KGs enable graph-based reasoning and facilitate the identification of implicit associations across disparate literature sources.</p>
<p>Formally, KGs are defined as G = (V, E), where V represents the set of vertices (nodes) and E represents edges (links).Relationships in the graph are often modeled as triples (h, r, t), where h (head) and t (tail) are nodes and r is the relation connecting them.</p>
<p>Their construction typically follows two principal methodologies: (i) co-occurrence modeling, where links between entities are established if they co-appear within the same article; and (ii) explicit relation extraction, where semantic relationships are directly identified using specialized NLP tools such as SemRep [26].Co-occurrence models are widely adopted due to their simplicity and scalability, while relation extraction methods provide greater precision and richer semantic information. 4In particular, co-occurrence-based approaches have gained popularity in LBD systems owing to their ease of implementation [28,12].Recent approaches also enable the direct construction of KGs based on predications (subject-relation-object triples) extracted from sources such as PubMed abstracts.Resources such as the UMLS and Open Biomedical Ontologies (OBO) offer rich terminological frameworks that enhance the integration and crossreferencing of knowledge.</p>
<p>We approach LBD by formulating it as a knowledge graph completion (KGC) task.KGC techniques aim to predict missing information in graphs, either by discovering new edges (link prediction) or by identifying missing nodes (node prediction).Depending on the method used to construct the KG, the elements h, r, and t (as previously defined) may differ: in co-occurrence-based graphs, all three components often represent concepts or terms, whereas in relational databases, h and t denote entities and r represents a predicate, such as treats.</p>
<p>Examples include structures like "Fish oil" → "Blood viscosity" → "Raynaud's disease" for co-occurrence graphs, or "Fish oil" → treats → "Raynaud's disease" for relational graphs.</p>
<p>Two main approaches to KGC are usually employed: (i) evaluating the plausibility of candidate triples (h, r, t) by assigning a predictive score, and (ii) inferring missing elements by submitting incomplete triples, such as (h, r), (h, t), or (r, t), and predicting the missing component (i.e., predicting t given (h, r), r given (h, t), or h given (r, t)).</p>
<p>Deep Learning</p>
<p>In contrast to traditional machine learning methods that rely on features explicitly constructed using domain knowledge, DL uses specialized and deep architectures to extract meaningful features from unstructured input, can automatically learn from simple inputs, and extracts task-specific representations of structures.</p>
<p>Crichton et al. [5] provided compelling evidence that neural network models are highly effective for advancing LBD.The authors built upon a multilayer perceptron (MLP) framework designed for both closed and open discovery tasks, achieving state-of-the-art performance on the PubTator and BioGRID datasets.Their approach begins by generating input representations through node embeddings using the Large-scale Information Network Embedding (LINE) algorithm [31], followed by various strategies for combining the embeddings of nodes along a discovery path-structured according to Swanson's ABC model-to construct the input for the neural model.</p>
<p>In closed discovery, the first approach uses a neural model to assign scores to individual A−B and B−C links, which are then aggregated to evaluate the full A−B−C path.The second approach combines the embeddings of A, B, and C into a single input vector, allowing the model to predict a score for the entire path directly, thus removing the need for an explicit aggregation step.In open discovery, the first method similarly scores A−B and B−C links and aggregates them, but additionally uses an accumulator function to integrate multiple paths leading to the same C.The second method employs a convolutional neural network (CNN) that processes stacked embeddings of multiple A−B−C paths, producing a unified score for each A−C pair without relying on separate aggregation or accumulation functions.(Unlike conventional CNN applications where images are used, the input here is a pseudo-image created by stacking vectorized A−B−C paths.)</p>
<p>While Crichton et al. [5] relied on embedding representations for all concepts as model inputs, their method required users to manually construct all possible hypothesis triples prior to evaluation, a process that is both time-consuming and reliant on substantial domain expertise.Addressing this limitation, Cuffy et al. [6] introduced a closed discovery framework that automates the ranking of potential linking B terms for a given A and C pair using a single forward propagation step through the DL model.This approach eliminates the need to generate all A−B−C triples a priori, thereby reducing the dependency on domain-specific knowledge and significantly streamlining the LBD workflow.</p>
<p>Cuffy et al. [7] introduced a further advancement by reformulating the LBD task as the prediction of implicit concept embeddings rather than direct relationship scoring.Instead of classifying triples, their model predicts the embedding of the linking concept (B) given the starting (A) and target (C) concepts.By comparing predicted embeddings against all candidate concepts, the MLP model identifies plausible intermediates, demonstrating its effectiveness in systematic knowledge discovery replication.</p>
<p>Beyond general-purpose LBD tasks, DL has been effectively applied in domainspecific applications, such as drug repurposing.Zhu et al. [39] introduced a BioBERT-based model enhanced with entity-aware attention mechanisms for drug-drug interaction extraction, while Gupta et al. [10] utilized an NSGA-III-based CNN architecture to optimize biomedical search engines.Rather et al. [25] further showcased DL's capacity to uncover latent biomedical relationships through word2vec-based embeddings.Taken together, these studies demonstrate the transformative potential of DL for LBD, facilitating more nuanced knowledge representation and discovery.</p>
<p>Language Models</p>
<p>LMs are nowadays regarded as fundamental components of NLP, tasked with estimating the probability distributions of linguistic units-such as words, phrases, or sentences-based on their contextual surroundings.The evolution of LMs can be delineated into several distinct stages: beginning with statistical language models (SLMs), progressing through neural language models (NLMs), advancing to pre-trained language models (PLMs), and culminating in the emergence of large language models (LLMs).SLMs utilize basic probabilistic frameworks to model word sequences (e.g., n-grams), whereas NLMs employ neural networks to capture complex syntactic and semantic patterns (e.g., RNNs, LSTMs, transformers).PLMs leverage large-scale textual corpora and self-supervised learning to encode general linguistic structures and knowledge (e.g., Bidirectional Encoder Representations from Transformers (BERT), Generative Pretrained Transformer (GPT)).</p>
<p>Here, we focus specifically on how recent developments in PLMs and LLMs have been integrated into LBD pipelines.While both PLMs and LLMs are trained on large corpora using self-supervised methods, LLMs represent a significant advancement in terms of model size, training data scale, and architectural complexity.Building upon the foundation established by PLMs, LLMs offer improved generalization, greater expressivity, enhanced contextual understanding, adaptability, and zero-shot reasoning capabilities-making them particularly well-suited for advanced LBD tasks.A comprehensive overview of the historical development of LMs is provided in recent surveys by Wang et al. [34] and Annepaka et al. [1].</p>
<p>PLMs have elevated the quality and scope of LBD by integrating deep contextual understanding into NLP pipelines.Used both as powerful preprocessing tools and as core components in downstream tasks such as named entity recognition and relation extraction, PLMs have enabled more accurate and scalable discovery workflows.For example, in our LBD approach to drug repurposing for Covid-19 [38], we employed BERT5 as a preprocessing tool to generate an accurate subset of semantic triples, which were then used to construct a KG.KGC algorithms were subsequently applied to this graph to predict potential drug repurposing candidates.</p>
<p>Compared to PLMs, LLMs exhibit remarkable adaptability, with recent empirical results indicating strong potential for their use as a general-purpose tool to support scientific reasoning [13].A growing body of evidence reveals a broad range of promising capabilities of LLM relevant to the scientific process, including the coherent integration of diverse knowledge concepts, the critical evaluation of existing studies, the generation of scientific hypotheses, and the identification of research gaps within scientific literature [21].State-of-the-art LLMs, such as BioGPT [20] and SciGLM [37] are trained on domain-specific corpora like PubMed and arXiv and are particularly effective at literature retrieval, document summarization, and question answering.They facilitate more efficient access to scientific information by identifying relevant publications, extracting key insights, and synthesizing knowledge across documents.</p>
<p>Specifically, building on the improved reasoning capabilities of LMs, the LBD community has begun developing methods that incorporate richer contextual information to address the limitations of traditional approaches, which are primarily based on the ABC model.Classical LBD techniques often fail to capture the nuanced contextual cues considered by human scientists during the ideation process and are largely restricted to predicting pairwise relationships between isolated concepts [13].To overcome these constraints, Wang et al. [33] introduced a novel framework, SciMON, which grounds LBD in natural language contexts, thereby narrowing the generative space in a more controlled and meaningful way.SciMON optimizes novel research hypotheses by iteratively refining idea suggestions derived from published literature until sufficient novelty is achieved.Unlike traditional models that merely predict conceptual links, SciMON generates complete sentences as outputs, offering a more nuanced and contextually rich representation of scientific knowledge.The authors report that it produces ideas that are both more original and exhibit greater conceptual depth than those generated by GPT-4.</p>
<p>A long-standing limitation of LBD has been its restriction to the biomedical domain, primarily due to the widespread availability of the PubMed database and auxiliary knowledge resources (e.g., UMLS vocabularies [3], SemMedDB repository [16], PubTator annotations [35]), which are freely available and optimized for computational access and analysis.However, LLM-powered LBD may have a much broader scope of applicability.In particular, Yang et al. [36] showed that the majority of published hypotheses in the social sciences can be structured in a manner compatible with the LBD framework.</p>
<p>In summary, recent advances in KGs, DL techniques, and LM development have significantly expanded the capabilities of LBD.Table 1 highlights the principal characteristics of the described approaches.Nevertheless, several key challenges remain, which are discussed in the following section.</p>
<p>Future Directions</p>
<p>Although LBD has made significant advances over the past five years, numerous open challenges remain to be addressed.The following discussion outlines several key areas of ongoing work, reflecting the current focus of our research efforts; however, the list is not intended to be exhaustive.</p>
<p>Advancing Interpretability</p>
<p>Interpretability remains one of the principal challenges associated with the application of DL techniques in science [24].Ensuring interpretability in LBD is not simply an auxiliary feature; it is foundational.While DL approaches offer considerable potential for enhancing hypothesis generation from large corpora, their inherent "black-box" nature continues to present significant obstacles for scientific domains where transparent reasoning processes are essential.In particular, many LBD methods, especially those rooted in Swanson's ABC model, focus primarily on hypothesis generation but often lack mechanisms for explaining the reasoning behind the generated hypotheses.While DL systems excel in extracting patterns from literature, they frequently fall short of providing understandable explanations, which symbolic systems have historically offered [2].</p>
<p>Traditional strategies, such as employing attention mechanisms or inspecting model coefficients, offer partial solutions by highlighting feature importance or visualizing internal representations [22].However, these approaches often lack a structured reasoning component and thus fall short of delivering full scientific explanatory power.A promising direction is the integration of neuro-symbolic AI into LBD methodologies.The neuro-symbolic approach aims to combine the pattern recognition capabilities of neural networks with the explicit reasoning structures of symbolic AI [27,2].This integration enables models not only to learn from data but also to reason in ways that are inherently interpretable and grounded in logical principles.Neuro-symbolic approaches have already been successfully applied to various NLP tasks [11].</p>
<p>Augmenting Data Resources</p>
<p>One of the principal limitations of current LBD applications lies in their restricted use of data resources.Most existing approaches rely primarily on Pub-Med, often limiting their textual input to article abstracts rather than utilizing the full texts.While abstracts offer a concise summary of findings, they frequently omit critical contextual relationships that could be valuable for complex hypothesis generation, particularly for LMs.Expanding beyond the biomedical domain to include full-text articles and additional knowledge bases presents a significant opportunity for advancing LBD.In particular, new bibliographic databases such as Semantic Scholar [19] have emerged as valuable resources.These platforms aggregate extensive metadata, citation networks, and, in some cases, full-text content across a wide range of scientific disciplines, offering richer semantic contexts for discovery processes.</p>
<p>In addition, auxiliary knowledge resources, such as the UMLS for the biomedical domain, are of significant importance, particularly during the preprocessing stages of LBD (e.g., guiding the extraction of knowledge concepts and the computation of predicates).Although widely integrated into LBD applications for its standardized vocabularies and extensive concept mappings, the use of UMLS is not without limitations.Issues such as term ambiguity and incomplete concept coverage can substantially impact the performance of downstream tasks.For instance, a significant portion of errors in tools like SemRep stem from difficulties in correctly identifying and normalizing biomedical entities using UMLS, accounting for up to 27% of errors in some evaluations [15].FurthermoreIn addition, auxiliary knowledge resources, such as the UMLS for the biomedical domain, are of significant importance, particularly during the preprocessing stages of LBD (e.g., guiding the extraction of knowledge concepts and the computation of predicates).Although widely integrated into LBD applications for its standardized vocabularies and extensive concept mappings, the use of UMLS is not without limitations.Issues such as term ambiguity and incomplete concept coverage can substantially impact the performance of downstream tasks.For instance, a significant portion of errors in tools like SemRep stem from difficulties in correctly identifying and normalizing biomedical entities using UMLS, accounting for up to 27% of errors in some evaluations [15].Finally, to the best of our knowledge at the time of writing, no comparably well-developed knowledge resource exists outside the life sciences domain.In our experience, the limited adoption of LBD beyond biomedicine is largely due to the greater terminological diversity and, in particular, the absence of standardized ontologies in the humanities and social sciences., to the best of our knowledge at the time of writing, no comparably well-developed knowledge resource exists outside the life sciences domain.In our experience, the limited adoption of LBD beyond biomedicine is largely due to the greater terminological diversity and, in particular, the absence of standardized ontologies in the humanities and social sciences.</p>
<p>Refining Benchmarking Practices</p>
<p>Knuth's [17] concept of literate programming, which emphasizes that computer programs should be readable and understandable by humans, closely aligns with open science initiatives that stress the importance of standardized practices and tools to ensure research outputs are independently verifiable and can support further scientific progress.</p>
<p>Following the principles of open science, we initiated a project aimed at promoting reproducibility within the field of LBD.Existing LBD approaches and results often remain difficult to replicate due to the lack of access to original datasets and unresolved programming dependencies.These limitations pose significant barriers to both the theoretical understanding and practical reuse of previously published methods.To address this gap, we have made publicly available benchmark datasets, replicable LBD case studies, and a collection of interactive Jupyter Notebooks that transparently document each step of the LBD pipeline, including data acquisition, text preprocessing, hypothesis discovery, and evaluation.Furthermore, we provide the LBD community with access to standardized benchmark datasets and prototypical LBD techniques presented through dockerized Jupyter environments, thereby greatly simplifying replication and extension.All associated materials are openly accessible at https: //github.com/akastrin/ida2025lbd.</p>
<p>Conclusion</p>
<p>This survey has reviewed the evolution of LBD over the past five years.We discussed the growing role of KGs, advances in DL methodologies, and the transformative impact of PLMs and LLMs on hypothesis generation and scientific reasoning.</p>
<p>Rapid advances in AI, particularly in the development of LLMs, are reshaping the scientific landscape at an unprecedented pace.These developments open up significant opportunities for treating scientific corpora as dynamic knowledge bases from which novel insights, hypotheses, and ideas can be systematically uncovered.Despite this progress, several fundamental challenges remain unresolved in LBD, notably issues related to scalability, dependence on structured data, the need for extensive manual curation, and the limited interpretability of current DL approaches.</p>
<p>Recent trends in neuro-symbolic AI suggest promising avenues for enhancing both the accuracy and explainability of LBD systems.By combining the strengths of DL with the reasoning capabilities of symbolic methods, these hybrid approaches aim to deliver more transparent and trustworthy discoveries, thereby enabling broader domain applicability of LBD beyond the biomedical sciences.</p>
<p>Fig. 1 .
1
Fig. 1.Swanson's ABC model of discovery.When a is related to b, and b is related to c, it suggests the possibility of an undiscovered indirect relationship between a and c.</p>
<p>Table 1 .
1
Summary of key strengths and limitations of recent approaches in LBD.
Approach StrengthsLimitationsKGsCaptures complex, heterogeneousRequires high-quality semanticassociations; enables context-annotation; extensive filtering of-driven subgraph creation.ten needed.DLOutperforms traditional base-Interpretability remains a chal-lines, especially when input rep-lenge; generalizability may beresentations are well-optimized.constrained by data representa-tion.LMsOffers potential for explainableStill emerging; scalability and val-AI; capable of processing hetero-idation challenges.geneous, cross-domain data.
Co-occurrence refers to the statistical tendency of two terms or concepts to appear together in text (e.g., fever and infection), without implying a specific semantic or causal relationship. In contrast, a semantic relation denotes a defined and meaningful connection between terms, such as a taxonomic link (e.g., influenza as a type of viral infection), regardless of how frequently they co-occur.
Due to its success in general NLP tasks, BERT has been adapted to various specialized domains, including biomedicine, resulting in models such as BioBERT[18], ClinicalBERT[14], PubMedBERT[9], and COVID-Twitter-BERT[23].
Acknowledgments.The authors acknowledge the financial support from the Slovenian Research and Innovation Agency through the Knowledge Technologies (Grant No. P2-0103), and Methodology for Data Analysis in Medical Sciences (Grant No. P3-0154) core research projects, as well as Embeddings-Based Techniques for Media Monitoring Applications (Grant No. L2-50070) research project.Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article.
Large language models: A survey of their development, capabilities, and applications. Y Annepaka, P Pakray, 10.1007/s10115-024-02310-4Knowledge and Information Systems. 6732025</p>
<p>Neuro-symbolic artificial intelligence: A survey. B P Bhuyan, A Ramdane-Cherif, R Tomar, T P Singh, 10.1007/s00521-024-09960-zNeural Computing and Applications. 36212024</p>
<p>The Unified Medical Language System (UMLS): Integrating biomedical terminology. O Bodenreider, 10.1093/nar/gkh061Nucleic Acids Research. 32Database2004</p>
<p>Growth rates of modern science: A latent piecewise growth curve approach to model publication numbers from established and new literature databases. L Bornmann, R Haunschild, R Mutz, 10.1057/s41599-021-00903-wHumanities and Social Sciences Communications. 812242021</p>
<p>Neural networks for open and closed literature-based discovery. G Crichton, S Baker, Y Guo, A Korhonen, 10.1371/journal.pone.0232891PLoS One. 155e02328912020</p>
<p>Exploring a deep learning neural architecture for closed literature-based discovery. C Cuffy, B T Mcinnes, 10.1016/j.jbi.2023.104362Journal of Biomedical Informatics. 1431043622023</p>
<p>Predicting implicit concept embeddings for singular relationship discovery replication of closed literature-based discovery. C Cuffy, B T Mcinnes, 10.3389/frma.2025.1509502Frontiers in Research Metrics and Analytics. 1015095022025</p>
<p>Fish-oil dietary supplementation in patients with Raynaud's phenomenon: A double-blind, controlled, prospective study. R A Digiacomo, J M Kremer, D M Shah, 10.1016/0002-9343(89)90261-1The American Journal of Medicine. 8621989</p>
<p>Domainspecific language model pretraining for biomedical natural language processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, 10.1145/3458754ACM Transactions on Computing for Healthcare. 312021</p>
<p>NSGA-III-based deep-learning model for biomedical search engines. M Gupta, N Kumar, B K Singh, N Gupta, 10.1155/2021/9935862Mathematical Problems in Engineering. 2021199358622021</p>
<p>Is neuro-symbolic AI meeting its promises in natural language processing? A structured review. Nayak Hamilton, A Božić, B Longo, L , 10.3233/SW-223228Semantic Web. 1542024</p>
<p>Literature based discovery: Models, methods, and trends. S Henry, B T Mcinnes, 10.1016/j.jbi.2017.08.011Journal of Biomedical Informatics. 742017</p>
<p>A computational inflection for scientific discovery. T Hope, D Downey, D S Weld, O Etzioni, E Horvitz, 10.1145/3576896Communications of the ACM. 6682023</p>
<p>K Huang, J Altosaar, R Ranganath, 10.48550/arXiv.1904.05342arXivClinicalBERT: Modeling clinical notes and predicting hospital readmission. 2020</p>
<p>Broad-coverage biomedical relation extraction with SemRep. H Kilicoglu, G Rosemblat, M Fiszman, D Shin, 10.1186/s12859-020-3517-7BMC Bioinformatics. 2111882020</p>
<p>SemMedDB: A PubMed-scale repository of biomedical semantic predications. H Kilicoglu, D Shin, M Fiszman, G Rosemblat, T C Rindflesch, 10.1093/bioinformatics/bts591Bioinformatics. 28232012</p>
<p>Literate programming. D E Knuth, The Computer Journal. 2721984</p>
<p>BioBERT: A pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, 10.1093/bioinformatics/btz682Bioinformatics. 3642020</p>
<p>S2ORC: The Semantic Scholar Open Research Corpus. K Lo, L L Wang, M Neumann, R Kinney, D Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>BioGPT: Generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, 10.1093/bib/bbac409Briefings in Bioinformatics. 2364092022</p>
<p>Z Luo, Z Yang, Z Xu, W Yang, X Du, 10.48550/arXiv.2501.04306LLM4SR: A survey on large language models for scientific research. 2025</p>
<p>Explainable artificial intelligence: A survey of needs, techniques, applications, and future direction. M Mersha, K Lam, J Wood, A K Alshami, J Kalita, 10.1016/j.neucom.2024.128111Neurocomputing. 5991281112024</p>
<p>COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter. M Müller, M Salathé, P E Kummervold, 10.3389/frai.2023.1023281Frontiers in Artificial Intelligence. 610232812023</p>
<p>Definitions, methods, and applications in interpretable machine learning. W J Murdoch, C Singh, K Kumbier, R Abbasi-Asl, B Yu, 10.1073/pnas.1900654116Proceedings of the National Academy of Sciences. the National Academy of Sciences2019116</p>
<p>Using deep learning towards biomedical knowledge discovery. N N Rather, C O Patel, S A Khan, 10.5815/ijmsc.2017.02.01International Journal of Mathematical Sciences and Computing. 322017</p>
<p>The interaction of domain knowledge and linguistic structure in natural language processing: Interpreting hypernymic propositions in biomedical text. T C Rindflesch, M Fiszman, 10.1016/j.jbi.2003.11.003Journal of Biomedical Informatics. 3662003</p>
<p>Neuro-symbolic artificial intelligence: Current trends. M K Sarker, L Zhou, A Eberhart, P Hitzler, 10.3233/AIC-210084AI Communications. 3432021</p>
<p>Learning the heterogeneous bibliographic information network for literature-based discovery. Y Sebastian, E G Siew, S O Orimaye, 10.1016/j.knosys.2016.10.015Knowledge-Based Systems. 1152017</p>
<p>Fish oil, Raynaud's syndrome, and undiscovered public knowledge. D R Swanson, 10.1353/pbm.1986.0087Perspectives in Biology and Medicine. 3011986</p>
<p>Undiscovered public knowledge. D R Swanson, 10.1086/601720The Library Quarterly. 5621986</p>
<p>LINE: Large-scale information network embedding. J Tang, M Qu, M Wang, M Zhang, J Yan, Q Mei, 10.1145/2736277.2741093Proceedings of the 24th International Conference on World Wide Web. A Gangemi, S Leonardi, A Panconesi, the 24th International Conference on World Wide Web2015International World Wide Web Conference Committee</p>
<p>A systematic review on literaturebased discovery: General overview, methodology, &amp; statistical analysis. M Thilakaratne, K Falkner, T Atapattu, 10.1145/3365756ACM Computing Surveys. 5261292019</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Q Wang, D Downey, H Ji, T Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. L W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2024</p>
<p>History, development, and principles of large language models: An introductory survey. Z Wang, Z Chu, T V Doan, S Ni, M Yang, W Zhang, 10.1007/s43681-024-00583-7AI and Ethics. 52025</p>
<p>PubTator 3.0: An AI-powered literature resource for unlocking biomedical knowledge. C H Wei, A Allot, P T Lai, R Leaman, S Tian, L Luo, 10.1093/nar/gkae235Nucleic Acids Research. 52W12024</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, 10.18653/v1/2024.findings-acl.804Findings of the Association for Computational Linguistics: ACL 2024. L W Ku, A Martins, V Srikumar, Association for Computational Linguistics2024</p>
<p>SciGLM: Training scientific language models with self-reflective instruction annotation and tuning. D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang, 10.48550/arXiv.1904.05342arXiv2024</p>
<p>Drug repurposing for COVID-19 via knowledge graph completion. R Zhang, D Hristovski, D Schutte, A Kastrin, M Fiszman, H Kilicoglu, 10.1016/j.jbi.2021.103696Journal of Biomedical Informatics. 1151036962021</p>
<p>Extracting drug-drug interactions from texts with BioBERT and multiple entity-aware attentions. Y Zhu, L Li, H Lu, A Zhou, X Qin, 10.1016/j.jbi.2020.103451Journal of Biomedical Informatics. 1061034512020</p>            </div>
        </div>

    </div>
</body>
</html>
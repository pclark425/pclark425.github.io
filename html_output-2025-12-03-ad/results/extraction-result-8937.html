<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8937 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8937</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8937</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-f02e8f1c9b5ab12ddfb1977570f9f5445a99a973</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f02e8f1c9b5ab12ddfb1977570f9f5445a99a973" target="_blank">Large Language Models are reasoners with Self-Verification</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked, and calculates an explainable verification score based on the accuracy.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8937.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8937.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Verification for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-verify prompting method in which an LLM first produces multiple chain-of-thought answers (forward reasoning) and then performs backward verification by rewriting conclusions as conditions and predicting masked original conditions to compute an interpretable verification score used to rank candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruct-GPT (code-davinci-002) / GPT-3 (code-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variants of OpenAI GPT-3 family used in experiments: code-davinci-002 (Instruct-GPT, 175B, instruction-tuned) and code-davinci-001 (GPT-3 standard, 175B); additional experiments use smaller public GPT-3 variants (text-ada-001, text-babbage-001, text-curie-001, text-davinci-002) as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Forward: generate K=5 candidate chain-of-thought answers via sampling CoT prompting. Backward: rewrite each candidate answer into a declarative conclusion, add it to original context as a new condition, mask target condition(s) and re-predict them using CoT; repeat P times (P=10) to obtain counts of consistent predictions; compute Score_y (sum of matches across masked conditions and repeats) and select the candidate with highest score. No fine-tuning; entirely prompt-based.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic, CommonsenseQA, Date Understanding (GSM8K, SingleEq, AddSub, MultiArith, AQUA-RAT, SVAMP, CSQA, Date Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks covering arithmetic word problems (numeric equation solving and multi-step math), commonsense multiple-choice QA, and date inference tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Instruct-GPT CoT+Self-Verification: GSM8K 65.14% (vs CoT 60.81%), SingleEq 93.40% (vs 91.01%), AddSub 86.33% (vs 82.78%), MultiArith 99.15% (vs 96.13%), AQUA-RAT 47.95% (vs 45.30%), SVAMP 76.99% (vs 75.87%), CSQA 77.83% (vs 77.42%). GPT-3 (code-davinci-001) CoT+SV: small absolute improvements reported (e.g., GSM8K 13.92% vs 13.84%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline chain-of-thought (CoT) prompting: Instruct-GPT CoT: GSM8K 60.81%, SingleEq 91.01%, AddSub 82.78%, MultiArith 96.13%, AQUA-RAT 45.30%, SVAMP 75.87%, CSQA 77.42%. (Other baselines: GPT-3 standard and prior fine-tuned SOTAs listed in Table 1.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering only: (1) forward CoT sampling to produce multiple candidates; (2) automated rewriting of question+candidate into declarative conclusions; (3) masking specific condition attributes (or asking True/False for items) and re-predicting via same LLM with CoT prompts; scoring is based on logical consistency between predicted masked values and original values. No additional models or fine-tuning; uses sampling/randomized decoding for diversity and repeated verification.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements across datasets: On Instruct-GPT, average improvement ~2.33% absolute across evaluated datasets; notable single-dataset gains include GSM8K +4.33% (60.81 -> 65.14) and SingleEq +2.39% (91.01 -> 93.40). Verification stage accuracy (probability of picking correct answer among candidates) exceeds both random baseline and CoT accuracy in per-case analyses (e.g., GSM8K verification accuracy 58.9% vs CoT 53.4% and random 35.7%). Multi-condition masking and larger model sizes yield larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Method depends on (a) presence of a correct candidate among generated answers (if none, verification cannot select correct answer); (b) the LLM's own backward reasoning ability — smaller models show little or negative benefit (Figure 3); (c) handcrafted prompts that may introduce bias; (d) method focuses on validating conclusions, not the internal correctness of entire reasoning chains; (e) increased computational cost from generating K candidates and P verification samples (though authors recommend P=10 as trade-off). The paper reports that benefits are much larger for high-performing (large) models and smaller models can sometimes be worsened by SV.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared versus plain CoT (baseline) and combined with forward-improvement methods: combining SV with Self-Consistency (SC) or PAL improves performance further (e.g., Instruct-GPT SC 78.00% -> SC+SV 78.32% on GSM8K; PAL 72.02% -> PAL+SV 72.89%). SV yields consistent but sometimes modest gains on top of strong forward methods; greater relative gains observed on arithmetic tasks than general QA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported analyses: (1) Model size ablation: larger models benefit more and show robust SV gains; smaller models sometimes degrade. (2) Number of few-shot exemplars: SV is more robust with fewer shots and can match performance of larger-shot CoT (with 2-4 shots). (3) Condition-masking ablation: multi-condition masking outperforms single-condition masking (Figure 5). (4) Verification repeat count P: increasing P increases performance up to diminishing returns; P=10 recommended (Figure 7). (5) CMV vs TFV: CMV generally outperforms TFV for arithmetic tasks (Figure 6).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8937.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8937.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Condition Mask Verification (CMV)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Condition Mask Verification (a CMV variant of Self-Verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A backward verification strategy that masks specific condition tokens (e.g., numeric values) in the original context, asks the LLM to re-predict the masked value given the candidate conclusion, and scores consistency between predicted and original values across repeated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruct-GPT (code-davinci-002) / GPT-3 variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLMs as used for SV experiments (primarily 175B Instruct-GPT and GPT-3 variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Condition Mask Verification (CMV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Select condition attributes (numbers, specific tokens) via regex, replace them with a mask token 'X' in turn, append a targeted question such as 'What is the answer of X?' and run CoT-style prompts to predict X; repeat P times (P typically 10) and count matches to original value to compute verification score; can mask multiple conditions and aggregate matches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic reasoning datasets (GSM8K, SingleEq, AddSub, MultiArith, AQUA-RAT, SVAMP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic word-problem datasets where conditions are numeric and can be masked and re-inferred.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>CMV as the verification method within SV yields the larger part of the arithmetic-task gains summarized for SV. The paper reports CMV outperforms TFV on arithmetic tasks (see Figure 6); combined SV with CMV achieves large increases (e.g., MultiArith Instruct-GPT CoT 96.13% -> CoT+SV 99.15%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline CoT and TFV comparisons: CoT alone yields lower accuracy (see dataset-specific CoT baseline numbers in Table 1). TFV results are generally lower than CMV for arithmetic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering with token masking of condition attributes; repeated sampling and CoT-style re-solving of masked attribute; scoring is simple match/indicator-based.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical: CMV consistently outperforms TFV for arithmetic datasets (Figure 6) and multi-condition masking improves verification accuracy (Figure 5). CMV-based SV produced the largest dataset gains on arithmetic tasks reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires reliable identification of meaningful maskable conditions (regex-based). Masking irrelevant conditions degrades verification quality (example: masking hours on Friday in a problem where it doesn't affect answer). CMV is less directly applicable to non-numeric QA without clear maskable attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to True-False Item Verification (TFV): CMV generally outperforms TFV on arithmetic tasks; TFV remains more broadly applicable for non-arithmetic tasks but with smaller gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Multi-condition masking outperforms single-condition masking; increasing number of masked conditions generally increases verification accuracy (Figure 5). P (repeats) ablation shows greater P improves score stability up to diminishing returns (Figure 7).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8937.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8937.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>True-False Item Verification (TFV)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>True-False Item Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general backward verification strategy that appends a 'True or False?' question asking whether the set of conditions plus the candidate conclusion are mutually satisfied, and uses the LLM's True/False outputs aggregated across repeated samplings as a verification score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruct-GPT (code-davinci-002) / GPT-3 variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLMs as used in the main SV experiments (Instruct-GPT 175B and other GPT-3 variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>True-False Item Verification (TFV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Add 'Do it is correct (True or False)?' (or similar) after the full set of conditions (including the rewritten candidate conclusion) and use the LLM to answer True/False under CoT-style prompting; repeat P times and count True votes to score candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General QA tasks (CommonsenseQA, Date Understanding) and also applied to arithmetic as a baseline verification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where a boolean judgement about the mutual consistency of conditions and conclusion can be asked.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>TFV-based SV yields modest improvements over CoT in general QA tasks (e.g., CSQA Instruct-GPT CoT 77.42% -> CoT+SV 77.83%) and some arithmetic improvements but generally lower than CMV.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline CoT numbers as in Table 1 (e.g., CSQA 77.42% for Instruct-GPT CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering: append a True/False question to the context and aggregate repeated LLM True/False answers as score.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>TFV produces measurable but smaller gains compared to CMV and overall CoT baselines, with improvements reported in Table 1 for several datasets (though arithmetic tasks benefit more from CMV). TFV is applicable across more diverse tasks where specific conditions are not easily maskable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less precise than CMV on arithmetic tasks because it yields boolean signals rather than explicit masked-value matches; can fail to use candidate conclusions effectively if the prompt lacks explicit guidance; lower gains on numeric datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to CMV: TFV is simpler and more general but typically yields smaller gains; compared to no verification (CoT) TFV still often improves but to a lesser extent than CMV on arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper reports TFV improvements but shows CMV > TFV for arithmetic tasks (Figure 6). Repeating P times mitigates score ties and improves confidence estimates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8937.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8937.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A forward-stage decoding strategy that samples multiple chain-of-thought rationales and then aggregates (majority vote) to obtain a final answer; used in this paper as a forward method that can be combined with SV for re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruct-GPT / GPT-3 (175B) in combination experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-consistency as implemented in prior work (Wang et al., 2023) applied to the same LLMs used here.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sampling multiple CoT chains (top-K or many samples) during forward reasoning and aggregating answers (e.g., majority vote) to improve robustness; in this paper SC is used to generate Top-2 candidates for SV re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same reasoning benchmarks (GSM8K, SingleEq, AddSub, MultiArith, AQUA-RAT, SVAMP, CSQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks used for reasoning evaluation; SC was used as an alternative forward-generation method whose outputs were then re-ranked by SV.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Examples: Instruct-GPT SC: GSM8K 78.00% -> SC+Self-Verification 78.32% (small improvement); other datasets show small positive deltas when SV re-ranks SC outputs (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>SC alone: GSM8K 78.00% (Instruct-GPT), SingleEq 96.78%, AddSub 91.64%, etc. (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Forward sampling plus majority/aggregation; in this paper SC is combined with SV by feeding top candidates from SC into the SV backward verifier for re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Combining SC with SV yields marginal but consistent improvements over SC alone (e.g., GSM8K +0.32% absolute), demonstrating SV can add backward-validation benefits on top of diverse forward samplings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements from applying SV on top of SC are generally small when forward reasoning is already strong; combining methods increases computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to plain CoT, SC gives larger forward gains; SV can re-rank SC outputs but typically yields smaller incremental gains than when re-ranking weaker forward outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Generate & rank: A multi-task framework for math word problems <em>(Rating: 1)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8937",
    "paper_id": "paper-f02e8f1c9b5ab12ddfb1977570f9f5445a99a973",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-Verification (SV)",
            "name_full": "Self-Verification for Large Language Models",
            "brief_description": "A generate-then-verify prompting method in which an LLM first produces multiple chain-of-thought answers (forward reasoning) and then performs backward verification by rewriting conclusions as conditions and predicting masked original conditions to compute an interpretable verification score used to rank candidates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruct-GPT (code-davinci-002) / GPT-3 (code-davinci-001)",
            "model_description": "Variants of OpenAI GPT-3 family used in experiments: code-davinci-002 (Instruct-GPT, 175B, instruction-tuned) and code-davinci-001 (GPT-3 standard, 175B); additional experiments use smaller public GPT-3 variants (text-ada-001, text-babbage-001, text-curie-001, text-davinci-002) as reported in the paper.",
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Forward: generate K=5 candidate chain-of-thought answers via sampling CoT prompting. Backward: rewrite each candidate answer into a declarative conclusion, add it to original context as a new condition, mask target condition(s) and re-predict them using CoT; repeat P times (P=10) to obtain counts of consistent predictions; compute Score_y (sum of matches across masked conditions and repeats) and select the candidate with highest score. No fine-tuning; entirely prompt-based.",
            "task_name": "Arithmetic, CommonsenseQA, Date Understanding (GSM8K, SingleEq, AddSub, MultiArith, AQUA-RAT, SVAMP, CSQA, Date Understanding)",
            "task_description": "Benchmarks covering arithmetic word problems (numeric equation solving and multi-step math), commonsense multiple-choice QA, and date inference tasks.",
            "performance_with_reflection": "Instruct-GPT CoT+Self-Verification: GSM8K 65.14% (vs CoT 60.81%), SingleEq 93.40% (vs 91.01%), AddSub 86.33% (vs 82.78%), MultiArith 99.15% (vs 96.13%), AQUA-RAT 47.95% (vs 45.30%), SVAMP 76.99% (vs 75.87%), CSQA 77.83% (vs 77.42%). GPT-3 (code-davinci-001) CoT+SV: small absolute improvements reported (e.g., GSM8K 13.92% vs 13.84%).",
            "performance_without_reflection": "Baseline chain-of-thought (CoT) prompting: Instruct-GPT CoT: GSM8K 60.81%, SingleEq 91.01%, AddSub 82.78%, MultiArith 96.13%, AQUA-RAT 45.30%, SVAMP 75.87%, CSQA 77.42%. (Other baselines: GPT-3 standard and prior fine-tuned SOTAs listed in Table 1.)",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineering only: (1) forward CoT sampling to produce multiple candidates; (2) automated rewriting of question+candidate into declarative conclusions; (3) masking specific condition attributes (or asking True/False for items) and re-predicting via same LLM with CoT prompts; scoring is based on logical consistency between predicted masked values and original values. No additional models or fine-tuning; uses sampling/randomized decoding for diversity and repeated verification.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative improvements across datasets: On Instruct-GPT, average improvement ~2.33% absolute across evaluated datasets; notable single-dataset gains include GSM8K +4.33% (60.81 -&gt; 65.14) and SingleEq +2.39% (91.01 -&gt; 93.40). Verification stage accuracy (probability of picking correct answer among candidates) exceeds both random baseline and CoT accuracy in per-case analyses (e.g., GSM8K verification accuracy 58.9% vs CoT 53.4% and random 35.7%). Multi-condition masking and larger model sizes yield larger gains.",
            "limitations_or_failure_cases": "Method depends on (a) presence of a correct candidate among generated answers (if none, verification cannot select correct answer); (b) the LLM's own backward reasoning ability — smaller models show little or negative benefit (Figure 3); (c) handcrafted prompts that may introduce bias; (d) method focuses on validating conclusions, not the internal correctness of entire reasoning chains; (e) increased computational cost from generating K candidates and P verification samples (though authors recommend P=10 as trade-off). The paper reports that benefits are much larger for high-performing (large) models and smaller models can sometimes be worsened by SV.",
            "comparison_to_other_methods": "Compared versus plain CoT (baseline) and combined with forward-improvement methods: combining SV with Self-Consistency (SC) or PAL improves performance further (e.g., Instruct-GPT SC 78.00% -&gt; SC+SV 78.32% on GSM8K; PAL 72.02% -&gt; PAL+SV 72.89%). SV yields consistent but sometimes modest gains on top of strong forward methods; greater relative gains observed on arithmetic tasks than general QA.",
            "ablation_study_results": "Reported analyses: (1) Model size ablation: larger models benefit more and show robust SV gains; smaller models sometimes degrade. (2) Number of few-shot exemplars: SV is more robust with fewer shots and can match performance of larger-shot CoT (with 2-4 shots). (3) Condition-masking ablation: multi-condition masking outperforms single-condition masking (Figure 5). (4) Verification repeat count P: increasing P increases performance up to diminishing returns; P=10 recommended (Figure 7). (5) CMV vs TFV: CMV generally outperforms TFV for arithmetic tasks (Figure 6).",
            "uuid": "e8937.0"
        },
        {
            "name_short": "Condition Mask Verification (CMV)",
            "name_full": "Condition Mask Verification (a CMV variant of Self-Verification)",
            "brief_description": "A backward verification strategy that masks specific condition tokens (e.g., numeric values) in the original context, asks the LLM to re-predict the masked value given the candidate conclusion, and scores consistency between predicted and original values across repeated samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruct-GPT (code-davinci-002) / GPT-3 variants",
            "model_description": "Same LLMs as used for SV experiments (primarily 175B Instruct-GPT and GPT-3 variants).",
            "reflection_method_name": "Condition Mask Verification (CMV)",
            "reflection_method_description": "Select condition attributes (numbers, specific tokens) via regex, replace them with a mask token 'X' in turn, append a targeted question such as 'What is the answer of X?' and run CoT-style prompts to predict X; repeat P times (P typically 10) and count matches to original value to compute verification score; can mask multiple conditions and aggregate matches.",
            "task_name": "Arithmetic reasoning datasets (GSM8K, SingleEq, AddSub, MultiArith, AQUA-RAT, SVAMP)",
            "task_description": "Arithmetic word-problem datasets where conditions are numeric and can be masked and re-inferred.",
            "performance_with_reflection": "CMV as the verification method within SV yields the larger part of the arithmetic-task gains summarized for SV. The paper reports CMV outperforms TFV on arithmetic tasks (see Figure 6); combined SV with CMV achieves large increases (e.g., MultiArith Instruct-GPT CoT 96.13% -&gt; CoT+SV 99.15%).",
            "performance_without_reflection": "Baseline CoT and TFV comparisons: CoT alone yields lower accuracy (see dataset-specific CoT baseline numbers in Table 1). TFV results are generally lower than CMV for arithmetic datasets.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineering with token masking of condition attributes; repeated sampling and CoT-style re-solving of masked attribute; scoring is simple match/indicator-based.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical: CMV consistently outperforms TFV for arithmetic datasets (Figure 6) and multi-condition masking improves verification accuracy (Figure 5). CMV-based SV produced the largest dataset gains on arithmetic tasks reported in Table 1.",
            "limitations_or_failure_cases": "Requires reliable identification of meaningful maskable conditions (regex-based). Masking irrelevant conditions degrades verification quality (example: masking hours on Friday in a problem where it doesn't affect answer). CMV is less directly applicable to non-numeric QA without clear maskable attributes.",
            "comparison_to_other_methods": "Compared directly to True-False Item Verification (TFV): CMV generally outperforms TFV on arithmetic tasks; TFV remains more broadly applicable for non-arithmetic tasks but with smaller gains.",
            "ablation_study_results": "Multi-condition masking outperforms single-condition masking; increasing number of masked conditions generally increases verification accuracy (Figure 5). P (repeats) ablation shows greater P improves score stability up to diminishing returns (Figure 7).",
            "uuid": "e8937.1"
        },
        {
            "name_short": "True-False Item Verification (TFV)",
            "name_full": "True-False Item Verification",
            "brief_description": "A general backward verification strategy that appends a 'True or False?' question asking whether the set of conditions plus the candidate conclusion are mutually satisfied, and uses the LLM's True/False outputs aggregated across repeated samplings as a verification score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruct-GPT (code-davinci-002) / GPT-3 variants",
            "model_description": "Same LLMs as used in the main SV experiments (Instruct-GPT 175B and other GPT-3 variants).",
            "reflection_method_name": "True-False Item Verification (TFV)",
            "reflection_method_description": "Add 'Do it is correct (True or False)?' (or similar) after the full set of conditions (including the rewritten candidate conclusion) and use the LLM to answer True/False under CoT-style prompting; repeat P times and count True votes to score candidates.",
            "task_name": "General QA tasks (CommonsenseQA, Date Understanding) and also applied to arithmetic as a baseline verification",
            "task_description": "Tasks where a boolean judgement about the mutual consistency of conditions and conclusion can be asked.",
            "performance_with_reflection": "TFV-based SV yields modest improvements over CoT in general QA tasks (e.g., CSQA Instruct-GPT CoT 77.42% -&gt; CoT+SV 77.83%) and some arithmetic improvements but generally lower than CMV.",
            "performance_without_reflection": "Baseline CoT numbers as in Table 1 (e.g., CSQA 77.42% for Instruct-GPT CoT).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineering: append a True/False question to the context and aggregate repeated LLM True/False answers as score.",
            "number_of_iterations": null,
            "evidence_for_improvement": "TFV produces measurable but smaller gains compared to CMV and overall CoT baselines, with improvements reported in Table 1 for several datasets (though arithmetic tasks benefit more from CMV). TFV is applicable across more diverse tasks where specific conditions are not easily maskable.",
            "limitations_or_failure_cases": "Less precise than CMV on arithmetic tasks because it yields boolean signals rather than explicit masked-value matches; can fail to use candidate conclusions effectively if the prompt lacks explicit guidance; lower gains on numeric datasets.",
            "comparison_to_other_methods": "Compared to CMV: TFV is simpler and more general but typically yields smaller gains; compared to no verification (CoT) TFV still often improves but to a lesser extent than CMV on arithmetic.",
            "ablation_study_results": "Paper reports TFV improvements but shows CMV &gt; TFV for arithmetic tasks (Figure 6). Repeating P times mitigates score ties and improves confidence estimates.",
            "uuid": "e8937.2"
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency Decoding",
            "brief_description": "A forward-stage decoding strategy that samples multiple chain-of-thought rationales and then aggregates (majority vote) to obtain a final answer; used in this paper as a forward method that can be combined with SV for re-ranking.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Instruct-GPT / GPT-3 (175B) in combination experiments",
            "model_description": "Self-consistency as implemented in prior work (Wang et al., 2023) applied to the same LLMs used here.",
            "reflection_method_name": "Self-Consistency (SC)",
            "reflection_method_description": "Sampling multiple CoT chains (top-K or many samples) during forward reasoning and aggregating answers (e.g., majority vote) to improve robustness; in this paper SC is used to generate Top-2 candidates for SV re-ranking.",
            "task_name": "Same reasoning benchmarks (GSM8K, SingleEq, AddSub, MultiArith, AQUA-RAT, SVAMP, CSQA)",
            "task_description": "Benchmarks used for reasoning evaluation; SC was used as an alternative forward-generation method whose outputs were then re-ranked by SV.",
            "performance_with_reflection": "Examples: Instruct-GPT SC: GSM8K 78.00% -&gt; SC+Self-Verification 78.32% (small improvement); other datasets show small positive deltas when SV re-ranks SC outputs (Table 1).",
            "performance_without_reflection": "SC alone: GSM8K 78.00% (Instruct-GPT), SingleEq 96.78%, AddSub 91.64%, etc. (Table 1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Forward sampling plus majority/aggregation; in this paper SC is combined with SV by feeding top candidates from SC into the SV backward verifier for re-ranking.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Combining SC with SV yields marginal but consistent improvements over SC alone (e.g., GSM8K +0.32% absolute), demonstrating SV can add backward-validation benefits on top of diverse forward samplings.",
            "limitations_or_failure_cases": "Improvements from applying SV on top of SC are generally small when forward reasoning is already strong; combining methods increases computational cost.",
            "comparison_to_other_methods": "Compared to plain CoT, SC gives larger forward gains; SV can re-rank SC outputs but typically yields smaller incremental gains than when re-ranking weaker forward outputs.",
            "uuid": "e8937.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Generate & rank: A multi-task framework for math word problems",
            "rating": 1
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1
        }
    ],
    "cost": 0.01564925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Better Reasoners with Self-Verification</h1>
<p>Yixuan Weng ${ }^{1}$, Minjun Zhu ${ }^{1,2 *}$, Fei Xia ${ }^{1,2}$, Bin $\mathbf{L i}^{3}$, Shizhu He ${ }^{1,21}$, Shengping Liu ${ }^{4}$, Bin Sun ${ }^{3}$, Kang Liu ${ }^{1,2,5}$, Jun Zhao ${ }^{1,2}$<br>${ }^{1}$ The Laboratory of Cognition and Decision Intelligence for Complex Systems, IA, CAS<br>${ }^{2}$ School of Artificial Intelligence, University of Chinese Academy of Sciences<br>${ }^{3}$ College of Electrical and Information Engineering, Hunan University<br>${ }^{4}$ Unisound, Beijing, China ${ }^{5}$ Shanghai Artificial Intelligence Laboratory<br>wengsyx@gmail.com, {shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn</p>
<h4>Abstract</h4>
<p>Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/ Self-Verification.</p>
<h2>1 Introduction</h2>
<p>The ability of reasoning in the process of thinking and decision-making is an essential aspect of human intelligence. Recently, chain of thought (CoT) prompting (Wei et al., 2022) has been a good way to solve the arithmetic, commonsense, and logical reasoning tasks with large language models (LLMs), which help the LLMs simulating the human thinking process when solving complex natural language processing (NLP) tasks. CoT guides LLMs to generate a series of intermediate reasoning steps to address complex problems rather than just predict</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>a final answer. This approach has been shown the advance performances on several challenging NLP tasks, even when using only a few or no training samples (Madaan et al., 2022; Saparov and He, 2022; Fu et al., 2022; Gu et al., 2023).</p>
<p>Although CoT can enable LLMs to solve complex reasoning tasks, it is highly sensitive to individual mistakes and vulnerable to error accumulation (Shen et al., 2021). If a tiny mistake occurs, it can change the meaning deviations of the whole statement (Xiao et al., 2022), leading to incorrect answers (Cobbe et al., 2021). That is especially problematic in using CoT for addressing multi-step precise reasoning (such as mathematical calculation). Due to the lack of the error correction mechanism, it is difficult for the LLMs to obtain correct results from the possible errors in multiple steps reasoning. Detecting and mitigating errors is essential to improve reasoning capabilities.</p>
<p>Previous methods resolve the above issue by training another verifier to evaluate the correctness of the model's output (Shen et al., 2021; Li et al., 2022). However, there are some drawbacks in these work. On the one hand, training a verifier requires a lot of human annotations and additional fine-tuned models, which limits its widespread use in other tasks and domains. On the other hand, the verifier fine-tuned by a language model is not easily explainable, making it difficult to assess the model's reliability based on its output scores. Therefore, the challenge of obtaining a better reasoner based on the LLMs is to get a verifier that can avoid manual annotation and additional training, so that it can be better extended and migrated to other fields and tasks.</p>
<p>To address this challenge and overcome the limitations of training verifiers, we propose utilizing LLMs as reasoners with self-verification for selecting better prediction results. In numerous decision-making tasks, humans often perform selfverification of inferred conclusions to mitigate mis-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The answer of a question can be verified by masking and predicting the conditions of the original contexts. To mimic the self-verification ability of human, we predict the accuracy of $f_C$ by predicting the original conditions $f_1$ or $f_2$ is right or not based on this conclusion.</p>
<p>takes (Poole and Mackworth, 2010). In this paper, we propose and demonstrate that LLMs possess a similar self-verification ability, the better reasoning with CoT is carried out in the following two steps, <strong>Forward Reasoning</strong> and <strong>Backward Verification</strong>. Specifically, in Forward Reasoning, LLM reasoners generate candidate answers using CoT, and the question and candidate answers form different conclusions to be verified. And in Backward Verification, We mask the original condition and predict its result using another CoT. We rank candidate conclusions based on a verification score, which is calculated by assessing the consistency between the predicted and original condition values. For example, as shown in Figure 1, by taking $f_2$ and $f_C$ as conditions to predict the value of condition attribute in $\hat{f}_1$, the correctness of $f_C$ can be evaluated by comparing the consistency of values of the predicted $\hat{f}_1$ and the original $f_1$ in verification.</p>
<p>Our method employs LLMs for self-verification with only a few prompts, eliminating the need for fine-tuning or gradient updating. This approach enables automatic verification of multiple candidate answers and corresponding conclusions, mitigating deviations from the correct thought chain in the original CoT. Our verification score arises from evaluating each step during the backward verification phase, rather than from the direct output of a neural network model (Cobbe et al., 2021; Li et al., 2022), enhancing the explainability of prediction outcomes and solution processes (Li et al., 2021; Yu et al., 2023; Zhu et al., 2023). We conducted experiments on various open-source datasets for mathematical reasoning, common sense, and logical reasoning tasks, achieving results beyond the baseline (e.g., 60.8 $\rightarrow$ 65.1 on GSM8K, 91.01 $\rightarrow$ 93.40 on SingleEq). In addition, we also attempt to combine our method with some approaches to improving forward reasoning, such as self-consistency (Wang et al., 2023b) and Least-to-Most (Zhou et al., 2023). The experimental results show that our method also improves upon these forward reasoning approaches.</p>
<p>Our contributions are summarized as follows:</p>
<ol>
<li>We propose and prove that large language models (LLMs) can self-verify their prediction results. The proposed method can provide interpretable verification scores without the need for train additional verifiers.</li>
<li>We have conducted extensive experiments with multiple LLMs, and the experimental results on multiple mathematical, commonsense, and logical reasoning datasets achieve a significant improvement compared to the baseline.</li>
<li>We introduced True-False Item Verification for General Tasks in the backward verification stage and proposed Condition Mask Verification based on the characteristics of Arithmetic Tasks. Our method can be applied to a wide range of reasoning datasets, potentially paving the way for self-validation to become a new paradigm following pre-training and prompt learning, thus motivating further exploration of the capabilities of LLMs.</li>
</ol>
<h2>2 Related Work</h2>
<p><strong>Language Model Reasoning</strong>. It has been extensively studied in order to evaluate the various reasoning abilities of language models (Arora et al., 2022; Madaan et al., 2022; Sun et al., 2022), including arithmetic reasoning (Koncel-Kedziorski et al., 2015; Roy and Roth, 2016; Patel et al., 2021; Cobbe et al., 2021), commonsense reasoning (Talmor et al., 2018; Bhagavatula et al., 2019; Geva</p>
<p>et al., 2021; Zhu et al., 2022b), and logical reasoning (Liu et al., 2020; Yu et al., 2020). To solve these reasoning tasks, researchers have utilized pre-trained language reasoning models (Asai and Hajishirzi, 2020; Deng et al., 2021; Xia et al., 2022) or fine-tuned general LLMs (Cobbe et al., 2021). Early work attempted to solve complex reasoning tasks using Seq2Seq models (Wang et al., 2018; Li et al., 2019). Later, specialized encoder-decoder architectures were designed to improve reasoning performance (Shen and Jin, 2020; Zhu et al., 2022a). More recent work has suggested to adopt pre-training tasks to improve arithmetic reasoning ability (Yoran et al., 2021; Wang et al., 2022b). However, these methods require a significant amount of human annotation. In this paper, we proposed to obtain answers automatically and verify them in multiple reasoning tasks.</p>
<p>In-context Learning. Large language models such as GPT-3 exhibit impressive few-shot learning ability (Lu et al., 2022; Qiao et al., 2022), and closely approximate the predictors computed by gradient descent (Akyürek et al., 2022). It requires only filling a few exemplars into context as prompts and without the need for finetuning on a dataset of training examples (Wang et al., 2022a; Weng et al., 2023a). However, this approach struggles with tasks requiring complex reasoning (Rae et al., 2021), which drives researchers to explore other prompting strategies. CoT (Wei et al., 2022) is a chained reasoning approach that inserts a multi-step reasoning path before generating the final answer. Wang et al. (2023c) proposed a self-consistency decoding strategy to vote on the reasoning path, and Kojima et al. (2022) demonstrated that LLMs could as zero-shot reasoners through the prompt "Let's think step-by-step". These methods focus on constructing the CoT but ignore the high sensitivity of LLMs to individual mistakes in generating these chains, so some of these conclusions by CoT may be unreliable (Dhuliawala et al., 2023; Chu et al., 2023; Weng et al., 2023b). In this paper, we proved that LLMs can self-verify their conclusions.</p>
<p>Answer Verification. It is a common method for evaluating and reordering candidate answers with a trained language understanding model. Kushman et al. (2014) train a classifier to select the best answer from candidate answers, while Roy and Roth (2016) train a global scoring model to guide the search process for better answers. Shen et al. (2021) proposed the joint training of answer generation and rank with language model. Cobbe et al. (2021) and Lightman et al. (2023) fine-tunes language model as a verifier, which calculates token-level and solution-level verification scores for a predicate result. However, the above method all need additional annotations. In our work, we do not require training examples and can provide an explainable verification score.</p>
<h2>3 The Proposed Method</h2>
<p>The proposed method can be used to verify prediction results. As shown in Figure 2, the process mainly consists of two steps. The first step, forward reasoning, is similar to the normal CoT, except that multiple candidate answers are generated through sampling decoding. In the second step, we calculate the verification scores for each candidate's answer by the self-verification method, and the answer with the highest score is selected as the final answer.</p>
<h3>3.1 Forward Reasoning</h3>
<p>In forward reasoning, the LLM reasoners generate candidate answers with the chain of thought prompting. We augment the input with several CoT prompts similar to the original query and then send it to the LLM. The LLM then performs sampling decoding to generate multiple candidates for verification.</p>
<p>As shown in Figure 2, for a reasoning task, the large language model $\mathcal{L} \mathcal{L M}$ is given a question $\mathcal{X}$ which is accompanied by a chain of thought set C . In few-shot setting, the whole prompt also contains other question-CoT prompt-answer tuples. The input $\mathcal{X}$ can be further subdivided into $\mathcal{X}=\left{f_{1}, f_{2}, \ldots, f_{R}, q\right}$, where each $f_{i}$ is a condition (fact), and $q$ is a question, both represented as natural language clause or sub-sentences.</p>
<p>Specifically, in order to generate step-by-step solutions with CoT, we followed Wei et al. (2022) and designed CoT prompt set C for the reasoning dataset (e.g., the GSM8K dataset), which contains $n$ samples, each sample has the question $\mathcal{X}$, chain of thout $t$, and the answer $\hat{y}$. These samples are used as the input of test-time. Each example in C is concatenated as a prompt:</p>
<p>$$
\mathrm{C}=\left(\hat{\mathcal{X}}<em 0="0">{0}, \hat{\mathbf{t}}</em>}, \hat{\mathbf{y}<em 1="1">{0}\right) ;\left(\hat{\mathcal{X}}</em>}, \hat{\mathbf{t}<em 1="1">{1}, \hat{\mathbf{y}}</em>}\right) ; \ldots ;\left(\hat{\mathcal{X}<em n="n">{n}, \hat{\mathbf{t}}</em>\right)
$$}, \hat{\mathbf{y}}_{n</p>
<p>Therefore, $\mathcal{L} \mathcal{L M}$ is required to follow the prompt of C to generate the chain of thought $\mathbf{t}_{\text {CoT }}$ before generating the final answer $\mathbf{y}$ :</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of self-verification. In the step one, LLM generates candidate answers and forms different conclusions. Then, in the step two, LLM verifies these conclusions in turn and computes the verification score.</p>
<p>$$
\mathcal{P}(\mathbf{y} \mid \mathrm{C}, \mathcal{X})=\mathcal{P}(\mathbf{t}<em _mathbf_C="\mathbf{C">{\mathbf{C} \mathbf{O} \mathbf{T}} \mid \mathrm{C}, \mathcal{X}) \times \mathcal{P})(\mathbf{y} \mid \mathrm{C}, \mathcal{X}, \mathbf{t}</em>)
$$} \mathbf{O} \mathbf{T}</p>
<p>To ensure the diversity of different answers, we adapt sampling decoding (Radford et al., 2019) to generate multiple $\mathbf{y}$ for $K$ times. Specifically, sampling decoding is a random decoding method, which can select the next word by sampling from a probability distribution over the possible words at each step. Multiple candidate answers can be obtained when repeatedly using sampling decoding. For example, we generate "18" and "2" as candidate answers in the example of Figure 2.</p>
<h3>3.2 Backward Verification</h3>
<p>Step 1 may generate multiple different answers, this step is used to verify and select the best answer. Backward verification involves several sub-steps. First, the original question with each candidate's answer is rewritten as a conclusion and then supplemented as a new condition (incarnadine color in Figure 2). Then, we considered two methods to construct new questions. In the general QA task, the True-False Item Verification is given based on all the conditions, asking the LLM whether these conditions are mutually satisfied, it has a broad applicability. In Arithmetic reasoning tasks, as the definite condition masks can indicate the reasoning direction of the language model, we propose the Condition Mask Verification method to design questions for the verification stage. Finally, we perform multiple experiments to compute the verification score by comparing the consistency between the predicted condition value and the original masked condition value, and select the candidate answer with the highest score as the final answer.</p>
<h3>3.2.1 Rewritten Candidate Conclusion</h3>
<p>Besides, we rewrite the original question with the candidate's answer as a conclusion and then supplement it as a new condition in the backward verification step. Specifically, we use the instruction prompt "Please change the questions and answers into complete declarative sentences [q] The answer is [y]" to change q and y into new declarative sentence $f_{\mathcal{Y}}$ by $\mathcal{L} \mathcal{L} \mathcal{M}$. As shown in Figure 2, we can rewrite the question and conclusion as "Jackie has 18 apples more than Adam".</p>
<h3>3.2.2 Condition Masking</h3>
<p>For question generation, the diversity of the problems makes it difficult to balance the need for coherence and fact consistency between questions and answers in practical operation (Sun et al., 2018; Ji et al., 2022). To tackle this issue, we included clear questions asking the language model to accurately predict.</p>
<p>True-False Item Verification (TFV). This approach can be applied to a wide range of reasoning QA tasks. We directly add "Do it is correct (True or False)?" after all the conditions, requiring the LLM to self-evaluate the correctness of these conditions.</p>
<p>Condition Mask Verification (CMV). Further,</p>
<p>we use regular expressions to filter out specific conditions, such as numbers, and then mask them in turn. If we do not mask all conditions but randomly select a condition, unnecessary conditions may be masked, which will significantly impact the verification answer. For example, "Dana worked 9 hours on Friday, 10 hours on Saturday, and 3 hours on Sunday. She earns $\$ 13$ per hour. How much money did Dana earn in weekend?", since condition 1 (9 hours) does not affect the conclusion, it is difficult to predict it correctly. We replace all occurrences of $f$ in the original $\mathcal{X}$ with " $X$ " in turn, and ask $\mathcal{L} \mathcal{L M}$ to re-predict it. Then we rewrite the question. For example, we might find a value in $f_{1}$ and replace it with " $X$ ". We can then add "What is the answer of ' $X$ '?" to the end of the new question, effectively turning it into an equation. This technique helps to guide the language model towards the correct answer.</p>
<h3>3.2.3 Verification Score Calculation</h3>
<p>This backward verification chain of thought is similar to solving an equation. We design a chain of thought prompt, like forward reasoning, to guide LLM in generating a solving process. We input the newly constructed sentences into $\mathcal{L} \mathcal{L M}$. For TFV, we can directly count the number of answers that are True as the score, and for CMV, we will match its final result with the masked condition.</p>
<p>Due to the limited performance of LLM itself, if the condition is verified only once in the backward verification step, it is easy to have the same score, resulting in a lack of differentiation. To address this, we repeat the sampling decoding process $P$ times, so that the verification score can more accurately reflect the model's confidence for a given conclusion (Erd, 1970).</p>
<p>The verification score is calculated as follows:</p>
<p>$$
\operatorname{Score}<em p="1">{\mathbf{y}}= \begin{cases}\sum</em>}^{P}\left(\sum_{r=1}^{R} 1_{\left(\mathcal{L} \mathcal{L M<em r="r">{p}\left(\mathcal{X}-f</em>}+f_{p}\right)=f_{r}\right)}\right) &amp; \text { TFV } \ \sum_{p=1}^{P}\left(1_{\left(\mathcal{L} \mathcal{L M<em p="p">{p}\left(\mathcal{X}+f</em>
$$}\right)\right)}\right) &amp; \text { CMV }\end{cases</p>
<p>Where $1_{(\bullet)}$ is an indicator function.
Finally, we select the one with the highest verification score from the K candidate answers generated as a result.</p>
<p>$$
\text { Output }=\underset{k \in[0, K]}{\operatorname{argmax}}\left(\operatorname{Score}_{k}\right)
$$</p>
<p>For example for CMV, in Figure 2.3)Verification, we match the results generated by the selfverification of LLM with the masked conditions.
There is one " 10 " in the conclusion of $A_{1}$, so the verification score is 1 . There are four correct results in $A_{2}$, so the verification score is 4 , and we finally choose $A_{2}$, which has the highest verification score, as the final conclusion.</p>
<h2>4 Experiment Setting</h2>
<h3>4.1 Task and Dataset</h3>
<p>We evaluated eight datasets on three reasoning tasks: arithmetic reasoning, commonsense reasoning, and logical reasoning. These datasets are highly heterogeneous in terms of their input formats (see Appendix A. 2 for the detailed description of each dataset. Examples of different datasets are given in Table 7 of Appendix A.4).</p>
<ul>
<li>Arithmetic Reasoning. We performed experiments on the following 6 arithmetic datasets: SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014), MultiArith (Roy and Roth, 2016), AQUA-RAT (Ling et al., 2017), GSM8K (Cobbe et al., 2021), and SVAMP (Arkil et al., 2021).</li>
<li>Commonsense Reasoning. CommonsenseQA (CSQA) (Talmor et al., 2018) is the most typical dataset of the task, which requires commonsense knowledge about the world to accurately answer questions with complex meanings.</li>
<li>Logical Reasoning. Date Understanding (DU) (Srivastava et al., 2022) involves inferring a date from a given context.</li>
</ul>
<h3>4.2 Model</h3>
<p>We conducted experiments to evaluate the original GPT-3 (Chen et al., 2021) (code-davinci-001) model and the Instruct-GPT model (Ouyang et al., 2022) (code-davinci-002). Additionally, we conducted analysis experiments with public GPT-3 (Brown et al., 2020). All prediction results of different reasoning tasks and datasets are obtained by OpenAI's API ${ }^{1}$. Appendix A. 3 shows the reproducibility statement.</p>
<h3>4.3 Prompts</h3>
<p>We conducted all experiments in the few-shot setting without any fine-tuning of the original LLM To ensure a fair comparison, we used the same</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Method</td>
<td></td>
<td></td>
<td>Arithmetic Tasks</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>General Tasks</td>
<td></td>
</tr>
<tr>
<td>Method</td>
<td></td>
<td></td>
<td>GSM8K</td>
<td>SingleEq</td>
<td>AddSub</td>
<td>MultiArith</td>
<td>AQUA-RAT</td>
<td>SVAMP</td>
<td>CSQA</td>
</tr>
<tr>
<td>Previous SOTA (Fine-tune)</td>
<td></td>
<td></td>
<td>$35^{\circ} / 57^{\circ}$</td>
<td>$32.5^{\circ}$</td>
<td>$94.9^{\circ}$</td>
<td>$60.5^{\circ}$</td>
<td>$37.9^{\top}$</td>
<td>$57.4^{\circ}$</td>
<td>$91.2^{\circ}$</td>
</tr>
<tr>
<td>9-12 year olds</td>
<td></td>
<td></td>
<td>$60^{\prime}$</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-3 Standard</td>
<td></td>
<td></td>
<td>19.7</td>
<td>86.8</td>
<td>90.9</td>
<td>44.0</td>
<td>29.5</td>
<td>69.9</td>
<td>82.3</td>
</tr>
<tr>
<td>GPT-3 (175B) CoT</td>
<td>CoT</td>
<td></td>
<td>13.84</td>
<td>60.20</td>
<td>58.55</td>
<td>45.85</td>
<td>18.90</td>
<td>38.42</td>
<td>46.75</td>
</tr>
<tr>
<td></td>
<td>CoT+Self-Verification</td>
<td></td>
<td>13.92 _{(+0.08)}</td>
<td>60.61 _{(+0.41)}</td>
<td>59.07 _{(+0.52)}</td>
<td>46.19 _{(+0.34)}</td>
<td>27.04 _{(+0.14)}</td>
<td>38.96 _{(+0.54)}</td>
<td>47.68 _{(+0.92)}</td>
</tr>
<tr>
<td>Instruct-GPT (175B) CoT</td>
<td>CoT</td>
<td></td>
<td>60.81</td>
<td>91.01</td>
<td>82.78</td>
<td>96.13</td>
<td>45.30</td>
<td>75.87</td>
<td>77.42</td>
</tr>
<tr>
<td></td>
<td>CoT+Self-Verification</td>
<td></td>
<td>65.14 _{(+0.20)}</td>
<td>93.40 _{(+0.99)}</td>
<td>86.33 _{(+0.50)}</td>
<td>99.15 _{(+0.02)}</td>
<td>47.95 _{(+0.03)}</td>
<td>76.99 _{(+1.12)}</td>
<td>77.83 _{(+0.41)}</td>
</tr>
<tr>
<td>Self-Consistency Decoding (Wang et al., 2023c) For Forward Reasoning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 (175B) CoT</td>
<td>SC</td>
<td></td>
<td>23.40</td>
<td>70.25</td>
<td>68.65</td>
<td>79.82</td>
<td>25.60</td>
<td>54.58</td>
<td>54.92</td>
</tr>
<tr>
<td></td>
<td>SC+ Self-Verification</td>
<td></td>
<td>23.59 _{(+0.19)}</td>
<td>70.50 _{(+0.25)}</td>
<td>68.71 _{(+0.06)}</td>
<td>80.01 _{(+0.19)}</td>
<td>28.98 _{(+3.00)}</td>
<td>54.68 _{(+0.1)}</td>
<td>55.09 _{(+0.17)}</td>
</tr>
<tr>
<td>Instruct-GPT (175B) SC</td>
<td>SC</td>
<td></td>
<td>78.00</td>
<td>96.78</td>
<td>91.64</td>
<td>100.0</td>
<td>52.01</td>
<td>86.77</td>
<td>81.43</td>
</tr>
<tr>
<td></td>
<td>SC+Self-Verification</td>
<td></td>
<td>78.32 _{(+0.52)}</td>
<td>96.85 _{(+0.07)}</td>
<td>92.03 _{(+0.39)}</td>
<td>100.0 _{(+0.0)}</td>
<td>52.25 _{(+0.24)}</td>
<td>86.94 _{(+0.17)}</td>
<td>81.53 _{(+0.1)}</td>
</tr>
<tr>
<td>PAL (Gao et al., 2023) For Forward Reasoning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 (175B) CoT</td>
<td>PAL</td>
<td></td>
<td>31.82</td>
<td>63.98</td>
<td>63.15</td>
<td>61.52</td>
<td>30.56</td>
<td>42.69</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PAL+Self-Verification</td>
<td></td>
<td>32.87 _{(+1.05)}</td>
<td>65.45 _{(+1.47)}</td>
<td>64.15 _{(+1.0)}</td>
<td>61.76 _{(+0.34)}</td>
<td>30.90 _{(+0.34)}</td>
<td>42.78 _{(+0.09)}</td>
<td>-</td>
</tr>
<tr>
<td>Instruct-GPT (175B) PAL</td>
<td>PAL</td>
<td></td>
<td>72.02</td>
<td>96.08</td>
<td>92.64</td>
<td>99.15</td>
<td>59.75</td>
<td>79.45</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PAL+Self-Verification</td>
<td></td>
<td>72.89 _{(+0.87)}</td>
<td>96.52 _{(+0.34)}</td>
<td>93.78 _{(+1.14)}</td>
<td>99.87 _{(+0.72)}</td>
<td>60.21 _{(+0.46)}</td>
<td>80.24 _{(+0.79)}</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 1: Problem solve rate (%) on reasoning datasets. The previous SoTA results (baselines) are respectively obtained from: (a) GPT-3 175B finetuned (Cobbe et al., 2021); (b) GPT-3 175B finetuned plus an additional 175B verifier (Cobbe et al., 2021); (c) Hu et al. (2019); (d) Roy and Roth (2016); (e) Roy and Roth (2016); (f) Amini et al. (2019); (g) Pi et al. (2022); (h) Xu et al. (2022); (i) (Cobbe et al., 2021). In addition, we also attempted to use self-consistency (SC) (Wang et al., 2023c) or PAL (Gao et al., 2023) (Since this method uses extra programs to replace the results of numerical calculations, we mainly compare it in Arithmetic Tasks.) Decoding to generate candidate answers during the Forward Reasoning stage and combine it with Self-Verification. prompts as in Wei et al. (2022) for forward reasoning. We made several changes of the prompts for backward verification (the details are shown in Appendix A.5).</p>
<h3>4.4 Implementation</h3>
<p>In each experiment, we perform CoT prompting on the LLMs, then LLMs generate conclusions (answers) by sampling decoding without top-k truncation. When forward reasoning, we generated $K=5$ candidate answers (conclusions). In backward verification, each candidate conclusion generated $P=10$ times, and the maximum token length of each decoding was 168. After LLM generates the output, we only select the part of the text that conforms to the conclusion format. Appendix A.1 shows the specific strategy for different tasks. In addition, to ensure a fair comparison, we ran each experiment three times and calculated the average result.</p>
<h2>5 Result and Analysis</h2>
<p>The main experimental results are shown in Table 1. The table shows that the proposed self-verification method (SV) can improve previous methods in all datasets. Our method achieved a new state-of-the-art (SOTA) performance in six of these eight datasets. Appendix A. 4 shows specific examples of language model self-verification for each dataset. Additionally, we observed that self-verification led to an average increase of 2.33\% in the highperforming Instruct-GPT model, which indicates that the model with strong forward reasoning capabilities also benefits from the self-verification mechanism. The detailed experimental conclusions and analysis are described as follows:</p>
<p>The current self-verification method is more suitable for arithmetic reasoning tasks than other reasoning tasks. We find that the average performance improvement of arithmetic reasoning tasks $(1.67 \% / 2.84 \% \uparrow)$ is higher than that of other reasoning tasks $(0.62 \% / 0.78 \% \uparrow)$ in Table 1. We believe the reason is that it is easier to find the required mask conditions for arithmetic reasoning</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Problem solve rate (\%) in difference size models. The text-ada-001 (0.4B), text-babbage-001 (1.3B), text-curie-001 (7B) and text-davinci-002 (175B) models are used respectively.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Subtract the problem solve rate (\%) of CoT from the problem solve rate (\%) of self-verification in different size models. The pink area means that the use of self-verification has a negative impact.</p>
<p>Figure 3: The self-verification ability of models with different sizes.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Problem solve rate (\%) comparison of 2-shot to 8 -shot prompts.
tasks, but other reasoning tasks used TFV that cannot determine the exact conditions. In the future, we will consider the targeted condition selection and masking for other reasoning tasks.</p>
<p>The self-validation method can be combined with improved methods for forward reasoning. We report the results of combining selfconsistency or PoL at the bottom of Table 1 separately. Specifically, for combining self-consistency, we use the Top-2 candidate results obtained from self-consistency in the Forward Reasoning stage and then use self-validation to re-rank the candidate results; for combining PAL, we require the generation of runnable programs in Forward Reasoning to obtain candidate answers. We find that this approach still can achieve better performance than self-consistency, demonstrating that self-verification can be combined with a series of existing methods for improving forward calculation to achieve further gains. We believe that the self-
verification can re-rank candidate answers from the perspective of backward validation, providing more robust results.</p>
<p>Larger language models are better reasoners with self-verification. Figure 3 shows the capability of GPT-3 models with parameters ranging from 0.4B to 175B. The experimental results suggest that the self-verification ability tends to be more robust as the number of parameters increases, aligning with the limited experimental results in Wei et al. (2022). This indicates that self-verification is an emergent property that arises in larger models, where stronger reasoning capacities allow them to derive reliable and accurate conclusions during the context learning process (Ho et al., 2022; Wang et al., 2023a). Consequently, their performance in the backward verification phase is also enhanced. However, smaller models are prone to generate erroneous text during the generation process, and augment them with self-verification abilities presents a challenge.</p>
<p>With the different number of few-shots, the reasoning ability of models using selfverification has significantly improved. Figure 4 demonstrate the impact of different sample sizes on three arithmetic reasoning datasets. We observe that the self-verification method exhibits greater robustness with smaller samples, even with only 2-shots (At this time, it has $99.6 \%$ performance of 8 -shot, while CoT has only $98.7 \%$ ). In addition, we find that even with only four samples ( 2 CoT samples +2 self-verification samples), self-verification outperforms the CoT with eight samples, which</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Comparison of problem solve rate (%) between single-condition verification and multiple-condition verification.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Comparison of problem solve rate (%) for the "CMV" and the "TFV" in arithmetic tasks.</p>
<p>highlights the importance of answer verification in scenarios of limited data.</p>
<p><strong>The more verification conditions are used, the better self-verification reasoning ability.</strong> We observed the effect of using the single conditional mask on six different arithmetic datasets for Condition Mask Verificat in Figure 5. As each number in these datasets' input can be thought of as a condition, we can study the impact of increasing the number of validation conditions. In most experiments, we found that the multi-condition mask performed better than the single-condition mask, and both performed better than the original CoT. These results suggest that the accuracy of verification scores improves as the number of available conditions increases.</p>
<p><strong>Masked conditions can guide the LLMs to reason more effectively.</strong> As shown in Figure 6, we compared the results of using CMV (Conditional Masked Verification) and TFV (Token Form Verification) for self-verification. We found that the performance of CMV is generally better than TFV. We believe this is because the lack of explicit goals can lead to a lack of use of existing conclusions, so CMV is more helpful in stimulating the self-</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: The computational resource of the proposed method on GSM8K.</p>
<p>verification ability of the model. However, due to its simplicity, TFV can be applied to a variety of tasks (including common sense reasoning and logical reasoning, both with improvements compared to the CoT baseline) for self-verification, making it highly adaptable to different scenarios.</p>
<p><strong>Fewer computational resources can also improve performance through self-verification.</strong> In Figure 7, we show the results of changing the number of <em>P</em> generated in Backward Verification. We find that even when <em>P</em> = 2, only a small increase in computational overhead is needed, and there is still an improvement in CoT baseline. Considering that performance starts to slowly increase when P is</p>
<p>increased to 10, we recommend choosing an appropriate value for P (e.g. $\mathrm{P}=10$ ) to achieve a balance between performance and resource consumption.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Num.</th>
<th style="text-align: center;">Avg. Ans.</th>
<th style="text-align: center;">Ran. Acc.</th>
<th style="text-align: center;">CoT Acc.</th>
<th style="text-align: center;">Veri. Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">$601 / 1306$</td>
<td style="text-align: center;">2.80</td>
<td style="text-align: center;">$35.7 \%$</td>
<td style="text-align: center;">$53.4 \%$</td>
<td style="text-align: center;">$\mathbf{5 8 . 9 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Addsub</td>
<td style="text-align: center;">$77 / 377$</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">$46.9 \%$</td>
<td style="text-align: center;">$56.7 \%$</td>
<td style="text-align: center;">$\mathbf{7 4 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">AQUA</td>
<td style="text-align: center;">$123 / 219$</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">$36.9 \%$</td>
<td style="text-align: center;">$50.3 \%$</td>
<td style="text-align: center;">$\mathbf{5 1 . 2 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Multiarith</td>
<td style="text-align: center;">$73 / 593$</td>
<td style="text-align: center;">2.19</td>
<td style="text-align: center;">$45.6 \%$</td>
<td style="text-align: center;">$71.2 \%$</td>
<td style="text-align: center;">$\mathbf{8 9 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">SingleEq</td>
<td style="text-align: center;">$51 / 501$</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">$43.3 \%$</td>
<td style="text-align: center;">$51.0 \%$</td>
<td style="text-align: center;">$\mathbf{7 4 . 5 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">$274 / 954$</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">$39.4 \%$</td>
<td style="text-align: center;">$51.5 \%$</td>
<td style="text-align: center;">$\mathbf{5 8 . 4 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Further analysis of the experimental results in Table 1. Where "Num." represents the total number of samples in this setting and the total number of original dataset samples, "Avg. Ans." means the average number of candidate answers, "Ran. Acc." = 1 / "Avg. Ans.". "CoT Acc." refers to the accuracy of the CoT method when temperature $=0$. And "Veri. Acc." refers to the probability of picking out the correct answer from the candidate answers in backward verification stage.</p>
<p>Analysis of the accuracy of the verification results. We conducted additional analysis of the InstructGPT results in Table 1 under a new setting where we only considered cases in the forward inference phase that contained one correct answer and $N$ incorrect answers (where $N&gt;=1$ and $N&lt;=4$ ). The results in Table 2 provide further evidence that the proposed self-verification technique can effectively improve the accuracy of commonsense reasoning models. Across all 6 datasets, the verification accuracy is consistently and considerably higher than both the random guessing baseline and the standalone CoT model accuracy. For example, on the challenging GSM8K dataset, the verification stage obtains $58.9 \%$ accuracy, substantially outperforming the $53.4 \%$ CoT accuracy and $35.7 \%$ random guess accuracy. The largest accuracy gains are witnessed on the MultiArith and SingleEq datasets, where the verification stage lifts the accuracy by $17.8 \%$ and $23.5 \%$ respectively over the CoT model. This indicates that the self-verification technique is particularly adept at rectifying errors made by the CoT model on arithmetic and symbolic equation problems. The consistent accuracy improvements demonstrate that allowing the model to verify its own predictions provides a simple yet effective way to enhance commonsense reasoning. These comprehensive results validate self-verification as a promising approach to refine LLMs and reduce reasoning errors.</p>
<h2>6 Conclusion</h2>
<p>In this study, we show that large language models have a strong ability to self-verification, allowing
them to assess the conclusions they generate accurately. We propose a novel method that uses self-verification to generate interpretable scores for ranking results in few-shot tasks. Our approach demonstrates the potential of using self-verification to improve the accuracy and reliability of large language models in reasoning tasks. By relying on the self-verification ability of large language models, we significantly improved the accuracy of three types of reasoning tasks. All in all, we posit that the self-verification capability of large language models will have an extensive positive impact, as it enables the enhancement of their reasoning abilities through a simple process of self-verification.</p>
<h2>Limitations</h2>
<p>Our self-verification method relies on large language models (LLMs). It provides few-shot prompts to guide the model in verifying its own results, but it is worth noting that these prompts are artificially constructed and may introduce bias. The effectiveness of our method is limited by the presence of accurate answers within the candidate conclusions generated by the LLM, our experiments demonstrate that the capability of self-verification depends on the model's reasoning abilities, which means it is challenging to augment the reasoning performance of smaller language models, while the benefits are greater for high-performing models (in Figure 3). For the sake of usability, our method focuses on the conclusions derived from the reasoning rather than the reasoning process itself, and is thus not suitable for evaluating the LLM's inference procedure. Additionally, the method necessitates generating multiple candidate inference chains and conclusions, leading to increased computational costs; however, we demonstrate that only a minimal increase (merely 1 x ) in expenditure is required to substantially enhance the model's inference capabilities (in Figure 7).</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Key R\&amp;D Program of China (No.2022ZD0118501) and the National Natural Science Foundation of China (No.62376270, No.U1936207, No.61976211, No.62171183). This work was supported by the Strategic Priority Research Program of Chinese Academy of Sciences (No.XDA27020100), Youth Innovation Promotion Association CAS, and OPPO Research Fund.</p>
<h2>References</h2>
<p>Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661.</p>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik KoncelKedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. north american chapter of the association for computational linguistics.</p>
<p>Patel Arkil, Bhattamishra Satwik, and Goyal Navin. 2021. Are nlp models really able to solve simple math word problems?</p>
<p>Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel J. Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher Ré. 2022. Ask me anything: A simple strategy for prompting language models.</p>
<p>Akari Asai and Hannaneh Hajishirzi. 2020. Logicguided data augmentation and regularization for consistent question answering. meeting of the association for computational linguistics.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen tau Yih, and Yejin Choi. 2019. Abductive commonsense reasoning. Learning.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374.</p>
<p>Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,</p>
<p>Bing Qin, and Ting Liu. 2023. A survey of chain of thought reasoning: Advances, frontiers and future.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, and Huan Sun. 2021. Reasonbert: Pre-trained to reason with distant supervision. empirical methods in natural language processing.</p>
<p>Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models. ArXiv, abs/2309.11495.</p>
<p>Paul Erd. 1970. On a new law of large numbers. J. Anal. Math, 23(103111):8.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics.</p>
<p>Yuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Lei Liu, Xiaoyan Zhu, and Minlie Huang. 2023. Eva2.0: Investigating open-domain chinese dialogue systems with large-scale pre-training. Machine Intelligence Research, 20(2):207-219.</p>
<p>Charles R Harris, K Jarrod Millman, Stéfan J Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. 2020. Array programming with numpy. Nature, 585(7825):357-362.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. empirical methods in natural language processing.</p>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. 2019. A multi-type multi-span network for reading comprehension that requires discrete reasoning. empirical methods in natural language processing.</p>
<p>Tianbo Ji, Chenyang Lyu, Gareth Jones, Liting Zhou, and Yvette Graham. 2022. Qascore - an unsupervised unreferenced metric for the question generation evaluation.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 271-281, Baltimore, Maryland. Association for Computational Linguistics.</p>
<p>Bin Li, Encheng Chen, Hongru Liu, Yixuan Weng, Bin Sun, Shutao Li, Yongping Bai, and Meiling Hu. 2021. More but correct: Generating diversified and entity-revised medical response. arXiv preprint arXiv:2108.01266.</p>
<p>Jierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian Dai, and Dongxiang Zhang. 2019. Modeling intrarelation in math word problems with different functional multi-head attentions. meeting of the association for computational linguistics.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. ArXiv, abs/2305.20050.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation : Learning to solve and explain algebraic word problems. meeting of the association for computational linguistics.</p>
<p>Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. international joint conference on artificial intelligence.</p>
<p>Jinghui Lu, Rui Zhao, Brian Mac Namee, Dongsheng Zhu, Weidong Han, and Fei Tan. 2022. What makes pre-trained language models better zero/few-shot learners?</p>
<p>Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems. north american chapter of the association for computational linguistics.</p>
<p>Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu Chen. 2022. Reasoning like program executors.</p>
<p>David L Poole and Alan K Mackworth. 2010. Artificial Intelligence: foundations of computational agents. Cambridge University Press.</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with language model prompting: A survey.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. CoRR, abs/2112.11446.</p>
<p>Subhro Roy and Dan Roth. 2016. Solving general arithmetic word problems. arXiv: Computation and Language.</p>
<p>Abulhair Saparov and He He. 2022. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021. Generate \&amp; rank: A multi-task framework for math word problems. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2269-2279.</p>
<p>Yibin Shen and Cheqing Jin. 2020. Solving math word problems with multi-encoders and multi-decoders. international conference on computational linguistics.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka{s}, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart{I}omiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria</p>
<p>Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha SohlDickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesuijoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia ContrerasOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem {S}enel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Micha{I} Sw{e}drowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennghoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Mi{I}kowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramón Risco Delgado, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam</p>
<p>Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima (Shammie) Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, SooHwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.</p>
<p>Tian-Xiang Sun, Xiang-Yang Liu, Xi-Peng Qiu, and Xuan-Jing Huang. 2022. Paradigm shift in natural language processing. Machine Intelligence Research, 19(3):169-183.</p>
<p>Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, and Shi Wang. 2018. Answer-focused and positionaware neural question generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3930-3939, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. north american chapter of the association for computational linguistics.</p>
<p>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022a. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, and Xiaojiang Liu. 2018. Translating a math word problem to a expression tree. empirical methods in natural language processing.</p>
<p>Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2022b. Logic-driven context extension and data augmentation for logical reasoning of text. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1619-1629, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023a. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Yixuan Weng, Bin Li, Fei Xia, Minjun Zhu, Bing Sun, Shizhu He, Kang Liu, and Jun Zhao. 2023a. Large language models need holistically thought in medical conversational qa. ArXiv, abs/2305.05410.</p>
<p>Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. 2023b. Mastering symbolic operations: Augmenting language models with compiled neural networks.</p>
<p>Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu, Bin Sun, Shutao Li, and Jun Zhao. 2022. Medconqa: Medical conversational question answering system based on knowledge graphs. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. 2022. A survey on non-autoregressive generation for neural machine translation and beyond.</p>
<p>Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, and Xuedong Huang. 2022. Human parity on commonsenseqa: Augmenting self-attention with external attention.</p>
<p>Ori Yoran, Alon Talmor, and Jonathan Berant. 2021. Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills. arXiv: Computation and Language.</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset requiring logical reasoning. international conference on learning representations.</p>
<p>Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. 2023. Towards better chain-of-thought prompting strategies: A survey.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models.</p>
<p>Minjun Zhu, Bin Li, Yixuan Weng, and F. Xia. 2022a. A knowledge storage and semantic space alignment method for multi-documents dialogue generation. In Workshop on Document-grounded Dialogue and Conversational Question Answering.</p>
<p>Minjun Zhu, Yixuan Weng, Shizhu He, Kang Liu, and Jun Zhao. 2022b. Reasonchainqa: Text-based complex question answering with explainable evidence chains. ArXiv, abs/2210.08763.</p>
<p>Minjun Zhu, Yixuan Weng, Shizhu He, Cunguang Wang, Kang Liu, Li Cai, and Jun Zhao. 2023. Learning to build reasoning chains by reliable path retrieval. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE.</p>
<h2>A Appendix</h2>
<h2>A. 1 Answer Cleansing</h2>
<p>Our answer cleaning strategy is consistent with Wang et al. (2023b) and Zhang et al. (2022). The first number after selecting "The answer is" is regarded as an output for arithmetic tasks, and we use Numpy (Harris et al., 2020) to compare it with the standard answer. For multiple choice tasks, we regard the first capital letter as output.</p>
<p>For the "True-False Item Verification", we use "True or False" to select answer. Table 3 summarizes a list of answer cleansing approaches used across all the experiments.</p>
<h2>A. 2 Dataset Details</h2>
<p>Our method is evaluated on eight benchmark datasets that cover arithmetic reasoning, commonsense reasoning, and logical reasoning tasks. The statistics of the datasets are shown in Table 6.</p>
<p>We list the details for all datasets used in this paper.</p>
<ul>
<li>GSM8K: https://github.com/openai/ grade-school-math</li>
<li>SingleEq: https://gitlab.cs. washington.edu/ALGES/TACL2015</li>
<li>AddSub: https://www.cs.washington. edu/nlp/arithmetic</li>
<li>MultiArith: http://cogcomp.cs. illinois.edu/page/resource_view/98</li>
<li>AQUA-RAT: https://github.com/ deepmind/AQuA</li>
<li>SVAMP: https://github.com/ arkilpatel/SVAMP</li>
<li>CSQA: https://www.tau-nlp.org/ commonsenseqa</li>
<li>Data Understanding: https://github. com/google/BIG-bench</li>
</ul>
<h2>A. 3 Reproducibility Statement</h2>
<p>All our experiments in the main text were run using the OpenAI API on November 20th to December 20th, 2022. The main experiment was run on November 25th to December 10th, the singlecondition rxperiment was run on November 20th to 25th, the Few CoT prompts experiment was run on December 12th, the True-False Item Verification experiment was run on December 12th to 15th, the different sizes models experiment was run on December 16th, and the computational reasource experiment was run on December 18th.</p>
<h2>A. 4 Additional Experiment Results</h2>
<p>In Table 5, we show whether to generate real examples of multiple condition masks. We found that if only the first condition mask is used, the score is zero, and multiple evidence masks can obtain more accurate verification scores.</p>
<p>Then, Table shows the difference between (A) Conditional Masked Verification and (B) TrueFalse Item Verification. The difference between the two lies in whether the condition mask is correct and the template for the question.</p>
<p>Finally, we generated some selfverification examples using the Instruct-GPT (code-davinci-002) model. As show in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Answer <br> Format</th>
<th style="text-align: center;">Answer Cleansing Approach</th>
<th style="text-align: center;">Pseudo Code <br> (Example in Pytorch 3.7)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">Pick up the first number encountered in the text.</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { pred = pred.replace(".", "") } \ &amp; \text { pred = [s for } \mathrm{s} \text { in re.findall(r'-7\d+.7\d+ } \ &amp; \text { ', pred)] } \ &amp; \text { pred = pred[0] } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Multiple- <br> Choice</td>
<td style="text-align: center;">Pick up the first large letter encountered in the text.</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { pred = re.findall(r'A</td>
</tr>
<tr>
<td style="text-align: center;">True or False</td>
<td style="text-align: center;">Pick up the first "True" or "False" encountered in the text after removing unnecessary letters.</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { pred = pred.lower() } \ &amp; \text { pred = re.sub("\"</td>
</tr>
<tr>
<td style="text-align: center;">Yes or No</td>
<td style="text-align: center;">Pick up the first "yes" or "no" encountered in the text after removing unnecessary letters.</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { pred = pred.lower() } \ &amp; \text { pred = re.sub("\"</td>
</tr>
<tr>
<td style="text-align: center;">Free Format</td>
<td style="text-align: center;">Just remove unnecessary letters, such as ".".</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { pred = re.sub("\"</td>
</tr>
</tbody>
</table>
<p>Table 3: Detail description of answer cleansing.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: Sample comparison of different verification methods</p>
<h2>A. 5 Forward reasoning prompt</h2>
<p>For fairness, we used the same CoT prompts cue as Wei et al. (2022) in forward reasoning.</p>
<p>As show in Table 8, Table 9, Table 10, Table 11.</p>
<h2>A. 6 Backward Verification prompt</h2>
<p>When backward verification, we manually wrote some examples referring to Wei et al. (2022).</p>
<p>As show in Table 12, Table 13, Table 14, Table 15 .</p>
<h2>A. 7 True-False Item Verification prompt</h2>
<p>In order to test True-False Item Verification, we designed some prompts to guide the LLMs to consider whether the conclusion is wrong.</p>
<p>As show in Table 16, Table 17.</p>
<h1>EXAMPLE</h1>
<p>Q: Tim wanted to make lemonade for a pool party. For a gallon of lemonade, his recipe called for 1 cup of fresh lemon juice. He found that 6 lemons would yield 1 cup of juice. He figured he would need to make 4 gallons of lemonade for the party. His best friend Allen asked if Tim could make an extra gallon for him that was twice as tart as the other gallons. How many lemons will Tim need? A: The answer is 36. (That's the right answer.)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">True-False <br> Item <br> Verification</th>
<th style="text-align: left;">If Tim will need 36 lemons, and he will need to make 4 gallons of lemonade for the party, so he will need <br> $4 * 6=24$ lemons, he will need to make an extra gallonfor him that was twice as tart as the other gallons, <br> so he will need $24 * 2=48$ lemons, $48!=36$, which is different from the theme. The answer is False. [#]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Conditional <br> Masked <br> Verification</td>
<td style="text-align: left;">Tim wanted to make lemonade for a pool party. For a gallon of lemonade, his recipe called for X cup of <br> fresh lemon juice. He found that 6 lemons would yield 1 cup of juice. He figured he would need to make <br> 4 gallons of lemonade for the party. His best friend Allen asked if Tim could make an extra gallon for <br> him that was twice as tart as the other gallons. Tim will need 36.0 lemons. $X * 6 * 4+X * 6 * 2=36$, <br> $36 * X=36, X=1$. The answer is $1 .[\checkmark]$</td>
</tr>
</tbody>
</table>
<p>Table 4: We have selected an actual generated examples to further demonstrate the impact of different verification methods.</p>
<h2>EXAMPLE 2 (Condition is not required)</h2>
<p>Q: Dana worked 9 hours on Friday, 10 hours on Saturday, and 3 hours on Sunday.
She earns $\$ 13$ per hour. How much money did Dana earn in weekend?</p>
<p>A: Dana earns 13 dollars per hour. She worked 10 hours on Saturday, and 3 hours on Sunday. So she earned $13 *(10+3)=13 * 13=169$ dollars. The answer is 169. [ $\checkmark]$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Condition 1 mask</th>
<th style="text-align: left;">Condition 2 mask</th>
<th style="text-align: center;">$\ldots$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Q: Dana worked 'X' hours on Friday, 10 hours <br> on Saturday, and 3 hours on Sunday. She earns <br> $\$ 13$ per hour. Dana earn 169 money in weekend.</td>
<td style="text-align: left;">Q: Dana worked 9 hours on Friday, 'X' hours <br> on Saturday, and 3 hours on Sunday. She earns <br> $\$ 13$ per hour. Dana earn 169 money in weekend.</td>
<td style="text-align: center;">$\ldots$</td>
</tr>
<tr>
<td style="text-align: left;">Separate score: 0</td>
<td style="text-align: left;">Separate score: 8</td>
<td style="text-align: center;">$\ldots$</td>
</tr>
</tbody>
</table>
<p>Total verification score: 19</p>
<p>Table 5: This is an example of the need to use multiple Conditions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Number of samples</th>
<th style="text-align: center;">Average words</th>
<th style="text-align: center;">Answer Format</th>
<th style="text-align: center;">Lience</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">SingleEq</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">AddSub</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">Unspecified</td>
</tr>
<tr>
<td style="text-align: left;">MultiArith</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">Unspecified</td>
</tr>
<tr>
<td style="text-align: left;">AQUA-RAT</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">CSQA</td>
<td style="text-align: center;">1221</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Unspecified</td>
</tr>
<tr>
<td style="text-align: left;">Data Understanding</td>
<td style="text-align: center;">369</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">Unspecified</td>
</tr>
</tbody>
</table>
<p>Table 6: Dataset Description.</p>
<p>Table 7: Example outputs by self-verification for each datasets.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Question</th>
<th>Self-Verification</th>
</tr>
</thead>
<tbody>
<tr>
<td>GSM8K</td>
<td>Q: Matthew has a collection of 12 unique toy soldiers. He wants to sell them for a fair price. He found a buyer who is willing to pay for half his collection $\$ 5$ per toy, and for the other half ' X ' per toy. If Matthew would agree to that offer, He would earn 72.0 dollars.</td>
<td>A: Matthew has 12 unique toy soldiers. He wants to sell them for a fair price. He found a buyer who is willing to pay for half his collection $\$ 5$ per toy. and for the other half \$X per toy. If Matthew would agree to that offer. He would earn 72.0 dollars. $12 / 2=66 * 5=3012-6=66 * X=$ $42 \mathrm{X}=42 / 6 \mathrm{X}=7$ The answer is $7 .[\checkmark]$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Continued on next page</td>
</tr>
</tbody>
</table>
<p>Table 7 - Continued from previous page</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Self-Verification</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Four students scored a total of 251 points in a board game. Naomi scored 68 of the points. Yuri scored 'X' more than half as many points as Naomi and Brianna scored 17 points more than Naomi. Jojo scored 54 points.</td>
<td style="text-align: center;">Four students scored a total of 251 points. Naomi scored 68 points. Yuri scored X points. Brianna scored 17 points more than Naomi. Jojo scored 54 points. So: $68+X+68+17+54=251, X=$ $251-68-68-17-54, X=94$. The answer is 94 . $<a href="\mathbf{G T}: \mathbf{1 0}">\times</a>$</td>
</tr>
<tr>
<td style="text-align: center;">SingleEq</td>
<td style="text-align: center;">Q: Sam grew 4 watermelons, but the rabbits ate ' X ' watermelons. Sam have 1 watermelon left.</td>
<td style="text-align: center;">A:Sam grew 4 watermelons. But the rabbits ate $X$ watermelons. So, we can write the following equation: $4-\mathrm{X}=1, \mathrm{X}=4-1, \mathrm{X}=3$. The answer is $3 .[\checkmark]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Melanie, Benny, Sandy, and Jessica each have ' X ' dozen Pokemon cards. They have 432 pokemon cards in all.</td>
<td style="text-align: center;">A: Melanie has X dozen Pokemon cards. Benny has X dozen Pokemon cards. Sandy has X dozen Pokemon cards. Jessica has X dozen Pokemon cards. So, they have 4 X dozen Pokemon cards in total. $X * 4=432, X=432 / 4, X=108$. The answer is 108. $<a href="\mathbf{G T}: \mathbf{9}">\times</a>$</td>
</tr>
<tr>
<td style="text-align: center;">Addsub</td>
<td style="text-align: center;">Q: Karin 's science class weighed plastic rings for an experiment They found that the orange ring weighed ' X ' ounce, the purple ring weighed 0.3333333333333333 ounce, and the white ring weighed 0.416666666666667 ounce . The total weight of the plastic rings is 0.8333333333333334 .</td>
<td style="text-align: center;">A: The orange ring weighed 0.08333333333333333 ounce. the purple ring weighed 0.3333333333333333 ounce. and the white ring weighed 0.4166666666666667 ounce. The total weight of the plastic rings is 0.83333333333333334 . $0.33333333333333333+0.4166666666666667$ $=0.833333333333333340.8333333333333334-$ 0.4166666666666667 $=0.4166666666666667$ 0.4166666666666667 - 0.3333333333333333 $=0.08333333333333334$ The answer is 0.08333333333333334 . $[\checkmark]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: There were 6 roses in the vase <br> . Mary cut some roses from her flower garden . There are now ' X ' roses in the vase. She cut 10 roses.</td>
<td style="text-align: center;">A: There were originally 6 roses in the vase. Mary cut 10 roses from her flower garden. So there are now X roses in the vase. <code> $6-10=\mathrm{X} \mathrm{X}=$ $-4&lt;/$ code $&gt;$ The answer is $-4 .<a href="\mathbf{G T}: \mathbf{1 6}">\times</a>$</td>
</tr>
<tr>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">Q: Luke was organizing his baseball cards in a binder with 3 on each page. If he had 3 new cards and ' X ' old cards to put in the binder, he use 4 pages.</td>
<td style="text-align: center;">A: Luke had 3 new cards and X old cards. He would use 4.0 pages. So, we can write the following equation: $3+X=3 * 4.0,3+X=12, X=12$ $-3 \mathrm{X}=9$. The answer is $9 .[\checkmark]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Will bought 7 boxes of chocolate candy and gave 3 to his little brother. If each box has ' X ' pieces inside it, Will still have 16 pieces.</td>
<td style="text-align: center;">A: Will bought 7 boxes of chocolate candy. Each box has X pieces inside it. He gave 3 boxes to his little brother. So he has 4 boxes left. $7 * \mathrm{X}=\mathrm{Y}$, $\mathrm{Y}-3 * \mathrm{X}=16, \mathrm{Y}=16+3 * \mathrm{X}, \mathrm{Y}=16+3 * \mathrm{X}$. The answer is $16 .<a href="\mathbf{G T}: 4">\times</a>$</td>
</tr>
</tbody>
</table>
<p>Table 7 - Continued from previous page</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Self-Verification</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">Q: Jake has 3 fewer peaches and 10 more apples than Steven. Steven has ' X ' peaches and 15 apples. Steven has 3.0 more apples than peaches.</td>
<td style="text-align: center;">A: Jake has 3 fewer peaches and 10 more apples than Steven. Steven has X peaches and 15 apples. Steven has 3.0 more apples than peaches. $\mathrm{X}+3$ $=15, \mathrm{X}=15-3, \mathrm{X}=1.2$ The answer is $12 .[\checkmark]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: An industrial machine can make 6 shirts a minute. It worked for 5 minutes yesterday and for ' X ' minutes today. The machine made 72 shirts today.</td>
<td style="text-align: center;">A: The machine can make 6 shirts a minute. It worked for 5 minutes yesterday and for $X$ minutes today. The machine made 72 shirts today. So, we can write the following equation: $6 * 5+6 * \mathrm{X}=$ $72,30+6 \mathrm{X}=72,6 \mathrm{X}=72-30,6 \mathrm{X}=42, \mathrm{X}=42$ $/ 6 \mathrm{X}=7$. The answer is $7 .<a href="\mathbf{G T}: \mathbf{1 2}">\times</a>$</td>
</tr>
<tr>
<td style="text-align: center;">CommonsenseQA</td>
<td style="text-align: center;">Q: If there is a pond with trees around it, it likely located in forest.</td>
<td style="text-align: center;">A: he forest is likely located where there is a pond with trees around it. The answer is Yes. [ $\checkmark$ ]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: The flasks was used to distill elements, it is used in chemistry lab.</td>
<td style="text-align: center;">A: The answer should be the place where the flask is used. But the chemistry lab is not suitable place where the flask is used. The answer is No. $<a href="\mathbf{G T}:$ Yes">\times</a></td>
</tr>
<tr>
<td style="text-align: center;">Date <br> Understand.</td>
<td style="text-align: center;">Q: It was 'X'/01/2021 a week ago. Today is 09/08/2021.</td>
<td style="text-align: center;">A: If today is 09/08/2021, so the date a week ago is 09/01/2021. The answer is 09. [ $\checkmark$ ]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Yesterday, Jan 21, 2011, Jane ate 2 pizzas and 5 wings. One week ago from today is 01/15/2011.</td>
<td style="text-align: center;">A: one week ago from today is 01/15/2011. So today is 01/22/2011. So yesterday is 01/21/2011. So Jan 'X'1 is 01/21/2011. So 'X'011 is 2011. So The answer is 2011. $<a href="\mathbf{G T}: \mathbf{B}">\times</a>$</td>
</tr>
</tbody>
</table>
<p>Table 8: Few-shot exemplars for full chain of thought prompt for AQuA.</p>
<h1>Prompt for AQuA algebraic word problems</h1>
<p>Q: John found that the average of 15 numbers is 40 . If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64
A: If 10 is added to each number, then the mean of the numbers also increases by 10 . So the new mean would be 50 . The answer is (a).
Q: If $a / b=3 / 4$ and $8 a+5 b=22$, then find the value of $a$.
Answer Choices: (a) $1 / 2$ (b) $3 / 2$ (c) $5 / 2$ (d) $4 / 2$ (e) $7 / 2$
A: If $a / b=3 / 4$, then $b=4 a / 3$. So $8 a+5(4 a / 3)=22$. This simplifies to $8 a+20 a / 3=22$, which means $44 a / 3=22$. So a is equal to $3 / 2$. The answer is (b).
Q: A person is traveling at $20 \mathrm{~km} / \mathrm{hr}$ and reached his destiny in 2.5 hr then find the distance?
Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km
A: The distance that the person traveled would have been $20 \mathrm{~km} / \mathrm{hr} * 2.5 \mathrm{hrs}=50 \mathrm{~km}$. The answer is (e).
Q: How many keystrokes are needed to type the numbers from 1 to 500 ?
Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788
A: There are 9 one-digit numbers from 1 to 9 . There are 90 two-digit numbers from 10 to 99 . There are 401 three-digit numbers from 100 to $500.9+90(2)+401(3)=1392$. The answer is (b).</p>
<p>Table 9: Few-shot exemplars for full chain of thought prompt for Date Understanding.</p>
<h1>Prompt for Date Understanding</h1>
<p>Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?
A: If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today is 12/30/2014. So one week from today will be 01/05/2015. So the answer is 01/05/2015.</p>
<p>Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY?
A: If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the first monday, would be six days later. So today is 01/07/2019. So the answer is 01/07/2019.</p>
<p>Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?
A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is $05 / 23 / 1943$.</p>
<p>Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?
A: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. So the answer is 04/20/1969.
Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?
A: Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.
Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY?
A: The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so today is 02/28/2017. So yesterday was 02/27/2017. So the answer is 02/27/2017.</p>
<p>Table 10: Few-shot exemplars for full chain of thought prompt for math word problems. This set of exemplars was used for all math word problem datasets except AQuA.</p>
<h1>Prompt for Math Word Problems</h1>
<p>Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 $=6$. The answer is 6 .</p>
<p>Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are originally 3 cars. 2 more cars arrive. $3+2=5$. The answer is 5 .
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had $32+42=74$. After eating 35 , they had $74-$ $35=39$. The answer is 39 .</p>
<p>Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?
A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny $20-12=8$. The answer is 8 .</p>
<p>Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. $5+4=9$. The answer is 9 .</p>
<p>Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?
A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So $5 * 4=20$ computers were added. $9+20$ is 29 . The answer is 29 .</p>
<p>Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?
A: Michael started with 58 golf balls. After losing 23 on tuesday, he had $58-23=35$. After losing 2 more, he had $35-2=$ 33 golf balls. The answer is 33 .</p>
<p>Q: Olivia has $\$ 23$. She bought five bagels for $\$ 3$ each. How much money does she have left?
A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be $5 \times 3=15$ dollars. So she has $23-15$ dollars left. $23-15$ is 8 . The answer is 8 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ OpenAI's API: https://openai.com/api/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
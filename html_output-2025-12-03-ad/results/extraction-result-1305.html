<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1305 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1305</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1305</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-53046511</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1810.05687v3.pdf" target="_blank">Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</a></p>
                <p><strong>Paper Abstract:</strong> We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https://sites.google.com/view/simopt</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1305.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1305.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NVIDIA Flex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NVIDIA Flex (GPU-based physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPU-accelerated physics simulator used by the authors to run highly-parallel rigid- and soft-body simulations for reinforcement learning; it served as the primary simulation engine for training policies and sampling randomized environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>NVIDIA Flex</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>GPU-based physics simulator that uses a maximal-coordinate representation to simulate rigid-body (and soft-body-like) dynamics and supports highly parallel execution of many simulated scene instances on one or more GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotic dynamics (rigid-body and soft-body dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity GPU-based physics simulator (designed to capture detailed rigid-body dynamics and approximate complex dynamics such as rope swinging), but not differentiable and with limitations in exact real-world replication.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>models rigid-body dynamics with maximal-coordinate representation; used to simulate non-rigid phenomena in tasks (e.g. a peg on a soft rope) as practiced in the paper; deterministic core but randomness introduced via sampled simulation parameters; non-differentiable (treated as black-box); supports massive parallelism (many agents per GPU). Does not guarantee exact replication of all real-world phenomena; can suffer from catastrophic failure in unrealistic randomized scenarios (e.g. joint limits reached).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>policy π_{θ,p_φ} (PPO-trained reinforcement learning agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-loop continuous control policies trained with Proximal Policy Optimization (PPO) on a 2-layer fully connected neural network (64 units per layer used in ablations); policies output 7 joint velocity commands (plus gripper command for drawer task).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>robotic manipulation control (swing-peg-in-hole with soft rope dynamics; cabinet drawer opening involving contact dynamics) — i.e., learning closed-loop control policies for dynamic contact and compliant-object manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robots (ABB Yumi and Franka Panda) and target scenes with different object/robot configurations (e.g. shifted cabinet positions)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Swing-peg-in-hole: after two SimOpt iterations, policy trained with Flex + adapted parameter distribution achieved 90% success (18/20 trials) on the real robot; Drawer opening: after distribution adaptation, evaluated on 20 real trials and succeeded in opening the drawer on all trials (100%).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The paper does not compare different simulator engines or intrinsic fidelity levels; it analyzes effects of simulation parameter distribution breadth (domain randomization width) rather than simulator fidelity per se — showing overly wide parameter randomization can produce infeasible scenarios and worsen learning.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue exact replication of real-world dynamics is unnecessary; instead, using a strong physics-simulator prior (Flex) plus adapting simulation parameter distributions from a small number of real roll-outs suffices for successful transfer. They emphasize the simulator need not be differentiable and full real-world state does not need to be observed for updates.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Wide/unconstrained randomization in the simulator produced many infeasible instances (e.g. peg too large, rope too short) causing failure to learn; large variance in key parameters (e.g. cabinet position stddev increased from 2cm to 10cm) led to policies that could not open the drawer. Also, extreme randomization could cause 'catastrophic breakdown' of the physics simulation (robot joints reaching unrealistic limits).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1305.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1305.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-differentiable GPU physics simulator (black-box)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-differentiable GPU-based physics simulator (treated as black-box for parameter optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper treats their simulator as a deterministic, non-differentiable black-box model of system dynamics and optimizes a distribution over its parameters with sampling and a REPS-style (relative-entropy) update to close the sim-to-real loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>non-differentiable GPU-based physics simulator (conceptual; implemented with NVIDIA Flex in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A deterministic physics engine that can be randomized by sampling simulation parameters ξ ∼ p_φ(ξ); because it is non-differentiable the authors optimize parameter distributions with sampling and REPS (relative-entropy constrained updates) rather than gradient-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotic dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity in modeled dynamics (capable of capturing complex contact and compliant behaviors for policy training) though not differentiable and limited by modeling approximations; fidelity is adjustable via parameter distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>deterministic core; probabilistic dynamics are introduced by sampling simulation parameters; supports full-state access for reward computation in sim though only partial observations used from real world; non-differentiable (precludes gradient backprop through physics); amenable to large-scale parallel rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SimOpt-updated policy agents (PPO-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agents trained with PPO on parameterized ensembles of simulator instances; parameter-distribution updates are made via sampling-based REPS optimization using discrepancy costs between simulated and real observation trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>learning robust closed-loop control policies for robotic manipulation tasks (handling contact dynamics and compliant/soft dynamics like ropes) via simulation-then-sparse real-rollout adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real robots and shifted target-scene configurations (e.g. cabinet offsets 15cm and 22cm)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Using SimOpt (sampling + REPS updates) reduced required domain randomization breadth and enabled successful transfer: e.g., drawer transfer required 3 SimOpt iterations for a 15cm offset (versus failing under naive wide domain randomization), and 5 iterations for a 22cm offset; swing-peg-in-hole required 2 SimOpt iterations to reach ~90% real-world success (20 trials).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>No direct comparison of different intrinsic simulator fidelity settings; the paper shows that adapting parameter distributions (narrow→targeted) outperforms naive training on very wide parameter distributions, which often contain infeasible settings that prevent learning.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper states exact replication of the real world is not required and full state observations in the real world are unnecessary; instead, a simulator with a strong prior and adaptable parameter distributions is sufficient for transfer. They note that if a differentiable simulator were available, gradient-based updates could be used, but it is not required.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Failure observed when using overly wide parameter distributions (infeasible task instances and simulator instability), and when attempting trajectory-based parameter learning for soft rope without continuous real-time state observability (cannot set all simulator internal states). Also open-loop replay of real actions in sim was less effective than closed-loop policy rollouts for some partially unobserved dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>EPOpt: Learning robust neural network policies using model ensembles <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>GPU-accelerated robotic simulation for distributed reinforcement learning <em>(Rating: 2)</em></li>
                <li>Fast model identification via physics engines for data-efficient policy search <em>(Rating: 2)</em></li>
                <li>Preparing for the unknown: Learning a universal policy with online system identification <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1305",
    "paper_id": "paper-53046511",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "NVIDIA Flex",
            "name_full": "NVIDIA Flex (GPU-based physics simulator)",
            "brief_description": "A GPU-accelerated physics simulator used by the authors to run highly-parallel rigid- and soft-body simulations for reinforcement learning; it served as the primary simulation engine for training policies and sampling randomized environments.",
            "citation_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
            "mention_or_use": "use",
            "simulator_name": "NVIDIA Flex",
            "simulator_description": "GPU-based physics simulator that uses a maximal-coordinate representation to simulate rigid-body (and soft-body-like) dynamics and supports highly parallel execution of many simulated scene instances on one or more GPUs.",
            "scientific_domain": "mechanics / robotic dynamics (rigid-body and soft-body dynamics)",
            "fidelity_level": "high-fidelity GPU-based physics simulator (designed to capture detailed rigid-body dynamics and approximate complex dynamics such as rope swinging), but not differentiable and with limitations in exact real-world replication.",
            "fidelity_characteristics": "models rigid-body dynamics with maximal-coordinate representation; used to simulate non-rigid phenomena in tasks (e.g. a peg on a soft rope) as practiced in the paper; deterministic core but randomness introduced via sampled simulation parameters; non-differentiable (treated as black-box); supports massive parallelism (many agents per GPU). Does not guarantee exact replication of all real-world phenomena; can suffer from catastrophic failure in unrealistic randomized scenarios (e.g. joint limits reached).",
            "model_or_agent_name": "policy π_{θ,p_φ} (PPO-trained reinforcement learning agent)",
            "model_description": "Closed-loop continuous control policies trained with Proximal Policy Optimization (PPO) on a 2-layer fully connected neural network (64 units per layer used in ablations); policies output 7 joint velocity commands (plus gripper command for drawer task).",
            "reasoning_task": "robotic manipulation control (swing-peg-in-hole with soft rope dynamics; cabinet drawer opening involving contact dynamics) — i.e., learning closed-loop control policies for dynamic contact and compliant-object manipulation.",
            "training_performance": null,
            "transfer_target": "real-world robots (ABB Yumi and Franka Panda) and target scenes with different object/robot configurations (e.g. shifted cabinet positions)",
            "transfer_performance": "Swing-peg-in-hole: after two SimOpt iterations, policy trained with Flex + adapted parameter distribution achieved 90% success (18/20 trials) on the real robot; Drawer opening: after distribution adaptation, evaluated on 20 real trials and succeeded in opening the drawer on all trials (100%).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "The paper does not compare different simulator engines or intrinsic fidelity levels; it analyzes effects of simulation parameter distribution breadth (domain randomization width) rather than simulator fidelity per se — showing overly wide parameter randomization can produce infeasible scenarios and worsen learning.",
            "minimal_fidelity_discussion": "Authors argue exact replication of real-world dynamics is unnecessary; instead, using a strong physics-simulator prior (Flex) plus adapting simulation parameter distributions from a small number of real roll-outs suffices for successful transfer. They emphasize the simulator need not be differentiable and full real-world state does not need to be observed for updates.",
            "failure_cases": "Wide/unconstrained randomization in the simulator produced many infeasible instances (e.g. peg too large, rope too short) causing failure to learn; large variance in key parameters (e.g. cabinet position stddev increased from 2cm to 10cm) led to policies that could not open the drawer. Also, extreme randomization could cause 'catastrophic breakdown' of the physics simulation (robot joints reaching unrealistic limits).",
            "uuid": "e1305.0",
            "source_info": {
                "paper_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Non-differentiable GPU physics simulator (black-box)",
            "name_full": "Non-differentiable GPU-based physics simulator (treated as black-box for parameter optimization)",
            "brief_description": "The paper treats their simulator as a deterministic, non-differentiable black-box model of system dynamics and optimizes a distribution over its parameters with sampling and a REPS-style (relative-entropy) update to close the sim-to-real loop.",
            "citation_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
            "mention_or_use": "use",
            "simulator_name": "non-differentiable GPU-based physics simulator (conceptual; implemented with NVIDIA Flex in experiments)",
            "simulator_description": "A deterministic physics engine that can be randomized by sampling simulation parameters ξ ∼ p_φ(ξ); because it is non-differentiable the authors optimize parameter distributions with sampling and REPS (relative-entropy constrained updates) rather than gradient-based methods.",
            "scientific_domain": "mechanics / robotic dynamics",
            "fidelity_level": "medium-to-high fidelity in modeled dynamics (capable of capturing complex contact and compliant behaviors for policy training) though not differentiable and limited by modeling approximations; fidelity is adjustable via parameter distributions.",
            "fidelity_characteristics": "deterministic core; probabilistic dynamics are introduced by sampling simulation parameters; supports full-state access for reward computation in sim though only partial observations used from real world; non-differentiable (precludes gradient backprop through physics); amenable to large-scale parallel rollouts.",
            "model_or_agent_name": "SimOpt-updated policy agents (PPO-trained)",
            "model_description": "Reinforcement learning agents trained with PPO on parameterized ensembles of simulator instances; parameter-distribution updates are made via sampling-based REPS optimization using discrepancy costs between simulated and real observation trajectories.",
            "reasoning_task": "learning robust closed-loop control policies for robotic manipulation tasks (handling contact dynamics and compliant/soft dynamics like ropes) via simulation-then-sparse real-rollout adaptation.",
            "training_performance": null,
            "transfer_target": "real robots and shifted target-scene configurations (e.g. cabinet offsets 15cm and 22cm)",
            "transfer_performance": "Using SimOpt (sampling + REPS updates) reduced required domain randomization breadth and enabled successful transfer: e.g., drawer transfer required 3 SimOpt iterations for a 15cm offset (versus failing under naive wide domain randomization), and 5 iterations for a 22cm offset; swing-peg-in-hole required 2 SimOpt iterations to reach ~90% real-world success (20 trials).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "No direct comparison of different intrinsic simulator fidelity settings; the paper shows that adapting parameter distributions (narrow→targeted) outperforms naive training on very wide parameter distributions, which often contain infeasible settings that prevent learning.",
            "minimal_fidelity_discussion": "Paper states exact replication of the real world is not required and full state observations in the real world are unnecessary; instead, a simulator with a strong prior and adaptable parameter distributions is sufficient for transfer. They note that if a differentiable simulator were available, gradient-based updates could be used, but it is not required.",
            "failure_cases": "Failure observed when using overly wide parameter distributions (infeasible task instances and simulator instability), and when attempting trajectory-based parameter learning for soft rope without continuous real-time state observability (cannot set all simulator internal states). Also open-loop replay of real actions in sim was less effective than closed-loop policy rollouts for some partially unobserved dimensions.",
            "uuid": "e1305.1",
            "source_info": {
                "paper_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "EPOpt: Learning robust neural network policies using model ensembles",
            "rating": 2,
            "sanitized_title": "epopt_learning_robust_neural_network_policies_using_model_ensembles"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "GPU-accelerated robotic simulation for distributed reinforcement learning",
            "rating": 2,
            "sanitized_title": "gpuaccelerated_robotic_simulation_for_distributed_reinforcement_learning"
        },
        {
            "paper_title": "Fast model identification via physics engines for data-efficient policy search",
            "rating": 2,
            "sanitized_title": "fast_model_identification_via_physics_engines_for_dataefficient_policy_search"
        },
        {
            "paper_title": "Preparing for the unknown: Learning a universal policy with online system identification",
            "rating": 1,
            "sanitized_title": "preparing_for_the_unknown_learning_a_universal_policy_with_online_system_identification"
        }
    ],
    "cost": 0.011843,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</p>
<p>Yevgen Chebotar 
Ankur Handa 
Viktor Makoviychuk 
Miles Macklin 
Jan Issac 
Nathan Ratliff 
Dieter Fox 
Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</p>
<p>Fig. 1. Policies for opening a cabinet drawer and swing-peg-in-hole tasks trained by alternatively performing reinforcement learning with multiple agents in simulation and updating simulation parameter distribution using a few real world policy executions.Abstract-We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https: //sites.google.com/view/simopt.
 Fig. 1
. Policies for opening a cabinet drawer and swing-peg-in-hole tasks trained by alternatively performing reinforcement learning with multiple agents in simulation and updating simulation parameter distribution using a few real world policy executions.</p>
<p>Abstract-We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https: //sites.google.com/view/simopt.</p>
<p>I. INTRODUCTION</p>
<p>Learning continuous control in real world complex environments has seen a wide interest in the past few years and in particular focusing on learning policies in simulators and transferring to real world, as we still struggle with finding ways to acquire the necessary amount of experience and data in the real world directly. While there have been recent attempts on learning by collecting large scale data directly on real robots [1,2,3,4], such an approach still remains challenging as collecting real world data is prohibitively laborious and expensive. Simulators offer several advantages, e.g. they can run faster than real-time and allow for acquiring large diversity of training data. However, due to the imprecise simulation models and lack of high fidelity replication of real world scenes, policies learned in simulations often cannot be directly applied on real-world systems, a phenomenon also known as the reality gap [5]. In this work, we focus on closing the reality gap by learning policies on distributions of simulated scenarios that are optimized for a better policy transfer.</p>
<p>Training policies on a large diversity of simulated scenarios by randomizing relevant parameters, also known as domain randomization, has shown a considerable promise for the real world transfer in a range of recent works [6,7,8,9]. However, design of the appropriate simulation parameter distributions remains a tedious task and often requires a substantial expert knowledge. Moreover, there are no guarantees that the applied randomization would actually lead to a sensible real world policy as the design choices made in randomizing the parameters tend to be somewhat biased by the expertise of the practitioner. In this work, we apply a data-driven approach and use real world data to adapt simulation randomization such that the behavior of the policies trained in simulation better matches their behavior in the real world. Therefore, starting with some initial distribution of the simulation parameters, we can perform learning in simulation and use real world roll-outs of learned policies to gradually change the simulation randomization such that the learned policies transfer better to the real world without requiring the exact replication of the real world scene in simulation. This approach falls into the domain of modelbased reinforcement learning. However, we leverage recent developments in physics simulations to provide a strong prior of the world model in order to accelerate the learning process. Our system uses partial observations of the real world and only needs to compute rewards in simulation, therefore lifting the requirement for full state knowledge or reward instrumentation in the real world.</p>
<p>II. RELATED WORK</p>
<p>The problem of finding accurate models of the robot and the environment that can facilitate the design of robotic controllers in real world dates back to the original works on system identification [10]. In the context of reinforcement learning (RL), model-based RL explored optimizing policies using learned models [11]. In [12,13], the data from realworld policy executions is used to fit a probabilistic dynamics model, which is then used for learning an optimal policy. Although our work follows the general principle of modelbased reinforcement learning, we aim at using a simulation engine as a form of parameterized model that can help us to embed prior knowledge about the world.</p>
<p>Overcoming the discrepancy between simulated models and the real world has been addressed through identifying simulation parameters [14], finding common feature representations of real and synthetic data [15], using generative models to make synthetic images more realistic [16], fine-tuning the policies trained in simulation in the real world [17], learning inverse dynamics models [18], multiobjective optimization of task fitness and transferability to the real world [19], training on ensembles of dynamics models [20] and training on a large variety of simulated scenarios [6]. Domain randomization of textures was used in [7] to learn to fly a real quadcopter by training an image based policy entirely in simulation. Peng et al. [21] use randomization of physical parameters of the scene to learn a policy in simulation and transfer it to real robot for pushing a puck to a target position. In [9], randomization of physical properties and object appearance is used to train a dexterous robotic hand to perform in-hand manipulation. Yu et al. [22] propose to not only train a policy on a distribution of simulated parameters, but also learn a component that predicts the system parameters from the current states and actions, and use the prediction as an additional input to the policy. In [23], an upper confidence bound on the estimated simulation optimization bias is used as a stopping criterion for a robust training with domain randomization. In [24], an auxiliary reward is used to encourage policies trained in source and target environments to visit the same states.</p>
<p>Combination of system identification and dynamics randomization has been used in the past to learn locomotion for a real quadruped [25], non-prehensile object manipulation [26] and in-hand object pivoting [27]. In our work, we recognize domain randomization and system identification as powerful tools for training general policies in simulation. However, we address the problem of automatically learning simulation parameter distributions that improve policy transfer, as it remains challenging to do it manually. Furthermore, as also noticed in [28], simulators have an advantage of providing a full state of the system compared to partial observations of the real world, which is also used in our work for designing better reward functions.</p>
<p>The closest to our approach are the methods from [29,30,31,32,33] that propose to iteratively learn simulation parameters and train policies. In [29], an iterative system identification framework is used to optimize trajectories of a bipedal robot in simulation and calibrate the simulation parameters by minimizing the discrepancy between the real world and simulated execution of the trajectories. Although we also use the real world data to compute the discrepancy of the simulated executions, we are able to use partial observations of the real world instead of the full states and we concentrate on learning general policies by finding simulation parameter distribution that leads to a better transfer without the need for exact replication of the real world environment. [30] suggests to optimize the simulation parameters such that the value function is well approximated in simulation without replicating the real world dynamics. We also recognize that exact replication of the real world dynamics might not be feasible, however a suitable randomization of the simulated scenarios can still lead to a successful policy transfer. In addition, our approach does not require estimating the reward in the real world, which might be challenging if some of the reward components can not be observed. [31] and [32] consider grounding the simulator using real world data. However, [31] requires a human in the loop to select the best simulation parameters, and [32] needs to fit additional models for the real robot forward dynamics and simulator inverse dynamics. Finally, our work is closest to the adaptive EPOpt framework of Rajeswaran et al. [33], which optimizes a policy over an ensemble of models and adapts the model distribution using data from the target domain. EPOpt optimizes a risk-sensitive objective to obtain robust policies, whereas we optimize the average performance which is a risk-neutral objective. Additionally, EPOpt updates the model distribution by employing Bayesian inference with a particle filter, whereas we update the model distribution using an iterative KLdivergence constrained procedure. More importantly, they focus on simulated environments while in our work, we develop an approach that is shown to work in real world and apply it to two real robot tasks.</p>
<p>III. CLOSING THE SIM-TO-REAL LOOP A. Simulation randomization</p>
<p>Let M = (S, A, P, R, p 0 , γ, T ) be a finite-horizon Markov Decision Process (MDP), where S and A are state and action spaces, P : S × A × S → R + is a state-transition probability function or probabilistic system dynamics, R : S × A → R a reward function, p 0 : S → R + an initial state distribution, γ a reward discount factor, and T a fixed horizon. Let τ = (s 0 , a 0 , . . . , s T , a T ) be a trajectory of states and actions and R(τ ) = T t=0 γ t R(s t , a t ) the trajectory reward. The goal of reinforcement learning methods is to find parameters θ of a policy π θ (a|s) that maximize the expected discounted reward over trajectories induced by the policy:
E π θ [R(τ )]
where s 0 ∼ p 0 , s t+1 ∼ P (s t+1 |s t , a t ) and a t ∼ π θ (a t |s t ).</p>
<p>In our work, the system dynamics are either induced by a simulation engine or real world. As the simulation engine itself is deterministic, a reparameterization trick [34] can be applied to introduce probabilistic dynamics. In particular, we define a distribution of simulation parameters ξ ∼ p φ (ξ) parameterized by φ. The resulting probabilistic system dynamics of the simulation engine are P ξ∼p φ = P (s t+1 |s t , a t , ξ).</p>
<p>As it was shown in [6,7,9], it is possible to design a distribution of simulation parameters p φ (ξ), such that a policy trained on P ξ∼p φ would perform well on a realworld dynamics distribution. This approach is also known as domain randomization and the policy training maximizes the expected reward under the dynamics induced by the distribution of simulation parameters p φ (ξ):
max θ E P ξ∼p φ <a href="1">E π θ [R(τ )]</a>
Domain randomization requires a significant expertise and tedious manual fine-tuning to design the simulation param- After training a policy on current distribution, we sample the policy both in the real world and for a range of parameters in simulation. The discrepancy between the simulated and real observations is used to update the simulation parameter distribution in SimOpt. eter distribution p φ (ξ). Furthermore, as we show in our experiments, it is often disadvantageous to use overly wide distributions of simulation parameters as they can include scenarios with infeasible solutions that hinder successful policy learning, or lead to exceedingly conservative policies. Instead, in the next section, we present a way to automate the learning of p φ (ξ) that makes it possible to shape a suitable randomization without the need to train on very wide distributions.</p>
<p>B. Learning simulation randomization</p>
<p>The goal of our framework is to find a distribution of simulation parameters that brings observations or partial observations induced by the policy trained under this distribution closer to the observations of the real world. Let π θ,p φ be a policy trained under the simulated dynamics distribution P ξ∼p φ as in the objective (1), and let D(τ ob ξ , τ ob real ) be a measure of discrepancy between real world observation trajectories τ ob real = (o 0,real . . . , o T,real ) and simulated observation trajectories τ ob ξ = (o 0,ξ . . . , o T,ξ ) sampled using policy π θ,p φ and the dynamics distribution P ξ∼p φ . It should be noted that the inputs of the policy π θ,p φ and observations used to compute D(τ ob ξ , τ ob real ) are not required to be the same. The goal of optimizing the simulation parameter distribution is to minimize the following objective:
min φ E P ξ∼p φ E π θ,p φ D(τ ob ξ , τ ob real )(2)
This optimization would entail training and real robot evaluation of the policy π θ,p φ for each φ. This would require a large amount of RL iterations and more critically real robot trials. Hence, we develop an iterative approach to approximate the optimization by training a policy π θ,p φ i on the simulation parameter distribution from the previous iteration and using it 
π θ,p φ i ← RL(env) 6: τ ob real ∼ RealRollout(π θ,p φ i ) 7: ξ ∼ Sample(p φi ) 8: τ ob ξ ∼ SimRollout(π θ,p φ i , ξ) 9: c(ξ) ← D(τ ob ξ , τ ob real ) 10: p φi+1 ← UpdateDistribution(p φi , ξ, c(ξ), )
for both, sampling the real world observations and optimizing the new simulation parameter distribution p φi+1 :
min φi+1 E P ξ i+1 ∼p φ i+1 E π θ,p φ i D(τ ob ξi+1 , τ ob real )(3)s.t. D KL p φi+1 p φi ≤ ,
where we introduce a KL-divergence step between the old simulation parameter distribution p φi and the updated distribution p φi+1 to avoid going out of the trust region of the policy π θ,p φ i trained on the old simulation parameter distribution. Fig. 3 shows the general structure of our algorithm that we call SimOpt.</p>
<p>C. Implementation</p>
<p>Here we describe particular implementation choices for the components of our framework used in this work. However, it should be noted that each of the components is replaceable. Algorithm 1 describes the order of running all the components in our implementation.</p>
<p>The RL training is performed on a GPU based simulator using a parallelized version of proximal policy optimization (PPO) [35] on a multi-GPU cluster [36]. We parameterize our simulation parameter distribution as a Gaussian, i.e. p φ (ξ) ∼ N (µ, Σ) with φ = (µ, Σ). We choose weighted 1 and 2 norms between simulation and real world observations for our observation discrepancy function D:
D(τ ob ξ , τ ob real ) = (4) w 1 T i=0 |W (o i,ξ − o i,real )| + w 2 T i=0 W (o i,ξ − o i,real ) 2 2 ,
where w 1 and w 2 are the weights of the 1 and 2 norms, and W are the importance weights for each observation dimension. We additionally apply a Gaussian filter to the distance computation to account for misalignments of the trajectories.</p>
<p>As we use a non-differentiable simulator we employ a sampling-based gradient-free algorithm based on relative entropy policy search [37] for optimizing the objective (3), which is able to perform updates of p φ with an upper bound on the KL-divergence step. By doing so, the simulator can be treated as a black-box, as in this case p φ can be optimized directly by only using samples ξ ∼ p φ and the corresponding costs c(ξ) coming from D(τ ob ξ , τ ob real ).</p>
<p>Sampling of simulation parameters and the corresponding policy roll-outs is highly parallelizable, which we use in our experiments to evaluate large amounts of simulation parameter samples. As noted above, single components of our framework can be exchanged. In case of availability of a differentiable simulator, the objective (3) can be defined as a loss function for optimizing with gradient descent. Furthermore, for cases where 1 and 2 norms are not applicable, we can employ other forms of discrepancy functions, e.g. to account for potential domain shifts between observations [15,38,39]. Alternatively, real world and simulation data can be additionally used to train D(τ ob ξ , τ ob real ) to discriminate between the observations by minimizing the prediction loss of classifying observations as simulated or real, similar to the discriminator training in the generative adversarial framework [40,41,42]. Finally, a higher-dimensional generative model p φ (ξ) can be employed to provide a multi-modal randomization of the simulated environments.</p>
<p>IV. EXPERIMENTS</p>
<p>In our experiments we aim at answering the following questions: (1) How does our method compare to pure domain randomization? (2) How learning a simulation parameter distribution compares to training on a very wide parameter distribution? (3) How many SimOpt iterations and real world trials are required for a successful transfer of robotic manipulation policies? (4) Does our method work for different real world tasks and robots?</p>
<p>We start by performing an ablation study in simulation by transferring policies between scenes with different initial state distributions, such as different poses of the cabinet in the drawer opening task. We demonstrate that updating the distribution of the simulation parameters leads to a successful policy transfer in contrast to just using an initial distribution of the parameters without any updates. As we observe, training on very wide parameter distributions is significantly more difficult and prone to fail than updating a conservative initial distribution.</p>
<p>Next, we show that we can successfully transfer policies to real robots, such as ABB Yumi and Franka Panda, for complex articulated tasks such as cabinet drawer opening, and tasks with non-rigid bodies and complex dynamics, such as swing-peg-in-hole task with the peg swinging on a soft rope. The policies can be transferred with a very small amount of real robot trials and leveraging large-scale training on a multi-GPU cluster.</p>
<p>A. Tasks</p>
<p>We evaluate our approach on two robot manipulation tasks: cabinet drawer opening and swing-peg-in-hole.</p>
<p>1) Swing-peg-in-hole: The goal of this task is to put a peg attached to a robot hand on a rope into a hole placed at a 45 degrees angle. Manipulating a soft rope leads to a swinging motion of the peg, which makes the dynamics of the task more challenging. The task set up in the simulation and real world using a 7-DoF Yumi robot from ABB is depicted in Fig. 4. An example of a wide distribution of simulation parameters in the swing-peg-in-hole task where it is not possible to find a solution for many of the task instances. 2) Drawer opening: In the drawer opening task, the robot has to open a drawer of a cabinet by grasping and pulling it with its fingers. This task involves an ability to handle contact dynamics when grasping the drawer handle. For this task, we use a 7-DoF Panda arm from Franka Emika. Simulated and real world settings are shown in Fig. 1 on the left. This task is operated on a 10D observation space: 7D robot joint angles and 3D position of the cabinet drawer handle. The reward function consists of the distance penalty between the handle and end-effector positions, the angle alignment of the end-effector and the drawer handle, opening distance of the drawer and indicator function ensuring that both robot fingers are on the handle.</p>
<p>We would like to emphasize that our method does not require the full state information of the real world, e.g. we do not need to estimate the rope diameter, rope compliance etc. to update the simulation parameter distribution in the swingpeg-in-hole task. The output of our policies consists of 7 joint velocity commands and an additional gripper command for the drawer opening task.</p>
<p>B. Simulation engine</p>
<p>We use NVIDIA Flex as a high-fidelity GPU based physics simulator that uses maximal coordinate representation to simulate rigid body dynamics. Flex allows a highly parallel implementation and can simulate multiple instances of the scene on a single GPU. We use the multi-GPU based RL infrastructure developed in [36] to leverage the highly parallel nature of the simulator.</p>
<p>C. Simulated experiments</p>
<p>We aim at understanding what effect a wide simulation parameter distribution can have on learning robust policies, and how we can improve the learning performance and the  transferability of the policies using our method to adjust simulation randomization. Fig. 4 shows an example of a significantly wide distribution of simulation parameters for the swing-peg-in-hole task. In this case, peg size, rope properties and size of the peg box were randomized. As we can observe, a large part of the randomized instances does not have a feasible solution, i.e. when the peg is too large for the hole or the rope is too short. Finding a suitably wide parameter distribution would require manual fine-tuning of the randomization parameters.</p>
<p>Moreover, learning performance depends strongly on the variance of the parameter distribution. We investigate this in a simulated cabinet drawer opening task with a Franka arm which is placed in front of a cabinet. We randomize the position of the cabinet along the lateral direction (Xcoordinate) while keeping all other simulation parameters constant. We train our policies on a 2 layer neural network with fully connected layers of 64 units each with PPO for 200 iterations. As we increase the variance of the cabinet position, we observe that the policies learned tend to be conservative i.e. they do end up reaching the handle of the drawer but fail to open it. This is shown in Fig. 5 where we plot the reward as a function of number of iterations used to train the RL policy. We start with a standard deviation of 2cm (σ 2 = 7e − 4) and increase it to 10cm (σ 2 = 0.01). As shown in the plot, the policy is sensitive to the choice of this parameter and only manages to open the drawer when the standard deviation is 2cm. We note that the reward difference may not seem that significant but realize that it is dominated by reaching reward. Increasing variance further, in an attempt to cover a wider operating range, can often lead to simulating unrealistic scenarios and catastrophic breakdown of the physics simulation with various joints of the robot reaching their limits. We also observed that the policy is extremely sensitive to variance in all three axes of the cabinet position i.e. policy only ever converges when the standard deviation is 2cm and fails to learn even reaching the handle otherwise.</p>
<p>In our next set of experiments, we perform policy transfer from the source to target drawer opening scene where position of the cabinet in the target scene is offset by a distance of 15cm and 22cm. After training the policy with RL, it is run on the target scene to collect roll-outs. These roll-outs are then used to perform several SimOpt iterations to optimize simulation parameters that best explain the current roll-outs. We noticed that the RL training can be sped up by initializing the policy with the weights from the previous SimOpt iteration, effectively reducing the number of needed PPO iterations from 200 to 10 after the first SimOpt iteration. The whole process is repeated until the learned policy starts to successfully open the drawer in the target scene. We found that it took overall 3 iterations of doing RL and SimOpt to learn to open the drawer when the cabinet was offset by 15cm. Such a large distance of 15cm would have required the standard deviation of the cabinet position to be 10cm for any naïve domain randomization based training which fails to produce a policy that opens the drawer as shown in Fig. 5. Our method leverages roll-outs from the target scene and changes the distribution of cabinet position such that the training on this new distribution allows opening the drawer. We further note that the number of iterations increases to 5 as we increase the target cabinet distance to 22cm highlighting that our method is able to operate on a wider range of mismatch between the current scene and the target scene. Fig. 6 shows how the source distribution variance adapts to the target distribution variance for this experiment and Fig. 7 shows that our method starts with a conservative guess of the initial distribution of the parameters and changes it using target scene roll-outs until policy behaviour in target and source scene starts to match.</p>
<p>D. Real robot experiments</p>
<p>In our real robot experiments, SimOpt is used to learn simulation parameter distribution of the manipulated objects and the robot. We run our experiments on 7-DoF Franka Panda and ABB Yumi robots. The RL training and SimOpt simulation parameter sampling is performed using a cluster of 64 GPUs for running the simulator with 150 simulated agents per GPU. In the real world, we use object tracking Fig. 7. Policy performance in the target drawer opening environment trained on randomized simulation parameters at different iterations of SimOpt. As the source environment distribution gets adjusted, the policy transfer improves until the robot can successfully solve the task in the fourth SimOpt iteration. Fig. 8. Running policies trained in simulation at different iterations of SimOpt for real world swing-peg-in-hole and drawer opening tasks. Left: SimOpt adjusts physical parameter distribution of the soft rope, peg and the robot, which results in a successful execution of the task on a real robot after two SimOpt iterations. Right: SimOpt adjusts physical parameter distribution of the robot and the drawer. Before updating the parameters, the robot pushes too much on the drawer handle with one of its fingers, which leads to opening the gripper. After one SimOpt iteration, the robot can better control its gripper orientation, which leads to an accurate task execution.   with DART [43] to continuously track the 3D positions of the peg in the swing-peg-in-hole task and the handle of the cabinet drawer in the drawer opening task, as well as initialize positions of the peg box and the cabinet in simulation. DART operates on depth images and requires 3D articulated models of the objects. We learn multi-variate Gaussian distributions of the simulation parameters parameterized by a mean and a full covariance matrix, and perform several updates of the simulation parameter distribution per SimOpt iteration using the same real world roll-outs to minimize the number of real world trials.
µ init diag(Σ init ) µ f inal
1) Swing-peg-in-hole: Fig. 8 (left) demonstrates the behavior of the real robot execution of the policy trained in simulation over 3 iterations of SimOpt. At each iteration, we perform 100 iterations of RL in approximately 7 minutes and 3 roll-outs on the real robot using the currently trained policy to collect real-world observations. Then, we run 3 update  steps of the simulation parameter distribution with 9600 simulation samples per update. In the beginning, the robot misses the hole due to the discrepancy of the simulation parameters and the real world. After a single SimOpt iteration, the robot is able to get much closer to the hole, however not being able to insert the peg as it requires a slight angle to go into the hole, which is non-trivial to achieve using a soft rope. Finally, after two SimOpt iterations, the policy trained on a resulting simulation parameter distribution is able to swing the peg into the hole in 90% of the times when evaluated on 20 trials. Table I shows the initial mean and the diagonal of the covariance matrix of the swing-peg-in-hole simulation parameters, and the shifted mean at the end of the SimOpt training. We observe that the most significant changes occur in the physical parameters of the rope that influence its dynamical behavior and the robot parameters, especially scaling of the policy actions.
µ init diag(Σ init ) µ f inal
2) Drawer opening: For drawer opening, we learn a Gaussian distribution of simulation parameters initialized with a mean and a diagonal covariance matrix described in Table II. Fig. 8 (right) shows the drawer opening behavior before and after performing a SimOpt update. During each SimOpt iteration, we run 200 iterations of RL for approximately 22 minutes, perform 3 real robot roll-outs and 20 update steps of the simulation distribution using 9600 samples per update step. Before updating the parameter distribution, the robot is able to reach the handle and start opening the drawer. However, it cannot exactly replicate the learned behavior from simulation and does not keep the gripper orthogonal to the drawer, which results in pushing too much on the handle from the bottom with one of the robot fingers. As the finger gripping force is limited, the fingers begin to open due to a larger pushing force. After adjusting the parameter distribution including robot and drawer properties, as shown in Table II, the robot is able to better control its gripper orientation and by evaluating on 20 trials can open the drawer at all times keeping the gripper orthogonal to the handle.</p>
<p>E. Comparison to trajectory-based parameter learning</p>
<p>In our work, we run a closed-loop policy in simulation to obtain simulated roll-outs for SimOpt optimization. Alternatively, we could directly set the simulator to states and execute actions from the real world trajectories as proposed in [29,30]. However, such a setting is not always possible as we might not be able to observe all required variables for setting the internal state of the simulator at each time point, e.g. the current bending configuration of the rope in the swing-peg-in-hole task, which we are able to initialize but can not continually track with our real world set up.</p>
<p>Without being able to set the simulator to the real world states continuously, we still can try to copy the real world actions and execute them in an open-loop manner in simulation. However, in our simulated experiments we notice that especially when making particular state dimensions unobservable for SimOpt cost computation, such as X-position of the cabinet in the drawer opening task, executing a closedloop policy still leads to meaningful simulation parameter updates compared to the open-loop execution. We believe in this case the robot behavior is still dependent on the particular simulated scenario due to the closed-loop nature of the policy, which also reflects in the joint trajectories of the robot that are still included in the SimOpt cost function. This means that by using a closed-loop policy we can still update the simulation parameter distribution even without explicitly including some of the relevant observations in the SimOpt cost computation.</p>
<p>V. CONCLUSIONS</p>
<p>Closing the simulation to reality transfer loop is an important component for a robust transfer of robotic policies.</p>
<p>In this work, we demonstrated that adapting simulation randomization using real world data can help in learning simulation parameter distributions that are particularly suited for a successful policy transfer without the need for exact replication of the real world environment. In contrast to trying to learn policies using very wide distributions of simulation parameters, which can simulate infeasible scenarios, we are able to start with distributions that can be efficiently learned with reinforcement learning, and modify them for a better transfer to the real scenario. Our framework does not require full state of the real environment and reward functions are only needed in simulation. We showed that updating simulation distributions is possible using partial observations of the real world while the full state still can be used for the reward computation in simulation. We evaluated our approach on two real world robotic tasks and showed that policies can be transferred with only a few iterations of simulation updates using a small number of real robot trials.</p>
<p>In this work, we applied our method to learning uni-modal simulation parameter distributions. We plan to extend our framework to multi-modal distributions and more complex generative simulation models in future work. Furthermore, we plan to incorporate higher-dimensional sensor modalities, such as vision and touch, for both policy observations and factors of simulation randomization. Tables III and IV show the SimOpt distribution update parameters for swing-peg-in-hole and drawer opening tasks including REPS [37] parameters, settings of the discrepancy function D(τ ob ξ , τ ob real ), weights of each observation dimension in the discrepancy function, and reinforcement learning settings such as parallelized PPO [35,36] training parameters and task reward weights.  </p>
<p>VI. APPENDIX</p>
<p>Fig. 3 .
3The pipeline for optimizing the simulation parameter distribution.</p>
<p>Fig. 1
1on the right. Our observation space consists of 7-DoF arm joint configurations and 3D position of the peg. The reward function for the RL training in simulation includes the distance of the peg from the hole, angle alignment with the hole and a binary reward for solving the task.</p>
<p>Fig. 5 .
5Performance of the policy training with domain randomization for different variances of the distribution of the cabinet position along the X-axis in the drawer opening task.</p>
<p>Fig. 6 .
6Initial distribution of the cabinet position in the source environment, located at extreme left, slowly starts to change to the target environment distribution as a function of running 5 iterations of SimOpt.</p>
<p>scaling (7D) [0.5 . . . 0.</p>
<p>action scaling (7D) [0.26 . . . 0.26] 0.01 [0.19 . . .</p>
<p>Algorithm 1 SimOpt framework 1: p φ0 ← Initial simulation parameter distribution 2: ← KL-divergence step for updating p φ 3: for iteration i ∈ {0, . . . , N } do4: </p>
<p>env ← Simulation(p φi ) </p>
<p>5: </p>
<p>TABLE I
ISWING-PEG-IN-HOLE: SIMULATION PARAMETER DISTRIBUTION.</p>
<p>TABLE II DRAWER
IIOPENING: SIMULATION PARAMETER DISTRIBUTION.</p>
<p>TABLE III SWING
III-PEG-IN-HOLE: SIMOPT PARAMETERS. distribution update parameters Number of REPS updates per SimOpt iteration 20 Number of simulation parameter samples per update 9600 L2-distance between end-effector and drawer handle -0.5 Angular alignment of end-effector with drawer handle -0.07TABLE IV DRAWER OPENING: SIMOPT PARAMETERS.Simulation Timesteps per simulation parameter sample 
453 </p>
<p>KL-threshold 
1.0 </p>
<p>Minimum temperature of sample weights 
0.001 
Discrepancy function parameters </p>
<p>L1-cost weight 
0.5 </p>
<p>L2-cost weight 
1.0 </p>
<p>Gaussian smoothing standard deviation (timesteps) 
5 </p>
<p>Gaussian smoothing truncation (timesteps) 
4 
Observation dimensions cost weights </p>
<p>Joint angles (7D) 
0.5 </p>
<p>Drawer position (3D) 
1.0 
PPO parameters </p>
<p>Number of agents 
400 </p>
<p>Episode length 
150 </p>
<p>Timesteps per batch 
151 </p>
<p>Clip parameter 
0.2 
γ 
0.99 
λ 
0.95 </p>
<p>Entropy coefficient 
0.0 </p>
<p>Optimization epochs 
5 </p>
<p>Optimization batch size per agent 
8 </p>
<p>Optimization step size 
5e-4 </p>
<p>Desired KL-step 
0.01 
RL reward weights </p>
<p>Opening distance of the drawer 
-0.4 </p>
<p>Keeping fingers around the drawer handle bonus 
0.005 </p>
<p>Action penalty 
-0.005 </p>
<p>ACKNOWLEDGEMENTSWe would like to thank Alexander Lambert, Balakumar Sundaralingam and Giovanni Sutanto for their help with the robot experiments, and David Ha, James Davidson, Lerrel Pinto and Fabio Ramos for their helpful feedback on the draft of the paper. We would also like to thank the GPU cluster and infrastucture team at NVIDIA for their help all the way through this project.
Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, I. J. Robotics Res. 374-5S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data col- lection. I. J. Robotics Res., 37(4-5):421-436, 2018.</p>
<p>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. L Pinto, A Gupta, ICRA. L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In ICRA, 2016.</p>
<p>Collective robot reinforcement learning with distributed asynchronous guided policy search. A Yahya, A Li, M Kalakrishnan, Y Chebotar, S Levine, IROS. A. Yahya, A. Li, M. Kalakrishnan, Y. Chebotar, and S. Levine. Collective robot reinforcement learning with distributed asynchronous guided policy search. In IROS, 2017.</p>
<p>Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, S Levine, CoRR, abs/1806.10293D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Her- zog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manip- ulation. CoRR, abs/1806.10293, 2018.</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, European Conference on Artificial Life. SpringerN. Jakobi, P. Husbands, and I. Harvey. Noise and the reality gap: The use of simulation in evolutionary robotics. In European Conference on Artificial Life. Springer, 1995.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, IROS. J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IROS, 2017.</p>
<p>Cad2rl: Real single-image flight without a single real image. F Sadeghi, S Levine, RSS. F. Sadeghi and S. Levine. Cad2rl: Real single-image flight without a single real image. RSS, 2017.</p>
<p>Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. S James, A J Davison, E Johns, abs/1707.02267CoRRS. James, A. J. Davison, and E. Johns. Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. CoRR, abs/1707.02267, 2017.</p>
<p>Learning dexterous in-hand manipulation. M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, J Schneider, S Sidor, J Tobin, P Welinder, L Weng, W Zaremba, abs/1808.00177CoRRM. Andrychowicz, B. Baker, M. Chociej, R. Jozefow- icz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dex- terous in-hand manipulation. CoRR, abs/1808.00177, 2018.</p>
<p>System identification -theory for the user. L Ljung, Prentice HallL. Ljung. System identification -theory for the user. Prentice Hall, 1999.</p>
<p>A survey on policy search for robotics. Foundations and Trends in Robotics. M P Deisenroth, G Neumann, J Peters, M. P. Deisenroth, G. Neumann, and J. Peters. A survey on policy search for robotics. Foundations and Trends in Robotics, pages 388-403, 2013.</p>
<p>Pilco: A modelbased and data-efficient approach to policy search. M P Deisenroth, C E Rasmussen, ICML. M. P. Deisenroth and C. E. Rasmussen. Pilco: A model- based and data-efficient approach to policy search. In ICML, 2011.</p>
<p>Learning to control a low-cost manipulator using dataefficient reinforcement learning. M P Deisenroth, C E Rasmussen, D Fox, RSS. M. P. Deisenroth, C. E. Rasmussen, and D. Fox. Learning to control a low-cost manipulator using data- efficient reinforcement learning. In RSS, 2011.</p>
<p>Physically consistent state estimation and system identification for contacts. S Kolev, E Todorov, Humanoids. S. Kolev and E. Todorov. Physically consistent state estimation and system identification for contacts. In Humanoids, 2015.</p>
<p>Towards adapting deep visuomotor representations from simulated to real environments. E Tzeng, C Devin, J Hoffman, C Finn, X Peng, S Levine, K Saenko, T Darrell, abs/1511.07111CoRRE. Tzeng, C. Devin, J. Hoffman, C. Finn, X. Peng, S. Levine, K. Saenko, and T. Darrell. Towards adapting deep visuomotor representations from simulated to real environments. CoRR, abs/1511.07111, 2015.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, S Levine, V Vanhoucke, abs/1709.07857CoRRK. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kel- cey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. CoRR, abs/1709.07857, 2017.</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Vecerik, T Rothrl, N Heess, R Pascanu, R Hadsell, CoRLA. A. Rusu, M. Vecerik, T. Rothrl, N. Heess, R. Pas- canu, and R. Hadsell. Sim-to-real robot learning from pixels with progressive nets. In CoRL, 2017.</p>
<p>Transfer from simulation to real world through learning deep inverse dynamics model. P F Christiano, Z Shah, I Mordatch, J Schneider, T Blackwell, J Tobin, P Abbeel, W Zaremba, abs/1610.03518CoRRP. F. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell, J. Tobin, P. Abbeel, and W. Zaremba. Transfer from simulation to real world through learning deep inverse dynamics model. CoRR, abs/1610.03518, 2016.</p>
<p>Crossing the reality gap in evolutionary robotics by promoting transferable controllers. S Koos, J.-B Mouret, S Doncieux, GECCO. ACM. S. Koos, J.-B. Mouret, and S. Doncieux. Crossing the reality gap in evolutionary robotics by promoting transferable controllers. In GECCO. ACM, 2010.</p>
<p>Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids. I Mordatch, K Lowrey, E Todorov, IROS. I. Mordatch, K. Lowrey, and E. Todorov. Ensemble- cio: Full-body dynamic motion planning that transfers to physical humanoids. In IROS, 2015.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, In ICRA. X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In ICRA, 2018.</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, J Tan, C K Liu, G Turk, RSS. W. Yu, J. Tan, C. K. Liu, and G. Turk. Preparing for the unknown: Learning a universal policy with online system identification. In RSS, 2017.</p>
<p>Domain randomization for simulation-based policy optimization with transferability assessment. F Muratore, F Treede, M Gienger, J Peters, CoRLF. Muratore, F. Treede, M. Gienger, and J. Peters. Domain randomization for simulation-based policy op- timization with transferability assessment. In CoRL, 2018.</p>
<p>Mutual alignment transfer learning. M Wulfmeier, I Posner, P Abbeel, abs/1707.07907CoRRM. Wulfmeier, I. Posner, and P. Abbeel. Mutual alignment transfer learning. CoRR, abs/1707.07907, 2017.</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, RSS. J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. In RSS, 2018.</p>
<p>Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system. K Lowrey, S Kolev, J Dao, A Rajeswaran, E Todorov, SIMPAR. K. Lowrey, S. Kolev, J. Dao, A. Rajeswaran, and E. Todorov. Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical sys- tem. In SIMPAR, 2018.</p>
<p>Reinforcement learning for pivoting task. R Antonova, S Cruciani, C Smith, D Kragic, abs/1703.00472CoRRR. Antonova, S. Cruciani, C. Smith, and D. Kragic. Reinforcement learning for pivoting task. CoRR, abs/1703.00472, 2017.</p>
<p>Asymmetric actor critic for image-based robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, P Abbeel, abs/1710.06542CoRRL. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel. Asymmetric actor critic for image-based robot learning. CoRR, abs/1710.06542, 2017.</p>
<p>Simulationbased design of dynamic controllers for humanoid balancing. J Tan, Z Xie, B Boots, C K Liu, IROS. J. Tan, Z. Xie, B. Boots, and C. K. Liu. Simulation- based design of dynamic controllers for humanoid balancing. In IROS, 2016.</p>
<p>Fast model identification via physics engines for dataefficient policy search. S Zhu, A Kimmel, K E Bekris, A Boularias, IJCAI. ijcai.org. S. Zhu, A. Kimmel, K. E. Bekris, and A. Boularias. Fast model identification via physics engines for data- efficient policy search. In IJCAI. ijcai.org, 2018.</p>
<p>Humanoid robots learning to walk faster: From the real world to simulation and back. A Farchy, S Barrett, P Macalpine, P Stone, AAMAS. A. Farchy, S. Barrett, P. MacAlpine, and P. Stone. Humanoid robots learning to walk faster: From the real world to simulation and back. In AAMAS, 2013.</p>
<p>Grounded action transformation for robot learning in simulation. J Hanna, P Stone, AAAI. J. Hanna and P. Stone. Grounded action transformation for robot learning in simulation. In AAAI, 2017.</p>
<p>Epopt: Learning robust neural network policies using model ensembles. A Rajeswaran, S Ghotra, S Levine, B Ravindran, abs/1610.01283CoRRA. Rajeswaran, S. Ghotra, S. Levine, and B. Ravindran. Epopt: Learning robust neural network policies using model ensembles. CoRR, abs/1610.01283, 2016.</p>
<p>Auto-encoding variational Bayes. CoRR, abs/1312. D P Kingma, M Welling, 6114D. P. Kingma and M. Welling. Auto-encoding varia- tional Bayes. CoRR, abs/1312.6114, 2013.</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, abs/1707.06347CoRRJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.</p>
<p>Gpu-accelerated robotic simulation for distributed reinforcement learning. J Liang, V Makoviychuk, A Handa, N Chentanez, M Macklin, D Fox, CoRL. J. Liang, V. Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and D. Fox. Gpu-accelerated robotic sim- ulation for distributed reinforcement learning. CoRL, 2018.</p>
<p>Relative entropy policy search. J Peters, K Mlling, Y Altun, AAAI. J. Peters, K. Mlling, and Y. Altun. Relative entropy policy search. In AAAI, 2010.</p>
<p>Simultaneous deep transfer across domains and tasks. E Tzeng, J Hoffman, T Darrell, K Saenko, ICCV. E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015.</p>
<p>Time-contrastive networks: Self-supervised learning from video. P Sermanet, C Lynch, Y Chebotar, J Hsu, E Jang, S Schaal, S Levine, In ICRA. P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, and S. Levine. Time-contrastive networks: Self-supervised learning from video. In ICRA, 2018.</p>
<p>I J Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A C Courville, Y Bengio, Generative adversarial nets. NIPSI. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Ben- gio. Generative adversarial nets. In NIPS, 2014.</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, NIPS. J. Ho and S. Ermon. Generative adversarial imitation learning. In NIPS, 2016.</p>
<p>Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets. K Hausman, Y Chebotar, S Schaal, G S Sukhatme, J J Lim, NIPS. K. Hausman, Y. Chebotar, S. Schaal, G. S. Sukhatme, and J. J. Lim. Multi-modal imitation learning from un- structured demonstrations using generative adversarial nets. In NIPS, 2017.</p>
<p>Dart: Dense articulated real-time tracking. T Schmidt, R A Newcombe, D Fox, RSS. T. Schmidt, R. A. Newcombe, and D. Fox. Dart: Dense articulated real-time tracking. In RSS, 2014.</p>            </div>
        </div>

    </div>
</body>
</html>
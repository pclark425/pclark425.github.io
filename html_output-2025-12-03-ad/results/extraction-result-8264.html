<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8264 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8264</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8264</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-272368345</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.01281v3.pdf" target="_blank">Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs</a></p>
                <p><strong>Paper Abstract:</strong> To enhance the reasoning capabilities of large language models (LLMs), self-consistency has become a popular approach, combining multiple samplings with majority voting. However, current methods are computationally expensive and time-consuming due to the need for numerous samplings. To address this, this paper introduces path-consistency, which leverages the confidence of earlier-generated answers to identify the most promising prefix and guide the generation of subsequent branches. By dynamically guiding the generation of subsequent branches based on this prefix, path-consistency mitigates both the errors and redundancies from random or less useful sampling in self-consistency. This approach reduces errors and redundancies from random sampling, significantly accelerating inference by minimizing token consumption. Our extensive empirical results demonstrate that path-consistency improves inference latency by up to 40.5\%, while maintaining task accuracy across various tasks, including mathematical reasoning, commonsense reasoning, and symbolic reasoning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8264.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8264.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Path-Consistency (prefix-enhanced self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic inference procedure that extracts confident intermediate reasoning steps (prefixes) from early sampled chains and uses those prefixes to guide subsequent branch generation, reducing token consumption and latency while preserving reasoning diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B (primary); also evaluated on DeepSeek-V3 and Llama-3.2-1B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama3-8B backbone used for most experiments; nucleus sampling (temperature=0.6, top-p=0.9) with 20 sampled reasoning paths per example. Additional evaluations run on DeepSeek-V3 and a smaller Llama-3.2-1B instruct model in the appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting (CoT)', 'self-consistency style multiple-sampling with majority/voting-based confidence', 'prefix-guided sampling (path-consistency - iterative extract-and-sample)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompting is used to elicit multi-step reasoning paths. The method samples multiple reasoning paths (like self-consistency) but periodically computes a confidence metric (beta confidence) over answers in a window; when confidence exceeds threshold, short prefixes (first t steps) from the most confident paths are extracted and appended to the prompt for subsequent windows. Sampling then continues conditioned on these prefixes, iteratively increasing prefix length until final answers are generated and aggregated (majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (intended to preserve the diversity of full-path sampling while using constrained, prefix-guided conditioning to reduce redundant/wasteful branches)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared path-consistency (PC) to: self-consistency (SC) baseline (20 samples), Adaptive-Consistency (AC) (confidence-based early stop), and Early-Stopping Self-Consistency (ESC) (window-based identical-answer exit). Ablations included varying confidence thresholds (0.5-1.0), different maximum prefix levels (level-3 vs level-4), prefix extraction frequencies (windows splitting 20 branches), and experiments on a smaller model; all runs used nucleus sampling (temp=0.6, top-p=0.9) and 20 branches unless otherwise stated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Arithmetic reasoning (GSM8K, SVAMP, ASDiv, MultiArith), Commonsense reasoning (StrategyQA, RuinNames, SalientTranslation), Symbolic reasoning (Boolean Expressions, Tracking Shuffled Objects, Logical Deduction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported aggregate/representative results: average accelerations reported in paper introduction — arithmetic tasks: 28.7% speedup on average; commonsense: 20.9%; symbolic: 20.3%. Dataset-level examples (from tables in paper): GSM8K accuracy improved from SC baseline to PC (example: SC reported ≈64.1% in one table, PC up to 67.8% — +3.7 to +3.8 percentage points depending on threshold) with speedups up to +28.9% overall and token reductions up to −24.0% (Table 2/3). SVAMP: PC speedup up to +48.3% and small accuracy changes (≈ −0.5 to +0.6 pp across thresholds). MultiArith: accuracy increases reported up to +2.2 pp and speedups ≈ +42.7%. Tracking Shuffled Objects (symbolic): PC showed highest accuracy gain of +5.2 pp vs baseline and speedups ≈ +18.4%. Table 5 (comparison of techniques) shows tokens per GSM8K problem: SC 3,860 tokens, AC 2,248 (−41.7%), ESC 2,710 (−29.8%), PC 1,630 (−57.8%); accuracy on GSM8K in that table: SC 68.4%, AC 67.2%, ESC 67.2%, PC 67.8%. The paper also reports extreme per-level speedups (prefix level-3) up to +78.9% in some GSM8K settings (but with different tradeoffs by prefix level and confidence threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Path-consistency reduces wasted token generation on incorrect branches and reduces redundancy on correct branches (Figure 5). It increases the number of high-confidence final answers and reduces samples clustered around ~0.5 confidence (Figure 7). The authors present a theoretical analysis showing PC will not worsen the 'truth-in-hands-of-a-few' failure mode provided the initial majority-confidence P_vote ≥ 0.5, and advocate using confidence thresholds to protect against selecting harmful prefixes. Aggressive prefixing (lower threshold, more levels) can improve speed but risks selecting suboptimal prefixes and reducing accuracy, especially on harder/different-format tasks (e.g., Ruin Names or long-input tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Path-consistency preserves much of the diversity benefit of self-consistency while substantially reducing token consumption and inference latency (often 20–40% speedups, sometimes higher at deeper prefixing levels) without degrading—and in some cases improving—accuracy across arithmetic, commonsense, and symbolic reasoning benchmarks. Unlike other acceleration methods (AC, ESC) that gain speed primarily by early-exiting and thus sacrificing diversity, PC purposely extracts and samples from prefix sets to maintain diversity while improving efficiency and robustness; it is model-agnostic and requires no additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8264.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8264.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (multiple-sample majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling strategy that elicits multiple chain-of-thought reasoning traces from a model and aggregates final answers (majority voting) to improve reasoning accuracy over greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B (used as baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As used in experiments, CoT prompting with 20 sampled reasoning paths per problem (nucleus sampling: temp=0.6, top-p=0.9).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting', 'diverse multi-sampling with majority voting aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate N independent CoT reasoning paths per question; final answer chosen via majority vote over outputs. Relies on diversity of sampled reasoning chains to surface correct answers even when single greedy decode fails.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (explicitly relies on sampling diversity across full reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as baseline against PC, AC, and ESC. In experiments, SC uses 20 sampled paths (per Wang et al. 2023) for near-converged accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same benchmarks as PC: GSM8K, SVAMP, ASDiv, MultiArith, StrategyQA, RuinNames, SalientTranslation, Boolean Expressions, Tracking Shuffled Objects, Logical Deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Baseline numbers in paper vary by table: e.g., Table 5 lists SC tokens per GSM8K problem = 3,860 with accuracy 68.4% (table-specific). Table 3 lists SC accuracies per dataset (e.g., GSM8K 64.1% in one table context). SC provides the reference accuracy against which PC and other optimizers are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>SC's performance depends on the diversity of reasoning samples; it improves accuracy over greedy decoding but can be computationally expensive because it repeats full-path generation for many branches and wastes tokens on incorrect/redundant branches.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency is effective because of diversity across sampled reasoning paths, but it is computationally expensive; PC intends to keep SC's benefits while reducing redundant computation by leveraging intermediate prefix information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8264.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8264.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive-Consistency (confidence-based early stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that introduces a stopping rule: if a highly confident answer is observed during sampling, terminate further sampling and return that answer to save computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaptiveconsistency for efficient reasoning and coding with llms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B (used as baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as an alternative optimization to self-consistency; uses confidence measures (beta confidence or similar) to decide early termination of sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting', 'self-consistency sampling with early stop based on confidence']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Samples reasoning paths; after each new sample, compute a confidence metric over observed answers; if confidence exceeds a threshold, stop sampling and output the current majority answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>more similar (effectively reduces the number of samples by early exit, thereby reducing the realized diversity of full-path samples)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared directly with PC and SC in tables (Table 5 and others). The paper reports token usage and accuracy for AC vs SC vs PC (same generation hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA (reported in Table 5) and other benchmarks in broader evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 5 (representative): GSM8K AC tokens per problem = 2,248 (−41.7% vs SC 3,860), accuracy 67.2% (vs SC 68.4% in that table). SVAMP tokens 935 (−59.7%), accuracy 79.7% (vs SC 80.4%). StrategyQA tokens 706 (−60.9%), accuracy 71.2% (similar to SC 71.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>AC achieves significant token/latency reductions by early stopping but does so by sacrificing sampling diversity; this can be effective when confidence estimates are reliable but may risk selecting an incorrect high-confidence minority answer in difficult cases.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Adaptive-consistency attains large efficiency gains by truncating sampling when a confident answer appears, but this comes at the cost of reduced realized diversity; PC is presented as an alternative that preserves more diversity while achieving equal or greater efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8264.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8264.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early-Stopping Self-Consistency (window-based early exit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An optimization that divides sampling into windows; if all answers in a window are identical, stop further sampling and exit early to save computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B (used as baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a baseline optimizer; it splits the overall branch budget into equal windows and checks for identical answers within a window to trigger early exit.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting', 'self-consistency sampling with windowed identical-answer early exit']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate branches in fixed-size windows; after each window, if all generated answers in that window are identical, stop and return that answer (no further branches). This reduces tokens by early termination when sampling has converged.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>more similar (reduces realized diversity by potentially stopping early when uniform answers are observed)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>ESC compared with PC, SC, and AC across same datasets and sampling hyperparameters; token consumption and accuracy reported in Table 5 and other tables.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA (reported in Table 5) and other benchmarks used in main evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Representative numbers (Table 5): GSM8K ESC tokens per problem = 2,710 (−29.8% vs SC 3,860), accuracy 67.2% (vs SC 68.4%). SVAMP tokens 1,154 (−50.3%), accuracy 79.2% (vs SC 80.4%). StrategyQA tokens 782 (−56.7%), accuracy 70.8% (vs SC 71.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ESC yields meaningful efficiency gains but, like AC, achieves them by truncating sampling and thereby reducing effective diversity; the paper notes ESC and AC both sacrifice diversity to get speedups, while PC aims to retain diversity through prefix sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>ESC is an effective early-exit optimization but tends to reduce sampling diversity; PC is proposed to be a preferable alternative because it maintains diverse exploration via sampling from an extracted prefix set while still reducing redundant computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8264.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8264.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits step-by-step intermediate reasoning from language models to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B (CoT prompts used as base prompting format)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT-style few-shot prompts (few-shot exemplars provided for arithmetic, symbolic, etc.) used to elicit multi-step rationales which are then sampled and aggregated by SC and PC.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (stepwise reasoning traces)', 'combined with sampling-based aggregation methods (SC, PC, AC, ESC)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompting that asks model to produce intermediate steps before final answer; forms the basic unit (full chain or prefix) that self-consistency and path-consistency sample over and/or condition upon.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse when combined with sampling; base CoT is a prompting technique rather than a diversity policy itself</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>CoT is the prompting backbone for all experiments; diversity arises from sampling multiple CoT traces (nucleus sampling, temp 0.6, top-p 0.9). Comparisons center on how to exploit multiple CoT traces (full-path sampling vs prefix-guided sampling vs early exit).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same benchmarks as above (arithmetic, commonsense, symbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported as a standalone numeric baseline in isolation in this paper (CoT is the prompting paradigm used across methods), but the efficacy of CoT traces is implicit in all reported accuracy numbers for SC, PC, AC, ESC.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT generates interpretable intermediate steps which can be mined (as PC does) for useful prefixes; the paper leverages CoT to extract prefixes and argues that useful intermediate steps can be reliably detected via simple confidence metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT remains the underlying elicitation mechanism for multi-step reasoning; path-consistency leverages intermediate CoT steps (prefixes) to accelerate sampling without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Adaptiveconsistency for efficient reasoning and coding with llms. <em>(Rating: 2)</em></li>
                <li>Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-evaluation guided beam search for reasoning. <em>(Rating: 1)</em></li>
                <li>Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8264",
    "paper_id": "paper-272368345",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "PC",
            "name_full": "Path-Consistency (prefix-enhanced self-consistency)",
            "brief_description": "A dynamic inference procedure that extracts confident intermediate reasoning steps (prefixes) from early sampled chains and uses those prefixes to guide subsequent branch generation, reducing token consumption and latency while preserving reasoning diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B (primary); also evaluated on DeepSeek-V3 and Llama-3.2-1B-Instruct",
            "model_description": "Open-source Llama3-8B backbone used for most experiments; nucleus sampling (temperature=0.6, top-p=0.9) with 20 sampled reasoning paths per example. Additional evaluations run on DeepSeek-V3 and a smaller Llama-3.2-1B instruct model in the appendix.",
            "reasoning_methods": [
                "chain-of-thought prompting (CoT)",
                "self-consistency style multiple-sampling with majority/voting-based confidence",
                "prefix-guided sampling (path-consistency - iterative extract-and-sample)"
            ],
            "reasoning_methods_description": "CoT prompting is used to elicit multi-step reasoning paths. The method samples multiple reasoning paths (like self-consistency) but periodically computes a confidence metric (beta confidence) over answers in a window; when confidence exceeds threshold, short prefixes (first t steps) from the most confident paths are extracted and appended to the prompt for subsequent windows. Sampling then continues conditioned on these prefixes, iteratively increasing prefix length until final answers are generated and aggregated (majority vote).",
            "reasoning_diversity": "both (intended to preserve the diversity of full-path sampling while using constrained, prefix-guided conditioning to reduce redundant/wasteful branches)",
            "reasoning_diversity_experimental_setup": "Compared path-consistency (PC) to: self-consistency (SC) baseline (20 samples), Adaptive-Consistency (AC) (confidence-based early stop), and Early-Stopping Self-Consistency (ESC) (window-based identical-answer exit). Ablations included varying confidence thresholds (0.5-1.0), different maximum prefix levels (level-3 vs level-4), prefix extraction frequencies (windows splitting 20 branches), and experiments on a smaller model; all runs used nucleus sampling (temp=0.6, top-p=0.9) and 20 branches unless otherwise stated.",
            "task_or_benchmark": "Arithmetic reasoning (GSM8K, SVAMP, ASDiv, MultiArith), Commonsense reasoning (StrategyQA, RuinNames, SalientTranslation), Symbolic reasoning (Boolean Expressions, Tracking Shuffled Objects, Logical Deduction).",
            "performance_results": "Reported aggregate/representative results: average accelerations reported in paper introduction — arithmetic tasks: 28.7% speedup on average; commonsense: 20.9%; symbolic: 20.3%. Dataset-level examples (from tables in paper): GSM8K accuracy improved from SC baseline to PC (example: SC reported ≈64.1% in one table, PC up to 67.8% — +3.7 to +3.8 percentage points depending on threshold) with speedups up to +28.9% overall and token reductions up to −24.0% (Table 2/3). SVAMP: PC speedup up to +48.3% and small accuracy changes (≈ −0.5 to +0.6 pp across thresholds). MultiArith: accuracy increases reported up to +2.2 pp and speedups ≈ +42.7%. Tracking Shuffled Objects (symbolic): PC showed highest accuracy gain of +5.2 pp vs baseline and speedups ≈ +18.4%. Table 5 (comparison of techniques) shows tokens per GSM8K problem: SC 3,860 tokens, AC 2,248 (−41.7%), ESC 2,710 (−29.8%), PC 1,630 (−57.8%); accuracy on GSM8K in that table: SC 68.4%, AC 67.2%, ESC 67.2%, PC 67.8%. The paper also reports extreme per-level speedups (prefix level-3) up to +78.9% in some GSM8K settings (but with different tradeoffs by prefix level and confidence threshold).",
            "qualitative_findings": "Path-consistency reduces wasted token generation on incorrect branches and reduces redundancy on correct branches (Figure 5). It increases the number of high-confidence final answers and reduces samples clustered around ~0.5 confidence (Figure 7). The authors present a theoretical analysis showing PC will not worsen the 'truth-in-hands-of-a-few' failure mode provided the initial majority-confidence P_vote ≥ 0.5, and advocate using confidence thresholds to protect against selecting harmful prefixes. Aggressive prefixing (lower threshold, more levels) can improve speed but risks selecting suboptimal prefixes and reducing accuracy, especially on harder/different-format tasks (e.g., Ruin Names or long-input tasks).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Path-consistency preserves much of the diversity benefit of self-consistency while substantially reducing token consumption and inference latency (often 20–40% speedups, sometimes higher at deeper prefixing levels) without degrading—and in some cases improving—accuracy across arithmetic, commonsense, and symbolic reasoning benchmarks. Unlike other acceleration methods (AC, ESC) that gain speed primarily by early-exiting and thus sacrificing diversity, PC purposely extracts and samples from prefix sets to maintain diversity while improving efficiency and robustness; it is model-agnostic and requires no additional training.",
            "uuid": "e8264.0",
            "source_info": {
                "paper_title": "Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "SC",
            "name_full": "Self-Consistency (multiple-sample majority voting)",
            "brief_description": "A sampling strategy that elicits multiple chain-of-thought reasoning traces from a model and aggregates final answers (majority voting) to improve reasoning accuracy over greedy decoding.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "Llama3-8B (used as baseline in this paper)",
            "model_description": "As used in experiments, CoT prompting with 20 sampled reasoning paths per problem (nucleus sampling: temp=0.6, top-p=0.9).",
            "reasoning_methods": [
                "chain-of-thought prompting",
                "diverse multi-sampling with majority voting aggregation"
            ],
            "reasoning_methods_description": "Generate N independent CoT reasoning paths per question; final answer chosen via majority vote over outputs. Relies on diversity of sampled reasoning chains to surface correct answers even when single greedy decode fails.",
            "reasoning_diversity": "diverse (explicitly relies on sampling diversity across full reasoning paths)",
            "reasoning_diversity_experimental_setup": "Used as baseline against PC, AC, and ESC. In experiments, SC uses 20 sampled paths (per Wang et al. 2023) for near-converged accuracy.",
            "task_or_benchmark": "Same benchmarks as PC: GSM8K, SVAMP, ASDiv, MultiArith, StrategyQA, RuinNames, SalientTranslation, Boolean Expressions, Tracking Shuffled Objects, Logical Deduction.",
            "performance_results": "Baseline numbers in paper vary by table: e.g., Table 5 lists SC tokens per GSM8K problem = 3,860 with accuracy 68.4% (table-specific). Table 3 lists SC accuracies per dataset (e.g., GSM8K 64.1% in one table context). SC provides the reference accuracy against which PC and other optimizers are compared.",
            "qualitative_findings": "SC's performance depends on the diversity of reasoning samples; it improves accuracy over greedy decoding but can be computationally expensive because it repeats full-path generation for many branches and wastes tokens on incorrect/redundant branches.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-consistency is effective because of diversity across sampled reasoning paths, but it is computationally expensive; PC intends to keep SC's benefits while reducing redundant computation by leveraging intermediate prefix information.",
            "uuid": "e8264.1",
            "source_info": {
                "paper_title": "Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "AC",
            "name_full": "Adaptive-Consistency (confidence-based early stopping)",
            "brief_description": "A method that introduces a stopping rule: if a highly confident answer is observed during sampling, terminate further sampling and return that answer to save computation.",
            "citation_title": "Adaptiveconsistency for efficient reasoning and coding with llms.",
            "mention_or_use": "use",
            "model_name": "Llama3-8B (used as baseline comparison)",
            "model_description": "Applied as an alternative optimization to self-consistency; uses confidence measures (beta confidence or similar) to decide early termination of sampling.",
            "reasoning_methods": [
                "chain-of-thought prompting",
                "self-consistency sampling with early stop based on confidence"
            ],
            "reasoning_methods_description": "Samples reasoning paths; after each new sample, compute a confidence metric over observed answers; if confidence exceeds a threshold, stop sampling and output the current majority answer.",
            "reasoning_diversity": "more similar (effectively reduces the number of samples by early exit, thereby reducing the realized diversity of full-path samples)",
            "reasoning_diversity_experimental_setup": "Compared directly with PC and SC in tables (Table 5 and others). The paper reports token usage and accuracy for AC vs SC vs PC (same generation hyperparameters).",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA (reported in Table 5) and other benchmarks in broader evaluation.",
            "performance_results": "Table 5 (representative): GSM8K AC tokens per problem = 2,248 (−41.7% vs SC 3,860), accuracy 67.2% (vs SC 68.4% in that table). SVAMP tokens 935 (−59.7%), accuracy 79.7% (vs SC 80.4%). StrategyQA tokens 706 (−60.9%), accuracy 71.2% (similar to SC 71.7%).",
            "qualitative_findings": "AC achieves significant token/latency reductions by early stopping but does so by sacrificing sampling diversity; this can be effective when confidence estimates are reliable but may risk selecting an incorrect high-confidence minority answer in difficult cases.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Adaptive-consistency attains large efficiency gains by truncating sampling when a confident answer appears, but this comes at the cost of reduced realized diversity; PC is presented as an alternative that preserves more diversity while achieving equal or greater efficiency.",
            "uuid": "e8264.2",
            "source_info": {
                "paper_title": "Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ESC",
            "name_full": "Early-Stopping Self-Consistency (window-based early exit)",
            "brief_description": "An optimization that divides sampling into windows; if all answers in a window are identical, stop further sampling and exit early to save computation.",
            "citation_title": "Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning.",
            "mention_or_use": "use",
            "model_name": "Llama3-8B (used as baseline comparison)",
            "model_description": "Applied as a baseline optimizer; it splits the overall branch budget into equal windows and checks for identical answers within a window to trigger early exit.",
            "reasoning_methods": [
                "chain-of-thought prompting",
                "self-consistency sampling with windowed identical-answer early exit"
            ],
            "reasoning_methods_description": "Generate branches in fixed-size windows; after each window, if all generated answers in that window are identical, stop and return that answer (no further branches). This reduces tokens by early termination when sampling has converged.",
            "reasoning_diversity": "more similar (reduces realized diversity by potentially stopping early when uniform answers are observed)",
            "reasoning_diversity_experimental_setup": "ESC compared with PC, SC, and AC across same datasets and sampling hyperparameters; token consumption and accuracy reported in Table 5 and other tables.",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA (reported in Table 5) and other benchmarks used in main evaluations.",
            "performance_results": "Representative numbers (Table 5): GSM8K ESC tokens per problem = 2,710 (−29.8% vs SC 3,860), accuracy 67.2% (vs SC 68.4%). SVAMP tokens 1,154 (−50.3%), accuracy 79.2% (vs SC 80.4%). StrategyQA tokens 782 (−56.7%), accuracy 70.8% (vs SC 71.7%).",
            "qualitative_findings": "ESC yields meaningful efficiency gains but, like AC, achieves them by truncating sampling and thereby reducing effective diversity; the paper notes ESC and AC both sacrifice diversity to get speedups, while PC aims to retain diversity through prefix sampling.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "ESC is an effective early-exit optimization but tends to reduce sampling diversity; PC is proposed to be a preferable alternative because it maintains diverse exploration via sampling from an extracted prefix set while still reducing redundant computation.",
            "uuid": "e8264.3",
            "source_info": {
                "paper_title": "Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting strategy that elicits step-by-step intermediate reasoning from language models to improve multi-step problem solving.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "Llama3-8B (CoT prompts used as base prompting format)",
            "model_description": "CoT-style few-shot prompts (few-shot exemplars provided for arithmetic, symbolic, etc.) used to elicit multi-step rationales which are then sampled and aggregated by SC and PC.",
            "reasoning_methods": [
                "chain-of-thought (stepwise reasoning traces)",
                "combined with sampling-based aggregation methods (SC, PC, AC, ESC)"
            ],
            "reasoning_methods_description": "Prompting that asks model to produce intermediate steps before final answer; forms the basic unit (full chain or prefix) that self-consistency and path-consistency sample over and/or condition upon.",
            "reasoning_diversity": "diverse when combined with sampling; base CoT is a prompting technique rather than a diversity policy itself",
            "reasoning_diversity_experimental_setup": "CoT is the prompting backbone for all experiments; diversity arises from sampling multiple CoT traces (nucleus sampling, temp 0.6, top-p 0.9). Comparisons center on how to exploit multiple CoT traces (full-path sampling vs prefix-guided sampling vs early exit).",
            "task_or_benchmark": "Same benchmarks as above (arithmetic, commonsense, symbolic).",
            "performance_results": "Not reported as a standalone numeric baseline in isolation in this paper (CoT is the prompting paradigm used across methods), but the efficacy of CoT traces is implicit in all reported accuracy numbers for SC, PC, AC, ESC.",
            "qualitative_findings": "CoT generates interpretable intermediate steps which can be mined (as PC does) for useful prefixes; the paper leverages CoT to extract prefixes and argues that useful intermediate steps can be reliably detected via simple confidence metrics.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "CoT remains the underlying elicitation mechanism for multi-step reasoning; path-consistency leverages intermediate CoT steps (prefixes) to accelerate sampling without retraining.",
            "uuid": "e8264.4",
            "source_info": {
                "paper_title": "Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Adaptiveconsistency for efficient reasoning and coding with llms.",
            "rating": 2,
            "sanitized_title": "adaptiveconsistency_for_efficient_reasoning_and_coding_with_llms"
        },
        {
            "paper_title": "Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning.",
            "rating": 2,
            "sanitized_title": "escape_skyhigh_cost_earlystopping_selfconsistency_for_multistep_reasoning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-evaluation guided beam search for reasoning.",
            "rating": 1,
            "sanitized_title": "selfevaluation_guided_beam_search_for_reasoning"
        },
        {
            "paper_title": "Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning.",
            "rating": 1,
            "sanitized_title": "deductive_beam_search_decoding_deducible_rationale_for_chainofthought_reasoning"
        }
    ],
    "cost": 0.0161175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PATH-CONSISTENCY WITH PREFIX ENHANCEMENT FOR EFFICIENT INFERENCE IN LLMS</p>
<p>Jiace Zhu zhujiace@sjtu.edu.cn 
Shanghai Jiao Tong University</p>
<p>Yuanzhe Huang 
Shanghai Jiao Tong University</p>
<p>Yingtao Shen 
Shanghai Jiao Tong University</p>
<p>An Zou an.zou@sjtu.edu.cn 
Shanghai Jiao Tong University</p>
<p>Jie Zhao zhaojie@microsoft.com 
Microsoft Corporation</p>
<p>PATH-CONSISTENCY WITH PREFIX ENHANCEMENT FOR EFFICIENT INFERENCE IN LLMS
B08C585EEB515369437B6C69DE42482D
To enhance the reasoning capabilities of large language models (LLMs), selfconsistency has become a popular approach, combining multiple samplings with majority voting.However, current methods are computationally expensive and time-consuming due to the need for numerous samplings.To address this, this paper introduces path-consistency, which leverages the confidence of earliergenerated answers to identify the most promising prefix and guide the generation of subsequent branches.By dynamically guiding the generation of subsequent branches based on this prefix, path-consistency mitigates both the errors and redundancies from random or less useful sampling in self-consistency.This approach reduces errors and redundancies from random sampling, significantly accelerating inference by minimizing token consumption.Our extensive empirical results demonstrate that path-consistency improves inference latency by up to 40.5%, while maintaining task accuracy across various tasks, including mathematical reasoning, commonsense reasoning, and symbolic reasoning.</p>
<p>INTRODUCTION</p>
<p>The range of tasks that large language models (LLMs) can accomplish is continuously expanding, as the scale and complexity of models continue to grow recently.However, this advancement has not yet endowed LLMs with sufficiently robust reasoning capabilities Rae et al. (2021); Wu et al. (2016); Guo et al. (2018); Chen et al. (2022).To address this shortcoming and further extend the application scope of LLMs, Chain-of-Thought (CoT) Wei et al. (2022) prompting has emerged in response.CoT prompting uses reasoning problems and processes as input prompts to guide language models in generating reasoning paths and final answers, aiming to mimic the thought processes humans might use when solving mathematical or logical problems.This enables LLMs to be applied in an increasing number of specific scenarios, such as mathematical reasoning Cobbe et al. (2021); Miao et al. (2020) and logical reasoning Geva et al. (2021).To better accomplish these complex tasks, there are several CoT-based optimization methods Li et al. (2022); Chen et al. (2022).One of the common and effective methods is self-consistency Wang et al. (2023), a technique involving multiple sampling and majority voting.In this approach, the model generates multiple reasoning paths for a given input, with the final decision based on the most frequently occurring output among the samples.</p>
<p>Self-consistency significantly enhances the reasoning capabilities of LLMs by sampling a large number of examples during inference.However, it has limitations in practical applications.The basic self-consistency technique frequently invokes the model to generate numerous reasoning paths when solving specific problems Wang et al. (2023).As model size and task complexity increase, the time and computation cost of self-consistency increase sharply, making it a critical issue in practical applications.Motivated by this challenge, this paper proposes path-consistency, which is designed to reduce the computation and time cost by leveraging intermediate information from earlier generation results to assist in subsequent generations.As illustrated in fig. 1, the proposed dynamic inference method, path-consistency, continuously extracts appropriate reasoning steps from already generated reasoning paths to serve as "prefixes".These prefixes are then used to guide and accelerate the generation of subsequent reasoning steps.</p>
<p>The proposed path-consistency offers several key advantages: (1) It accelerates inference speed and reduces token consumption, while significantly preserving or even improving task accuracy.(2) The method requires no additional computation, fine-tuning, or training, ensuring the generation quality of the model remains intact.( 3) It is model-agnostic, making it easy to deploy and apply to various models and tasks in practical scenarios.Furthermore, it integrates seamlessly with existing optimization methods, achieving even better acceleration performance.</p>
<p>We evaluated path-consistency using ten different datasets, resulting in an average acceleration of 28.7% for mathematical reasoning tasks, 20.9% for commonsense reasoning, and 20.3% for symbolic reasoning.Moreover, by continuously extracting potentially correct prefixes, this method has minimal impact on task accuracy and can even enhance it.</p>
<p>BACKGROUND</p>
<p>2.1 REASONING WITH SELF-CONSISTENCY Self-consistency Wang et al. (2023) is a sampling strategy distinct from greedy decoding Vaswani et al. (2017), significantly enhancing the reasoning performance of language models.This approach generates multiple reasoning paths and aggregates the final output through majority voting.</p>
<p>Earlier research primarily focused on optimizing reasoning for individual tasks  Paul et al. (2024).For example, self-evaluation guided by random beam search Xie et al. (2024) leverages additional language models to search for the optimal inference path.Other methods, such as Deductive Beam Search Zhu et al. (2024), optimize inference by emphasizing the relationships between each reasoning step.However, these approaches often require increased computational resources and sometimes involve extensive model calls to improve task accuracy in LLMs.This results in additional time costs, always making them inefficient in practical applications and contrary to our original intention.</p>
<p>APPROACHES FOR EFFICIENT REASONING</p>
<p>Recent works have attempted to achieve efficient reasoning by using smaller models or increasing inference speed.Distillation Hinton et al. (2015), as an effective model compression technique, has been employed to create smaller models for reasoning tasks Fu et al. (2023); Ho et al. (2023);Magister et al. (2023); Li et al. (2023).Additionally, various approaches have sought to improve inference speed by altering inference strategies, thereby avoiding modifications to the model architecture and retraining Aggarwal et al. (2023); Li et al. (2024).However, these methods often compromise the diversity inherent in self-consistency, which can negatively impact the quality of the generated outputs.The proposed path-consistency is a model-agnostic method applicable to most common models, including compressed models, to enhance inference efficiency while maintaining task accuracy.An "extract-and-sample" inference process of the proposed path-consistency.It seeks the "optimal path" in the form of the "prefix", thereby progressively reducing the number of generated tokens and significantly shortening inference latency.</p>
<p>MOTIVATION</p>
<p>Self-consistency in large language models (LLMs) involves generating multiple branches of reasoning, each potentially leading to different answers.The final answer is determined through aggregation methods, such as majority voting.Unlike greedy decoding, self-consistency avoids the shortcomings of local optimality and reduces the randomness of single-step sampling.However, the primary drawback of self-consistency is the significant computational redundancy.With N branches, each answer is derived from N similar but independent inference processes, leading to an N -fold increase in computational cost compared to greedy decoding for a single problem.To improve self-consistency, the goal is to achieve similar sampling effects while reducing the time cost of redundant computations.</p>
<p>We propose an intuitive hypothesis: for example, a particular mathematical problem might have five different reasoning paths, p 1 to p 5 .If the model frequently errs on paths p 1 to p 3 while p 4 and p 5 are relatively simpler, then full self-consistency wastes significant computational resources on the problematic paths.As shown in fig.2, statistics on the number of tokens generated during self-consistency across various datasets reveal that over 25% and sometimes even 50% of tokens are wasted on incorrect branches.By sampling multiple times only on p 4 and p 5 , we could enhance resource utilization and improve output accuracy.Furthermore, storing limited information from paths p 4 and p 5 to guide subsequent branch generation could significantly accelerate inference speed and efficiency.</p>
<p>Additionally, self-consistency involves extensive redundant processes without yielding intermediate results, with the final answer only emerging at the end.If useful information could be identified early in the generation process to guide subsequent branching, outcomes might improve.Intuitively, when tackling complex problems, using simple criteria to preliminarily assess the quality or correctness of the current generation during intermediate stages can enhance the effectiveness of subsequent steps.Our method aims to reduce the time wasted on incorrect branches while increasing the efficiency of generating correct inference paths.</p>
<p>METHODOLOGY 4.1 PATH-CONSISTENCY</p>
<p>Based on the internal mechanisms of self-consistency, we propose an automated dynamic reasoning approach path-consistency that continually seeks the "optimal path" in the form of "prefix".This will progressively reduce the number of generated tokens and significantly shorten inference latency.The methodology is shown in fig. 3 and table 1, and can be described as the following "extract-andsample" process:</p>
<p>• Determine the maximum number of branches and the highest prefix level for the specific task, and use these to divide all branches into multiple windows of equal length.Begin by generating a small number of branches for the first window.• Then, use a confidence assessment metric1 to assess the confidence of the answers generated in the current window.If the confidence exceeds the set threshold, extract shorter prefixes from these optimal paths (e.g., the first step of the current optimal reasoning paths) to guide subsequent generation; if the confidence is low, continue generating branches for the subsequent windows at the current prefix level to ensure a safer prefix selection.• Randomly sample from the extracted prefixes as part of the prompt to guide subsequent generation.After the branches for the next window are generated, continue using the confidence assessment metric to find the optimal inference path and extract the prefix for the next level (e.g., the first two steps of the current optimal reasoning paths).• Repeat the above steps iteratively, extending the prefix length until the optimal reasoning path is identified and aggregated to produce the final answer.</p>
<p>Table 1 illustrates the reasoning process of path-consistency with an example.We have taken a reasoning path from different stages of the branching process as samples in table 1.In the initial stage, the model generates the full reasoning paths and answers normally.When a particular answer appears frequently and has a higher confidence level compared to other answers within the current branch, the first step of the corresponding reasoning path is extracted and used as the input prompt for the next stage.As shown in table 1, as the number of branches increases, the prefix length grows while the newly generated portion of the path subsequently shortens.</p>
<p>Table 1: The generated parts are indicated in blue in the table, with "Level-X prefix" denoting the action of extracting the prefix for the X th time.After three prefix extractions, the model only needs to generate the final sentence of the conclusion based on the prefix.</p>
<p>CoT-prompting: Q: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?A: To make a robe, we need 2 bolts of blue fiber and half that much white fiber.We can write this as 2 + 1/2 * 2. Now 1/2 * 2 = 1, so the equation becomes 2 + 1 = 3.The answer is 3.</p>
<p>Level-I prefix:</p>
<p>Q: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?A: To make a robe, we need 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = 1 bolt of white fiber. 2 + 1 = 3 bolts in total.The answer is 3.</p>
<p>Level-II prefix:</p>
<p>Q: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?A: To make a robe, we need 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = 1 bolt of white fiber. 1 + 2 = 3 bolts in total.The answer is 3.</p>
<p>Level-III prefix:</p>
<p>Q: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?A: To make a robe, we need 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = 1 bolt of white fiber. 1 + 2 = 3 bolts in total.The answer is 3.</p>
<p>When solving complex problems, LLM takes a question, represented as q, and generates a distribution of answers, denoted as P (a | q), after producing a reasoning path.During inference, the model generates a multi-step reasoning path R = r 1 , r 2 , • • • , r T = r 1:T under the guidance of CoT.This process can also be expressed as
P (a | q) = E R∼P (R|q) P (a | q, R).(1)
In basic self-consistency, the generation process is mechanically repeated numerous times without variation, whereas path-consistency attempts to gather advantageous intermediate information from the early stages of the generation process for subsequent generations.Path-consistency gradually identifies sufficiently confident partial reasoning steps
R prefix = [r 1 , r 2 , • • • , r t ]
and incorporates them as part of the input, thereby better guiding the subsequent reasoning steps and the generation of the answer a.Thus, this process can be expressed as
P (a | q, R prefix ) = E R∼P (R|q,R prefix ) P (a | q, R).(2)
Combining the above expressions, we obtain the following formula via the law of total probability:
P (a | q) = R prefix P (R prefix | q)P (a | q, R prefix ).(3)
Path-consistency utilizes basic majority voting or other lightweight confidence metrics to model the distribution P (R prefix | q), facilitating the transformation from P (a | q) to P (a | q, R prefix ).This approach enhances efficiency while maintaining the model's performance across various tasks.</p>
<p>PROBLEMS WITH "TRUTH IS IN HANDS OF A FEW"</p>
<p>In the basic self-consistency method, when generating answers for particularly challenging questions, the problems with "Truth Is in the Hands of a Few" may still occur despite the repeated generation of multiple branches.This means that the correct answer may not be the most frequently occurring one, leading to an incorrect final answer in majority voting.In the proposed path-consistency method, during the continuous exploration for the optimal path, if the problems with "Truth Is in the Hands of a Few" are encountered at a specific prefix selection, there's a concern that this undesirable phenomenon may be exacerbated in subsequent branches.We will use the following to demonstrate that the proposed approach does not worsen this problem.</p>
<p>To present the analysis, we make the following assumptions: the probability of generating the correct answer is p 0 , and apart from the correct answer, the model generates only one unique incorrect answer.The total number of branches is set to N , and prefix selection based on majority voting is performed only once at N 2 .</p>
<p>If the correct answer is the majority at the time of prefix selection, the correct prefix will be selected to guide subsequent generation, increasing the probability of generating the correct answer in the remaining branches to p 1 = p 0 +∆p; Conversely, if the incorrect answer is the majority at this point, the probability of generating the correct answer in subsequent branches decreases to p 2 = p 0 − ∆p.Using the binomial distribution formula, the probability of the correct answer being in the majority during the vote is given by:
Pvote = N 2 k=⌈ N 4 ⌉ N 2 k p k 0 (1 − p0) N 2 −k .(4)
After prefix selection, the probability of obtaining the correct answer in the subsequent N 2 branches increases or decreases to
Pinc = N 2 k=⌈ N 4 ⌉ N 2 k p k 1 (1 − p1) N 2 −k = N 2 k=⌈ N 4 ⌉ N 2 k (p0 + ∆p) k (1 − p0 − ∆p) N 2 −k = N 2 k=⌈ N 4 ⌉ N 2 k p k 0 1 + ∆p p0 k (1 − p0) N 2 −k 1 − ∆p 1 − p0 N 2 −k ≈ N 2 k=⌈ N 4 ⌉ N 2 k p k 0 (1 − p0) N 2 −k 1 + k p0 ∆p 1 − N 2 − k 1 − p0 ∆p ≈ N 2 k=⌈ N 4 ⌉ N 2 k p k 0 (1 − p0) N 2 −k 1 + k − N 2 p0 p0(1 − p0) ∆p ,(5)
and
Pdec = N 2 k=⌈ N 4 ⌉ N 2 k p k 2 (1 − p2) N 2 −k = N 2 k=⌈ N 4 ⌉ N 2 k (p0 − ∆p) k (1 − p0 + ∆p) N 2 −k ≈ N 2 k=⌈ N 4 ⌉ N 2 k p k 0 (1 − p0) N 2 −k 1 − k − N 2 p0 p0(1 − p0) ∆p .(6)
The two equations above use the first-order Taylor expansion, neglecting higher-order terms.The relationship between the two values is
P inc + P dec = 2 • P vote .(7)
Meanwhile, the final probability of obtaining the correct answer after prefix selection is
P ′ correct = P vote • P inc + (1 − P vote ) • P dec . (8)
If no prefix selection is performed, the probability of generating the correct answer in subsequent steps remains unchanged and is equal to the probability at N 2 with majority voting: P correct = P vote .To ensure that accuracy is not adversely affected, we require P ′ correct ≥ P correct , which can be simplified to:
P vote ≥ P vote − P dec P inc − P dec .(9)
By combining eq. ( 7), we can obtain P vote ≥ 0.5.This proves that if the model's initial performance ensures P vote ≥ 0.5, path-consistency will be sufficiently reliable and will not harm accuracy.</p>
<p>During prefix selection, employing confidence-based criteria can make this process more reliable.For instance, using the beta confidence criteria Aggarwal et al. ( 2023)
0.5 0 p N 2 −vm • (1 − p) vm dp,(10)
where v m represents the number of major elements.A prefix is considered sufficiently reliable for selection only if this value exceeds a predefined confidence threshold C threshold .</p>
<p>While this calculation is a rough estimate and the actual scenario is likely more complex, it provides insight into the effect of path-consistency on problems where "Truth Is in the Hands of a Few".Essentially, path-consistency tends to enhance the accuracy of self-consistency: when selfconsistency performs well for a particular input, path-consistency may perform even better; when self-consistency performs poorly, the accuracy of path-consistency might decrease, but not significantly.Moreover, prefix selection guided by a confidence threshold provides a safer alternative to direct majority-vote selection.Hyperparameters: For LLM inference with self-consistency and path-consistency, we employed nucleus sampling with a temperature of 0.6 and top-p of 0.9, generating 20 paths for each example.</p>
<p>According to Wang et al. (2023), 20 sampled reasoning paths almost achieve convergence in task accuracy, with only marginal efficiency gains observed when increasing to 40 paths.Therefore, 20 paths are more suitable for evaluating the optimization capability of path-consistency in terms of efficiency.Meanwhile, we used the beta confidence criteria in eq. ( 10) for confidence calculation in path-consistency Aggarwal et al. (2023), with threshold values ranging from 0.5 to 1.0.</p>
<p>Metrics:</p>
<p>We employed multiple metrics to comprehensively compare the performance of pathconsistency against baselines, including reasoning accuracy, inference latency, and token consumption during reasoning.Since inference latency is highly dependent on the operating environment and hardware configuration, considering the absolute value of inference latency is not meaningful.Instead, we calculated speedup under identical conditions as a standardized evaluation metric.All experiments were conducted on a single NVIDIA GeForce RTX 3090 GPU.Table 2 presents the performance and efficiency of path-consistency on the GSM8K dataset across different confidence thresholds and prefix levels.The experiments compare path-consistency with the basic self-consistency method in terms of task accuracy, inference latency, and token consumption.The results show that, across all confidence criteria, path-consistency not only enhances task accuracy but also provides a maximum acceleration of 28.9%.Observing the acceleration process of path-consistency, it is evident that as the prefix level increases, the speed improvement becomes more significant, reaching up to 78.9% at level-3 while maintaining at least 27.0%, as shown in fig. 4.</p>
<p>EVALUATION OF PATH-CONSISTENCY</p>
<p>A CASE STUDY ON GSM8K</p>
<p>PERFORMANCE ON DIFFERENT TASKS</p>
<p>Arithmetic Reasoning: Table 3, rows 1-4, shows the task performance on arithmetic reasoning datasets, along with three additional datasets.Path-consistency ensures task performance that is almost comparable to or even better than the baseline under various confidence settings.On GSM8K, SVAMP, ASDiv, and MultiArith, accuracy improvements of up to 3.8%, 0.6%, 0.2%, and 2.2% are observed, respectively.Due to the varying difficulty levels of the datasets for the LLM, the optimal confidence threshold differs for each dataset.For instance, higher or lower confidence thresholds may enhance performance on SVAMP and ASDiv.</p>
<p>The results also highlight the efficiency improvements on arithmetic reasoning datasets.Pathconsistency achieves acceleration rates of up to 28.9%, 48.3%, 42.0%, and 42.7% on GSM8K, SVAMP, ASDiv, and MultiArith, respectively, along with a corresponding decrease in token consumption.Comparing these results reveals that for a specific dataset, there exists a proper confidence threshold at which path-consistency simultaneously enhances both task performance and efficiency.</p>
<p>Commonsense Reasoning: Table 3, rows 5-7, reports the performance of path-consistency on commonsense reasoning datasets.In terms of task accuracy, path-consistency slightly underperforms compared to the baseline on the StrategyQA and Ruins Names tasks.However, this decline can be mitigated by properly adjusting the confidence threshold.On the Salient Translation task, pathconsistency still maintains better task performance compared to the baseline.Regarding acceleration, path-consistency provides up to 34.2%, 10.4%, and 30.5% speed improvements on StrategyQA, Ruins Names, and Salient Translation datasets, respectively.</p>
<p>According to the results, it can be found that the performance of path-consistency on the Ruins Names task is less pronounced compared to other datasets.This is because the Ruins Names task is more difficult, with longer input content and longer required prompts, making the benefits of the prefixing behavior less noticeable.Symbolic Reasoning: In general, lower confidence thresholds indicate a more aggressive prefix selection strategy, often resulting in more significant efficiency improvements.However, in the Logical Deduction task, the efficiency improvement is actually higher at a confidence threshold of 0.8 compared to 0.7.This is because a more aggressive prefix selection strategy is more likely to choose suboptimal prefixes, which makes the subsequent branching generation less effective.Specifically, we found that with a confidence threshold of 0.7, the efficiency improvement at prefix level-2 is more significant than at prefix level-3, as prefix level-3 generates many erroneous paths, leading to this phenomenon.Reducing Error and Redundancy: In section 3, we characterized the challenges of selfconsistency.On one hand, it wastes computational resources on incorrect branches.On the other hand, it repeats the same computations without obtaining useful intermediate information.Figure 5 shows the changes in the proportion of tokens generated by path-consistency on correct or incorrect reasoning paths.The results show a decrease in the number of tokens wasted on incorrect branches.Additionally, for correct branches, there is a significant reduction in redundant tokens, which greatly improves efficiency while maintaining accuracy.</p>
<p>Hyperparameters Analysis: We explored the impact of prefix extraction frequency on pathconsistency.With the setting of 20 branches, if the highest prefix level is set to level-3, the prefix is extracted every 5 branches; if the highest prefix level is level-4, the prefix is extracted every 4 branches.Table 4 indicates that maintaining the highest prefix level at level-3 preserves higher task accuracy.Increasing the highest level enables a more aggressive strategy, leading to a more noticeable acceleration.</p>
<p>COMPARISON WITH SIMILAR WORK</p>
<p>We compared path-consistency (PC) to the following methods: (1) Self-consistency (SC) Wang et al. (2023) is the baseline that combines multiple samplings with majority voting.( 2) Adaptiveconsistency (AC) Aggarwal et al. (2023) introduces the concept of confidence for the first time.If a highly confident answer is identified, the sampling process is terminated, and the answer is selected as the final output, preventing any further branch sampling and inference.( 3) Early-stopping self-consistency (ESC) Li et al. (2024) divides branches into equally sized windows.When all the answers generated within a given window are identical, then exit directly.</p>
<p>As shown in the experimental data in table 5, the path-consistency achieves task accuracy comparable to other optimization methods while consuming fewer tokens.The diversity of the reasoning paths is the key to a better performance in self-consistency Wang et al. (2023).AC and ESC achieve significant acceleration by sacrificing diversity through early exit strategies under large branching.</p>
<p>In contrast, path-consistency strives to preserve the diversity of reasoning paths during the sampling process from the prefix set.It makes more efficient use of available information, achieving task accuracy comparable to other methods with fewer branches and tokens.We can also observe the scalability of path-consistency with larger model sizes.Due to the inherently strong reasoning capabilities of larger models, the impact of the three optimization methods on accuracy remains relatively stable.However, path-consistency still achieves a substantial reduction in token consumption.This demonstrates that path-consistency exhibits excellent robustness and scalability, consistently providing efficiency improvements across different models and at larger scales.Furthermore, as shown in the Appendix, we also conducted experiments on a much smaller model.</p>
<p>CONCLUSION</p>
<p>This paper proposes path-consistency, which achieves internal optimization of self-consistency, extracting information from early branches in the form of "prefixes", guiding the generation of subsequent branches more efficiently.The effectiveness of path-consistency across a broad range of tasks, including arithmetic, commonsense, and symbolic reasoning, demonstrates its robustness in various application areas.Additionally, path-consistency is a lightweight, model-agnostic method that requires minimal additional computational resources and scales effectively to larger models.</p>
<p>A APPENDIX</p>
<p>A.1 FURTHER ANALYSIS</p>
<p>A.1.1 PERFORMANCE IN SMALL MODEL</p>
<p>As presented in the table 7, path-consistency substantially enhances the inference efficiency of small models on reasoning tasks.Nevertheless, due to the inherent accuracy limitations of the base models, the performance of path-consistency is inevitably influenced as well.6, we examined the relationship between the confidence of the majority element in the tenth branch of the self-consistency baseline and the final answer across all datasets.It was observed that a confidence level around 0.8 serves as a critical threshold for determining the correctness of the final answer.Therefore, selecting confidence levels in the range of [0.7, 0.8, 0.9] is a more appropriate choice.</p>
<p>A.1.3 PROTECTION OF HIGH-CONFIDENCE ANSWERS</p>
<p>As illustrated in the Figure 7, the distribution of confidence for the final answers across all samples is shown.The implementation of path-consistency has led to a significant increase in the number of high-confidence answers, while reducing the number of samples with confidence levels around 0.5.This suggests that path-consistency effectively resolves many ambiguous cases.Consequently, this approach not only avoids compromising task accuracy but may even enhance it.Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: We start with 15 trees.Later we have 21 trees.The difference must be the number of trees they planted.So, they must have planted 21 -15 = 6 trees.The answer is 6.Q: Shawn has five toys.For Christmas, he got two toys each from his mom and dad.How many toys does he have now?</p>
<p>A: He has 5 toys.He got 2 from mom, so after that he has 5 + 2 = 7 toys.Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys.The answer is 9.</p>
<p>Q: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?A: There are 4 days from monday to thursday.5 computers were added each day.That means in total 4 * 5 = 20 computers were added.There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.The answer is 29.</p>
<p>Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?A: Michael initially had 58 balls.He lost 23 on Tuesday, so after that he has 58 -23 = 35 balls.On Wednesday he lost 2 more so now he has 35 -2 = 33 balls.The answer is 33.</p>
<p>Q: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?A: She bought 5 bagels for $3 each.This means she spent 5 * $3 = $15 on the bagels.She had $23 in beginning, so now she has $23 -$15 = $8.The answer is 8. Q: Do hamsters provide food for any animals?A: Hamsters are prey animals.Prey are food for predators.Thus, hamsters provide food for some animals.The answer is yes.</p>
<p>Q: Yes or no: Hydrogen's atomic number squared exceeds number of Spice Girls?</p>
<p>A: Hydrogen has an atomic number of 1. 1 squared is 1.There are 5 Spice Girls.Thus, Hydrogen's atomic number squared is less than 5.The answer is no.</p>
<p>Q: Yes or no: Would a pear sink in water?A: The density of a pear is about 0.6 g/cm 3 , which is less than water.Objects less dense than water float.Thus, a pear would float.The answer is no.</p>
<p>Q: Yes or no: Is it common to see frost during some college commencements?A: College commencement ceremonies can happen in December, May, and June.December is in the winter, so there can be frost.Thus, there could be frost at some commencements.The answer is yes.</p>
<p>Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?A: The War in Vietnam was 6 months.The gestation period for a llama is 11 months, which is more than 6 months.Thus, a llama could not give birth twice during the War in Vietnam.The answer is no.</p>
<p>Q: Could Brooke Shields succeed at University of Pennsylvania?A: Brooke Shields went to Princeton University.Princeton University is about as academically rigorous as the University of Pennsylvania.Thus, Brooke Shields could also succeed at the University of Pennsylvania.The answer is yes.</p>
<p>1 arXivFigure 1 :Figure 2 :
112
Figure 1: Path-consistency extracts prefixes from earlier generated inference paths to guide the inference of subsequent branches.</p>
<p>Figure3: An "extract-and-sample" inference process of the proposed path-consistency.It seeks the "optimal path" in the form of the "prefix", thereby progressively reducing the number of generated tokens and significantly shortening inference latency.</p>
<p>We evaluated the performance and efficiency improvement of path-consistency across the following types of tasks: (1) Arithmetic Reasoning, including GSM8K Cobbe et al. (2021), SVAMP Patel et al. (2021), ASDiv Miao et al. (2020), and MultiArith Roy &amp; Roth (2015); (2) Commonsense Reasoning, including StrategyQA Geva et al. (2021), Ruin Names, and Salient Translation; (3) Symbolic Reasoning, including Boolean Expressions, Tracking Shuffled Objects, as well as Logical Deduction Srivastava et al. (2023).Model: We used the open-source model Llama3-8B Dubey et al. (2024) as our backbone model.During input, we employed prompts similar to those used in CoT Wei et al. (2022).</p>
<p>Figure 4 :
4
Figure 4: Speedup of inference.</p>
<p>Figure 5 :
5
Figure 5: The change in the proportion of tokens generated by path-consistency on correct or incorrect paths.</p>
<p>Figure 6 :
6
Figure6: The distribution of the majority element's confidence across all samples when self-consistency reaches the 10 th branch.</p>
<p>Figure 7 :
7
Figure 7: The distribution of confidence for the final answers across all samples on GSM8K.</p>
<p>Q:</p>
<p>If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: There are 3 cars in the parking lot already. 2 more arrive.Now there are 3 + 2 = 5 cars.The answer is 5. Q: Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?A: Leah had 32 chocolates and Leah's sister had 42.That means there were originally 32 + 42 = 74 chocolates.35 have been eaten.So in total they still have 74 -35 = 39 chocolates.The answer is 39.Q: Jason had 20 lollipops.He gave Denny some lollipops.Now Jason has 12 lollipops.How many lollipops did Jason give to Denny?A: Jason had 20 lollipops.Since he only has 12 now, he must have given the rest to Denny.The number of lollipops he has given to Denny must have been 20 -12 = 8 lollipops.The answer is 8.</p>
<p>Table 2 :
2
Comparison of inference latency speedup and average token consumption reduction under different prefix levels, demonstrating the effect of path-consistency on GSM8K.
Cthreshold = 0Cthreshold = 0.7Cthreshold = 0.8Cthreshold = 0.9Speedup (%)Decrease (%)6SHHGXSMethodAccuarcy (%) Total Level-1 Level-2 Level-3 Tokens (#) Total Level-1 Level-2 Level-3SC64.196.60PC (C=0)66.6 (+2.5)+28.9+18.8+42.6+78.973.37-24.0-16.9-32.7-47.4PC (C=0.7) 67.1 (+3.0)+18.0+10.0+25.3+46.581.09-16.1-9.4-21.3-33.8PC (C=0.8) 67.8 (+3.7)+17.4+9.8+25.0+43.980.91-16.2-9.8-21.8-33.3PC (C=0.9) 66.6 (+2.5)+10.2+4.2+13.3+27.087.01-9.9-4.2-12.7-23.0&amp;R7SURPSWLQJ /HYHO/HYHO/HYHO</p>
<p>Table 3
3, rows 8-10,compares the performance of different meth-ods on the symbolic reasoning datasets. Path-consistency performs exceptionally well interms of task accuracy, especially in the Track-ing Shuffled Objects task, where it achievesthe highest improvement of 5.2% among alldatasets. Additionally, compared to the base-line, it delivers approximately a 20% speedupin inference latency across tasks such asBoolean Expressions, Tracking Shuffled Ob-jects, and Logical Deduction.</p>
<p>Table 3 :
3
The evaluation results of path-consistency across various tasks, including (i) Arithmetic
reasoning: GSM8K, SVAMP, ASDiv, MultiArith (MA); (ii) Commonsense reasoning: StrategyQA(SQA), RuinNames (RN), SalientTranslation (ST); (iii) Symbolic reasoning: BooleanExpression(BE), TrackingShuffledObjects (TSO), LogicalDeduction (LD). C=0 indicates that no confidencethreshold is applied; the prefix corresponding to the majority element is extracted directly.Dataset Metrics (%) SCPC (C=0) PC (C=0.7) PC (C=0.8) PC (C=0.9) Dataset Metrics (%) SCPC (C=0) PC (C=0.7) PC (C=0.8) PC (C=0.9)Acc.64.1 66.6 (+2.6) 67.1 (+3.1) 67.8 (+3.8) 66.6 (+2.6)Acc.76.0 71.2 (-4.8)74.4 (-1.6)72 (-4.0)71.6 (-4.4)GSM8KSpeedup-+28.9+18.0+17.4+10.2RNSpeedup-+10.4+7.8+7.4+4.7Decrease--24.0-16.1-16.2-9.9Decrease--9.8-7.2-6.6-4.7Acc.79.3 78.8 (-0.5)78.5 (-0.8)79.2 (-0.1) 79.9 (+0.6)Acc.54.8 58.4 (+3.6) 57.2 (+2.4) 56.4 (+1.6) 55.2 (+0.4)SVAMPSpeedup-+48.3+36.6+35.7+24.8STSpeedup-+30.5+19.6+17.3+10.2Decrease--36.6-29.4-29.7-22.3Decrease--25.8-18.3-16.4-10.6Acc.81.0 80.1 (-0.9) 81.2 (+0.2)80 (-1.0)80.6 (-0.4)Acc.88.490 (+1.6)89.6 (+1.2)88.8(+0.4)89.6 (+1.2)ASDivSpeedup-+42.0+32.1+31.1+24.9BESpeedup-+26.6+24.3+25.2+22.0Decrease--32.7-27.2-27.1-22.6Decrease--21.3-19.8-20.3-18.3Acc.96.7 97.8 (+1.1) 97.2 (+0.5) 98.9 (+2.2) 98.3 (+1.6)Acc.54.058 (+4.0)56 (+2.0)59.2 (+5.2) 58.4 (+4.4)MASpeedup-+42.7+40.8+40.5+33.8TSOSpeedup-+18.4+12.9+11.3+6.8Decrease--34.8-33.3-33.1-27.2Decrease--16.7-12.4-10.8-6.8Acc.71.4 70.8 (-0.6)70.6 (-0.8)71.3 (-0.1)71.1 (-0.3)Acc.78.8 76.8 (-2,0) 78.8 (+0.0) 77.2 (-1.6)76 (-2.8)SQASpeedup-+34.2+29.4+24.3+16.4LDSpeedup-+31.6+23.1+30.1+19.1Decrease--27.8-24.0-22.1-16.0Decrease--26.1-20.5-24.8-17.5</p>
<p>Table 4 :
4
Impacts of maximum path levels of pathconsistency on GSM8K.
MethodLevel Accuracy (%) Sqeedup (%) Decrease (%)PC (C=0.7)367.1+18.0-16.1466.3+20.5-20.0PC (C=0.8)367.8+17.4-16.2467.8+18.0-17.8</p>
<p>Table 5 :
5
The performance of various inference techniques on GSM8K, Svamp and StrategyQA.
GSM8KSvampStratrgyQAMethodAccuracy (%) Tokens (# per problem) Accuracy (%) Tokens (# per problem) Accuracy (%) Tokens (# per problem)SC Wang et al. (2023)68.43,86080.42,32171.71,805AC Aggarwal et al. (2023)67.22,248 (-41.7%)79.7935 (-59.7%)71.2706 (-60.9%)ESC Li et al. (2024)67.22,710 (-29.8%)79.21,154 (-50.3%)70.8782 (-56.7%)PC67.81,630 (-57.8%)79.2816 (-64.8%)71.3700 (-61.2%)</p>
<p>Table 6 :
6
Evaluation results of different inference techniques on DeepSeek-V3.
GSM8KSvampStratrgyQAMethodAccuracy (%) Tokens (# per problem) Accuracy (%) Tokens (# per problem) Accuracy (%) Tokens (# per problem)PC91.878694.726384.5230AC Aggarwal et al. (2023)91.6828 (+5.34%)94.7363 (+38.0%)84.5326 (+41.7%)ESC Li et al. (2024)91.6868 (+10.4%)94.7358 (+36.1%)84.2315 (+36.9%)5.4 DISCUSSION
Liu et al. (2024) SCALABILITYPath-consistency doesn't alter the model generation process.It is a lightweight, model-agnostic approach that requires no additional computation.As demonstrated in table 6, it maintains strong robustness even when applied to another open-source model, DeepSeek-V3Liu et al. (2024).</p>
<p>Table 7 :
7
Evaluation results of different inference techniques on Llama-3.2-1B-Instruct.
GSM8KSvampStratrgyQAMethodAccuracy (%) Tokens (# per problem) Accuracy (%) Tokens (# per problem) Accuracy (%) Tokens (# per problem)PC47.3143062.071163.1799AC Aggarwal et al. (2023)48.1274861.180063.91401ESC Li et al. (2024)47.9232461.992463.71136A.1.2 ROBUSTNESS TO CONFIDENCE THRESHOLDAs shown in Figure</p>
<p>Table 8 :
8
Few-shot exemplars for arithmetic reasoning.
10prompt_plus_prefix = prompt + prefix11start_time = time.time()12generation =self.model.completion_function(prompt_plus_prefix, ** kwargs)19if (branch_id + 1) % (self.max_branch / (self.max_level + 1)) ==0:prefix_list, prefix_level = get_prefix(answers,self.confidence_thres, reasoning, prefix_list,prefix_level)21final_answer = integrate_answer(answers)22info = {'answer' : final_answer, 'answers' : answers, 'latency' :times, 'generations' : generations}return infoA.2.4 WRAP THE MODELAbstract base class for completion models defines the interface for completion models. All sub-classes must implement the "completion function" method. Concrete implementation of Comple-tionModel using the Llama model uses a text generation model (like Llama) to generate completionsbased on a given prompt.Listing 4: Inference1 class CompletionModel(ABC):2@abstractmethod3def completion_function(self, prompt: str) -&gt; str:4pass56 class LlamaModel(CompletionModel):7def <strong>init</strong>(self, generator):1213results = self.generator.text_completion(14prompt_list,14
13 end_time = time.time()14 times.append(end_time-start_time) 15 reasoning.append(prefix+ generation) 16 answer = extract_answer(generation, self.ans_type)answers.append(answer)18 generations.append(generation)8 self.generator= generator 9 10 def completion_function(self, prompt: str, ** kwargs) -&gt; str: 11 prompt_list = [prompt]</p>
<p>Table 9 :
9
Few-shot exemplars for StrategyQA.</p>
<p>The proposed path-consistency allows a flexible choice of confidence assessment metrics (i.e., stopping criteria), based on intended objective and requirements. The confidence assessment metric used in the evaluation is the beta confidence criteriaAggarwal et al. (2023).
Q: Which of the following is a humorous edit of this artist or movie name: 'whitesnake'?Options: (A) whitesnape, (B) whitesnapke, (C) whitesnuake, (D) mwhitesnake A: The original name is "whitesnake".This is the name of an old English hard rock band.It is a compound word, formed by the words "white" and "snake".(A) "whitesnape": It is formed by the combination of "white" and "snake"; therefore, "snake" has been changed to "snape".Snape makes a reference to the fictional character Severus Snape in the Harry Potter series, so (A) is indeed a meaningful and funny edit.(B) "whitesnapke": It is formed by the combination of "white" and "snapke", but "snapke" is not an actual word; therefore, "whitesnapke" is not humorous.(C) "whitesnuake": It is formed by the combination of "white" and "snuake", but "snuake" is not an actual word; therefore, "whitesnuake" is not humorous.(D) "mwhitesnake": It is formed by the combination of "m", "white", and "snake", but the prefix "-m "seems arbitrary; therefore, "mwhitesnake" is not meaningful or humorous.Above the above, the only humorous edit is(A).So the answer is (A).Q: Which of the following is a humorous edit of this artist or movie name: 'one of our dinosaurs is missing'?Options: (A) ofne of our dinosaurs is missing, (B) one af our dinosaurs is missing, (C) one of our dinosaurs is pissing, (D) one of our dinosaur is missing A: The original name is "one of our dinosaurs is missing".This is the name of an old British movie.(A) "ofne of our dinosaurs is missing": Here "one of" is changed to "ofne", but the word "ofne" is not an actual word.(B) "one af our dinosaurs is missing": Here the word "of" is changed to "af", but the word "af" is not an actual word.(C) "one of our dinosaurs is pissing": Here the word "missing" is changed to "pissing", and "one of our dinosaurs is pissing" is indeed a very whimsical and mischievous edit.This change truly ruins the original title of the movie.(D) "one of our dinosaur is missing": Here the word "dinosaurs" is changed to "dinosaur", but "dinosaur" is singular but should be plural in the title; this change therefore feels arbitrary and not humorous.Above the above, the only humorous edit is (C).So the answer is (C).Q: Which of the following is a humorous edit of this artist or movie name: 'counting crows'?Options: (A) countingy crows, (B) counting cows, (C) courting crows, (D) coutnting crows A: The original name is "counting crows".This is the name of an American rock band.Historically, the band name comes from the British nursery rhyme "One for Sorrow", which is about counting of magpies.(A) "countingy crows": Here the word "counting" is changed to "countingy", but the word "countingy" is not an actual word.(B) "counting cows": Here the word "crows" is changed to "cows", and this is indeed a playful and meaningful edit that ruins the original name of the band.(C) "courting crows": Here the word "counting" is changed to "courting", and "courting" is an actual word; however, "courting crows" does not sound as humorous as "counting cows".(D) "coutnting crows": Here the word "counting" is changed to "coutnting", but the word "coutnting" is not an actual word.Above the above, the only humorous edit is(B).So the answer is (B).Table11: Few-shot exemplars for SalientTranslation.Q:The following translations from German to English contain a particular error.That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity.Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed.Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed.Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms.Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations.Dropped Content: A significant clause in the translation is removed.Please identify that error.Source: In der Liste der Baudenkmale in Lenzen (Elbe) sind alle Baudenkmale der brandenburgischen Stadt Lenzen (Elbe) und ihrer Ortsteile aufgelistet.Translation: In the list of architectural monuments in Lenzen all architectural monuments of the Brandenburg city of Lenzen and its districts are listed.The translation contains an error pertaining to: Options: (A) Modifiers or Adjectives, (B) Numerical Values, (C) Negation or Antonyms, (D) Named Entities, (E) Dropped Content, (F) Facts A: Let's think step by step.We solve this question by first translating the source sentence to English and then by comparing our translation with the provided translation.According to Google Translate, the correct translation of the source sentence from German to English is "The list of monuments in Lenzen (Elbe) includes all the monuments in the Brandenburg town of Lenzen (Elbe) and its districts."On the other hand, the provided translation is "In the list of architectural monuments in Lenzen all architectural monuments of the Brandenburg city of Lenzen and its districts are listed."Note that Lenzen (Elbe) is changed to Lenzen in the original translation; so, there is a named entity error.Because an entity in the original source sentence is changed to a different entity in the translation, the translation contains an error pertaining to Named Entities.So the answer is (D).Q:The following translations from German to English contain a particular error.That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity.Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed.Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed.Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms.Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations.Dropped Content: A significant clause in the translation is removed.Please identify that error.Source: Auf dieser Seite sind die Baudenkmaler der oberbayerischen Groben Kreisstadt Landsberg am Lech zusammengestellt.Translation: On this page are compiled the architectural monuments of the town of Landsberg am Lech.The translation contains an error pertaining to: Options: (A) Modifiers or Adjectives, (B) Numerical Values, (C) Negation or Antonyms, (D) Named Entities, (E) Dropped Content, (F) Facts A: Let's think step by step.We solve this question by first translating the source sentence to English and then by comparing our translation with the provided translation.According to Google Translate, the correct translation of the source sentence from German to English is "The monuments of the Upper Bavarian district town of Landsberg am Lech are compiled on this page."On the other hand, the provided translation is "On this page are compiled the architectural monuments of the town of Landsberg am Lech."Note that an important detail about the location of Landsberg am Lech is omitted in the original translation: The translation should have said "Upper Bavarian district town of Landsberg am Lech".Because a significant clause in the translation was removed, the translation contains an error pertaining to Dropped Content.So the answer is (E).Q:The following translations from German to English contain a particular error.That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity.Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed.Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed.Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms.Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations.Dropped Content: A significant clause in the translation is removed.Please identify that error.Source: Leba ist eine Kleinstadt und ein Badeort im Powiat Leborski der polnischen Woiwodschaft Pommern.Translation: Eba is not a small town and seaside resort in the Powiat Leborski county of the Pomeranian Voivodeship of Poland.The translation contains an error pertaining to: Options: (A) Modifiers or Adjectives, (B) Numerical Values, (C) Negation or Antonyms, (D) Named Entities, (E) Dropped Content, (F) Facts A: Let's think step by step.We solve this question by first translating the source sentence to English and then by comparing our translation with the provided translation.According to Google Translate, the correct translation of the source sentence from German to English is "Łeba is a small town and seaside resort in the Powiat Leborski of the Polish Pomeranian Voivodeship."On the other hand, the provided translation is "Leba is not a small town and seaside resort in the Powiat Leborski county of the Pomeranian Voivodeship of Poland."Note that the provided sentence says, "Leba is not a small town ..." However, the translation should have been "Leba is a small town ..." Because a negation is introduced at the beginning of the sentence and has fundamentally changed the meaning of the original source, the translation contains an error pertaining to Negation or Antonyms.So the answer is (C).
Let's sample step by step: Adaptiveconsistency for efficient reasoning and coding with llms. Pranjal Aggarwal, Aman Madaan, Yiming Yang, arXiv:2305.118602023arXiv preprint</p>
<p>Giving bert a calculator: Finding operations and arguments with reading comprehension. Daniel Andor, Luheng He, Kenton Lee, Emily Pitler, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Specializing smaller language models towards multi-step reasoning. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot, International Conference on Machine Learning. PMLR2023</p>
<p>Injecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Long text generation via adversarial training with leaked information. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>Distilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, stat. 105092015</p>
<p>Large language models are reasoning teachers. Namgyu Ho, Laura Schmid, Seyoung Yun, 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics2023</p>
<p>Symbolic chain-of-thought distillation: Small models can also "think" step-by-step. Liunian Harold, Li , Jack Hessel, Youngjae Yu, Xiang Ren, Kai Wei Chang, Yejin Choi, 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics2023</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336On the advance of making language models better reasoners. 2022arXiv preprint</p>
<p>Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, Kan Li, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsShort Papers20232</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Refiner: Reasoning feedback on intermediate representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European Chapterthe Association for Computational Linguistics20241</p>
<p>Measuring and improving bert's mathematical abilities by predicting the order of reasoning. Piotr Piekos, Mateusz Malinowski, Henryk Michalewski, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingShort Papers20212</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.114462021arXiv preprint</p>
<p>Numnet: Machine reading comprehension with numerical reasoning. Yankai Qiu Ran, Peng Lin, Jie Li, Zhiyuan Zhou, Liu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.11366202329arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Google's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, Mohammad Quoc V Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.081442016arXiv preprint</p>
<p>Self-evaluation guided beam search for reasoning. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, Michael Xie, Advances in Neural Information Processing Systems. 202436</p>
<p>Coder reviewer reranking for code generation. Tianyi Zhang, Tao Yu, B Tatsunori, Mike Hashimoto, Wen-Tau Lewis, Daniel Yih, Sida I Fried, Wang, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning. Tinghui Zhu, Kai Zhang, Jian Xie, Yu Su, arXiv:2401.176862024Few-shot exemplars for BooleanExpressions12arXiv preprint</p>
<p>Let's evaluate A: A = True and False = False. Let's evaluate B: B = not True and True = not (True and True) = not (True) = False. Plugging in A and B, we get: Z = A and B = False and False = False. So the answer is False. Q: not not ( not ( False ) ) is A: Let's think step by step. is A: Let's think step by step. Plugging in A, we get: Z = not ( ( A ) ) = not ( ( True ) ) = not True = False. So the answer is False. Q: True and False and not True and True is A: Let's think step by step. Remember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is "not", "and", "or", respectively. We first simplify this expression "Z" as follows: "Z = not not ( not ( False ) ) = not not ( A )" where "A = not ( False )</p>
<p>Plugging in A, we get: Z = not not ( A ) = not not (True) = not not False = True. So the answer is True. Table. 13Few-shot exemplars for TrackingShuffledObjectLet's evaluate A: A = not ( False ) = not False = True</p>
<p>At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball. As the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. Bob Alice, Claire , At the end of the game, Bob has the Options: (A) yellow ball, (B) blue ball, (C) pink ball A: Let's think step by step</p>
<p>At the start: Alice: yellow, Bob: blue, Claire: pink. </p>
<p>Claire and Alice swap balls: Alice: pink, Bob: blue, Claire: yellow. </p>
<p>Alice and Bob swap balls: Alice: blue, Bob: pink, Claire: yellow. </p>
<p>At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball. As the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. Bob Alice, Claire , At the end of the game, Alice has the Options: (A) white ball, (B) purple ball, (C) pink ball A: Let's think step by step</p>
<p>At the start: Alice: white, Bob: purple, Claire: pink. </p>
<p>Bob and Claire swap balls: Alice: purple, Bob: pink, Claire: white. </p>
<p>At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick. Throughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. AliceBoband Claire are dancers at a square dance. At the end of the dance, Alice is dancing with Options: (A) Lola, (B) Rodrigo, (C) Patrick A: Let's think step by step</p>
<p>At the start: Alice: Lola. Claire : Rodrigo, Patrick, Bob</p>
<p>Alice and Bob switch partners: Alice: Rodrigo. Lola, Claire: PatrickBob</p>
<p>Claire and Bob switch partners: Alice: Rodrigo. LolaBob; Claire</p>
<p>At the end of the dance, Alice is dancing with Patrick. So the answer is (C). Alice Bob, Partners, Table. Rodrigo, Claire: Lola14Few-shot exemplars for LogicalDeduction</p>
<p>The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy. Options: (A) Amy finished last, (B) Eli finished last. C) Eve finished last A: Let's think step by step</p>
<p>Eve finished above Amy: "(above) ? Eve ? Amy ? (below). </p>
<p>Eli Finished Below Amy, Amy ? Eli ? (below). </p>
<p>) and (2) we get the following ordering: "(above) Eve Amy Eli (below). Combining, According to this ordering, the person who finished last (the one at the bottom of this list) is Eli. Eli finished last. So the answer is (B)</p>
<p>On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost. Options: (A) The white book is the leftmost, (B) The green book is the leftmost. The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. C) The orange book is the leftmost A: Let's think step by step</p>
<p>The green book is to the right of the white book. left) ? white ? green ? (right)</p>
<p>The orange book is the rightmost: "(left) ? white ? green orange (right). </p>
<p>) and (2) we get the following ordering: "(left) white green orange (right). Combining, According to this ordering, the leftmost book is the white book. The white book is the leftmost. So the answer is (A)</p>
<p>On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left. Options: (A) The red book is the leftmost, (B) The gray book is the leftmost. The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. C) The white book is the leftmost A: Let's think step by step</p>
<p>The red book is the second from the left: "(left) ?. white red gray ? (right)</p>
<p>) and (2) we get the following ordering: "(left) white red gray (right). Combining, According to this ordering, the leftmost book is the white book. The white book is the leftmost. So the answer is (C)</p>            </div>
        </div>

    </div>
</body>
</html>
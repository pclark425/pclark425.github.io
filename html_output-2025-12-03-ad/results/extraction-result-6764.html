<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6764 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6764</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6764</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-f6417c02f190016f7b1d381b0e5947816c378182</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f6417c02f190016f7b1d381b0e5947816c378182" target="_blank">Can Github issues be solved with Tree Of Thoughts?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The Tree of Thoughts framework alone is not enough to give LLMs the critical reasoning capabilities to outperform existing methods, and key areas for improvement such as deepening the thought process and introducing agentic capabilities are identified.</p>
                <p><strong>Paper Abstract:</strong> While there have been extensive studies in code generation by large language models (LLM), where benchmarks like HumanEval have been surpassed with an impressive 96.3% success rate, these benchmarks predominantly judge a model's performance on basic function-level code generation and lack the critical thinking and concept of scope required of real-world scenarios such as solving GitHub issues. This research introduces the application of the Tree of Thoughts (ToT) language model reasoning framework for enhancing the decision-making and problem-solving abilities of LLMs for this complex task. Compared to traditional input-output (IO) prompting and Retrieval Augmented Generation (RAG) techniques, ToT is designed to improve performance by facilitating a structured exploration of multiple reasoning trajectories and enabling self-assessment of potential solutions. We experimentally deploy ToT in tackling a Github issue contained within an instance of the SWE-bench. However, our results reveal that the ToT framework alone is not enough to give LLMs the critical reasoning capabilities to outperform existing methods. In this paper we analyze the potential causes of these shortcomings and identify key areas for improvement such as deepening the thought process and introducing agentic capabilities. The insights of this research are aimed at informing future directions for refining the application of ToT and better harnessing the potential of LLMs in real-world problem-solving scenarios.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6764",
    "paper_id": "paper-f6417c02f190016f7b1d381b0e5947816c378182",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003319,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can Github issues be solved with Tree Of Thoughts?</h1>
<p>Ricardo La Rosa<br>ricardo@larosa.dev</p>
<p>Corey Hulse<br>coreyohulse@gmail.com</p>
<p>Bangdi Liu<br>buddytt0915@gmail.com</p>
<h4>Abstract</h4>
<p>While there have been extensive studies in code generation by large language models (LLM), where benchmarks like HumanEval(Chen et al., 2021) have been surpassed with an impressive $96.3 \%$ success rate, these benchmarks predominantly judge a model's performance on basic function-level code generation and lack the critical thinking and concept of scope required of real-world scenarios such as solving GitHub issues. This research introduces the application of the Tree of Thoughts (ToT) (Yao et al., 2023b) language model reasoning framework for enhancing the decision-making and problem-solving abilities of LLMs for this complex task. Compared to traditional input-output (IO) prompting and Retrieval Augmented Generation (RAG) techniques, ToT is designed to improve performance by facilitating a structured exploration of multiple reasoning trajectories and enabling self-assessment of potential solutions. We experimentally deploy ToT in tackling a Github issue contained within an instance of the SWE-bench(Jimenez et al., 2024a). However, our results reveal that the ToT framework alone is not enough to give LLMs the critical reasoning capabilities to outperform existing methods. In this paper we analyze the potential causes of these shortcomings and identify key areas for improvement such as deepening the thought process and introducing agentic( $\mathrm{Ng}, 2024$ ) capabilities. The insights of this research are aimed at informing future directions for refining the application of ToT and better harnessing the potential of LLMs in realworld problem-solving scenarios.</p>
<h2>1 Introduction</h2>
<p>Tree of Thoughts (ToT) (Yao et al., 2023b) is a language model reasoning framework designed to enhance the autonomy and intelligence of language models (LMs) in decision-making and problemsolving tasks. ToT managed to outperform InputOutput prompting (IO), Chain of Thought (CoT)
(Wei et al., 2022), and Self Consistency with CoT (CoT-SC) in several reasoning based tasks such as Game of 24, Crosswords and Creative Writing. Despite the promising results of ToT on these basic tasks, there is an absence of studies that apply ToT towards more complex tasks that more closely model the real-world. Our research aims to put this framework to test in one of the most challenging software engineering tasks for large language models: resolving GitHub issues. This task requires an overall sense of scope and understanding of the repository when making changes, which requires stronger critical reasoning skills than previous basic code generation tasks. We anticipate that the ToT method will outperform both IO prompting and Retrieval Augmented Generation (RAG) techniques in this task. This expectation is based on ToT's ability to instill a stronger ability in LLMS for decision-making, evaluating multiple reasoning paths and self-assessing choices to determine the subsequent course of action.</p>
<h2>2 Related Work</h2>
<p>Prior work on solving Github issues has been done by the Princeton NLP team as part of SWE-Bench(Jimenez et al., 2024a). They released two fine-tuned models, SWE-Llama 7B and SWE-Llama 13B based on CodeLlama(Rozière et al., 2023) with Retrieval Augmented Generation (RAG). Further work then was done with the introduction of SWE-agent(Yang et al., 2024) which is a large language model-based agent system that operates within an Agent-Computer Interface (ACI). Another recent work is LLM-Based Multi-Agent Framework for GitHub Issue ReSolution (MAGIS) (Tao et al., 2024) which introduces Multi-Agency whereby leveraging the collaboration of various agents with distinct roles in the planning and coding process to resolve GitHub issues.</p>
<h2>3 Data</h2>
<h3>3.1 SWE-bench</h3>
<p>We utilized the dataset provided by SWEBench(Jimenez et al., 2024a) as the basis for the experiments. SWE-bench is a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a code-base and an issue, a language model is tasked with generating a code patch that resolves the described problem. With SWE-Bench, you can:</p>
<ul>
<li>Train or fine-tune a model with their preprocessed datasets.</li>
<li>Run inference on existing models.</li>
<li>Evaluate a model against the benchmark and determine the correctness of a solution proposed by the model.</li>
</ul>
<p>This dataset is composed of a wide variety of tasks, such as filing a bug report or making a feature request, that the model will be charged with completing. The main similarity between these tasks is that they all require the model to generate a git patch to an existing code-base based on the problem statement of the Github issue. The revised code-base is then evaluated using the internal testing framework of the repository. If the proposed patch passes these tests then the model's proposed changes are considered successful and the task is counted as passed.</p>
<h3>3.2 SWE-bench Lite</h3>
<p>In order to reduce costs we used the SWEbench_Lite(Jimenez et al., 2024b) dataset which is a canonical subset of SWE-bench that has been curated to make evaluation less costly. Instances from the original dataset that match the following criteria are not considered:</p>
<ol>
<li>Include images, external links, references to specific commits, and references to other pull requests</li>
<li>Contain problem statements with fewer than 40 words</li>
<li>Edit more than one file</li>
<li>Have a gold patch with more than three edit hunks</li>
<li>Create or delete files</li>
<li>Contain tests with error messages checks</li>
</ol>
<p>After filtering out the instances who violated the above standards, the result is a smaller dataset of 23 instances in the dev split and 300 instances in the test split.</p>
<h3>3.3 Motivation</h3>
<p>Traditional benchmarks in Natural Language Processing (NLP) often focus on relatively short input and output sequences that are not representative of real-world tasks.</p>
<p>Table 1: HumanEval Leaderboard</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Success Rate(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AgentCoder(GPT-4) (Huang et al., 2024)</td>
<td style="text-align: center;">96.3</td>
</tr>
<tr>
<td style="text-align: left;">LDB + Reflexion(GPT-3.5) (Zhong et al., 2024)</td>
<td style="text-align: center;">95.1</td>
</tr>
<tr>
<td style="text-align: left;">Language Agent Search Tree(GPT-4) (Zhou et al., 2023)</td>
<td style="text-align: center;">94.4</td>
</tr>
<tr>
<td style="text-align: left;">L2MAC(GPT-4) (Holt et al., 2024)</td>
<td style="text-align: center;">90.2</td>
</tr>
</tbody>
</table>
<p>As shown in table 1 LLMs demonstrate remarkable performance on the HumanEval(Chen et al., 2021) benchmark. However, this benchmark exhibits several notable weaknesses: scope limited to function-level code generation, lack of diversity by focusing mainly on algorithmic tasks, and a lack of contextual and environmental interaction. In contrast, we considered that SWE-bench, emphasizes tasks that adequately model real-world scenarios where the interdependencies of the code base as a whole must be take into account when generating new patches, and the testing framework is able to use a built in testing framework to evaluate if the model's code correctly fits into the existing code base.</p>
<p>Table 2: SWE-bench Lite Leaderboard</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Success Rate(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SWE-agent + GPT 4</td>
<td style="text-align: center;">17.00</td>
</tr>
<tr>
<td style="text-align: left;">SWE-agent + Claude 3 Opus</td>
<td style="text-align: center;">11.67</td>
</tr>
<tr>
<td style="text-align: left;">RAG + Claude 3 Opus</td>
<td style="text-align: center;">4.00</td>
</tr>
<tr>
<td style="text-align: left;">RAG + GPT4</td>
<td style="text-align: center;">2.67</td>
</tr>
<tr>
<td style="text-align: left;">RAG + Claude 2</td>
<td style="text-align: center;">2.00</td>
</tr>
<tr>
<td style="text-align: left;">RAG + SWE-Llama 13B</td>
<td style="text-align: center;">1.67</td>
</tr>
<tr>
<td style="text-align: left;">RAG + SWE-Llama 7B</td>
<td style="text-align: center;">1.33</td>
</tr>
<tr>
<td style="text-align: left;">RAG + GPT 3.5</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">GPT 4</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT 3.5</td>
<td style="text-align: center;">0.00</td>
</tr>
</tbody>
</table>
<p>Table 2 paints a different picture. Even with Retrieval Augmented Generation, performance sees limited improvement due to the difficulties that</p>
<p>LLMs face when handling long context inputs, notably in tasks like resolving GitHub issues at the repository level, where using large portions of the repository as input is impractical.</p>
<h2>4 Models</h2>
<p>We utilized three open-source models for this research. The first model was CodeLlama 34B. (Jimenez et al., 2024a) highlighted that variants of CodeLlama were not capable of following detailed instructions in order to make repository-wide code edits, and typically resorted to outputting placeholder responses or unrelated code. To address this issue, they performed supervised fine-tuning on the 7 billion-parameter and 13 billion-parameter variants. The resulting models were shown to be highly successful at maintaining specialized repositories and could be run on consumer hardware to resolve GitHub issues. Based on these observations, we opted for the 34 billion parameter version of CodeLlama, which had been quantized to 4-bit precision and fine-tuned using a select portion of the SWE-bench dataset.</p>
<p>Table 3: Models</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Patch generation <br> strategy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CodeLlama 34B</td>
<td style="text-align: center;">Supervised fine-tuning</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8x7B</td>
<td style="text-align: center;">In-context learning</td>
</tr>
<tr>
<td style="text-align: left;">Llama2 70B</td>
<td style="text-align: center;">In-context learning</td>
</tr>
<tr>
<td style="text-align: left;">Llama3 70B Instruct</td>
<td style="text-align: center;">In-context learning</td>
</tr>
</tbody>
</table>
<p>Table 3 presents the models employed in this research and their respective patch generation strategies. We broadened our approach by adding three larger models: Mixtral-8x7B (Jiang et al., 2024), which comprises eight 7-billion parameter models with a Sparsely-Gated Mixture-of-Experts layer (MoE) (Shazeer et al., 2017), and the 70-billion parameter models Llama2 70B and Llama3 70B Instruct.
Considering their in-context learning capabilities, we hypothesized that these larger models would correctly generate patches in the unified diff format when presented with few-shot examples.</p>
<h2>5 Methods</h2>
<h3>5.1 Baselines</h3>
<p>We use a standard input-output (IO) prompt with five-shot examples.</p>
<h3>5.2 Tree of Thoughts setup</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ToT setup with $n=5, k=5$ and $b=1$</p>
<p>As shown in figure 1 we built a ToT with depth $d=2$ with one intermediate thought step. The input is a Chain of Thought style prompt where the model is asked to generate $n$ plans and votes for the best one, then similarly generate $k$ patches based on the best plan. In order to guarantee plan diversity we increased the temperature to a determined value $t$. The last step is to rate the patches: the patch with the highest score is chosen. The breadth limit is always set to $b=1$ as a consequence the breadth-first search (BFS) only maintains the most promising state per step in a greedy approach.</p>
<p>We forked the official Tree of Thoughts Github repository(Yao et al., 2023a) and created a branch to add a new Task class called SWETask. This class was designed to solve the instances within the SWE-bench Lite dataset, based on the mentioned ToT setup.</p>
<h3>5.2.1 Prompting</h3>
<p>A zero-shot vote prompt was used to sample votes for plan selection and zero-shot score prompt is used to make patches scores. Example of prompts used are in appendix A.</p>
<h3>5.3 Metrics</h3>
<p>We used the SWE-bench metrics which is the percentage of task instances that are correctly solved by the model. In order to judge whether or not the model correctly solves an individual task we will use the following scoring method:</p>
<p>If the evaluation produces incorrect outputs for any step in the task then the entire task is treated as a failure. In order to be counted as a success, the evaluation must succeed at every step of and</p>
<p>Table 4: SWE-bench Evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Result</th>
<th style="text-align: center;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fails at any step</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Correctly completes all steps</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>pass all the associated test cases. Taking the total number of successful tasks over the number of attempted tasks will serve as our primary metric.</p>
<h3>5.3.1 Evaluation</h3>
<p>We will execute generated patches against the corresponding task instances of the benchmark to determine whether or not it resolves the associated Github issue. The SWE-bench refers to such patch generations as the prediction patch. The benchmark framework performs the following steps for testing:</p>
<ol>
<li>Installs repository at base commit according to task instructions</li>
<li>Applies test patch, prediction patch and run tests</li>
<li>Checks prediction logs to see the pass/fail status of each test</li>
</ol>
<h3>5.4 Fine-tuning</h3>
<p>We loaded the CodeLlama model with FastLanguageModel loader from the unsloth(Han and Han, 2023) library that extends Hugging Face's Parameter-Efficient Fine-Tuning (PEFT) (Mangrulkar et al., 2022) which provides several performance optimizations for training and inference.</p>
<p>Table 5: CodeLlama training parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Quantization</td>
<td style="text-align: center;">4 -bit</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$2 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">AdamW 8-bit</td>
</tr>
<tr>
<td style="text-align: left;">Warmup ratio</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">Number of epochs</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Max seq length</td>
<td style="text-align: center;">16384</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Per device train batch size</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Grad accumulation Steps</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>The trainer was setup with the parameters shown in table 5. Additionally, we applied the following techniques:</p>
<ol>
<li>Supervised training: Supervised Fine-tuning Trainer SFTTrainer from Hugging Face's Transformer Reinforcement Learning (TRL) (von Werra et al., 2020) library.</li>
<li>Quantization: We used a pre-quantized 4-bit model to reduce memory usage.</li>
<li>Lower Ranking Adaptation(LoRA): Using QLoRA(Dettmers et al., 2023) we only updated 1 to $10 \%$ of all parameters.</li>
<li>Rotary Positional Embedding(RoPE) Scaling: the of RoPE (Su et al., 2022) scaling using Kiao Ken's method(Ken, 2023) made the context window flexible.</li>
</ol>
<h3>5.5 Inference via API</h3>
<p>To mitigate the computational challenges and time constraints associated with the experiments the Groq API was utilized for both Mixtral-8x7B and Llama2 70B, achieving an impressive average throughput of 500 tokens per second; A large boost in the generation speed of patches. Looking forward, we are encouraged by the expected arrival of Language Processing Units (LPU) (Abts et al., 2022) Inference Engines which promise to significantly advance the field by facilitating the adoption of frameworks like ToT across a broad spectrum of applications.</p>
<h2>6 Results</h2>
<p>Table 6: SWE-bench Lite results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Success Rate (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">ToT</td>
</tr>
<tr>
<td style="text-align: left;">CodeLlama 34B*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 70B*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8x7B*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3 70B Instruct*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p><em>33\% and </em>50\% of dataset respectively.
We conducted the experiments using a subset of 100 instances, representing $33 \%$ of the full dataset from the SWE-bench_Lite benchmark. To test our last model Llama-3 70B Instruct, we ran a bigger subset of 150 instances representing $50 \%$ of the dataset. These sample sizes were selected to provide a representative snapshot of ToT's performance, while also considering the computational</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: SWE-bench Evaluation (Jimenez et al., 2024a)
constraints and resource requirements associated with evaluating the models on the complete dataset.</p>
<p>The three models performed as poorly as inputoutput (IO) prompting, this is shown in table 6. We observed that while all the generated git patches were syntactically correct (demonstrating the incontext learning capabilities of the LMs), none of them were able to successfully pass the benchmark.</p>
<p>Table 7: Accepted patches</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accepted Patches (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">ToT</td>
</tr>
<tr>
<td style="text-align: left;">CodeLlama 34B*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 70B*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8x7B*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3 70B Instruct*</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p><em>33\% and </em>50\% of dataset respectively.
As presented in Table 7, it is noteworthy that for Llama-3 70B Instruct, $10 \%$ of the generated patches were accepted. However, these patches subsequently failed to satisfy certain test cases within the test suite. In other words, these patches subsequently failed to meet the benchmark success criteria. In contrast, with CodeLlama 34B, Llama2 70B, and Mixtral-8x7B all the proposed patches were rejected outright, precluding any chance for testing.</p>
<h2>7 Analysis</h2>
<p>The findings indicate that Tree of Thoughts (ToT) was not effective for the specific task that was examined. This research acknowledges several limitations within the experimental setup that may have impacted the efficacy of the ToT framework. Specifically, we identify the following weaknesses:</p>
<ol>
<li>The use of a relatively shallow thought process tree, consisting of only two thought steps.</li>
</ol>
<p>This was problematic because it did not allow the complex tasks asked of the model to be decomposed into smaller, more manageable sub-tasks that individual reasoning steps could be applied to.
2. Providing only the repository name, problem statement, and git commit is insufficient for the model to fully comprehend and address the requirements of the task. However, as shown in table 2 many complex tasks cannot be accomplished through a single step or a solitary tool invocation. Even with RAG conducting similarity searches over the contents of the sizable Git repository proved to be an ineffective way of supplying the model with the necessary task-specific information. Allowing the model access to explore and examine the file contents of the associated GitHub repository could have facilitated a more informed and accurate approach.
3. The validation of the individual thought steps was conducted through a voting mechanism, rather than comparing the outputs to a defined ground truth. Incorporating a symbolic validation component, similar to the utilized by SWE-bench itself, could have provided a more robust means of evaluating the correctness of the generated patches.</p>
<h2>8 Conclusion</h2>
<p>Despite the limitations observed in the current experimental setup, we hypothesize that with a redesigned approach and addressing the identified weaknesses, the potential of the Tree of Thoughts (ToT) framework could be better realized and more effectively demonstrated. Specifically, a setup with agentic design patterns(Ng, 2024) where the large language model is:</p>
<ol>
<li>Leveraged to autonomously break down the objective into smaller sub-tasks. This allows the model to dynamically determine the optimal sequence of steps required to accomplish the resolution of the GitHub issue.</li>
<li>Provided with access to tools via function calling(Kim et al., 2024). This enables the model to independently make requests for the purpose of gathering information, taking action, or manipulating data. For example, being able to do a code search in the git repository and open sections of a file like the Agent-Computer Interface (ACI) introduced by SWE-agent.</li>
<li>Provided with code search that is project structure aware like AutoCodeRover(Zhang et al., 2024). Instead of searching over files by plain string matching, the model can search for relevant code context (functions/classes) in the syntax tree.</li>
</ol>
<p>Additional improvements are related to the patch generation, such as:</p>
<ol>
<li>Freeing the large language model from the direct responsibility of generating the code patches. Instead, leveraging the use of tools via function calling to handle the patch generation, allowing the model to focus on planning, debugging and code generation.</li>
<li>Incorporating a reliable ground truth for validating the generated patches.</li>
</ol>
<p>Finally, since the model no longer generates unified patches, fine-tuning as an optimization strategy becomes less effective.</p>
<h2>Known Project Limitations</h2>
<p>Speed and Cost: It is important to note that while ToT may enhance decision-making and problemsolving capabilities of large language models, this sophistication can result in slower processing times due to the additional computational steps involved. The evaluation of multiple reasoning paths and self-assesses choices, inherently demands more resources and compute time, including a notable rise in prompt and generation tokens. However, the trade-off for this slower speed and higher cost is a potential increase in the accuracy and relevance of the outcomes, particularly in complex tasks. Adjustments to the framework's parameters can offer
some mitigation of these issues, allowing users to make a balance between speed/cost and accuracy. Search Methods: This research leverages the use of classical search algorithms, such as BreadthFirst Search (BFS) and the current setup can be considered a form of heuristic search, akin to the A* algorithm, where the heuristic at each search node is provided by the large language model's own self-assessment of the generated thought. While this search strategy is straightforward to implement, it represents a relatively naive approach. We anticipate that the use of more advanced search algorithms, such as Monte Carlo Tree Search (MCTS), could potentially yield improved results. This expectation is informed by prior work, such as the research conducted by (Hao et al., 2023) with Reasoning viA Planning.</p>
<h2>Authorship Statement</h2>
<p>Ricardo La Rosa conceptualized the core experiments and hypothesis, conducted the majority of the experiments, analyzed the data, and wrote the core of the manuscript. Corey Hulse helped conduct validation of the experiments and oversaw final edits of deliverables. Bangdi Liu provided critical feedback, and assisted with revisions to the manuscript and previous deliverables. All authors read and approved the final version of the manuscript.</p>
<h2>Acknowledgements</h2>
<p>We express our sincere gratitude to Christopher Potts, Petra Parikova, and our course facilitator, Jonathan Gomes Selman, for their valuable contributions and support throughout the duration of Stanford's XCS224U course. We are grateful for their generous availability and support.</p>
<h2>References</h2>
<p>Dennis Abts, Garrin Kimmell, Andrew Ling, John Kim, Matt Boyd, Andrew Bitar, Sahil Parmar, Ibrahim Ahmed, Roberto DiCecco, David Han, John Thompson, Michael Bye, Jennifer Hwang, and Jeremy Fowers. 2022. A software-defined tensor streaming multiprocessor for large-scale machine learning. In Proceedings of the 49th Annual International Symposium on Computer Architecture, ISCA '22, page 567-580, New York, NY, USA. Association for Computing Machinery.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg</p>
<p>Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, and Fotios Chantzis. 2021. Evaluating large language models trained on code.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.</p>
<p>Daniel Han and Michael Han. 2023. Unsloth: 30x faster llm training.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992.</p>
<p>Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. 2024. L2mac: Large language model automatic computer for extensive code generation.</p>
<p>Dong Huang, Qingwen Bu, Jie M. Zhang, Michael Luck, and Heming Cui. 2024. Agentcoder: Multi-agentbased code generation with iterative testing and optimisation.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mixtral of experts.</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2024a. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations.</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2024b. SWE-bench Lite Dataset. Number of rows: 323, File size: 1.29 MB .</p>
<p>Kaio Ken. 2023. Things i'm learning while training superhot. https://kaiokendev.github.io/ til.</p>
<p>Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, and Amir Gholami. 2024. An llm compiler for parallel function calling.</p>
<p>Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin</p>
<p>Bossan. 2022. Peft: State-of-the-art parameterefficient fine-tuning methods. https://github. com/huggingface/peft.</p>
<p>Andrew Ng. 2024. Agentic design patterns. https://www.deeplearning.ai/ the-batch/issue-244.</p>
<p>Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code.</p>
<p>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. Roformer: Enhanced transformer with rotary position embedding.</p>
<p>Wei Tao, Yucheng Zhou, Zhang Wenqiang, and Yu Cheng. 2024. MAGIS: Llm-based multi-agent framework for github issue resolution.</p>
<p>Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. 2020. Trl: Transformer reinforcement learning. https://github.com/ huggingface/trl.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>John Yang, Carlos E. Jimenez, Alexander Wettig, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent computer interfaces enable software engineering language models.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Official repo of tree of thoughts. https://github.com/princeton-nlp/ tree-of-thought-11m.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023b. Tree of Thoughts: Deliberate problem solving with large language models.</p>
<p>Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. Autocoderover: Autonomous program improvement.</p>
<p>Lily Zhong, Zilong Wang, and Jingbo Shang. 2024. Ldb: A large language model debugger via verifying runtime execution step-by-step.</p>
<p>Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree search unifies reasoning acting and planning in language models.</p>
<h1>A Prompts</h1>
<div class="codehilite"><pre><span></span><code><span class="mf">1</span><span class="w"> </span><span class="n">plan_prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">&#39;</span><span class="s">&quot;&#39;Given the Repository url, Base commit</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="n">Problem</span><span class="w"> </span><span class="n">statement</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">github</span><span class="w"> </span><span class="n">issue</span><span class="mf">.</span><span class="w"> </span><span class="n">Please</span>
<span class="w">        </span><span class="n">write</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">solve</span><span class="w"> </span><span class="n">it</span><span class="mf">.</span>
<span class="mf">2</span><span class="err">}\</span><span class="n">mathrm</span><span class="err">{</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="kr">for</span><span class="n">mat</span><span class="p">:</span>
<span class="mf">3</span>
<span class="mf">4</span><span class="w"> </span><span class="err">\</span><span class="n">text</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="n">Plan</span><span class="p">:</span><span class="err">}</span>
<span class="mf">5</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">here</span><span class="mf">.</span>
<span class="mf">6</span>
<span class="mf">7</span><span class="w"> </span><span class="p">(</span><span class="kr">input</span><span class="p">)</span>
<span class="mf">8</span><span class="w"> </span><span class="err">&#39;&#39;&#39;</span>
</code></pre></div>

<h2>Snippet 1: Plan prompt</h2>
<div class="codehilite"><pre><span></span><code><span class="mi">1</span><span class="w"> </span><span class="n">patch_prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">&#39;&quot;&#39;</span><span class="n">Given</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Repository</span><span class="w"> </span><span class="n">url</span><span class="p">,</span><span class="w"> </span><span class="n">Base</span><span class="w"> </span><span class="n">commit</span><span class="p">,</span>
<span class="w">    </span><span class="n">Problem</span><span class="w"> </span><span class="n">statement</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">github</span><span class="w"> </span><span class="n">issue</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">plan</span><span class="p">.</span>
<span class="w">        </span><span class="n">Please</span><span class="w"> </span><span class="n">write</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="n">git</span><span class="w"> </span><span class="n">patch</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">solve</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="mi">2</span>
<span class="mi">3</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">format</span><span class="o">:</span>
<span class="mi">4</span>
<span class="mi">5</span><span class="w"> </span><span class="n">Patch</span><span class="o">:</span>
<span class="mi">6</span><span class="w"> </span><span class="err">&#39;&#39;&#39;</span><span class="n">diff</span>
<span class="mi">7</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">patch</span><span class="w"> </span><span class="n">here</span><span class="p">.</span>
<span class="mi">8</span><span class="w"> </span><span class="err">&#39;&#39;&#39;</span>
<span class="mi">9</span>
<span class="mi">10</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">patch</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">unified</span><span class="w"> </span><span class="n">diff</span><span class="w"> </span><span class="n">format</span><span class="p">.</span>
<span class="w">        </span><span class="nl">Example</span><span class="p">:</span>
<span class="mi">11</span>
<span class="mi">12</span><span class="w"> </span><span class="err">&#39;&#39;&#39;</span><span class="n">diff</span>
<span class="mi">13</span><span class="w"> </span><span class="n">diff</span><span class="w"> </span><span class="o">--</span><span class="n">git</span><span class="w"> </span><span class="n">a</span><span class="o">/</span><span class="n">file</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="n">b</span><span class="o">/</span><span class="n">file</span><span class="p">.</span><span class="n">py</span>
<span class="mi">14</span><span class="w"> </span><span class="o">---</span><span class="w"> </span><span class="n">a</span><span class="o">/</span><span class="n">file</span><span class="p">.</span><span class="n">py</span>
<span class="mi">15</span><span class="w"> </span><span class="o">+++</span><span class="w"> </span><span class="n">b</span><span class="o">/</span><span class="n">file</span><span class="p">.</span><span class="n">py</span>
<span class="mi">16</span><span class="w"> </span><span class="mi">@8</span><span class="w"> </span><span class="mi">-1</span><span class="p">,</span><span class="mi">27</span><span class="w"> </span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">35</span><span class="w"> </span><span class="p">@@</span>
<span class="mi">17</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">euclidean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="o">:</span>
<span class="mi">18</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">b</span><span class="o">:</span>
<span class="mi">19</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">b</span>
<span class="mi">20</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">a</span>
<span class="mi">21</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="o">:</span>
<span class="mi">22</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">a</span>
<span class="mi">23</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">euclidean</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="mi">24</span><span class="w"> </span><span class="err">&#39;&#39;&#39;</span>
<span class="mi">25</span>
<span class="mi">26</span><span class="w"> </span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
<span class="mi">27</span><span class="w"> </span><span class="err">&#39;&#39;&#39;</span>
</code></pre></div>

<p>Snippet 2: Patch prompt</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1</span><span class="w"> </span><span class="n">vote_prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">&#39;</span><span class="s">&quot;&#39;Given an instruction and several choices</span>
<span class="w">    </span><span class="p">,</span><span class="w"> </span><span class="n">decide</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">choice</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">most</span><span class="w"> </span><span class="n">promising</span><span class="mf">.</span><span class="w"> </span><span class="n">Analyze</span>
<span class="w">    </span><span class="n">each</span><span class="w"> </span><span class="n">choice</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">detail</span><span class="p">,</span><span class="w"> </span><span class="kr">then</span><span class="w"> </span><span class="n">conclude</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">last</span>
<span class="w">    </span><span class="n">line</span><span class="w"> </span><span class="s">&quot;The best choice is (a)&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="nb">int</span><span class="n">eger</span><span class="w"> </span><span class="n">id</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">choice</span><span class="mf">.</span><span class="err">&#39;&#39;&#39;</span>
</code></pre></div>

<p>Snippet 3: Vote prompt</p>
<div class="codehilite"><pre><span></span><code><span class="mf">2</span><span class="w"> </span><span class="n">score_prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">&#39;</span><span class="s">&quot;&#39;Analyze the following patch, then at</span>
<span class="w">    </span><span class="n">the</span><span class="w"> </span><span class="n">last</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">conclude</span><span class="w"> </span><span class="s">&quot;Therefore the correctness</span>
<span class="w">    </span><span class="n">score</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="s">&quot;, where (s) is an integer from 1 to</span>
<span class="w">    </span><span class="mf">10.</span><span class="err">&#39;&#39;&#39;</span>
</code></pre></div>

<p>Snippet 4: Score prompt</p>            </div>
        </div>

    </div>
</body>
</html>
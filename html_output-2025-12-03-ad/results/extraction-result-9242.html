<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9242 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9242</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9242</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-d7ac65d335b5d847f4f5826313a8732bc7abc7a8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d7ac65d335b5d847f4f5826313a8732bc7abc7a8" target="_blank">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work first estimates the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A", and then fits calibration parameters that cause the prediction for this input to be uniform across answers.</p>
                <p><strong>Paper Abstract:</strong> GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9242.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9242.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Example-permutation effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to training-example permutation (ordering) in few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The order (permutation) of few-shot training examples in a left-to-right prompt can dramatically change LLM accuracy; permutations can cause swings from near-chance to near-state-of-the-art performance on the same model and same examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B (primary reported), also observed on larger sizes</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 sentiment classification (4-shot analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification (Positive/Negative) using in-context few-shot prompting where training examples are placed inline before a test example.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot (4-shot) left-to-right natural-language prompt with a fixed format (e.g., 'Input: ... Sentiment: ...') and a fixed set of training examples; the experiment varies the permutation (ordering) of the four training examples across all possible permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Other permutations of the same set of training examples (i.e., identical examples but different orderings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy range: 54.3% (near-chance) up to 93.4% (near state-of-the-art) on GPT-3 2.7B when only changing permutation of 4 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Same examples with other permutations achieved the alternate values above (i.e., the numbers are the compared performances).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+39.1 percentage points (93.4% - 54.3%) depending only on permutation</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Attributed primarily to recency and majority-label biases: the model overweights answers that appear near the end of the prompt and/or answers that are more frequent in the prompt, causing permutations that place particular labels later to skew predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Selected ten different sets of 4 SST-2 training examples; for each set evaluated accuracy for all possible permutations. Left-to-right autoregressive generation; classification by probability of label tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9242.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9242.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-format effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to prompt format (template / wording / layout)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The specific template or wording used to present the task to a left-to-right LM (question/answer style, conversational style, webpage-like, different label tokens, etc.) substantially affects few-shot performance and exhibits high variance across prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B (primary reported), observed across sizes including 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 sentiment classification (format sweep)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification where the same training examples are embedded in different prompt templates (15 manually designed formats including QA, conversational, Web-like, label variations).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot (1–4 shots in experiments) using multiple hand-designed prompt formats (15 formats for SST-2); formats vary in label surface forms, prompt wording, and layout.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Other prompt templates from the set of 15 (examples listed in Table 7) — e.g., 'Answer:' vs 'Sentiment:' label names, 'good/bad' vs 'positive/negative', QA-style vs declarative style.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported high mean differences and large variance across formats (visualized in Figure 3 and Figure 7); some formats substantially outperform others on average (exact per-format numbers are reported in the paper's figures/tables for the 15 formats).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Differences arise because formats change the contextual signals that interact with the LM's intrinsic biases (e.g., label-token commonness and recency), changing output distribution and confidence. Label token choice and phrasing can favor tokens common in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Manually designed 15 prompt formats for SST-2 (Table 7); experiments computed mean and std accuracy across formats for fixed sets of training examples; also performed analogous format paraphrase experiments for LAMA relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9242.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9242.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority-label bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority label bias in in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Language models tend to favor labels that occur more frequently in the prompt (i.e., the majority class in the provided examples), leading to degraded accuracy when the prompt's class distribution is unrepresentative of the task distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B (reported), observed across sizes</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text classification and generation (SST-2, LAMA examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where label tokens or training-answer tokens appear in the prompt; the model's prediction distribution is analyzed relative to prompt label frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompts containing multiple labeled examples; experiments vary class balance among training examples (e.g., unbalanced vs balanced prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Balanced vs unbalanced training-example label distributions in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example statistics: on 4-shot LAMA with GPT-3 2.7B, 50.2% of model predictions repeat one of the four training answers (correct repeat rate 24.7%), indicating over-representation of training answers in outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The LM's outputs are shifted toward labels that are more frequent in the prompt; this shift explains drops in accuracy when going from 0-shot to 1-shot if the single shot's label is unrepresentative.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Constructed prompts with varied class balances (e.g., P P N N, P P P N) and measured fraction of predictions per class on a balanced validation set (visualized in Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9242.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9242.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recency bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recency bias (preference for answers near the end of the prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive LMs disproportionately repeat answers that occur later in the prompt, so examples placed near the end exert stronger influence on predictions than earlier ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B (reported), observed across sizes</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text classification and fact-retrieval (SST-2, LAMA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot prompts where the order of training examples is manipulated to place certain labels near the end; measurement of how often each training-answer is repeated.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot (4-shot) prompts with controlled permutations to create different recency positions for labels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Permutations that place a given label earlier vs later in the prompt (e.g., P P N N vs N N P P).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strong skewing observed: e.g., a 'P P P N' training set caused nearly 90% of predictions to be Negative despite 3/4 examples being Positive; for 4-shot LAMA the model 'overpredicts' the 1st/2nd/3rd/4th training example by +8.5%, +8.3%, +14.3%, +16.1% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because autoregressive models update hidden state left-to-right, more recent context exerts larger influence; this combines with majority-label bias producing large shifts in output distribution depending on example ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Measured per-position repeat rates across relations and training-example sets; reported overprediction as model repeat rate minus ground-truth repeat rate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9242.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9242.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Common-token bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias toward tokens common in pretraining data (common-token bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs preferentially output tokens (label names or entities) that are frequent in their pretraining corpus, which distorts predictions for tasks where the correct answers are rarer tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B, 2.7B (reported examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DBPedia topic classification; LAMA fact retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-class topic classification where each class is tied to a label token (e.g., 'book' vs 'artist'), and fact retrieval where entity tokens are predicted.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>0-/few-shot prompts where model selects among label tokens or generates single-token answers (LAMA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Observed that on DBPedia GPT-3 predicts the 'book' class 11× more often than 'artist'; correlation r=0.67 between token frequency in web/books ngrams and model prediction rate for DBPedia labels (0-shot validation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Label-token frequencies in pretraining cause uneven prior probabilities favoring common tokens; thus different label name choices or mapping can substantially change accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Token frequency measured via Google Ngrams; predictions measured in 0-shot setting on validation data; applies both to label-token choice in classification and to entity frequency in fact-retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9242.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9242.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contextual calibration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual calibration (content-free-input based probability correction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-free calibration method that estimates an LM's contextual bias by querying the model with a content-free test input and then rescales output probabilities (diagonal affine transform) so the content-free input is scored uniformly across target answers, improving mean accuracy and reducing variance across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 and GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3: 2.7B, 13B, 175B; GPT-2: 1.5B (GPT-2 XL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple: text classification (SST-2, AGNews, TREC, DBPedia, CB, RTE), fact retrieval (LAMA), information extraction (ATIS, MIT Movies)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero/few-shot in-context tasks across classification and generation where label tokens or first-token generation probabilities are used for prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot and few-shot prompts (0–8 training examples). For calibration, a content-free input (e.g., 'N/A', '[MASK]', empty string) is appended as the test input to estimate model bias; predictions are adjusted by applying W = diag(p_cf)^{-1} and b = 0 to output probabilities (vector scaling on probabilities), then argmax for final prediction. Ensemble over three content-free inputs is used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline greedy decoding without calibration; also compared to an 'oracle calibration' that fits diagonal W on validation data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Substantial improvements across tasks; examples from Table 1: AGNews (GPT-3 2.7B) 0-shot baseline 44.7% -> calibrated 63.2% (+18.5 pp); 4-shot 43.3% -> 71.1% (+27.8 pp). Reported maximum absolute improvement up to 30.0% on some tasks. Calibration also improves many GPT-2 results (e.g., GPT-2 AGNews 0-shot 44.0% -> 60.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to baseline greedy decoding (no calibration) and to oracle diagonal calibration (validation-based): contextual calibration approaches oracle performance (Figure 8 for AGNews) and consistently outperforms baseline across formats/shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Improvements reported up to +30.0 percentage points absolute; typical multi-task gains shown in Table 1 (many per-task deltas in the range +5 to +28 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Calibration counteracts a largely additive/ multiplicative shift in the model's output distribution induced by prompt context and intrinsic token priors. Estimating the shift with a content-free input and undoing it causes predictions to reflect relative evidence in the test input rather than the prompt-induced bias.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Calibration procedure: compute model's probability vector p_cf for a content-free input (ensemble over 'N/A', '[MASK]', empty string), set diagonal W = diag(p_cf)^{-1}, b = 0, adjust test-time probability vector p_test -> softmax(W p_test + b) effectively by multiplying probabilities by 1/p_cf then renormalizing; experiments across 0–8 shots, five random sets of training examples for each setting (format fixed in many experiments), and additional sweeps across prompt formats. Diagonal (vector) scaling used to keep parameter count manageable; alternative bias-only approach (b = -p_cf, W = I) works better for generation in some cases and was noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9242.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9242.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle vs contextual calibration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of data-free contextual calibration to validation-based (oracle) diagonal calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation showing that the proposed content-free-input-based calibration achieves performance close to a validation-data-fitted diagonal affine calibration (oracle), indicating the data-free method effectively estimates contextual bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (reported experiment in Figure 8), also applicable to other sizes</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AGNews (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>News-topic classification (4-way) using few-shot prompting; comparison of calibration methods.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompt formats with fixed training examples; compared contextual (content-free) calibration with diagonal W fit via validation data (oracle).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Oracle calibration (learn diagonal W on validation set) vs contextual calibration (content-free input, no labeled data) vs baseline (no calibration).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Contextual calibration's mean accuracy on AGNews was reported to be surprisingly close to the oracle diagonal calibration (visualized in Figure 8); precise numbers per setting shown in paper's figure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Contextual calibration nearly matches oracle diagonal calibration and outperforms the uncalibrated baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The content-free input captures the prompt- and format-specific bias sufficiently well to produce a near-optimal diagonal correction without labeled validation data.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Oracle diagonal W determined by searching over validation set; contextual calibration used ensemble of three content-free inputs; plotted mean ± std over different training-example sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9242.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9242.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Choice-of-content-free-input effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of different content-free inputs used for calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The particular content-free string used to estimate contextual bias (e.g., 'N/A', '[MASK]', empty string, gibberish) affects calibration effectiveness, but many choices work well and ensembling helps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B (examples reported), also applied across sizes</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (1-shot) and AGNews (0-shot) ablations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Test how different content-free inputs used to compute p_cf affect calibrated accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Calibration via a single content-free input (or ensemble) plugged into the same prompt format used for test inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Various content-free inputs: 'N/A', '[MASK]', empty string, 'the', 'abc', 'the man.', random gibberish sequences, and ensembles of multiple content-free inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3: For SST-2 (1-shot), uncalibrated baseline 66.5% -> calibration with 'N/A' 74.2%, '[MASK]' 74.5%, empty string 72.9%, ensemble ('N/A','[MASK]',empty) 79.0%, other choices ranged 69.1% to 79.4%; For AGNews (0-shot) baseline 48.5% -> ensemble 66.5% (other single choices varied).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Ensembling over content-free inputs gave the best results in these ablations; single choices also improved over baseline but with variation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Ensembling improved SST-2 1-shot to 79.0% from 66.5% baseline (+12.5 pp) in the ablation table.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different content-free strings interact differently with prompt/context but many neutral or nonsensical tokens elicit the model's contextual prior; ensembling reduces variance from this choice.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation table (Table 3) reports accuracies for multiple content-free inputs; the paper used an ensemble of 'N/A', '[MASK]', and the empty string in main experiments because it yielded strong results on AGNews and SST-2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>How can we know what language models know? <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 2)</em></li>
                <li>It's not just size that matters: Small language models are also few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9242",
    "paper_id": "paper-d7ac65d335b5d847f4f5826313a8732bc7abc7a8",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Example-permutation effect",
            "name_full": "Sensitivity to training-example permutation (ordering) in few-shot prompts",
            "brief_description": "The order (permutation) of few-shot training examples in a left-to-right prompt can dramatically change LLM accuracy; permutations can cause swings from near-chance to near-state-of-the-art performance on the same model and same examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "2.7B (primary reported), also observed on larger sizes",
            "task_name": "SST-2 sentiment classification (4-shot analysis)",
            "task_description": "Binary sentiment classification (Positive/Negative) using in-context few-shot prompting where training examples are placed inline before a test example.",
            "presentation_format": "Few-shot (4-shot) left-to-right natural-language prompt with a fixed format (e.g., 'Input: ... Sentiment: ...') and a fixed set of training examples; the experiment varies the permutation (ordering) of the four training examples across all possible permutations.",
            "comparison_format": "Other permutations of the same set of training examples (i.e., identical examples but different orderings).",
            "performance": "accuracy range: 54.3% (near-chance) up to 93.4% (near state-of-the-art) on GPT-3 2.7B when only changing permutation of 4 examples.",
            "performance_comparison": "Same examples with other permutations achieved the alternate values above (i.e., the numbers are the compared performances).",
            "format_effect_size": "+39.1 percentage points (93.4% - 54.3%) depending only on permutation",
            "explanation_or_hypothesis": "Attributed primarily to recency and majority-label biases: the model overweights answers that appear near the end of the prompt and/or answers that are more frequent in the prompt, causing permutations that place particular labels later to skew predictions.",
            "null_or_negative_result": false,
            "experimental_details": "Selected ten different sets of 4 SST-2 training examples; for each set evaluated accuracy for all possible permutations. Left-to-right autoregressive generation; classification by probability of label tokens.",
            "uuid": "e9242.0",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Prompt-format effect",
            "name_full": "Sensitivity to prompt format (template / wording / layout)",
            "brief_description": "The specific template or wording used to present the task to a left-to-right LM (question/answer style, conversational style, webpage-like, different label tokens, etc.) substantially affects few-shot performance and exhibits high variance across prompt formats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "2.7B (primary reported), observed across sizes including 175B",
            "task_name": "SST-2 sentiment classification (format sweep)",
            "task_description": "Binary sentiment classification where the same training examples are embedded in different prompt templates (15 manually designed formats including QA, conversational, Web-like, label variations).",
            "presentation_format": "Few-shot (1–4 shots in experiments) using multiple hand-designed prompt formats (15 formats for SST-2); formats vary in label surface forms, prompt wording, and layout.",
            "comparison_format": "Other prompt templates from the set of 15 (examples listed in Table 7) — e.g., 'Answer:' vs 'Sentiment:' label names, 'good/bad' vs 'positive/negative', QA-style vs declarative style.",
            "performance": "Reported high mean differences and large variance across formats (visualized in Figure 3 and Figure 7); some formats substantially outperform others on average (exact per-format numbers are reported in the paper's figures/tables for the 15 formats).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Differences arise because formats change the contextual signals that interact with the LM's intrinsic biases (e.g., label-token commonness and recency), changing output distribution and confidence. Label token choice and phrasing can favor tokens common in pretraining.",
            "null_or_negative_result": false,
            "experimental_details": "Manually designed 15 prompt formats for SST-2 (Table 7); experiments computed mean and std accuracy across formats for fixed sets of training examples; also performed analogous format paraphrase experiments for LAMA relations.",
            "uuid": "e9242.1",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Majority-label bias",
            "name_full": "Majority label bias in in-context learning",
            "brief_description": "Language models tend to favor labels that occur more frequently in the prompt (i.e., the majority class in the provided examples), leading to degraded accuracy when the prompt's class distribution is unrepresentative of the task distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "2.7B (reported), observed across sizes",
            "task_name": "Text classification and generation (SST-2, LAMA examples)",
            "task_description": "Tasks where label tokens or training-answer tokens appear in the prompt; the model's prediction distribution is analyzed relative to prompt label frequency.",
            "presentation_format": "Few-shot prompts containing multiple labeled examples; experiments vary class balance among training examples (e.g., unbalanced vs balanced prompts).",
            "comparison_format": "Balanced vs unbalanced training-example label distributions in the prompt.",
            "performance": "Example statistics: on 4-shot LAMA with GPT-3 2.7B, 50.2% of model predictions repeat one of the four training answers (correct repeat rate 24.7%), indicating over-representation of training answers in outputs.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The LM's outputs are shifted toward labels that are more frequent in the prompt; this shift explains drops in accuracy when going from 0-shot to 1-shot if the single shot's label is unrepresentative.",
            "null_or_negative_result": false,
            "experimental_details": "Constructed prompts with varied class balances (e.g., P P N N, P P P N) and measured fraction of predictions per class on a balanced validation set (visualized in Figure 4).",
            "uuid": "e9242.2",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Recency bias",
            "name_full": "Recency bias (preference for answers near the end of the prompt)",
            "brief_description": "Autoregressive LMs disproportionately repeat answers that occur later in the prompt, so examples placed near the end exert stronger influence on predictions than earlier ones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "2.7B (reported), observed across sizes",
            "task_name": "Text classification and fact-retrieval (SST-2, LAMA)",
            "task_description": "Few-shot prompts where the order of training examples is manipulated to place certain labels near the end; measurement of how often each training-answer is repeated.",
            "presentation_format": "Few-shot (4-shot) prompts with controlled permutations to create different recency positions for labels.",
            "comparison_format": "Permutations that place a given label earlier vs later in the prompt (e.g., P P N N vs N N P P).",
            "performance": "Strong skewing observed: e.g., a 'P P P N' training set caused nearly 90% of predictions to be Negative despite 3/4 examples being Positive; for 4-shot LAMA the model 'overpredicts' the 1st/2nd/3rd/4th training example by +8.5%, +8.3%, +14.3%, +16.1% respectively.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Because autoregressive models update hidden state left-to-right, more recent context exerts larger influence; this combines with majority-label bias producing large shifts in output distribution depending on example ordering.",
            "null_or_negative_result": false,
            "experimental_details": "Measured per-position repeat rates across relations and training-example sets; reported overprediction as model repeat rate minus ground-truth repeat rate.",
            "uuid": "e9242.3",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Common-token bias",
            "name_full": "Bias toward tokens common in pretraining data (common-token bias)",
            "brief_description": "LLMs preferentially output tokens (label names or entities) that are frequent in their pretraining corpus, which distorts predictions for tasks where the correct answers are rarer tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B, 2.7B (reported examples)",
            "task_name": "DBPedia topic classification; LAMA fact retrieval",
            "task_description": "Multi-class topic classification where each class is tied to a label token (e.g., 'book' vs 'artist'), and fact retrieval where entity tokens are predicted.",
            "presentation_format": "0-/few-shot prompts where model selects among label tokens or generates single-token answers (LAMA).",
            "comparison_format": null,
            "performance": "Observed that on DBPedia GPT-3 predicts the 'book' class 11× more often than 'artist'; correlation r=0.67 between token frequency in web/books ngrams and model prediction rate for DBPedia labels (0-shot validation).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Label-token frequencies in pretraining cause uneven prior probabilities favoring common tokens; thus different label name choices or mapping can substantially change accuracy.",
            "null_or_negative_result": false,
            "experimental_details": "Token frequency measured via Google Ngrams; predictions measured in 0-shot setting on validation data; applies both to label-token choice in classification and to entity frequency in fact-retrieval.",
            "uuid": "e9242.4",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Contextual calibration",
            "name_full": "Contextual calibration (content-free-input based probability correction)",
            "brief_description": "A data-free calibration method that estimates an LM's contextual bias by querying the model with a content-free test input and then rescales output probabilities (diagonal affine transform) so the content-free input is scored uniformly across target answers, improving mean accuracy and reducing variance across prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 and GPT-2",
            "model_size": "GPT-3: 2.7B, 13B, 175B; GPT-2: 1.5B (GPT-2 XL)",
            "task_name": "Multiple: text classification (SST-2, AGNews, TREC, DBPedia, CB, RTE), fact retrieval (LAMA), information extraction (ATIS, MIT Movies)",
            "task_description": "Zero/few-shot in-context tasks across classification and generation where label tokens or first-token generation probabilities are used for prediction.",
            "presentation_format": "Zero-shot and few-shot prompts (0–8 training examples). For calibration, a content-free input (e.g., 'N/A', '[MASK]', empty string) is appended as the test input to estimate model bias; predictions are adjusted by applying W = diag(p_cf)^{-1} and b = 0 to output probabilities (vector scaling on probabilities), then argmax for final prediction. Ensemble over three content-free inputs is used.",
            "comparison_format": "Baseline greedy decoding without calibration; also compared to an 'oracle calibration' that fits diagonal W on validation data.",
            "performance": "Substantial improvements across tasks; examples from Table 1: AGNews (GPT-3 2.7B) 0-shot baseline 44.7% -&gt; calibrated 63.2% (+18.5 pp); 4-shot 43.3% -&gt; 71.1% (+27.8 pp). Reported maximum absolute improvement up to 30.0% on some tasks. Calibration also improves many GPT-2 results (e.g., GPT-2 AGNews 0-shot 44.0% -&gt; 60.0%).",
            "performance_comparison": "Compared to baseline greedy decoding (no calibration) and to oracle diagonal calibration (validation-based): contextual calibration approaches oracle performance (Figure 8 for AGNews) and consistently outperforms baseline across formats/shot counts.",
            "format_effect_size": "Improvements reported up to +30.0 percentage points absolute; typical multi-task gains shown in Table 1 (many per-task deltas in the range +5 to +28 pp).",
            "explanation_or_hypothesis": "Calibration counteracts a largely additive/ multiplicative shift in the model's output distribution induced by prompt context and intrinsic token priors. Estimating the shift with a content-free input and undoing it causes predictions to reflect relative evidence in the test input rather than the prompt-induced bias.",
            "null_or_negative_result": false,
            "experimental_details": "Calibration procedure: compute model's probability vector p_cf for a content-free input (ensemble over 'N/A', '[MASK]', empty string), set diagonal W = diag(p_cf)^{-1}, b = 0, adjust test-time probability vector p_test -&gt; softmax(W p_test + b) effectively by multiplying probabilities by 1/p_cf then renormalizing; experiments across 0–8 shots, five random sets of training examples for each setting (format fixed in many experiments), and additional sweeps across prompt formats. Diagonal (vector) scaling used to keep parameter count manageable; alternative bias-only approach (b = -p_cf, W = I) works better for generation in some cases and was noted.",
            "uuid": "e9242.5",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Oracle vs contextual calibration",
            "name_full": "Comparison of data-free contextual calibration to validation-based (oracle) diagonal calibration",
            "brief_description": "An ablation showing that the proposed content-free-input-based calibration achieves performance close to a validation-data-fitted diagonal affine calibration (oracle), indicating the data-free method effectively estimates contextual bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B (reported experiment in Figure 8), also applicable to other sizes",
            "task_name": "AGNews (classification)",
            "task_description": "News-topic classification (4-way) using few-shot prompting; comparison of calibration methods.",
            "presentation_format": "Few-shot prompt formats with fixed training examples; compared contextual (content-free) calibration with diagonal W fit via validation data (oracle).",
            "comparison_format": "Oracle calibration (learn diagonal W on validation set) vs contextual calibration (content-free input, no labeled data) vs baseline (no calibration).",
            "performance": "Contextual calibration's mean accuracy on AGNews was reported to be surprisingly close to the oracle diagonal calibration (visualized in Figure 8); precise numbers per setting shown in paper's figure.",
            "performance_comparison": "Contextual calibration nearly matches oracle diagonal calibration and outperforms the uncalibrated baseline.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "The content-free input captures the prompt- and format-specific bias sufficiently well to produce a near-optimal diagonal correction without labeled validation data.",
            "null_or_negative_result": false,
            "experimental_details": "Oracle diagonal W determined by searching over validation set; contextual calibration used ensemble of three content-free inputs; plotted mean ± std over different training-example sets.",
            "uuid": "e9242.6",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Choice-of-content-free-input effect",
            "name_full": "Effect of different content-free inputs used for calibration",
            "brief_description": "The particular content-free string used to estimate contextual bias (e.g., 'N/A', '[MASK]', empty string, gibberish) affects calibration effectiveness, but many choices work well and ensembling helps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "2.7B (examples reported), also applied across sizes",
            "task_name": "SST-2 (1-shot) and AGNews (0-shot) ablations",
            "task_description": "Test how different content-free inputs used to compute p_cf affect calibrated accuracy.",
            "presentation_format": "Calibration via a single content-free input (or ensemble) plugged into the same prompt format used for test inference.",
            "comparison_format": "Various content-free inputs: 'N/A', '[MASK]', empty string, 'the', 'abc', 'the man.', random gibberish sequences, and ensembles of multiple content-free inputs.",
            "performance": "Table 3: For SST-2 (1-shot), uncalibrated baseline 66.5% -&gt; calibration with 'N/A' 74.2%, '[MASK]' 74.5%, empty string 72.9%, ensemble ('N/A','[MASK]',empty) 79.0%, other choices ranged 69.1% to 79.4%; For AGNews (0-shot) baseline 48.5% -&gt; ensemble 66.5% (other single choices varied).",
            "performance_comparison": "Ensembling over content-free inputs gave the best results in these ablations; single choices also improved over baseline but with variation.",
            "format_effect_size": "Ensembling improved SST-2 1-shot to 79.0% from 66.5% baseline (+12.5 pp) in the ablation table.",
            "explanation_or_hypothesis": "Different content-free strings interact differently with prompt/context but many neutral or nonsensical tokens elicit the model's contextual prior; ensembling reduces variance from this choice.",
            "null_or_negative_result": false,
            "experimental_details": "Ablation table (Table 3) reports accuracies for multiple content-free inputs; the paper used an ensemble of 'N/A', '[MASK]', and the empty string in main experiments because it yielded strong results on AGNews and SST-2.",
            "uuid": "e9242.7",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2,
            "sanitized_title": "language_models_as_knowledge_bases"
        },
        {
            "paper_title": "How can we know what language models know?",
            "rating": 2,
            "sanitized_title": "how_can_we_know_what_language_models_know"
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 2,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "It's not just size that matters: Small language models are also few-shot learners",
            "rating": 1,
            "sanitized_title": "its_not_just_size_that_matters_small_language_models_are_also_fewshot_learners"
        }
    ],
    "cost": 0.015851999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Calibrate Before Use: Improving Few-Shot Performance of Language Models</h1>
<p>Tony Z. Zhao ${ }^{<em> 1}$ Eric Wallace ${ }^{</em> 1}$ Shi Feng ${ }^{2}$ Dan Klein ${ }^{1}$ Sameer Singh ${ }^{3}$</p>
<h4>Abstract</h4>
<p>GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pretraining data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to $30.0 \%$ absolute) and reduces variance across different choices of the prompt.</p>
<h2>1. Introduction</h2>
<p>Few-shot learning-the ability to learn tasks with limited examples-is an important aspect of intelligence (Lake et al., 2015; Yogatama et al., 2019). Recent work shows that large neural language models can perform few-shot learning without finetuning (Radford et al., 2019; Brown et al., 2020). Specifically, GPT-3 (Brown et al., 2020) can perform numerous tasks when provided a few examples in a natural language prompt. For example, to perform sentiment analysis one can condition GPT-3 on a prompt such as:</p>
<p>Input: Subpar acting. Sentiment: Negative Input: Beautiful film. Sentiment: Positive Input: Amazing. Sentiment:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>where the first two lines correspond to two training examples and the last line is a test example. To make predictions, the model predicts whether the subsequent token is more likely to be the word "Positive" or "Negative".</p>
<p>This style of few-shot "in-context" learning is interesting because it shows that the model can learn without parameter updates. And, more importantly, it has numerous practical advantages over the now-standard approach of finetuning (Radford et al., 2018; Devlin et al., 2019). First, it allows practitioners to "rapidly prototype" NLP models: changing the prompt immediately leads to a new model. Second, it provides a fully natural language interface to a machine learning model, which allows users-even those without technical expertise-to create NLP systems. Finally, since in-context learning reuses the same model for each task, it reduces memory requirements and system complexity when serving many different tasks.</p>
<p>However, despite these promises, we show that GPT-3's accuracy can be highly unstable across different prompts (Section 3). A prompt contains three components: a format, a set of training examples, and a permutation (ordering) for those examples. We show that different choices for these factors can lead to highly different accuracies, e.g., changing the permutation of the training examples in a sentiment analysis prompt can change accuracy from near chance (54\%) to near state-of-the-art (93\%). This instability implies that GPT-3 users, who typically design prompts manually, cannot expect to consistently obtain good accuracy.</p>
<p>We next analyze what causes this instability. We identify three pitfalls of language models that lead them to be biased toward certain answers during few-shot learning. In particular, they suffer from majority label bias, recency bias, and common token bias (Section 4). The majority label and recency biases lead the model to predict training answers that appear frequently or near the end of the prompt. For example, a prompt that ends with a Negative training example may cause a bias towards the Negative class. On the other hand, the common token bias leads the model to prefer answers that are frequent in its pre-training data, e.g., it prefers "United States" over "Saint Lucia", which is likely suboptimal for the task of interest.</p>
<p>We identify that these biases typically result in a shift in the output distribution of the model. We can thus coun-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Few-shot learning can be highly unstable across different choices of the prompt. Above, we plot the mean accuracy ( $\pm$ one standard deviation) across different choices of the training examples for three different datasets and model sizes. We show that our method, contextual calibration, improves accuracy, reduces variance, and overall makes tools like GPT-3 more effective for end users.
teract these biases by "calibrating" the output distribution. Concretely, we estimate the model's bias towards certain answers by feeding in a dummy test input that is content-free. In the prompt above for example, if we replace "Amazing." with the string "N/A", the model predicts $62 \%$ Positive. We then fit the calibration parameters so that the content-free input has uniform scores for each answer. This contextual calibration procedure provides a good setting of the calibration parameters without additional training data.</p>
<p>We test the effectiveness of contextual calibration on a range of tasks (Section 5). Contextual calibration consistently improves GPT-3 and GPT-2's accuracy (up to $30.0 \%$ absolute) across different choices of the prompt format and examples (e.g., Figure 1). It also makes the accuracy more stable across different prompts, thus mitigating the need for prompt engineering. Overall, contextual calibration is a simple method that makes language models better few-shot learners: it enables end users to obtain higher accuracy with considerably less effort.</p>
<h2>2. Background and Experimental Setup</h2>
<p>Neural autoregressive language models (LMs) take as input a sequence of tokens and output a probability distribution over the next token. Large neural LMs can perform tasks in a zero- or few-shot manner using in-context learning (Radford et al., 2019; Brown et al., 2020). To do so, a natural language prompt is fed into the model. This prompt contains three components: a format, a set of training examples, and a permutation (ordering) of the training examples.</p>
<p>Prompt Format The prompt format is a template which consists of placeholders for the training and test example(s) and possibly a natural language description of the task. For example, the format of the prompt in Section 1 is a template with the style: "Input:" input "Sentiment:" label. Many
alternate formats exist, e.g., one could frame the task as question answering.</p>
<p>Prompt Training Examples The prompt's training examples are used to teach the LM how to solve the task at hand. The prompt from Section 1 consists of two training examples; we refer to this as "two-shot" learning. We also consider "zero-shot" learning, where no training examples are present.</p>
<p>Training Example Permutation When training examples are used, they have a particular permutation, e.g., the "Subpar acting" example comes first in the prompt from Section 1. The permutation matters because neural language models update their hidden states in a left-to-right-fashion.</p>
<p>To make predictions on an input, we slot it into the test placeholder and generate from the LM. For example, see the "Amazing." test example in the prompt from Section 1. For generation tasks, we generate greedily from the LM until it produces a newline character. For classification tasks, the probability for each class is given by the probability assigned to its associated label name, e.g., the words "Negative" and "Positive" for sentiment classification.</p>
<h3>2.1. Datasets and Prompt Formats</h3>
<p>We use datasets for three tasks: text classification, fact retrieval, and information extraction. We use a fixed prompt format for each dataset unless otherwise specified. We show the format and examples from each dataset in Appendix B.</p>
<p>Text Classification We study text classification using six datasets: sentiment analysis using SST-2 (Socher et al., 2013), 6-way question classification using TREC (Voorhees \&amp; Tice, 2000), textual entailment using 3-way CB (de Marneffe et al., 2019) and binary RTE (Dagan et al., 2005) from SuperGLUE (Wang et al., 2019), and topic classification</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. There is high variance in GPT-3's accuracy as we change the prompt's training examples, as well as the permutation of the examples. Here, we select ten different sets of four SST-2 training examples. For each set of examples, we vary their permutation and plot GPT-3 2.7B's accuracy for each permutation (and its quartiles).
using the 4-way AGNews (Zhang et al., 2015) and 14-way DBPedia (Zhang et al., 2015) datasets. The prompt in Section 1 shows an example of the sentiment analysis task.</p>
<p>Fact Retrieval We evaluate fact retrieval with LAMA (Petroni et al., 2019). The dataset consists of knowledge base triples that are placed into templates with missing objects, e.g. "Obama was born in". We use these templates as our prompts, and remove the relations where the missing answer is not at the end of the template (left-to-right LMs cannot solve these). The answers are always single tokens, and we report average accuracy across all triples.</p>
<p>Information Extraction We consider information extraction using two slot filling datasets, ATIS (Hemphill et al., 1990) and MIT Movies trivial0k13 (Liu et al., 2012). We use two random slots for each dataset, airline and departure date for ATIS, and director name and movie genre for MIT Movies. The answer for both datasets is a span of text from the input, e.g., the ATIS airline task is to predict "american airlines" when given the sentence "list a flight on american airlines from toronto to san diego". We use Exact Match between the model's generated output and the ground-truth span as our evaluation metric.</p>
<h3>2.2. Model Details</h3>
<p>We run our experiments on three sizes of GPT-3 (2.7B, 13B, and 175B parameters) as well as GPT-2 (1.5B parameters). We access GPT-3 using the OpenAI API. We release code to replicate our experiments. ${ }^{1}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. There is high variance in GPT-3's accuracy as we change the prompt format. In this figure, we use ten different prompt formats for SST-2. For each format, we plot GPT-3 2.7B's accuracy for different sets of four training examples, along with the quartiles.</p>
<h2>3. Accuracy Varies Highly Across Prompts</h2>
<p>This section studies how GPT-3's accuracy changes as we vary each aspect of the prompt (training examples, permutation, format). We focus on a subset of the datasets to simplify our analysis; in Section 5 we show that our findings hold across all of the datasets we study.</p>
<p>GPT-3's accuracy depends highly on both selection and permutation of training examples. Concretely, we use a fixed prompt format and choose different random sets of training examples. For each set of training examples, we evaluate the accuracy for all possible permutations.</p>
<p>Figure 2 shows the results for SST-2 (4-shot, GPT-3 2.7B). Surprisingly, varying the permutation can be as important, or even more important, than which training examples are chosen. For example, varying the permutation of the training examples can cause accuracy to go from near chance (54.3\%) to near state-of-the-art (93.4\%). For a qualitative example of the sensitivity to permutations, see Table 2 in Appendix A. This high importance on example order is in contrast to standard machine learning, where the ordering of examples during training is typically an afterthought.</p>
<p>The variance persists with more data and larger models. Adding more training examples into the prompt does not necessarily reduce the variance in accuracy. We sweep over the number of training examples for three different datasets in Figure 1 (red curves). The variance remains high even when we use 16 training examples. Moreover, adding more training examples can sometimes hurt accuracy (e.g., mean accuracy drops from $36.0 \%$ to $25.9 \%$ for DBPedia 0 -shot to 1 -shot). The variance in accuracy can also remain high when using larger models, e.g., the left of Figure 1.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Majority label and recency biases cause GPT-3 to become biased towards certain answers and help to explain the high variance across different examples and orderings. Above, we use 4-shot SST-2 with prompts that have different class balances and permutations, e.g., [P P N N] indicates two positive training examples and then two negative. We plot how often GPT-3 2.7B predicts Positive on the balanced validation set. When the prompt is unbalanced, the predictions are unbalanced (majority label bias). In addition, balanced prompts that have one class repeated near the end, e.g., end with two Negative examples, will have a bias towards that class (recency bias).</p>
<p>GPT-3's accuracy depends highly on prompt format. We next keep the set of training examples and permutations fixed but vary the prompt format. We focus on SST-2, and we manually design an additional 14 prompt formats. The formats include question-answer templates, conversationstyle templates, prompts that resemble Web pages, and variations on the label names (all formats available in Table 7 in Appendix B). The accuracy for ten of the formats is shown in Figure 3. We find that some of the formats are better than others on average. However, all of the formats still suffer from high variance across different training sets.</p>
<h2>4. What Causes the High Variance?</h2>
<p>We next analyze why GPT-3's accuracy varies across different training examples, permutations, and prompt formats. Concretely, we show that the variance arises because LM s are biased towards outputting answers that are (1) frequent in the prompt (majority label bias), (2) towards the end of the prompt (recency bias), and (3) common in the pre-training data (common token bias).</p>
<p>Majority Label Bias We find that GPT-3 is biased towards answers that are frequent in the prompt. A trivial case is when a text classification prompt has a class imbalance, e.g., more Positive than Negative sentiment examples. This is demonstrated in the "unbalanced" region of Figure 4: when one class is more common, GPT-3 2.7B is heavily biased towards predicting that class. Since the SST-2 sentiment analysis dataset is balanced, this bias causes large accuracy degradations. The majority label bias also explains why we frequently observe a drop in accuracy when moving from 0 -shot to 1 -shot-we found that the drop is due to the model frequently repeating the class of the one training example.</p>
<p>The majority label bias also occurs for generation tasks. On the validation set for 4-shot LAMA with GPT-3 2.7B, 50.2\% of the model predictions are a repeat of one of the four train-
ing answers (the correct repeat rate is $24.7 \%$ ). Overall, the majority label bias helps to explain why different choices for the training examples heavily influence GPT-3's accuracyit shifts the distribution of model predictions.</p>
<p>Recency Bias The model's majority label bias is aggravated by its recency bias: the tendency to repeat answers that appear towards the end of the prompt. The "balanced" region of Figure 4 demonstrates this. For instance, when two Negative examples appear at the end (P P N N), the model will heavily prefer the Negative class. Moreover, the recency bias can outweigh the majority label bias, e.g., the "P P P N" training set leads to nearly $90 \%$ of predictions being Negative, despite $\frac{3}{4}$ of the training examples being Positive.</p>
<p>Recency bias also affects generation tasks. For 4-shot LAMA, the training answers that are closer to the end of the prompt are more likely to be repeated by the model. Concretely, the model "overpredicts" the answer from the 1st, 2nd, 3rd, and 4th training example by $8.5 \%, 8.3 \%, 14.3 \%$, and $16.1 \%$, respectively. ${ }^{2}$ Overall, recency bias helps to explain why the permutation of the training examples is important-the ordering of the examples heavily influences the distribution of the model predictions.</p>
<p>Common Token Bias Finally, we find that GPT-3 is biased towards outputting tokens that are common in its pretraining distribution, which is likely suboptimal for the distribution of answers on the downstream task. A simple case of this occurs for the LAMA fact retrieval dataset, where the model often predicts common entities such as "America" when the ground-truth answer is instead a rare entity.</p>
<p>A more nuanced case of the common token bias occurs for</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>text classification. Recall that the model makes predictions by generating the label name associated with each class. Because certain label names appear more frequently in the pre-training data, the model will be inherently biased towards predicting certain classes. For example, on DBPedia (a balanced 14-way topic classification dataset), GPT-3 predicts the "book" class $11 \times$ more often than the "artist" class. In fact, there is a moderate correlation $(r=0.67)$ between the frequency of a DBPedia label name and the rate at which GPT-3 predicts its class. ${ }^{3}$ Overall, the common token bias helps to explain why the choice of label names is important, and why the model struggles on rare answers.</p>
<p>The Impact of Biases on Model Predictions We find that the end result of the above three biases is typically a simple shift in the model's output distribution. For example, Figure 5 visualizes this shift for a SST-2 sentiment prompt.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. The Positive class probability for 25 random test inputs for a particular sentiment analysis prompt. Negative ground-truth examples are marked with $\boldsymbol{\sim}$ and Positive are marked with $\boldsymbol{\bullet}$.</p>
<p>The prompt used in Figure 5 and the model's intrinsic biases cause it to frequently predict high confidence for the Positive class. Since the default $50 \%$ threshold is used to make predictions, this results in frequent false positives. Importantly, note that if we could optimally set the classification threshold ( $\mathrm{p}($ Positive $)=0.68$ in this case), the classifier would be highly accurate ( $94 \%$ on the validation set).</p>
<h2>5. Contextual Calibration</h2>
<p>Thus far, we have shown that GPT-3 is biased towards certain answers due to the prompt and the model's intrinsic biases. Here, we look to correct this by "calibrating" the model's output probabilities. ${ }^{4}$ A common technique for adjusting output probabilities is to apply an affine transformation (Platt, 1999; Guo et al., 2017):</p>
<p>$$
\hat{\mathbf{q}}=\operatorname{softmax}(\mathbf{W} \hat{\mathbf{p}}+\mathbf{b})
$$</p>
<p>where a weight matrix $\mathbf{W}$ and a bias vector $\mathbf{b}$ are applied to the original probabilities $\hat{\mathbf{p}}$ to get the new probabilities</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$\hat{\mathbf{q}} .{ }^{5}$ For classification tasks, $\hat{\mathbf{p}}$ is the set of probabilities that are associated with each label name, renormalized to one. For generation tasks, $\hat{\mathbf{p}}$ is the entire set of probabilities for the first token. ${ }^{6}$ In this paper, we restrict the matrix $\mathbf{W}$ to be diagonal, known as vector scaling (Guo et al., 2017), to prevent the parameters from growing quadratically in the size of $\hat{\mathbf{p}}$ (which is $\approx 50,000$ for generation tasks).</p>
<p>The main challenge in the zero- or few-shot setting is that we do not have data to learn $\mathbf{W}$ and $\mathbf{b}$. We thus propose a novel data-free procedure to infer a good setting of these parameters. The key idea is that the model's bias towards certain answers can be estimated by feeding in a contentfree input such as the string "N/A". For example, consider the two-shot prompt:</p>
<p>Input: Subpar acting. Sentiment: Negative Input: Beautiful film. Sentiment: Positive Input: N/A Sentiment:
where "N/A" serves as the test input. Ideally, GPT-3 would score this test input as $50 \%$ Positive and $50 \%$ Negative. However, the model's biases cause it to score this input as $61.8 \%$ Positive. Note that this error is contextual: a different choice of the training examples, permutation, and format will lead to different predictions for the content-free input.</p>
<p>We can correct this error by setting $\mathbf{W}$ and $\mathbf{b}$ so that the class scores for the content-free input are uniform. We first obtain $\hat{\mathbf{p}}$ for the content-free input, denoted $\hat{\mathbf{p}}<em _mathrm_cf="\mathrm{cf">{\mathrm{cf}}$. We then set $\mathbf{W}=\operatorname{diag}\left(\hat{\mathbf{p}}</em>$ and take the argmax.}}\right)^{-1}$ and $\mathbf{b}$ to the all-zero vector. ${ }^{7}$ To make test predictions, we compute $\mathbf{W} \hat{\mathbf{p}}+\mathbf{b</p>
<p>Implementation Details This contextual calibration procedure adds trivial amounts of computational overhead and is implemented in a few lines of code (compute and save $\hat{\mathbf{p}}_{\mathrm{cf}}$, adjust output probabilities). For the content-free input, many good choices exist, including "N/A", the empty string, and gibberish tokens. In all our experiments, we average the probabilities from three content-free inputs: "N/A", "[MASK]", and the empty string. ${ }^{8}$ One could also craft the content-free input in a task-specific manner. We explore this for LAMA, where we replace the subject with the contentfree input, e.g., we use "N/A was born in" as the input.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>LM</th>
<th>0-shot</th>
<th></th>
<th>1-shot</th>
<th></th>
<th>4-shot</th>
<th></th>
<th>8-shot</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Baseline</td>
<td>Ours</td>
<td>Baseline</td>
<td>Ours</td>
<td>Baseline</td>
<td>Ours</td>
<td>Baseline</td>
<td>Ours</td>
</tr>
<tr>
<td>Text Classification</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AGNews</td>
<td>2.7B</td>
<td>$44.7_{0.0}$</td>
<td>$\mathbf{6 3 . 2}_{0.0}$</td>
<td>$33.0_{5.1}$</td>
<td>$\mathbf{5 9 . 6}_{6.4}$</td>
<td>$43.3_{8.3}$</td>
<td>$\mathbf{7 1 . 1}_{8.5}$</td>
<td>$50.8_{7.8}$</td>
<td>$\mathbf{7 2 . 7}_{5.8}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$43.9_{0.0}$</td>
<td>$\mathbf{7 3 . 9}_{0.0}$</td>
<td>$62.1_{6.3}$</td>
<td>$\mathbf{7 7 . 1}_{3.8}$</td>
<td>$61.0_{10.9}$</td>
<td>$\mathbf{8 5 . 9}_{1.3}$</td>
<td>$79.1_{2.6}$</td>
<td>$\mathbf{8 4 . 3}_{2.5}$</td>
</tr>
<tr>
<td>TREC</td>
<td>2.7B</td>
<td>$31.0_{0.0}$</td>
<td>$\mathbf{3 8 . 8}_{0.0}$</td>
<td>$24.3_{6.4}$</td>
<td>$\mathbf{3 6 . 8}_{7.7}$</td>
<td>$25.8_{11.5}$</td>
<td>$\mathbf{3 8 . 6}_{13.2}$</td>
<td>$29.3_{8.0}$</td>
<td>$\mathbf{4 4 . 3}_{11.4}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$47.4_{0.0}$</td>
<td>$\mathbf{5 7 . 4}_{0.0}$</td>
<td>$57.7_{6.0}$</td>
<td>$\mathbf{7 5 . 7}_{1.4}$</td>
<td>$60.2_{7.6}$</td>
<td>$\mathbf{6 9 . 7}_{1.4}$</td>
<td>$45.6_{4.0}$</td>
<td>$\mathbf{6 6 . 9}_{6.5}$</td>
</tr>
<tr>
<td>CB</td>
<td>2.7B</td>
<td>$44.6_{0.0}$</td>
<td>$\mathbf{5 0 . 0}_{0.0}$</td>
<td>$\mathbf{3 3 . 8}_{16.6}$</td>
<td>$33.0_{7.3}$</td>
<td>$43.5_{11.9}$</td>
<td>$\mathbf{5 4 . 2}_{4.7}$</td>
<td>$43.9_{8.4}$</td>
<td>$\mathbf{5 3 . 0}_{7.7}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$30.4_{0.0}$</td>
<td>$\mathbf{4 8 . 2}_{0.0}$</td>
<td>$50.9_{6.7}$</td>
<td>$\mathbf{5 1 . 8}_{7.2}$</td>
<td>$45.2_{19.4}$</td>
<td>$\mathbf{6 0 . 7}_{6.7}$</td>
<td>$59.6_{11.3}$</td>
<td>$\mathbf{6 5 . 0}_{7.9}$</td>
</tr>
<tr>
<td>RTE</td>
<td>2.7B</td>
<td>$44.8_{0.0}$</td>
<td>$\mathbf{4 9 . 5}_{0.0}$</td>
<td>$49.6_{2.9}$</td>
<td>$\mathbf{5 0 . 4}_{2.7}$</td>
<td>$44.0_{1.4}$</td>
<td>$\mathbf{5 4 . 5}_{4.7}$</td>
<td>$49.2_{1.9}$</td>
<td>$\mathbf{5 4 . 8}_{2.8}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$\mathbf{5 7 . 8}_{0.0}$</td>
<td>$\mathbf{5 7 . 8}_{0.0}$</td>
<td>$\mathbf{6 2 . 9}_{2.7}$</td>
<td>$62.8_{2.3}$</td>
<td>$58.7_{11.9}$</td>
<td>$\mathbf{6 0 . 4}_{8.1}$</td>
<td>$\mathbf{6 6 . 2}_{5.8}$</td>
<td>$65.5_{2.5}$</td>
</tr>
<tr>
<td>SST-2</td>
<td>2.7B</td>
<td>$57.2_{0.0}$</td>
<td>$\mathbf{7 1 . 4}_{0.0}$</td>
<td>$67.3_{7.9}$</td>
<td>$\mathbf{7 9 . 1}_{8.3}$</td>
<td>$59.1_{10.2}$</td>
<td>$\mathbf{7 9 . 9}_{7.8}$</td>
<td>$54.0_{4.3}$</td>
<td>$\mathbf{8 2 . 0}_{5.5}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$71.6_{0.0}$</td>
<td>$\mathbf{7 5 . 8}_{0.0}$</td>
<td>$93.3_{2.8}$</td>
<td>$\mathbf{9 4 . 7}_{1.4}$</td>
<td>$93.6_{3.3}$</td>
<td>$\mathbf{9 4 . 3}_{1.0}$</td>
<td>$\mathbf{9 5 . 6}_{1.0}$</td>
<td>$95.3_{0.7}$</td>
</tr>
<tr>
<td>DBPedia</td>
<td>2.7B</td>
<td>$36.0_{0.0}$</td>
<td>$\mathbf{3 8 . 7}_{0.0}$</td>
<td>$25.9_{4.4}$</td>
<td>$\mathbf{6 1 . 6}_{2.9}$</td>
<td>$61.0_{12.8}$</td>
<td>$\mathbf{6 6 . 0}_{7.5}$</td>
<td>$72.6_{4.5}$</td>
<td>$\mathbf{7 4 . 8}_{5.0}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$22.0_{0.0}$</td>
<td>$\mathbf{5 9 . 7}_{0.0}$</td>
<td>$79.3_{3.0}$</td>
<td>$\mathbf{8 5 . 3}_{2.2}$</td>
<td>$84.6_{5.8}$</td>
<td>$\mathbf{8 6 . 9}_{4.0}$</td>
<td>$82.3_{7.8}$</td>
<td>$\mathbf{8 6 . 9}_{1.9}$</td>
</tr>
<tr>
<td>Fact Retrieval</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LAMA</td>
<td>2.7B</td>
<td>$14.0_{0.0}$</td>
<td>$\mathbf{2 2 . 7}_{0.0}$</td>
<td>$29.7_{1.8}$</td>
<td>$\mathbf{3 1 . 6}_{1.3}$</td>
<td>$35.8_{3.8}$</td>
<td>$\mathbf{3 7 . 4}_{3.4}$</td>
<td>$\mathbf{4 2 . 5}_{1.3}$</td>
<td>$\mathbf{4 2 . 5}_{1.4}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$23.5_{0.0}$</td>
<td>$\mathbf{3 0 . 1}_{0.0}$</td>
<td>$48.9_{2.3}$</td>
<td>$\mathbf{4 9 . 0}_{1.4}$</td>
<td>$\mathbf{6 2 . 0}_{2.4}$</td>
<td>$61.8_{2.9}$</td>
<td>$\mathbf{6 3 . 8}_{1.0}$</td>
<td>$63.6_{1.3}$</td>
</tr>
<tr>
<td>Information Extraction</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MIT-G</td>
<td>2.7B</td>
<td>$5.0_{0.0}$</td>
<td>$\mathbf{5 . 7}_{0.0}$</td>
<td>$26.7_{11.4}$</td>
<td>$\mathbf{3 7 . 9}_{5.7}$</td>
<td>$53.1_{7.8}$</td>
<td>$\mathbf{5 4 . 7}_{6.0}$</td>
<td>$59.0_{4.7}$</td>
<td>$\mathbf{5 9 . 1}_{4.8}$</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>$15.0_{0.0}$</td>
<td>$\mathbf{1 8 . 7}_{0.0}$</td>
<td>$47.3_{3.9}$</td>
<td>$\mathbf{5 2 . 0}_{7.9}$</td>
<td>$57.9_{4.8}$</td>
<td>$\mathbf{5 8 . 9}_{4.0}$</td>
<td>$59.0_{4.7}$</td>
<td>$\mathbf{5 9 . 1}_{4.8}$</td>
</tr>
<tr>
<td>MIT-D</td>
<td>2.7B</td>
<td>$46.3_{0.0}$</td>
<td>$\mathbf{4 7 . 0}_{0.0}$</td>
<td>$42.0_{13.0}$</td>
<td>$\mathbf{5 3 . 5}_{13.5}$</td>
<td>$73.5_{4.9}$</td>
<td>$\mathbf{7 4 . 1}_{5.0}$</td>
<td>$\mathbf{7 5 . 3}_{1.0}$</td>
<td>$75.1_{1.3}$</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>$36.3_{0.0}$</td>
<td>$\mathbf{3 8 . 7}_{0.0}$</td>
<td>$58.6_{21.4}$</td>
<td>$\mathbf{7 2 . 8}_{4.0}$</td>
<td>$75.4_{1.9}$</td>
<td>$\mathbf{7 5 . 9}_{2.1}$</td>
<td>$\mathbf{7 7 . 8}_{0.5}$</td>
<td>$\mathbf{7 7 . 8}_{0.5}$</td>
</tr>
<tr>
<td>ATIS-A</td>
<td>2.7B</td>
<td>$10.8_{0.0}$</td>
<td>$\mathbf{1 4 . 0}_{0.0}$</td>
<td>$29.8_{12.8}$</td>
<td>$\mathbf{3 3 . 1}_{9.4}$</td>
<td>$43.0_{26.2}$</td>
<td>$\mathbf{4 7 . 3}_{21.3}$</td>
<td>$55.6_{5.0}$</td>
<td>$\mathbf{5 8 . 8}_{4.0}$</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>$49.5_{0.0}$</td>
<td>$\mathbf{5 2 . 7}_{0.0}$</td>
<td>$69.6_{17.4}$</td>
<td>$\mathbf{7 1 . 8}_{17.1}$</td>
<td>$67.5_{10.4}$</td>
<td>$\mathbf{6 9 . 6}_{13.4}$</td>
<td>$63.4_{4.6}$</td>
<td>$\mathbf{6 4 . 5}_{4.0}$</td>
</tr>
<tr>
<td>ATIS-D</td>
<td>2.7B</td>
<td>$6.4_{0.0}$</td>
<td>$\mathbf{1 2 . 9}_{0.0}$</td>
<td>$42.3_{28.8}$</td>
<td>$\mathbf{6 5 . 6}_{20.8}$</td>
<td>$75.0_{6.7}$</td>
<td>$\mathbf{8 3 . 4}_{4.2}$</td>
<td>$81.0_{8.8}$</td>
<td>$\mathbf{8 8 . 3}_{3.7}$</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>$4.0_{0.0}$</td>
<td>$\mathbf{5 . 0}_{0.0}$</td>
<td>$\mathbf{9 7 . 9}_{0.6}$</td>
<td>$95.5_{4.6}$</td>
<td>$\mathbf{9 8 . 0}_{0.6}$</td>
<td>$97.8_{0.7}$</td>
<td>$\mathbf{9 8 . 8}_{0.3}$</td>
<td>$\mathbf{9 8 . 8}_{0.3}$</td>
</tr>
</tbody>
</table>
<p>Table 1. Contextual calibration improves accuracy across a range of tasks. We show the mean and standard deviation across different choices of the training examples (the prompt format is fixed). The LM column indicates the GPT-3 size (see Appendix A for GPT-2 results). The Baseline column shows the standard approach of greedy decoding (Brown et al., 2020) and Ours corresponds to greedy decoding after modifying the output probabilities using contextual calibration. We bold the better result of the baseline and ours. MIT-G, MIT-D, ATIS-A, and ATIS-D indicate the MIT Genre, MIT Director, ATIS Airline, and ATIS Departure Date datasets.</p>
<h3>5.1 Results for Contextual Calibration</h3>
<p>Here, we evaluate the effectiveness of contextual calibration across all of our datasets and LMs. We first use a fixed prompt format and select five different random sets of training examples, placing them in an arbitrary order in the prompt. We do not artificially balance the labels of the training examples for the classification tasks. We use the same sets of training examples for the baseline (standard decoding without calibration) and contextual calibration. We use labeling budgets of $0-8$ examples; using more than 8 shots causes the cost of querying the OpenAI API to become prohibitively expensive.</p>
<p>Table 1 shows the results and Figure 1 in Section 1 plots the same data for a subset of the tasks.</p>
<p>Improves Mean And Worst-Case Accuracy Contextual calibration dramatically improves GPT-3's average and worst-case accuracy, by up to $30.0 \%$ absolute. These gains hold for both classification and generation tasks. Contextual calibration also sometimes allows GPT-3 2.7B to outperform the GPT-3 175B baseline-by up to 19.3\%-despite being over 50x smaller.</p>
<p>Can Reduce Variance Across Training Sets Figure 6 plots the difference in the standard deviation between the baseline and contextual calibration for all tasks from Table 1. Contextual calibration reduces the variance considerably in a majority of cases, and it does not increase variance by much in the remaining cases.</p>
<p>Reduces Drop from 0-shot to 1-shot For the baseline, there are four cases where there is a drop in accuracy when</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Std Dev of Contextual Calibration - Baseline
Figure 6. Aside from improving mean accuracy, contextual calibration also reduces the standard deviation of accuracy across different choices of the training examples. We plot the difference in standard deviation between contextual calibration and the baseline from Table 1.
moving from 0 -shot to 1-shot (TREC, AGNews, DBpedia, SST-2). We attribute this drop to the majority label bias (see discussion in Section 4). Calibration removes this drop in three out of four cases.</p>
<p>Improves GPT-2 We also test GPT-2 1.5B (see Table 4 in Appendix A). We find that like GPT-3, GPT-2's accuracy also highly varies across different prompts. This suggests that the variance that we observe for few-shot in-context learning is a general problem for LMs. Second, contextual calibration works out-of-the-box for GPT-2-it improves the mean accuracy and reduces variance for most tasks.</p>
<p>Improves Accuracy Across Formats In our next set of experiments, we use a fixed set of training examples and vary the prompt format. We use the 15 prompt formats for SST-2 discussed in Section 3. We also create 15 prompt formats for each of three random relations in LAMA (P20, P159, P19) by using the paraphrases of the original LAMA templates generated by Jiang et al. (2020b). Figure 7 shows the results before and after calibration for SST-2, and Figure 9 in Appendix A show the results for LAMA. Contextual calibration improves the average and worst-case accuracy for both datasets, and reduces the variance for SST-2.</p>
<h3>5.2. Ablations on Contextual Calibration</h3>
<p>We finally conduct two analyses/ablations on contextual calibration. We first analyze how effective contextual calibration is at inferring a good setting of $\mathbf{W}$. To do so, we compare its accuracy to an "oracle calibration" method that uses the validation set to find the best possible diagonal $\mathbf{W}$. We evaluate this oracle on AGNews, and find that contextual calibration is surprisingly close to it (Figure 8).</p>
<p>We also study how the choice of content-free input affects accuracy. In Table 3 in Appendix A, we show the accuracy for SST-2 and AGNews for different choices of the content-free input. The choice of content-free input matters, however, many good choices exist.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. GPT-3 has high variance across different prompt formats; contextual calibration reduces this variance and improves mean accuracy. We show the mean accuracy ( $\pm$ standard deviation) over 15 different prompt formats for SST-2.</p>
<h2>6. Discussion</h2>
<p>Does Calibration Eliminate the Need to Engineer Prompts? The motivation behind "prompt engineering" is that not all prompts lead to the same accuracy. Thus, one should tune the prompt's format and examples to achieve the best possible performance (Brown et al., 2020; Gao et al., 2020). Contextual calibration does not eliminate the need to engineer prompts, however, it does mitigate it: contextual calibration makes the accuracy of the best, average, and worst-case prompts more similar (and higher).</p>
<p>Should You Finetune in the Few-shot Setting? We use a fixed LM with no finetuning. As mentioned in Section 1, there are numerous reasons not to finetune: it enables rapid prototyping, provides a fully natural language interface, and is more efficient in terms of memory requirements and system complexity when serving many different tasks. Moreover, like in-context learning without contextual calibration, finetuning can be unstable in the few-shot setting (Schick \&amp; Schütze, 2021). Nevertheless, if these disadvantages are acceptable or avoidable, finetuning can improve accuracy over in-context learning in some cases (Schick \&amp; Schütze, 2020; Gao et al., 2020). An interesting direction for future work is to study the interplay between contextual calibration and finetuning, e.g., does contextual calibration alleviate the need to finetune, or vice versa?</p>
<h2>7. Related Work</h2>
<p>Few-shot Learning with Language Models Recent work uses LMs to solve NLP tasks, e.g., for story cloze prediction (Schwartz et al., 2017), knowledge base completion (Petroni et al., 2019), and Winograd schemas (Trinh \&amp; Le, 2018). Radford et al. (2019) and Brown et al. (2020)</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Contextual calibration, despite using no training data, achieves similar accuracy to an "oracle" calibration that finds the best $\mathbf{W}$ using the validation set. The plot shows GPT-3 175B's mean accuracy ( $\pm$ standard deviation) on AGNews over different choices of the training examples.
show that large LMs can be used to solve a myriad of tasks in a few-shot manner via in-context learning. Our paper provides a simple modification to their setting that improves performance. Asking LMs to complete natural language prompts is also used as a method to "probe" LMs, e.g., analyzing their factual (Petroni et al., 2019; Jiang et al., 2020b; Shin et al., 2020) or commonsense knowledge (Bosselut et al., 2019). Our results suggest that these probing methods may underestimate model accuracy, and we recommend that future work take advantage of contextual calibration.</p>
<p>Volatility of Few-shot Learning in NLP Recent work shows that when using masked language models such as BERT for zero-shot learning, the prompt format can impact accuracy (Petroni et al., 2019; Jiang et al., 2020b; Shin et al., 2020). Independent and concurrent work also shows that when finetuning masked language models on few examples, the choice of training examples can impact results (Schick \&amp; Schütze, 2020; Gao et al., 2020). We show that similar instabilities occur for in-context learning (i.e., no finetuning) with left-to-right language models. We also show a surprising instability associated with example ordering. Moreover, unlike past work, we analyze why these instabilities occur, and we use insights from this analysis to mitigate the issues.</p>
<p>Failures of Language Models We identify failures when LMs are used for in-context learning (e.g., recency bias). Past work identifies similar failures when LMs are used for text generation. For example, neural LMs often repeat themselves (Holtzman et al., 2020), suffer from overconfidence (Braverman et al., 2020; Jiang et al., 2020a), suffer from recency bias (Khandelwal et al., 2018; Ravfogel et al., 2019), and prefer generic responses instead of rare text ( Li et al., 2016; Logan et al., 2019). Past work mitigates these
degeneracies by modifying the model's output probabilities or generation schemes, e.g., explicitly preventing repetitions (Paulus et al., 2018) or using sampling instead of greedy decoding (Holtzman et al., 2020).</p>
<h2>8. Conclusion and Future Work</h2>
<p>We show that few-shot learning can be highly volatile across different choices of the prompt. Through a detailed analysis, we identify that this volatility arises from biases in LMs, e.g., their tendency to output recent or common tokens. We use these insights to develop contextual calibration-a simple procedure to adjust the model's output probabilities-which improves accuracy, reduces variance, and overall makes tools like GPT-3 more effective for end users.</p>
<p>Looking at the bigger picture, our results inspire two future research directions in few-shot learning for NLP. First, on the methods side, we show that good few-shot learning requires attention to detail: small but non-trivial decisions such as calibration can greatly influence results. This makes it difficult to correctly develop and compare new methods (e.g., pretraining schemes or model architectures). We thus hope to make other few-shot learning methods more robust, and also expand our techniques to cover a wider ranger of tasks (e.g., calibration for open-ended generation). Second, on the analysis side, our results highlight the need to understand what GPT-3 learns from the prompt. The model has an impressive ability to improve with more training examples, however, we show that the model learns some superficial patterns such as repetition of common answers. We hope to better understand and analyze the dynamics of in-context learning in future work.</p>
<h2>Acknowledgements</h2>
<p>We thank OpenAI for providing academic access to the GPT3 API. We thank Sewon Min, Nikhil Kandpal, Nelson Liu, Girish Sastry, Marco Tulio Ribeiro, and the members of Berkeley NLP for valuable feedback on the paper.</p>
<p>This work was supported by DARPA under the LwLL program/Grant No. FA8750-19-1-0504, DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, DARPA and the Air Force Research Laboratory (AFRL), and NSF award #IIS-1756023.</p>
<h2>References</h2>
<p>Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. COMET: Commonsense transformers for automatic knowledge graph construction. In $A C L, 2019$.</p>
<p>Braverman, M., Chen, X., Kakade, S., Narasimhan, K.,</p>
<p>Zhang, C., and Zhang, Y. Calibration, entropy rates, and memory in language models. In ICML, 2020.</p>
<p>Brier, G. W. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 1950.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In NeurIPS, 2020.</p>
<p>Dagan, I., Glickman, O., and Magnini, B. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Workshop, 2005.
de Marneffe, M.-C., Simons, M., and Tonhauser, J. The CommitmentBank: Investigating projection in naturally occurring discourse. In Sinn und Bedeutung, 2019.</p>
<p>Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.</p>
<p>Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020.</p>
<p>Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In ICML, 2017.</p>
<p>Hemphill, C. T., Godfrey, J. J., and Doddington, G. R. The ATIS spoken language systems pilot corpus. In Speech and Natural Language Workshop, 1990.</p>
<p>Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In $I C L R$, 2020.</p>
<p>Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can we know when language models know? arXiv preprint arXiv:2012.00955, 2020a.</p>
<p>Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we know what language models know? In TACL, 2020b.</p>
<p>Khandelwal, U., He, H., Qi, P., and Jurafsky, D. Sharp nearby, fuzzy far away: How neural language models use context. In $A C L, 2018$.</p>
<p>Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. In Science, 2015.</p>
<p>Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. A diversity-promoting objective function for neural conversation models. In NAACL, 2016.</p>
<p>Liu, J., Cyphers, S., Pasupat, P., McGraw, I., and Glass, J. A conversational movie search system based on conditional random fields. In INTERSPEECH, 2012.</p>
<p>Logan, R. L., Liu, N. F., Peters, M. E., Gardner, M., and Singh, S. Barack's wife Hillary: Using knowledge-graphs for fact-aware language modeling. In ACL, 2019.</p>
<p>Paulus, R., Xiong, C., and Socher, R. A deep reinforced model for abstractive summarization. In $I C L R, 2018$.</p>
<p>Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. Language models as knowledge bases? In EMNLP, 2019.</p>
<p>Platt, J. C. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, 1999.</p>
<p>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pretraining. Technical Report, 2018.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. Technical Report, 2019.</p>
<p>Ravfogel, S., Goldberg, Y., and Linzen, T. Studying the inductive biases of RNNs with synthetic variations of natural languages. In NAACL, 2019.</p>
<p>Schick, T. and Schütze, H. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020.</p>
<p>Schick, T. and Schütze, H. Exploiting cloze questions for few-shot text classification and natural language inference. In EACL, 2021.</p>
<p>Schwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y., and Smith, N. A. The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In $A C L, 2017$.</p>
<p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP, 2020.</p>
<p>Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.</p>
<p>Trinh, T. H. and Le, Q. V. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018.</p>
<p>Voorhees, E. M. and Tice, D. M. Building a question answering test collection. In SIGIR, 2000.</p>
<p>Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In NeurIPS, 2019.</p>
<p>Yogatama, D., d'Autume, C. d. M., Connor, J., Kocisky, T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L., Dyer, C., et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.</p>
<p>Zhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification. In NeurIPS, 2015.</p>
<h1>A. Additional Results on Variance and Calibration</h1>
<p>Table 2 shows an example of the sensitivity to ordering.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt (test input not shown)</th>
<th style="text-align: center;">Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Review: the whole thing 's fairly lame, making it par for <br> the course for disney sequels .</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: Negative</td>
<td style="text-align: center;">$88.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Review: this quiet, introspective and entertaining indepen- <br> dent is worth seeking .</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: Positive</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Review: this quiet, introspective and entertaining indepen- <br> dent is worth seeking .</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: Positive</td>
<td style="text-align: center;">$51.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Review: the whole thing 's fairly lame, making it par for <br> the course for disney sequels .</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2. Top: a prompt consisting of two training examples (the test input is not shown) that leads to good test accuracy for GPT-3 2.7B (88.5\%). Bottom: simply reversing the order of the two examples causes the accuracy to drop to near random chance ( $51.3 \%$ ).</p>
<p>Table 3 demonstrates that the choice of content-free input does affect accuracy, however, many good choices exist.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Content-free Input</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">AGNews</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Uncalibrated Baseline</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">48.5</td>
</tr>
<tr>
<td style="text-align: left;">N/A</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: left;">[MASK]</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: left;">$\cdot$</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">64.7</td>
</tr>
<tr>
<td style="text-align: left;">N/A, [MASK], '</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">66.5</td>
</tr>
<tr>
<td style="text-align: left;">the</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: left;">abc</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: left;">the man.</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: left;">dasjhasjkdhjskdhds</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: left;">nfjkhdvy84tr9bpuirvwe</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">65.5</td>
</tr>
</tbody>
</table>
<p>Table 3. We show the accuracy for 1-shot SST-2 and 0-shot AGNews over different choices for the content-free input. The choice of content-free input matters, however, many good choices exist. The token " indicates the empty string. Recall that in our experiments, we ensemble over N/A, [MASK], and the empty string.</p>
<p>Figure 9 shows how GPT-3 accuracy changes as the prompt format is varied for LAMA, with and without calibration.</p>
<p>Table 4 shows the effect of calibration for GPT-2.</p>
<h2>B. Prompt Formats Used</h2>
<p>Tables 5 and 6 show the default prompt format used for all tasks. Table 7 shows the 15 different formats used when studying the effect of prompt format for SST-2.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Contextual calibration improves GPT-3's accuracy across various prompt formats for LAMA. We plot GPT-2 2.7B's mean accuracy over 15 different formats for the LAMA "place of death" relation (P20), "Headquarter Location" relation (P159), and "place of birth" relation (P19).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">0-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">1-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">4-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">8-shot</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
</tr>
<tr>
<td style="text-align: center;">Text Classification</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$44.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 0}_{0.0}$</td>
<td style="text-align: center;">$45.4_{8.4}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 9}_{5.7}$</td>
<td style="text-align: center;">$44.6_{12.2}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 0}_{13.6}$</td>
<td style="text-align: center;">$57.1_{11.6}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 1}_{7.3}$</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$24.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 3}_{0.0}$</td>
<td style="text-align: center;">$21.5_{5.2}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 1}_{2.6}$</td>
<td style="text-align: center;">$23.1_{5.9}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 2}_{2.2}$</td>
<td style="text-align: center;">$32.7_{7.5}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 1}_{3.6}$</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$\mathbf{4 4 . 6}_{0.0}$</td>
<td style="text-align: center;">$17.9_{0.0}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 6}_{10.0}$</td>
<td style="text-align: center;">$47.1_{12.2}$</td>
<td style="text-align: center;">$40.0_{8.3}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 4}_{7.3}$</td>
<td style="text-align: center;">$48.9_{5.7}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 2}_{1.4}$</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$\mathbf{5 1 . 0}_{0.0}$</td>
<td style="text-align: center;">$48.5_{0.0}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 6}_{2.1}$</td>
<td style="text-align: center;">$56.3_{2.4}$</td>
<td style="text-align: center;">$53.2_{6.0}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 5}_{1.8}$</td>
<td style="text-align: center;">$54.9_{3.0}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 7}_{1.29}$</td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$60.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 0}_{0.0}$</td>
<td style="text-align: center;">$66.7_{17.9}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 0}_{11.4}$</td>
<td style="text-align: center;">$64.9_{8.4}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 8}_{10.9}$</td>
<td style="text-align: center;">$54.5_{4.6}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 6}_{8.8}$</td>
</tr>
<tr>
<td style="text-align: center;">DBPedia</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$\mathbf{6 4 . 3}_{0.0}$</td>
<td style="text-align: center;">$58.3_{0.0}$</td>
<td style="text-align: center;">$33.6_{18.9}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 5}_{9.4}$</td>
<td style="text-align: center;">$53.0_{14.8}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 3}_{8.1}$</td>
<td style="text-align: center;">$66.0_{3.6}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 3}_{8.7}$</td>
</tr>
<tr>
<td style="text-align: center;">Fact Retrieval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LAMA</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$14.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 7}_{0.0}$</td>
<td style="text-align: center;">$29.7_{1.8}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 6}_{1.3}$</td>
<td style="text-align: center;">$35.8_{3.8}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 4}_{3.4}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 5}_{1.3}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 5}_{1.4}$</td>
</tr>
<tr>
<td style="text-align: center;">Information Extraction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MIT-G</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$7.7_{0.0}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 0}_{0.0}$</td>
<td style="text-align: center;">$32.9_{10.0}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 2}_{4.1}$</td>
<td style="text-align: center;">$44.3_{6.5}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 7}_{5.8}$</td>
<td style="text-align: center;">$56.9_{2.5}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 5}_{2.5}$</td>
</tr>
<tr>
<td style="text-align: center;">MIT-D</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$29.3_{0.0}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 7}_{0.0}$</td>
<td style="text-align: center;">$26.2_{10.5}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 8}_{4.8}$</td>
<td style="text-align: center;">$70.5_{2.5}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 4}_{1.8}$</td>
<td style="text-align: center;">$77.1_{4.4}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 1}_{3.9}$</td>
</tr>
<tr>
<td style="text-align: center;">ATIS-A</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$15.1_{0.0}$</td>
<td style="text-align: center;">$\mathbf{3 5 . 5}_{0.0}$</td>
<td style="text-align: center;">$41.5_{11.7}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 4}_{7.5}$</td>
<td style="text-align: center;">$55.1_{18.9}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 8}_{11.7}$</td>
<td style="text-align: center;">$63.4_{10.6}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 9}_{10.4}$</td>
</tr>
<tr>
<td style="text-align: center;">ATIS-D</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$1.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{2 . 5}_{0.0}$</td>
<td style="text-align: center;">$62.3_{9.2}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 7}_{4.3}$</td>
<td style="text-align: center;">$81.1_{3.6}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 2}_{7.2}$</td>
<td style="text-align: center;">$81.8_{4.5}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 9}_{5.0}$</td>
</tr>
</tbody>
</table>
<p>Table 4. Contextual calibration improves accuracy for GPT-2. This table is analogous to Table 1 but shows results for GPT-2 XL.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Label Names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Sentiment: Positive <br> Review: Horrific movie, don't see it. <br> Sentiment:</td>
<td style="text-align: center;">Positive, Negative</td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">Article: USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump. <br> Answer: Business <br> Article: New hard-drive based devices feature color screens, support for WMP 10. <br> Answer:</td>
<td style="text-align: center;">World, Sports, Business, Technology</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">Classify the questions based on whether their answer type is a Number, Location, Person, Description, Entity, or Abbreviation. <br> Question: How did serfdom develop in and then leave Russia? <br> Answer Type: Description <br> Question: When was Ozzy Osbourne born? <br> Answer Type:</td>
<td style="text-align: center;">Number, Location, Person, Description, Entity, Abbreviation</td>
</tr>
<tr>
<td style="text-align: center;">DBPedia</td>
<td style="text-align: center;">Classify the documents based on whether they are about a Company, School, Artist, Athlete, Politician, Transportation, Building, Nature, Village, Animal, Plant, Album, Film, or Book. <br> Article: Geoffrey D. Falksen (born July 31 1982) is an American steampunk writer. <br> Answer: Artist <br> Article: The Perrin River is a 1.3-mile-long ( 2.1 km ) tidal river in the U.S. state of Virginia. It is a small inlet on the north shore of the York River near that river's mouth at Chesapeake Bay. <br> Answer:</td>
<td style="text-align: center;">Company, School, Artist, Athlete, Politician, Transportation, Building, Nature, Village, Animal, Plant, Album, Film, Book</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">But he ended up eating it himself. I was reluctant to kiss my mother, afraid that somehow her weakness and unhappiness would infect me. Naturally I didn't think for a minute that my life and spirit could stimulate her. <br> question: her life and spirit could stimulate her mother. True, False, or Neither? answer: Neither <br> Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? <br> question: Valence was helping. True, False, or Neither? answer:</td>
<td style="text-align: center;">True, False, Neither</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">Others argue that Mr. Sharon should have negotiated the Gaza pullout - both to obtain at least some written promises of better Palestinian behavior, and to provide Mr. Abbas with a prime prize to show his people that diplomacy, not violence, delivered Gaza. question: Mr. Abbas is a member of the Palestinian family. True or False? answer: False <br> The program will include Falla's "Night in the Gardens of Spain," Ravel's Piano Concerto in G, Berlioz's Overture to "Beatrice and Benedict," and Roy Harris' Symphony No. 3. question: Beatrice and Benedict is an overture by Berlioz. True or False? answer:</td>
<td style="text-align: center;">True, False</td>
</tr>
</tbody>
</table>
<p>Table 5. The prompts used for text classification. We show one training example per task for illustration purposes. The right column shows the label names (to make predictions, we check the LM's probability for these tokens).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LAMA</td>
<td style="text-align: center;">Alexander Berntsson was born in Sweden <br> Khalid Karami was born in</td>
</tr>
<tr>
<td style="text-align: center;">ATIS <br> (Airline)</td>
<td style="text-align: center;">Sentence: what are the two american airlines flights that leave from dallas to san francisco in the evening Airline name: american airlines <br> Sentence: list a flight on american airlines from toronto to san diego Airline name:</td>
</tr>
<tr>
<td style="text-align: center;">ATIS <br> (Depart Date)</td>
<td style="text-align: center;">Sentence: please list any flight available leaving oakland california tuesday arriving philadelphia wednesday Depart date - Day name: tuesday <br> Sentence: show me all all flights from pittsburgh to atlanta on wednesday which leave before noon and serve breakfast <br> Depart date - Day name:</td>
</tr>
<tr>
<td style="text-align: center;">MIT Movies (Genre)</td>
<td style="text-align: center;">Sentence: last to a famous series of animated movies about a big green ogre and his donkey and cat friends Genre: animated <br> Sentence: what is a great comedy featuring the talents of steve carell as a loser looking for a friend Genre:</td>
</tr>
<tr>
<td style="text-align: center;">MIT Movies <br> (Director)</td>
<td style="text-align: center;">Sentence: in 2005 director christopher nolan rebooted a legendary dc comics superhero with a darker grittier edge in which movie <br> Director: christopher nolan <br> Sentence: what 1967 mike nichols film features dustin hoffman in romantic interludes with anne bancroft as mrs robinson <br> Director:</td>
</tr>
</tbody>
</table>
<p>Table 6. The prompts used for generation tasks. We show one training example per task for illustration purposes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Format ID</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Label Names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Answer: Positive <br> Review: Horrific movie, don't see it. <br> Answer:</td>
<td style="text-align: center;">Positive, Negative</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Answer: good <br> Review: Horrific movie, don't see it. <br> Answer:</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">My review for last night's film: This movie is amazing! The critics agreed that this movie was good My review for last night's film: Horrific movie, don't see it. The critics agreed that this movie was</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Here is what our critics think for this month's films. <br> One of our critics wrote "This movie is amazing!". Her sentiment towards the film was positive. <br> One of our critics wrote "Horrific movie, don't see it". Her sentiment towards the film was</td>
<td style="text-align: center;">positive, negative</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Critical reception [ edit ] <br> In a contemporary review, Roger Ebert wrote "This movie is amazing!". Entertainment Weekly agreed, and the overall critical reception of the film was good. <br> In a contemporary review, Roger Ebert wrote "Horrific movie, don't see it". Entertainment Weekly agreed, and the overall critical reception of the film was</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Positive Review? Yes <br> Review: Horrific movie, don't see it. <br> Positive Review?</td>
<td style="text-align: center;">Yes, No</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Question: Is the sentiment of the above review Positive or Negative? <br> Answer: Positive <br> Review: This movie is amazing! <br> Question: Is the sentiment of the above review Positive or Negative? <br> Answer:</td>
<td style="text-align: center;">Positive, Negative</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Question: Did the author think that the movie was good or bad? <br> Answer: good <br> Review: This movie is amazing! <br> Question: Did the author think that the movie was good or bad? <br> Answer:</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Question: Did the author of the following tweet think that the movie was good or bad? <br> Tweet: This movie is amazing! <br> Answer: good <br> Question: Did the author of the following tweet think that the movie was good or bad? <br> Tweet: Horrific movie, don't see it <br> Answer:</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">This movie is amazing! My overall feeling was that the movie was good Horrific movie, don't see it. My overall feeling was that the movie was</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">This movie is amazing! I liked the movie. <br> Horrific movie, don't see it. I</td>
<td style="text-align: center;">liked, hated</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">This movie is amazing! My friend asked me if I would give the movie 0 or 5 stars, I said 5 Horrific movie, don't see it. My friend asked me if I would give the movie 0 or 5 stars, I said</td>
<td style="text-align: center;">0,5</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Input: This movie is amazing! <br> Sentiment: Positive <br> Input: Horrific movie, don't see it. <br> Sentiment:</td>
<td style="text-align: center;">Positive, Negative</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Positive: True <br> Review: Horrific movie, don't see it. <br> Positive:</td>
<td style="text-align: center;">True, False</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Stars: 5 <br> Review: Horrific movie, don't see it. <br> Stars:</td>
<td style="text-align: center;">5,0</td>
</tr>
</tbody>
</table>
<p>Table 7. The different prompt formats used when studying the effect of format for SST-2. We show one training example for illustration.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The frequency of a token on the web is calculated using Google Ngrams https://books.google.com/ngrams. The predictions are from the 0 -shot setting on the validation set.
${ }^{4}$ The output of GPT-3 is biased (its outputs are shifted), similar to how measurement devices such as voltage meters or weighing scales are biased. Just like how these devices require "calibration before use", where the devices' outputs are scaled/zeroed-out, we hope to apply a similar calibration procedure to LMs. This goal is distinct from statistical calibration (Brier, 1950; Guo et al., 2017), i.e., aligning a model's confidence estimate with its true accuracy.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ This affine transformation is usually applied to the logits, i.e., prior to the softmax. However, we only have access to GPT-3's output probabilities in the OpenAI API.
${ }^{6}$ We only calibrate the prediction of the first output token for generation tasks. This is reasonable because, for the tasks we consider, we found that the model's predictions are highly deterministic after generating the first token.
${ }^{7}$ An alternate solution is to set $\mathbf{b}$ to $-\hat{\mathbf{p}}_{\mathrm{cf}}$ and $\mathbf{W}$ to the identity. Empirically, this alternate solution yields higher accuracy for generation tasks (where the dimensionality of $\hat{\mathbf{p}}$ is large). The solution in the main text performs better for classification.
${ }^{8}$ We found this simple ensemble to achieve the best results for AGNews, and we reuse it for all other datasets. See Section 5.2 for an ablation on the choice of content-free input.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
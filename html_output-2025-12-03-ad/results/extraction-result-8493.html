<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8493 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8493</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8493</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-f52af5abe78ca2af836f70ce193f0161bc2e6264</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f52af5abe78ca2af836f70ce193f0161bc2e6264" target="_blank">clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions.</p>
                <p><strong>Paper Abstract:</strong> Recent work has proposed a methodology for the systematic evaluation of"Situated Language Understanding Agents"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value. Our general framework for implementing and evaluating games with LLMs is available at https://github.com/clembench .</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8493.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8493.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Priv/Shared (Scorekeeping) - GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Private/Shared Scorekeeping Game (GPT-4 agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probing two-part game in which an answerer agent (here GPT-4) participates in a primary Q&A interaction while a Game Master intermittently probes the agent for what it believes the questioner knows; used to evaluate the agent's ability to keep and update a model of shared vs private information (slotkeeping).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 (as Answerer Agent in Private/Shared)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A chat-optimized LLM playing the answerer role in the Private/Shared scorekeeping game; it must fill slots in a form and respond to GM probes about which slots have been shared with the questioner.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>OpenAI GPT-4, instruction-/chat-optimized large transformer model (closed-access); reported as the top-performing model in the benchmark for many games and strong at following prompts and slot-filling.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Private/Shared (Scorekeeping) Game (clembench v1.0)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A dialogue game that requires the agent to perform slot-filling in a primary interaction and to answer binary probes (shared/not-shared) about what the other interlocutor knows; evaluates maintenance and update of an agent model and conversational grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>conversational slot-memory / agent-model (short-term episodic conversational memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Implicit memory implemented by (1) the LLM's internal context/state (hidden activations) plus (2) the explicit dialogue history supplied in the prompt; the Game Master separately maintains the ground-truth slot state and issues probes referencing that state.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>The agent receives the dialogue history (concatenated turns) and GM probes in its context window; it uses this context to answer probes and to fill slots. The Game Master supplies structured prompts (turn tags) and probes to elicit the agent's memory-based judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Quality score 90.79 (mean, percent scale 0-100) with standard deviation 8.2 (reported for GPT-4 self-play on the private/shared game).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No explicit ablation of memory vs no-memory. The paper compares models (GPT-4, GPT-3.5, Claude, etc.) on the same game to show relative ability to keep and report shared information; Claude and GPT-4 performed best on slot-filling and probing metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not explicitly identified as an architectural memory; empirically, models that are better aligned to prompts (GPT-4, Claude) and that adhere to structured turn tags perform best. The authors highlight structured prompting (tags for turns/slots) and using the Game Master to enforce formats as effective practice.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Common failure modes include (a) failing to use the required player tag (causing abortions), (b) anticipating or inventing slots/turns (pretending information was shared), and (c) mistakenly marking shared slots as still private; reprompting sometimes corrected tagging errors. No external persistent memory mechanism was used.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use structured turn/slot tags enforced by a Game Master; reprompting can fix tagging errors; RLHF/instruction-tuned models perform better at maintaining the conversational state; the Game Master should keep canonical slot state and probe the model to measure its internal representation of shared knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8493.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8493.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wordle Memory Integration - GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wordle Variants Testing Inter-Turn Feedback Integration (GPT-4 agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Word-guessing (Wordle-like) experiments that probe an agent's ability to integrate per-turn letter-feedback into future guesses; variants include plain Wordle, Wordle+clue, and Wordle+clue+critic (critic provides agreement/disagreement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 (as Guesser)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM acting as the guesser in Wordle variants; must produce valid 5-letter guesses, interpret letter-feedback across turns, and optionally incorporate a semantic clue or a critic's opinion.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>OpenAI GPT-4, chat-optimized transformer with strong instruction-following capabilities and larger context-handling than earlier GPT series (closed access).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Wordle / Wordle+Clue / Wordle+Clue+Critic (clembench v1.0)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Turn-based single-player word-guessing games where after each guess the agent receives letter-position feedback (green/yellow/red). Variants add an initial semantic clue and a separate critic agent that agrees/disagrees; the task tests use of iterative feedback (working memory) and integration of semantic clues.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working short-term memory (inter-turn feedback history stored in context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>No explicit external memory module; memory is the concatenated sequence of previous guesses and the corresponding letter-feedback provided in the prompt history (within the model's context window). The critic variant adds another agent's opinion as additional context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>The agent must read prior guesses and the guess_feedback supplied by the Game Master in the context and use that to constrain the next guess. In the critic variant an extra message (agreement/disagreement + rationale) is also concatenated into context and used as an additional signal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Traditional Wordle success 0.23 (23% of episodes) and speed metric 3.67 (100/t average -> corresponds to ~turn 4) for GPT-4; with clue success 0.73 and speed 49.67 (indicating average success ~turn 2); with clue+critic success 0.80 and speed 49.11 (reported in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparisons across variants act like an ablation: traditional vs +clue vs +clue+critic show that providing semantic clues and/or a critic greatly improves GPT-4's success and speed, indicating that additional structured context or external critique compensates for poor iterative-feedback integration. No explicit removal of context-history (memory) baseline was run.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Providing structured extra information (semantic clue) and an explicit critic/self-critique yields the best empirical performance for GPT-4 in this task; these act as externalized signals that augment or scaffold the agent's implicit working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Many models (including several open-source ones) failed to incorporate letter-feedback across turns, repeating letters from earlier guesses; high abort rates for some models due to formatting errors; GPT-4 still solves relatively few traditional Wordle episodes without additional clues/critic, indicating imperfect inter-turn memory/integration.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>To improve iterative-feedback tasks, (a) provide semantic scaffolding (clues) or a critic to externalize/clarify constraints, (b) enforce strict structured formats via a Game Master, and (c) prefer instruction-tuned/RLHF models (GPT-4/GPT-3.5/Claude) which better follow turn structure. The authors note the need to address models' failure to accumulate feedback across turns (a key memory/integration shortcoming).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8493.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8493.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Game Master / Context-Window Memory Scaffold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Programmatic Game Master as External State and Context-Window Constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The clembench framework uses a programmatic Game Master (GM) to maintain canonical game state (slots, target words, feedback), validate agent moves, and present only formally correct, game-relevant context to LLM players; this both enforces rules and functions as an externalized memory scaffold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Game Master + cLLM players (framework component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The Game Master is a programmatic component that tracks game state, parses and validates LLM outputs, enforces move formats, and supplies structured prompts containing the relevant context (serving as external memory to the LLM players).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>various (GPT-4, GPT-3.5, Claude, open models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Multiple chat-optimized and open models were evaluated in self-play within the framework; none of the agents used a distinct external memory module beyond the GM-managed history and prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>clembench (framework-wide usage across games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A benchmark/framework for turn-based text/dialogue games where the Game Master stores ground-truth state and feeds structured, validated context to LLM players; challenges include turn-based state updates, format adherence, and context-window management.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external programmatic state (Game Master) + context-window (LLM internal)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>The GM keeps authoritative state (targets, taboo words, slot ground truth, letter-feedback) in program memory; for each turn it injects the necessary state into the prompt. The LLM's memory of past turns is the concatenated context window containing previous GM messages and agent turns.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>The GM integrates memory by (a) holding canonical state and (b) serializing the relevant parts into the prompt each turn; the LLM integrates this provided context into its next output via standard context attention mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using the GM to provide structured context enabled gameplay; many performance metrics reported (e.g., higher % played and quality when GM enforced format). No numeric 'with vs without GM' ablation is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No direct ablation removing the Game Master was performed. The paper argues the GM is necessary to constrain general-purpose LLMs to the game and to obtain measurable, comparable behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Authors recommend using a programmatic GM to maintain canonical state and to enforce structured, parseable turn formats; this externalized state + strict prompting is the practical best-practice in their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Context-window limits (token budgets) can cause early dialogue history (including instructions) to be cropped, which degrades performance and can lead to aborted games; long LLM outputs (hallucinated appended turns) can break the expected per-turn format and cause game abortion.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Maintain concise prompts and set maximum tokens to avoid cropping important instructions; keep the GM authoritative for state to avoid inconsistent agent-internal memories; use strict, parseable tagging for turns; consider additional scaffolds (critic, clues) to reduce reliance on fragile inter-turn memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can visual dialogue models do scorekeeping? exploring how dialogue representations incrementally encode shared knowledge. <em>(Rating: 2)</em></li>
                <li>Language models as agent models <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Dialogue games for benchmarking language understanding: Motivation, taxonomy, strategy <em>(Rating: 2)</em></li>
                <li>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8493",
    "paper_id": "paper-f52af5abe78ca2af836f70ce193f0161bc2e6264",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "Priv/Shared (Scorekeeping) - GPT-4",
            "name_full": "Private/Shared Scorekeeping Game (GPT-4 agent)",
            "brief_description": "A probing two-part game in which an answerer agent (here GPT-4) participates in a primary Q&A interaction while a Game Master intermittently probes the agent for what it believes the questioner knows; used to evaluate the agent's ability to keep and update a model of shared vs private information (slotkeeping).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 (as Answerer Agent in Private/Shared)",
            "agent_description": "A chat-optimized LLM playing the answerer role in the Private/Shared scorekeeping game; it must fill slots in a form and respond to GM probes about which slots have been shared with the questioner.",
            "llm_model_name": "GPT-4",
            "llm_model_description": "OpenAI GPT-4, instruction-/chat-optimized large transformer model (closed-access); reported as the top-performing model in the benchmark for many games and strong at following prompts and slot-filling.",
            "benchmark_name": "Private/Shared (Scorekeeping) Game (clembench v1.0)",
            "benchmark_description": "A dialogue game that requires the agent to perform slot-filling in a primary interaction and to answer binary probes (shared/not-shared) about what the other interlocutor knows; evaluates maintenance and update of an agent model and conversational grounding.",
            "memory_used": true,
            "memory_type": "conversational slot-memory / agent-model (short-term episodic conversational memory)",
            "memory_architecture": "Implicit memory implemented by (1) the LLM's internal context/state (hidden activations) plus (2) the explicit dialogue history supplied in the prompt; the Game Master separately maintains the ground-truth slot state and issues probes referencing that state.",
            "memory_integration_strategy": "The agent receives the dialogue history (concatenated turns) and GM probes in its context window; it uses this context to answer probes and to fill slots. The Game Master supplies structured prompts (turn tags) and probes to elicit the agent's memory-based judgments.",
            "performance_with_memory": "Quality score 90.79 (mean, percent scale 0-100) with standard deviation 8.2 (reported for GPT-4 self-play on the private/shared game).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No explicit ablation of memory vs no-memory. The paper compares models (GPT-4, GPT-3.5, Claude, etc.) on the same game to show relative ability to keep and report shared information; Claude and GPT-4 performed best on slot-filling and probing metrics.",
            "best_memory_strategy": "Not explicitly identified as an architectural memory; empirically, models that are better aligned to prompts (GPT-4, Claude) and that adhere to structured turn tags perform best. The authors highlight structured prompting (tags for turns/slots) and using the Game Master to enforce formats as effective practice.",
            "limitations_or_failure_cases": "Common failure modes include (a) failing to use the required player tag (causing abortions), (b) anticipating or inventing slots/turns (pretending information was shared), and (c) mistakenly marking shared slots as still private; reprompting sometimes corrected tagging errors. No external persistent memory mechanism was used.",
            "recommendations_or_conclusions": "Use structured turn/slot tags enforced by a Game Master; reprompting can fix tagging errors; RLHF/instruction-tuned models perform better at maintaining the conversational state; the Game Master should keep canonical slot state and probe the model to measure its internal representation of shared knowledge.",
            "uuid": "e8493.0",
            "source_info": {
                "paper_title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Wordle Memory Integration - GPT-4",
            "name_full": "Wordle Variants Testing Inter-Turn Feedback Integration (GPT-4 agent)",
            "brief_description": "Word-guessing (Wordle-like) experiments that probe an agent's ability to integrate per-turn letter-feedback into future guesses; variants include plain Wordle, Wordle+clue, and Wordle+clue+critic (critic provides agreement/disagreement).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 (as Guesser)",
            "agent_description": "LLM acting as the guesser in Wordle variants; must produce valid 5-letter guesses, interpret letter-feedback across turns, and optionally incorporate a semantic clue or a critic's opinion.",
            "llm_model_name": "GPT-4",
            "llm_model_description": "OpenAI GPT-4, chat-optimized transformer with strong instruction-following capabilities and larger context-handling than earlier GPT series (closed access).",
            "benchmark_name": "Wordle / Wordle+Clue / Wordle+Clue+Critic (clembench v1.0)",
            "benchmark_description": "Turn-based single-player word-guessing games where after each guess the agent receives letter-position feedback (green/yellow/red). Variants add an initial semantic clue and a separate critic agent that agrees/disagrees; the task tests use of iterative feedback (working memory) and integration of semantic clues.",
            "memory_used": true,
            "memory_type": "working short-term memory (inter-turn feedback history stored in context window)",
            "memory_architecture": "No explicit external memory module; memory is the concatenated sequence of previous guesses and the corresponding letter-feedback provided in the prompt history (within the model's context window). The critic variant adds another agent's opinion as additional context.",
            "memory_integration_strategy": "The agent must read prior guesses and the guess_feedback supplied by the Game Master in the context and use that to constrain the next guess. In the critic variant an extra message (agreement/disagreement + rationale) is also concatenated into context and used as an additional signal.",
            "performance_with_memory": "Traditional Wordle success 0.23 (23% of episodes) and speed metric 3.67 (100/t average -&gt; corresponds to ~turn 4) for GPT-4; with clue success 0.73 and speed 49.67 (indicating average success ~turn 2); with clue+critic success 0.80 and speed 49.11 (reported in Table 4).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Direct comparisons across variants act like an ablation: traditional vs +clue vs +clue+critic show that providing semantic clues and/or a critic greatly improves GPT-4's success and speed, indicating that additional structured context or external critique compensates for poor iterative-feedback integration. No explicit removal of context-history (memory) baseline was run.",
            "best_memory_strategy": "Providing structured extra information (semantic clue) and an explicit critic/self-critique yields the best empirical performance for GPT-4 in this task; these act as externalized signals that augment or scaffold the agent's implicit working memory.",
            "limitations_or_failure_cases": "Many models (including several open-source ones) failed to incorporate letter-feedback across turns, repeating letters from earlier guesses; high abort rates for some models due to formatting errors; GPT-4 still solves relatively few traditional Wordle episodes without additional clues/critic, indicating imperfect inter-turn memory/integration.",
            "recommendations_or_conclusions": "To improve iterative-feedback tasks, (a) provide semantic scaffolding (clues) or a critic to externalize/clarify constraints, (b) enforce strict structured formats via a Game Master, and (c) prefer instruction-tuned/RLHF models (GPT-4/GPT-3.5/Claude) which better follow turn structure. The authors note the need to address models' failure to accumulate feedback across turns (a key memory/integration shortcoming).",
            "uuid": "e8493.1",
            "source_info": {
                "paper_title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Game Master / Context-Window Memory Scaffold",
            "name_full": "Programmatic Game Master as External State and Context-Window Constraints",
            "brief_description": "The clembench framework uses a programmatic Game Master (GM) to maintain canonical game state (slots, target words, feedback), validate agent moves, and present only formally correct, game-relevant context to LLM players; this both enforces rules and functions as an externalized memory scaffold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Game Master + cLLM players (framework component)",
            "agent_description": "The Game Master is a programmatic component that tracks game state, parses and validates LLM outputs, enforces move formats, and supplies structured prompts containing the relevant context (serving as external memory to the LLM players).",
            "llm_model_name": "various (GPT-4, GPT-3.5, Claude, open models)",
            "llm_model_description": "Multiple chat-optimized and open models were evaluated in self-play within the framework; none of the agents used a distinct external memory module beyond the GM-managed history and prompt context.",
            "benchmark_name": "clembench (framework-wide usage across games)",
            "benchmark_description": "A benchmark/framework for turn-based text/dialogue games where the Game Master stores ground-truth state and feeds structured, validated context to LLM players; challenges include turn-based state updates, format adherence, and context-window management.",
            "memory_used": true,
            "memory_type": "external programmatic state (Game Master) + context-window (LLM internal)",
            "memory_architecture": "The GM keeps authoritative state (targets, taboo words, slot ground truth, letter-feedback) in program memory; for each turn it injects the necessary state into the prompt. The LLM's memory of past turns is the concatenated context window containing previous GM messages and agent turns.",
            "memory_integration_strategy": "The GM integrates memory by (a) holding canonical state and (b) serializing the relevant parts into the prompt each turn; the LLM integrates this provided context into its next output via standard context attention mechanisms.",
            "performance_with_memory": "Using the GM to provide structured context enabled gameplay; many performance metrics reported (e.g., higher % played and quality when GM enforced format). No numeric 'with vs without GM' ablation is reported.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No direct ablation removing the Game Master was performed. The paper argues the GM is necessary to constrain general-purpose LLMs to the game and to obtain measurable, comparable behaviour.",
            "best_memory_strategy": "Authors recommend using a programmatic GM to maintain canonical state and to enforce structured, parseable turn formats; this externalized state + strict prompting is the practical best-practice in their setup.",
            "limitations_or_failure_cases": "Context-window limits (token budgets) can cause early dialogue history (including instructions) to be cropped, which degrades performance and can lead to aborted games; long LLM outputs (hallucinated appended turns) can break the expected per-turn format and cause game abortion.",
            "recommendations_or_conclusions": "Maintain concise prompts and set maximum tokens to avoid cropping important instructions; keep the GM authoritative for state to avoid inconsistent agent-internal memories; use strict, parseable tagging for turns; consider additional scaffolds (critic, clues) to reduce reliance on fragile inter-turn memory.",
            "uuid": "e8493.2",
            "source_info": {
                "paper_title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can visual dialogue models do scorekeeping? exploring how dialogue representations incrementally encode shared knowledge.",
            "rating": 2
        },
        {
            "paper_title": "Language models as agent models",
            "rating": 2
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Dialogue games for benchmarking language understanding: Motivation, taxonomy, strategy",
            "rating": 2
        },
        {
            "paper_title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "rating": 1
        }
    ],
    "cost": 0.0157905,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents</h1>
<p>Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov<br>Brielen Madureira, Philipp Sadler, David Schlangen ${ }^{1}$<br>Computational Linguistics, Department of Linguistics<br>University of Potsdam, Germany<br>${ }^{1}$ German Research Center for Artificial Intelligence (DFKI), Berlin, Germany<br>first.last@uni-potsdam.de</p>
<h4>Abstract</h4>
<p>Recent work has proposed a methodology for the systematic evaluation of "Situated Language Understanding Agents"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chatoptimised LLMs are, to an extent, capable of following game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models generally performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value. Our general framework for implementing and evaluating games with LLMs is available at https://github.com/clembench.</p>
<h2>1 Introduction</h2>
<p>There is an old joke:
A guy has a dog that plays checkers. "My goodness," everyone says, "that's amazing. What a brilliant dog!" - "Not really," he replies, "I beat him four games out of five."
This joke nicely reflects where we are with</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An episode of the taboo word game
interaction-tuned language models such as ChatGPT and GPT-4 (OpenAI, 2023). ${ }^{1,2}$ While the public discussion is dominated by what amounts to an unguided breadth-first search of tasks that can be "done" by these models (seeing "sparks" of generality in the process, Bubeck et al. (2023)), systematic investigations into how well these tasks are actually done, when looked at in depth, are only now beginning to appear (Liu et al., 2023; Bang et al., 2023)— often with results not dissimilar to what disappoints the dog owner in the joke, who apparently is looking for a challenging checkers partner and not a clever dog.</p>
<p>In this paper, we take the analogy even further and indeed look at how well these models can play interactive, language-based games, like that illustrated in Figure 1. In recent work, Schlangen</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>(2023a) has argued that such Dialogue Games ("constructed activities driven by language use") are a good systematic way of probing for the situated language understanding of language-using agents. In other recent work, Andreas (2022) has argued that LLMs are models of such agents. We bring these claims together and investigate what we can learn about the capabilities of cLLMs by exposing them to constrained game-like settings. Beyond making it possible to control the buildup of context in which to interpret the language, the game setting also has the advantage that we can generate novel instances that are unlikely to have been seen in any kind of training data, even if the game itself may have been. We describe a framework for implementing such games in a way that they can be tested in self-play of cLLMs-through the use of a programmatic "Game Master" that controls the game flow, as in the example in Figure 1-and we show results for five cooperative games that we have implemented in this framework, testing as game play agents the models Anthropic Claude, AlephAlpha Luminous, OpenAI GPT3, GPT3.5, GPT4 and open access ones such as Falcon, OpenAssistant, Vicuna and Koala. ${ }^{3}$</p>
<p>Our main findings are:</p>
<ul>
<li>Game instruction following in the best models generally is good, and is what marks the difference between models such as GPT-3 and newer models; likely as an effect of instruction tuning (Wei et al., 2022; Zhong et al., 2021) and learning from human feedback (Ouyang et al., 2022; Stiennon et al., 2020);</li>
<li>The performance differences across games tracks the development cycle, with newer models generally performing better;</li>
<li>The performance metrics are not saturated; and under the reasonable assumption that human performance would be near the ceiling, there is a wide gap between model performance and this.
Our contributions are:</li>
<li>A flexible, extensible framework for the implementation of Dialogue Games as test instruments, which enables fast evaluation on a large (and extensible) set of models. The code repository is available via: https://github.com/
${ }^{3}$ gpt4: (OpenAI, 2023); gpt3.5: (Ouyang et al., 2022); gpt3: (Brown et al., 2020); claude: (Bai et al., 2022); luminous-supreme: (AlephAlpha, 2023); falcon-40binstruct (Almazrouci et al., 2023), open-assistant-12b (Köpf et al., 2023), vicuna-12b (Chiang et al., 2023) and koala13b (Geng et al., 2023)
<img alt="img-1.jpeg" src="img-1.jpeg" /></li>
</ul>
<p>Figure 2: Anchoring Processes and Representational Domains from (Schlangen, 2023a,b) (left), and links to Dialogue Games described here
clembench.</p>
<ul>
<li>A collection of implemented and well-motivated games, together constituting version 1.0 of what we call the clem benchmark.</li>
<li>An in-depth evaluation of the performance of current state-of-the-art cLLMs on these games.</li>
</ul>
<h2>2 Background: Situated Agents, Dialogue Games, and LLMs as Agent Models</h2>
<p>Schlangen (2023a) introduces Dialogue Games as follows:</p>
<p>A Dialogue Game is a constructed activity with a clear beginning and end, in which players attempt to reach a predetermined goal state primarily by means of producing and understanding linguistic material.</p>
<p>The claim is that such Dialogue Games can serve as valid instruments for evaluating models of situated language understanding, provided that an argument can be given for how a specific game challenges aspects of the underlying construct. As a model of this (not directly observable, but to be measured) construct he proposes what is illustrated here in in Figure 2, which analyses situated language understanding into a number of representational and procedural demands. Rather than going through these in detail here, we will illustrate them through the discussion of how the implemented games challenge these various aspects.</p>
<p>Andreas (2022) argues that LLMs "infer approximate, partial representations of the beliefs, desires, and intentions possessed by the agent that produced the context". If that is so, and if the finer-grained</p>
<p>analysis of the relevant beliefs, desires, and intentions involved in game play that we reviewed in the previous paragraph is on the right track, then such games should form a valid instrument for measuring the degree to which LLMs do indeed approximate these capabilities.</p>
<p>Figure 2 illustrates how the example games implemented and evaluated here connect to the construct. (All games require a minimal form of discourse model being built, insofar as earlier information constrains later moves; and all games require a minimal type of agent model, insofar as the game instructions need to be taken on as own "intentions".) We will argue for these connections in detail below, but first we need to describe the scaffolding required to turn LLMs into game players.</p>
<h2>3 From Game To Benchmark</h2>
<h3>3.1 Terminology</h3>
<p>First, some terminology: A Dialogue Game Realisation (DGR) fixes for a given game the prompt templates (with which the game is described to the players) and the logic of the Game Master (the programmatic component keeping the game on track; see below). An instance of a DGR fixes the goal (e.g., in a word-guessing game, the word to guess) and the configuration. A data set is a collection of instances. An experiment fixes the players that get to play through a data set; e.g., as either being a human participant, or as a computer model (with all its free parameters fixed). For each episode (play of an instance), the experiment results in an interaction record. This record is what gets evaluated, both at a turn-by-turn level (progress made in the game) as well as for whether (or to what degree) the goal was reached. The benchmark then is a specific collection of datasets, and a benchmark result is the evaluation of (the interaction records of) a fixed combination of players over the benchmark.</p>
<h3>3.2 Turn-Based Text Games via Prompting</h3>
<p>Not all kinds of Dialogue Games in the sense of Schlangen (2023a) can be realised with LLMs as players. For now, the games need to be textbased (although we do realise games below that use character-encodings for image-like structures), and they need to be turn-based, so that each turn can be one prompting of a player to produce its move for this turn. We realise single-player games as well as two-player games. In order to keep the interaction focussed on the interactional task / the
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Schematic View of the Game Flow</p>
<p>Dialogue Game, we insert a (programmatic) Game Master into the interaction, whose task it is to keep track of the game state and to parse the reactions by the player, ensuring that only game-relevant actions are passed on and that the rules of the game are followed. In the taboo game shown in Figure 1 for example, the Game Master checks that the description given by player A does indeed not contain the "taboo" words (see description of the game below), before passing the message on to player B. In general, the Game Master is responsible for parsing the responses of the players and ensuring their formal adequacy. (At the moment, this is rather strict, leading to even slight variations being disregarded. At this point, we see this as still preferable over requiring more content-based parsing.) Thereby, the "general purpose" nature of any participating model is hidden, and it can be evaluated purely for its performance in the game.</p>
<p>The games considered here are self-contained in the sense that each game opens with a description of the game rules and instructions regarding the form of the response expected from a player; the game play consists in the player choosing the content of the move. This makes it possible to separately evaluate the ability to play the game (follow the instructions) and the level of expertise at playing it (e.g., by how fast or how well the goal has been reached in a given episode). Figure 3 shows a schematic view of how the Game Master controls a two-player game by making use of prompt templates that are filled in based on the current game state.</p>
<h3>3.3 The clemagne Framework</h3>
<p>We have implemented a Python framework that provides the general pattern (prompting, Game Master) described above, and takes care of the infrastructure of routing the player turns to the various model APIs (or, in the case of human players, to an appropriate interface). It is easily extensible to include new language-processing models (of type "string to string"; that is, models that can be prompted with a context and that return text). The framework also takes care of the separation of instance collections into datasets, of running (with different model settings) the experiments constituting the benchmark and evaluation based on scoring. All games described in the next section are implemented in this framework.</p>
<h2>4 The Games in v1.0 of the Benchmark</h2>
<p>All games described here challenge the rulefollowing capabilities of the players. In all games, the game objectives and the rules, including formal constraints on the game moves, are described verbally to the player. What these instructions leave implicit are general strategic considerations of game play, such as that repetitions in a guessing game have no strategic value. The Game Master validates each player move according to the formal constraints, and if after a certain amount of reprompting still no valid move is produced, the game is aborted. We measure for all games the proportion of games that were aborted in this way, giving us for each player a measure of their general ability to follow rules.</p>
<p>In the following, we briefly describe each game in general terms and define for each game a quality score with which to quantify the players' level of competence of playing it (beyond just following the rules so as to avoid the game play being aborted). Note that these metrics typically evaluate the pair of players together and cannot make rolebased distinctions. All further details, such as how we realised the game through prompts and how we instantiated the realisation into game instances, are collected in the Appendix. Note that we did not specifically optimise the prompts for performance of any given model; we just made sure that our reference model GPT-3.5 seemed to be able to follow them. In any case, all models are challenged with exactly the same prompts for each game instance, ensuring validity of the relative outcomes. Other metrics common to all games are described in Ap-
pendix B. The games described here are those we selected for a first version of the benchmark, with the aim of breadth with respect to the model in Figure 2; we see as an advantage of the framework that it is easy to implement more games, and expect the benchmark to be extended through contributions from the community.</p>
<h3>4.1 A Simple Word Game: Taboo</h3>
<p>In this game, one player has to describe to another player a concept, without using the concept name and any of a given list of semantically related words. The task of the other player then is to guess this word. If the player guesses wrongly, the first player can attempt a different description, following the same constraints. To play this game, the players must be able to access information from the interface between what we called above the language model and the world model, to provide and understand concept descriptions. The additional constraints on word use challenge a player's ability to apply given rules to the given situation.</p>
<p>We have seen an example of game play already above in Figure 1, which indicated that the role of the Game Master is to provide the target word to player A, and to validate the players' moves (A cannot use taboo words; B either guesses correctly or not). We allow three rounds of guessing. The quality score for taboo is speed, scored non-linearly as $\frac{100}{n}$ (with $n$ as the number of required moves), or 0 (if $n&gt;3$ ). For Figure 1 hence this would yield 50.</p>
<h3>4.2 Word-Guessing w/ Letter-Based Feedback</h3>
<p>We also implemented some variations of the popular word-guessing game "Wordle". ${ }^{4}$ The basic mechanics of this game is that letter-based feedback is provided on guesses of 5 -letter words, which incrementally constrains the set of possible words. If the target word for example is APPLE, a guess of ALONE would yield the following information: A appears at this position, L appears elsewhere, O does not occur, N does not occur, E occurs at this position. We also implement non-standard variations where a textual clue is given at the beginning as to the identity of the word. These games are one-player games (although we technically realised the computation of letter-feedback as the contribution of a player B). We also implemented a variant where there is a more active additional player, who can give feedback on the choice of player A be-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Excerpt of wordle+clue+critic game play (GPT4/GPT4)
fore it is played, giving A the opportunity to select differently. These game variants again challenge knowledge from language and world model, as well as, in a rudimentary form, in the "critic" variant, simulating processes of conversational grounding / negotiation. Figure 4 shows an excerpt of a game played with critic. The quality score for all variants again is speed (with a maximum of 6 guesses).</p>
<h3>4.3 Drawing Instruction Giving and Following</h3>
<p>In this game, player A is given an image (here represented as a grid of characters, with $\square$ representing an empty cell), and their task is to instruct player B to reconstruct this image, starting from an empty grid. (See Figure 5 for an example.) Hence, to be successful both player A and B must form, in a limited multimodal way, a model of a (very much abstracted) situation. The game stops when player A signals that their description is complete. The quality score is the F1-score of player B's grid relative to player A's target and the non-empty "pixels": if all, and only, the target cells have been changed as desired, it is 100 , if none have, it is 0 . We test with compact instances, which allow for higher level descriptions (as in the example), and random grids, which do not; see Appendix E.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: An episode of the drawing game</p>
<h3>4.4 A Picture Reference Game</h3>
<p>We also implemented a simple Lewis Signalling Game (Lewis, 1969), where A is presented with three grids (of the type also used in drawing; shown in Figure 6) and the task to make B (who is also presented with the same three grids, but potentially in a different order) identify a pre-specified one. As in drawing, this game challenges the formation of a situation model, and, to be done efficiently, needs access to analogical information from the agent's world model (e.g., to describe the second grid in Figure 6 as "looks like a T"). There is a long tradition in psychology to use such reference games to provide insights into communicative behaviour (see, e.g., (Yule, 1997)). The quality score for this game is a simple binary success measure: Did B identify the target, or not?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">1st Grid</th>
<th style="text-align: center;">2nd Grid</th>
<th style="text-align: center;">3rd Grid</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">XXXXX</td>
<td style="text-align: center;">XXXXX</td>
<td style="text-align: center;">XXXXX</td>
</tr>
<tr>
<td style="text-align: center;">$\square \backslash X \square$</td>
<td style="text-align: center;">$\square \backslash X \square$</td>
<td style="text-align: center;">$\square \square \square \square$</td>
</tr>
<tr>
<td style="text-align: center;">XXXXX</td>
<td style="text-align: center;">$\square \backslash X \square$</td>
<td style="text-align: center;">$\square \square \square \square$</td>
</tr>
<tr>
<td style="text-align: center;">$\square \backslash X \square$</td>
<td style="text-align: center;">$\square \backslash X \square$</td>
<td style="text-align: center;">$\square \square \square \square$</td>
</tr>
<tr>
<td style="text-align: center;">XXXXX</td>
<td style="text-align: center;">$\square \backslash X \square$</td>
<td style="text-align: center;">XXXXX</td>
</tr>
</tbody>
</table>
<p>Figure 6: Sample grids for the reference game</p>
<h3>4.5 Scorekeeping: Private and Shared</h3>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: An example of the primary interaction in private/shared</p>
<p>The final game in v1.0 is structurally a bit different, as besides the main dialogical interaction, there is a secondary probing interaction going on, which is the target of interest here. In the primary interaction, an answerer agent goes through a form with a questioner. In the probing interaction, the game master probes the answerer agent on what they "think" that the questioner agent already knows. Each instance specifies the list of information that is to be exchanged, and so the game master can keep track of which information has already been shared. If a piece of information has not yet been shared, the answerer should not expect the questioner to know it. This game challenges the keeping of an agent model and its update through conversational grounding processes. Figure 7 shows an edited transcript of the primary interaction, and Figure 8 shows an example of a probing interaction that the game master can interject. For evaluation, we compute the slot-filling accuracy throughout the main interaction and the agreement between the model's answers and the ground truth in the probing rounds. Because each probe is a binary decision (shared or not), the random performance would be high, so we use Cohen's $\kappa$ (Cohen, 1960) to control for chance. The quality score is the harmonic mean between the slot-filling accuracy and the probing $\kappa$ (truncated at 0 ).</p>
<h2>5 Results</h2>
<p>As detailed in the Appendix, the full benchmark (v1.0) consists of 250 instances: 30 for taboo, 30 for wordle, 30 for wordle+clue, 30 for wordle+clue+critic, 40 for drawing, 40 for reference, and 50 for private/shared.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: An example of the secondary interaction in private/shared; the model sees each question separately, with the primary dialogue as context</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Overview of benchmark results</p>
<table>
<thead>
<tr>
<th>model</th>
<th>version</th>
<th>here</th>
<th>P</th>
<th>T</th>
<th>I</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt-4</td>
<td>0314</td>
<td>4</td>
<td>n/a</td>
<td>n/a</td>
<td>Y</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>0301</td>
<td>3.5</td>
<td>n/a</td>
<td>n/a</td>
<td>Y</td>
</tr>
<tr>
<td>text-davinci</td>
<td>003</td>
<td>3</td>
<td>175</td>
<td>300</td>
<td>Y</td>
</tr>
<tr>
<td>claude</td>
<td>v1.3</td>
<td>cl</td>
<td>52</td>
<td>n/a</td>
<td>Y</td>
</tr>
<tr>
<td>luminous-supreme</td>
<td>2023-01</td>
<td>lm</td>
<td>70</td>
<td>588</td>
<td>Y</td>
</tr>
<tr>
<td>falcon-40b-instruct</td>
<td>2023-06</td>
<td>flc</td>
<td>40</td>
<td>600</td>
<td>Y</td>
</tr>
<tr>
<td>vicuna-13b</td>
<td>2023-06</td>
<td>vcn</td>
<td>13</td>
<td>1.4 k</td>
<td>Y</td>
</tr>
<tr>
<td>open-assistant-12b</td>
<td>2023-06</td>
<td>ost</td>
<td>12</td>
<td>400</td>
<td>Y</td>
</tr>
<tr>
<td>koala-13b</td>
<td>2023-06</td>
<td>ko</td>
<td>13</td>
<td>1.4 k</td>
<td>Y</td>
</tr>
</tbody>
</table>
<p>Table 1: The evaluated models with the details about number of parameters in billions (P), trained data size (tokens) in billions (T), and whether they were instruction tuned (I). Y: yes, n/a: publicly not available.</p>
<p>We ran the benchmark on the models (closed and open-access) shown in Table 1 with self-play (a model plays all non-static players in a game). In</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">all</th>
<th style="text-align: center;">taboo</th>
<th style="text-align: center;">wordle</th>
<th style="text-align: center;">wordle+cl</th>
<th style="text-align: center;">wordle+cr</th>
<th style="text-align: center;">drawing</th>
<th style="text-align: center;">reference</th>
<th style="text-align: center;">priv/sh</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathbf{l m} / \mathbf{l m}$</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">16.24</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">10.34</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">00.00</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">$0.0(-)$</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">ko/ko</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">14.76</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">86.67</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">1.47</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">20.0 (44.72)</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">flc/flc</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">50.0 (-)</td>
<td style="text-align: center;">100.0 (-)</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">ost/ost</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">20.85</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">14.29</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">1.73</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">33.33 (51.64)</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">vcn/vcn</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">13.58</td>
<td style="text-align: center;">5.08</td>
<td style="text-align: center;">56.67</td>
<td style="text-align: center;">13.33</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">4.24</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">31.25</td>
<td style="text-align: center;">100.0 (0.0)</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">25.0 (50.0)</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">cl/cl</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">74.76</td>
<td style="text-align: center;">76.92</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">46.43</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">37.06</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">49.58</td>
<td style="text-align: center;">68.75 (38.71)</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">30.56 (40.13)</td>
<td style="text-align: center;">30.77 (48.04)</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">82.5 (38.48)</td>
<td style="text-align: center;">84.87 (18.87)</td>
</tr>
<tr>
<td style="text-align: center;">3/3</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">44.50</td>
<td style="text-align: center;">28.81</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">36.67</td>
<td style="text-align: center;">23.33</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">16.0</td>
</tr>
<tr>
<td style="text-align: center;">15.77</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">35.46</td>
<td style="text-align: center;">76.47 (43.72)</td>
<td style="text-align: center;">1.25 (5.59)</td>
<td style="text-align: center;">31.36 (38.99)</td>
<td style="text-align: center;">50.0 (50.0)</td>
<td style="text-align: center;">38.7 (27.78)</td>
<td style="text-align: center;">36.36 (48.85)</td>
<td style="text-align: center;">14.1 (25.21)</td>
</tr>
<tr>
<td style="text-align: center;">3.5/3.5</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">85.86</td>
<td style="text-align: center;">69.49</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">93.33</td>
<td style="text-align: center;">76.67</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">64.0</td>
</tr>
<tr>
<td style="text-align: center;">37.02</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">43.12</td>
<td style="text-align: center;">71.95 (44.79)</td>
<td style="text-align: center;">$0.0(0.0)$</td>
<td style="text-align: center;">28.57 (46.0)</td>
<td style="text-align: center;">13.19 (30.16)</td>
<td style="text-align: center;">60.28 (25.95)</td>
<td style="text-align: center;">55.0 (50.38)</td>
<td style="text-align: center;">72.83 (13.07)</td>
</tr>
<tr>
<td style="text-align: center;">3.5/4</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">86.75</td>
<td style="text-align: center;">69.49</td>
<td style="text-align: center;">(single pl.)</td>
<td style="text-align: center;">(single pl.)</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">42.39</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">48.87</td>
<td style="text-align: center;">62.6 (45.15)</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">10.42 (17.42)</td>
<td style="text-align: center;">64.95 (25.45)</td>
<td style="text-align: center;">57.5 (50.06)</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">4/3.5</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">82.78</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">(single pl.)</td>
<td style="text-align: center;">(single pl.)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">55.61</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">93.59 (23.45)</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">46.67 (42.92)</td>
<td style="text-align: center;">81.0 (21.54)</td>
<td style="text-align: center;">47.5 (50.57)</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">4/4</td>
<td style="text-align: center;">\% played</td>
<td style="text-align: center;">96.06</td>
<td style="text-align: center;">94.92</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">59.48</td>
<td style="text-align: center;">qlty score</td>
<td style="text-align: center;">61.93</td>
<td style="text-align: center;">76.19 (37.45)</td>
<td style="text-align: center;">3.67 (8.4)</td>
<td style="text-align: center;">49.67 (42.09)</td>
<td style="text-align: center;">49.11 (38.46)</td>
<td style="text-align: center;">89.06 (22.28)</td>
<td style="text-align: center;">75.0 (43.85)</td>
<td style="text-align: center;">90.79 (8.2)</td>
</tr>
</tbody>
</table>
<p>Table 2: Results Overview. For each model (pairing), shows how many games were played to completion (\% played), an indicator of rule-following capabilities. "qlty score" indicates how well the completed games were played (higher is better, max is 100; standard deviation in parentheses). all is the average over all games scores, the remaining columns show results broken down by game (averaged over all episodes). Values below model names are their clemscore. Updates / additional models posted at https://github.com/clembench.
addition, we run pairs of gpt-4 and gpt-3.5 to test if a supposedly better model (here gpt-4) can leverage the other. Following Srivastava et al. (2022), we requested greedy sampling (i.e., temperature 0 ). One run of the benchmark, somewhat surprisingly, on average took more than 600 minutes to complete, due to API latency, and cost around $50 \$$ in API fees.</p>
<p>Table 2 gives the overall results of this benchmark run. For each model (pairing), we first show what percentage of instances were played to completion (i.e., not aborted because of problems of the players in following the instructions). We then show what the average quality of the play was for those played instances, using each game's quality score. The first columm (macro-)averages the numbers over all games, with the remaining ones giving the per-game results. Figure 9 provides the same information in a graphical format, plotting "percentage played" against "quality". A perfect model—and, we suspect, since these are simple games, human performance-would be clustered in
the top right corner (all instances played, with high quality). As we can see from the results, the GPT family tends to perform better than the other models we tested, with an increase in quality from 3 to 3.5 to 4 . There is a jump in the ability to play games to completion (that is, to follow the prompt instructions as to the format of the game play moves) from 3 to 3.5 , with a smaller increase from 3.5 to 4 . Still, even the best performing model, GPT-4, does not reach $100 \%$ on "percentage played", with the reduction mostly due to drawing and, somewhat surprisingly, taboo - perhaps due to the negative nature of the game constraints ("don't say X").</p>
<p>When it comes to the quality of the game play (in those episodes played to completion), we see a similar trend, with GPT4 overall performing best. We also see that there is ample room for improvement, with the best average score standing at 60.59 . An outlier in terms of quality is wordle, where even though almost all models manage to stick to the local rules (produce a 5 -letter word), even the best-performing model, GPT4, only reaches</p>
<p>4.56 on the quality metric, indicating that very few games are actually solved, and those only at the last attempt. This indicates that all models fail at integrating the feedback across turns and using it to constrain their guesses. The capabilities of dealing with verbal meaning definitions are shown by the large improvement that wordle+clue exhibits (to 47.89). Interestingly, GPT4 is able to profit from (via another instance, self-)criticism, improving further to 50.11.</p>
<p>Again somewhat surprisingly, performance on the "multimodal" games (which require verbalisation of character-based graphics) is not bad. For drawing, as a deviation from the trend, GPT3.5 proved to be better at sticking to the game format ( $97.5 \%$ of episodes played to completion), although GPT4 still reached higher quality on its completed games. reference sees Claude performing best, against the trend for all other games.</p>
<h2>6 Discussion</h2>
<p>Insights from Taboo game: The results show that the open source models and Luminous cannot, or only badly, play the taboo game. Claude shows a strong performance with $76.92 \%$ played games and a quality score of $68.75 \%$. The best scores are achieved by the GPT-4/4 pair with $94.92 \%$ played games and a quality score of $76.19 \%$. We hypothesise that this is an effect of RLHF training time so that the model is highly aligned with the prompts given by the user (the game master). An indication is given by the increase from $28.81 \%$ to $69.49 \%$ of games played when comparing GPT-3 (a foundation model) and GPT-3.5 (an RLHF fine-tuned model) where the quality score remains similar. Both models share the knowledge representation and are similarly good in retrieving that knowledge, but the latter is better aligned.</p>
<p>Insights from Reference and Drawing games: Claude and GPT 3.5 and 4 models get the best played ratio, which indicates that the generated outputs match the expected format. As this game is single turn, unlike the other games, errors cannot accummulate over turns. In Drawing, Luminous, Claude and the open access models did not manage to follow instructions. The generated outputs included the repetition of the text in given instructions, which leads for games to be aborted. The played ratio of GPT 3.5 is higher than GPT-4. By looking at some selected instances, we saw that the outputs from GPT-4 are simply the appended
text of multiple turns (on Player A side) instead of generating each instruction separately in a single turn. GPT-3.5 is better at following the instructions ( 97.5 vs. 77.5 in played score) but GPT-4 is better at getting the target drawing ( 60.2 vs 89.0 in quality score), in those cases where the format was correct.</p>
<p>Insights from the Scorekeeping game: Games were aborted mostly due to the models failing to use the correct player tag. This is particularly complicated in this game because we are trying to simulate a multi-party conversation, with one tag for each interlocutor. Interestingly, sometimes a mere reprompt with a generic addition (e.g. "Please answer this question carefully.") would trigger it to generate the right tag, even though the mistake was not added to the history. Another issue is that sometimes models would anticipate or invent slots and upcoming turns. Anticipating is not totally incorrect, but it makes it harder for the GM to check for the private/shared status of a slot. Claude and GPT-4 played the slot filling part very well; their mistakes came mostly from the scorekeeping component, with mixed results in abstract and concrete domains. In almost all cases, their main type of mistake was considering shared slot values to be still private.</p>
<p>Insights from the Wordle game: Models other than GPT-4 could not adhere to the game rules in at least half of the episodes they played. A significant observation is that most of these models did not incorporate the letter feedback to enhance their subsequent word guesses. This is evident from the repetition of letters from previous guesses in subsequent iterations. Figure 19a illustrates this observation. The turn at which a correct guess is made provide insights into the efficiency of the guessing strategy. In the traditional Wordle variant, GPT-4 takes an average of four turns (refer to Table 4 speed metric) to guess correctly, while it improves to two turns in extended variants. The presence of clue and feedback from the critic both improve the success rate and speed for the GPT-4 model. On the other hand, for other models the Played score degrades in the extended variants.</p>
<h2>7 Related Work</h2>
<p>Playing games and learning from self-play stands at the beginnings of the "deep learning revolution" (Mnih et al., 2013; Silver et al., 2017). ${ }^{5}$ What is</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>different here is the zero- or few-shot nature of our test, where the testing mode is different from the learning mode-this of course only being enabled by "foundation models" (Brown et al., 2020). The latest-apparent-qualitative jump has only recently been taken, so there are not that many papers yet that attempt a systematic evaluation; see, inter alia, (Liu et al., 2023; Bang et al., 2023). To our knowledge, game play of the kind proposed here has not yet been used for the systematic evaluation of these models. The idea of testing game play is mentioned in (Bang et al., 2023; Bubeck et al., 2023), and also already technically possible in (Srivastava et al., 2022), but has not been systematically executed there.</p>
<p>A superficial similarity also exists to approaches like HuggingGPT (Shen et al., 2023) in that these approaches pair LLMs with scaffolding (as in our Game Master). A crucial difference, however, is that for us the task of the Game Master is to constrain the LLM and to "keep it focused", as it were, on the game, rather than to extend its capabilities.</p>
<p>Park et al. (2023) also acted on the realisation that cLLMs can simulate agents which can be put into "self-play", but developed this idea in a different direction, towards investigating the emerging "social behaviour".</p>
<p>Newly proposed benchmarks such as AlpacaEval (Li et al., 2023), Chatbot Arena (LMSYS, 2023) and Open LLM Leaderboard (HuggingFace, 2023) focus on comparing models outputs either running them on existing datasets, employ human annotators to choose which output is preferred, or simply ask another LLM to evaluate the outputs; these benchmarks do not test the interactive dialogue aspects of chat-based LLMs. Another important aspect to note here is that using existing datasets for benchmarking might jeopardise the point of keeping the test instances unseen because those instances could have been part of the training data for these large language models. The datasets for clembench have been created from scratch and adding new games or new instances to the existing games is easy to ensure continued fair benchmarking.</p>
<h2>8 Roadmap</h2>
<p>Important next steps on our roadmap include testing the models' abilities to handle languages other than English and integrating the framework with the slurk chat tool (Götze et al., 2022) in order to enable game play with human players. We also
plan to experiment with games that have more than two players as well as games that require multimodal context such as images. We are also excited about the potential to use this as an instrument for testing models across size variations and training checkpoints, to analyse what it takes to acquire the capabilities tested here. Lastly, with the measuring instrument introduced here in place, we can also turn to improving individual models (rather than testing existing models out of the box) so as to optimise their performance on a particular game or set of games.</p>
<h2>9 Conclusions</h2>
<p>We have shown that current chat-optimised large language models can indeed serve as models of interactive agents, at least for controlled and ruleconstituted activities such as verbal games. We have described our general implementation of a framework for implementing rules to be played in "self-play" by such models, with the main idea being that a programmatic component, the "Game Master" can control the interaction and ensure that only formally correct moves are registered. We have described our example implementations and instantiations of such games, arguing that they span the breadth of the sub-capabilities involved in situated language processing (if only on a relatively superficial level). Finally, we have shown that the evaluation of the game play can serve as an instrument to distinguish between models in terms of their language capabilities. With this work, we have aimed to open a complementary avenue for evaluating these models, beyond more classical reference-based NLP task evaluation or preferencebased evaluation, and into the realm of interactive language use. Much remains to be done, but we hope that our framework can support some of this future work.</p>
<h2>Acknowledgements</h2>
<p>The work reported here has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), grants 423217434 ("RECOLAGE") and 317633480 (SFB 1287); and by Bundesministerium für Bildung und Forschung (BMBF, German Federal Ministry of Research), project "COCOBOTS" (01IS21102A). We thank the anonymous reviewers for their helpful feedback.</p>
<h2>Limitations</h2>
<p>As indicated above, the number of instances per experiment is not large. As we did not observe very large deviations in results, we kept the numbers small to reduce (monetary and environmental) cost; acknowledging that larger sets (and tests with different temperature settings) may increase the fine-grainedness of the results. In addition, limited context size may be an issue in models that hallucinate long utterances: if the beginning of the dialogue history gets cropped, the instructions are deleted. We set a maximum number of tokens for the open models. As also discussed above, one limitation that we soon want to overcome is that of a retriction to English language prompts and game instances.</p>
<h2>Limits on reproducibility of closed access models</h2>
<p>Some models under evaluation are only accessible via a programming interface which basically adds a black box on top of a black box (GPT-3/3.5/4, Luminous, Claude). The mechanics (and exact models invoked) behind these interfaces might change at any time and consequently the results of successive runs might vary arbitrarily. For the closed models tested here, the best we can do is to provide the timestamp of the testing and the versioning information, to the extent that it is available to us.</p>
<h2>Limits on selection of open access models</h2>
<p>The selection of open access models was based on looking at high-ranked models on existing benchmarks (LMSYS, 2023; HuggingFace, 2023) and identifying the candidate ones. Another criterion for the selection was the availability of model weights publicly to ensure the reproducibility of the study.</p>
<h2>Ethics Statement</h2>
<p>Using paid proprietary APIs with underlying models about which little is known (training data, model architecture) in academic research is less than ideal. At the moment, the models tested here seem to be the only ones that are even able to follow the structure of the games as instructed. It is our hope that open models will catch up soon, and proper research can be done with them.</p>
<h2>References</h2>
<p>AlephAlpha. 2023. What is luminous? https://docs.aleph-alpha.com/docs/ introduction/luminous/. Accessed: 2023-06-12.</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.</p>
<p>Jacob Andreas. 2022. Language models as agent models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5769-5779, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. CoRR, abs/2302.04023.</p>
<p>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro,</p>
<p>and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \%{ }^{\circledR}$ chatgpt quality.</p>
<p>Herbert H Clark and Susan E Brennan. 1991. Grounding in communication. In Perspectives on socially shared cognition., pages 127-149. American Psychological Association.</p>
<p>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37-46.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post.</p>
<p>Jana Götze, Maike Paetzel-Prüsmann, Wencke Liermann, Tim Diekmann, and David Schlangen. 2022. The slurk interaction server framework: Better data for better dialog models. In Proceedings of the Language Resources and Evaluation Conference, pages 4069-4078, Marseille, France. European Language Resources Association.</p>
<p>HuggingFace. 2023. Open 11m leaderboard. https://huggingface.co/spaces/ HuggingFaceH4/open_11m_leaderboard. Accessed: 2023-06-12.</p>
<p>Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023. Openassistant conversations - democratizing large language model alignment. CoRR, abs/2304.07327.</p>
<p>David Lewis. 1969. Convention. Harvard University Press.</p>
<p>David Lewis. 1979. Scorekeeping in a language game. In Semantics from different points of view, pages 172187. Springer.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.</p>
<p>Nelson F. Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating verifiability in generative search engines. CoRR, abs/2304.09848.</p>
<p>LMSYS. 2023. LMSYS chat leaderboard. https:// chat.lmsys.org/?leaderboard. Accessed: 2023-06-12.</p>
<p>Brielen Madureira and David Schlangen. 2022. Can visual dialogue models do scorekeeping? exploring how dialogue representations incrementally encode shared knowledge. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 651664, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. CoRR, abs/2304.03442.
A. L. Samuel. 1959. Some studies in machine learning using the game of checkers. IBM Journal of Research and Development, 3(3):210-229.</p>
<p>David Schlangen. 2023a. Dialogue games for benchmarking language understanding: Motivation, taxonomy, strategy. CoRR, abs/2304.07007.</p>
<p>David Schlangen. 2023b. On general language undertanding. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore. Association for Computational Linguistics.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580.</p>
<p>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George Van Den Driessche, Thore Graepel, and Demis Hassabis. 2017. Mastering the game of Go without human knowledge. Nature, 550(7676):354-359.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek,</p>
<p>Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>George Yule. 1997. Referential Communication Tasks. Routledge, New York, USA.</p>
<p>Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2856-2878, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<h2>A Detailed Benchmark Results</h2>
<p>In this section, we include additional visualisations of the overall results. Figure 10a is a graphical representation of the main results in Table 2. Figure 10c illustrates the percentage of played and aborted games; played games are further split into successful (perfect performance) and lost games. Figure 10b presents the comparison of clemscores for each model.</p>
<h2>B Common metrics</h2>
<p>Besides each game's specific scores, the following metrics are computed for all games:</p>
<ul>
<li>Quality Score: A custom performance score, normalised to the interval $[0,100]$, representing the quality of the game play. This is used to compare models across different games, similar
to the preferred score in Srivastava et al. (2022). Measures: episode performance.</li>
<li>Aborted: At the episode level, either 0 or 1 whether the game play has been aborted (1) or not (0). A game counts as aborted when a violation of the game rules happens, for example a response is not parsable by the rule that specifies it's format as "TYPE: <text>" (or re-prompt for n turns). Measures: episode performance.</li>
<li>Loss: At the episode level, either 0 or 1 whether the (non-aborted) game has been successful (0) or not (1). Measures: episode performance.</li>
<li>Success: At the episode level, either 0 or 1 whether the (non-aborted) game play has been successful (1) or not (0). Measures: episode performance.</li>
<li>Request Count: total number of request given to the model by the GM (usually 1 per turn, but for games with re-prompting this might be $&gt;1$ per turn). Measured at: turn and episode level.</li>
<li>Parsed Request Count: total number of request that could be parsed successfully (the model's response complies to the game rules; accumulates over the episode). Measured at: turn and episode level.</li>
<li>Violated Request Count: game master checks the outputted text and decides whether it matches the "game form" (also as a log action), if not then this is a violation of the game rules; total count of failures in a episode; turn-based (can be &gt;= 0). Measured at: turn and episode level.</li>
<li>Request Success Ratio: parsing success rate - or prompt has been successful if the output can be parsed properly. It is computed by dividing the parsed request count by the total request count . Measures: episode performance.</li>
</ul>
<p>Together, these scores allow for more finegrained insights into the performance of the models.
clemscore To facilitate easy comparison of models, we define a score summarising the performance of a model in the benchmark as a whole. The the $\%$ of actually played games (i.e. not aborted) and the average quality score (over all episodes) are computed for each game, and rounded to two decimals. Then, the macro-average quality score and the</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" />
(a) Overview of \% played games and micro-average quality score for all models and games. Perfect performance in the benchmark would be represented with all markers overlapping in the top right corner.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Other views on the main results
macro-average \% played are computed as the mean over game scores. clemscore is the macro-average quality score multiplied by the macro-average proportion of played games. Given $N$ games, the clemscore of a given model is computed as follows:</p>
<p>$$
\left(\frac{1}{N} \sum_{i=1}^{N} q_{i}\right)\left(\frac{1}{100 N} \sum_{i=1}^{N} \% p_{i}\right)
$$</p>
<p>where $\% p_{i}$ is the percentage of played episodes (i.e. episodes that were not aborted) for game $i$, rounded to two decimals, and $q_{i}$ is the mean qual-
ity score across all game $i$ episodes that were not aborted, rounded to two decimals.</p>
<h2>C Game: Taboo</h2>
<h2>C. 1 Game Details</h2>
<p>In this game a Describer describes a target word for a Guesser. The Describer must explain the target word concept without using neither the word itself, nor a number of related words. For example, when the target word is mark, the Describer might be told not to use the words label, tag or stamp. After each</p>
<p>incorrect guess by the Guesser, the Describer can add to their description. The game ends when the Guesser guesses correctly or a maximum number of turns has been reached.</p>
<p>When the cLLM is playing the Describer, then the game tests its ability to describe concepts and give meaning definitions. In addition, the game tests its helpfulness in the game context: e.g., if a Describer does not alter or extend its initial description after an incorrect guess, we consider this as unhelpful behavior. When playing as a Guesser, then the game tests the cLLM's ability to access its world model. In addition, similarly as above, if a Guesser repeats an earlier guess though given a different description, the model has not aligned well enough to the game goal (has not "understood" the game constraints).</p>
<h3>C. 2 Instantiation</h3>
<p>The players are each given their own prompts, as shown in Figure 11. We set the maximum number of guesses to 3.</p>
<p>Target Words. We use an English word frequency list based on web data (Brants and Franz, 2006) ${ }^{6}$ to derive a list of lemmatized target word candidates. From these candidates we remove all that occur less than 5 times per 1 million tokens.</p>
<p>Frequency-based Experiments. The remaining candidates are sorted into 3 equally-sized bins based on their frequency in the corpus. The resulting bins can be interpreted as (i) low-frequency words that occur up 9.4 times per 1 million tokens, (ii) the medium-frequency words occur up to 25.1 times per 1 million tokens and (iii) the highfrequency tokens occur up to 1,2951 times in 1 million tokens. The assumption is that the word level frequency is a proxy for a cLLM's difficulty to describe or understand a word (because it has seen it more or less times during training).</p>
<p>Game Instances. From each frequency group we (uniformly) sample 20 words as the target words. We manually ensure that the final word list does not contain inappropriate words such as vulgar language. Then we use the Merriam Webster Thesaurus API to find all synsets for a particular target word. We concatenate the synsets and sample 3 words as the related words. This means that the related words cover a variety of target word meanings. If for some reasons only less than 3 related</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>```
TEMPLATE C.1.1
You are playing a collaborative word guessing
game in which you have to describe a target
word for another player to guess.
Rules:
(a) You have to reply in the form: CLUE: <some
text>. Guesses from the other player will start
with GUESS.
(b) You cannot use the target word itself,
parts or morphological variants of it in your
description.
(c) In addition, the same rules apply for
related words which are provided below.
End conditions:
(i) If you use the target word or a related
word in your description, then you lose.
(ii) If the other player can guess the target
word in $N$ tries, you both win.
Let us start.
This is the target word that you need to
describe and that the other player needs to
guess:
$TARGET_WORD$
Related words are:
$REL_WORD$</p>
<div class="codehilite"><pre><span></span><code>
</code></pre></div>

<p>Important: You are under time pressure, give
short descriptions that are to the point!</p>
<div class="codehilite"><pre><span></span><code>
</code></pre></div>

<p>TEMPLATE C.1.2
You are playing a collaborative word guessing
game in which you have to guess a target word
that another player describes to you.
You can make one guess at each trial. You win
when you guess the target word. You lose when
you cannot guess it in $N$ tries.
After each trial you will get a new hint from
the other player which starts with CLUE.
Make your guesses by just saying the word using
the following form: GUESS: <a word>
Let us start.
```</p>
<p>Figure 11: The Describer and Guesser prompts for the Taboo game.</p>
<p>words could be chosen via the API, then we manually search for the synonyms words on the Merriam Webster webpage and choose the highest ranked ones.</p>
<p>Evaluation We measure the following metrics at the episode-level:</p>
<ol>
<li>Success: Whether or not the Guesser guessed the target word.</li>
<li>Abort: 1 if any player did not follow the rules, and 0 otherwise.</li>
<li>Speed (Quality Score): How early the Guesser guessed the word as measured by $100 / t$, where $t$ is the turn number in which the target was found. When the game was unsuccessful, speed is 0 . For aborted games, speed is undefined.</li>
</ol>
<p>Example transcripts We present example transcripts in Figures 12 and 13.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">Game Master</th>
<th style="text-align: center;">B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[x</td>
<td style="text-align: center;">or]</td>
<td style="text-align: center;">CLUE: A place where cars</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">and people share the same space.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">[OR</td>
<td style="text-align: center;">OR] continue</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">[OR</td>
<td style="text-align: center;">OR] A place</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">where cars and people share the same space</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[OR</td>
<td style="text-align: center;">R] CLUE: A place where cars and people share the same space</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">[OR</td>
<td style="text-align: center;">R] GUESS: Street</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">[OR</td>
<td style="text-align: center;">OR] continue</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">[OR</td>
<td style="text-align: center;">OR] street</td>
</tr>
</tbody>
</table>
<p>Figure 12: Example of a successful gameplay. The target word was street, taboo words were road, asphalt, drive.</p>
<h3>C.3 Additional Discussion of Results</h3>
<p>The results in Table 2 (see main text) indicate that the open source models Koala, Falcon, OpenAssistant are not able to play the taboo game at all. The same holds for Luminous. The open source Vicuna model plays at least some games and these with $100 \%$ success. The performances are better for the GPT-* family of models and Claude.</p>
<p>Here we see Claude is a strong competitor with $76.92 \%$ of played games and a quality score (Speed) of $68.75 \%$. These scores exceed the performances of the other pairings (3/3, 3.5/3.5, 3.5/4,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13: Example of a game that was aborted because the Describer violated the game rules (the description has to begin with CLUE:). The target word was israel, taboo words were country, tel aviv, jew.
$4 / 3.5$ ) except for GPT-4. The GPT-4 model with self-play is playing almost all games ( $94.92 \%$ ) and achieves a high quality score ( $76.19 \%$ ). This means that GPT-4 is following the rules of the game in almost all cases.</p>
<p>We hypothesise that this might due to an even longer training with RLHF (and Claude is catching up) so that the model is highly aligned with the prompts given by the user (the game master). An indicator that this hypothesize is justified is given by the jump in games played between GPT-3 (a foundation model) and GPT-3.5 (an RLHF finetuned model) from $28.81 \% \rightarrow 69.49 \%$ while the quality score remains similar between these model. As GPT-3.5 is based on GPT-3 the knowledge representation is shared between these two and both are similarly good in retrieving that knowledge.</p>
<p>Now the pairing of the GPT-3.5 and GPT-4 models show an interesting picture. Both pairings are playing about the same number of games ( $69.49 \%$ vs $66.1 \%$ ) and as the number is the same (or similar) as the GPT-3.5 self-play results we can argue that the aborted games are due to GPT-3.5. On the other hand we see that the quality score (Speed)</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Experiment</th>
<th>n</th>
<th>Aborted</th>
<th>Played</th>
<th>Speed</th>
<th>Success</th>
<th>Lose</th>
</tr>
</thead>
<tbody>
<tr>
<td>3-3</td>
<td>0_high</td>
<td>20</td>
<td>70.00</td>
<td>30.00</td>
<td>100.00</td>
<td>30.00</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>78.95</td>
<td>21.05</td>
<td>75.00</td>
<td>15.79</td>
<td>5.26</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>65.00</td>
<td>35.00</td>
<td>57.14</td>
<td>20.00</td>
<td>15.00</td>
</tr>
<tr>
<td>3.5-3.5</td>
<td>0_high</td>
<td>20</td>
<td>25.00</td>
<td>75.00</td>
<td>86.67</td>
<td>65.00</td>
<td>10.00</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>42.11</td>
<td>57.89</td>
<td>72.73</td>
<td>42.11</td>
<td>15.79</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>25.00</td>
<td>75.00</td>
<td>56.67</td>
<td>45.00</td>
<td>30.00</td>
</tr>
<tr>
<td>3.5-4</td>
<td>0_high</td>
<td>20</td>
<td>25.00</td>
<td>75.00</td>
<td>74.44</td>
<td>65.00</td>
<td>10.00</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>42.11</td>
<td>57.89</td>
<td>68.18</td>
<td>42.11</td>
<td>15.79</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>25.00</td>
<td>75.00</td>
<td>46.67</td>
<td>40.00</td>
<td>35.00</td>
</tr>
<tr>
<td>4-3.5</td>
<td>0_high</td>
<td>20</td>
<td>30.00</td>
<td>70.00</td>
<td>96.43</td>
<td>70.00</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>26.32</td>
<td>73.68</td>
<td>100.00</td>
<td>73.68</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>45.00</td>
<td>55.00</td>
<td>81.82</td>
<td>45.00</td>
<td>10.00</td>
</tr>
<tr>
<td>4-4</td>
<td>0_high</td>
<td>20</td>
<td>10.00</td>
<td>90.00</td>
<td>83.33</td>
<td>85.00</td>
<td>5.00</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>$/$</td>
<td>100.00</td>
<td>71.93</td>
<td>84.21</td>
<td>15.79</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>5.00</td>
<td>95.00</td>
<td>73.68</td>
<td>75.00</td>
<td>20.00</td>
</tr>
<tr>
<td>cl-cl</td>
<td>0_high</td>
<td>14</td>
<td>21.43</td>
<td>78.57</td>
<td>77.27</td>
<td>78.57</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>18</td>
<td>22.22</td>
<td>77.78</td>
<td>75.00</td>
<td>66.67</td>
<td>11.11</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>25.00</td>
<td>75.00</td>
<td>56.67</td>
<td>50.00</td>
<td>25.00</td>
</tr>
<tr>
<td>flc-flc</td>
<td>0_high</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td>ko-ko</td>
<td>0_high</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td>lm-lm</td>
<td>0_high</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td>ost-ost</td>
<td>0_high</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
<tr>
<td>vcn-vcn</td>
<td>0_high</td>
<td>20</td>
<td>90.00</td>
<td>10.00</td>
<td>100.00</td>
<td>10.00</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>1_medium</td>
<td>19</td>
<td>94.74</td>
<td>5.26</td>
<td>100.00</td>
<td>5.26</td>
<td>$/$</td>
</tr>
<tr>
<td></td>
<td>2_low</td>
<td>20</td>
<td>100.00</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
<td>$/$</td>
</tr>
</tbody>
</table>
<p>Table 3: Overview of the models performances for all experiments in Taboo. The cells are color coded with traffic light colors so that green means high performance and red means low performance.
jumps over 31.09 scores (from $62.6 \%$ to $93.59 \%$ ). This shows that the GPT-4 model is a better Describer than GPT-3.5 and it is a better prompter for "knowledge retrieval" than GPT-3.5 (with a quality score of $71.95 \%$ ).</p>
<p>Still, especially the number of games played (without a rule violation) is less than what we would expect from a human player. We will test human abilities to play this game in a future iteration using slurk.</p>
<p>Effect of word frequency on model performance. Figure 14a shows that the word frequency of the
target words has no clear effect on the number of games played for the models. But we can see in Figure 14b that the frequency indeed impacts the quality score (Speed) of the models: with a lower frequency the models have a harder time to find the correct word to be guessed. This is reasonable - but also a bit counter-intuitive as these models are expected to have enough capacity to store everything - because when a word is seen in more contexts during training, then the Describer has a better chance (a) to either prompt for the context that is most often seen and thus has been</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" />
(a) For each model the mean number of games played (by experiment).
<img alt="img-12.jpeg" src="img-12.jpeg" />
(b) For each model the mean main scores (Speed) (by experiment).</p>
<p>Figure 14: Two important performance indicators for the models separated by experiment.
more manifested in the model's weights or (b) can prompt in various ways for the target word (probing the Guesser for the knowledge). Detailed results are given in Table 3.</p>
<h2>D Game: Wordle</h2>
<p>The popular word guessing game "Wordle" gained global attention, in which players are challenged to guess a five-letter word in six attempts. After each guess, the player receives feedback indicating which letters are in the correct position, which letters are correct but in the wrong position, and which letters are incorrect, to help them strategise their next guess. The objective of the game is to guess the target word using the fewest possible guesses, and the game ends when the player guesses correctly or exhausts all six attempts.</p>
<h2>D. 1 Game Details</h2>
<p>Wordle (Traditional Variant) This game evaluates three key aspects of cLLM's capabilities. Firstly, it assesses how well the cLLM comprehends the game rules, which involves generating valid English words consisting of exactly five letters. Secondly, it measures how effectively cLLM uses guess feedback to generate its next guesses. Thirdly, it measures how quickly cLLM can guess the target word if it succeeds.</p>
<p>In traditional gameplay, cLLM plays the role of "Player A", and a deterministic wordle bot plays the role of "Player B". Game begins with the game master prompting Player A to guess the target word. The game master parses Player A's response and forwards it to Player B, which evaluates the closeness of the guess word to the target word and returns the feedback. The game master sends the feedback to Player A for the next guess and the cycle continues until the target word is guessed correctly or all six attempts are exhausted. The prompt template of this variant is available in Figure 15a.</p>
<p>Wordle (+ Semantics-Based Clue) This is a Wordle variant where the guesser (Player A) gets a clue before starting to guess. For example, for the target word PRIDE, the clue could be "pack of lions". The rest of the game rules follow the same as the traditional game variant. cLLM plays the role of the "player A", and a deterministic wordle bot plays the role of "player B".</p>
<p>The primary aim of testing this variant is to evaluate the efficacy of Player A in effectively utilising the supplementary information provided by a clue to improve its guess of the target word. The clue serves as an aid to narrow down the possible word options. The success of the game depends on Player A's ability to integrate the clue with the guess_feedback. Player A's explanation offers insights into how the cLLM links the clue phrase and the guess_feedback. The prompt template is available in Figure 15b.</p>
<p>Wordle (+ Clue, + Critic) This game variant also begins with the guesser (Player A) who attempts to guess the target word based on a given clue. In contrast to other game variants, where the guessed word is immediately evaluated for its proximity to the target word, in this variant, the guessed word and the clue are forwarded to another player known as the critic, to get an opinion on the correctness of the guess. The critic responds with either agreement or disagreement, providing their rationale based on the information given. The critic's response is then relayed to the guesser, who can decide to stick with their initial guess or change it based on the feedback received. Figure 16a shows the prompt structure for the Player A, Figure 16b shows the prompt structure for the critic role and Figure 18 depicts the prompts fed to the guesser to share the critic's opinion.</p>
<p>This game variant helps to investigate the in-</p>
<h2>TEMPLATE D.1.1</h2>
<p>You are a language wizard who likes to guess words by using the given rules.</p>
<p>Welcome to Wordle! You have six attempts to guess the target word, a valid English word of five lowercase letters (a-z). Please use the tags "guess:" and "explanation:" to provide a concise explanation for each guess.</p>
<p>For instance, if your guess is "apple", your response should be guess: apple explanation: this is a common five-letter English word, and I am starting my guess with this word.</p>
<p>After each guess, your answer will be validated, and you will receive feedback indicating which letters are correct (green), which letters are correct but in the wrong position (yellow), and which letters are incorrect (red). This feedback can be useful in determining which letters to include or exclude in your next guess.</p>
<p>For example, the feedback for "apple" might be:
guess_feedback:
a 〈yellow〉 p 〈yellow〉 p 〈green〉 l 〈yellow〉 e〈red&gt;</p>
<p>The explanation should contain details about how the guess_feedback is used to arrive at a new guess.</p>
<p>Let's begin with your first guess.</p>
<h2>TEMPLATE D.1.2</h2>
<p>guess: hello
explanation: This is a common five-letter English word, and I am starting my guess with this word.
(a) Player A (Guesser) in the Wordle Game Basic Variant</p>
<p>TEMPLATE D.1.3
You are a language wizard who likes to guess words by using the given rules.</p>
<p>Welcome to Wordle! You have six attempts to guess the target word, a valid English word of five lowercase letters (a-z). Please use the tags "guess:" and "explanation:" to provide a concise explanation for each guess.</p>
<p>To help you make an informed guess, you will receive a clue for the word, such as clue: snowy white.</p>
<p>Here is an example guess based on the clue: guess: apple
explanation: In the fairy tail Snow White, the girl is killed because she eats a poisoned apple. And the word apple has 5 letters.</p>
<p>After each guess, your answer will be validated, and you will receive feedback indicating which letters are correct (green), which letters are correct but in the wrong position (yellow), and which letters are incorrect (red). This feedback can be useful in determining which letters to include or exclude in your next guess.</p>
<p>For example, the feedback for "apple" might be:
guess_feedback:
a 〈yellow〉 p 〈yellow〉 p 〈green〉 l 〈yellow〉 e〈red&gt;</p>
<p>The explanation should contain details about how both the guess_feedback and clue work together in tandem to formulate a new guess.</p>
<p>Let's begin with your first guess.
clue:display</p>
<h2>TEMPLATE D.1.4</h2>
<p>guess: screen
explanation: A display can refer to a computer screen, and "screen" is a valid English word with five letters.
(b) Player A (Guesser) in the Wordle Game Variant (+Clue)</p>
<h2>TEMPLATE D.1.5</h2>
<p>You are a language wizard who likes to guess words by using the given rules.</p>
<p>Welcome to Wordle! You have six attempts to guess the target word, a valid English word of five lowercase letters (a-z). Please use the tags "guess:" and "explanation:" to provide a concise explanation for each guess.</p>
<p>To help you make an informed guess, you will receive a clue for the word, such as clue: "snowy white"</p>
<p>Here is an example guess based on the clue:
guess: apple
explanation: In the fairy tail Snow White, the girl is killed because she eats a poisoned apple. And the word apple has 5 letters.</p>
<p>I will then indicate whether I agree or disagree with your guess and provide rationale, but agreeing with a guess does not confirm its correctness. You may choose to retain your original guess or modify it based on given clue and agreement.</p>
<p>After each guess, your answer will be validated, and you will receive feedback indicating which letters are correct (green), which letters are correct but in the wrong position (yellow), and which letters are incorrect (red). This feedback can be useful in determining which letters to include or exclude in your next guess.</p>
<p>For example, the feedback for "apple" might be:
guess_feedback:
a 〈yellow〉 p 〈yellow〉 p 〈green〉 l 〈yellow〉 e〈red&gt;</p>
<p>Here are some rules to keep in mind: 1. Please do not generate any guess feedback or clue on your own 2. Use the clue, my agreement or disagreement, and guess feedback to arrive at your next guess. This should be reflected in your explanation as well.</p>
<p>Let's begin with your first guess.
clue:display</p>
<h2>TEMPLATE D.1.6</h2>
<p>guess: shelf
explanation: A shelf is a common item used for displaying objects. It has 5 letters.
(a) Prompt template for Player A (Guesser) in the Wordle Game Variant (+Clue, +Critic)</p>
<h2>TEMPLATE D.1.7</h2>
<p>I need your assistance with a word game in which we need to find a 5-letter word using a clue, a guess and an explanation for the guess.</p>
<p>For example, if the clue is "snow white" and the guess is "apple", you will get the information like this:
clue:snow white
guess:apple
explanation:the fairy tale Snow White involves the girl eating a poisoned apple, which has 5 letters.</p>
<p>Your task is to either agree or disagree with my guess based on the given clue.</p>
<p>If you disagree, I will make another guess, and if you agree, I will proceed in the game using the same guess.</p>
<p>Initially, guess feedback is not available, but as the game proceeds, it becomes available. The guess feedback is not for current guess, but rather an indication of what letters can be present in the current guess. A letter highlighted in green indicates that it is a correct letter in the correct position, while yellow indicates that it is a correct letter in the wrong position, and red indicates a wrong letter. At this point, you should use all the provided information, including the clue, guess, explanation, and guess feedback, to decide whether the given guess matches the clue, and also stick to guess feedback or not.</p>
<p>Please respond in lowercase letters and stick to this format:
agreement:yes or no
explanation:your reason for doing so
Please note that you must not guess any word based on the riddle clue and stick to the given format while responding.</p>
<p>Let's begin.
clue:display
guess: shelf
explanation:A shelf is a common item used for displaying objects. It has 5 letters.</p>
<h2>TEMPLATE D.1.8</h2>
<p>agreement: no
explanation: None of the letters in "shelf" match with the letters that could be present in the word based on the given guess feedback.
(b) Prompt template for Player B (Critic) in the Wordle Game Variant (+Clue, +Critic)</p>
<p>Figure 16: Wordle prompt templates for players with clue and critic variants</p>
<p>fluence of the critic's role in the guesser's performance and can lead to interesting possibilities in human-machine interaction, where the human can be aided by the cLLM as the critic. We tested the game using the same cLLM for both roles, as well as different cLLMs for each role, employing distinct prompts for each.</p>
<p>Instantiation In our experiments, we use a list of 2,309 possible target words and a list of 12,953 valid guess words. ${ }^{8}$ For textual clues, we use New York Times crossword clues. ${ }^{9}$ We sort the target words by word frequency. ${ }^{10}$ Out of the initial 2,309 target words, frequency details are not available for one word, and clues are not available for 39 words. These words are subsequently excluded from the experiments. The remaining 2,269 target words are sorted based on their word frequency (descending frequency) and then divided into three equal groups. The first group which contains highfrequency words, has a total of 756 words. The second group, consisting of words with medium frequency, also contains 756 words. Finally, the third group, which contains low-frequency words, has a total of 757 words. To evaluate our methodology, we chose (random seed: 42) 10 words from each frequency group, resulting in a total of 30 target words for evaluation purposes, for each game variant. As metrics, we keep track of the success rate (how often the guesser guessed the target word, within the limit of 6 guesses), the average speed (if successful, then at which turn), and for each turn closeness (based on the letter-feedback). We also keep track of whether the guesser repeats a guess (a strategic failure), and, in the critic variant, whether the guesser changes the guess after feedback.</p>
<p>Error Handling The experiments revolve closely around the cLLM models, which are expected to respond in a specific format and adhere to certain rules. However, there are multiple scenarios where the responses from these models may result in errors.</p>
<ol>
<li>In the Wordle game, a subset of valid fiveletter English words is used. In certain scenarios, the guesser (Player A - cLLM) may guess
<sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>a valid 5-letter word that is not among the allowed guesses. In such cases, cLLM will be asked to guess another word. This reprompting process continues until cLLM makes an allowed guess.</li>
<li>The Wordle game has a strict rule that allows guessing only 5-letter words. Sometimes, the models respond with words that do not adhere to this restriction, causing the reprompting. We allow two reprompting attempts, after which the game is considered aborted.</li>
<li>Sometimes, the response of the cLLM doesn't follow the expected format as stated in the prompt. In such cases, we reprompt the cLLM to generate the response in the expected format. When faced with these circumstances, we usually give two reprompts before declaring the game as aborted.</li>
</ol>
<p>Evaluation For each episode, we record the number of guesses made by the guesser. If the guesser correctly guessed the word in six or fewer attempts, the game is counted as a success. If the guesser exhausted all six attempts, the game is counted as a failure. If the guesser's response does not conform to the game rules, the game is counted as aborted. Of the successful games, the average number of guesses taken to guess the word is computed. For all the games, we also measured how close the guess gets to the target word with each turn. The following are the metrics measured for each episode.</p>
<ol>
<li>Success: This is a binary value and measures whether the guesser guessed the target word or not.</li>
<li>Aborted: This is a binary value and measures whether the game aborted due to noncompliance with the game rules (words not containing 5 letters, words containing symbols other than alphabets).</li>
<li>Speed: How early the word was guessed as measured by $100 / t$, where $t$ is the turn number in which the target was found.</li>
<li>Closeness: This contains the score ranging from 0-to-25 and determines how effectively the guesser utilizes the guess feedback. If a letter is at the correct position 5-points are awarded, and 3-points for letter at other position and 0 -points for incorrect letters, leading to 25 points for a correct guess. Ideally this score should be increase across the turns.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">wordle</th>
<th style="text-align: center;">Played</th>
<th style="text-align: center;">Aborted</th>
<th style="text-align: center;">Success</th>
<th style="text-align: center;">Lose</th>
<th style="text-align: center;">Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{lm}-\mathrm{lm}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">ko-ko</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">flc-flc</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">UNDEF</td>
</tr>
<tr>
<td style="text-align: left;">ost-ost</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">vcn-vcn</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">cl-cl</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">3-3</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">1.25</td>
</tr>
<tr>
<td style="text-align: left;">3.5-3.5</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">4-4</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 3}$</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">3.67</td>
</tr>
<tr>
<td style="text-align: left;">wordle + clue</td>
<td style="text-align: center;">Played</td>
<td style="text-align: center;">Aborted</td>
<td style="text-align: center;">Success</td>
<td style="text-align: center;">Lose</td>
<td style="text-align: center;">Speed</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{lm}-\mathrm{lm}$</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">ko-ko</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">20.00</td>
</tr>
<tr>
<td style="text-align: left;">flc-flc</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr>
<td style="text-align: left;">ost-ost</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">vcn-vcn</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">25.00</td>
</tr>
<tr>
<td style="text-align: left;">cl-cl</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">30.56</td>
</tr>
<tr>
<td style="text-align: left;">3-3</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">31.36</td>
</tr>
<tr>
<td style="text-align: left;">3.5-3.5</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">28.57</td>
</tr>
<tr>
<td style="text-align: left;">4-4</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3}$</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">49.67</td>
</tr>
<tr>
<td style="text-align: left;">wordle + clue + critic</td>
<td style="text-align: center;">Played</td>
<td style="text-align: center;">Aborted</td>
<td style="text-align: center;">Success</td>
<td style="text-align: center;">Lose</td>
<td style="text-align: center;">Speed</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{lm}-\mathrm{lm}$</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">ko-ko</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">UNDEF</td>
</tr>
<tr>
<td style="text-align: left;">flc-flc</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">ost-ost</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">vcn-vcn</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">cl-cl</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">30.77</td>
</tr>
<tr>
<td style="text-align: left;">3-3</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr>
<td style="text-align: left;">3.5-3.5</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">13.19</td>
</tr>
<tr>
<td style="text-align: left;">4-4</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8}$</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">49.11</td>
</tr>
</tbody>
</table>
<p>Table 4: Detailed results for the wordle games (traditional, clue, critic variants).</p>
<h3>D. 2 Additional Discussion of Results</h3>
<p>The detailed results for all three variants of the wordle game is given in Table 4. In terms of overall performance, the best model is GPT-4, followed by GPT-3.5 and Claude, with GPT-3 following after them.</p>
<p>GPT-4 is the only model that can always follow the game rules (Played) in all 3 variants. While its performance for the traditional wordle game is relatively low with a success rate of 0.23 , this score is greatly increased by adding a clue ( 0.73 ) or a critic (0.8). Likewise, the speed metric is increased from 3.67 to 49.67 and 49.11 , respectively, meaning that on average the model can find the target word on the second guess in these settings.</p>
<p>For the other models, the regular wordle game seems to be too difficult to play, i.e. following only letter-based feedback is too difficult as the high Lose numbers show. Except for Falcon, they can however follow the game rules (cf. Played) in at least half of the episodes.</p>
<p>For several models, the Played score decreases in the extended games variants (clue; clue+critic). The game rules seem to be too difficult for Luminous, Koala, Vicuna, and GPT-3 that drop to scores
$0.03,0.17,0.13,0.37$ in the clue variant and 0.10 , $0.00,0.20,0.23$ in the clue+critic variant. For GPT3.5, the drop is smaller (from 1.00 to 0.93 and 0.77 , respectively).</p>
<p>Because of the high number of aborted episodes, we present results for Closeness only for the GPT-4 model in Figure 17. The figure shows the closeness score for all episode that GPT-4 has played, grouped by game variant and word frequency. It appears that the word frequency may have an effect in the case of the extended game variants: The higher the word frequency, the more stable the progression towards the target seems to be. This figure also reflects the speed: We can see that adding a clue and critic results in the model being able to guess the target word correctly on the first attempt (indicated by 'circle' markers in the plots) in multiple episodes.</p>
<p>Comparison with Human Performance While we were unable to find actual playing statistics, there are plenty of blog posts advising players on playing strategies. The New York Times (who is hosting the official game version) is suggesting the</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://github.com/3b1b/videos/blob/ master/_2022/wordle/data/allowed_words.txt https://github.com/3b1b/videos/blob/master/ _2022/wordle/data/possible_words.txt
${ }^{9}$ https://www.kaggle.com/datasets/darinhawley/ new-york-times-crossword-clues-answers-19932021
${ }^{10}$ https://www.kaggle.com/datasets/rtatman/ english-word-frequency&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Thanks are due to Carl T. Bergstrom for bringing this joke and its applicability to the situation to our attention; https:// fediscience.org/@ct_bergstrom/110273442253894015.
${ }^{2}$ We will call such models cLLMs from here on, for "chatoptimized LLM", with the suggested pronunciation "clem".&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
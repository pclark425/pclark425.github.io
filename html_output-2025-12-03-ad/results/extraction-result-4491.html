<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4491 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4491</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4491</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-47cab9b4f16c5dcb34c109a60281df0138f885b8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/47cab9b4f16c5dcb34c109a60281df0138f885b8" target="_blank">Symbolic Regression with a Learned Concept Library</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset, which substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms.</p>
                <p><strong>Paper Abstract:</strong> We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, uses zero-shot queries to a large language model (LLM) to discover and evolve concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered, hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LaSR can be used to discover a novel and powerful scaling law for LLMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4491.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4491.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LASR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Regression with a Learned Concept Library (LASR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An SR framework that interleaves evolutionary search with zero-shot LLM queries to induce a reusable natural-language concept library, using LLM-guided mutation/crossover and concept abstraction/evolution to discover compact symbolic laws from data (including a novel LLM scaling law).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LASR (Symbolic Regression with a Learned Concept Library)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LASR augments a multi-population genetic symbolic-regression engine (PySR) with three LLM-guided operations: Llminit (LLM-based initialization), LlmMutate (LLM-conditioned mutation), and LlmCrossover (LLM-conditioned crossover). At each evolutionary step a mixture probability p determines whether a standard genetic operator or an LLM-guided operator is used. After each generation LASR extracts Pareto-optimal and worst-performing expressions and issues a zero-shot prompt (CONCEPTABSTRACTION) to an LLM to summarize recurring useful patterns into natural-language concepts, which are added to a concept library. A ConceptEvolution step uses LLM prompts to propose new concept variants. Concepts are sampled (top-K recent) and concatenated with task variable names and DSL specs to prompt the LLM to produce concept-respecting candidate programs. The loop alternates between hypothesis evolution (maximize p(D|π) and parsimony) and concept abstraction/evolution (maximize p(π|C) and p(C)). Zero-shot prompting and in-context examples are used (no fine-tuning). LASR was used to (i) discover equations on the Feynman benchmark and (ii) induce a compact empirical LLM scaling law from BigBench data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125, llama3-8b (used as backbones in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>llama3-8b: 8B; gpt-3.5-turbo-0125: not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics (Feynman equations benchmark), Machine Learning / NLP (LLM scaling laws on BigBench), synthetic symbolic-math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (operates on datasets); evaluated on Feynman dataset (100 target equations) and BigBench subset (204 tasks, 53,812 samples; used 43,049 train / 10,763 validation in reported fit)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Symbolic mathematical equations and empirical scaling laws (explicit closed-form equations, e.g., scaling laws relating training hyperparameters and performance)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Discovered scaling-law form: score = A / ((train_steps / B)^{#shots}) + E. Also proposed Chinchilla-augmented skeletons (see paper for fitted parameters). LASR recovered many Feynman physics equations (exact-match solves: 72/100 in reported table).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Data-driven symbolic regression augmented by LLMs: (1) evolve candidate symbolic expressions by mixed genetic + LLM-guided operators, (2) extract Pareto-optimal expressions and worst expressions, (3) prompt LLM zero-shot to abstract recurring patterns into natural-language concepts, (4) use those concepts to bias subsequent LLM-guided generation of programs. No direct parsing of literature; extraction is from datasets and candidate expressions, not from paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Quantitative evaluation on standard SR benchmarks (Feynman equations: exact-match solve rate and MSE thresholds), evaluation on a procedurally-generated synthetic set (R^2), and quantitative fit comparison on BigBench (MSE on held-out validation set) against baseline scaling-law skeletons (Chinchilla and variants). Qualitative analysis of learned concepts also presented.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Feynman exact-match solve rate: LASR 72/100 (table) vs PySR 59/100; Cascade ablations show up to 71/100 depending on p and backbone. Synthetic dataset R^2: PySR 0.070 vs LaSR (llama3-8b, 0.1%) 0.913. BigBench scaling-law MSE (validation): Equation 4 MSE = 0.03598 ± 0.00265; Chinchilla MSE = 0.03610 ± 0.00268; Modified Chinchilla MSE = 0.03598 ± 0.00264.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>On Feynman benchmark: 72/100 exact matches; on synthetic benchmark substantial R^2 improvement (0.913 vs 0.070); BigBench: discovered equation achieves MSE comparable to Chinchilla baseline (see metrics above).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Authors note LASR cannot guarantee correctness or insightfulness of learned concepts (concepts may be misleading or inconsistent), concept library consistency is not enforced, results are bottlenecked by underlying LLM capabilities and compute, and learned concepts may reflect model/data quirks; possible risk of dataset memorization was investigated and synthetic experiments indicate improvements persist when memorization is unlikely.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against PySR (ablation without LLM operations), GPlearn, AFP variants, DSR, uDSR, AI Feynman. LASR outperformed these baselines on the Feynman benchmark (e.g., PySR 59/100 exact vs LASR 72/100). On BigBench scaling-law fitting LASR's discovered skeleton matched Chinchilla performance with fewer free parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Regression with a Learned Concept Library', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4491.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4491.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FunSearch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mathematical discoveries from program search with large language models (Funsearch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses pretrained large language models to implement program-space mutation operations over a database of executable programs to discover novel mathematical results and optimized programs, particularly demonstrated in extremal combinatorics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical discoveries from program search with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FunSearch (LLM-guided program search)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>FunSearch conditions program generation on a static specification (a database of executable programs) and uses a pretrained LLM as a mutation operator to propose program variations; these candidates are executed/evaluated and high-performing ones guide further search. It was used to find mathematical/algorithmic discoveries in extremal combinatorics (as cited). In LASR's description FunSearch is described as using an LLM to implement mutation on executable programs to find super-optimized programs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics / theoretical computer science (mathematical discovery, extremal combinatorics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (single referenced study)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Mathematical discoveries / optimized programmatic solutions (theorems, optimized combinatorial constructions)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not specified in detail in LASR paper; referenced as producing mathematical discoveries in extremal combinatorics in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM-conditioned program mutation over an executable program database; candidate programs are executed and evaluated to guide search.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not detailed in LASR; the original FunSearch work (cited) validated via execution/evaluation of candidate programs and demonstrated novel mathematical findings (see original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LASR notes FunSearch conditions on a static specification whereas LASR learns a concept library; LASR did not directly compare to FunSearch due to resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>FunSearch is presented as prior related work; LASR positions itself as a generalization (dynamic learned concept library vs static spec) but does not empirically compare in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Regression with a Learned Concept Library', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4491.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4491.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR: Scientific equation discovery via programming with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method leveraging pretrained LLMs to generate program sketches for symbolic regression; sketches' parameters are optimized and cached to generate new sketches and guide discovery of equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>llm-sr: Scientific equation discovery via programming with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-SR uses a pretrained LLM to produce program sketches (partial program templates) which are then optimized (fit parameters) and cached; the cached sketches/datapoints are used to generate additional sketches and accelerate equation discovery. It frames SR as program generation driven by an LLM-based sketching mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific equation discovery / symbolic regression (general scientific domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (single referenced study)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Symbolic equations and programmatic hypotheses for data (mathematical expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not provided in LASR text.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM-generated program sketches followed by parameter optimization and caching.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not detailed in LASR; referenced as related work. Validation presumably via symbolic regression benchmarks in original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Described as orthogonal to LASR; LASR suggests its concept library could be used to guide lower-level sketch generation in frameworks like LLM-SR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work; no empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Regression with a Learned Concept Library', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4491.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4491.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context symbolic regression: Leveraging language models for function discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that leverages language models' in-context learning to perform symbolic regression, framing function discovery as an in-context generation problem for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In-context symbolic regression: Leveraging language models for function discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>In-context symbolic regression (LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Frames symbolic regression as an in-context learning problem for LLMs: the DSL and I/O examples are serialized into prompts and the LLM autoregressively generates candidate functions. Emphasizes using LLMs' in-context capabilities to discover mathematical functions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Symbolic regression / function discovery (general scientific domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (single referenced study)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Mathematical functions and symbolic expressions</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not given in LASR text.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompting LLMs in-context with examples and DSL specification to directly generate candidate symbolic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described here; referenced as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in LASR.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in LASR.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LASR positions its approach as complementary: instead of relying solely on amortized generation, LASR uses a learned concept library to bias search and combine LLM guidance with evolutionary search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned among related LLM-for-SR methods; no direct empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Regression with a Learned Concept Library', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4491.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4491.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LILO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LILO: Learning interpretable libraries by compressing and documenting code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A library-learning method that uses LLM guidance to assist program induction and to auto-document learned library modules, coupling code compression with interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lilo: Learning interpretable libraries by compressing and documenting code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LILO (LLM-assisted library learning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LILO applies LLM guidance to assist in program induction and to automatically document / compress learned library functions, framing library learning as learning interpretable subroutines with language-based documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Program synthesis / library learning (applies to scientific program induction contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (single referenced study)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Programmatic abstractions and interpretable library functions (indirectly supports discovery of reusable programmatic motifs rather than direct numeric laws)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not applicable in LASR text.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM guidance for program induction and auto-documentation of learned library modules.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not detailed in LASR; cited as related work in library-learning literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in LASR.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in LASR.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LASR notes similarity in problem formulation (optimizing over programs and natural-language descriptions) but differs by optimizing over natural-language concept libraries rather than DSL libraries; no empirical comparison here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as related prior work on library learning; no direct empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Regression with a Learned Concept Library', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathematical discoveries from program search with large language models <em>(Rating: 2)</em></li>
                <li>llm-sr: Scientific equation discovery via programming with large language models <em>(Rating: 2)</em></li>
                <li>In-context symbolic regression: Leveraging language models for function discovery <em>(Rating: 2)</em></li>
                <li>Lilo: Learning interpretable libraries by compressing and documenting code <em>(Rating: 1)</em></li>
                <li>Training compute-optimal large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4491",
    "paper_id": "paper-47cab9b4f16c5dcb34c109a60281df0138f885b8",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "LASR",
            "name_full": "Symbolic Regression with a Learned Concept Library (LASR)",
            "brief_description": "An SR framework that interleaves evolutionary search with zero-shot LLM queries to induce a reusable natural-language concept library, using LLM-guided mutation/crossover and concept abstraction/evolution to discover compact symbolic laws from data (including a novel LLM scaling law).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LASR (Symbolic Regression with a Learned Concept Library)",
            "system_description": "LASR augments a multi-population genetic symbolic-regression engine (PySR) with three LLM-guided operations: Llminit (LLM-based initialization), LlmMutate (LLM-conditioned mutation), and LlmCrossover (LLM-conditioned crossover). At each evolutionary step a mixture probability p determines whether a standard genetic operator or an LLM-guided operator is used. After each generation LASR extracts Pareto-optimal and worst-performing expressions and issues a zero-shot prompt (CONCEPTABSTRACTION) to an LLM to summarize recurring useful patterns into natural-language concepts, which are added to a concept library. A ConceptEvolution step uses LLM prompts to propose new concept variants. Concepts are sampled (top-K recent) and concatenated with task variable names and DSL specs to prompt the LLM to produce concept-respecting candidate programs. The loop alternates between hypothesis evolution (maximize p(D|π) and parsimony) and concept abstraction/evolution (maximize p(π|C) and p(C)). Zero-shot prompting and in-context examples are used (no fine-tuning). LASR was used to (i) discover equations on the Feynman benchmark and (ii) induce a compact empirical LLM scaling law from BigBench data.",
            "model_name": "gpt-3.5-turbo-0125, llama3-8b (used as backbones in experiments)",
            "model_size": "llama3-8b: 8B; gpt-3.5-turbo-0125: not specified in paper",
            "scientific_domain": "Physics (Feynman equations benchmark), Machine Learning / NLP (LLM scaling laws on BigBench), synthetic symbolic-math benchmarks",
            "number_of_papers": "Not applicable (operates on datasets); evaluated on Feynman dataset (100 target equations) and BigBench subset (204 tasks, 53,812 samples; used 43,049 train / 10,763 validation in reported fit)",
            "law_type": "Symbolic mathematical equations and empirical scaling laws (explicit closed-form equations, e.g., scaling laws relating training hyperparameters and performance)",
            "law_examples": "Discovered scaling-law form: score = A / ((train_steps / B)^{#shots}) + E. Also proposed Chinchilla-augmented skeletons (see paper for fitted parameters). LASR recovered many Feynman physics equations (exact-match solves: 72/100 in reported table).",
            "extraction_method": "Data-driven symbolic regression augmented by LLMs: (1) evolve candidate symbolic expressions by mixed genetic + LLM-guided operators, (2) extract Pareto-optimal expressions and worst expressions, (3) prompt LLM zero-shot to abstract recurring patterns into natural-language concepts, (4) use those concepts to bias subsequent LLM-guided generation of programs. No direct parsing of literature; extraction is from datasets and candidate expressions, not from paper text.",
            "validation_approach": "Quantitative evaluation on standard SR benchmarks (Feynman equations: exact-match solve rate and MSE thresholds), evaluation on a procedurally-generated synthetic set (R^2), and quantitative fit comparison on BigBench (MSE on held-out validation set) against baseline scaling-law skeletons (Chinchilla and variants). Qualitative analysis of learned concepts also presented.",
            "performance_metrics": "Feynman exact-match solve rate: LASR 72/100 (table) vs PySR 59/100; Cascade ablations show up to 71/100 depending on p and backbone. Synthetic dataset R^2: PySR 0.070 vs LaSR (llama3-8b, 0.1%) 0.913. BigBench scaling-law MSE (validation): Equation 4 MSE = 0.03598 ± 0.00265; Chinchilla MSE = 0.03610 ± 0.00268; Modified Chinchilla MSE = 0.03598 ± 0.00264.",
            "success_rate": "On Feynman benchmark: 72/100 exact matches; on synthetic benchmark substantial R^2 improvement (0.913 vs 0.070); BigBench: discovered equation achieves MSE comparable to Chinchilla baseline (see metrics above).",
            "challenges_limitations": "Authors note LASR cannot guarantee correctness or insightfulness of learned concepts (concepts may be misleading or inconsistent), concept library consistency is not enforced, results are bottlenecked by underlying LLM capabilities and compute, and learned concepts may reflect model/data quirks; possible risk of dataset memorization was investigated and synthetic experiments indicate improvements persist when memorization is unlikely.",
            "comparison_baseline": "Compared against PySR (ablation without LLM operations), GPlearn, AFP variants, DSR, uDSR, AI Feynman. LASR outperformed these baselines on the Feynman benchmark (e.g., PySR 59/100 exact vs LASR 72/100). On BigBench scaling-law fitting LASR's discovered skeleton matched Chinchilla performance with fewer free parameters.",
            "uuid": "e4491.0",
            "source_info": {
                "paper_title": "Symbolic Regression with a Learned Concept Library",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "FunSearch",
            "name_full": "Mathematical discoveries from program search with large language models (Funsearch)",
            "brief_description": "A method that uses pretrained large language models to implement program-space mutation operations over a database of executable programs to discover novel mathematical results and optimized programs, particularly demonstrated in extremal combinatorics.",
            "citation_title": "Mathematical discoveries from program search with large language models",
            "mention_or_use": "mention",
            "system_name": "FunSearch (LLM-guided program search)",
            "system_description": "FunSearch conditions program generation on a static specification (a database of executable programs) and uses a pretrained LLM as a mutation operator to propose program variations; these candidates are executed/evaluated and high-performing ones guide further search. It was used to find mathematical/algorithmic discoveries in extremal combinatorics (as cited). In LASR's description FunSearch is described as using an LLM to implement mutation on executable programs to find super-optimized programs.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Mathematics / theoretical computer science (mathematical discovery, extremal combinatorics)",
            "number_of_papers": "Not applicable (single referenced study)",
            "law_type": "Mathematical discoveries / optimized programmatic solutions (theorems, optimized combinatorial constructions)",
            "law_examples": "Not specified in detail in LASR paper; referenced as producing mathematical discoveries in extremal combinatorics in the cited work.",
            "extraction_method": "LLM-conditioned program mutation over an executable program database; candidate programs are executed and evaluated to guide search.",
            "validation_approach": "Not detailed in LASR; the original FunSearch work (cited) validated via execution/evaluation of candidate programs and demonstrated novel mathematical findings (see original paper).",
            "performance_metrics": "Not reported in detail in this paper.",
            "success_rate": "Not reported in this paper.",
            "challenges_limitations": "LASR notes FunSearch conditions on a static specification whereas LASR learns a concept library; LASR did not directly compare to FunSearch due to resource constraints.",
            "comparison_baseline": "FunSearch is presented as prior related work; LASR positions itself as a generalization (dynamic learned concept library vs static spec) but does not empirically compare in this paper.",
            "uuid": "e4491.1",
            "source_info": {
                "paper_title": "Symbolic Regression with a Learned Concept Library",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-SR",
            "name_full": "LLM-SR: Scientific equation discovery via programming with large language models",
            "brief_description": "A method leveraging pretrained LLMs to generate program sketches for symbolic regression; sketches' parameters are optimized and cached to generate new sketches and guide discovery of equations.",
            "citation_title": "llm-sr: Scientific equation discovery via programming with large language models",
            "mention_or_use": "mention",
            "system_name": "LLM-SR",
            "system_description": "LLM-SR uses a pretrained LLM to produce program sketches (partial program templates) which are then optimized (fit parameters) and cached; the cached sketches/datapoints are used to generate additional sketches and accelerate equation discovery. It frames SR as program generation driven by an LLM-based sketching mechanism.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Scientific equation discovery / symbolic regression (general scientific domains)",
            "number_of_papers": "Not applicable (single referenced study)",
            "law_type": "Symbolic equations and programmatic hypotheses for data (mathematical expressions)",
            "law_examples": "Not provided in LASR text.",
            "extraction_method": "LLM-generated program sketches followed by parameter optimization and caching.",
            "validation_approach": "Not detailed in LASR; referenced as related work. Validation presumably via symbolic regression benchmarks in original paper.",
            "performance_metrics": "Not reported in this paper.",
            "success_rate": "Not reported in this paper.",
            "challenges_limitations": "Described as orthogonal to LASR; LASR suggests its concept library could be used to guide lower-level sketch generation in frameworks like LLM-SR.",
            "comparison_baseline": "Mentioned in related work; no empirical comparison in this paper.",
            "uuid": "e4491.2",
            "source_info": {
                "paper_title": "Symbolic Regression with a Learned Concept Library",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "In-context SR",
            "name_full": "In-context symbolic regression: Leveraging language models for function discovery",
            "brief_description": "An approach that leverages language models' in-context learning to perform symbolic regression, framing function discovery as an in-context generation problem for LLMs.",
            "citation_title": "In-context symbolic regression: Leveraging language models for function discovery",
            "mention_or_use": "mention",
            "system_name": "In-context symbolic regression (LLM-based)",
            "system_description": "Frames symbolic regression as an in-context learning problem for LLMs: the DSL and I/O examples are serialized into prompts and the LLM autoregressively generates candidate functions. Emphasizes using LLMs' in-context capabilities to discover mathematical functions.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Symbolic regression / function discovery (general scientific domains)",
            "number_of_papers": "Not applicable (single referenced study)",
            "law_type": "Mathematical functions and symbolic expressions",
            "law_examples": "Not given in LASR text.",
            "extraction_method": "Prompting LLMs in-context with examples and DSL specification to directly generate candidate symbolic expressions.",
            "validation_approach": "Not described here; referenced as related work.",
            "performance_metrics": "Not reported in LASR.",
            "success_rate": "Not reported in LASR.",
            "challenges_limitations": "LASR positions its approach as complementary: instead of relying solely on amortized generation, LASR uses a learned concept library to bias search and combine LLM guidance with evolutionary search.",
            "comparison_baseline": "Mentioned among related LLM-for-SR methods; no direct empirical comparison in this paper.",
            "uuid": "e4491.3",
            "source_info": {
                "paper_title": "Symbolic Regression with a Learned Concept Library",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LILO",
            "name_full": "LILO: Learning interpretable libraries by compressing and documenting code",
            "brief_description": "A library-learning method that uses LLM guidance to assist program induction and to auto-document learned library modules, coupling code compression with interpretability.",
            "citation_title": "Lilo: Learning interpretable libraries by compressing and documenting code",
            "mention_or_use": "mention",
            "system_name": "LILO (LLM-assisted library learning)",
            "system_description": "LILO applies LLM guidance to assist in program induction and to automatically document / compress learned library functions, framing library learning as learning interpretable subroutines with language-based documentation.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Program synthesis / library learning (applies to scientific program induction contexts)",
            "number_of_papers": "Not applicable (single referenced study)",
            "law_type": "Programmatic abstractions and interpretable library functions (indirectly supports discovery of reusable programmatic motifs rather than direct numeric laws)",
            "law_examples": "Not applicable in LASR text.",
            "extraction_method": "LLM guidance for program induction and auto-documentation of learned library modules.",
            "validation_approach": "Not detailed in LASR; cited as related work in library-learning literature.",
            "performance_metrics": "Not reported in LASR.",
            "success_rate": "Not reported in LASR.",
            "challenges_limitations": "LASR notes similarity in problem formulation (optimizing over programs and natural-language descriptions) but differs by optimizing over natural-language concept libraries rather than DSL libraries; no empirical comparison here.",
            "comparison_baseline": "Mentioned as related prior work on library learning; no direct empirical comparison in this paper.",
            "uuid": "e4491.4",
            "source_info": {
                "paper_title": "Symbolic Regression with a Learned Concept Library",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathematical discoveries from program search with large language models",
            "rating": 2
        },
        {
            "paper_title": "llm-sr: Scientific equation discovery via programming with large language models",
            "rating": 2
        },
        {
            "paper_title": "In-context symbolic regression: Leveraging language models for function discovery",
            "rating": 2
        },
        {
            "paper_title": "Lilo: Learning interpretable libraries by compressing and documenting code",
            "rating": 1
        },
        {
            "paper_title": "Training compute-optimal large language models",
            "rating": 1
        }
    ],
    "cost": 0.016803,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Symbolic Regression with a Learned Concept Library</h1>
<p>Arya Grayeli<em><br>UT Austin, Foundry Technologies<br>Atharva Sehgal</em><br>UT Austin<br>Amar Costilla-Reyes<br>MIT<br>Miles Cranmer<br>University of Cambridge<br>Swarat Chaudhuri<br>UT Austin</p>
<h4>Abstract</h4>
<p>We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LASR, uses zero-shot queries to a large language model (LLM) to discover and evolve abstract concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered, hypotheses are used in a new round of concept abstraction and evolution. We validate LASR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LASR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LASR can be used to discover a new and powerful scaling law for LLMs.</p>
<h2>1 Introduction</h2>
<p>Symbolic regression (SR) [33] is the task of finding succinct programmatic hypotheses - written in a flexible, domain-specific programming language - that best explain a dataset. Initially proposed in the 1970s, SR has recently emerged as a prominent approach to automated scientific discovery, with applications in domains from astrophysics [30, 12] to chemistry [2, 22] to medicine [51].
Computational complexity is a fundamental challenge in SR, as the space of hypotheses that an SR algorithm must search is discrete and exponential. Previous work has approached this challenge using methods like genetic programming [41, 10], neural-guided search [11, 43], deep reinforcement learning [38] and hybrid algorithms [28]. However, new tools to enhance the scalability of SR remain a critical need for applications in SR and scientific discovery.
In this paper, we show that abstraction and knowledge-directed discovery can be powerful principles in building such scaling tools in SR. State-of-the-art genetic algorithms for SR [10] evolve pools of candidate hypotheses using random mutation and crossover operations. By contrast, a human scientist does not just randomly mutate their explanations of data. Instead, they synthesize background knowledge and empirical observations into abstract concepts, then use these concepts to derive new explanations. We show that zero-shot queries to large language models (LLMs) can be used to implement such a discovery process on top of a standard SR algorithm.
Concretely, we present a new method for symbolic regression, called LASR, that discovers a library of abstract, reusable and interpretable textual concepts and uses it to accelerate SR. LASR alternates between three phases: (i) concept-directed hypothesis evolution, where standard genetic operations</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of LASR. LASR iteratively refines a library of interpretable textual concepts which are used to bias the search for hypotheses for scientific discovery tasks. This involves three distinct phases: (Top) finding optimal hypotheses within a concept-directed hypothesis evolution, (Right) leveraging the optimal hypotheses to find new concept abstractions, and (Left) iterating on learned concepts to discover new concepts to accelerate hypothesis evolution. LASR introduces an orthogonal direction of improvement over current symbolic regression algorithms [10] (in gray).
over hypotheses are interleaved with LLM-guided mutation and crossover operations conditioned on known library concepts; (ii) the LLM-based abstraction of patterns in known high-performing hypotheses into new concepts; and (iii) the LLM-directed evolution of concepts into more succinct and general forms. Together, these three steps form an open-ended alternating maximization loop that combines evolutionary exploration with the exploitation of the LLM's background knowledge and in-context learning ability.
We experimentally compare LASR on Feynman Equations [26] — a popular SR benchmark in which the goal is to discover 100 equations from the Feynman Lectures in Physics - against several state-of-the-art genetic and deep learning approaches. LASR can discover 66 of the 100 target equations, while the best existing approach can solve 59. To address the concern that LASR's performance could be attributed to test set leakage, we compare LASR with a state-of-the-art genetic approach on a suite of synthetic benchmarks. We show that LASR substantially outperforms the baseline. Finally, we support the contribution of LASR to the research community by evaluating the methodology in a case study where the goal is to find new scaling laws for LLMs.
In summary, the contributions of this paper are as follows:</p>
<ul>
<li>We pose the problem of discovering an open-ended, reusable concept library that can accelerate solutions to downstream SR tasks.</li>
<li>We present LASR, a method for combining zero-shot LLM queries and standard evolutionary operations to simultaneously induce a concept library and high-performing hypotheses. LASR's strategy of using LLMs to accelerate evolutionary algorithms may have future applications in settings beyond SR.</li>
<li>We offer promising experimental results, including a demonstration that LASR outperforms state-of-the-art algorithms in standard SR tasks and synthetic domains, as well as a case study that uses LASR to discover a novel LLM scaling law.</li>
</ul>
<h1>2 Problem Formulation</h1>
<p>Symbolic Regression. We formulate symbolic regression (SR) as a program synthesis [6] problem. The inputs to this problem include a language $\mathcal{L}$ of programmatic hypotheses and a dataset $\mathcal{D}:=$ $\left{\left(\mathbf{x}<em i="i">{i}, \mathbf{y}</em>\right)\right}<em _mathcal_L="\mathcal{L">{i=1}^{N}$ of input-output examples. The syntax of $\mathcal{L}$ is described by a context-free grammar [24]. The grammar allows each hypothesis $\pi$ to be represented using a set of mathematical operators (e.g., addition, multiplication, trigonometric functions) that facilitate the composition of simpler hypotheses into more complex ones. We abstractly define the fitness of a hypothesis $\pi$ as the likelihood $p</em>$.
In order to prevent finding non-useful solutions, we impose a prior probability distribution $p_{\mathcal{L}}(\pi)$ over hypotheses $\pi$ that penalizes syntactically complex hypotheses. We now pose SR as the task of finding a hypothesis $\pi^{\star}$ that maximizes the fitness while minimizing syntactic complexity. The problem can be expressed as a maximum a posteriori (MAP) estimation problem [15]:}}(\mathcal{D} \mid \pi)$ that it generates $\mathcal{D</p>
<p>$$
\pi^{\star}=\arg \max <em _mathcal_L="\mathcal{L">{\pi} p</em>)=\arg \max }}(\pi \mid \mathcal{D<em _mathcal_L="\mathcal{L">{\pi} \underbrace{p</em>}}(\mathcal{D} \mid \pi)<em _mathcal_L="\mathcal{L">{\text {optimization }} \cdot \underbrace{p</em>
$$}}(\pi)}_{\text {regularization }</p>
<p>Recent work leverages large language models (LLMs) for program synthesis [8, 19, 32]. Large language models (LLMs) approach program synthesis as a token prediction problem, directly approximating the likelihood of programs by training on internet-scale datasets. That is,</p>
<p>$$
p_{\mathcal{L}}(\pi \mid \mathcal{D}) \approx p_{\mathrm{LLM}}(\langle\pi\rangle \mid\langle\mathcal{L}\rangle, \operatorname{desc}(\mathcal{D}))
$$</p>
<p>where $\langle\pi\rangle$ and $\langle\mathcal{L}\rangle$ are, respectively, textual representations of $\pi$ and a specification of the syntax of $\mathcal{L}$, and the task description $\operatorname{desc}(\mathcal{D})$ is a few-shot serialization of a subset of the examples in $\mathcal{D}$.</p>
<p>Symbolic Regression with Latent Concept Libraries. Classical symbolic regression typically assumes no prior knowledge or intuition about the problem. In contrast, human scientific discovery often leverages empirical patterns [52] and intuitions derived from previously observed data. For example, recognizing a 'power law relationship between variables' has led to the formulation of fundamental empirical laws across various fields, such as the Arrhenius equation in Chemistry, the Rydberg formula in Physics, Zipf's law in Linguistics, and Moore's law in Computer Science.
We model such empirical patterns as natural-language concepts drawn from a latent concept library $\mathcal{C}$. We frame the relationship between the concept library and programs as a Hierarchical Bayesian model consisting of: (i) a prior $p(\mathcal{C})$ representing the natural distribution over concept libraries; (ii) a model $p_{\mathcal{L}}(\pi \mid \mathcal{C})$ that quantifies the likelihood of various hypotheses for a given concept library $\mathcal{C}$; and (iii) the previously mentioned fitness function $p_{\mathcal{L}}(\mathcal{D} \mid \pi)$ for programs $\pi$. We assume that the distributions $p(\mathcal{C})$ and $p_{\mathcal{L}}(\pi \mid \mathcal{C})$ can be approximated using LLMs. That is, we can prompt an LLM to generate interesting concepts, and we can prompt an LLM with a set of concepts to generate token-sequence representations of hypotheses that adhere to the concepts. Now we state the problem of symbolic regression with latent concept learning as one of simultaneously inducing an optimal concept library and an optimal programmatic hypothesis:</p>
<p>$$
\arg \max <em _mathcal_C="\mathcal{C" _pi_="\pi,">{\pi, \mathcal{C}} p(\pi, \mathcal{C} \mid \mathcal{D})=\arg \max </em>}} \underbrace{p(\mathcal{D} \mid \pi)<em LLM="LLM" _By="{By" _text="\text">{\text {By execution }} \cdot \underbrace{p(\pi \mid \mathcal{C})}</em>
$$}} \cdot \underbrace{p(\mathcal{C})}_{\text {By LLM }</p>
<h2>3 Method</h2>
<p>LASR performs a two-stage evolution over natural-language concepts and programmatic hypotheses. The two stages follow an alternating maximization strategy shown in Figure 1: (1) Hypothesis evolution: We fix the set of concepts and focus on maximizing the hypotheses' fitness to the dataset, and (2) Concept abstraction and evolution: We leverage the best hypotheses found to induce a new library of concepts.
In the rest of this section, we first describe PySR, the SR algorithm [10] that LASR extends. Next, we show how to modify this algorithm into one guided by natural-language concepts. Finally, we show how these concepts can be naturally extracted and evolved into new concepts. The full LASR algorithm is presented in Algorithm 1 and visualized in Figure 2. LASR is built in Julia with an additional Python interface ${ }^{2}$ and uses an open-source, optimized framework for LLM inference [25].</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A single step of LASR. LASR induces multiple hypothesis populations that are evolved using a scalable evolutionary algorithm. Concept guidance is provided by randomly replacing symbolic operations with concept-directed LLM operations with probability $p$. After each iteration, the top-performing programs are summarized into natural language concepts, which are evolved to form new concepts that are sampled to guide the search in the next iteration.</p>
<p>Base Algorithm: PySR. LASR builds on PySR [10], a scalable, parallelizable genetic search algorithm for SR. The search in PySR maintains multiple populations $\left{\Pi_{1}, \ldots, \Pi_{k}\right}$ of hypotheses, with each hypothesis represented as an expression tree. In its initialization step, captured by a procedure INITIALIZEPOPULATIONS, PySR creates a new expression at random to insert into a population. After running this step, PySR runs a genetic search, encapsulated in a procedure SRCYCLE, which evolve these populations in parallel, simplifies and optimizes the constants of the resulting hypotheses, and then migrates top-performing hypotheses between populations.
Like other evolutionary algorithms, the search in PySR uses symbolic mutation and crossover operations. The mutation step is broken into many categories, each with distinct weighting, to either mutate a constant, mutate an operator, add a node (append, prepend, insert), delete a subtree of an expression tree, simplify the tree, initialize a new tree, or do nothing. One of these operations is randomly selected at each call to a mutation request, and each operation executes itself at random but within user-provided constraints. For example, deleting a subtree is done by choosing a random node to replace with a randomly-generated leaf node such as a feature or constant. The crossover step involves swapping random subtrees of two expressions in a population.</p>
<p>LLM-guided Hypothesis Evolution. LASR speeds up PySR by injecting natural language priors into its search procedure. To do this, we modify the INITIALIZEPOPULATIONS procedure to use an LLM-augmented initialization operation, and the SRCYCLE routine to use LLM-augmented versions of its symbolic mutation and crossover operations. The altered procedures are named Llminit, LlmMutate, and LlmCrossover, respectively. These operations do not replace their standard genetic counterparts. Instead, we introduce a hyperparameter $p$ that, with a fixed probability, substitutes the standard genetic operation with the LLM-based operation. This enables "doping" each population with a program that respects the language priors, while ensuring that we do not bottleneck the local exploration of the search space.
The LLM-guided operations follow the same base format: they sample multiple concepts from the concept library, concatenate these concepts with the task-specific variable names and language operations, and append a specialized prompt for each task. We employ zero-shot prompts (see Appendix A. 2 for more details) to avoid sampling biases. In further detail:</p>
<ul>
<li>Llminit: The Llminit function takes an initial set of concepts and uses them to initialize the populations for the evolutionary search step. The initial set of concepts can either be instantiated from an optional set of user-provided "hints" or generated by the LLM.</li>
<li>LlmMutate: For mutation within a population, we sample a set of $l$ concepts from the concept library $C$, and construct a prompt that uses this set of concepts to mutate an expression $\pi_{i}$ into $\pi_{j}$. The prompt to the LLM takes inspiration from the standard genetic mutation operation, and asks it to mutate the expression given the concepts sampled from the library.</li>
</ul>
<p>Algorithm 1 Pseudocode for LASR. LASR takes as input an optional set of user-provided hints $\mathcal{C}_{0}$, a dataset of input-output pairs of high-dimensional data $\mathcal{D}$, and four hyperparameters: the number of iterations $I$, the number of populations $K$, the number of steps for concept evolution $M$, and the mixture probability of using LLM-based or GP-based evolutionary operators $p$. LASR produces two artifacts: the evolved library of concepts $\mathcal{C}$ and the expression with the highest fitness score $\pi^{<em>}$.
1: function $\operatorname{LASR}\left(\mathcal{C}<em i="i">{0}, \mathcal{D}=\left{\left(\mathbf{x}</em>}, \mathbf{y<em i="1">{i}\right)\right}</em>\right)$
2: $\mathcal{C} \leftarrow$ INITIALIZECONTEXTLIBRARY $\left(\mathcal{C}}^{N}, I, K, M, \mathrm{p<em 1="1">{0}\right)$ $\triangleright$ Add (optional) user hints to library.
3: $\left{\Pi</em>, K)$
4: for _ in range $(I)$ do
5: for $i$ in range $(K)$ do
6: $\quad \Pi_{i} \leftarrow \operatorname{SRCYCLE}\left(\Pi_{i}, \mathcal{D}, \mathcal{C}, p\right)$
$\triangleright$ Interleaved Symbolic + LLM Search
7: $\quad \mathcal{F} \leftarrow$ EXTRACTPARETOFRONTIER $\left(\left{\Pi_{1} \ldots \Pi_{K}\right}, \mathcal{D}\right)$
$\triangleright$ Includes positive + negative programs
8: $\quad \mathcal{C} \leftarrow \mathcal{C} \cup$ CONCEPTABSTRACTION $(\mathcal{F}, \mathcal{C})$
9: for _ in range $(M)$ do
10: $\mathcal{C} \leftarrow$ CONCEPTEVOLUTION( $\mathcal{C}$ )
11: $\pi^{}, \ldots \Pi_{K}\right} \leftarrow$ INITIALIZEPOPULATIONS $(\mathcal{C</em>} \leftarrow$ BESTEXPRESSION $(\mathcal{F})$
$\triangleright$ Based on dataset loss and program complexity
12: return $\mathcal{C}, \pi^{*}$</p>
<ul>
<li>LlmCrossover: The LlmCrossover function also samples a set of $l$ concepts from the concept library along with two hypotheses $\pi_{i}$ and $\pi_{j}$ to construct a new expression $\pi_{k}$, which reuses sub-expression trees from the two hypotheses while respecting the sampled concepts. Our implementation is inspired by prior work [40] - see Figure 4.</li>
</ul>
<p>Concept Abstraction. After each iteration of symbolic regression, we use a function EXTRACTPARETOFRONTIER to collect: (i) the hypotheses, across all populations, that are Pareto-optimal with respect to the criteria of syntactic simplicity and dataset loss; (ii) the hypotheses with the worst loss across all populations. The resulting set of hypotheses $\mathcal{F}=\left{\pi_{1}^{<em>} ., \pi_{a}^{</em>} \ldots \pi_{1}^{-} ., \pi_{b}^{-}\right}$captures the trends that were most helpful and most detrimental to performance during hypothesis search. Now we use the CONCEPTAbstraction function, which uses a zero-shot prompt to extract a natural language concept $c^{*}$ that summarizes the positive trends while eschewing negative trends. This concept is subsequently added to the concept library. The prompt for the function is presented in Figure 5.</p>
<p>Concept Evolution. Each concept in $\mathcal{C}$ represents trends that were useful at a previous state in the search process. After adding new concepts into the library, we use a function CONCEPTEVOLUTION to evolve the library to include new ideas that logically follow from the ideas in the current library. The implementation of this function follows that of the LlmCrossover operation in that we are using multiple concepts as a reference to generate new ones, with the key distinction that, unlike in the LlmCrossover operation, the fitness of each generated concept here is difficult to quantify. Thus, we include all the generated responses in the concept library. While these concepts may sometimes be inaccurate, they increase the evolutionary algorithm's exploration ability.</p>
<h1>4 Experiments</h1>
<p>We demonstrate the effectiveness of LASR on multiple tasks integral to scientific discovery. First, we evaluate LASR's performance on the Feynman Equation dataset, a widely adopted scientific discovery benchmark, under a variety of ablations and additional priors. Second, we measure the effect of data leakage by evaluating LASR's performance on a procedurally generated synthetic dataset of challenging equations. Finally, we conduct a case study using LASR to discover LLM scaling laws with data from the BIG-Bench evaluation suite [16].
LASR's main focus is to serve as a practical toolkit for scientists. Therefore, our evaluation primarily targets slightly noisy environments, using exact solution rate to gauge performance rather than statistical similarity measures like correlation $R^{2}$, which are less relevant to scientific discovery applications. Additional experiments characterizing the practical behavior of LASR are in Appendix A.5. Information regarding compute usage is in Appendix A.3.1.</p>
<table>
<thead>
<tr>
<th>GPlearn</th>
<th>AFP</th>
<th>AFP-FE</th>
<th>DSR</th>
<th>uDSR</th>
<th>AIFeynman</th>
<th>PySR</th>
<th>LaSR</th>
</tr>
</thead>
<tbody>
<tr>
<td>20/100</td>
<td>24/100</td>
<td>26/100</td>
<td>23/100</td>
<td>40/100</td>
<td>38/100</td>
<td>59/100</td>
<td>72/100</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on 100 Feynman equations from [49]. We report exact match solve rate for all models. LASR achieves the best exact match solve rate using the same hyperparameters as PySR.</p>
<table>
<thead>
<tr>
<th>Type of Solve</th>
<th>PySR</th>
<th>LASR (Llama3-8B)</th>
<th></th>
<th></th>
<th>LASR (GPT-3.5)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>$p=1$</td>
<td>$p=5$</td>
<td>$p=10$</td>
<td>$p=1$</td>
</tr>
<tr>
<td>Exact Solve</td>
<td>59/100</td>
<td>67/100</td>
<td>69/100</td>
<td>71/100</td>
<td>72/100</td>
</tr>
<tr>
<td>Almost Solve</td>
<td>7/100</td>
<td>5/100</td>
<td>6/100</td>
<td>2/100</td>
<td>3/100</td>
</tr>
<tr>
<td>Close</td>
<td>16/100</td>
<td>9/100</td>
<td>12/100</td>
<td>12/100</td>
<td>10/100</td>
</tr>
<tr>
<td>Not Close</td>
<td>18/100</td>
<td>19/100</td>
<td>13/100</td>
<td>16/100</td>
<td>15/100</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation results on Feynman dataset by cascading LASR’s LLM backbone (llama3-8b, gpt-3.5-turbo) and changing the probability of calling the model ( $p=[0.01,0.05,0.10]$ ) in the order of increasing concept guidance. LASR outperforms PySR even with minimal concept guidance using an open-source LLM.</p>
<h1>4.1 Comparison against baselines in the Feynman Equation Dataset</h1>
<p>Dataset: The Feynman Equations dataset is a well established benchmark for Symbolic Regression algorithms [49]. This dataset consists of 100 physics equations extracted from the Feynman lectures on Physics. We compare against performance reported on SRBench [26]: a continuously updated public benchmark of SR methods on many datasets. Specifically, we compare against GPlearn, AFP, AFP-FE, DSR, uDSR, PySR, and AI Feynman [42, 47, 49, 28, 38]. Within this subset PySR represents an ablation of our model without the LLM genetic operations and the concept evolution (Section 3). We evaluate on a slightly noisy version of this dataset in order to simulate experimental errors common in scientific discovery domains. More details are presented in Appendix A.4.1.</p>
<p>Setup: We instantiate LASR using gpt-3.5-turbo-0125 [4] as the backbone LLM and calling it with $p=0.01$ for 40 iterations, and compare our results with PySR which uses the same default hyperparameters. For the other baselines, we use the numbers reported in SRBench with one exception being uDSR [28], for which we couldn’t find any benchmarking numbers. For this method, we derive the exact solve rate from [37].</p>
<p>Results: We showcase results in Table 1. We draw three observations from this experiment. First, LASR achieves a higher exact solve rate than all other baselines. Second, both PySR and LASR outperform the other baselines by a wide margin, indicating that scalable and efficient synthesis is imperative to practical scientific discovery algorithms. Finally, and most notably, a subset of the equations LASR finds could not be derived with any of the previous methods.</p>
<h3>4.2 Cascading Experiments</h3>
<p>LASR’s performance is inherently bottlenecked by the reasoning capabilities of the backbone LLMs and the frequency of their invocation in each iteration. To evaluate the effect of the backbone LLM on LASR’s performance, we instantiate a model cascade over two of LASR’s hyperparameters: the backbone model (llama3-8b [17], gpt-3.5-turbo-0125) and the probability $p$ with which we call that model in the evolution step $(p=[1 \%, 5 \%, 10 \%])$.</p>
<p>Setup: Our cascade operates as a tournament. We start LASR with the configuration that provides the least language guidance (llama3-8b at $p=1 \%$ ) and progressively increase the value of $p$ and then the backbone model. Each subsequent model is only evaluated on the problems that the previous model could not solve. We compare this against PySR’s performance on the Feynman equation dataset. To ensure a fair comparison, we cascade PySR using the same procedure but find it does not solve any additional equations. For this experiment, we tag each equation with a qualitative rating comparing the equation to the ground truth form (Exact Solve, Almost Solve, Close, and Not Close). An in-depth discussion on this metric is presented in Section A.7.</p>
<p>Results: Our results are presented in 2. We draw two key observations from these results. First, LASR outperforms PySR even with minimal concept guidance (llama3-8b at $p=1 \%$ ). Second,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Evaluation results for ablations/extensions of LASR. (Left): We ablate three components of LASR: Concept Evolution, Concept Library, and variable names and evaluate their MSE solve rate performance on the Feynman dataset over 40 iterations. We find that each component contributes to accelerating search at different stages in the search process. (Right): We extend LASR by providing an initial concept library $\mathcal{C}_{0}$ in the form of user provided hints. We find that natural language hints significantly increases the speed of solving equations.</p>
<p>increasing the backbone model size and the mixture probability substantially enhances LASR's performance, indicating that as the language reasoning capabilities of LLMs improve, so will our performance.</p>
<h3>4.3 Ablation Experiments</h3>
<p>We conduct ablations on the use of Concept Evolution (skip phase three from Figure 1), Concept Library (skip phase two and three), variable names, and user hints. Figure 3 shows how these ablations affect performance over 40 iterations. We designate an equation as "solved" if, after N iterations, the MSE of our predicted equation is less than 10^-11. This metric differs from 'Exact Solved' as defined in the prior experiments: an equation can be 'exactly solved' yet have an MSE higher than 10^-11 due to the noise floor in the target variables, and an equation can have low loss but not be an exact match. We observe from the results that: (1) Removing variable names results in a substantial performance drop, as we lose semantic meaning provided by variables (for instance, observing θ could suggest employing trigonometric functions on θ). (2) Learning a concept library enables faster convergence to solutions. Without the concept library, task convergence is slower, and widens under higher concept guidance conditions (p &gt; 0.1%).</p>
<h3>4.4 Qualitative Analysis and User Hints</h3>
<p>The concept library provides an interpretable window into our evolutionary search process. To showcase the concepts learned by LASR, we take a sample equation from the Feynman dataset, the electric field of a dipole $E_f = \frac{\partial p_a \cos \theta \sin \theta}{4\pi e t^2}$ and comment on the libraries learned at various intervals. We see rudimentary concepts emerge in the second iteration:</p>
<p>"The presence of basic trigonometric functions like sin in the good expressions contributes to their quality, indicating a connection to physical concepts such as waveforms or periodic phenomena."</p>
<p>And, in subsequent iterations, the concepts become even more refined:</p>
<p>"The good mathematical expressions exhibit a balance between mathematical operations such as multiplication, division, and trigonometric functions, which are known to have physical interpretations and relevance in various scientific phenomena."</p>
<p>This iterative refinement helps LASR consistently maintain high-quality concepts, allowing it to converge to an exact match within 40 iterations. By contrast, PySR and the concept library ablations fail to converge on an exact match solution, returning equations that — while low-loss — involve many extra terms absent from the ground truth. This reinforces our hypothesis that injecting semantic meaning into the search process not only improves search efficiency, but also regularizes against</p>
<p>complex equations - as the LLM-generated concepts help filter out irrelevant terms. A deeper qualitative analysis is in Appendix A.8.</p>
<p>Extending LaSR with Hints: A benefit of LaSR is that its search can be initialized with a set of user-specified, natural-language "hints." To evaluate this capability, we generate hints for each equation based on variations of the chapter title of the Feynman lecture that the equation belongs to. We intentionally keep the hints vague to see if knowledge about just the general field is sufficient in improving LaSR's performance. We showcase results in Figure 3. We observe a noticeable boost in performance from injecting these hints, even for our weakest performing model, indicating that even minimal user input can substantially enhance LaSR's effectiveness in discovering equations.</p>
<h1>4.5 Data Leakage Validation</h1>
<p>An important consideration in using LLMs for existing SR problems is the possibility that the LLM was exposed to the hold-out problems in the validation set, presenting an unfair advantage to LLMs trained on massive datasets. Intuitively, LaSR generates its own concepts which are conditioned on suboptimal programs, which are unlikely to be within the LLM's memorized responses. To validate this, we generate a dataset of 41 synthetic equations that are engineered to deviate from common physical and mathematical structures and have arbitrary variables. For example, one such equation is $y=\frac{0.782 x_{3}+0.536}{x_{2} e^{x_{1}}\left(\log x_{2}-x_{3} e^{c_{0} x_{1}}\right)}$. We find that PySR struggles to solve equations with these characteristics (given 400 iterations). Hence, solving such equations hinges on the language guidance components.</p>
<p>We run LaSR with Llama3-8B at $0.1 \%$. We then compare our synthesized program's test set $R^{2}$ with that of PySR's. We justify using correlation instead of exact-match as we are not motivated by the application of results for scientific discovery in this experiment. Our results are summarized in Table 3 and show that LaSR's concept-guided synthesis still provides a considerable performance boost compared to PySR demonstrating that LaSR can outperform PySR even when data leaking is not possible.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">PySR</th>
<th style="text-align: center;">LaSR <br> (Llama3-8B, $0.1 \%$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">$\mathbf{0 . 9 1 3}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation results of data leakage. We present the test set $R^{2}$ of PySR and of LaSR on a synthetic symbolic regression dataset. Higher $R^{2}$ is better.</p>
<h3>4.6 Using LaSR to discover LLM Scaling Laws</h3>
<p>So far, we have demonstrated that LaSR can discover equations that are practical but already known (Feynman Dataset) and equations that are novel but aren't practical (Synthetic Dataset). To investigate LaSR's utility in finding novel and practical empirical trends, we investigate whether LaSR can discover novel LLM scaling laws on the BigBench dataset [16]. More details on this experiment are presented in Section A.6.
Traditionally, to identify an LLM scaling law, practitioners must first manually posit a "skeleton equation" with a fixed set of known variables and unknown free parameters, and then optimize the unknown parameters based on a dataset of model hyperparameters and resulting dataset fitness [23, 1, 5]. Instead of starting with a predefined equation, we use LaSR to discover the skeleton equation that best fits various subsets of the BigBench dataset.
Setup. BigBench contains 204 tasks with scored responses from 55 language models trained with different hyperparameters. We evaluate on the subset of tasks where the preferred metric is 'Multiple choice grade' ( 53,812 samples). Our goal is to find the equation that best predicts the test score given the model hyperparameters and the dataset hyperparameters. We run LaSR with 3840 populations of 200 candidates each for 7 hours (overnight). The runtime of LaSR is comparable to other SR algorithms for this experiment as the slowest operation isn't generating candidate equations but rather optimizing and evaluating candidate equations.
Results. LaSR discovers the following scaling law on the subset of BigBench:</p>
<p>$$
\text { score }=\frac{A}{\left(\frac{\text { train,steps }}{B}\right)^{# \text {shots }}}+E
$$</p>
<table>
<thead>
<tr>
<th>Scaling Law Skeleton</th>
<th>MSE Loss</th>
<th>Free Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Equation 4</td>
<td>$0.03598 \pm 0.00265$</td>
<td>3</td>
</tr>
<tr>
<td>Chinchilla [23]</td>
<td>$0.03610 \pm 0.00268$</td>
<td>5</td>
</tr>
<tr>
<td>Modified Chinchilla</td>
<td>$\mathbf{0 . 0 3 5 9 8} \pm \mathbf{0 . 0 0 2 6 4}$</td>
<td>5</td>
</tr>
<tr>
<td>Residual Term Only</td>
<td>$0.09324 \pm 0.01992$</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Table 4: Preliminary results on evaluating different LLM scaling laws. We measure MSE loss on a held out subset of BigBench [16]. The equation discovered with LASR performs as well as the Chinchilla equation [23] on BigBench while using less free parameters. The residual term skeleton equation $(\mathrm{score}=E)$ also performs well.
where score is the MCQ grade, train_steps is the number of training steps for the model, and #shots is the number of in-context examples provided during inference. The fitted free parameters are presented in Equation 5.
Qualitative Evaluation: Equation 4 describes an empirical relationship between training hyperparameters (training steps) and inference hyperparameters (number of shots). It asserts that increasing the number of shots exponentially increases the model's performance for low-resource models, while having diminishing gains as the number of training steps of the model increase. This observation is consistent with work in scaling test-time compute [46].
As the output artifacts of LASR are interpretable, we can integrate this empirical relationship between training steps and number of shots into known scaling laws. Specifically, we can augment the chinchilla scaling law as follows:</p>
<p>$$
\begin{aligned}
&amp; \text { score }=\frac{A}{(\text { train_steps } \cdot \text { batch_size })^{0}}+\frac{B}{(# \text { params })^{0}}+E \
&amp; \text { score }=\frac{A}{(\text { train_steps } \cdot \text { batch_size })^{0} \cdot \text { #shots }}+\frac{B}{(# \text { params })^{0}}+E
\end{aligned}
$$</p>
<p>Quantitative Evaluation: We fit the free parameters of each equation on the training set (43,049 samples) and measure the MSE loss between the actual grade and the predicted grade on the validation set ( 10,763 samples). The results are presented in Table 4. We find that the Equation 4's performance, as well as modified Chinchilla's performance, is competitive with that of Chinchilla's in predicting the MCQ grade. However, the horizontal line score $=E$ demonstrates acceptable performance as well. We believe increasing the scale of these preliminary experiments (with richer datasets or longer search horizon) will lead to additional empirical findings.</p>
<h1>5 Related Work</h1>
<p>Symbolic Regression. The field of SR started in the 1970s [18, 29] and has recently become a prominent approach to AI-for-science [33, 34, 40]. Two algorithmic themes here are:
Non-parametric Algorithms: Most work on SR focuses on improving search efficiency using heuristics or parallelization. Specifically, PySR [10] builds a multi-population evolutionary algorithm that incorporates various preexisting heuristics [39], and introduces novel ones such as simulated annealing, an evolve-simplify-optimize loop, and an adaptive parsimony metric. PySR has been successfully applied to study problems in domains such as cosmology [12], international economics [50], and climate modeling [20]. LASR extends PySR to enable the discovery of latent concepts.
Parametric Algorithms: Recent work in SR and program synthesis has often used neural networks to accelerate search [43, 40, 38, 28, 34, 13, 35]. The interplay between the neural and the symbolic components in these works can be abstracted into two categories: (1) leveraging LLMs to induce program scaffolds [34, 40, 35], and (2) learning a neural policy to accelerate search [38, 28, 43, 13]. We highlight two methods from the first category: Funsearch [40] and LLM-SR [45]. Funsearch [40] uses a pretrained LLM to implement a mutation operator on a database of executable programs under a fixed specification to find super-optimized programs in extremal combinatorics. LASR is a generalization of FunSearch: while FunSearch conditions program generation on a static "specification" (analogous to our concept library), we discover the concept library in the course of the</p>
<p>algorithm. We do not compare against FunSearch due to resource constraints. As for LLM-SR [45], it leverages a pretrained LLM for generating program sketches [36]. The sketch parameters are optimized and cached in a database, which is in turn used to generate new sketches. Our work is an orthogonal direction of improvement. It is technically possible to "plug" the LLM-SR framework (or other LLM-based search algorithms [35]) into LASR and use our generated concepts to guide the lower-level search component.</p>
<p>The second category includes methods like DSR [38], which, just like LASR, frame SR as a sequence modeling problem. However, the search in LASR leverages a learned concept library and the language and code biases in LLMs, instead of relying on amortization alone.</p>
<p>Program Synthesis with Foundation Models. Recent work in program synthesis models program generation as a sequence prediction problem. Under this paradigm, the DSL and the input-output specification is serialized in the prompt and a code-generation foundation model [31, 7, 4] is leveraged to autoregressively generate candidate programs. This approach has been impactful in many areas including spreadsheet formula prediction [13, 8], competitive programming [32], and visual programming [48, 21, 9]. LASR is similar to work in this area in that the LLM Mutate, LLM Crossover, and LLM Initialization functions all follow the sequence prediction paradigm to synthesize mathematical equations, relying on guidance from the concept library.</p>
<p>Program Synthesis with Library Learning. Deploying classical program synthesizers in a new domain often necessitate hand-engineering DSLs to enable scalable synthesis. This severely limits the generality and practicality of such methods. An emerging direction of research - called library learning - attempts to learn the DSL and the programs simultaneously [15, 3, 19, 27, 53, 14, 44, 54]. This is typically framed as a hierarchical Bayesian optimization problem over the space of programs and the space of library functions that generate those programs. Notably, [19] uses LLM guidance to assist in program induction and in auto-documenting learned library modules and [53] considers learning programs under a latent distribution over the space of natural language and the space of the DSL. LASR shares a similar problem formulation to these works, but optimizes over the space of programs and over the space of natural language descriptions of these programs.</p>
<h1>6 Conclusion</h1>
<p>We have presented LASR, a framework that uses zero-shot queries to an LLM to induce abstract, reusable concepts that can be used to accelerate SR. We have shown that LASR outperforms state-of-the-art approaches on the standard Feynman equation task. We have also used the algorithm to discover a novel scaling law for LLMs.</p>
<p>A key benefit of LASR is that its capabilities are ultimately bottlenecked by those of the underlying LLM. LLMs are rapidly gaining capability and getting cheaper, and future versions of LASR should be able to tap into this progress.</p>
<p>Many directions of research remain open. First, our strategy of accelerating evolutionary search with LLM-based concept induction may be applicable beyond the SR setting. Future research should explore such applications. Second, while our approach here was entirely based on in-context learning, it is worth exploring if finetuning improves the performance of the LLM. Finally, we evaluated the learned concept library exclusively on the downstream SR task. However, the library may also be valuable in other tasks such as clustering or explanation synthesis. Exploring these other tasks is an attractive topic for future work.</p>
<p>Limitations. The current instantiation of LASR has several limitations. First, it cannot guarantee that the concepts it learns are correct or insightful. Even a concept that leads to strong performance in downstream SR tasks may do so because of quirks of the model and data, and end up misleading scientists using the method in a discovery process. Also, we do not currently have a way to ensure that the learned concepts are mutually consistent. Finally, our evaluation here was constrained by our compute budgets for LLMs and search. Whether the trends we see generalize to higher-compute regimes remains to be seen.</p>
<p>Acknowledgements: This research was partially supported by the NSF Expeditions in Computing Award #CCF-1918651, the NSF National AI Institute for Foundations of Machine Learning (IFML), and ARO award #W911NF-21-1-0009. We thank Foundry Technologies for providing substantial computational resources for our experiments.</p>
<h1>References</h1>
<p>[1] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[2] Rohit Batra, Le Song, and Rampi Ramprasad. Emerging materials intelligence ecosystems propelled by machine learning. Nature Reviews Materials, 6(8):655-678, 2021.
[3] Matthew Bowers, Theo X Olausson, Lionel Wong, Gabriel Grand, Joshua B Tenenbaum, Kevin Ellis, and Armando Solar-Lezama. Top-down synthesis for library learning. Proceedings of the ACM on Programming Languages, 7(POPL):1182-1213, 2023.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[5] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. In The Eleventh International Conference on Learning Representations, 2023.
[6] Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. Neurosymbolic programming. Foundations and Trends® in Programming Languages, 7(3):158-243, 2021.
[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[8] Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou. Spreadsheetcoder: Formula prediction from semi-structured context. In International Conference on Machine Learning, pages 1661-1672. PMLR, 2021.
[9] Mia Chiquier, Utkarsh Mall, and Carl Vondrick. Evolving interpretable visual classifiers with large language models, 2024.
[10] Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582, 2023.
[11] Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho. Discovering symbolic models from deep learning with inductive biases. In Neural Information Processing Systems, 2020.
[12] Benjamin L Davis and Zehao Jin. Discovery of a planar black hole mass scaling relation for spiral galaxies. The Astrophysical Journal Letters, 956(1):L22, 2023.
[13] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. In ICML, 2017.
[14] Kevin Ellis, Lucas Morales, Mathias Sablé-Meyer, Armando Solar-Lezama, and Josh Tenenbaum. Learning libraries of subroutines for neurally-guided Bayesian program induction. In Advances in Neural Information Processing Systems, pages 7805-7815, 2018.
[15] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning. arXiv preprint arXiv:2006.08381, 2020.
[16] Aarohi Srivastava et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023.
[17] Abhimanyu Dubey et al. The llama 3 herd of models, 2024.
[18] Donald Gerwin. Information processing, data inferences, and scientific generalization. Behavioral Science, 19(5):314-325, 1974.</p>
<p>[19] Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X Olausson, Muxin Liu, Joshua B Tenenbaum, and Jacob Andreas. Lilo: Learning interpretable libraries by compressing and documenting code. arXiv preprint arXiv:2310.19791, 2023.
[20] Arthur Grundner, Tom Beucler, Pierre Gentine, and Veronika Eyring. Data-driven equation discovery of a cloud cover parameterization. Journal of Advances in Modeling Earth Systems, 16(3):e2023MS003763, 2024.
[21] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953-14962, 2023.
[22] Alberto Hernandez, Adarsh Balasubramanian, Fenglin Yuan, Simon AM Mason, and Tim Mueller. Fast, accurate, and transferable many-body interatomic potentials by symbolic regression. npj Computational Materials, 5(1):112, 2019.
[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
[24] John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. Introduction to automata theory, languages, and computation, 3rd Edition. Pearson international edition. Addison-Wesley, 2007.
[25] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.
[26] William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabrício Olivetti de França, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H Moore. Contemporary symbolic regression methods and their relative performance. arXiv preprint arXiv:2107.14351, 2021.
[27] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
[28] Mikel Landajuela, Chak Shing Lee, Jiachen Yang, Ruben Glatt, Claudio P Santiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, and Brenden K Petersen. A unified framework for deep symbolic regression. Advances in Neural Information Processing Systems, 35:3398533998, 2022.
[29] Pat Langley. Bacon: A production system that discovers empirical laws. In International Joint Conference on Artificial Intelligence, 1977.
[30] Pablo Lemos, Niall Jeffrey, Miles Cranmer, Shirley Ho, and Peter Battaglia. Rediscovering orbital mechanics with machine learning. Machine Learning: Science and Technology, 4(4):045002, 2023.
[31] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.
[32] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.
[33] Nour Makke and Sanjay Chawla. Interpretable scientific discovery with symbolic regression: a review. Artificial Intelligence Review, 57(1):2, 2024.
[34] Matteo Merler, Nicola Dainese, and Katsiaryna Haitsiukevich. In-context symbolic regression: Leveraging language models for function discovery. arXiv preprint arXiv:2404.19094, 2024.
[35] Elliot Meyerson, Mark J. Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K. Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting, 2024.</p>
<p>[36] Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine. Neural sketch learning for conditional program generation. ICLR, 2018.
[37] Deep Symbolic Optimization Organization. Srbench symbolic solution. https: //github.com/dso-org/deep-symbolic-optimization/blob/master/images/ srbench_symbolic-solution.png. Accessed: 2024-05-22.
[38] Brenden K Petersen, Mikel Landajuela, T Nathan Mundhenk, Claudio P Santiago, Soo K Kim, and Joanne T Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871, 2019.
[39] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 4780-4789, 2019.
[40] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468-475, 2024.
[41] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324(5923):81-85, 2009.
[42] Michael D. Schmidt and Hod Lipson. Age-fitness pareto optimization. In Annual Conference on Genetic and Evolutionary Computation, 2010.
[43] Ameesh Shah, Eric Zhan, Jennifer J Sun, Abhinav Verma, Yisong Yue, and Swarat Chaudhuri. Learning differentiable programs with admissible neural heuristics. In Advances in Neural Information Processing Systems, 2020.
[44] Richard Shin, Miltiadis Allamanis, Marc Brockschmidt, and Oleksandr Polozov. Program synthesis and semantic parsing with learned code idioms. In Advances in Neural Information Processing Systems, pages 10825-10835, 2019.
[45] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400, 2024.
[46] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024.
[47] Trevor Stephens. gplearn: Genetic programming in python, with a scikit-learn inspired api, 2024. Accessed: 2024-05-22.
[48] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.
[49] Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic regression. Science Advances, 6(16):eaay2631, 2020.
[50] Sergiy Verstyuk and Michael R Douglas. Machine learning the gravity equation for international trade. Available at SSRN 4053795, 2022.
[51] Marco Virgolin, Ziyuan Wang, Tanja Alderliesten, and Peter AN Bosman. Machine learning for the prediction of pseudorealistic pediatric abdominal phantoms for radiation dose reconstruction. Journal of Medical Imaging, 7(4):046501-046501, 2020.
[52] Eugene P Wigner. The unreasonable effectiveness of mathematics in the natural sciences. In Mathematics and science, pages 291-306. World Scientific, 1990.
[53] Catherine Wong, Kevin M Ellis, Joshua Tenenbaum, and Jacob Andreas. Leveraging language to learn program abstractions and search heuristics. In International conference on machine learning, pages 11193-11204. PMLR, 2021.
[54] Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, and Nick Haber. Parsel: Algorithmic reasoning with language models by composing decompositions. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</p>
<h1>A Appendix</h1>
<h2>A. 1 Broader Societal Impacts</h2>
<p>We have presented LASR: a symbolic regression framework that leverages concept guidance to accelerate symbolic regression. We hope that LASR helps accelerate the search for empirical laws in the broader scientific community. In this section, we discuss the broader societal impacts and ethical considerations of our work.</p>
<p>Potential for Misuse: As with other ML techniques, symbolic regression can be leveraged by bad actors to inflict societal harm. Our experiments show that LASR accelerates the search for empirical laws from raw observations. In our setting, we are restricted to observations about physical phenomena. However, a malicious actor could misuse LASR to find patterns in datasets that violate personal rights.
Privacy Concerns: As mentioned before, LASR enables finding patterns in raw observations. We hope that LASR is leveraged by scientists to explain physical phenomena. However, it is possible to use such models to learn behavioral profiles without the active knowledge or explicit consent of the subjects.
Bias and Fairness: LASR generates two artifacts: a hypothesis that maximizes a fitness function (represented as an equation) and a library of concepts that helped discover that hypothesis. LASR ensures fairness and lack of bias in the generated equation as long as the fitness function is free of biases as well. However, we leverage foundation models to induce our library of concepts which could be trained on biased data which may reflect in our concept library. Furthermore, we cannot directly evaluate the efficacy of the concept library and its factual correctness. This doesn't affect equation generation - since equations are quantitatively evaluated. However, a human analyzing the concepts LASR learns might misinterpret trends that the model picks up on.</p>
<h2>A. 2 LLM Prompts</h2>
<p>Note that in the prompts in Figure 4, Figure 5, and Figure 6, we refer to our hypothesis as expressions and the concepts as hypotheses and suggestions. This prompting style was found to work best for the LLM.</p>
<h2>A. 3 Implementation Details</h2>
<h2>A.3.1 Compute Usage</h2>
<p>We run all experiments on a server node with 8xA100 GPUs with 80 GB of VRAM each. However, our experiments can be reproduced with a GPU with 16 GB of VRAM. We were even able to run LASR on a laptop utilizing a quantized model hosted locally ${ }^{3}$. Moreover, certain models are hosted on external servers (such as gpt-3-turbo-0125) which allows running LASR on machines without GPUs. For this project, we chose to run llama3-8b using vLLM [25]. However, our framework is compatible with any LLM inference framework that allows hosting an OpenAI compliant RESTful server. For reference, each iteration makes around 60,000 calls. Each call to the LLM is just under 1000 tokens. This gives an upper bound on total compute of $60,000,000$ tokens per iteration if $p=100 \%$. Hence, running our model at $p=1 \%$ for 40 iterations would result in just under 25 M tokens for each equation (around 4 hours on a single A100).</p>
<h2>A.3.2 Concept Sampling</h2>
<p>In order to determine which concepts from the concept library we sample for the LLM Hypothesis Evolution, we randomly choose the top-K most recent concepts in the library. This ensures that we use the latest concepts, which are generally reflective of more informed hypotheses, and thus better to use. In practice, we set $K=20$. Additionally, for Concept Evolution, we exclude the top-K most recent concepts from being used, and rather use older concepts. This is motivated by the desire to not have the concept library converge on a few ideas, rather we want diversity of thought. Our concepts are intended to be longer lasting than the hypotheses that generated them, similar to how observational data comes and goes, but the conclusions from them are more persistent.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: LlmCrossover prompt with an example output. LlmMutation and Llminit follow the same structure but with slightly different wording and with one and no reference expressions, respectively. Variables within double braces are replaced with the instance specific arguments. These prompts are available in prompts/*.txt in the linked repository.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: LLM Concept Abstraction prompt with an example output. The LLM Concept Crossover function follows a similar structure, with a modified task description for crossover on concepts.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: LLM Concept Evolution prompt with an example output. The LLM Concept Evolution prompt follows a similar structure to the concept abstraction and LLM operation prompts with slight modifications.</p>
<div class="codehilite"><pre><span></span><code><span class="n">niterations</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
<span class="n">ncyclesperiteration</span><span class="o">=</span><span class="mi">550</span><span class="p">,</span>
<span class="n">populations</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
<span class="n">population_size</span><span class="o">=</span><span class="mi">33</span><span class="p">,</span>
<span class="n">maxsize</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="n">binary_operators</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;+&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;+&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;-&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;/&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot; &quot;</span><span class="w"> </span><span class="s2">&quot; ],</span>
<span class="n">unary_operators</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;exp&quot;</span><span class="p">,</span><span class="s2">&quot;log&quot;</span><span class="p">,</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span><span class="s2">&quot;sin&quot;</span><span class="p">,</span><span class="s2">&quot;cos&quot;</span><span class="p">],</span>
<span class="n">weight_randomize</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="n">nested_constraints</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;cos&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;cos&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;cos&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;exp&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;log&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;log&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;log&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;sqrt&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;sqrt&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">}},</span>
<span class="n">constraints</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;cos&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;exp&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;log&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;sqrt&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pow&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="p">)},</span>
</code></pre></div>

<p>Figure 7: The PySR hyperparameters used in all experiments. Whenever possible, we use the default PySR parameters.</p>
<h1>A.3.3 Hyperparameters</h1>
<p>Figure 7 showcases the hyperparameters used for all our experiments. Wherever possible, we use the default PySR parameters. Additionally, LaSR introduces three new hyperparameters: (1) \% of LLM calls, (2) List of user hints, and (3) a dictionary of parameters pertaining to backend LLM communication. Following other methods in SRBench, we utilize only a subset of the necessary operators for solving the Feynman equations, excluding special operators like arcsin and arctan. These operators are seldom required, and removing them speeds up the search process. We generally set the number of iterations to 40 . However, certain experiments may demand more or less iterations.</p>
<h2>A. 4 Dataset Details</h2>
<h2>A.4.1 Feynman Equations</h2>
<p>Dataset: The Feynman Equation dataset is a widely adopted benchmark for scientific discovery [49]. The dataset consists of 100 physics equations extracted from the Feynman lectures on Physics. Each equation is in the form $y=f\left(x_{1}, x_{2}, \ldots\right)$. The number of input variables ranges from two to ten, and the dataset provides 100,000 samples for each equation. We compare against publically available methods benchmarked on SRBench [26]. SRBench is a continuously updated benchmark which catalogs the performance of various methods on the Feynman dataset as well as other symbolic regression problems. Specifically, we compare against GPlearn, AFP, AFP-FE, DSR, uDSR, PySR, and the original AI Feynman algorithm [42, 47, 49, 28, 38]. Within this subset, notably, PySR represents an ablation of our model without the LLM genetic operations and the concept evolution (Section 3). We evaluate on a slightly noisy version of this dataset in order to simulate experimental errors common in scientific discovery domains. Specifically, we compare numbers against those reported in and reproduced by SRBench with a target noise of 0.001 .</p>
<p>Methodology: For the Feynman dataset, we took the equations and the bounds at which each variable was sampled at and generated our dataset. Then, we added additional noise of 0.001 to our target variable, following the noise formula detailed in the Appendix A. 4 of [26], as well as additional random noise variables with arbitrary names to force the model for proper feature selection. We then evaluate exact matches by looking at if the predicted equation symbolically simplifies into the ground truth equation. For the ablation graphs, we used the PySR hyperparameter "early_stop_condition" to check if there is a "solution" after $N$ iterations.</p>
<h2>A.4.2 Synthetic Dataset</h2>
<p>For the synthetic dataset, we ran a script that generates uncommon mathematical hypotheses that satisfy our constraints at random. Then, we ran PySR for 400 iterations and found all the equations that PySR performed poorly in, i.e. MSE loss greater than 1, while having a complexity less than 20.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">PySR ( $10^{6}$ Iterations, 10 hour timeout)</th>
<th style="text-align: center;">LaSR (40 iterations)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Exact Solve</td>
<td style="text-align: center;">$59+3 / 100$</td>
<td style="text-align: center;">$\mathbf{7 2 / 1 0 0}$</td>
</tr>
</tbody>
</table>
<p>Table 5: An asymmetric comparison of PySR and LaSR on the Feynman equations dataset (A.5.1). We run PySR for 10 hours per equation (thresholded to $10^{6}$ iterations) and compare the exact solve rate with that of LaSR run for 40 iterations. We find that PySR is able to discover three more equations, but LaSR still substantially outperforms PySR.</p>
<p>For these 41 remaining equations, we then compared LaSR and PySR after 20 iterations using the average of their test set $R^{2}$ for each hypothesis.</p>
<h1>A. 5 Additional Experiments</h1>
<p>This section highlights additional experiments on various benchmarks to further characterize LaSR's performance.</p>
<h2>A.5.1 Asymmetric comparison with PySR</h2>
<p>A common concern with evaluating genetic optimization algorithms w.r.t number of iterations is that the 'hard stop' after a certain number of iterations might yield populations that haven't fully converged to their optimal values.
To account for this discrepancy in performance, we conduct an asymmetric comparison with PySR on the Feynman Equations dataset. Specifically, we allow PySR to run uninterrupted for 10 hours per equation, with the maximum number of iterations set to $1 \times 10^{6}$. We compare the results with that of LaSR run interrupted for 40 iterations per equation (hence the asymmetric comparison). The results are detailed in Table 5.
Overall, we find that, despite running PySR substantially longer than LaSR, LaSR still substantially outperforms PySR. This is because PySR, like other evolutionary algorithms, is susceptible to falling in local minima and converges to this local minima extremely fast. This indicates that supplementing 'local search' strategies with LLM guidance is useful for symbolic regression.</p>
<h2>A.5.2 Subset of equations discovered by LaSR on the Feynman Dataset</h2>
<p>Equation Number \&amp; Reference Ground Truth Equation Discovered Equation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Equation 2 (1.6.20)</th>
<th style="text-align: center;">$p(x)=\frac{b}{x+2 a} e^{-x^{2} / 2 x^{2}}$.</th>
<th style="text-align: center;">$f=\left(\frac{0.80214177}{x}\right)\left(0.6059228\left(\sqrt{x}\right)^{1.944455}\right) \times 0.86212635$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Equation 7 (1.11.19)</td>
<td style="text-align: center;">$\mathbf{a} \cdot \mathbf{b}=a_{x} b_{x}+a_{y} b_{y}+a_{z} b_{z}$.</td>
<td style="text-align: center;">$A=\left(x_{125}\right)+\left(\left(x_{225}\right)+\left(x_{325}\right)\right)$</td>
</tr>
<tr>
<td style="text-align: center;">Equation 51 (1.50.26)</td>
<td style="text-align: center;">$x_{\text {out }}(t)=K\left(\cos \omega t+\epsilon \cos ^{2} \omega t\right)$.</td>
<td style="text-align: center;">$x=\left(\left(\left(\cos (\omega t)\left[x_{1}(n-2.991099 \times 10^{-6}\right)\right)-1.37413245 \times 10^{-6}\right)+x_{1}\right) \cos (\omega t)+1.575935\right)$</td>
</tr>
<tr>
<td style="text-align: center;">Equation 21 (1.18.4)</td>
<td style="text-align: center;">$\mathbf{R}=\frac{m_{1} a_{1}+m_{2} a_{2}}{m_{1}+m_{2}}$</td>
<td style="text-align: center;">$r=\frac{\left(m_{1} r_{1}\right)+\left(m_{2} r_{1}\right)}{m_{1}+m_{2}}$</td>
</tr>
<tr>
<td style="text-align: center;">Equation 63 (11.11.20)</td>
<td style="text-align: center;">$P=\frac{N_{1} A_{1}}{N T}$.</td>
<td style="text-align: center;">$\operatorname{Pol}=\left(\frac{N_{1} a_{1}}{N T}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">Equation 96 (111.15.14)</td>
<td style="text-align: center;">$m_{\text {off }}=\frac{k T}{2.997}$.</td>
<td style="text-align: center;">$m=k\left(\frac{k}{E_{1}-0.99924841} \frac{1}{0.9892225+10^{-6}}-0.00011104094\right) \frac{1}{P}-0.011250258$</td>
</tr>
<tr>
<td style="text-align: center;">Equation 57 (11.6.15h)</td>
<td style="text-align: center;">$E_{\perp}=\frac{k T}{2.95} \cdot \frac{2 \cos A \sin T}{2.997 \cdot 10^{-6}}-0.39283985) \frac{1}{P} \cdot\left(P^{0.00012676959}+0.23802117\right)$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 6: A subset of equations discovered by LaSR on the Feynman Equations dataset (over PySR). The equations are presented in the form discovered by LaSR, and usually reduce to the ground truth equations after some simplification steps. Note that there are minor discrepancies in the variable names between the ground truth equations found in the online Feynman Lectures (https://www.feynmanlectures.caltech.edu) and those in our Feynman Equations dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Ground Truth Equation</th>
<th style="text-align: center;">Discovered Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\exp \left(\frac{C-\log \left(y_{2}\right)}{C-y_{4}}+\left(\sqrt{y_{1}+y_{4}}+\sqrt{y_{2}}\right)\right)$</td>
<td style="text-align: center;">$\left(\frac{\left(y_{4}+C+y_{1}\right)}{C} \cdot\left(y_{2}+C\right)\right)^{C}+y_{1}$</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\left(y_{1}+C\right)+\left(\sqrt{y_{2}} \cdot\left(y_{1} \cdot\left(C-y_{3}\right) \cdot y_{3}\right)\right)}{\cos \left(\cos \left(y_{1}\right)\right)}$</td>
<td style="text-align: center;">$y_{3} \cdot \frac{C-\left(y_{1} \cdot\left(y_{2}-\left(\cos \left(\left(y_{1}+C\right) \cdot C\right) \cdot C\right)+C\right)\right)}{C}-C$</td>
</tr>
<tr>
<td style="text-align: center;">$\sqrt{\exp \left(y_{1}\right) \cdot y_{1}} \cdot\left(\cos \left(y_{1}\right)+2 y_{1}\right)$</td>
<td style="text-align: center;">$\left(y_{1}^{C}+C\right) \cdot\left(\sqrt{\exp \left(y_{1}\right)}-C\right)+$ <br> $\left(C-\left(\frac{\left(y_{1}^{2} \cdot \exp \left(\cos \left(y_{1}\right)\right)\right)}{C}+y_{4}-C\right)\right)$</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\exp \left(\log \left(y_{4}+y_{2} \cdot y_{1}\right)\right)}{\left(\cos \left(y_{2}\right)-\sqrt{y_{1}}+\frac{y_{2}}{C}-C\right)}$</td>
<td style="text-align: center;">$y_{1} \cdot\left(\left(y_{2} \cdot \cos \left(y_{2}\right)\right)-y_{4}-\left(y_{3}+1\right) \cdot y_{2}\right)-1$</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\left(y_{3}+y_{2}\right) \cdot\left(\left(\left(y_{3}+C\right) \cdot y_{2}+y_{1}+y_{3}\right)-C\right)}{\left(\sqrt{y_{2}}+\exp \left(C+\sqrt{y_{2}}\right)\right) \cdot\left(\left(y_{4} \cdot y_{3}\right) \cdot \log \left(y_{3}\right)\right)}$</td>
<td style="text-align: center;">$\frac{\left(\left(y_{2}+y_{3}\right) \cdot\left(y_{3}+\left(y_{2} \cdot\left(y_{3}+1\right)\right)+1+y_{1}\right)-C+y_{3}\right)}{\left(\frac{\left(\sqrt{y_{2}^{\prime \prime}}-C\right) \cdot\left(y_{4} \cdot C \cdot\left(y_{3}-C\right)\right)-\sin \left(y_{3}^{\prime \prime}+C\right)\right) \cdot \log \left(y_{3}\right)}+C}$</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\left(C \cdot y_{3}\right) \cdot \exp \left(\sqrt{y_{2}^{\prime}}-y_{1}\right)}{C /\left(y_{2}+y_{1}\right)}$</td>
<td style="text-align: center;">$\left(y_{2} \cdot \frac{C-\sqrt{y_{2}^{\prime \prime}}}{\left(\sqrt{y_{1}^{\prime \prime}}+C\right)^{y_{2}}}\right) \cdot y_{3}-C$</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\left(y_{2}+C\right) \cdot \sqrt{y_{1}}}{C} \cdot \sqrt{\frac{\exp \left(y_{1}\right)}{y_{1}}}$</td>
<td style="text-align: center;">$\frac{C-y_{2}}{\sin \left(\frac{C}{y_{1}+C}\right)^{C}}-\cos \left(C-y_{1}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">$y_{1} \cdot\left(\frac{y_{1}^{2}}{\cos \left(C^{2} y_{4}-C\right)}\right) / \exp \left(C \cdot y_{5}\right)$</td>
<td style="text-align: center;">$\left(\left(y_{1}-C\right) \cdot\left(\sin \left(y_{4}+C\right) \cdot y_{1}\right) \cdot\left(y_{5}+y_{1}-C\right)\right) \cdot$</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\exp \left(\log \left(y_{3}\right) \cdot \sqrt{y_{2}}\right)-}{\exp \left(\sqrt{\left(y_{3}+y_{4}\right)+\left(y_{1} \cdot y_{2}\right)} \cdot C\right)}$</td>
<td style="text-align: center;">$\left(\frac{y_{3} \cdot\left(y_{2} / C\right)^{C}+C-y_{4}-y_{2}}{-y_{2}}\right)$</td>
</tr>
</tbody>
</table>
<p>Table 7: Qualitative evaluation of LASR on the synthetic equations dataset (A.5.3). We attempt to recover the ground truth equation from a slightly noisy dataset generated from the ground truth equations. None of the algorithms we tested (including LASR) are able to recover the ground truth equations, underscoring the challenge of exact symbolic match on this dataset. A study on the $R^{2}$ performance is presented in Table 3.</p>
<h1>A.5.3 Qualitative comparison of Synthetic Dataset equations</h1>
<p>In Table 3, we reported $R^{2}$ test set performance of LASR and PySR on the synthetic dataset. In this section, we qualitatively compare the equations discovered by LASR and PySR on the procedurally generated dataset of equations. None of the algorithms recover the ground truth form, but we find that LASR's equations fit much better to the ground truth data than PySR's equations. These equations are presented in Table 3.</p>
<h2>A.5.4 Stochasticity of LASR and PySR</h2>
<p>A common concern with using LLM's for scientific discovery is that the equations could be obtained due to dataset memorization rather than reasoning and learning on the equations form. To explore this further, we run LASR with the same hyperparameters twice with different seeds. We expect LASR to behave like PySR (or any stochastic evolutionary algorithm) and produce two syntactically different programs that achieve a similar data fitness score. However, if the algorithm purely relies on training set memorization, we expect the model to find the same equation form in both experiments. We present results in Table 8.</p>
<p>Overall, both equations fit well to the underlying dataset and reduce to the ground truth. Yet, despite using the same hyperparameters, the functional forms exhibit sharp differences. This further reinforces our hypothesis that LASR's performance is not simply the result of regurgitated memorized responses.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ TheBloke/Mistral-7B-Instruct-v0.2-GGUF using llama.cpp&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
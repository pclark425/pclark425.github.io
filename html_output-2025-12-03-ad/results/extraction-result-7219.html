<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7219 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7219</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7219</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-257636780</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.11436v2.pdf" target="_blank">Mind meets machine: Unravelling GPT-4's cognitive psychology</a></p>
                <p><strong>Paper Abstract:</strong> Cognitive psychology delves on understanding perception, attention, memory, language, problem-solving, decision-making, and reasoning. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required. In this study, we focus on the evaluation of GPT-4's performance on a set of cognitive psychology datasets such as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how GPT-4 processes and integrates cognitive psychology with contextual information, providing insight into the underlying cognitive processes that enable its ability to generate the responses. We show that GPT-4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior state-of-the-art models. Our results strengthen the already available assessments and confidence on GPT-4's cognitive psychology abilities. It has significant potential to revolutionize the field of AI, by enabling machines to bridge the gap between human and machine reasoning.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7219.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7219.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Commonsense Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multiple-choice benchmark (12,247 questions, 5 choices each) designed to test commonsense reasoning using ConceptNet-derived distractors; human accuracy reported by the dataset authors is high.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art generative pre-trained transformer language model developed by OpenAI (transformer-based LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>commonsense reasoning / general knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice questions targeting commonsense knowledge and reasoning; each question has five answer choices and is designed to be challenging via ConceptNet-based distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 89% (reported by CommonsenseQA authors)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 83.2% (reported in this paper; also described as ~84%)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>not reported (model accessed via ChatGPT-Plus per Methods)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>CommonsenseQA: "Commonsenseqa: A question answering challenge targeting commonsense knowledge" (Talmor et al., 2018) — human accuracy ~89% (as reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports prior language-model baseline accuracy 55.9% (original CommonsenseQA paper) and contrasts GPT-4's much higher accuracy (~83.2%). Prompting details (zero/few-shot, chain-of-thought, or temperature) are not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7219.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7219.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATH (subtasks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATH dataset (scholastic contest problems; subcategories: Prealgebra, Geometry, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of ~12,500 math contest problems across topics and difficulty levels, evaluated with exact-match metrics; includes detailed step-by-step solutions for training/interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art generative pre-trained transformer language model developed by OpenAI (transformer-based LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MATH (Prealgebra and Geometry reported separately)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>mathematical problem solving / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Exact-match evaluation of model answers to multi-step mathematical contest problems covering topics (prealgebra, algebra, geometry, etc.) and graded by difficulty (1–5); problems can include diagrams represented in Asymptote.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (exact match)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>reported human performance varies by expertise: ~40% (example: a CS PhD with low math interest) to 90% (three-time IMO gold medallist) as cited in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Prealgebra: 82% accuracy (reported); Geometry: 35% accuracy (reported). Overall GPT-4 aggregated MATH performance not precisely given.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>not reported (model accessed via ChatGPT-Plus per Methods)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>MATH dataset paper: "Measuring mathematical problem solving with the math dataset" (Hendrycks et al., 2021) — human examples reported in that work and summarized here (40% to 90% depending on solver skill).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper notes prior LLMs (GPT-2/GPT-3 and other models) had very low MATH accuracies (<10%), and reports GPT-4 much higher on some subcategories (prealgebra) but poor on others (geometry). No statistical tests are reported; prompting and chain-of-thought training details not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7219.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7219.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SuperGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SuperGLUE benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite, harder successor to GLUE consisting of multiple NLU tasks intended to better measure human-level language understanding under low-data and varied task formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art generative pre-trained transformer language model developed by OpenAI (transformer-based LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SuperGLUE (aggregate benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>language understanding / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A suite of difficult NLU tasks (multiple datasets/tasks aggregated) designed to measure general language understanding and reasoning; evaluation typically reported as aggregate score/accuracy relative to human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>aggregate accuracy/score (benchmark score reported as percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>aggregate score 91.2% (reported for GPT-4 in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>not reported (model accessed via ChatGPT-Plus per Methods)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper references the original SuperGLUE finding that BERT scores roughly 20 points worse than humans on the benchmark but does not report a numeric human baseline here; GPT-4's 91.2% is reported without detailed prompting or statistical testing. Human baseline value is therefore not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7219.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7219.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HANS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic Analysis for NLI Systems (HANS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic dataset for Natural Language Inference (NLI) designed to detect shallow syntactic heuristics (lexical overlap, subsequence, component) by producing examples where common heuristics fail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art generative pre-trained transformer language model developed by OpenAI (transformer-based LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>HANS</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>natural language inference / reasoning (diagnostic)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Constructed premise–hypothesis pairs that expose reliance on shallow heuristics; evaluates whether models correctly label entailment vs non-entailment when heuristics would mislead.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (entailment / non-entailment classification)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy range 76%–97% (reported in this paper as human performance range on HANS examples)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 100% (reported for GPT-4 in this paper; authors caution this may be due to dataset subset selection where examples were all non-entailment and model may exploit that heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>not reported (model accessed via ChatGPT-Plus per Methods)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>HANS paper: "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference" (McCoy et al., 2019) — human performance range reported in this paper's summary</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors warn GPT-4's perfect accuracy may reflect dataset selection bias (used examples may have been all non-entailment) and potential memorization of the heuristic; prior models (e.g., BERT) performed very poorly on non-entailment subsets (<10% in one cited category). No formal statistical comparison reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Commonsenseqa: A question answering challenge targeting commonsense knowledge <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Superglue: A stickier benchmark for general-purpose language understanding systems <em>(Rating: 2)</em></li>
                <li>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Hellaswag: Can a machine really finish your sentence? <em>(Rating: 1)</em></li>
                <li>Winogrande: An adversarial winograd schema challenge at scale <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7219",
    "paper_id": "paper-257636780",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "CommonsenseQA",
            "name_full": "Commonsense Question Answering",
            "brief_description": "A multiple-choice benchmark (12,247 questions, 5 choices each) designed to test commonsense reasoning using ConceptNet-derived distractors; human accuracy reported by the dataset authors is high.",
            "citation_title": "MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "A state-of-the-art generative pre-trained transformer language model developed by OpenAI (transformer-based LLM).",
            "model_size": null,
            "test_name": "CommonsenseQA",
            "test_category": "commonsense reasoning / general knowledge",
            "test_description": "Multiple-choice questions targeting commonsense knowledge and reasoning; each question has five answer choices and is designed to be challenging via ConceptNet-based distractors.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 89% (reported by CommonsenseQA authors)",
            "llm_performance": "accuracy 83.2% (reported in this paper; also described as ~84%)",
            "prompting_method": "not reported (model accessed via ChatGPT-Plus per Methods)",
            "fine_tuned": false,
            "human_data_source": "CommonsenseQA: \"Commonsenseqa: A question answering challenge targeting commonsense knowledge\" (Talmor et al., 2018) — human accuracy ~89% (as reported in this paper)",
            "statistical_significance": null,
            "notes": "Paper reports prior language-model baseline accuracy 55.9% (original CommonsenseQA paper) and contrasts GPT-4's much higher accuracy (~83.2%). Prompting details (zero/few-shot, chain-of-thought, or temperature) are not reported.",
            "uuid": "e7219.0"
        },
        {
            "name_short": "MATH (subtasks)",
            "name_full": "MATH dataset (scholastic contest problems; subcategories: Prealgebra, Geometry, etc.)",
            "brief_description": "A dataset of ~12,500 math contest problems across topics and difficulty levels, evaluated with exact-match metrics; includes detailed step-by-step solutions for training/interpretability.",
            "citation_title": "MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "A state-of-the-art generative pre-trained transformer language model developed by OpenAI (transformer-based LLM).",
            "model_size": null,
            "test_name": "MATH (Prealgebra and Geometry reported separately)",
            "test_category": "mathematical problem solving / reasoning",
            "test_description": "Exact-match evaluation of model answers to multi-step mathematical contest problems covering topics (prealgebra, algebra, geometry, etc.) and graded by difficulty (1–5); problems can include diagrams represented in Asymptote.",
            "evaluation_metric": "accuracy (exact match)",
            "human_performance": "reported human performance varies by expertise: ~40% (example: a CS PhD with low math interest) to 90% (three-time IMO gold medallist) as cited in the paper",
            "llm_performance": "Prealgebra: 82% accuracy (reported); Geometry: 35% accuracy (reported). Overall GPT-4 aggregated MATH performance not precisely given.",
            "prompting_method": "not reported (model accessed via ChatGPT-Plus per Methods)",
            "fine_tuned": false,
            "human_data_source": "MATH dataset paper: \"Measuring mathematical problem solving with the math dataset\" (Hendrycks et al., 2021) — human examples reported in that work and summarized here (40% to 90% depending on solver skill).",
            "statistical_significance": null,
            "notes": "Paper notes prior LLMs (GPT-2/GPT-3 and other models) had very low MATH accuracies (&lt;10%), and reports GPT-4 much higher on some subcategories (prealgebra) but poor on others (geometry). No statistical tests are reported; prompting and chain-of-thought training details not provided.",
            "uuid": "e7219.1"
        },
        {
            "name_short": "SuperGLUE",
            "name_full": "SuperGLUE benchmark",
            "brief_description": "A composite, harder successor to GLUE consisting of multiple NLU tasks intended to better measure human-level language understanding under low-data and varied task formats.",
            "citation_title": "MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "A state-of-the-art generative pre-trained transformer language model developed by OpenAI (transformer-based LLM).",
            "model_size": null,
            "test_name": "SuperGLUE (aggregate benchmark)",
            "test_category": "language understanding / reasoning",
            "test_description": "A suite of difficult NLU tasks (multiple datasets/tasks aggregated) designed to measure general language understanding and reasoning; evaluation typically reported as aggregate score/accuracy relative to human baselines.",
            "evaluation_metric": "aggregate accuracy/score (benchmark score reported as percentage)",
            "human_performance": null,
            "llm_performance": "aggregate score 91.2% (reported for GPT-4 in this paper)",
            "prompting_method": "not reported (model accessed via ChatGPT-Plus per Methods)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The paper references the original SuperGLUE finding that BERT scores roughly 20 points worse than humans on the benchmark but does not report a numeric human baseline here; GPT-4's 91.2% is reported without detailed prompting or statistical testing. Human baseline value is therefore not provided in this paper.",
            "uuid": "e7219.2"
        },
        {
            "name_short": "HANS",
            "name_full": "Heuristic Analysis for NLI Systems (HANS)",
            "brief_description": "A diagnostic dataset for Natural Language Inference (NLI) designed to detect shallow syntactic heuristics (lexical overlap, subsequence, component) by producing examples where common heuristics fail.",
            "citation_title": "MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "A state-of-the-art generative pre-trained transformer language model developed by OpenAI (transformer-based LLM).",
            "model_size": null,
            "test_name": "HANS",
            "test_category": "natural language inference / reasoning (diagnostic)",
            "test_description": "Constructed premise–hypothesis pairs that expose reliance on shallow heuristics; evaluates whether models correctly label entailment vs non-entailment when heuristics would mislead.",
            "evaluation_metric": "accuracy (entailment / non-entailment classification)",
            "human_performance": "accuracy range 76%–97% (reported in this paper as human performance range on HANS examples)",
            "llm_performance": "accuracy 100% (reported for GPT-4 in this paper; authors caution this may be due to dataset subset selection where examples were all non-entailment and model may exploit that heuristic)",
            "prompting_method": "not reported (model accessed via ChatGPT-Plus per Methods)",
            "fine_tuned": false,
            "human_data_source": "HANS paper: \"Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference\" (McCoy et al., 2019) — human performance range reported in this paper's summary",
            "statistical_significance": null,
            "notes": "Authors warn GPT-4's perfect accuracy may reflect dataset selection bias (used examples may have been all non-entailment) and potential memorization of the heuristic; prior models (e.g., BERT) performed very poorly on non-entailment subsets (&lt;10% in one cited category). No formal statistical comparison reported.",
            "uuid": "e7219.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "rating": 2,
            "sanitized_title": "commonsenseqa_a_question_answering_challenge_targeting_commonsense_knowledge"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "rating": 2,
            "sanitized_title": "superglue_a_stickier_benchmark_for_generalpurpose_language_understanding_systems"
        },
        {
            "paper_title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "rating": 2,
            "sanitized_title": "right_for_the_wrong_reasons_diagnosing_syntactic_heuristics_in_natural_language_inference"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Hellaswag: Can a machine really finish your sentence?",
            "rating": 1,
            "sanitized_title": "hellaswag_can_a_machine_really_finish_your_sentence"
        },
        {
            "paper_title": "Winogrande: An adversarial winograd schema challenge at scale",
            "rating": 1,
            "sanitized_title": "winogrande_an_adversarial_winograd_schema_challenge_at_scale"
        }
    ],
    "cost": 0.0091605,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</p>
<p>Sifatkaur Dhingra sifatkaurd13@gmail.com 
Manmeet Singh manmeet.cat@tropmet.res.in 
Vaisakh Sb vaisakh.sb@tropmet.res.in 
Neetiraj Malviya neetirajmalviya@gmail.com 
Sukhpal Singh Gill s.s.gill@qmul.ac.uk </p>
<p>Department of Psychology
Indian Institute of Tropical Meteorology Pune
Indian Institute of Tropical Meteorology Pune
Defence Institute Of Advanced Technology Pune
Nowrosjee Wadia College Pune
India, India, India, India</p>
<p>Queen Mary University of London London
United Kingdom</p>
<p>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</p>
<p>Cognitive psychology delves on understanding perception, attention, memory, language, problemsolving, decision-making, and reasoning. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required. In this study, we focus on the evaluation of GPT-4's performance on a set of cognitive psychology datasets such as CommonsenseQA, Super-GLUE, MATH and HANS. In doing so, we understand how GPT-4 processes and integrates cognitive psychology with contextual information, providing insight into the underlying cognitive processes that enable its ability to generate the responses. We show that GPT-4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior state-of-the-art models. Our results strengthen the already available assessments and confidence on GPT-4's cognitive psychology abilities. It has significant potential to revolutionize the field of AI, by enabling machines to bridge the gap between human and machine reasoning.</p>
<p>Introduction</p>
<p>Cognitive psychology aims to decipher how humans learn new things, retain knowledge, and recall it when needed. Cognitive psychologists seek to understand how the mind works by conducting studies on people's thoughts and actions and by using other experimental methods like brain imaging and computer modelling. Understanding the human mind and developing our cognitive skills to excel in a variety of areas is the ultimate objective of cognitive psychology. Language models have come a long way since the first statistical models for modelling language were introduced. With the advent of deep learning and the availability of large amounts of data, recent years have seen a rapid evolution of language models that have achieved human-like performance on many language tasks. Large Language Models (LLMs) are a type of artificial intelligence framework that have garnered significant attention in recent years due to their remarkable language processing capabilities (Harrer 2023). These models are trained on vast amounts of text data and are able to generate coherent, human-like responses to natural language queries. One of the key features of LLMs is their ability to generate novel and creative responses to text-based prompts, which has led to their increasing use in fields such as chatbots, question answering systems, and language translation. The use of self-attention has been a key factor in this success, as it allows for more efficient and accurate modeling of long-range dependencies within the input sequence, resulting in better performance compared to traditional RNN-based models. LLMs have demonstrated impressive performance on a wide range of language tasks, including language modeling, machine translation, sentiment analysis, and text classification. These capabilities have led to the increased use of LLMs in various fields, including language-based customer service, virtual assistants, and creative writing. One of the key areas measuring intelligence in humans, other species and machines is the cognitive psychology. There are several tasks that are considered to be the benchmarks for testing cognitive psychology. Some of them are text interpretation, computer vision, planning and reasoning. For cognitive psychology to work, we rely on a complex and potent social practise: the attribution and assessment of thoughts and actions [1]. The scientific psychology of cognition and behaviour, a relatively recent innovation, focuses primarily on the information-processing mechanisms and activities that characterise human cognitive and behavioural capabilities. Researchers have attempted to create systems that could use natural language to reason about their surroundings [2] or that could use a world model to get a more profound comprehension of spoken language [3]. The report introducing GPT-4 [4] has tested the HellaSwag [5] and WinoGrande [6] datasets for cognitive psychology. Although, these tests are relevant, they lack the sophistication required to understand deep heuristics of GPT-4. Hellaswag entails the task of finishing a sentence and WinoGrande involves identifying the correct noun for the pronouns in a sentence, which are quite simple. Other tasks and standardized datasets [7] which test the psychology are needed in order to perform a comprehensive assessment of cognitive psychology for GPT-4. Moreover GPT-4 needs to go through complex reasoning tasks than just predicting the last word of the sentence such as in Hellaswag, to emerge as a model capable of high-level intelligence. [8] note that SuperGLUE [9], CommonsenseQA [10], MATH [11] and HANS [12] are four such datasets that are needed to be tested for a comprehensive cognitive psychology evaluation of AI models. In this study, we evaluate the performance of GPT-4 on the SuperGLUE, CommonsenseQA, MATH and HANS datasets. This is a work in progress and we are performing continuous tests with the other datasets as suggested by [8]. Our study can be used to build up higher-order psychological tests using GPT-4.</p>
<p>Datasets and Methodology</p>
<p>In this study, four datasets have been used to test the cognitive psychology capabilities of GPT-4. The four datasets are CommonsenseQA, MATH, SuperGLUE and HANS. They are described as below:</p>
<p>CommonsenseQA</p>
<p>CommonsenseQA is a dataset composed for testing commonsense reasoning. There are 12,247 questions in the dataset, each with 5 possible answers. Workers using Amazon's Mechanical Turk were used to build the dataset. The goal of the dataset is to evaluate the commonsense knowledge using CONCEPTNET to generate difficult questions. The language model tested in the CommonsenseQA paper has an accuracy of 55.9 % whereas the authors report that human accuracy on the dataset is around 89 %.</p>
<p>MATH</p>
<p>The MATH dataset includes almost 12,500 problems from scholastic mathematics contests. Machine learning models take a mathematical problem as input and produce an answer-encoding sequence, such as f rac23. After normalisation, their answers are distinct, therefore MATH may be evaluated using exact match instead of heuristic metrics like BLEU. Problems in seven different areas of mathematics, including geometry, are categorised by complexity from 1 to 5, and diagrams can be expressed in text using the Asymptote language. This allows for a nuanced evaluation of problem-solving skills in mathematics across a wide range of rigour and content. Problems now have comprehensive, detailed, step-by-step answers. To improve learning and make model outputs more interpretable, models can be trained on these to develop their own step-by-step solutions. The MATH dataset presents a significant challenge, with accuracy rates for big language models ranging from 3.0% to 6.9%. Models attain up to 15% accuracy on the least difficulty level and can develop step-by-step answers that are coherent and on-topic even when erroneous, suggesting that they do possess some mathematical knowledge despite their low accuracies. The results of human evaluations on MATH show that it may be difficult for humans as well; a computer science PhD student who does not really like mathematics scored about 40%, while a three-time IMO gold medallist scored 90%.</p>
<p>SuperGLUE</p>
<p>SuperGLUE is an updated version of the GLUE benchmark that includes a more challenging set of language understanding tasks. Using the gap between human and machine performance as a metric, SuperGLUE improves upon the GLUE benchmark by defining a new set of difficult Natural Language Understanding (NLU) problems. About half of the tasks in the SuperGLUE benchmark have fewer than 1k instances, and all but one have fewer than 10k examples, highlighting the importance of different task formats and low-data training data problems. As compared to humans, SuperGLUE scores roughly 20 points worse when using BERT as a baseline in the original study. To get closer to human-level performance on the benchmark, the authors argue that advances in multi-task, transfer, and unsupervised/self-supervised learning approaches are essential.</p>
<p>HANS</p>
<p>The strength of neural networks lies in their ability to analyse a training set for statistical patterns and then apply those patterns to test instances that come from the same distribution. This advantage is not without its drawbacks, however, as statistical learners, such as traditional neural network designs, tend to rely on simplistic approaches that work for the vast majority of training samples rather than capturing the underlying generalisations. The loss function may not motivate the model to learn to generalise to increasingly difficult scenarios in the same way a person would if heuristics tend to produce mostly correct results. This problem has been observed in several applications of AI. Contextual heuristics mislead object-recognition neural networks in computer vision, for example; a network that can accurately identify monkeys in a normal situation may mistake a monkey carrying a guitar for a person, since guitars tend to co-occur with people but not monkeys in the training set. Visual question answering systems are prone to the same heuristics. This problem is tackled by HANS (Heuristic Analysis for NLI Systems), which uses heuristics to determine if a premise sentence entails (i.e., suggests the truth of) a hypothesis sentence. Neural Natural Language Inference (NLI) models have been demonstrated to learn shallow heuristics based on the presence of specific words, as has been the case in other fields. As not often appears in the instances of contradiction in normal NLI training sets, a model can categorise all inputs containing the word not as contradiction. HANS prioritises heuristics that are founded on elementary syntactic characteristics. Think about the entailment-focused phrase pair below:</p>
<p>Premise: The judge was paid by the actor.</p>
<p>Hypothesis: The actor paid the judge.</p>
<p>An NLI system may accurately label this example not by deducing the meanings of these lines but by assuming that the premise involves any hypothesis whose terms all occur in the premise. Importantly, if the model is employing this heuristic, it will incorrectly classify the following as entailed even when it is not.</p>
<p>Premise: The actor was paid by the judge.</p>
<p>Hypothesis: The actor paid the judge.</p>
<p>HANS is intended to detect the presence of such faulty structural heuristics. The authors focus on the lexical overlap, subsequence, and component heuristics. These heuristics are not legitimate inference procedures despite often producing correct labels. Rather than just having reduced overall accuracy, HANS is meant to ensure that models using these heuristics fail on specific subsets of the dataset. Four well-known NLI models, including BERT, are compared and contrasted using the HANS dataset. For this dataset, all models significantly underperformed the chance distribution, with accuracy just exceeding 0% in most situations.</p>
<p>Methodology</p>
<p>We test the four datasets as described above to test the cognitive psychology capabilities of GPT-4. The model is accessed using the ChatGPT-Plus offered by OpenAI. We evaluate these models as shown in the results and discussion section.</p>
<p>Results</p>
<p>We will first discuss the human and machine skill of the different models traditionally used in the datasets used to test cognitive psychology. As compared to humans, SuperGLUE scores roughly 20 points worse when using BERT as a baseline in the original study. To get closer to human-level performance on the benchmark, the authors argue that advances in multi-task, transfer, and unsupervised/self-supervised learning approaches are essential. The language model tested in the CommonsenseQA paper has an accuracy of 55.9 % whereas the authors report that human accuracy on the dataset is around 89 %. The accuracy of humans on HANS dataset ranged from 76-97 % and the authors show   </p>
<p>Conclusions</p>
<p>GPT-4, which is a state-of-the-art large language model, is a revolution in the field of psychology since it gives psychologists unprecedented resources to use in their studies and work. This sophisticated AI model offers psychologists and psychiatrists to learn more about the human mind and come up with novel treatment theories and approaches. It provides an avenue for improved efficacy of psychological therapies and allowing professionals to spend more time with clients, leading to deeper and more fruitful therapeutic bonds. The potential applications of GPT-4 can only be realized if the model is thoroughly tested on basic tests of reasoning and cognition. Cognitive psychology enables the humans to perform various activities [13] in their personal and professional lives. We show that the performance of GPT-4 greatly surpasses the language model used in the original studies from where the different datasets are sourced, thus it can make a tool of day-to-day utility for psychologists. This development can lead to cascading benefits in addressing the mental health challenges faced by today's society. Problem: $ABCD$, a rectangle with $AB = 12$ and $BC = 16$, is the base of pyramid $P$, which has a height of $24$. A plane parallel to $ABCD$ is passed through $P$, dividing $P$ into a frustum $F$ and a smaller pyramid $P'$. Let $X$ denote the center of the circumsphere of $F$, and let $T$ denote the apex of $P$. If the volume of $P$ is eight times that of $P'$, then the value of $XT$ can be expressed as $ \frac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. Compute the value of $m + n$.</p>
<p>Finally, we have $m + n = 108 + 10 = \boxed{118}$. </p>
<p>Figure 1 :
1Datasets used in the study with the different categories contained in them. that the BERT model performed below 10 % on the non-entailment category. The human performance on MATH varied from 40-90 % and GPT-2/GPT-3 showed accuracies below 10 %.</p>
<p>Figure 1
1shows that GPT-4 has an accuracy of 83.2 % on CommonSenseQA, data, we find that GPT-4 has an accuracy of around 84%, 82 % on prealgebra, 35% on geometry, 100% on HANS and 91.2 % on SuperGLUE. It is to be noted that the perfect results on HANS data might be because all the examples used are of non-entailment, as the model might be memorizing this particular heuristic. The experiments to generate GPT-4 results with mixed data from HANS are ongoing.</p>
<p>She caught the first snowflake of Sher life on her tongue, she was naively excited to be spending a Swhat in a region that actually had snow? question concept: punishing choices: SA:cloud B:december C:air D:africa E:winterwhat in a region that actually had snow? question concept: punishing choices:Problem: Find $a+b+c$ if the graph 
of the equation $y=ax^2+bx+c$ 
is a parabola with vertex $(5,3)$, 
vertical axis of symmetry, and contains 
the point $(2,0)$. Level: Level 5 Type: 
Algebra </p>
<p>Non entailment
entailmentProblem: What is the greatest integer $x$ for which $\frac79 &gt; \frac{x} {13}$? Level: Level 3 Type: Prealgebra Solution: Sentence1 Tanks were developed by Britain and France, and were first used in combat by the British during a battle. Sentence2 Tanks were developed by Britain and France, and were first used in combat by the British during a battle with German forces. Entailment or Non-EntailmentFigure 2: Examples of sample prompts and the respective responses of GPT4 on CommonsenseQA, MATH and SuperGLUE datasets$a+b+c = -
\frac{7}{3}$. </p>
<p>the greatest 
integer $x$ that 
satisfies the 
inequality is $x = 
10$. </p>
<p>E:winter </p>
<p>Dataset/Task 
Sample Prompt 
GPT4 
response </p>
<p>Commonsense 
reasoning </p>
<p>Algebra </p>
<p>Prealgebra </p>
<p>SuperGLUE </p>
<p>Geometry </p>
<p>Text and patterns: For effective chain of thought. Aman Madaan, Amir Yazdanbakhsh, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. Figure 3: Accuracy of GPT4 on cognitive psychology tasksAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. Figure 3: Accuracy of GPT4 on cognitive psychology tasks</p>
<p>A basis for a mathematical theory of computation. John Mccarthy, Studies in Logic and the Foundations of Mathematics. Elsevier26John McCarthy. A basis for a mathematical theory of computation. In Studies in Logic and the Foundations of Mathematics, volume 26, pages 33-70. Elsevier, 1959.</p>
<p>Understanding natural language. Terry Winograd, Cognitive psychology. 31Terry Winograd. Understanding natural language. Cognitive psychology, 3(1):1-191, 1972.</p>
<p>Gpt-4 technical report. Openai, OpenAI. Gpt-4 technical report. 2023.</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.07830arXiv preprintRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 649Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.</p>
<p>Systematic generalization and emergent structures in transformers trained on structured tasks. Yuxuan Li, James L Mcclelland, arXiv:2210.00400arXiv preprintYuxuan Li and James L McClelland. Systematic generalization and emergent structures in transformers trained on structured tasks. arXiv preprint arXiv:2210.00400, 2022.</p>
<p>Probing the psychology of ai models. Richard Shiffrin, Melanie Mitchell, Proceedings of the National Academy of Sciences. 120102300963120Richard Shiffrin and Melanie Mitchell. Probing the psychology of ai models. Proceedings of the National Academy of Sciences, 120(10):e2300963120, 2023.</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 32Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937arXiv preprintAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874arXiv preprintDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. Thomas Mccoy, Ellie Pavlick, Tal Linzen, arXiv:1902.01007arXiv preprintR Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.</p>
<p>Using large language models to simulate multiple humans. Gati Aher, I Rosa, Adam Tauman Arriaga, Kalai, arXiv:2208.10264arXiv preprintGati Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans. arXiv preprint arXiv:2208.10264, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
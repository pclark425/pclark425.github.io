<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6775 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6775</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6775</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-21ae9d85c3c31114abe39acf56f18a117e3e3ead</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/21ae9d85c3c31114abe39acf56f18a117e3e3ead" target="_blank">Large Language Models as Minecraft Agents</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work applies and evaluates LLMs in the builder and architect settings, introduces clarification questions and examines the challenges and opportunities for improvement, and presents a platform for online interaction with the agents.</p>
                <p><strong>Paper Abstract:</strong> In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6775.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6775.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, instruction-tuned transformer language model from OpenAI used in this paper as the primary LLM for both builder and architect roles in a Minecraft-like collaborative building task; evaluated on IGLU NLP Evaluation accuracy and shown to ask clarification questions and handle complex spatial descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models as Minecraft Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction‑tuned large decoder transformer (OpenAI). Treated as a conversational/instruction-following LLM in zero‑shot prompting with constrained JSON output.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Collaborative Builder Task (Minecraft voxel building)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / grounded instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>IGLU NLP Evaluation (target structures drawn from the Minecraft Dialogue Corpus / Narayan-Chen et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero‑shot instruction prompting with a detailed role prompt and forced JSON output (explicit instruction prompt requiring only JSON responses); per-turn consumption of utterance and JSON response.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Iterative dialogue-based clarification and instruction interpretation (no explicit chain‑of‑thought or external search reported); reasoning emerges from the LLM's internal language capabilities and iterative exchanges.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>World state and target structures encoded as JSON lists of block tuples [[x,y,z,color], ...]; builder responses required JSON: {"add": [[x,y,z,color], ...], "remove": [...], "confidence": float, "question": "..."}. Coordinate system and bounds provided in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on IGLU NLP Evaluation (instruction→block placement interpretation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>37.6% accuracy (Table 1: IGLU NLP Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>GPT-4 can ask clarification questions when instructions are underspecified, handle references to existing blocks/groups, continue spatial patterns (e.g., diagonal sequences), and describe grouped placements (e.g., 'column'). For complex architect tasks it required multiple exchanges to converge to correct instructions; builder (language-only) produced valid structured outputs and was competitive with prior bespoke models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>No formal ablation studies reported; comparison across models shown (GPT-4 vs GPT-3.5 vs Llama2/Vicuna) via accuracy table but no component-level ablation (e.g., with/without clarification ability) was given.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Accuracy remains modest (37.6%); paper restricts to the language component (does not evaluate low-level agent navigation/placement). Architect role struggled on complex targets and required repeated refinement; no quantitative architect evaluation provided. GPT-4 model size and internal training details not specified in the paper. Reliance on strict JSON output causes non-JSON responses to be ignored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Minecraft Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6775.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6775.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A behind-the-scenes OpenAI instruction-tuned LLM (smaller/older than GPT-4) tested as a builder/architect in the Minecraft collaborative building task, evaluated using the same prompt scheme and IGLU NLP metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models as Minecraft Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction‑tuned decoder transformer from OpenAI; used with explicit instruction prompts and required JSON-format outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Collaborative Builder Task (Minecraft voxel building)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / grounded instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>IGLU NLP Evaluation (Minecraft Dialogue Corpus targets)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero‑shot instruction prompting with task and world constraints, JSON output requirement.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Iterative interpretation of instructions and optional clarification questions; no external search or explicit reasoning algorithm reported.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Same JSON encoding of world/targets: lists of [x,y,z,color]; builder outputs JSON with 'add','remove','confidence','question'.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on IGLU NLP Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>34.4% accuracy (Table 1: IGLU NLP Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Performs slightly worse than GPT-4 but still demonstrates capability to interpret spatial instructions and produce JSON actions; can ask clarification when instructions lack details.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>No ablations; performance reported relative to other models in table only.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower accuracy than GPT-4; same limitations as the study (language-only, ignores physical navigation); no detailed failure-mode analysis beyond general comments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Minecraft Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6775.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6775.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 70B Chat (instruction‑tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large chat model (70B parameters) evaluated in the builder/architect roles using the same instruction prompt and JSON interface; shows lower performance than closed models on IGLU NLP Evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models as Minecraft Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation LLaMA‑derived chat model (chat‑tuned) used with instruction prompt; decoder transformer, publicly released by Meta and referenced in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Collaborative Builder Task (Minecraft voxel building)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / grounded instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>IGLU NLP Evaluation (Minecraft Dialogue Corpus targets)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero‑shot instruction prompting with JSON response constraint and explicit coordinate system in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Instruction interpretation via model's internal language reasoning and iterative clarification when needed; no external reasoning module.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>World and targets encoded in JSON lists [[x,y,z,color], ...]; builder prompt requires specific JSON keys and coordinate conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on IGLU NLP Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>23.9% accuracy (Table 1: IGLU NLP Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Capable of following spatial instructions but lags behind GPT models; the paper highlights gaps between open models and closed models like GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>No ablation; only cross-model accuracy comparisons reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower accuracy (23.9%); authors note need for prompt engineering, few-shot or finetuning to close gap; no specialized improvements applied in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Minecraft Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6775.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6775.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 13B Chat (instruction‑tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller open-source chat model (13B) tested as builder/architect using the paper's JSON instruction prompts and evaluated on IGLU NLP Evaluation, showing reduced performance compared to larger and closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models as Minecraft Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation LLaMA-derived 13B parameter chat model, instruction/chat tuned; used zero-shot with strict JSON output requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Collaborative Builder Task (Minecraft voxel building)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / grounded instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>IGLU NLP Evaluation (Minecraft Dialogue Corpus targets)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero‑shot instruction prompting with JSON-format output constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Language-based interpretation and optional clarification questions; no explicit external reasoning technique.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>JSON-encoded world/target: arrays of [x,y,z,color]; builder asked to return specified JSON keys.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on IGLU NLP Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>15.0% accuracy (Table 1: IGLU NLP Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Smaller Llama2 model shows weaker spatial instruction understanding; demonstrates sensitivity of task performance to model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>No ablation; only size-based comparison through reported accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Low accuracy (15%); authors suggest improvements via prompt engineering or finetuning; no internal probing or attention analyses presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Minecraft Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6775.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6775.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna 13B (open chat model fine‑tuned from LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source chat model (13B) derived from LLaMA and fine‑tuned on user-shared conversations, evaluated here with the same JSON instruction approach and showing the lowest accuracy among reported models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models as Minecraft Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open‑source chat model fine‑tuned from LLaMA on conversational data; used with zero‑shot instruction prompt requiring JSON output.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Collaborative Builder Task (Minecraft voxel building)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / grounded instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>IGLU NLP Evaluation (Minecraft Dialogue Corpus targets)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero‑shot instruction prompting with enforced JSON output format.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Direct instruction interpretation; may ask clarification questions but no explicit advanced reasoning technique used.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>JSON format for world and actions: [[x,y,z,color], ...] and builder JSON response schema.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on IGLU NLP Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>6.3% accuracy (Table 1: IGLU NLP Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Performs poorly relative to larger/chat‑tuned models; indicates sensitivity to model quality/fine‑tuning for spatial instruction grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>No ablation; only cross-model comparison shown.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Very low accuracy (6.3%); same study limitations (language-only evaluation, no navigation), and no further analyses provided for failure cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Minecraft Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Collaborative dialogue in minecraft <em>(Rating: 2)</em></li>
                <li>Interactive grounded language understanding in a collaborative environment: Iglu 2021 <em>(Rating: 2)</em></li>
                <li>IGLU 2022: Interactive grounded language understanding in a collaborative environment at neurips 2022 <em>(Rating: 2)</em></li>
                <li>Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback <em>(Rating: 2)</em></li>
                <li>Learning to solve voxel building embodied tasks from pixels and natural language instructions <em>(Rating: 1)</em></li>
                <li>Learning to execute instructions in a minecraft dialogue <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6775",
    "paper_id": "paper-21ae9d85c3c31114abe39acf56f18a117e3e3ead",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large, instruction-tuned transformer language model from OpenAI used in this paper as the primary LLM for both builder and architect roles in a Minecraft-like collaborative building task; evaluated on IGLU NLP Evaluation accuracy and shown to ask clarification questions and handle complex spatial descriptions.",
            "citation_title": "Large Language Models as Minecraft Agents",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction‑tuned large decoder transformer (OpenAI). Treated as a conversational/instruction-following LLM in zero‑shot prompting with constrained JSON output.",
            "model_size": null,
            "puzzle_name": "Collaborative Builder Task (Minecraft voxel building)",
            "puzzle_type": "spatial reasoning / grounded instruction following",
            "dataset_name": "IGLU NLP Evaluation (target structures drawn from the Minecraft Dialogue Corpus / Narayan-Chen et al. 2019)",
            "prompting_method": "Zero‑shot instruction prompting with a detailed role prompt and forced JSON output (explicit instruction prompt requiring only JSON responses); per-turn consumption of utterance and JSON response.",
            "reasoning_technique": "Iterative dialogue-based clarification and instruction interpretation (no explicit chain‑of‑thought or external search reported); reasoning emerges from the LLM's internal language capabilities and iterative exchanges.",
            "internal_representation": "World state and target structures encoded as JSON lists of block tuples [[x,y,z,color], ...]; builder responses required JSON: {\"add\": [[x,y,z,color], ...], \"remove\": [...], \"confidence\": float, \"question\": \"...\"}. Coordinate system and bounds provided in the prompt.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Accuracy on IGLU NLP Evaluation (instruction→block placement interpretation)",
            "performance": "37.6% accuracy (Table 1: IGLU NLP Evaluation)",
            "analysis_findings": "GPT-4 can ask clarification questions when instructions are underspecified, handle references to existing blocks/groups, continue spatial patterns (e.g., diagonal sequences), and describe grouped placements (e.g., 'column'). For complex architect tasks it required multiple exchanges to converge to correct instructions; builder (language-only) produced valid structured outputs and was competitive with prior bespoke models.",
            "ablation_comparison": "No formal ablation studies reported; comparison across models shown (GPT-4 vs GPT-3.5 vs Llama2/Vicuna) via accuracy table but no component-level ablation (e.g., with/without clarification ability) was given.",
            "limitations": "Accuracy remains modest (37.6%); paper restricts to the language component (does not evaluate low-level agent navigation/placement). Architect role struggled on complex targets and required repeated refinement; no quantitative architect evaluation provided. GPT-4 model size and internal training details not specified in the paper. Reliance on strict JSON output causes non-JSON responses to be ignored.",
            "uuid": "e6775.0",
            "source_info": {
                "paper_title": "Large Language Models as Minecraft Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5",
            "brief_description": "A behind-the-scenes OpenAI instruction-tuned LLM (smaller/older than GPT-4) tested as a builder/architect in the Minecraft collaborative building task, evaluated using the same prompt scheme and IGLU NLP metric.",
            "citation_title": "Large Language Models as Minecraft Agents",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Instruction‑tuned decoder transformer from OpenAI; used with explicit instruction prompts and required JSON-format outputs.",
            "model_size": null,
            "puzzle_name": "Collaborative Builder Task (Minecraft voxel building)",
            "puzzle_type": "spatial reasoning / grounded instruction following",
            "dataset_name": "IGLU NLP Evaluation (Minecraft Dialogue Corpus targets)",
            "prompting_method": "Zero‑shot instruction prompting with task and world constraints, JSON output requirement.",
            "reasoning_technique": "Iterative interpretation of instructions and optional clarification questions; no external search or explicit reasoning algorithm reported.",
            "internal_representation": "Same JSON encoding of world/targets: lists of [x,y,z,color]; builder outputs JSON with 'add','remove','confidence','question'.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Accuracy on IGLU NLP Evaluation",
            "performance": "34.4% accuracy (Table 1: IGLU NLP Evaluation)",
            "analysis_findings": "Performs slightly worse than GPT-4 but still demonstrates capability to interpret spatial instructions and produce JSON actions; can ask clarification when instructions lack details.",
            "ablation_comparison": "No ablations; performance reported relative to other models in table only.",
            "limitations": "Lower accuracy than GPT-4; same limitations as the study (language-only, ignores physical navigation); no detailed failure-mode analysis beyond general comments.",
            "uuid": "e6775.1",
            "source_info": {
                "paper_title": "Large Language Models as Minecraft Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama2-70b-chat",
            "name_full": "Llama 2 70B Chat (instruction‑tuned variant)",
            "brief_description": "An open-source large chat model (70B parameters) evaluated in the builder/architect roles using the same instruction prompt and JSON interface; shows lower performance than closed models on IGLU NLP Evaluation.",
            "citation_title": "Large Language Models as Minecraft Agents",
            "mention_or_use": "use",
            "model_name": "Llama2-70b-chat",
            "model_description": "Open foundation LLaMA‑derived chat model (chat‑tuned) used with instruction prompt; decoder transformer, publicly released by Meta and referenced in paper.",
            "model_size": "70B",
            "puzzle_name": "Collaborative Builder Task (Minecraft voxel building)",
            "puzzle_type": "spatial reasoning / grounded instruction following",
            "dataset_name": "IGLU NLP Evaluation (Minecraft Dialogue Corpus targets)",
            "prompting_method": "Zero‑shot instruction prompting with JSON response constraint and explicit coordinate system in prompt.",
            "reasoning_technique": "Instruction interpretation via model's internal language reasoning and iterative clarification when needed; no external reasoning module.",
            "internal_representation": "World and targets encoded in JSON lists [[x,y,z,color], ...]; builder prompt requires specific JSON keys and coordinate conventions.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Accuracy on IGLU NLP Evaluation",
            "performance": "23.9% accuracy (Table 1: IGLU NLP Evaluation)",
            "analysis_findings": "Capable of following spatial instructions but lags behind GPT models; the paper highlights gaps between open models and closed models like GPT-4.",
            "ablation_comparison": "No ablation; only cross-model accuracy comparisons reported.",
            "limitations": "Lower accuracy (23.9%); authors note need for prompt engineering, few-shot or finetuning to close gap; no specialized improvements applied in this study.",
            "uuid": "e6775.2",
            "source_info": {
                "paper_title": "Large Language Models as Minecraft Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama2-13b-chat",
            "name_full": "Llama 2 13B Chat (instruction‑tuned variant)",
            "brief_description": "Smaller open-source chat model (13B) tested as builder/architect using the paper's JSON instruction prompts and evaluated on IGLU NLP Evaluation, showing reduced performance compared to larger and closed models.",
            "citation_title": "Large Language Models as Minecraft Agents",
            "mention_or_use": "use",
            "model_name": "Llama2-13b-chat",
            "model_description": "Open foundation LLaMA-derived 13B parameter chat model, instruction/chat tuned; used zero-shot with strict JSON output requirements.",
            "model_size": "13B",
            "puzzle_name": "Collaborative Builder Task (Minecraft voxel building)",
            "puzzle_type": "spatial reasoning / grounded instruction following",
            "dataset_name": "IGLU NLP Evaluation (Minecraft Dialogue Corpus targets)",
            "prompting_method": "Zero‑shot instruction prompting with JSON-format output constraint.",
            "reasoning_technique": "Language-based interpretation and optional clarification questions; no explicit external reasoning technique.",
            "internal_representation": "JSON-encoded world/target: arrays of [x,y,z,color]; builder asked to return specified JSON keys.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Accuracy on IGLU NLP Evaluation",
            "performance": "15.0% accuracy (Table 1: IGLU NLP Evaluation)",
            "analysis_findings": "Smaller Llama2 model shows weaker spatial instruction understanding; demonstrates sensitivity of task performance to model scale.",
            "ablation_comparison": "No ablation; only size-based comparison through reported accuracies.",
            "limitations": "Low accuracy (15%); authors suggest improvements via prompt engineering or finetuning; no internal probing or attention analyses presented.",
            "uuid": "e6775.3",
            "source_info": {
                "paper_title": "Large Language Models as Minecraft Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Vicuna-13b",
            "name_full": "Vicuna 13B (open chat model fine‑tuned from LLaMA)",
            "brief_description": "An open-source chat model (13B) derived from LLaMA and fine‑tuned on user-shared conversations, evaluated here with the same JSON instruction approach and showing the lowest accuracy among reported models.",
            "citation_title": "Large Language Models as Minecraft Agents",
            "mention_or_use": "use",
            "model_name": "Vicuna-13b",
            "model_description": "Open‑source chat model fine‑tuned from LLaMA on conversational data; used with zero‑shot instruction prompt requiring JSON output.",
            "model_size": "13B",
            "puzzle_name": "Collaborative Builder Task (Minecraft voxel building)",
            "puzzle_type": "spatial reasoning / grounded instruction following",
            "dataset_name": "IGLU NLP Evaluation (Minecraft Dialogue Corpus targets)",
            "prompting_method": "Zero‑shot instruction prompting with enforced JSON output format.",
            "reasoning_technique": "Direct instruction interpretation; may ask clarification questions but no explicit advanced reasoning technique used.",
            "internal_representation": "JSON format for world and actions: [[x,y,z,color], ...] and builder JSON response schema.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Accuracy on IGLU NLP Evaluation",
            "performance": "6.3% accuracy (Table 1: IGLU NLP Evaluation)",
            "analysis_findings": "Performs poorly relative to larger/chat‑tuned models; indicates sensitivity to model quality/fine‑tuning for spatial instruction grounding.",
            "ablation_comparison": "No ablation; only cross-model comparison shown.",
            "limitations": "Very low accuracy (6.3%); same study limitations (language-only evaluation, no navigation), and no further analyses provided for failure cases.",
            "uuid": "e6775.4",
            "source_info": {
                "paper_title": "Large Language Models as Minecraft Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Collaborative dialogue in minecraft",
            "rating": 2
        },
        {
            "paper_title": "Interactive grounded language understanding in a collaborative environment: Iglu 2021",
            "rating": 2
        },
        {
            "paper_title": "IGLU 2022: Interactive grounded language understanding in a collaborative environment at neurips 2022",
            "rating": 2
        },
        {
            "paper_title": "Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback",
            "rating": 2
        },
        {
            "paper_title": "Learning to solve voxel building embodied tasks from pixels and natural language instructions",
            "rating": 1
        },
        {
            "paper_title": "Learning to execute instructions in a minecraft dialogue",
            "rating": 1
        }
    ],
    "cost": 0.010428999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models as Minecraft Agents</h1>
<p>Chris Madge<br>Queen Mary University of London<br>c.j.madge@qmul.ac.uk</p>
<p>Massimo Poesio<br>Queen Mary University of London<br>m.poesio@qmul.ac.uk</p>
<h4>Abstract</h4>
<p>In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.</p>
<h2>1 Introduction</h2>
<p>The development of conversational agents able to operate in virtual world environments has long been of interest in AI (Winograd, 1972). In recent years, much of this research has focused on developing agents able to operate in game environments. Game environments provide an ideal sandbox for studying task-oriented conversational agents in games (Szlam et al., 2019), which has motivated the development of multiple platforms in which such research can be carried out (Johnson et al., 2016; Urbanek et al., 2019; CallisonBurch et al., 2022) (Gray et al., 2019; Ogawa et al., 2020; Köhn et al., 2020), data gathering exercises (Narayan-Chen et al., 2019; Jayannavar et al., 2020; Mohanty et al., 2022) and competitions (Kiseleva et al., 2022a). In this work we study the development of conversational agents able to operate in one of the most popular of these virtual world settings, the Minecraft world, and to carry out the "Collaborative Builder Task" proposed in (NarayanChen et al., 2019), in which an architect, who has a target structure to achieve, provides instructions to a builder, to complete that structure.</p>
<p>In previous research, deep neural methods were used to model both of these roles. These models used a CNN to model the world state, and an RNN to model the text input, with a similar decoder to produce the utterances (Narayan-Chen et al., 2019; Jayannavar et al., 2020; Kiseleva et al., 2023; Shi
et al., 2022). An alternative approach has been to separate the builder task into interpretation of natural language instructions, translating them into structured commands using a fine tuned T5 model (Raffel et al., 2020), then using a separate model to enact the required actions in the world (Kiseleva et al., 2023).</p>
<p>However, while it has been proposed using modern large language models (LLMs) such as Llama and GPT-4 for this purpose has been proposed (Hromei et al., 2023), to our knowledge this has yet to be explored. LLMs are arguably one of the most interesting advancements in natural language technology, if not the field of machine learning, in recent years. Some studies suggest that beyond their conversational ability (natural language understanding and generation (Achiam et al., 2023)), such models demonstrate the ability to solve novel tasks akin to an artificial general intelligence (Bubeck et al., 2023) with great potential for serving as the foundation for interactive agents (Xi et al., 2023). There are also many applications of LLMs as autonomous agents or embodied AI in interactive environments, particularly in robotics systems ( Wu et al., 2023; Ahn et al., 2022) and multi-agent simulations with cooperation/interaction (Park et al., 2023; Lin et al., 2023; Chen et al., 2023), industrial automation (Xia et al., 2023) and planning (Nottingham et al., 2023; Wang et al., 2023a; Zhu et al., 2023). However, we are not aware of studies using LLMs to implement Minecraft agents.</p>
<p>Our first contribution are LLM-based builder and architect agents. We developed builder and architect with both open-source and closed-source LLMs. We evaluate the builder by comparing it to previous comparable agents. Our second contribution is giving to these agents the ability to ask clarification questions (Skrynnik et al., 2022). This is one ability of conversational agents that remains relatively unexplored in this setting; to our knowledge, there is only a single model attempt-</p>
<p>ing this (Mehta et al., 2023), using BART (Lewis et al., 2019), but none with current LLMs. Our third contribution is a test environment, which we believe is highly suited to devising and testing different approaches to these problems in a simple task orientated setting. We deliberately present our environment as a web-based online platform to support future large scale experimentation with human participation. Our implementation draws inspiration from the web-based Interactive Grounded Language Understanding (IGLU) data collection system (Mohanty et al., 2022; Mehta et al., 2023).</p>
<h2>2 Related Work</h2>
<h3>2.1 The Original Architect and Builder Models for the Minecraft Task</h3>
<p>Several models for both the architect and builder roles have been proposed. The first architect model (Narayan-Chen et al., 2019) uses a seq2seq RNN that aside from the usual sequence of tokens as an input, also takes the current and target world state. This is decoded to the Architect utterance.</p>
<p>Following on from the Architect Model, the "Builder Action Prediction" model was created (Jayannavar et al., 2020). This model took also combined a CNN with an RNN, although the decoder now predicted a series of actions the agent could follow.</p>
<h3>2.2 IGLU</h3>
<p>The Interactive Grounded Language Understanding shared tasks (Kiseleva et al., 2022a,b) were proposed to stimulate research on building interactive agents that learn to solve a task while provided with grounded natural language instructions in a collaborative environment. The more recent edition of the shared task (Kiseleva et al., 2022b) involved two subtasks: 'Building Structures' (developing a conversational agent able to engage in the conversation), and 'Asking Clarification Questions' (developing a model able to ask clarification questions at the appropriate point).</p>
<p>The baseline (Skrynnik et al., 2022) provided for the 2022 IGLU task (Kiseleva et al., 2023) is split into two separate modules. One language component responsible for interpreting the natural language utterances and transforming them into simplified structured instructions, and a reinforcement learning component responsible for navigating the agent in the world and performing actions to realise those instructions.</p>
<p>Several builder models were entered in the IGLU 2022 competition task (Kiseleva et al., 2022b). The creators of the competition reflect upon the most notable winning submissions (Kiseleva et al., 2023). The top architecture took an end-to-end approach concatenating outputs from a language model and a CNN, which is then fed into a transformer designed specifically to improve and fine tune language models through reinforcement learning. However, second and third place both made improvements on the baselines architecture approach with further fine tuning on the T5 transformer model (Raffel et al., 2020).</p>
<h3>2.3 Using LLMs to generate Embodied AI in an Interactive Environment</h3>
<p>There are many applications of LLMs as autonomous agents or embodied AI in interactive environments (Wang et al., 2023b; Xi et al., 2023). Many of these do operate in a Minecraft world. However, they are targeted towards planning (e.g. dividing tasks to reach a final goal, including "crafting"/combining items, to reach that outcome) (Nottingham et al., 2023; Wang et al., 2023a; Zhu et al., 2023), not the creation of 3D structures.</p>
<h3>2.4 Generating and Responding to Clarification Questions</h3>
<p>Generating and responding to clarification questions generative remains relatively unexplored for this task. There have been models that label utterances as requiring clarification (Shi et al., 2022), or rank clarification questions appropriateness to an instruction in an information retrieval style (Kiseleva et al., 2023; Mohanty et al., 2023), but only one generative model (Mehta et al., 2023) using BART (Lewis et al., 2019). Whilst the notion of using modern LLMs such as Llama and GPT-4 for this purpose has been proposed, to our knowledge this is yet to be explored (Hromei et al., 2023).</p>
<h2>3 Methodology</h2>
<p>In this work we created models for the "Collaborative Builder Task" (Narayan-Chen et al., 2019) in which two agents collaborate in a voxel based world to build a structure. The architect is given a target world structure, and provides natural language instructions to a builder, who cannot see the target structure, to place blocks to achieve that goal structure. Unlike the majority of previous approaches, both agents are able to provide and answer questions.</p>
<table>
<thead>
<tr>
<th>Chat</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Class of Machine (Class of Machine)</td>
</tr>
<tr>
<td>GPT-4</td>
<td>37.6</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>34.4</td>
</tr>
<tr>
<td>Llama2-70b-chat</td>
<td>23.9</td>
</tr>
<tr>
<td>Llama2-13b-chat</td>
<td>15.0</td>
</tr>
<tr>
<td>Vicuna-13b</td>
<td>6.3</td>
</tr>
</tbody>
</table>
<h3>3.1 The Builder</h3>
<p>The builder agent operates as follows: The initial prompt to the LLM contains instructions for the task, constraints of the world (min/max for $x, y, z$ ), coordinate mappings (east/south/west/north) and requests that the LLM respond in a specified JSON format only (see 5 for full prompt). Following the initial prompt, on each turn, the agent consumes an utterance (instruction) and produces a sequence of actions in addition to an optional clarification question in the originally specified format. This JSON encoded representation is used to directly update the world. If the result does not constitute a valid JSON response it is disregarded. The builder is evaluated against IGLU RL baseline sub-task. We evaluate the agent based on its ability to interpret a natural language instruction and produce the correct 3D coordinates and colours for the target structure (not manoeuvre an agent in a 3D space to interact with the world and realise that target structure).</p>
<p>The closest work to our methodology is [mehta2023gpt3], similarly we focus solely on the language component of this task. We do not examine the task of manipulating an agent to place blocks as done in previous tasks [narayan-chen2019gpt; jayannavar2020gpt4; shi2022gpt4], as it is the language element that we are most interested in.</p>
<h3>3.2 The Architect</h3>
<p>The architect differs slightly from the builder. The prompt, along with architect specific instructions, contains the target state of the world (see 5). Following the initial prompt the architect consumes every utterance given by the builder, along with the current state of the world encoded in a structured JSON format. The target structures are taken from the Minecraft Dialogue Corpus [narayan-chen2019gpt] and evaluated empirically through repeated interactions.</p>
<h3>3.3 The LLMs</h3>
<p>In this work we test against the LLMs: GPT-4; GPT-3.5; Llama2 [touvron2023llama] (the chat variation in various parameter sizes) and Vicuna [chiang2023vicuna]. To our knowledge, the number of parameters used in the GPT models have not been formally released, although they are rumoured to be magnitudes greater than those used in the open models. The prompts used for both tasks are given in the appendix.</p>
<p>Table 1: Accuracy on IGLU NLP Evaluation</p>
<p>In Figure 1 we deliberately give an incomplete instruction, "place a block on the ground". We can see GPT-4 identifies the missing properties, asking for clarification on the colour and position with the question: "What color should the block be and where specifically should I place it?". When the builder responds "red" , this is resolved. We</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Builder complex structures (GPT-4)</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Architect - Simple Structure (GPT-4)</p>
<p>also should here a simple example of resolving reference to an existing block.</p>
<p>In Figure 2, we can see that GPT-4 will accept descriptions of quite complex structures, references to world positions, and continue a pattern between those positions. In addition, it will accept references that point to groups of blocks or existing structures/word spaces and carry out repetition.</p>
<h3>4.2 Architect</h3>
<p>In Figure 3, for the purpose of illustrating the example, the target structure is shown (this would normally be hidden from the builder). We can see, for simpler structures, the architect gives accurate instructions, can describe groups of placements (e.g., "column"), respond to questions and provide clarification.</p>
<p>However, as the structures increase in complexity (see Figure 4), the architect struggles to start. Each time the builder follows the architect's instruction, it revises it, getting one step closer to the true goal. The architect does eventually arrive at the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Architect - Complex Structure (GPT-4)</p>
<p>appropriate instruction, after repeated exchanges between the builder and architect.</p>
<h2>5 Conclusion</h2>
<p>In this work, we demonstrate LLMs acting as agents in a Minecraft-like block world task. The builder performs favourably over past bespoke models. In addition, the LLM has an existing built-in capability to ask and answer questions. In future work, we will look at methods of improving openly available LLM models to close the gap with the fine-tuned baseline, ranging from prompt engineering (e.g., introducing few-shot prompting, etc.) to finetuning and other methods. Our early experiments with the architect do show some challenges, and further work needs to be done to give a quantitative evaluation against existing architect models.</p>
<h2>Acknowledgements</h2>
<p>This research was funded by ARCIDUCA, EPSRC EP/W001632/1</p>
<h2>References</h2>
<ul>
<li>Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. <em>arXiv preprint arXiv:2303.08774</em>.</li>
<li>Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. 2022. Do as i can, not as i say: Ground-</li>
</ul>
<p>ing language in robotic affordances. arXiv preprint arXiv:2204.01691.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Chris Callison-Burch, Gaurav Singh Tomar, Lara J Martin, Daphne Ippolito, Suma Bailis, and David Reitter. 2022. Dungeons and dragons as a dialog challenge for artificial intelligence. arXiv preprint arXiv:2210.07109.</p>
<p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Jonathan Gray, Kavya Srinet, Yacine Jernite, Haonan Yu, Zhuoyuan Chen, Demi Guo, Siddharth Goyal, C Lawrence Zitnick, and Arthur Szlam. 2019. Craftassist: A framework for dialogue-enabled interactive agents. arXiv preprint arXiv:1907.08584.</p>
<p>Claudiu D Hromei, Daniele Margiotta, Danilo Croce, and Roberto Basili. 2023. An end-to-end transformerbased model for interactive grounded language understanding. In Proceedings of the Seventh Workshop on Natural Language for Artificial Intelligence (NL4AI 2023) co-located with 22th International Conference of the Italian Association for Artificial Intelligence (AI $^{\circ}$ IA 2023).</p>
<p>Prashant Jayannavar, Anjali Narayan-Chen, and Julia Hockenmaier. 2020. Learning to execute instructions in a minecraft dialogue. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 2589-2602.</p>
<p>Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. 2016. The malmo platform for artificial intelligence experimentation. In Ijcai, pages $4246-4247$.</p>
<p>Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha Mohanty, Maartje ter Hoeve, Mikhail Burtsev, Alexey Skrynnik, Artem Zholus, Aleksandr Panov, Kavya Srinet, et al. 2022a. Interactive grounded language understanding in a collaborative environment: Iglu 2021. In NeurIPS 2021 Competitions and Demonstrations Track, pages 146-161. PMLR.</p>
<p>Julia Kiseleva, Alexey Skrynnik, Artem Zholus, Shrestha Mohanty, Negar Arabzadeh, MarcAlexandre Côté, Mohammad Aliannejadi, Milagro Teruel, Ziming Li, Mikhail Burtsev, et al. 2023. Interactive grounded language understanding in a collaborative environment: Retrospective on iglu 2022 competition. In NeurIPS 2022 Competition Track, pages 204-216. PMLR.</p>
<p>Julia Kiseleva, Alexey Skrynnik, Artem Zholus, Shrestha Mohanty, Negar Arabzadeh, MarcAlexandre Côté, Mohammad Aliannejadi, Milagro Teruel, Ziming Li, Mikhail Burtsev, Maartje ter Hoeve, Zoya Volovikova, Aleksandr Panov, Yuxuan Sun, Kavya Srinet, Arthur Szlam, and Ahmed Awadallah. 2022b. Iglu 2022: Interactive grounded language understanding in a collaborative environment at neurips 2022.</p>
<p>Arne Köhn, Julia Wichlacz, Christine Schäfer, Alvaro Torralba, Jörg Hoffmann, and Alexander Koller. 2020. Mc-saar-instruct: a platform for minecraft instruction giving agents. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 53-56.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
<p>Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. 2023. Agentsims: An open-source sandbox for large language model evaluation.</p>
<p>Nikhil Mehta, Milagro Teruel, Patricio Figueroa Sanz, Xin Deng, Ahmed Hassan Awadallah, and Julia Kiseleva. 2023. Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback. arXiv preprint arXiv:2304.10750.</p>
<p>Shrestha Mohanty, Negar Arabzadeh, Julia Kiseleva, Artem Zholus, Milagro Teruel, Ahmed Awadallah, Yuxuan Sun, Kavya Srinet, and Arthur Szlam. 2023. Transforming human-centered ai collaboration: Redefining embodied agents capabilities through interactive grounded language instructions.</p>
<p>Shrestha Mohanty, Negar Arabzadeh, Milagro Teruel, Yuxuan Sun, Artem Zholus, Alexey Skrynnik, Mikhail Burtsev, Kavya Srinet, Aleksandr Panov, Arthur Szlam, et al. 2022. Collecting interactive multi-modal datasets for grounded language understanding. arXiv preprint arXiv:2211.06552.</p>
<p>Anjali Narayan-Chen, Prashant Jayannavar, and Julia Hockenmaier. 2019. Collaborative dialogue in minecraft. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages $5405-5415$.</p>
<p>Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. 2023. Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling.</p>
<p>Haruna Ogawa, Hitoshi Nishikawa, Takenobu Tokunaga, and Hikaru Yokono. 2020. Gamification platform for collecting task-oriented dialogue data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 7084-7093.</p>
<p>Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Zhengxiang Shi, Yue Feng, and Aldo Lipani. 2022. Learning to execute actions or ask clarification questions. arXiv preprint arXiv:2204.08373.</p>
<p>Alexey Skrynnik, Zoya Volovikova, Marc-Alexandre Côté, Anton Voronov, Artem Zholus, Negar Arabzadeh, Shrestha Mohanty, Milagro Teruel, Ahmed Awadallah, Aleksandr Panov, et al. 2022. Learning to solve voxel building embodied tasks from pixels and natural language instructions. arXiv preprint arXiv:2211.00688.</p>
<p>Arthur Szlam, Jonathan Gray, Kavya Srinet, Yacine Jernite, Armand Joulin, Gabriel Synnaeve, Douwe Kiela, Haonan Yu, Zhuoyuan Chen, Siddharth Goyal, et al. 2019. Why build an assistant in minecraft? arXiv preprint arXiv:1907.09273.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Jack Urbanek, Angela Fan, Siddharth Karamcheti, Emily Dinan Saachi Jain, Samuel Humeau, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291.</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2023b. A survey on large language model based autonomous agents.</p>
<p>Terry Winograd. 1972. Understanding natural language. Cognitive psychology, 3(1):1-191.</p>
<p>Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023. Tidybot: Personalized robot assistance with large language models. arXiv preprint arXiv:2305.05658.</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864.</p>
<p>Yuchen Xia, Manthan Shenoy, Nasser Jazdi, and Michael Weyrich. 2023. Towards autonomous system: flexible modular production system enhanced with large language model agents. arXiv preprint arXiv:2304.14721.</p>
<p>Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. 2023. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.</p>
<h2>Appendix</h2>
<h2>Architect prompt</h2>
<p>You are an agent in a voxel world, where the most northernly point is $0,0,-5$; the most westerly point $-5,0,0$; the most eastern point is $5,0,0$; the most southern $0,0,5$ and the $y$-axis is up and down, with $y=0$ being the minimum. Your task is to give instructions to a human to place blocks to achieve the target world state: «target_world_state» where the target world instructions are in the format [[x,y,z,color], ...]. Give easy to interpret instructions, do not directly mention the coordinates. The builder will respond with the coordinates of the blocks they have placed in the same format. Don't ask for coordinates, they will always be given. Avoid long instructions with multiple steps and start building the structure from the ground up.</p>
<h2>Builder prompt</h2>
<p>You are an agent in a voxel world, where the most northernly point is $0,0,-5$; the most westerly point $-5,0,0$; the most eastern point is $5,0,0$; the most southern $0,0,5$ and the $y$-axis is up and down, with $y=0$ being the minimum. Describe the coordinates of the blocks their colours (must be one of: blue, yellow, green, orange, purple, red) and whether the action is to add or remove them, your confidence in your interpretation of the instruction and optionally a question if the instruction is potentially unclear, in the JSON format: {"add": [[x,y,z,color], ...], "remove": [[x,y,z,color], ...], "confidence": 0.0, "question": "..."}. Give the JSON only, no additional dialog.</p>
<p>.1 Transcripts</p>
<h1>Transcript for Figure 1:</h1>
<p>Architect Place a stone on the ground
Builder [[0 0 0]] "blue" "putdown"]
Builder What colour should the block be and where specifically should I place it?
Architect red
Builder [[0 0 0]] "red" "putdown"]
Architect add a yellow on top of that one
Builder [[0 1 0] "yellow" "putdown"]</p>
<h2>Transcript for Figure 2:</h2>
<p>Architect from the most south west corner, place blocks going up diagonally into the center
Builder [[-5 0 5]] "blue" "putdown"],[[-4 1 4] "blue" "putdown"],[[-3 2 3] "blue" "putdown"],[[-2 3 2] "blue" "putdown"],[[-1 4 1] "blue" "putdown"],[[0 5 0] "blue" "putdown"]
Architect repeat this from the other corners in different colours</p>
<h2>Transcript for Figure 3:</h2>
<p>Builder hello architect
Architect Hello builder, today we will start by building a four block high green column. Please place a green block two steps to the west and three steps to the south from your current position. Then, stack another green block directly on top of it.
Builder two steps to the west from the center of the grid?
Architect Yes, that's correct. From the center of the grid move two steps to the west and three steps to the south. Then, place a green block. Stack another green block directly on top of the first one.</p>
<h2>Transcript for Figure 4:</h2>
<p>Builder hello architect
Architect Hello Builder. Let's get started. First, I'd like you to place a red block two steps north and one step to the west from the most southern point.
Builder correct?
Architect Not exactly. Try placing the red block three steps to the north and one to the west from the most southern point.
Builder correct?
Architect No, that isn't correct. Could you please move the red block just one block further to the north?
Builder correct?
Architect Not quite correct. You need to move the block one position north, so it's one step north and one step west from where you started. Additionally, lift it one level above the ground.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://gitlab.aicrowd.com/aicrowd/
challenges/iglu-challenge-2022/
iglu-2022-rl-mhb-baseline/-/blob/master/local_
nlp_evaluation.py&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
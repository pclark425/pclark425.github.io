<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8283 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8283</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8283</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-254877175</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.acl-long.817.pdf" target="_blank">A Survey of Deep Learning for Mathematical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8283.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8283.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that supplies or elicits intermediate natural-language reasoning steps (rationales) so large language models generate explicit multi-step reasoning chains before the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-002 / 175B) and other LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer language models (e.g., GPT-3 175B) used in few-shot or zero-shot in-context settings; models generate token-by-token with large parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (single human-annotated rationale prompt)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Provide one or a few examples where the input is followed by a detailed natural-language step-by-step rationale and the final answer; the model is expected to imitate that style and produce a reasoning chain then answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Standard few-shot CoT uses one (or a few) hand-crafted reasoning-chain demonstrations as in-context examples; early work relied on single human-annotated chains per example.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Various math benchmarks cited (e.g., MultiArith, GSM8K, MMLU-STEM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports few-shot-CoT GPT-3 (175B) achieves 93.0% on MultiArith (reported in survey). Minerva/PaLM variants also evaluated with CoT-style prompting for other benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Single human-provided CoT can substantially improve complex multi-step problem performance compared to direct answer prompting, but is vulnerable to incorrect single-path errors and to unstable selection of in-context exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT prompting enables LLMs to produce intermediate reasoning which improves multi-step problem solving, but reliance on single chain demonstrations is brittle and can lead to inconsistent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8283.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8283.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency for Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An outcome-based ensemble method that samples multiple reasoning paths (chains) from a model and aggregates answers (e.g., majority vote) to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002 / 175B) and GPT-family LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer language models used to stochastically sample multiple outputs (different reasoning chains) from a single prompt, then marginalize answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (sampling multiple reasoning chains)', 'ensemble/marginalization (majority voting over sampled answers)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Sample many chains by using temperature/random sampling from the same CoT prompt; collect final answers from each sampled chain and choose the most frequent answer as the prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparison between single best-chain CoT decoding and sampling multiple chains then marginalizing; presented as an outcome-based alternative to single-chain CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Math reasoning benchmarks (survey cites use on standard CoT tasks; exact reported task in originating paper is math/QA datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey lists Self-Consistency applied on Codex (175B); the originating work reports improved accuracy over single-chain CoT (survey does not list numeric delta here).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Sampling diverse reasoning paths helps correct for individual erroneous chains and improves overall answer accuracy and robustness; acts like an ensemble over reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Using multiple sampled reasoning paths and marginalizing (self-consistency) yields better and more robust performance than relying on a single decoded chain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8283.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8283.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-Most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A process-based prompting method that decomposes a complex problem into a sequence of subproblems and solves them sequentially, using earlier subproblem answers to solve later ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (175B) and similar LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LMs prompted in a staged manner: first to decompose problem into subquestions, then to answer subquestions sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['problem decomposition (process-based chain-of-thought variant)', 'sequential subproblem solving']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompting procedure first asks model to produce subproblems (decomposition), then in subsequent prompts solves each subproblem using prior answers, implementing a staged reasoning process.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Contrasts process-based multi-step prompting (decompose-and-solve) with single-shot CoT; emphasizes structured multi-stage prompting rather than one long chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Complex multi-step reasoning tasks (survey references application to math/problem solving datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey notes Least-to-Most is used with Codex (175B); originating work reports that decomposition-based prompting improves performance on compositional tasks (survey does not reproduce numeric results).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Decomposition into subproblems makes complex reasoning more tractable and can reduce propagation of error by handling smaller steps; applicable when problem naturally decomposes.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Process-based decomposition (least-to-most) can enable LLMs to handle more complex, compositional reasoning than single-shot CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8283.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8283.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program-of-Thoughts (PoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-of-Thoughts Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach where the model expresses reasoning as executable code/programs (rather than only natural language), delegating computation to an external executor to increase correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / code-capable LLMs (175B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LMs prompted to generate programs (e.g., Python) that encode the reasoning steps; program is executed to produce final answer, separating symbolic computation from language reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['programmatic reasoning (tool-use / external execution)', 'chain-of-thought expressed as code']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt examples show structured programs that implement the reasoning plan; the model is evaluated by executing generated code to obtain exact numeric answers, avoiding arithmetic errors from free-text reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared programmatic reasoning (PoT) to natural-language CoT and code-based few-shot prompting; explores disentangling computation from reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Numerical reasoning and math QA tasks (survey cites PoT for numerical reasoning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey lists PoT (Few-shot-PoT on GPT-3 175B); originating work reports accuracy improvements on numerical reasoning tasks versus standard CoT (survey does not provide exact numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Using executable programs reduces arithmetic and procedural mistakes by delegating calculation to a deterministic executor; improves reliability for computation-heavy steps.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Disentangling computation via program generation leads to more accurate numerical reasoning than relying solely on natural language chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8283.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8283.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-/Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented and Automatic Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that automatically construct or retrieve effective in-context CoT exemplars (rather than hand-crafting them), using retrieval or clustering to produce diverse or relevant prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chain of thought prompting in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / Codex (175B) as used in cited works</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs augmented with retrieval modules that fetch semantically similar examples or clustering-based pipelines that auto-generate CoT exemplars to include in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['in-context example retrieval', 'automated exemplar generation (clustering / auto-CoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Retrieve semantically similar in-context examples (via embedding similarity) or automatically cluster examples to build exemplar CoT demonstrations for prompting; Auto-CoT uses clustering to create exemplars without human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Comparisons include random/heuristic example selection vs. retrieval-based selection vs. automatically generated CoT exemplars; used to study impact of exemplar choice and diversity on performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Math/problem-solving benchmarks (survey references retrieval-CoT and Auto-CoT applied to GPT-3/Codex on math tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey notes retrieval and Auto-CoT methods (Zhang et al., 2023) improve prompt quality and downstream accuracy compared to random exemplar selection; exact numbers not reproduced in survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Choosing semantically similar or well-clustered CoT exemplars reduces instability in few-shot performance; automatic generation reduces human annotation cost and can produce diverse demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Optimizing selection or automatic generation of CoT exemplars (rather than random selection) improves few-shot reasoning accuracy and reduces variance across example choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8283.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8283.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complexity-based Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complexity-Based Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategy that selects in-context demonstrations with more complex/longer reasoning chains, hypothesizing that complexity-aligned examples better teach chain structure for difficult problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Complexity-based prompting for multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large conditional language models where in-context examples are chosen by a heuristic measuring reasoning-chain complexity (e.g., number of steps).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['example selection by chain complexity', 'process-based prompting emphasis']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Pick in-context exemplars that contain longer/more complex reasoning chains to better scaffold complex test queries; used as an alternative example-selection heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared complexity-selected exemplars versus random or other selection strategies to measure downstream gains on multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multi-step reasoning/math tasks (survey cites Fu et al., 2023 application)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports complexity-based prompting (Fu et al., 2023) as a method applied to GPT-3 (175B) and shows performance improvements over random selection in the originating paper (survey does not list exact numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Selecting exemplars with higher reasoning complexity can improve LLM capability on hard multi-step problems; indicates exemplar-complexity alignment matters.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Complexity-aware selection of demonstrations improves multi-step reasoning over naive exemplar choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8283.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8283.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Teaching / Diverse Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Teaching (diverse prompting via self-generated prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to introduce diverse prompts by letting models generate alternative prompts/solutions ('self-teaching') to increase diversity of reasoning exemplars and improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the advance of making language models better reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / Codex (175B) (as discussed in survey citing Li et al. 2022a)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LMs iteratively generate new prompts or demonstrations (self-generated exemplars) to be used as additional in-context examples, increasing variability in reasoning patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['self-generated diverse demonstrations', 'ensemble-style prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Use the model itself to produce alternative reasoning exemplars (diverse prompts) which are then used in-context to prompt the model for better/frequent reasoning patterns; complements sampling-based diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared few-shot with manually curated exemplars to pipelines that augment exemplars via model-generated variations (self-teaching) to increase exemplar diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Math and reasoning tasks (survey references Li et al. 2022a using self-teaching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey states introducing diverse prompts via self-teaching can produce higher diversity and improve performance; specific numeric gains are in the cited work rather than reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Model-generated diverse exemplars can complement sampling-based multiple-path approaches and reduce reliance on single human-crafted chains; increasing exemplar diversity can stabilize few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Diverse in-context exemplars generated via self-teaching help create more robust reasoning behavior than relying solely on a small set of hand-crafted exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8283.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8283.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program-Aided (PAL / PaL / PAL-like)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-Aided Language Models (PAL / Program-aided approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that have models produce intermediate programs or use external executors/tools (calculator, Python) to improve compositional and numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pal: Program-aided language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Program-capable LLMs (e.g., code-aware GPT/Codex / PaLM variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs prompted to output short programs implementing the reasoning (e.g., PAL, PoT, PAL variants); code is executed to obtain exact outputs and reduce free-text arithmetic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['tool use / external execution', 'programmatic chain-of-thought']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt the model to output code performing the reasoning; execute code externally to get deterministic results; often combined with decomposition or CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared natural-language-only CoT versus programmatic approaches (PoT / PAL) and hybrid schemes where programs are used to carry out computation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Numerical reasoning and math QA datasets (survey references PAL / programmatic prompting literature)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey states program-aided approaches reduce arithmetic mistakes and improve reliability on computation-heavy benchmarks; numeric specifics are reported in the original papers rather than reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Delegating computation to executors increases correctness and can be combined with diverse prompting or decomposition strategies for best effect.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Programmatic/tool-augmented reasoning is an effective way to improve numerical accuracy and robustness compared to pure natural-language reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8283.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8283.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Outcome-based vs Process-based taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Outcome-based (ensemble) vs Process-based (hand-crafted decomposition) approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Taxonomy distinguishing (i) process-based methods that hand-craft or design richer multi-step demonstrations (decomposition) and (ii) outcome-based methods that aggregate multiple reasoning paths (ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Survey-level discussion across many LLMs (GPT-3, Codex, PaLM/Minerva)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-level categorization rather than a single model: process-based = structured prompts (least-to-most, PoT), outcome-based = sampling + marginalization (self-consistency, ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['process-based prompting (complex hand-crafted demonstrations, decomposition)', 'outcome-based prompting (multiple sampled reasoning paths and aggregation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Process-based: design richer stepwise demonstrations or decomposition (least-to-most, PoT); Outcome-based: produce many candidate reasoning traces via sampling and choose consensus answer (self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey contrasts the two families conceptually and cites specific works as representatives (Zhou et al., 2023; Wang et al., 2023; Chen et al., 2022b).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General math and multi-step reasoning benchmarks (various datasets cited across survey)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports that both families yield benefits: process-based helps on complex compositional tasks; outcome-based improves robustness by marginalizing over paths; numerical trade-offs are reported in originating works.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Process-based approaches reduce difficulty by structured decomposition; outcome-based approaches reduce sensitivity to single-path errors  together they represent complementary ways to increase effective reasoning diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Diverse reasoning (either via process decomposition or via multiple sampled outcomes) improves robustness and accuracy compared to single, similar reasoning chains; methods are complementary and combined approaches (e.g., program + sampling) can be advantageous.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Deep Learning for Mathematical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models. <em>(Rating: 2)</em></li>
                <li>Complexity-based prompting for multi-step reasoning. <em>(Rating: 2)</em></li>
                <li>On the advance of making language models better reasoners. <em>(Rating: 2)</em></li>
                <li>Pal: Program-aided language models. <em>(Rating: 2)</em></li>
                <li>Learning to retrieve prompts for in-context learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8283",
    "paper_id": "paper-254877175",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting method that supplies or elicits intermediate natural-language reasoning steps (rationales) so large language models generate explicit multi-step reasoning chains before the final answer.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (text-davinci-002 / 175B) and other LLMs",
            "model_description": "Large autoregressive transformer language models (e.g., GPT-3 175B) used in few-shot or zero-shot in-context settings; models generate token-by-token with large parameter counts.",
            "reasoning_methods": [
                "chain-of-thought (single human-annotated rationale prompt)"
            ],
            "reasoning_methods_description": "Provide one or a few examples where the input is followed by a detailed natural-language step-by-step rationale and the final answer; the model is expected to imitate that style and produce a reasoning chain then answer.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Standard few-shot CoT uses one (or a few) hand-crafted reasoning-chain demonstrations as in-context examples; early work relied on single human-annotated chains per example.",
            "task_or_benchmark": "Various math benchmarks cited (e.g., MultiArith, GSM8K, MMLU-STEM)",
            "performance_results": "Survey reports few-shot-CoT GPT-3 (175B) achieves 93.0% on MultiArith (reported in survey). Minerva/PaLM variants also evaluated with CoT-style prompting for other benchmarks.",
            "qualitative_findings": "Single human-provided CoT can substantially improve complex multi-step problem performance compared to direct answer prompting, but is vulnerable to incorrect single-path errors and to unstable selection of in-context exemplars.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CoT prompting enables LLMs to produce intermediate reasoning which improves multi-step problem solving, but reliance on single chain demonstrations is brittle and can lead to inconsistent outputs.",
            "uuid": "e8283.0",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency for Chain-of-Thought",
            "brief_description": "An outcome-based ensemble method that samples multiple reasoning paths (chains) from a model and aggregates answers (e.g., majority vote) to improve robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "mention",
            "model_name": "Codex (code-davinci-002 / 175B) and GPT-family LLMs",
            "model_description": "Large transformer language models used to stochastically sample multiple outputs (different reasoning chains) from a single prompt, then marginalize answers.",
            "reasoning_methods": [
                "chain-of-thought (sampling multiple reasoning chains)",
                "ensemble/marginalization (majority voting over sampled answers)"
            ],
            "reasoning_methods_description": "Sample many chains by using temperature/random sampling from the same CoT prompt; collect final answers from each sampled chain and choose the most frequent answer as the prediction.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Direct comparison between single best-chain CoT decoding and sampling multiple chains then marginalizing; presented as an outcome-based alternative to single-chain CoT.",
            "task_or_benchmark": "Math reasoning benchmarks (survey cites use on standard CoT tasks; exact reported task in originating paper is math/QA datasets)",
            "performance_results": "Survey lists Self-Consistency applied on Codex (175B); the originating work reports improved accuracy over single-chain CoT (survey does not list numeric delta here).",
            "qualitative_findings": "Sampling diverse reasoning paths helps correct for individual erroneous chains and improves overall answer accuracy and robustness; acts like an ensemble over reasoning traces.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Using multiple sampled reasoning paths and marginalizing (self-consistency) yields better and more robust performance than relying on a single decoded chain.",
            "uuid": "e8283.1",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Least-to-Most",
            "name_full": "Least-to-Most Prompting",
            "brief_description": "A process-based prompting method that decomposes a complex problem into a sequence of subproblems and solves them sequentially, using earlier subproblem answers to solve later ones.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "Codex (175B) and similar LLMs",
            "model_description": "Large transformer LMs prompted in a staged manner: first to decompose problem into subquestions, then to answer subquestions sequentially.",
            "reasoning_methods": [
                "problem decomposition (process-based chain-of-thought variant)",
                "sequential subproblem solving"
            ],
            "reasoning_methods_description": "Prompting procedure first asks model to produce subproblems (decomposition), then in subsequent prompts solves each subproblem using prior answers, implementing a staged reasoning process.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Contrasts process-based multi-step prompting (decompose-and-solve) with single-shot CoT; emphasizes structured multi-stage prompting rather than one long chain.",
            "task_or_benchmark": "Complex multi-step reasoning tasks (survey references application to math/problem solving datasets)",
            "performance_results": "Survey notes Least-to-Most is used with Codex (175B); originating work reports that decomposition-based prompting improves performance on compositional tasks (survey does not reproduce numeric results).",
            "qualitative_findings": "Decomposition into subproblems makes complex reasoning more tractable and can reduce propagation of error by handling smaller steps; applicable when problem naturally decomposes.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Process-based decomposition (least-to-most) can enable LLMs to handle more complex, compositional reasoning than single-shot CoT.",
            "uuid": "e8283.2",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Program-of-Thoughts (PoT)",
            "name_full": "Program-of-Thoughts Prompting",
            "brief_description": "A prompting approach where the model expresses reasoning as executable code/programs (rather than only natural language), delegating computation to an external executor to increase correctness.",
            "citation_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "mention_or_use": "mention",
            "model_name": "GPT-3 / code-capable LLMs (175B variants)",
            "model_description": "Large LMs prompted to generate programs (e.g., Python) that encode the reasoning steps; program is executed to produce final answer, separating symbolic computation from language reasoning.",
            "reasoning_methods": [
                "programmatic reasoning (tool-use / external execution)",
                "chain-of-thought expressed as code"
            ],
            "reasoning_methods_description": "Prompt examples show structured programs that implement the reasoning plan; the model is evaluated by executing generated code to obtain exact numeric answers, avoiding arithmetic errors from free-text reasoning.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared programmatic reasoning (PoT) to natural-language CoT and code-based few-shot prompting; explores disentangling computation from reasoning.",
            "task_or_benchmark": "Numerical reasoning and math QA tasks (survey cites PoT for numerical reasoning benchmarks)",
            "performance_results": "Survey lists PoT (Few-shot-PoT on GPT-3 175B); originating work reports accuracy improvements on numerical reasoning tasks versus standard CoT (survey does not provide exact numbers).",
            "qualitative_findings": "Using executable programs reduces arithmetic and procedural mistakes by delegating calculation to a deterministic executor; improves reliability for computation-heavy steps.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Disentangling computation via program generation leads to more accurate numerical reasoning than relying solely on natural language chain-of-thought.",
            "uuid": "e8283.3",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Retrieval-/Auto-CoT",
            "name_full": "Retrieval-Augmented and Automatic Chain-of-Thought Prompting",
            "brief_description": "Methods that automatically construct or retrieve effective in-context CoT exemplars (rather than hand-crafting them), using retrieval or clustering to produce diverse or relevant prompts.",
            "citation_title": "Automatic chain of thought prompting in large language models.",
            "mention_or_use": "mention",
            "model_name": "GPT-3 / Codex (175B) as used in cited works",
            "model_description": "LLMs augmented with retrieval modules that fetch semantically similar examples or clustering-based pipelines that auto-generate CoT exemplars to include in the prompt.",
            "reasoning_methods": [
                "in-context example retrieval",
                "automated exemplar generation (clustering / auto-CoT)"
            ],
            "reasoning_methods_description": "Retrieve semantically similar in-context examples (via embedding similarity) or automatically cluster examples to build exemplar CoT demonstrations for prompting; Auto-CoT uses clustering to create exemplars without human annotation.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Comparisons include random/heuristic example selection vs. retrieval-based selection vs. automatically generated CoT exemplars; used to study impact of exemplar choice and diversity on performance.",
            "task_or_benchmark": "Math/problem-solving benchmarks (survey references retrieval-CoT and Auto-CoT applied to GPT-3/Codex on math tasks)",
            "performance_results": "Survey notes retrieval and Auto-CoT methods (Zhang et al., 2023) improve prompt quality and downstream accuracy compared to random exemplar selection; exact numbers not reproduced in survey text.",
            "qualitative_findings": "Choosing semantically similar or well-clustered CoT exemplars reduces instability in few-shot performance; automatic generation reduces human annotation cost and can produce diverse demonstrations.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Optimizing selection or automatic generation of CoT exemplars (rather than random selection) improves few-shot reasoning accuracy and reduces variance across example choices.",
            "uuid": "e8283.4",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Complexity-based Prompting",
            "name_full": "Complexity-Based Prompting",
            "brief_description": "A strategy that selects in-context demonstrations with more complex/longer reasoning chains, hypothesizing that complexity-aligned examples better teach chain structure for difficult problems.",
            "citation_title": "Complexity-based prompting for multi-step reasoning.",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (175B)",
            "model_description": "Large conditional language models where in-context examples are chosen by a heuristic measuring reasoning-chain complexity (e.g., number of steps).",
            "reasoning_methods": [
                "example selection by chain complexity",
                "process-based prompting emphasis"
            ],
            "reasoning_methods_description": "Pick in-context exemplars that contain longer/more complex reasoning chains to better scaffold complex test queries; used as an alternative example-selection heuristic.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared complexity-selected exemplars versus random or other selection strategies to measure downstream gains on multi-step tasks.",
            "task_or_benchmark": "Multi-step reasoning/math tasks (survey cites Fu et al., 2023 application)",
            "performance_results": "Survey reports complexity-based prompting (Fu et al., 2023) as a method applied to GPT-3 (175B) and shows performance improvements over random selection in the originating paper (survey does not list exact numbers).",
            "qualitative_findings": "Selecting exemplars with higher reasoning complexity can improve LLM capability on hard multi-step problems; indicates exemplar-complexity alignment matters.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Complexity-aware selection of demonstrations improves multi-step reasoning over naive exemplar choices.",
            "uuid": "e8283.5",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Self-Teaching / Diverse Prompts",
            "name_full": "Self-Teaching (diverse prompting via self-generated prompts)",
            "brief_description": "A technique to introduce diverse prompts by letting models generate alternative prompts/solutions ('self-teaching') to increase diversity of reasoning exemplars and improve performance.",
            "citation_title": "On the advance of making language models better reasoners.",
            "mention_or_use": "mention",
            "model_name": "GPT-3 / Codex (175B) (as discussed in survey citing Li et al. 2022a)",
            "model_description": "Large LMs iteratively generate new prompts or demonstrations (self-generated exemplars) to be used as additional in-context examples, increasing variability in reasoning patterns.",
            "reasoning_methods": [
                "self-generated diverse demonstrations",
                "ensemble-style prompting"
            ],
            "reasoning_methods_description": "Use the model itself to produce alternative reasoning exemplars (diverse prompts) which are then used in-context to prompt the model for better/frequent reasoning patterns; complements sampling-based diversity.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared few-shot with manually curated exemplars to pipelines that augment exemplars via model-generated variations (self-teaching) to increase exemplar diversity.",
            "task_or_benchmark": "Math and reasoning tasks (survey references Li et al. 2022a using self-teaching)",
            "performance_results": "Survey states introducing diverse prompts via self-teaching can produce higher diversity and improve performance; specific numeric gains are in the cited work rather than reproduced here.",
            "qualitative_findings": "Model-generated diverse exemplars can complement sampling-based multiple-path approaches and reduce reliance on single human-crafted chains; increasing exemplar diversity can stabilize few-shot performance.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Diverse in-context exemplars generated via self-teaching help create more robust reasoning behavior than relying solely on a small set of hand-crafted exemplars.",
            "uuid": "e8283.6",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Program-Aided (PAL / PaL / PAL-like)",
            "name_full": "Program-Aided Language Models (PAL / Program-aided approaches)",
            "brief_description": "Methods that have models produce intermediate programs or use external executors/tools (calculator, Python) to improve compositional and numerical reasoning.",
            "citation_title": "Pal: Program-aided language models.",
            "mention_or_use": "mention",
            "model_name": "Program-capable LLMs (e.g., code-aware GPT/Codex / PaLM variants)",
            "model_description": "LLMs prompted to output short programs implementing the reasoning (e.g., PAL, PoT, PAL variants); code is executed to obtain exact outputs and reduce free-text arithmetic errors.",
            "reasoning_methods": [
                "tool use / external execution",
                "programmatic chain-of-thought"
            ],
            "reasoning_methods_description": "Prompt the model to output code performing the reasoning; execute code externally to get deterministic results; often combined with decomposition or CoT prompting.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared natural-language-only CoT versus programmatic approaches (PoT / PAL) and hybrid schemes where programs are used to carry out computation.",
            "task_or_benchmark": "Numerical reasoning and math QA datasets (survey references PAL / programmatic prompting literature)",
            "performance_results": "Survey states program-aided approaches reduce arithmetic mistakes and improve reliability on computation-heavy benchmarks; numeric specifics are reported in the original papers rather than reproduced here.",
            "qualitative_findings": "Delegating computation to executors increases correctness and can be combined with diverse prompting or decomposition strategies for best effect.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Programmatic/tool-augmented reasoning is an effective way to improve numerical accuracy and robustness compared to pure natural-language reasoning chains.",
            "uuid": "e8283.7",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Outcome-based vs Process-based taxonomy",
            "name_full": "Outcome-based (ensemble) vs Process-based (hand-crafted decomposition) approaches",
            "brief_description": "Taxonomy distinguishing (i) process-based methods that hand-craft or design richer multi-step demonstrations (decomposition) and (ii) outcome-based methods that aggregate multiple reasoning paths (ensembling).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Survey-level discussion across many LLMs (GPT-3, Codex, PaLM/Minerva)",
            "model_description": "Survey-level categorization rather than a single model: process-based = structured prompts (least-to-most, PoT), outcome-based = sampling + marginalization (self-consistency, ensembling).",
            "reasoning_methods": [
                "process-based prompting (complex hand-crafted demonstrations, decomposition)",
                "outcome-based prompting (multiple sampled reasoning paths and aggregation)"
            ],
            "reasoning_methods_description": "Process-based: design richer stepwise demonstrations or decomposition (least-to-most, PoT); Outcome-based: produce many candidate reasoning traces via sampling and choose consensus answer (self-consistency).",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey contrasts the two families conceptually and cites specific works as representatives (Zhou et al., 2023; Wang et al., 2023; Chen et al., 2022b).",
            "task_or_benchmark": "General math and multi-step reasoning benchmarks (various datasets cited across survey)",
            "performance_results": "Survey reports that both families yield benefits: process-based helps on complex compositional tasks; outcome-based improves robustness by marginalizing over paths; numerical trade-offs are reported in originating works.",
            "qualitative_findings": "Process-based approaches reduce difficulty by structured decomposition; outcome-based approaches reduce sensitivity to single-path errors  together they represent complementary ways to increase effective reasoning diversity.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Diverse reasoning (either via process decomposition or via multiple sampled outcomes) improves robustness and accuracy compared to single, similar reasoning chains; methods are complementary and combined approaches (e.g., program + sampling) can be advantageous.",
            "uuid": "e8283.8",
            "source_info": {
                "paper_title": "A Survey of Deep Learning for Mathematical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models.",
            "rating": 2,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Complexity-based prompting for multi-step reasoning.",
            "rating": 2,
            "sanitized_title": "complexitybased_prompting_for_multistep_reasoning"
        },
        {
            "paper_title": "On the advance of making language models better reasoners.",
            "rating": 2,
            "sanitized_title": "on_the_advance_of_making_language_models_better_reasoners"
        },
        {
            "paper_title": "Pal: Program-aided language models.",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Learning to retrieve prompts for in-context learning.",
            "rating": 1,
            "sanitized_title": "learning_to_retrieve_prompts_for_incontext_learning"
        }
    ],
    "cost": 0.020647,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Deep Learning for Mathematical Reasoning
Long PapersCopyright Long PapersJuly 9-14, 2023</p>
<p>Pan Lu 
UCLA</p>
<p>Liang Qiu 
UCLA</p>
<p>Wenhao Yu 
University of Notre Dame</p>
<p>Sean Welleck 
University of Washington</p>
<p>Kai-Wei Chang 
UCLA</p>
<p>A Survey of Deep Learning for Mathematical Reasoning</p>
<p>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
the 61st Annual Meeting of the Association for Computational LinguisticsLong Papers1July 9-14, 2023
Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.</p>
<p>Introduction</p>
<p>"The study of mathematics, like the Nile, begins in minuteness but ends in magnificence." -Charles Caleb Colton, English writer Mathematical reasoning is a key aspect of human intelligence that enables us to comprehend and make decisions based on numerical data and language. It is applicable in various fields, including science, engineering, finance, and everyday life, and encompasses a range of abilities, from basic skills such as pattern recognition and numerical operations to more advanced skills like problemsolving, logical reasoning, and abstract thinking. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has been a long-standing focus of research in the fields of machine learning and * denotes co-senior authors. natural language processing (NLP), dating back to the 1960s (Feigenbaum et al., 1963;Bobrow, 1964). In recent years, there has been a surge of interest in this area: for instance, the number of papers has grown from approximately 10 in 2018 to 66 in 2022 (see Figure 3 in the Appendix).</p>
<p>As deep learning continues to revolutionize NLP tasks such as question answering and machine translation (Sutskever et al., 2014;Devlin et al., 2019), it has also made significant strides in the field of mathematical reasoning (Wang et al., 2017;Yang and Deng, 2019;Geva et al., 2020;Wei et al., 2022). However, despite the impressive capabilities of these models, there is still a lack of a clear taxonomy of the different types of mathematical reasoning tasks and the specific capabilities required of deep learning models to solve them.</p>
<p>Previous literature has been limited to the discussion of specific aspects, such as solving math word problems (Bhattacharya, 2017;Zhang et al., 2019;Ughade and Kumbhar, 2019), representing numbers representation (Thawani et al., 2021), or solving informal problems (Meadows and Freitas, 2022). Additionally, with the recent advancements in large language models like GPT-3 (Brown et al., 2020), there is a growing need to understand the capabilities and limitations of these models in the context of mathematical reasoning. This is where a comprehensive survey of this rapidly advancing domain becomes crucial, as it can provide an overview of the current state and limitations of the field, and indicate further research areas.</p>
<p>In this paper, we survey over 180 papers from the NLP and AI communities in the field of deep learning for mathematical reasoning. We study various types of mathematical reasoning problems, such as math word problems, theorem proving, geometry problem solving, math question answering, and other quantitative problems ( 2, A). Additionally, we explore different deep learning architectures for mathematical reasoning, including neural networks ( 3), pre-trained language models ( 4), and recent in-context learning for large language models ( 5).</p>
<p>We also analyze existing benchmarks and find that there is less focus on multi-modal and lowresource settings ( 6.1). Our evidence-based studies suggest that current numeracy representations are insufficient and deep learning methods are inconsistent for mathematical reasoning ( 6.2). Following this, we suggest future research directions related to generalization and robustness, trustworthy reasoning, learning from feedback, and multimodal mathematical reasoning ( 7).</p>
<p>Mathematical Reasoning Tasks</p>
<p>In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?</p>
<p>Rationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5</p>
<p>Answer: (B) 6.5 Figure 2: An example of geometry problems.</p>
<p>Math Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as "Which kicker kicked the most field goals?" over the content of paragraphs.</p>
<p>Neural Networks for Mathematical Reasoning</p>
<p>Neural networks have become a popular tool in the field of mathematical reasoning, mirroring their success in NLP. In recent years, a number of different neural network architectures have been proposed for mathematical reasoning tasks, including Seq2Seq-based networks, graph-based networks, and attention-based networks. These methods are outlined in more detail in Table 8 in the Appendix.</p>
<p>Seq2Seq-based Networks for Math</p>
<p>Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).</p>
<p>Graph-based Networks for Math</p>
<p>Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).</p>
<p>Attention-based Networks for Math</p>
<p>The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).</p>
<p>Other Neural Networks for Math</p>
<p>Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.</p>
<p>Other deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.</p>
<p>Pre-trained Language Models for Mathematical Reasoning</p>
<p>Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.</p>
<p>Self-Supervised Learning for Math</p>
<p>Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: "all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.</p>
<p>Task-specific Fine-tuning for Math</p>
<p>Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).</p>
<p>In-context Learning for Mathematical Reasoning</p>
<p>Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).</p>
<p>Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.</p>
<p>Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the "Let's think step by step!" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.</p>
<p>In-context Example Selection</p>
<p>Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. </p>
<p>High-quality Reasoning Chains</p>
<p>Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).</p>
<p>Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.</p>
<p>Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through "selfteaching", as a complementary solution to produce a higher degree of diversity.</p>
<p>6 Discussion and Findings</p>
<p>Analysis of Benchmarks</p>
<p>The multi-modal setting is underexplored but is gaining increasing attention.  </p>
<p>Analysis of Deep Learning Methods</p>
<p>Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an "UNK" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as "15" and "98" in GPT-3, while another format like 1, 598 is split as three different tokens: "1", ",", and "598". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but</p>
<p>Problems</p>
<p>GPT-3 (text-davinci-002)</p>
<p>John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.</p>
<p>John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?</p>
<p>Mary has 5 balls.</p>
<p>John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.</p>
<p>John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.</p>
<p>John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.</p>
<p>John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.</p>
<p>Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an "F" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work</p>
<p>Generalization and Robustness</p>
<p>Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.</p>
<p>Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.</p>
<p>Trustworthy Reasoning</p>
<p>Recent advances in language models have demonstrated their powerful capabilities for mathematical reasoning. However, due to the potential for generating ungrounded answers (Nakano et al., 2021), users can't always trust the predicted outcomes or have to verify then with extra efforts. Even with recent prompting strategies that provide rationales before making predictions (Wei et al., 2022), language models can still hallucinate statements, produce flawed reasoning, and output wrong answers. Consequently, novel approaches that enable more reliable reasoning are needed urgently. Some potential directions for this include: (i) using language models to provide evidence, such as theorems, to support the reasoning process; (ii) incorporating a mechanism that makes a judgment when the model is unsure of the answer; and (iii) using a model itself or another module to detect and locate mistakes in a model's reasoning.</p>
<p>Learning from Feedback</p>
<p>Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).</p>
<p>Multi-modal Mathematical Reasoning</p>
<p>In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.</p>
<p>Conclusion</p>
<p>In this paper, we present a comprehensive survey of deep learning for mathematical reasoning. We review the various tasks, datasets, and deep learning approaches. We also identify several gaps in the existing datasets and methods. Finally, we outline directions for future research and highlight the potential for further exploration in this field. Our goal with this paper is to provide a comprehensive and useful resource for readers interested in the development of deep learning for mathematical reasoning. To aid in this effort, we have created a reading list that will be continually updated in a GitHub repository at https://github.com/lupantech/dl4math.</p>
<p>Limitations</p>
<p>One limitation of our survey work is that it is focused on the intersection of mathematical reasoning and deep learning over the past decade, which may not encompass the entire field and its history. Additionally, our evaluation of existing benchmarks and methods is based on a curated set of papers and may not fully represent the state of the art in the field. Furthermore, due to the fast-paced nature of the field, our survey may not reflect the latest developments and advancements which may have come out close to or after the survey was conducted. Despite these limitations, our survey still provides a valuable overview of the current state and key trends in the field of mathematical reasoning and deep learning, and can serve as a valuable resource for researchers and practitioners working in this field.</p>
<p>Broader Impact</p>
<p>Our survey paper on the intersection of mathematical reasoning and deep learning has the potential to significantly impact the field of artificial intelligence. By providing a comprehensive overview of the key tasks, datasets, and methods that have been developed in the past decade, we give researchers and practitioners a clear understanding of the current state-of-the-art and help them make informed decisions about their own research. Additionally, by evaluating existing benchmarks and methods and discussing future research directions, we aim to identify gaps in the current state of the art and guide future research and development efforts towards more advanced and effective mathematical reasoning systems. Overall, our survey has the potential to contribute to the advancement of mathematical reasoning and deep learning, and have a profound impact on machine learning and natural language processing.  </p>
<p>References</p>
<p>A Mathematical Reasoning Datasets</p>
<p>In this section, we will examine the various datasets currently available for the study of mathematical reasoning using deep learning methods. A summary of the commonly used datasets in this field can be found in Table 7.</p>
<p>A.1 Math Word Problem Solving</p>
<p>Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.</p>
<p>Existing MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.</p>
<p>Most MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.</p>
<p>A.2 Theorem Proving</p>
<p>Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating "proof steps" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.</p>
<p>Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.</p>
<p>Other resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).</p>
<p>Informal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in "standard" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.</p>
<p>An emerging area of research aims to combine elements of informal and formal theorem proving. </p>
<p>A.3 Geometry Problem Solving</p>
<p>Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.</p>
<p>Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.</p>
<p>A.4 Math Question Answering</p>
<p>Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks. Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning. In this work, we refer to these tasks as math question answering (MathQA). A large number of datasets have been presented recently. Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve "satisfactory" performance (Hendrycks et al., 2021b;Mishra et al., 2022b). To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021b) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.</p>
<p>Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning (Mishra et al., 2022b,a;Chen et al., 2023). NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.</p>
<p>A.5 Other Quantitative Problems</p>
<p>Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  </p>
<p>Figure 1 :
1Taxonomy of deep learning for mathematical reasoning. The associated tasks are elaborated in 2, with a comprehensive dataset list found in A. Deep learning methods are further discussed in 3, 4, and 5.</p>
<p>In triangle ABC, AD = 3 and BD = 14. Find CD.</p>
<p>For example, MWP-BERT (Liang et al., 2022b) uses a backbone of BERT (110M) (Devlin et al., 2019) and RoBERTa (123M) (Liu et al., 2019b) for Math Word Problems. Most recently, Minerva (Lewkowycz et al., 2022), which is based on the PaLM (Chowdhery et al., 2022) pre-trained</p>
<p>Representative work includes TaPEX (Liu et al., 2022b), which obtains a pre-training corpus by automatically synthesizing executable SQL queries and their execution outputs. LISA (Jiang et al., 2021) extracts lemmas and theorems by interacting with the Isabelle standard library and the Archive of Formal Proofs. GenBERT (Geva et al., 2020) generates numerical and textual pre-training datasets based on manually crafted and extracted templates.Pre-training tasks. General pre-training language models have two typical self-supervised learning tasks: (i) Masked Language Modeling (MLM), where it randomly masks a portion of words in each sequence to predict the outcome; (ii) Causal Lan-</p>
<p>(2022) attempt to address this issue by retrieving semantically similar examples. In addition, Fu et al. (2023) propose complexity-based prompting, which chooses examples with complex reasoning chains, i.e., chains with more reasoning steps, as the prompt. PromptPG (Lu et al., 2022b) learns to select optimal in-context examples via reinforcement learning (RL) from a candidate pool.</p>
<p>, Khot et al. (2022) leverage diverse decomposition structures and use different prompts to answer each sub-question. Apart from these multi-step reasoning methods, Chen et al. (2022b); Gao et al. (2022) propose programof-thoughts (PoT)</p>
<p>Figure 3 :
3Estimated counts of annually published papers on deep learning for mathematical reasoning. This field has been experiencing rapid growth since 2018.</p>
<p>; Han et al. (2022); Polu et al. (2023); Jiang et al. (2022b,a); Lample et al. (2022)). Example ITPs include Lean (Moura et al., 2015), Isabelle (Paulson, 1994), Coq (Barras et al., 1999), and Metamath (Megill and Wheeler, 2019).</p>
<p>For example, Wu et al. (2022b) explore translating informal statements into formal statements, while Jiang et al. (2022a) release a new version of the miniF2F benchmark augmented with informal statements and proofs, which we refer to as miniF2F+informal. Jiang et al. (2022a) explore translating provided (or generated) informal proofs into formal proofs.</p>
<p>For example, QuaRel (Tafjord et al., 2019) is a dataset of diverse story questions that involve 19 different types of quantities. McTaco (Zhou et al., 2019) studies temporal commonsense problems, while Fermi (Kalyan et al., 2021) studies Fermi problems whose answers can only be approximately estimated.</p>
<p>Table 1 :
1A typical math word problem.</p>
<p>Table 2 :
2Comparison of pre-training language models for mathematical reasoning.language model, has a size up to 540B parameters. </p>
<p>Pre-training corpus. There are generally two 
types of pre-training corpus for mathematical lan-
guage models. (i) Curated datasets from openly 
accessible sources. For example, Hendrycks et al. 
(2021b) present the first large-scale mathematics 
pre-training dataset with step-by-step solutions 
in natural language and L A T E X, called the Auxil-
iary Mathematics Problems and Solutions (AMPS). 
AMPS consists of Khan Academy and Mathemat-
ica data. Minerva (Lewkowycz et al., 2022) col-
lects a high-quality dataset containing scientific and 
mathematical data, which contains 38.5B tokens 
from webpages filtered for mathematical content 
and from papers submitted to the arXiv preprint 
server. Thor (Jiang et al., 2022b) pre-trains a lan-
guage model on the GitHub + arXiv subsets of 
The Pile (Gao et al., 2020). (ii) Synthetic datasets 
based on templates or interaction with engines. Re-
cent work </p>
<p>Table 3 :
3Finetuned pre-trained language models for downstream mathematical reasoning tasks.</p>
<p>Table 4 :
4In-context learning with large language models for mathematical reasoning. For GPT-3, all papers use the 
text-davinci-002 version; for Codex, all papers use the code-davinci-002. RL is short for reinforcement learning. </p>
<p>Table 5 :
5Language models struggle with large numbers.2019; Lu et al., 2021a), programs (Amini et al., 
2019; Chen et al., 2021c,a; Cao and Xiao, 2022; 
Chen et al., 2022a), and reasoning graphs (Zhang 
et al., 2021) have been proposed to train models 
for complex reasoning tasks. Python programs 
are used as reasoning annotations in (Austin et al., 
2021; Mishra et al., 2022a) due to their enhanced 
accessibility and readability. To imitate the rea-
soning process of a human, a more recent trend 
is to annotate solutions in natural language (Ling 
et al., 2017; Cobbe et al., 2021; Lu et al., 2022b; 
Hendrycks et al., 2021b; Lu et al., 2022a). </p>
<p>Table 6 :
6Examples where large language models are not consistent for mathematical reasoning.</p>
<p>Alexander A. Alemi, Franois Chollet, Niklas Een, Geoffrey Irving, Christian Szegedy, and Josef Urban. 2016. Deepmath -deep sequence models for premise selection. Advances in neural information processing systems (NeurIPS), 29. Reem Alghamdi, Zhenwen Liang, and Xiangliang Zhang. 2022. Armath: a dataset for solving arabic math word problems. In Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC), pages 351-362. Chris Alvin, Sumit Gulwani, Rupak Majumdar, and Supratik Mukhopadhyay. 2017. Synthesis of solutions for shaded area geometry problems. In The Thirtieth International Flairs Conference. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 2357-2367. Connor Anderson and Ryan Farrell. 2022. Improving fractal pre-training. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1300-1309. Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022. Exploring length generalization in large language models. In Advances in Neural Information Processing Systems (NeurIPS). Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, and Stanislas Polu. 2022. Proof artifact cotraining for theorem proving with language models. In International Conference on Learning Representations (ICLR). Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. 2019. Gamepad: A learning environment for theorem proving. In International Conference on Learning Representations (ICLR). Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, kos Kdr, Adam Trischler, and Yoshua Bengio. 2018. Figureqa: An annotated figure dataset for visual reasoning. In International Conference on Learning Representations (ICLR). Lei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu, Lianli Gao, Bing Tian Dai, and Heng Tao Shen. 2019. Template-based math word problem solvers with recursive neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pages 7144-7151. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations (ICLR).Peter Anderson, Xiaodong He, Chris Buehler, Damien 
Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 
2018. Bottom-up and top-down attention for image 
captioning and visual question answering. In Pro-
ceedings of the IEEE conference on computer vision 
and pattern recognition (CVPR), pages 6077-6086. </p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten 
Bosma, Henryk Michalewski, David Dohan, Ellen 
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. 
Program synthesis with large language models. arXiv 
preprint arXiv:2108.07732. </p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly 
learning to align and translate. In International Con-
ference on Learning Representations (ICLR). </p>
<p>Kshitij Bansal, Sarah Loos, Markus Rabe, Christian 
Szegedy, and Stewart Wilcox. 2019. Holist: An envi-
ronment for machine learning of higher order logic 
theorem proving. In International Conference on 
Machine Learning (ICML), pages 454-463. PMLR. </p>
<p>Bruno Barras, Samuel Boutin, Cristina Cornes, Judi-
cal Courant, Yann Coscoy, David Delahaye, Daniel 
de Rauglaudre, Jean-Christophe Fillitre, Eduardo 
Gimnez, Hugo Herbelin, et al. 1999. The coq proof 
assistant reference manual. INRIA, version, 6(11). </p>
<p>Taylor Berg-Kirkpatrick and Daniel Spokoyny. 2020. 
An empirical investigation of contextualized number 
prediction. In Proceedings of the 2020 Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP), pages 4754-4764. </p>
<p>Arindam Bhattacharya. 2017. A survey of question 
answering for math and science problem. arXiv 
preprint arXiv:1705.04530. 
Daniel G Bobrow. 1964. Natural language input for 
a computer problem solving system. AI Technical 
Reports. </p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie 
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind 
Neelakantan, Pranav Shyam, Girish Sastry, Amanda 
Askell, et al. 2020. Language models are few-shot 
learners. Advances in Neural Information Processing 
Systems (NeurIPS), 33:1877-1901. </p>
<p>Jie Cao and Jing Xiao. 2022. An augmented benchmark 
dataset for geometric question answering through 
dual parallel text encoding. In Proceedings of the 
29th International Conference on Computational Lin-
guistics (COLING), pages 1511-1520. </p>
<p>Yixuan Cao, Feng Hong, Hongwei Li, and Ping Luo. 
2021. A bottom-up dag structure extraction model 
for math word problems. In Proceedings of the AAAI 
Conference on Artificial Intelligence (AAAI), pages 
39-46. </p>
<p>Franois Charton. 2022. Linear algebra with transform-
ers. Transactions on Machine Learning Research. </p>
<p>Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, 
Chongyu Chen, and Xiaodan Liang. 2022a. Unigeo: 
Unifying geometry logical reasoning via reformu-
lating mathematical expression. In The 2022 Con-
ference on Empirical Methods in Natural Language 
Processing (EMNLP). </p>
<p>Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, 
Lingbo Liu, Eric Xing, and Liang Lin. 2021a. Geoqa: 
A geometric question answering benchmark towards 
multimodal numerical reasoning. In Findings of the 
Association for Computational Linguistics (ACL), 
pages 513-523. </p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, 
Henrique Ponde de Oliveira Pinto, Jared Kaplan, 
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg 
Brockman, et al. 2021b. Evaluating large lan-
guage models trained on code. arXiv preprint 
arXiv:2107.03374. </p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and 
William W Cohen. 2022b. Program of thoughts 
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint 
arXiv:2211.12588. </p>
<p>Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, 
Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, 
and Pan Lu. 2023. 
Theoremqa: A theorem-
driven question answering dataset. arXiv preprint 
arXiv:2305.12524. </p>
<p>Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena 
Shah, Iana Borova, Dylan Langdon, Reema Moussa, 
Matt Beane, Ting-Hao Huang, Bryan R Routledge, 
et al. 2021c. Finqa: A dataset of numerical reasoning 
over financial data. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language 
Processing (EMNLP), pages 3697-3711. </p>
<p>Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang 
Ma, Sameena Shah, and William Yang Wang. 2022c. 
Convfinqa: Exploring the chain of numerical rea-
soning in conversational finance question answering. 
arXiv preprint arXiv:2210.03849. </p>
<p>Ting-Rui Chiang and Yun-Nung Chen. 2019. 
Semantically-aligned equation generation for 
solving and reasoning math word problems. In 
Proceedings of the 2019 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies 
(NAACL-HLT), pages 2656-2668. </p>
<p>Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. 
Unifying vision-and-language tasks via text genera-
tion. In Proceedings of the 38th International Con-
ference on Machine Learning (ICML), pages 1931-
1942. </p>
<p>Kyunghyun Cho, Bart van Merrienboer Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares Holger 
Schwenk, and Yoshua Bengio. 2014. Learning 
phrase representations using rnn encoder-decoder for 
statistical machine translation. In Proceedings of the 
2014 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 1724-1734. </p>
<p>Shang-Ching Chou, Xiao-Shan Gao, and Jing-Zhong 
Zhang. 1996. Automated generation of readable 
proofs with geometric invariants. Journal of Auto-
mated Reasoning, 17(3):325-347. </p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, 
Maarten Bosma, Gaurav Mishra, Adam Roberts, 
Paul Barham, Hyung Won Chung, Charles Sutton, 
Sebastian Gehrmann, et al. 2022. Palm: Scaling 
language modeling with pathways. arXiv preprint 
arXiv:2204.02311. </p>
<p>Peter Clark, Oren Etzioni, Tushar Khot, Daniel 
Khashabi, Bhavana Mishra, Kyle Richardson, Ashish 
Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket 
Tandon, et al. 2020. From 'f'to 'a'on the ny regents 
science exams: An overview of the aristo project. AI 
Magazine, 41(4):39-53. </p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Jacob Hilton, Reiichiro Nakano, Christopher 
Hesse, and John Schulman. 2021. Training veri-
fiers to solve math word problems. arXiv preprint 
arXiv:2110.14168. </p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and 
Kristina Toutanova. 2019. BERT: Pre-training of 
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference 
of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies (NAACL-HLT), pages 4171-4186. </p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel 
Stanovsky, Sameer Singh, and Matt Gardner. 2019. 
Drop: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In Proceed-
ings of the 2019 Conference of the North American 
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 2368-2378. </p>
<p>Edward A Feigenbaum et al. 1963. Computers and 
thought. McGraw-Hill. </p>
<p>Yu Feng, Jing Zhang, Xiaokang Zhang, Lemao Liu, 
Cuiping Li, and Hong Chen. 2021. Injecting numer-
ical reasoning skills into knowledge base question 
answering models. arXiv preprint arXiv:2112.06109. </p>
<p>Deborah Ferreira and Andr Freitas. 2020a. Natural 
language premise selection: Finding supporting state-
ments for mathematical text. In Proceedings of the 
Twelfth Language Resources and Evaluation Confer-
ence, pages 2175-2182. </p>
<p>Deborah Ferreira and Andr Freitas. 2020b. Premise 
selection in natural language mathematical texts. In 
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages 
7365-7374. </p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and 
Tushar Khot. 2023. Complexity-based prompting for 
multi-step reasoning. In International Conference on 
Learning Representations (ICLR). </p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020. 
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027. </p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, 
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language 
models. arXiv preprint arXiv:2211.10435. </p>
<p>Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, 
Steven CH Hoi, Xiaogang Wang, and Hongsheng Li. 
2019. Dynamic fusion with intra-and inter-modality 
attention flow for visual question answering. In The 
IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR), pages 6639-6648. </p>
<p>Thibault Gauthier, Cezary Kaliszyk, Josef Urban, Ra-
mana Kumar, and Michael Norrish. 2021. TacticToe: 
Learning to Prove with Tactics. Journal of Auto-
mated Reasoning. </p>
<p>Jonas Gehring, Michael Auli, David Grangier, Denis 
Yarats, and Yann N Dauphin. 2017. Convolutional se-
quence to sequence learning. In International confer-
ence on machine learning (ICML), pages 1243-1252. 
PMLR. </p>
<p>Herbert Gelernter, James R Hansen, and Donald W 
Loveland. 1960. Empirical explorations of the ge-
ometry theorem machine. In Papers presented at 
the May 3-5, 1960, western joint IRE-AIEE-ACM 
computer conference, pages 143-149. </p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. 
Injecting numerical reasoning skills into language 
models. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL), pages 946-958. </p>
<p>Kevin Gimpel, Dipanjan Das, and Noah A Smith. 2010. 
Distributed asynchronous online learning for natural 
language processing. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 213-222. </p>
<p>Amelia Glaese, Nat McAleese, Maja Trbacz, John 
Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, 
Laura Weidinger, Martin Chadwick, Phoebe Thacker, 
et al. 2022. Improving alignment of dialogue agents 
via targeted human judgements. arXiv preprint 
arXiv:2209.14375. </p>
<p>Adam Grabowski, Artur Korniowicz, and Adam Nau-
mowicz. 2015. Four decades of mizar. Journal of 
Automated Reasoning, 55(3):191-198. </p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented 
language model pre-training. In International Con-
ference on Machine Learning (ICML), pages 3929-
3938. PMLR. </p>
<p>Yihan Hao, Mingliang Zhang, Fei Yin, and Linlin 
Huang. 2022. Pgdp5k: A diagram parsing dataset 
for plane geometry problems. In 26th International 
Conference on Pattern Recognition (ICPR). </p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian 
Sun. 2016. Deep residual learning for image recogni-
tion. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition (CVPR), pages 
770-778. </p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, 
Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 
2021a. Measuring massive multitask language under-
standing. In International Conference on Learning 
Representations (ICLR). </p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul 
Arora, Steven Basart, Eric Tang, Dawn Song, and 
Jacob Steinhardt. 2021b. Measuring mathematical 
problem solving with the math dataset. In 35th Con-
ference on Neural Information Processing Systems 
(NeurIPS) Track on Datasets and Benchmarks. </p>
<p>Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam 
Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. 
Pretrained transformers improve out-of-distribution 
robustness. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL), pages 2744-2751. 
Jonathan Herzig, Pawel Krzysztof Nowak, Thomas 
Mueller, Francesco Piccinno, and Julian Eisensch-
los. 2020. Tapas: Weakly supervised table parsing 
via pre-training. In Proceedings of the 58th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), pages 4320-4333. </p>
<p>Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long 
short-term memory. Neural computation, 9(8):1735-
1780. </p>
<p>Yining Hong, Qing Li, Daniel Ciao, Siyuan Huang, 
and Song-Chun Zhu. 2021a. Learning by fixing: 
Solving math word problems with weak supervision. 
In Proceedings of the AAAI Conference on Artificial 
Intelligence (AAAI), pages 4959-4967. </p>
<p>Yining Hong, Qing Li, Ran Gong, Daniel Ciao, Siyuan 
Huang, and Song-Chun Zhu. 2021b. Smart: A situa-
tion model for algebra story problems via attributed 
grammar. In AAAI, pages 13009-13017. </p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren 
Etzioni, and Nate Kushman. 2014. Learning to solve 
arithmetic word problems with verb categorization. 
In Proceedings of the 2014 Conference on Empirical 
Methods in Natural Language Processing (EMNLP). </p>
<p>Danqing Huang, Jing Liu, Chin-Yew Lin, and Jian Yin. 
2018. Neural math word problem solver with rein-
forcement learning. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics 
(COLING), pages 213-223. </p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, and Jian 
Yin. 2017. Learning fine-grained expressions to solve 
math word problems. In Proceedings of Empirical 
Methods in Natural Language Processing (EMNLP), 
pages 805-814. </p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, 
and Wei-Ying Ma. 2016. How well do computers 
solve math word problems? large-scale dataset con-
struction and evaluation. In Proceedings of the 54th 
Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 887-896. </p>
<p>Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda 
Li, Jiacheng Liu, Mateja Jamnik, Timothe Lacroix, 
Yuhuai Wu, and Guillaume Lample. 2022a. Draft, 
sketch, and prove: Guiding formal theorem provers 
with informal proofs. In Submitted to The Eleventh 
International Conference on Learning Representa-
tions. </p>
<p>Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, 
and Yuhuai Wu. 2021. Lisa: Language models of 
isabelle proofs. In 6th Conference on Artificial Intel-
ligence and Theorem Proving (AITP). </p>
<p>Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, 
Konrad Czechowski, Tomasz Odrzygd, Piotr 
Mio, Yuhuai Wu, and Mateja Jamnik. 2022b. Thor: 
Wielding hammers to integrate language models and 
automated theorem provers. Advances in Neural 
Information Processing Systems (NeurIPS), 35:8360-
8373. </p>
<p>Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning 
to reason deductively: Math word problem solving 
as complex relation extraction. In Proceedings of the 
60th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 5944-5955. </p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-
man, Chandra Bhagavatula, Ronan Le Bras, and 
Yejin Choi. 2022. Maieutic prompting: Logically 
consistent reasoning with recursive explanations. In 
Proceedings of the 2022 Conference on Empirical 
Methods in Natural Language Processing (EMNLP), 
pages 1266-1279. </p>
<p>Kushal Kafle, Brian Price, Scott Cohen, and Christopher 
Kanan. 2018. Dvqa: Understanding data visualiza-
tions via question answering. In Proceedings of the 
IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR), pages 5648-5656. </p>
<p>Cezary Kaliszyk, Franois Chollet, and Christian 
Szegedy. 2017. Holstep: A machine learning dataset 
for higher-order logic theorem proving. In Inter-
national Conference on Learning Representations 
(ICLR). </p>
<p>Ashwin Kalyan, Abhinav Kumar, Arjun Chan-
drasekaran, Ashish Sabharwal, and Peter Clark. 2021. 
How much coffee was consumed during emnlp 2019? 
fermi problems: A new reasoning challenge for ai. 
In Proceedings of the 2021 Conference on Empirical 
Methods in Natural Language Processing (EMNLP), 
pages 7318-7328. </p>
<p>Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wal-
lace, and Colin Raffel. 2022. Large language mod-
els struggle to learn long-tail knowledge. ArXiv, 
abs/2211.08411. </p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-
harwal, Oyvind Tafjord, Peter Clark, and Hannaneh 
Hajishirzi. 2020. Unifiedqa: Crossing format bound-
aries with a single qa system. In Findings of the 
Association for Computational Linguistics (EMNLP), 
pages 1896-1907. </p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao 
Fu, Kyle Richardson, Peter Clark, and Ashish Sab-
harwal. 2022. Decomposed prompting: A modular 
approach for solving complex tasks. arXiv preprint 
arXiv:2210.02406. 
Bugeun Kim, Kyung Seo Ki, Donggeon Lee, and Gah-
gene Gweon. 2020. Point to the expression: Solving 
algebraic word problems using the expression-pointer 
transformer model. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language 
Processing (EMNLP), pages 3768-3779. </p>
<p>Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 
2018. Bilinear attention networks. In Advances in 
Neural Information Processing Systems (NeurIPS), 
pages 1571-1581. </p>
<p>Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: 
Vision-and-language transformer without convolu-
tion or region supervision. In Proceedings of the 
38th International Conference on Machine Learning 
(ICML), pages 5583-5594. </p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In 36th Con-
ference on Neural Information Processing Systems 
(NeurIPS). </p>
<p>Rik Koncel-K., Subhro Roy, Aida Amini, Nate Kush-
man, and Hannaneh Hajishirzi. 2016. Mawps: A 
math word problem repository. In Proceedings of the 
2016 Conference of the North American Chapter of 
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL), pages 1152-
1157. </p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish 
Sabharwal, Oren Etzioni, and Siena Dumas Ang. 
2015. Parsing algebraic word problems into equa-
tions. Transactions of the Association for Computa-
tional Linguistics (TACL), 3:585-597. </p>
<p>Kundan Krishna, Jeffrey Bigham, and Zachary C Lipton. 
2021. Does pretraining for summarization require 
knowledge transfer? In Findings of the Association 
for Computational Linguistics: EMNLP 2021, pages 
3178-3189. </p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and 
Regina Barzilay. 2014. Learning to automatically 
solve algebra word problems. In Proceedings of the 
52nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 271-281. </p>
<p>Guillaume Lample and Franois Charton. 2020. Deep 
learning for symbolic mathematics. In International 
Conference on Learning Representations (ICLR). </p>
<p>Guillaume Lample, Timothee Lacroix, Marie-Anne 
Lachaux, Aurelien Rodriguez, Amaury Hayat, 
Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. 
2022. Hypertree proof search for neural theorem 
proving. Advances in Neural Information Processing 
Systems (NeurIPS), 35:26337-26349. </p>
<p>Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, 
Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-
Peng Lim. 2022. Mwptoolkit: an open-source frame-
work for deep learning-based math word problem 
solvers. In Proceedings of the AAAI Conference on 
Artificial Intelligence (AAAI), pages 13188-13190. </p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, 
Kevin Gimpel, Piyush Sharma, and Radu Soricut. 
2019. Albert: A lite bert for self-supervised learn-
ing of language representations. arXiv preprint 
arXiv:1909.11942. </p>
<p>Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick 
Haffner. 1998. Gradient-based learning applied to 
document recognition. Proceedings of the IEEE, 
86(11):2278-2324. </p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan 
Ghazvininejad, Abdelrahman Mohamed, Omer Levy, 
Veselin Stoyanov, and Luke Zettlemoyer. 2020. 
BART: Denoising sequence-to-sequence pre-training 
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL), pages 7871-7880. </p>
<p>Aitor Lewkowycz, Anders Johan Andreassen, 
David Dohan, Ethan Dyer, Henryk Michalewski, 
Vinay Venkatesh Ramasesh, Ambrose Slone, Cem 
Anil, Imanol Schlag, Theo Gutman-Solo, et al. 
2022. Solving quantitative reasoning problems with 
language models. In Advances in Neural Information 
Processing Systems (NeurIPS). </p>
<p>Jierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian 
Dai, and Dongxiang Zhang. 2019. Modeling intra-
relation in math word problems with different func-
tional multi-head attentions. In Proceedings of the 
57th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 6162-6167. </p>
<p>Jiwei Li, Alexander H Miller, Sumit Chopra, 
Marc'Aurelio Ranzato, and Jason Weston. 2017. Di-
alogue learning with human-in-the-loop. In Inter-
national Conference on Learning Representations 
(ICLR). </p>
<p>Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui 
Hsieh, and Kai-Wei Chang. 2020a. What does bert 
with vision look at? In Proceedings of the 58th An-
nual Meeting of the Association for Computational 
Linguistics (ACL), pages 5265-5275. </p>
<p>Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, 
Fengyuan Xu, and Sheng Zhong. 2020b. Graph-
to-tree neural networks for learning structured input-
output translation with applications to semantic pars-
ing and math word problem. In Findings of the As-
sociation for Computational Linguistics (EMNLP), 
pages 2841-2852. </p>
<p>Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C Paulson. 
2021. Isarstep: a benchmark for high-level mathe-
matical reasoning. In International Conference on 
Learning Representations (ICLR). </p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, 
Jian-Guang Lou, and Weizhu Chen. 2022a. On the 
advance of making language models better reasoners. 
arXiv preprint arXiv:2206.02336. 
Zhongli Li, Wenxuan Zhang, Chao Yan, Qingyu Zhou, 
Chao Li, Hongzhi Liu, and Yunbo Cao. 2022b. Seek-
ing patterns, not just memorizing procedures: Con-
trastive learning for solving math word problems. In 
Findings of the Association for Computational Lin-
guistics (ACL), pages 2486-2496. </p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris 
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian 
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-
mar, et al. 2022a. Holistic evaluation of language 
models. arXiv preprint arXiv:2211.09110. </p>
<p>Percy Liang and Dan Klein. 2009. Online em for unsu-
pervised models. In Proceedings of human language 
technologies: The 2009 annual conference of the 
North American chapter of the association for com-
putational linguistics (NAACL), pages 611-619. </p>
<p>Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, 
Yunshi Lan, Jie Shao, and Xiangliang Zhang. 2022b. 
Mwp-bert: Numeracy-augmented pre-training for 
math word problem solving. In Findings of the As-
sociation for Computational Linguistics (NAACL), 
pages 997-1009. </p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-
ang Ren. 2020. Birds have four legs?! numersense: 
Probing numerical commonsense knowledge of pre-
trained language models. In Proceedings of the 2020 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 6862-6868. </p>
<p>Xin Lin, Zhenya Huang, Hongke Zhao, Enhong Chen, 
Qi Liu, Hao Wang, and Shijin Wang. 2021. Hms: A 
hierarchical solver with dependency-enhanced under-
standing for math word problem. In Proceedings 
of the AAAI Conference on Artificial Intelligence 
(AAAI), pages 4232-4240. </p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word 
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL), pages 158-167. </p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B 
Dolan, Lawrence Carin, and Weizhu Chen. 2022a. 
What makes good in-context examples for gpt-3? 
In Proceedings of Deep Learning Inside Out (Dee-
LIO 2022): The 3rd Workshop on Knowledge Extrac-
tion and Integration for Deep Learning Architectures, 
pages 100-114. </p>
<p>Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi 
Lin, Weizhu Chen, and Jian-Guang Lou. 2022b. 
TAPEX: Table pre-training via learning a neural SQL 
executor. In International Conference on Learning 
Representations. </p>
<p>Qianying Liu, Wenyu Guan, Sujian Li, Fei Cheng, 
Daisuke Kawahara, and Sadao Kurohashi. 2020. Re-
verse operation based data augmentation for solving 
math word problems. IEEE Transactions on Audio, 
Speech and Language Processing. </p>
<p>Qianying Liu, Wenyv Guan, Sujian Li, and Daisuke 
Kawahara. 2019a. Tree-structured decoding for solv-
ing math word problems. In Proceedings of the 2019 
conference on empirical methods in natural language 
processing and the 9th international joint conference 
on natural language processing (EMNLP-IJCNLP), 
pages 2370-2379. </p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, 
Luke Zettlemoyer, and Veselin Stoyanov. 2019b. 
Roberta: A robustly optimized bert pretraining ap-
proach. Proceedings of the 2019 Conference of the 
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL-HLT). </p>
<p>Sarah Loos, Geoffrey Irving, Christian Szegedy, and 
Cezary Kaliszyk. 2017. Deep network guided proof 
search. arXiv preprint arXiv:1701.06972. </p>
<p>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan 
Huang, Xiaodan Liang, and Song-Chun Zhu. 2021a. 
Inter-gps: Interpretable geometry problem solving 
with formal language and symbolic reasoning. In 
The 59th Annual Meeting of the Association for Com-
putational Linguistics (ACL). </p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-
Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter 
Clark, and Ashwin Kalyan. 2022a. Learn to explain: 
Multimodal reasoning via thought chains for science 
question answering. In The 36th Conference on Neu-
ral Information Processing Systems (NeurIPS). </p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-
Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-
feng Gao. 2023. Chameleon: Plug-and-play compo-
sitional reasoning with large language models. arXiv 
preprint arXiv:2304.09842. </p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, 
Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, 
and Ashwin Kalyan. 2022b. Dynamic prompt learn-
ing via policy gradient for semi-structured mathe-
matical reasoning. In International Conference on 
Learning Representations (ICLR). </p>
<p>Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, 
Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun 
Zhu. 2021b. Iconqa: A new benchmark for abstract 
diagram understanding and visual language reason-
ing. In The 35th Conference on Neural Information 
Processing Systems (NeurIPS) Track on Datasets and 
Benchmarks. </p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, 
and Pontus Stenetorp. 2022c. Fantastically ordered 
prompts and where to find them: Overcoming few-
shot prompt order sensitivity. In Proceedings of the 
60th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 8086-8098. </p>
<p>The mathlib Community. 2020. The lean mathematical 
library. In CPP 2020 -Proceedings of the 9th ACM 
SIGPLAN International Conference on Certified Pro-
grams and Proofs, co-located with POPL 2020. </p>
<p>Jordan Meadows and Andre Freitas. 2022. A survey in 
mathematical language processing. arXiv preprint 
arXiv:2205.15231. </p>
<p>Norman D. Megill and David A. Wheeler. 2019. 
Metamath: A Computer Language for Mathematical 
Proofs. Lulu Press, Morrisville, North Carolina. 
http://us.metamath.org/downloads/metamath.pdf. </p>
<p>Yuanliang Meng and Anna Rumshisky. 2019. Solv-
ing math word problems with double-decoder trans-
former. arXiv preprint arXiv:1908.10924. </p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 
2020. A diverse corpus for evaluating and developing 
english math word problem solvers. In Proceedings 
of the 58th Annual Meeting of the Association for 
Computational Linguistics (ACL), pages 975-984. </p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, 
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations: 
What makes in-context learning work? Proceedings 
of Empirical Methods in Natural Language Process-
ing (EMNLP). </p>
<p>Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-
jes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 
2021. Deep learning based text classification: a 
comprehensive review. ACM Computing Surveys 
(CSUR), 54(3):1-40. </p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard 
Tang, Sean Welleck, Chitta Baral, Tanmay Rajpuro-
hit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, 
and Ashwin Kalyan. 2022a. Lila: A unified bench-
mark for mathematical reasoning. In Proceedings 
of the 2022 Conference on Empirical Methods in 
Natural Language Processing (EMNLP). </p>
<p>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, 
Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and 
Ashwin Kalyan. 2022b. Numglue: A suite of funda-
mental yet challenging mathematical reasoning tasks. 
In Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages 
3505-3523. </p>
<p>Eric Mitchell, Joseph J. Noh, Siyan Li, William S. Arm-
strong, Ananth Agarwal, Patrick Liu, Chelsea Finn, 
and Christopher D. Manning. 2022. Enhancing self-
consistency and performance of pretrained language 
models with nli. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language 
Processing (EMNLP). Association for Computational 
Linguistics. </p>
<p>Leonardo de Moura, Soonho Kong, Jeremy Avigad, 
Floris van Doorn, and Jakob von Raumer. 2015. The 
lean theorem prover (system description). In Inter-
national Conference on Automated Deduction, pages 
378-388. Springer. </p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, 
Long Ouyang, Christina Kim, Christopher Hesse, 
Shantanu Jain, Vineet Kosaraju, William Saunders, 
et al. 2021. Webgpt: Browser-assisted question-
answering with human feedback. arXiv preprint 
arXiv:2112.09332. </p>
<p>Allen Newell, John Clifford Shaw, and Herbert A Simon. 
1957. Empirical explorations of the logic theory 
machine: A case study in heuristic. In Proceedings of 
the Western Joint Computer Conference, IRE-AIEE-
ACM 1957. </p>
<p>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Olek-
sandr Polozov, Christopher Meek, Dragomir Radev, 
and Jianfeng Gao. 2023. Learning from self-sampled 
correct and partially-correct programs. In Inter-
national Conference on Learning Representations 
(ICLR). </p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, 
Henryk Michalewski, Jacob Austin, David Bieber, 
David Dohan, Aitor Lewkowycz, Maarten Bosma, 
David Luan, et al. 2021. Show your work: Scratch-
pads for intermediate computation with language 
models. arXiv preprint arXiv:2112.00114. </p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang, 
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 
2022. Training language models to follow instruc-
tions with human feedback. In Advances in Neural 
Information Processing Systems (NeurIPS). </p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 
2021. Are nlp models really able to solve simple 
math word problems? In Proceedings of the 2021 
Conference of the North American Chapter of the 
Association for Computational Linguistics: Human 
Language Technologies (NAACL-HIT), pages 2080-
2094. </p>
<p>Lawrence C. Paulson. 1994. Isabelle -A Generic The-
orem Prover (with a contribution by T. Nipkow), 
volume 828 of Lecture Notes in Computer Science. 
Springer. </p>
<p>Ethan Perez, Florian Strub, Harm De Vries, Vincent 
Dumoulin, and Aaron Courville. 2018. Film: Vi-
sual reasoning with a general conditioning layer. In 
Proceedings of the AAAI Conference on Artificial 
Intelligence (AAAI). </p>
<p>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Man-
tas Baksys, Igor Babuschkin, and Ilya Sutskever. 
2023. Formal mathematics statement curriculum 
learning. In International Conference on Learning 
Representations (ICLR), volume abs/2202.01344. </p>
<p>Stanislas Polu and Ilya Sutskever. 2020. Generative 
language modeling for automated theorem proving. 
arXiv preprint arXiv:2009.03393. </p>
<p>Jinghui Qin, Xiaodan Liang, Yining Hong, Jianheng 
Tang, and Liang Lin. 2021. Neural-symbolic solver 
for math word problems with auxiliary tasks. In 
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th 
International Joint Conference on Natural Language 
Processing (ACL), pages 5870-5881. </p>
<p>Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang, 
and Liang Lin. 2020. Semantically-aligned universal 
tree-structured solver for math word problems. In 
Proceedings of the 2020 Conference on Empirical 
Methods in Natural Language Processing (EMNLP), 
pages 3780-3789. </p>
<p>Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin 
Peng, Jianfeng Gao, and Song-Chun Zhu. 2022a. Val-
uenet: A new dataset for human value driven dialogue 
system. In Proceedings of the AAAI Conference on 
Artificial Intelligence (AAAI), pages 2468-2484. </p>
<p>Liang Qiu, Yizhou Zhao, Yuan Liang, Pan Lu, Weiyan 
Shi, Zhou Yu, and Song-chun Zhu. 2022b. Towards 
socially intelligent agents with mental state transition 
and human value. In Proceedings of the 23rd Annual 
Meeting of the Special Interest Group on Discourse 
and Dialogue, pages 146-158. </p>
<p>Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, 
Ning Dai, and Xuanjing Huang. 2020. Pre-trained 
models for natural language processing: A survey. 
Science China Technological Sciences, 63(10):1872-
1897. </p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, 
Dario Amodei, Ilya Sutskever, et al. 2020. Language 
models are unsupervised multitask learners. OpenAI 
Blog. </p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie 
Millican, Jordan Hoffmann, Francis Song, John 
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, et al. 2021. Scaling language models: 
Methods, analysis &amp; insights from training gopher. 
arXiv preprint arXiv:2112.11446. </p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine 
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, 
Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a unified text-to-text 
transformer. Journal of Machine Learning Research 
(JMLR), 21:1-67. </p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, 
and Eduard Hovy. 2019. Equate: A benchmark evalu-
ation framework for quantitative reasoning in natural 
language inference. In Proceedings of the 23rd Con-
ference on Computational Natural Language Learn-
ing (CoNLL), pages 349-361. </p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, 
and Sameer Singh. 2022. Impact of pretraining term 
frequencies on few-shot numerical reasoning. In 
Findings of the Association for Computational Lin-
guistics: EMNLP 2022, pages 840-854. </p>
<p>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian 
Sun. 2015. Faster r-cnn: Towards real-time object 
detection with region proposal networks. Advances </p>
<p>in neural information processing systems (NeurIPS), 
28. </p>
<p>Ryokan Ri and Yoshimasa Tsuruoka. 2022. Pretraining 
with artificial language: Studying transferable knowl-
edge in language models. In Proceedings of the 60th 
Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 7302-7315. </p>
<p>Benjamin Robaidek, Rik Koncel-Kedziorski, and Han-
naneh Hajishirzi. 2018. Data-driven methods for 
solving algebra word problems. arXiv preprint 
arXiv:1804.10718. </p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arith-
metic word problems. In Proceedings of the 2015 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1743-1752. </p>
<p>Subhro Roy and Dan Roth. 2017. Unit dependency 
graph and its application to arithmetic word problem 
solving. In Proceedings of the AAAI Conference on 
Artificial Intelligence (AAAI). </p>
<p>Subhro Roy and Dan Roth. 2018. Mapping to declara-
tive knowledge for word problem solving. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 6:159-172. </p>
<p>Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reason-
ing about quantities in natural language. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 3:1-13. </p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 
2022. Learning to retrieve prompts for in-context 
learning. North American Chapter of the Association 
for Computational Linguistics (NAACL). </p>
<p>Mrinmaya Sachan, Kumar Dubey, and Eric Xing. 2017. 
From textbooks to knowledge: A case study in har-
vesting axiomatic knowledge from textbooks to solve 
geometry problems. In Proceedings of Empirical 
Methods in Natural Language Processing (EMNLP), 
pages 773-784. </p>
<p>Mrinmaya Sachan and Eric Xing. 2017. Learning 
to solve geometry problems from natural language 
demonstrations in textbooks. In Proceedings of the 
6th Joint Conference on Lexical and Computational 
Semantics, pages 251-261. </p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and 
Pushmeet Kohli. 2020. Analysing mathematical rea-
soning abilities of neural models. In International 
Conference on Learning Representations (ICLR). </p>
<p>Tal Schuster, Ashwin Kalyan, Alex Polozov, and 
Adam Tauman Kalai. 2021. Programming puzzles. 
In Thirty-fifth Conference on Neural Information Pro-
cessing Systems (NeurIPS) Datasets and Benchmarks 
Track. </p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 
2016. Neural machine translation of rare words with 
subword units. In Proceedings of the 54th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), pages 1715-1725. </p>
<p>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren 
Etzioni, and Clint Malcolm. 2015. Solving geometry 
problems: Combining text and diagram interpreta-
tion. In Proceedings of Empirical Methods in Natural 
Language Processing (EMNLP), pages 1466-1476. </p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin 
Jiang, Ming Zhang, and Qun Liu. 2021. Generate &amp; 
rank: A multi-task framework for math word prob-
lems. In Findings of the Association for Computa-
tional Linguistics (EMNLP), pages 2269-2279. </p>
<p>Yibin Shen and Cheqing Jin. 2020. Solving math word 
problems with multi-encoders and multi-decoders. In 
Proceedings of the 28th International Conference on 
Computational Linguistics (COLING), pages 2924-
2934. </p>
<p>Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang 
Liu, and Yong Rui. 2015. Automatically solving 
number word problems by semantic parsing and rea-
soning. In Proceedings of the 2015 conference on 
empirical methods in natural language processing 
(EMNLP), pages 1132-1142. </p>
<p>Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. Mass: Masked sequence to sequence 
pre-training for language generation. In 36th Inter-
national Conference on Machine Learning (ICML). </p>
<p>Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, 
and Claire Cardie. 2019. Dream: A challenge data 
set and models for dialogue-based reading compre-
hension. Transactions of the Association for Compu-
tational Linguistics (TACL), 7:217-231. </p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. 
Sequence to sequence learning with neural networks. 
Advances in neural information processing systems 
(NeurIPS), 27. </p>
<p>Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau 
Yih, and Ashish Sabharwal. 2019. Quarel: A dataset 
and models for answering questions about qualitative 
relationships. In Proceedings of the AAAI Conference 
on Artificial Intelligence (AAAI), pages 7063-7071. </p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D Man-
ning. 2015. Improved semantic representations from 
tree-structured long short-term memory networks. In 
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the 7th 
International Joint Conference on Natural Language 
Processing (ACL), pages 1556-1566. </p>
<p>Avijit Thawani, Jay Pujara, and Ashwin Kalyan. 2022. 
Estimating numbers without regression. In 36th Con-
ference on Neural Information Processing Systems 
(NeurIPS 2022) Workshop on MATH-AI. </p>
<p>Avijit Thawani, Jay Pujara, Pedro A Szekely, and Filip 
Ilievski. 2021. Representing numbers in nlp: a survey 
and a vision. In Proceedings of the 2021 Conference </p>
<p>of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies (NAACL-HIT), pages 644-656. </p>
<p>Shounaak Ughade and Satish Kumbhar. 2019. Survey 
on mathematical word problem solving using natu-
ral language processing. In 2019 1st International 
Conference on Innovations in Information and Com-
munication Technology (ICIICT), pages 1-5. IEEE. </p>
<p>Shyam Upadhyay and Ming-Wei Chang. 2015. Draw: 
A challenging and diverse algebra word problem set. 
Technical report, Citeseer. </p>
<p>Shyam Upadhyay and Ming-Wei Chang. 2017. An-
notating derivations: A new evaluation strategy and 
dataset for algebra word problems. In Proceedings 
of the 15th Conference of the European Chapter of 
the Association for Computational Linguistics (ACL), 
pages 494-504. </p>
<p>Josef Urban. 2006. Mptp 0.2: Design, implementa-
tion, and initial experiments. Journal of Automated 
Reasoning, 37(1):21-43. </p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob 
Uszkoreit, Llion Jones, Aidan N Gomez, ukasz 
Kaiser, and Illia Polosukhin. 2017. Attention is all 
you need. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), pages 5998-6008. </p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, 
and Matt Gardner. 2019. Do nlp models know num-
bers? probing numeracy in embeddings. In Proceed-
ings of the 2019 Conference on Empirical Methods 
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 5307-5315. </p>
<p>Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, 
and Xiaojiang Liu. 2018a. Translating a math word 
problem to a expression tree. In Proceedings of the 
2018 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 1064-1069. </p>
<p>Lei Wang, Dongxiang Zhang, Lianli Gao, Jingkuan 
Song, Long Guo, and Heng Tao Shen. 2018b. Math-
dqn: Solving arithmetic word problems via deep re-
inforcement learning. In Proceedings of the AAAI 
Conference on Artificial Intelligence (AAAI). </p>
<p>Table 7 :
7A summarization of mathematical reasoning datasets.
Limitations Section on page 10.A2. Did you discuss any potential risks of your work?Limitations Section on page 10.A3. Do the abstract and introduction summarize the paper's main claims?Section 1.A4. Have you used AI writing assistants when working on this paper?Left blank. C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Not applicable. Left blank.D Did you use human annotators (e.g., crowdworkers) or research with human participants?Left blank.D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Not applicable. Left blank.D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Not applicable. Left blank.D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Not applicable. Left blank.D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable. Left blank.
Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 845-854.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, Advances in Neural Information Processing Systems (NeurIPS). Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Naturalproofs: Mathematical theorem proving in natural language. Sean Welleck, Jiacheng Liu, Hannaneh Ronan Le Bras, Yejin Hajishirzi, Kyunghyun Choi, Cho, Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track. Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. 2021. Naturalproofs: Mathematical theorem proving in nat- ural language. In Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track.</p>
<p>Naturalprover: Grounded mathematical proof generation with language models. Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, Yejin Choi, Advances in Neural Information Processing Systems (NeurIPS). Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. 2022a. Naturalprover: Grounded mathematical proof generation with lan- guage models. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Generating sequences by learning to selfcorrect. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, International Conference on Learning Representations. ICLRSean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to self- correct. In International Conference on Learning Representations (ICLR).</p>
<p>Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics. Sean Welleck, Peter West, Jize Cao, Yejin Choi, AAAI. Sean Welleck, Peter West, Jize Cao, and Yejin Choi. 2022b. Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics. In AAAI.</p>
<p>Basic principles of mechanical theorem proving in elementary geometries. Wu Wen-Tsun, https:/link.springer.com/article/10.1007/BF02328447Journal of automated Reasoning. 23Wu Wen-Tsun. 1986. Basic principles of mechanical theorem proving in elementary geometries. Journal of automated Reasoning, 2(3):221-252.</p>
<p>Holophrasm: a neural automated theorem prover for higher-order logic. Daniel Whalen, arXiv:1608.02644arXiv preprintDaniel Whalen. 2016. Holophrasm: a neural automated theorem prover for higher-order logic. arXiv preprint arXiv:1608.02644.</p>
<p>Cbam: Convolutional block attention module. Sanghyun Woo, Jongchan Park, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Joon-Young Lee, and In So KweonSanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018. Cbam: Convolutional block attention module. In Proceedings of the European conference on computer vision (ECCV), pages 3-19.</p>
<p>A knowledge-aware sequence-to-tree network for math word problem solving. Qinzhuo Wu, Qi Zhang, Jinlan Fu, Xuan-Jing Huang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Qinzhuo Wu, Qi Zhang, Jinlan Fu, and Xuan-Jing Huang. 2020. A knowledge-aware sequence-to-tree network for math word problem solving. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7137-7146.</p>
<p>An edge-enhanced hierarchical graph-to-tree network for math word problem solving. Qinzhuo Wu, Qi Zhang, Zhongyu Wei, Findings of the Association for Computational Linguistics (EMNLP). Qinzhuo Wu, Qi Zhang, and Zhongyu Wei. 2021a. An edge-enhanced hierarchical graph-to-tree network for math word problem solving. In Findings of the As- sociation for Computational Linguistics (EMNLP), pages 1473-1482.</p>
<p>Math word problem solving with explicit numerical values. Qinzhuo Wu, Qi Zhang, Zhongyu Wei, Xuan-Jing Huang, Proceedings of the 59th. the 59thQinzhuo Wu, Qi Zhang, Zhongyu Wei, and Xuan-Jing Huang. 2021b. Math word problem solving with ex- plicit numerical values. In Proceedings of the 59th</p>
<p>Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL). Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (ACL), pages 5859-5869.</p>
<p>Tianlong Ma, and Liang He. 2022a. A survey of human-in-the-loop for machine learning. Xingjiao Wu, Luwei Xiao, Yixuan Sun, Junhang Zhang, Future Generation Computer Systems. Xingjiao Wu, Luwei Xiao, Yixuan Sun, Junhang Zhang, Tianlong Ma, and Liang He. 2022a. A survey of human-in-the-loop for machine learning. Future Generation Computer Systems.</p>
<p>Google's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, V Quoc, Mohammad Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144arXiv preprintYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine trans- lation. arXiv preprint arXiv:1609.08144.</p>
<p>Int: An inequality benchmark for evaluating generalization in theorem proving. Yuhuai Wu, Albert Jiang, Jimmy Ba, Roger Baker Grosse, In International Conference on Learning Representations. ICLRYuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Baker Grosse. 2021c. Int: An inequality benchmark for evaluating generalization in theorem proving. In In- ternational Conference on Learning Representations (ICLR).</p>
<p>Mateja Jamnik, and Christian Szegedy. 2022b. Autoformalization with large language models. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe, Charles E Staats, Advances in Neural Information Processing Systems (NeurIPS). Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe, Charles E Staats, Mateja Jam- nik, and Christian Szegedy. 2022b. Autoformaliza- tion with large language models. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Insights into pre-training via simpler synthetic tasks. Yuhuai Wu, Felix Li, Percy Liang, arXiv:2206.10139arXiv preprintYuhuai Wu, Felix Li, and Percy Liang. 2022c. Insights into pre-training via simpler synthetic tasks. arXiv preprint arXiv:2206.10139.</p>
<p>Lime: Learning inductive bias for primitives of mathematical reasoning. Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, B Roger, Christian Grosse, Szegedy, PMLRInternational Conference on Machine Learning (ICML). Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse, and Christian Szegedy. 2021d. Lime: Learning inductive bias for primitives of math- ematical reasoning. In International Conference on Machine Learning (ICML), pages 11251-11262. PMLR.</p>
<p>A goal-driven tree-structured neural model for math word problems. Zhipeng Xie, Shichao Sun, International Joint Conference on Artificial Intelligence (IJCAI). Zhipeng Xie and Shichao Sun. 2019. A goal-driven tree-structured neural model for math word problems. In International Joint Conference on Artificial Intelli- gence (IJCAI), pages 5299-5305.</p>
<p>Show, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio, PMLRInternational conference on machine learning (ICML). Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual atten- tion. In International conference on machine learn- ing (ICML), pages 2048-2057. PMLR.</p>
<p>Learning to prove theorems via interacting with proof assistants. Kaiyu Yang, Jia Deng, PMLRInternational Conference on Machine Learning (ICML). Kaiyu Yang and Jia Deng. 2019. Learning to prove the- orems via interacting with proof assistants. In Inter- national Conference on Machine Learning (ICML), pages 6984-6994. PMLR.</p>
<p>An introduction to java geometry expert. Zheng Ye, Shang-Ching Chou, Xiao-Shan Gao, https:/link.springer.com/chapter/10.1007/978-3-642-21046-4_10International workshop on automated deduction in geometry. SpringerZheng Ye, Shang-Ching Chou, and Xiao-Shan Gao. 2008. An introduction to java geometry expert. In International workshop on automated deduction in geometry, pages 189-195. Springer.</p>
<p>Geore: A relation extraction dataset for chinese geometry problems. Wei Yu, Mengzhu Wang, Xiaodong Wang, Xun Zhou, Yongfu Zha, Yongjian Zhang, Shuyu Miao, Jingdong Liu, 35th Conference on Neural Information Processing Systems (NeurIPS) Workshop on Math AI for Education (MATHAI4ED). Wei Yu, Mengzhu Wang, Xiaodong Wang, Xun Zhou, Yongfu Zha, Yongjian Zhang, Shuyu Miao, and Jing- dong Liu. 2021a. Geore: A relation extraction dataset for chinese geometry problems. In 35th Conference on Neural Information Processing Systems (NeurIPS) Workshop on Math AI for Education (MATHAI4ED).</p>
<p>Improving math word problems with pre-trained knowledge and hierarchical reasoning. Weijiang Yu, Yingpeng Wen, Fudan Zheng, Nong Xiao, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)Weijiang Yu, Yingpeng Wen, Fudan Zheng, and Nong Xiao. 2021b. Improving math word problems with pre-trained knowledge and hierarchical reasoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3384-3394.</p>
<p>Generate rather than retrieve: Large language models are strong context generators. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, International Conference on Learning Representations. ICLRWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators. In International Confer- ence on Learning Representations (ICLR).</p>
<p>Solving arithmetic word problems by scoring equations with recursive neural networks. Klim Zaporojets, Giannis Bekoulis, Johannes Deleu, Thomas Demeester, Chris Develder, Expert Systems with Applications. 174114704Klim Zaporojets, Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2021. Solv- ing arithmetic word problems by scoring equations with recursive neural networks. Expert Systems with Applications, 174:114704.</p>
<p>The gap of semantic parsing: A survey on automatic math word problem solvers. Dongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian Dai, Heng Tao Shen, IEEE transactions on pattern analysis and machine intelligence. 42Dongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian Dai, and Heng Tao Shen. 2019. The gap of semantic parsing: A survey on automatic math word problem solvers. IEEE transactions on pattern analysis and machine intelligence, 42(9):2287-2305.</p>
<p>Teacher-student networks with multiple decoders for solving math word problem. Jipeng Zhang, Roy Ka-Wei Lee, Ee-Peng Lim, Wei Qin, Lei Wang, Jie Shao, Qianru Sun, International Joint Conference on Artificial Intelligence (IJCAI). Jipeng Zhang, Roy Ka-Wei Lee, Ee-Peng Lim, Wei Qin, Lei Wang, Jie Shao, and Qianru Sun. 2020a. Teacher-student networks with multiple decoders for solving math word problem. In International Joint Conference on Artificial Intelligence (IJCAI).</p>
<p>Graphto-tree learning for solving math word problems. Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, Ee-Peng Lim, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). the 58th Annual Meeting of the Association for Computational Linguistics (ACL)Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, and Ee-Peng Lim. 2020b. Graph- to-tree learning for solving math word problems. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics (ACL), pages 3928-3937.</p>
<p>Learning to understand plane geometry diagram. Ming-Liang Zhang, Fei Yin, Yi-Han Hao, Cheng-Lin Liu, 36th Conference on Neural Information Processing Systems (NeurIPS) Workshop on MATH-AI. Ming-Liang Zhang, Fei Yin, Yi-Han Hao, and Cheng- Lin Liu. 2022. Learning to understand plane geom- etry diagram. In 36th Conference on Neural Infor- mation Processing Systems (NeurIPS) Workshop on MATH-AI.</p>
<p>Noahqa: Numerical reasoning with interpretable graph question answering dataset. Qiyuan Zhang, Lei Wang, Sicheng Yu, Shuohang Wang, Yang Wang, Jing Jiang, Ee-Peng Lim, Findings of the Association for Computational Linguistics (EMNLP). Qiyuan Zhang, Lei Wang, Sicheng Yu, Shuohang Wang, Yang Wang, Jing Jiang, and Ee-Peng Lim. 2021. Noahqa: Numerical reasoning with interpretable graph question answering dataset. In Findings of the Association for Computational Linguistics (EMNLP), pages 4147-4161.</p>
<p>Machine number sense: A dataset of visual arithmetic problems for abstract and relational reasoning. Wenhe Zhang, Chi Zhang, Yixin Zhu, Song-Chun Zhu, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)Wenhe Zhang, Chi Zhang, Yixin Zhu, and Song-Chun Zhu. 2020c. Machine number sense: A dataset of visual arithmetic problems for abstract and relational reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pages 1332-1340.</p>
<p>Do language embeddings capture scales?. Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPXikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020d. Do language embeddings capture scales? In Proceedings of the Third BlackboxNLP Workshop on Analyzing and In- terpreting Neural Networks for NLP, pages 292-299.</p>
<p>Do language embeddings capture scales?. Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth, 10.18653/v1/2020.blackboxnlp-1.27Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPXikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020e. Do language embeddings capture scales? In Proceedings of the Third BlackboxNLP Workshop on Analyzing and In- terpreting Neural Networks for NLP, pages 292-299.</p>
<p>Dialogpt: Large-scale generative pre-training for conversational response generation. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System DemonstrationsYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020f. Dialogpt: Large-scale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics: System Demonstrations.</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, International Conference on Learning Representations. ICLRZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In International Confer- ence on Learning Representations (ICLR).</p>
<p>Ape210k: A large-scale and template-rich dataset of math word problems. Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, Jingming Liu, arXiv:2009.11506arXiv preprintWei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming Liu. 2020. Ape210k: A large-scale and template-rich dataset of math word problems. arXiv preprint arXiv:2009.11506.</p>
<p>Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. Yilun Zhao, Yunxiang Li, Chenying Li, Rui Zhang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL). the 60th Annual Meeting of the Association for Computational Linguistics (ACL)Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022. Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 6588-6600.</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, PMLRInternational Conference on Machine Learning (ICML). Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Im- proving few-shot performance of language models. In International Conference on Machine Learning (ICML), pages 12697-12706. PMLR.</p>
<p>Minif2f: a cross-system benchmark for formal olympiad-level mathematics. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, International Conference on Learning Representations. ICLRKunhao Zheng, Jesse Michael Han, and Stanislas Polu. 2022. Minif2f: a cross-system benchmark for for- mal olympiad-level mathematics. In International Conference on Learning Representations (ICLR).</p>
<p>Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, Proc. of the Conference on Empirical Methods in Natural Language Processing. of the Conference on Empirical Methods in Natural Language essingEMNLPBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. "Going on a vacation" takes longer than "Go- ing for a walk": A Study of Temporal Common- sense Understanding. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Leastto-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, International Conference on Learning Representations. Quoc Le, and Ed Chi. 2023ICLRDenny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least- to-most prompting enables complex reasoning in large language models. In International Conference on Learning Representations (ICLR).</p>
<p>Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, Tat-Seng Chua, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-JCNLP). the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-JCNLP)Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A question answer- ing benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th An- nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (ACL-JCNLP), pages 3277-3287.</p>            </div>
        </div>

    </div>
</body>
</html>
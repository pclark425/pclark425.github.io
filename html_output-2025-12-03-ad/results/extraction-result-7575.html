<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7575 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7575</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7575</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-259188004</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.09930v2.pdf" target="_blank">Flow-Bench: A Dataset for Computational Workflow Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> A computational workflow, also known as workflow, consists of tasks that must be executed in a specific order to attain a specific goal. Often, in fields such as biology, chemistry, physics, and data science, among others, these workflows are complex and are executed in large-scale, distributed, and heterogeneous computing environments prone to failures and performance degradation. Therefore, anomaly detection for workflows is an important paradigm that aims to identify unexpected behavior or errors in workflow execution. This crucial task to improve the reliability of workflow executions can be further assisted by machine learning-based techniques. However, such application is limited, in large part, due to the lack of open datasets and benchmarking. To address this gap, we make the following contributions in this paper: (1) we systematically inject anomalies and collect raw execution logs from workflows executing on distributed infrastructures; (2) we summarize the statistics of new datasets, and provide insightful analyses; (3) we convert workflows into tabular, graph and text data, and benchmark with supervised and unsupervised anomaly detection techniques correspondingly. The presented dataset and benchmarks allow examining the effectiveness and efficiency of scientific computational workflows and identifying potential research opportunities for improvement and generalization. The dataset and benchmark code are publicly available \url{https://poseidon-workflows.github.io/FlowBench/} under the MIT License.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7575.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7575.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuned BERT (BERT-base-uncased) with LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-only transformer (BERT-base-uncased) used as a supervised classifier by fine-tuning on textualized job-feature sentences from the FlowBench dataset; parameter-efficient fine-tuning was performed with LoRA to reduce trainable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base-uncased</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only bidirectional Transformer (BERT) fine-tuned for binary anomaly classification; fine-tuning performed with Low-Rank Adaptation (LoRA) for parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈110M</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>supervised fine-tuning on textual representations of tabular/job features (text classification)</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>"FEAT1 is VAL1 FEAT2 is VAL2 ... FEATn is VALn, LABEL" (dataset converted to sentence templates describing job features; used as supervised classification inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>FlowBench parsed dataset: textualized job descriptions derived from tabular features and experiment logs with synthetic anomaly labels (Normal, CPU_K, HDD_K) across 12 workflows; training/validation/test split 7:1:2.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>textualized job log entries derived from tabular features (converted from job-level numeric/categorical/time-stamp features to sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FlowBench (Flow-Bench) — parsed textual modality of the 12-workflow dataset introduced in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, Average Precision (AP), ROC-AUC, Recall@k (top-k), reported training time (sec.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-workflow supervised results (mean over 5 runs) example highlights from Table 3: 1000 Genome — Accuracy 0.776, AP 0.537, ROC-AUC 0.746, Recall@k 0.664, Train time 158.524s; EHT Smili — Accuracy 0.840, AP 0.399, ROC-AUC 0.657, Recall@k 0.342, Train time 26.594s; Predict Future Sales — Accuracy 0.861, AP 0.394, ROC-AUC 0.667, Recall@k 0.359, Train time 290.684s. Overall supervised BERT accuracy per-workflow range ≈0.776–0.861 in reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to graph-based GCNs and unsupervised PyOD/PyGOD models: supervised BERT-SFT is frequently among the top performing models on many workflows (outperforming some GCN runs on several datasets, and outperforming unsupervised baselines such as GMM / PyOD methods); the paper reports supervised and semi-supervised methods generally outperform unsupervised PyOD/PyGOD baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>fully supervised (fine-tuned on labeled FlowBench textual examples)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires labeled anomaly data (supervision). Performance varies strongly across workflows; some metrics (e.g., ROC-AUC) can be near random (≈0.5) for some workflows/metrics. Training cost and variability across workflow sizes noted; results reflect that different workflows and anomaly types affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Experiments ran on machine with NVIDIA A100 (40GB) and CPU; per-workflow BERT fine-tuning times reported in Table 3 (examples: 158.5s for 1000 Genome, 26.6s for EHT Smili, 290.7s for Predict Future Sales); LoRA reduced trainable parameters (paper reports training with 0.72% of BERT-base parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flow-Bench: A Dataset for Computational Workflow Anomaly Detection', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7575.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7575.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT-RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuned RoBERTa-large with LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger encoder-style Transformer (RoBERTa-large) fine-tuned (parameter-efficiently with LoRA) on the same textualized FlowBench job-feature sentences; evaluated as a supervised anomaly classifier and reported to be competitive or superior on many workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-based Transformer (RoBERTa-large) pre-trained variant of BERT family; fine-tuned for supervised anomaly classification using LoRA for parameter-efficient adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈355M</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>supervised fine-tuning on textualized job-feature sentences (text classification), parameter-efficient adaptation (LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>"FEAT1 is VAL1 FEAT2 is VAL2 ... FEATn is VALn, LABEL" (same textual template as other SFT models)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>FlowBench parsed textual dataset with synthetic anomaly labels across 12 workflows (same splits as BERT experiments); reported in supplementary comparisons (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>textualized job log entries derived from tabular features</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FlowBench (text modality)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (primary reported), AP, ROC-AUC, Recall@k; training time reported in supplementary materials</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in supplementary Table 6 as competitive with or better than BERT on some workflows (paper summary: "SFT-LLM models (BERT and RoBERTa-large) perform well, often among the top models"); example: for 1000 Genome RoBERTa reported slightly higher accuracy than BERT (Table 6 shows RoBERTa often improving BERT by a small margin), exact per-workflow numbers are in Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Often among top supervised performers, competitive with or outperforming GCNs and other supervised/semi-supervised baselines in many workflows (see Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>fully supervised (fine-tuned on labeled FlowBench textual examples)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher model size and compute demand; training times and resource usage larger than for BERT; still dependent on labeled data; performance varies across workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Larger compute and memory footprint than BERT (RoBERTa-large ≈355M parameters); experiments used A100 GPU; specific training times per workflow reported in supplementary Table 6 (not fully tabulated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flow-Bench: A Dataset for Computational Workflow Anomaly Detection', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7575.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7575.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT: Log anomaly detection via BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (Guo et al., 2021) using BERT-style models for anomaly detection in log data; cited in this paper as an example of using pre-trained language models for log/anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logbert: Log anomaly detection via bert</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (as used in LogBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained bidirectional Transformer encoder adapted to log-anomaly detection tasks (prior work), typically using supervised fine-tuning on labeled log anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies (depends on BERT variant used in original LogBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>supervised fine-tuning / domain adaptation of pretrained BERT on log anomaly labels (prior art cited)</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Not specified in FlowBench paper; original LogBERT constructs textual inputs from logs for model input (cited as prior art).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Public log anomaly datasets used in prior LogBERT work (cited in related work section) — not used in FlowBench experiments (this paper only cites it).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>unstructured log entries / textual logs</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>cited generally as using classification / detection metrics (AP, ROC-AUC, precision/recall) in prior work (FlowBench references Guo et al., 2021 in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>prior work used supervised fine-tuning (not zero-shot); FlowBench only cites the approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned by FlowBench only as related work to motivate LLM approaches for anomaly detection; no new failure cases discussed in FlowBench beyond general caveats about needing labels and domain differences.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flow-Bench: A Dataset for Computational Workflow Anomaly Detection', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logbert: Log anomaly detection via bert <em>(Rating: 2)</em></li>
                <li>Low-rank adaptation of large language models <em>(Rating: 2)</em></li>
                <li>BERT: pre-training of deep bidirectional transformers for language understanding <em>(Rating: 1)</em></li>
                <li>Pyod: A python toolbox for scalable outlier detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7575",
    "paper_id": "paper-259188004",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "SFT-BERT",
            "name_full": "Supervised Fine-Tuned BERT (BERT-base-uncased) with LoRA",
            "brief_description": "An encoder-only transformer (BERT-base-uncased) used as a supervised classifier by fine-tuning on textualized job-feature sentences from the FlowBench dataset; parameter-efficient fine-tuning was performed with LoRA to reduce trainable parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base-uncased",
            "model_description": "Encoder-only bidirectional Transformer (BERT) fine-tuned for binary anomaly classification; fine-tuning performed with Low-Rank Adaptation (LoRA) for parameter efficiency.",
            "model_size": "≈110M",
            "anomaly_detection_approach": "supervised fine-tuning on textual representations of tabular/job features (text classification)",
            "prompt_template": "\"FEAT1 is VAL1 FEAT2 is VAL2 ... FEATn is VALn, LABEL\" (dataset converted to sentence templates describing job features; used as supervised classification inputs)",
            "training_data": "FlowBench parsed dataset: textualized job descriptions derived from tabular features and experiment logs with synthetic anomaly labels (Normal, CPU_K, HDD_K) across 12 workflows; training/validation/test split 7:1:2.",
            "data_type": "textualized job log entries derived from tabular features (converted from job-level numeric/categorical/time-stamp features to sentences)",
            "dataset_name": "FlowBench (Flow-Bench) — parsed textual modality of the 12-workflow dataset introduced in this paper",
            "evaluation_metric": "Accuracy, Average Precision (AP), ROC-AUC, Recall@k (top-k), reported training time (sec.)",
            "performance": "Per-workflow supervised results (mean over 5 runs) example highlights from Table 3: 1000 Genome — Accuracy 0.776, AP 0.537, ROC-AUC 0.746, Recall@k 0.664, Train time 158.524s; EHT Smili — Accuracy 0.840, AP 0.399, ROC-AUC 0.657, Recall@k 0.342, Train time 26.594s; Predict Future Sales — Accuracy 0.861, AP 0.394, ROC-AUC 0.667, Recall@k 0.359, Train time 290.684s. Overall supervised BERT accuracy per-workflow range ≈0.776–0.861 in reported tables.",
            "baseline_comparison": "Compared to graph-based GCNs and unsupervised PyOD/PyGOD models: supervised BERT-SFT is frequently among the top performing models on many workflows (outperforming some GCN runs on several datasets, and outperforming unsupervised baselines such as GMM / PyOD methods); the paper reports supervised and semi-supervised methods generally outperform unsupervised PyOD/PyGOD baselines.",
            "zero_shot_or_few_shot": "fully supervised (fine-tuned on labeled FlowBench textual examples)",
            "limitations_or_failure_cases": "Requires labeled anomaly data (supervision). Performance varies strongly across workflows; some metrics (e.g., ROC-AUC) can be near random (≈0.5) for some workflows/metrics. Training cost and variability across workflow sizes noted; results reflect that different workflows and anomaly types affect performance.",
            "computational_cost": "Experiments ran on machine with NVIDIA A100 (40GB) and CPU; per-workflow BERT fine-tuning times reported in Table 3 (examples: 158.5s for 1000 Genome, 26.6s for EHT Smili, 290.7s for Predict Future Sales); LoRA reduced trainable parameters (paper reports training with 0.72% of BERT-base parameters).",
            "uuid": "e7575.0",
            "source_info": {
                "paper_title": "Flow-Bench: A Dataset for Computational Workflow Anomaly Detection",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "SFT-RoBERTa",
            "name_full": "Supervised Fine-Tuned RoBERTa-large with LoRA",
            "brief_description": "A larger encoder-style Transformer (RoBERTa-large) fine-tuned (parameter-efficiently with LoRA) on the same textualized FlowBench job-feature sentences; evaluated as a supervised anomaly classifier and reported to be competitive or superior on many workflows.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Encoder-based Transformer (RoBERTa-large) pre-trained variant of BERT family; fine-tuned for supervised anomaly classification using LoRA for parameter-efficient adaptation.",
            "model_size": "≈355M",
            "anomaly_detection_approach": "supervised fine-tuning on textualized job-feature sentences (text classification), parameter-efficient adaptation (LoRA)",
            "prompt_template": "\"FEAT1 is VAL1 FEAT2 is VAL2 ... FEATn is VALn, LABEL\" (same textual template as other SFT models)",
            "training_data": "FlowBench parsed textual dataset with synthetic anomaly labels across 12 workflows (same splits as BERT experiments); reported in supplementary comparisons (Table 6).",
            "data_type": "textualized job log entries derived from tabular features",
            "dataset_name": "FlowBench (text modality)",
            "evaluation_metric": "Accuracy (primary reported), AP, ROC-AUC, Recall@k; training time reported in supplementary materials",
            "performance": "Reported in supplementary Table 6 as competitive with or better than BERT on some workflows (paper summary: \"SFT-LLM models (BERT and RoBERTa-large) perform well, often among the top models\"); example: for 1000 Genome RoBERTa reported slightly higher accuracy than BERT (Table 6 shows RoBERTa often improving BERT by a small margin), exact per-workflow numbers are in Table 6.",
            "baseline_comparison": "Often among top supervised performers, competitive with or outperforming GCNs and other supervised/semi-supervised baselines in many workflows (see Table 6).",
            "zero_shot_or_few_shot": "fully supervised (fine-tuned on labeled FlowBench textual examples)",
            "limitations_or_failure_cases": "Higher model size and compute demand; training times and resource usage larger than for BERT; still dependent on labeled data; performance varies across workflows.",
            "computational_cost": "Larger compute and memory footprint than BERT (RoBERTa-large ≈355M parameters); experiments used A100 GPU; specific training times per workflow reported in supplementary Table 6 (not fully tabulated in main text).",
            "uuid": "e7575.1",
            "source_info": {
                "paper_title": "Flow-Bench: A Dataset for Computational Workflow Anomaly Detection",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LogBERT (related work)",
            "name_full": "LogBERT: Log anomaly detection via BERT",
            "brief_description": "Prior work (Guo et al., 2021) using BERT-style models for anomaly detection in log data; cited in this paper as an example of using pre-trained language models for log/anomaly detection.",
            "citation_title": "Logbert: Log anomaly detection via bert",
            "mention_or_use": "mention",
            "model_name": "BERT (as used in LogBERT)",
            "model_description": "Pretrained bidirectional Transformer encoder adapted to log-anomaly detection tasks (prior work), typically using supervised fine-tuning on labeled log anomalies.",
            "model_size": "varies (depends on BERT variant used in original LogBERT)",
            "anomaly_detection_approach": "supervised fine-tuning / domain adaptation of pretrained BERT on log anomaly labels (prior art cited)",
            "prompt_template": "Not specified in FlowBench paper; original LogBERT constructs textual inputs from logs for model input (cited as prior art).",
            "training_data": "Public log anomaly datasets used in prior LogBERT work (cited in related work section) — not used in FlowBench experiments (this paper only cites it).",
            "data_type": "unstructured log entries / textual logs",
            "dataset_name": null,
            "evaluation_metric": "cited generally as using classification / detection metrics (AP, ROC-AUC, precision/recall) in prior work (FlowBench references Guo et al., 2021 in related work)",
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": "prior work used supervised fine-tuning (not zero-shot); FlowBench only cites the approach.",
            "limitations_or_failure_cases": "Mentioned by FlowBench only as related work to motivate LLM approaches for anomaly detection; no new failure cases discussed in FlowBench beyond general caveats about needing labels and domain differences.",
            "computational_cost": null,
            "uuid": "e7575.2",
            "source_info": {
                "paper_title": "Flow-Bench: A Dataset for Computational Workflow Anomaly Detection",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logbert: Log anomaly detection via bert",
            "rating": 2
        },
        {
            "paper_title": "Low-rank adaptation of large language models",
            "rating": 2
        },
        {
            "paper_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "rating": 1
        },
        {
            "paper_title": "Pyod: A python toolbox for scalable outlier detection",
            "rating": 1
        }
    ],
    "cost": 0.0142585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Flow-Bench: A Dataset for Computational Workflow Anomaly Detection
13 Jun 2024</p>
<p>George Papadimitriou georgpap@isi.edu 
University of Southern California
Los AngelesCA</p>
<p>Hongwei Jin jinh@anl.gov 
Argonne National Laboratory Lemont
IL</p>
<p>Cong Wang cwang@renci.org 
Rajiv Mayani mayani@isi.edu 
University of Southern California
Los AngelesCA</p>
<p>Krishnan Raghavan kraghavan@anl.gov 
Argonne National Laboratory Lemont
IL</p>
<p>Anirban Mandal anirban@renci.org 
Prasanna Balaprakash 
Oak Ridge National Laboratory Oak Ridge
TN</p>
<p>Ewa Deelman deelman@isi.edu 
University of Southern California
Los AngelesCA</p>
<p>RENCI Chapel Hill
NC</p>
<p>Flow-Bench: A Dataset for Computational Workflow Anomaly Detection
13 Jun 2024355444886B7AA1ACA1052BAA006F6DF2arXiv:2306.09930v2[cs.DC]
A computational workflow, also known as workflow, consists of tasks that must be executed in a specific order to attain a specific goal.Often, in fields such as biology, chemistry, physics, and data science, among others, these workflows are complex and are executed in large-scale, distributed, and heterogeneous computing environments prone to failures and performance degradation.Therefore, anomaly detection for workflows is an important paradigm that aims to identify unexpected behavior or errors in workflow execution.This crucial task to improve the reliability of workflow executions can be further assisted by machine learning-based techniques.However, such application is limited, in large part, due to the lack of open datasets and benchmarking.To address this gap, we make the following contributions in this paper: (1) we systematically inject anomalies and collect raw execution logs from workflows executing on distributed infrastructures; (2) we summarize the statistics of new datasets, and provide insightful analyses; (3) we convert workflows into tabular, graph and text data, and benchmark with supervised and unsupervised anomaly detection techniques correspondingly.The presented dataset and benchmarks allow examining the effectiveness and efficiency of scientific computational workflows and identifying potential research opportunities for improvement and generalization.The dataset and benchmark code are publicly available https://poseidon-workflows.github.io/FlowBench/under the MIT License.* Equal contribution.Preprint.Under review.</p>
<p>Introduction</p>
<p>Computational workflows that run on distributed and parallel computing environments, such as Southern California Earthquake Center's (SCEC) Cybershake [Callaghan et al., 2014] and LIGO's PyCBC [Nitz et al., 2024], have become an integral part of scientific, engineering, and data science research, enabling the simulation and modeling of complex otherwise impossible complex phenomena.These fundamental challenges make the anomaly detection problem hard.Although standard anomaly detection benchmarks such as Zhao et al. [2019b] and Liu et al. [2022] can be applied to workflows.As is shown in this work, these methods are not very effective and do not scale well with the size of the workflow as well.Specifically, we show in Appendix I that many readily available anomaly detection methods result in out-of-memory (OOM) errors when encountering simple workflow data.On the other hand, the methods that do manage to work on the workflow data do not provide desirable precision.Moreover, there is still a lack of consensus on the best practices and benchmark datasets that facilitate the evaluation of different anomaly detection techniques on computational workflows, making it difficult to compare and reproduce the results of different studies.</p>
<p>These challenges indicate the need for careful analysis and study of workflow data, but the benchmarks needed for such analysis of workflows are not available in the literature.In this work, we present a set of novel workflows along with best practices and preliminary analysis that will provide the tools for future research to test and develop tools to facilitate anomaly detection on workflows.In particular, we present a benchmark study of anomaly detection techniques for workflows.We introduce a set of new workflow data that we have collected for anomaly detection and evaluate the performance of several state-of-the-art anomaly detection techniques, including tabular data anomaly detection [Zhao et al., 2019b], graph data anomaly detection [Liu et al., 2022, Kipf andWelling, 2017], and text data in supervise fine-tuned language models [Guo et al., 2021] in supervised and unsupervised way.Our evaluation is based on both supervised and unsupervised learning with several performance metrics, including accuracy, ROC-AUC score, F1-score, top-k score, etc.We compare the strengths and weaknesses of different anomaly detection techniques and reveal insights into their applicability and limitations in scientific workflow scenarios.</p>
<p>In spite of our benchmark being focused on computational workflows, it is important to note that challenges such as non-stationarity in input data distribution, differences in the number of nodes and features, the size of the data, and especially the lack of labels, provide a real-world example that can be readily utilized for the development of anomaly detection tools for the larger machine learning (ML) community.In contrast, popular benchmark tools such as PyOD [Zhao et al., 2019b] and PyGOD [Liu et al., 2022] do not provide these unique features.</p>
<p>Contributions</p>
<p>We introduce a set of workflow data that allows the careful study and development of novel anomaly detection approaches for computational workflows.We provide a comprehensive study to demonstrate the utility of our data with already available benchmarks and why novel methods are necessary.</p>
<p>Related Works</p>
<p>There are several techniques that have been proposed for anomaly detection.Statistical methods, machine learning (ML) methods, rule-based methods, and hybrid methods are the four major categories of these methods.Statistical methods formulate the anomaly detection problem as the detection of outliers in a distribution, and therefore, methods such as clustering [Sohn et al., 2023], principal component analysis (PCA) [Ma et al., 2023], and auto-regressive integrated moving average (ARIMA) models [Goldstein, 2023] are commonly utilized.On the other hand, the ML methods involve learning a classifier map between the anomaly class and the input data.These methods have been developed for both tabular and graph data, and popular methods include (graph) neural networks [Yuan et al., 2021], decision trees [Breiman, 2001], and support vector machines (SVMs) [Schölkopf et al., 2001].Hybrid methods take the probabilistic efficiency of statistical methods and combine it with the ability of ML methods to characterize complex distribution to improve the accuracy and robustness of anomaly detection [Kriegel et al., 2008].Most recently, leveraging supervised fine-tuning and training large language models (LLMs) [Vaswani et al., 2017] on anomaly-labeled text data enables them to amplify their ability to detect unusual language patterns specific to the domain, making them highly effective for anomaly detection.</p>
<p>Another class of anomaly detection methods involves rule-based methods or expert systems where a set of rules must be defined to characterize normal system behavior.These rules can then be used to detect anomalies [Liu et al., 2008] in the data.These rules can either be based on expert knowledge or derived from historical data or ML or probabilistic models that encode normal information.While a non-exhaustive list of anomaly detection methods is presented above, all these methods can be potentially used in workflows.However, it is important to note that the choice of technique(s) for anomaly detection in workflows depends on the specific requirements and characteristics of the system being monitored, as well as the goals of the anomaly detection.For instance, a graph neural network (GNN)-based approach is more suitable for workflows than methods designed for tabular data because the dependency structure present in workflow data is rather important to characterize anomalies in computational workflows.Furthermore, computational workflows are unique in the setup as each workflow differs in the number of jobs, the dependency graph, and other structures.While training a separate anomaly detection approach for each workflow is inefficient, typical approaches discussed above, except the graph neural network-based, are unsuitable when the input data dimensions change for every computational workflow.On the other hand, workflow data can be rather large, and complex ML-based approaches run out of memory when trying to detect anomalies in these datasets.</p>
<p>In the past, datasets containing system logs of large systems (e.g., BlueGene, Thunderbird) have been released to the community Oliner and Stearley [2007].However, these logs are unstructured, and they don't match the statistics and events to the workloads that were executed on these systems.Providing a dataset directly correlating system logs and the workflow application is important in creating new ML methods that better capture and identify workflow anomalies.</p>
<p>Dataset</p>
<p>The datasets used for benchmarking anomaly detection in computational workflows should represent the data types encountered in real applications.The datasets should include both normal and anomalous data to evaluate the performance of the base method.Some important statistics that should be considered when selecting datasets are:</p>
<p>• Data size: The dataset should be large enough to capture the variability in the data but not so large that it becomes computationally impractical to analyze and benchmark.</p>
<p>• Data dimensionality: The dataset's number of features or variables should represent the data types encountered in real applications.</p>
<p>• Data distribution: The dataset should represent the typical data distribution encountered in real applications.</p>
<p>Workflow Applications</p>
<p>Towards this end, we choose twelve workflow applications that are unique in their characteristics and are representative of real-world applications across multiple domains.1000 Genome Workflow The 1000 genome project [1000Genomes Project Consortium, 2012] provides a reference for human variation, having reconstructed the genomes of 2,504 individuals across 26 different populations.The test case we have here identifies mutational overlaps using data from the 1000 genomes project in order to provide a null distribution for rigorous statistical evaluation of potential disease-related mutations.This workflow (Figure 3 in Appendix B) is composed of five different tasks.</p>
<p>Montage Workflow</p>
<p>Montage is an astronomical image toolkit [Jacob et al., 2010] with components for re-projection, background matching, co-addition, and visualization of FITS files.Montage workflows typically follow a predictable structure based on the inputs, with each stage of the workflow often taking place in discrete levels separated by some synchronization/reduction tasks (mConcatFit and mBgModel).The workflow (Figure 4  Variant Calling Workflow The variant calling workflow contains all the steps to detect variations and identify changes in a DNA sequence compared to a reference genome.This workflow is adapted from the Data Carpentry Lesson -Data Wrangling and Processing for Genomics [Carpentry, 2023], that provides a pipeline to align population samples to the reference genome and detects the differences that exist.Variant calling is a very important process in bioinformatics that scientists have to perform. Figure 6 in Appendix B presents the workflow, which is composed of six different tasks.</p>
<p>CASA Workflows</p>
<p>The Collaborative Adaptive Sensing of the Atmosphere (CASA) [McLaughlin et al., 2009] has deployed a network of Doppler radars in the Dallas-Fort Worth (DFW) area, in order to improve observations, predictions and responses to hazardous weather events.The Wind Speed workflow [Lyons et al., 2019] is one of CASA's production pipelines that ingests single radar base data from all of the radars in the network and creates a gridded product representing the maximum observed wind speeds in the area.The workflow identifies areas of severe wind and correlates them with the location of critical infrastructure, such as hospitals.In case critical infrastructure is impacted, the workflow dispatches email alerts to the impacted locations.The workflow (Figure 7 in Appendix B) calculates the maximum observed wind velocity across multiple minutes of radar data, and has five different tasks.The Nowcast workflow [Lyons et al., 2019] computes short-term advection forecasts, by splitting gridded reflectivity radar data into 31 grids and computing reflectivity predictions over the next 30 minutes.Every minute new predictions need to be generated and Figure 8 in Appendix B presents a nowcast workflow that processes data across multiple minutes using three different tasks.</p>
<p>Soil Moisture Spatial Inference Engine Workflow The soil moisture spatial inference engine (SOMOSPIE) workflow processes spatial environmental data to generate training and evaluation datasets and produces machine learning models that can predict the moisture of the soil in the future.SOMOSPIE is a modular workflow with multiple stages and Figure 9 in Appendix B presents the data preparation part of the workflow to produce the training and evaluation data sets.The workflow is consisted of eight different tasks.</p>
<p>PyCBC Workflows PyCBC [Nitz et al., 2024] is a software package used to explore astrophysical sources of gravitational waves.It contains algorithms to analyze gravitational-wave data, detect coalescing compact binaries, and make bayesian inferences from gravitational-wave data collected by detectors (e.g., LIGO [Collaboration et al., 2015]).The PyCBC Inference [Biwer et al., 2019] workflow performs parameter estimation on gravitationalwave signals using Bayesian inference methods to incorporate prior knowledge and quantify uncertainties in the parameter estimates.Figure 10 in Appendix B presents the workflow that's consisted of nine different tasks.The PyCBC Search [Usman et al., 2016] workflow sifts through noisy detector data to identify gravitational waves.It's a complex workflow application that dynamically steers itself using at-runtine generated subworkflows with over fifty different tasks (Figure 11 in Appendix B).</p>
<p>EHT Workflows</p>
<p>The Event Horizon Telescope (EHT) workflows reproduce the first image of a black hole in the galaxy M87 and there are three image processing pipelines [Patel et al., 2022].</p>
<p>The EHT Difmap workflow performs image reconstruction by employing iterative convolution with difference mapping.Figure 12 in Appendix B shows the workflow with four different tasks.</p>
<p>The EHT Imaging workflow performs image reconstruction using the regularized maximum likelihood (RML) approach.Figure 13 in Appendix B shows the workflow with two different tasks.</p>
<p>The EHT Smili workflow uses the Sparse Modeling Imaging Library for Interferometry (SMILI)</p>
<p>to introduce pre-calibrated datasets and perform image reconstruction using the RML approach.Figure 14 in Appendix B shows the workflow with three different tasks.</p>
<p>Data Collection</p>
<p>To execute the workflows described above, manage their execution and collect events and statistics, we used the Pegasus Workflow Management System (WMS) [Deelman et al., 2015].</p>
<p>Workflow Model</p>
<p>In Pegasus, workflows are represented as Directed Acyclic Graphs (DAGs), where nodes represent jobs, and edges represent sequential dependencies and data dependencies between the jobs.A job can only be submitted when all its dependencies have been met.For a more formal definition, a workflow is described as a DAG G = (V, E), where Additionally, Pegasus collects provenance data and events during the execution of a workflow and associates them with the jobs that produced them.The Pegasus Panorama extensions [Deelman et al., 2017, SciTech, 2017, Papadimitriou et al., 2021] enhance the workflow DAG with features containing execution metrics and infrastructure statistics per job (v i ).These extensions enable the collection of execution traces of computational tasks, the collection of statistics of individual data transfers, the collection of infrastructure-related metrics, and all of them are stored in an Elasticsearch instance [ELK Stack, 2018].
V = {v 1 , • • • , v n } represents the set of n jobs and E ⊆ V 2 represents data dependencies between jobs. If e ij = (v i , v j ) ∈ E, job v i must complete its execution before v j can start, i.e., a directed edge v i → v j . We define succ(v i ) = {v k | (v i , v k ) ∈ E} (resp. pred(v i ) = {v k | (v k , v i ) ∈ E}) to be the successors (resp. predecessors) of the job v i ∈ V. A job (v i ) of</p>
<p>Execution Infrastructure</p>
<p>To facilitate the workflow execution, we provisioned resources at the NSF Chameleon Cloud [Keahey et al., 2020] and the FABRIC testbed [Baldin et al., 2019] (Figure 2 in Appendix A).We provisioned 4 VMs (16 Cores and 32GB RAM) on FABRIC and 3 Cascade Lake bare-metal nodes (48 Cores and 192GB RAM) on Chameleon.In FABRIC 1 had the role of submit node, 1 had the role of data node and 2 were responsible for network QoS to the workers (StarLight location).In Chameleon all nodes were Docker container executor nodes [Docker Inc., 2022] located in the same region (University of Chicago -UC).The connectivity between the two testbeds/regions was established over a high-speed layer 2 VLAN (25 Gbps).On our data node we configured an Apache HTTP server, and on our submit node we installed and configured the Pegasus WMS [Deelman et al., 2015] binaries to manage the workflow execution, alongside with HTCondor [HTCondor, 2023] to manage the baremetal resources we provisioned.To create an HTCondor pool of resources, we spawned HTCondor's Executor containers on the Docker nodes, and in the case of the Predict Future Sales workflow we created 18 containers with 8 cores and 32GB RAM each, and in all other cases we created 20 containers with 4 cores and 16GB RAM each.For the Predict Future Sales workflow, we assigned more resources to each executor container to accommodate the hyper-parameter optimization phase, which leverages multiple cores and demands more memory.Core and RAM resources were mapped to the containers using Docker's runtime options (cpuset, memory).To deliver data to the executor containers, we used the data node's HTTP server, and we capped the download and upload speed to 1Gbps per HTCondor Executor container.</p>
<p>Synthetic Anomalies</p>
<p>To introduce synthetic anomalies, we used Docker's runtime options to limit and shape the performance of the spawned container executor nodes.Through the runtime options, we were able to configure the amount of CPU time an executor container is allotted and limit the average I/O each executor container can perform.These capabilities are supported via Linux's control groups version 2 [Kernel, 2015].This approach offers sufficient isolation among the executors and allows us to obtain reproducible results from each type of experiment.</p>
<p>Table 1: Overall Dataset Statistics.We label the nodes as Normal, CPU K and HDD K.</p>
<p>Normal: No anomaly is introduced -normal conditions.</p>
<p>CPU K: M cores are advertised, but on some nodes, K cores are not allowed to be used.(K = 2, 3, 4 M = 4, 8 and K &lt; M ) HDD K: On some executor nodes, the average write speed to the disk is capped at K MB/s and the read speed at (2 × K) MB/s.(K = 5, 10) (3) the workflow management system logs; (4) an experiment log file mapping the workflow DAG executions to the injected anomalies; (5) provenance data of each job, gathered by the WMS; (6) an Elasticsearch instance with all the captured workflow events, transfer events and resource utilization traces.Since we acquired the logs and metrics using an ephemeral private cloud instance, there is no sensitive information included in the dataset.The total size of the raw logs is over 200GB.
DAG</p>
<p>Parsed Data</p>
<p>FlowBench Dataset Tabular Graph To generate the parsed version of the dataset, we combined information coming from the executable DAGs, the experiment log containing the anomaly labels, Pegasus' logs, and the events hosted in Elasticsearch (Figure 15).During parsing, a CSV log file gets generated for every workflow DAG execution and is saved under the corresponding anomaly folder.The features present in the parsed data are listed in Table 4 in the appendix with their corresponding description, and the total size of the parsed data is 783MB.More specific to text data, we prepare sentences to describe the job given the feature and values in the template of which can be used in the supervised fine-tuning of LLMs.All of our assets will be released under the MIT License.
"FEAT1 is VAL1 FEAT2 is VAL2 ... FEATn is VALn, LABEL"</p>
<p>ML Benchmarks on Computational Workflows</p>
<p>Anomaly detection in computational workflows is essential to ensure that experimental results are reliable and reproducible.A benchmark for anomaly detection in computational workflows would require selecting appropriate baseline methods and various metrics to evaluate the performance.We employ both the supervised and unsupervised approaches for the baselines due to the availability of labels in our scientific workflows.Table 5 in appendix D provides a set of selected algorithms with their details.</p>
<p>Supervised learning.Supervised anomaly detection models are trained on labeled data, where the job anomalies are already known.These models learn to identify anomalies based on the labeled data and can be used to detect anomalies in new, unlabeled data.Exampled classical methods are: support vector machine (SVM) [Hearst et al., 1998] , decision trees [Quinlan, 1986] , random forest [Breiman, 2001] , etc. Specific to the DAGs, there are (semi)supervised learning algorithms that can learn the structural and feature information simultaneously, such as GraphSAGE [Hamilton et al., 2017], GAT [Velivckovic et al., 2018], etc.</p>
<p>Unsupervised learning.Unsupervised anomaly detection models do not rely on labeled data and instead look for patterns or data points that are significantly different from the rest of the data.These methods are useful when it is difficult or impossible to obtain labeled data, which is typically the case in computational workflows.Exampled methods like: clustering (e.g., k-means [Lloyd, 1982]) , density-based methods (e.g., k-nearest neighbors [Ramaswamy et al., 2000]) , generative models (e.g., auto-encoder [Sakurada and Yairi, 2014]) , etc.In this work, we adopt the methods from PyOD [Zhao et al., 2019b], where the data are considered as tabular data, and PyGOD [Liu et al., 2022], where the data are considered as structural data, as benchmarks, reporting a set of metrics, and provide insightful analysis of the data we collected.</p>
<p>Benchmarking setup.We began by converting our parsed data from Section 3.3.1 into a PyTorch Geometric (PyG) dataset [Fey and Lenssen, 2019], where each graph G is a DAG with nodes as jobs and edges as dependencies.PyG is widely used for graph data and can be converted to PyTorch and numpy tabular format for convenience in adapting existing models.We then normalized the datasets column-wise (feature) and aligned the timestamps.Finally, we randomly split the data into training, validation, and testing sets with a ratio of 7:1:2 with tabular, graph, and text data.A detailed data processing is provided in appendix E. The experiments were conducted on a single machine that featured two AMD EPYC 7532 processors, 256GB of local RAM, and an NVIDIA A100 GPU with a capacity of 40GB RAM.Because of the different backend implementations using either scikit-learn [Pedregosa et al., 2011] or PyTorch [Paszke et al., 2019], we give priority to using the GPU with the assistance of the CPU.More detailed hyperparameter setups of each algorithm are included in appendix F.</p>
<p>Metrics and benchmark results.Table 1 shows that the binary labels in the dataset are imbalanced.Therefore, performance is evaluated using metrics such as ROC-AUC, average precision, and top-k recall scores, in which we initially set the value of k to the number of true anomalies in the test set.</p>
<p>In addition, we also provide the accuracy score for both supervised and unsupervised approaches for comparison across different methods.The experimental evaluation encompassed the twelve novel scientific workflow applications introduced in this work, employing both supervised (Table 3) and unsupervised (Table 2) learning methodologies.The reported results constitute the mean values across five independent runs.Due to space constraints, comprehensive descriptions of the benchmarks, hyperparameter configurations, and complete results are provided in Appendix I.In Table 2, we present a comparative analysis of the Graph Auto-encoder (GAE) and Gaussian Mixture Model (GMM) architectures, which operate on graph and tabular data formats, respectively.It is noteworthy that the GAE approach exhibited significantly longer training times, despite leveraging GPU acceleration, without yielding substantial performance improvements over the established methods implemented in the PyOD library.The inefficiency observed in handling large-scale workflows can be attributed to the computational complexity associated with operations such as graph completion and reconstruction, which operate on sparse graph representations.</p>
<p>In Table 3, we present a comparative evaluation of supervised learning approaches, including Graph Convolutional Networks (GCN) and Supervised Fine-Tuned (SFT) models based on the BERT architecture with Low-Rank Adaptation (LoRA) [Hu et al., 2021] for parameter-efficient training, resulting in training with 0.72% of total parameters in BERT-base-uncased pre-trained model.The GCN models operate on graph-structured data, while the SFT models leverage textual inputs.It is observed that both methodologies necessitate substantial training efforts due to the inherent complexity of the underlying architectures.Notably, the SFT models based on pre-trained model, despite their relatively compact size, achieves superior performance on certain workflows (e.g., EHT Smili) compared to their GCN counterparts.The textual data, coupled with the knowledge distilled from large-scale pre-training corpora, enables the LLMs to capture not only the topological dependencies encoded in the graph representations but also the intricate system configurations present in the textual descriptions.This synergistic approach facilitates a more comprehensive understanding of the underlying job requirements within the scientific workflows.</p>
<p>Given the results from benchmarking methods, there is a clear need to develop novel methods to advance the state of the art and improve the performance of anomaly detection methods that scales to large-scale scientific workflows.The results also highlight the importance of developing methods that can handle the complexity and heterogeneity of computational workflows, as well as the need for more research in this area to improve the reliability and reproducibility of computational workflows.</p>
<p>Generalization and Limitations</p>
<p>In this work, we provide twelve workflows, eleven of which correspond to scientific applications and one of which corresponds to a data science workflow.While it is important to develop novel methods to facilitate unsupervised anomaly detection on these workflows, the use of these workflows in developing methods for a larger set of workflow anomaly detection problems is also relevant.Therefore, an important aspect of this work is the ability of the benchmark dataset to simulate anomalies across different scientific domains.</p>
<p>Since the workflow data is modeled as a DAG, it is feasible to use the ML methods that are developed to be precise on these workflows to work with other datasets that can be represented as DAG.Therefore, one may design approaches that can be built for anomaly detection for any application that generates data represented as a DAG, including workflows that are not represented in this benchmark since a wide range of anomalies can occur in real-world computational workflows.</p>
<p>Moreover, as we provide the raw log data, researchers can use it to explore new methods and approaches for anomaly detection beyond the ones presented in this work.This can lead to the development of more accurate and robust anomaly detection algorithms that can be applied to a wider range of computational workflows.For example, taking the context of the raw logs as inputs for natural language processing (NLP) tasks, such as large language models, can enable the development of more advanced anomaly detection algorithms that can capture the complex relationships between the various components of computational workflows, beyond the scope of graphs only capture the dependence between jobs.</p>
<p>Despite their usefulness, benchmark datasets with synthetic anomalies in computational workflows have some limitations.One limitation is that they may not fully capture the complexity and variability of real-world computational workflows.Scientific and data science workflows can be highly heterogeneous and may contain anomalies that are difficult to simulate using synthetic data.Another limitation is that benchmark datasets may not reflect the specific characteristics of the data in a particular scientific domain.Anomaly detection algorithms that perform well on benchmark datasets may not necessarily perform well on real-world data in a different scientific domain.Therefore, it is important to evaluate the performance of anomaly detection algorithms on real-world computational workflows in addition to benchmark datasets.</p>
<p>Conclusion</p>
<p>In this paper, we developed benchmark datasets for anomaly detection in computational workflows.Specifically, we have introduced a new dataset for benchmarking anomaly detection techniques, which includes systematically injected anomalies and raw execution logs from workflows executing on distributed infrastructures.We have also summarized the statistics of new datasets, including twelve computational workflows.</p>
<p>Our benchmarking, including both supervised and unsupervised approaches, provides effective methods for detecting anomalies in computational workflows.However, we also highlight the need for more research in this area, particularly in developing techniques that can handle the complexity and heterogeneity of computational workflows.We hope the study provides valuable insights into the challenges of anomaly detection in computational workflows and offers a starting point for future research in this area.We hope that our benchmark dataset and analysis will be useful for researchers and practitioners working on improving the reliability and reproducibility of computational workflows.</p>
<p>Appendix</p>
<p>FlowBench code and data repository: https://github.com/PoSeiDon-Workflows/flowbench.We also provide our documentation: https://poseidon-workflows.github.io/FlowBench/.</p>
<p>A Workflow Execution Infrastructure</p>
<p>B Workflow Diagrams</p>
<p>C Parsed Features</p>
<p>Table 4 shows the features that are parsed from the Pegasus workflow logs, with the type of each feature and a brief description.</p>
<p>D Benchmark algorithms</p>
<p>Table 5 provide the selected benchmark algorithms and their references.The input X represents using features only, while A, X represents using both structure and features.</p>
<p>E Data processing</p>
<p>Followed by Table 4, we further processed the parsed data into a dataset in PyTorch-Geometric (PyG) format [Fey and Lenssen, 2019] for the graph-based algorithms.The PyG format is a standard format for graph data in deep learning, which is widely used in the graph neural network (GNN) community.The PyG format is a tuple of (X, A, y), where X is the feature matrix, A is the adjacency matrix, and y is the label vector.First, for the features with integers/strings as categories, we processed them into one-hot encoding, and then concatenate them into a feature vector X.Notice that there are multiple features with timestamps, we shifted the timestamp to the same starting point (i.e., the first timestamp</p>
<p>F Detailed benchmark settings for algorithms</p>
<p>Following in Table 5, we provide the detailed settings for each algorithm used in the benchmark.Notice that we use the default settings for all the algorithms, and we do not tune the hyperparameters for each algorithm.For the hyperparameters that are not listed below, we use the default settings in the implementation of each algorithm from PyOD and PyGOD.</p>
<p>• ABOD.N/A.</p>
<p>G Generalization</p>
<p>We are going to discuss generalization based on four points, 1) the type of applications, 2) the extensibility and reproducibility of the data collection 3) the type of injected anomalies and system anomalies, 4) problems involving of Applications: In our submission we presented 3 workflow applications that are described using an open-source workflow management tool and they are representative of the High Throughput Computing (HTC) workloads observed in the community.In previous work researchers have used similar workflows to provide a characterization of the scientific workflows seen in the wild [Juve et al., 2013], [Krawczuk et al., 2021].We selected these workflow applications for the size and complexity of their DAGs (Figure 3, Figure 4, Figure 5), the ability to acquire real execution traces within a reasonable</p>
<p>H Other Open Datasets</p>
<p>There are other open datasets that in the past, they have been used to analyze and predict system failures [Oliner and Stearley, 2007], [Zarza et al., 2010], [Zheng et al., 2011], and improve scheduling algorithms and cluster health [Wilkes, 2011], [Tirmazi et al., 2020].However, these datasets are not suitable for modeling and predicting workflow application performance since they are connections between the workflow application DAGs and the system usage, which we offer in our dataset.A dataset that observes the behavior of the workflow from the application perspective can be found in WfCommons [Coleman et al., 2022], but there are only 180 traces total that exist in the dataset, without any training labels, making the use of these data insufficient for training ML models.</p>
<p>I Supplementary Benchmarks</p>
<p>We include the complementary benchmark methods here, including semi-supervised ( DeepSAD, PReNet) methods provided from ADBench [Han et al., 2022].To expand Table 2 and 3, we report the additional results in Table 6, which reports the mean accuracy scores across 5 different runs, even though the unsupervised methods do not require labels for training.Some key observations that can be made from the table : • The supervised and semi-supervised models generally outperform the unsupervised PyOD and PyGOD models.</p>
<p>• The SFT-LLM models (BERT and RoBERTa-large) also perform well, often among the top models for each workflow.Interesting, with a relative model size of 110 M in BERT and 355 M parameters in RoBERTa-large, and parameter efficient training LoRA, is able to achieve competitive performance, showing the training time in Table 3.</p>
<p>• There are some cases where certain models fail to complete the task, indicated by "TLE" (Time Limit Exceeded) or "OOM" (Out Of Memory).This is due to the complexity of the model processing the graph, especially for the larger workflows and more runs in datasets.</p>
<p>Even with advanced hardware accelerators, e.g., NVIDIA A100 in our experiments, the model may still fail to complete the task.This is a common issue in graph-based deep learning models, particularly when processing large graphs and their entire structure.</p>
<p>• The performance of the models varies across the different workflows, highlighting the importance of evaluating on multiple real-world datasets.</p>
<p>The table provides a comprehensive comparison of the different anomaly detection models, allowing researchers to evaluate their performance across a wide range of datasets and identify the most suitable approaches for their specific use cases.In addition, due to the most accessible and widely used models from PyOD, we also present the results evaluated with top K precision scores with different models in Figure 17.It allows to compare the performance of the models across different datasets and identify the most effective models for each workflow.</p>
<p>a Pegasus workflow can be classified into one of these three categories: (1) Compute jobs describe computational tasks; (2) Transfer jobs move data to and from an execution site/node; (3) Auxiliary jobs create working directories, clean up unused data or perform other Pegasus internal bookkeeping tasks.</p>
<p>Figure 1 :
1
Figure 1: Overview of FlowBench</p>
<p>Figure 2 :
2
Figure 2: Overview of the execution infrastructure.The deployment spans across the Chameleon Cloud and the FABRIC testbed.Chameleon hosts the workers while FABRIC hosts the networking infrastructure, the workflow submission node and the data storage node.Docker containers are deployed on baremetal nodes and interference (CPU, HDD) is introduced using cgroups.An experimental controller on the workflow submission node orchestrates the anomaly injection, workflow execution triggering and data labeling.</p>
<p>Figure 3 ,
3
Figure 3, 4 and 5 are the workflow diagrams of the 1000 Genome, Montage and Predict Future Sales workflows, respectively.</p>
<p>Figure 3 :
3
Figure 3: Overview of the 1000Genome sequencing analysis workflow.The workflow creates a branch for each chromosome and each individual task is processing a subset of the Phase 3 data (equally distributed).</p>
<p>Figure 4 :
4
Figure 4: Overview of the Montage workflow.In this case, the workflow uses images captured by the Digitized Sky Survey (DSS) [Association of Universities for Research inAstronomy, 1994] and creates a branch for each band that is requested to be processed during the workflow generation.The size of the first level of each branch depends on the size of the section of the sky to be analyzed, while the second level on the number of overlapping images stored in the archive.</p>
<p>Field</p>
<p>all dependencies have been met and job can be dispatched pre_script_start ts Epoch ts when the pre-script started executing pre_script_end ts Epoch ts when the pre-script stopped executing submit ts Epoch ts when the job was submitted to the queue stage_in_start ts Epoch ts when the data stage in started stage_in_end ts Epoch ts when the data stage in ended stage_in_effective_bytes_per_sec float Bytes written per second (input data) execute_start ts Epoch ts when the execution starts execute_end ts Epoch ts when the execution ends stage_out_start ts Epoch ts when the data stage out started stage_out_end ts Epoch ts when the data stage out ended stage_out_effective_bytes_per_sec float Bytes written per second (output data) post_script_start ts Epoch ts when the post-script started executing post_script_end ts Epoch ts when the post-script ended executing wms_delay float Composite field estimating the delay introduced by the WMS while preparing the job for submission pre_script_delay float Composite field estimating the delay introduced by the pre-script queue_delay float Composite field estimating the time spent in the queue runtime float Total runtime of the job, based on execute start and end post_script_delay float Composite field estimating the delay introduced by the post-script stage_in_delay float Total time spend staging in data, based on stage in start and end stage_in_bytes float Total bytes staged in stage_out_delay float Total time spend staging out data, based on stage out start and end stage_out_bytes float Total bytes staged out kickstart_user string Name of the user submitted the job kickstart_site string Name of the execution site kickstart_hostname string Hostname of the worker node the job executed on kickstart_transformations string Mapping of the executable locations kickstart_executables string Names of the invoked executables kickstart_executables_argv string Command line arguments used to invoke the executables kickstart_executables_cpu_time</p>
<p>Figure 5 :
5
Figure5: Overview of the Predict Future Sales workflow.The workflow splits the data into 3 item categories and trains 3 XGBoost models that are later combined, using an ensemble technique.It contains 3 hyperparameter tuning subworkfows, that test different sets of features and picks the best performing one.The number of HPO tasks is configurable and depends on the number of combinations that will be tested.</p>
<p>Figure 10 :
10
Figure 10: Overview of the PyCBC Inference Workflow.The inference workflow performs parameter estimation on gravitational wave signals using prior knowledge and Bayesian approaches.</p>
<p>Figure 15 :
15
Figure 15: Converting the raw data to the parsed CSV data</p>
<p>FigureFigure 17 :
17
Figure 16: Workflow visualizations</p>
<p>in Appendix B) uses the Montage to transform astronomical images into custom mosaics, and is composed of eight different tasks.
Predict Future Sales (Data Science) Workflow The predict future sales workflow [Kaggle,2020] tackles Kaggle's predict future sales competition by receiving daily historical sales data fromJanuary 2013 to October 2015 and attempting to predict the sales for November 2015. It includespreprocessing, feature engineering, dataset segmentation based on type and sales performance,hyperparameter tuning, training of 3 XGBoost models for each group, and an ensemble techniqueto combine the model predictions into a single output file. Figure 5 in Appendix B illustrates theeleven-task workflow.</p>
<p>Table 2 :
2
Unsupervised Model Examples
WorkflowAccuracyGAE (PyGOD) AP ROC-AUC Recall@k Train time(sec.) AccuracyGMM (PyOD) AP ROC-AUC Recall@k Train time(sec.)1000 Genome.664 .334.523.1314.950.660 .343.525.1330.042CASA Nowcast.702 .213.467.04839.390.767 .259.561.1950.529CASA Wind Speed.709 .202.461.0398.978.759 .227.538.1600.085EHT Difmap.722 .214.491.0852.204.748 .226.528.1450.031EHT Imaging.730 .175.459.0361.728.817 .267.603.2650.020EHT Smili.710 .195.446.0101.847.761 .200.504.0820.026Montage.715 .199.466.04512.533.753 .221.529.1450.107Predict Future Sales.736 .183.478.0636.725.773 .200.533.1530.053PyCBC Inference.762 .181.508.1141.924.781 .194.537.1590.020PyCBC Search.722 .185.457.03111.313.759 .196.516.1260.108SOMOSPIE.723 .191.468.0473.168.761 .204.522.1300.033Variant Calling.722 .210.490.0858.832.763 .241.549.1780.081</p>
<p>Table 3 :
3
Supervised Model Examples
WorkflowAccuracyGNN (GCN) AP ROC-AUC Recall@k Train time(sec.) AccuracyLLM-SFT (BERT-base-uncased) AP ROC-AUC Recall@k Train time(sec.)1000 Genome.794 .376.555.15098.153.776 .537.746.664158.524CASA Nowcast.784 .217.502.0072413.249.796 .294.584.2083215.485CASA Wind Speed.791 .210.502.005264.580.795 .233.534.090515.584EHT Difmap.786 .215.502.00627.223.784 .282.583.23154.185EHT Imaging.817 .190.506.01411.002.825 .175.500.00019.759EHT Smili.805 .195.500.00013.392.840 .399.657.34226.594Montage.795 .207.502.0061092.069.803 .240.538.091572.677Predict Future Sales.814 .186.500.000184.743.861 .394.667.359290.684PyCBC Inference.815 .185.500.00015.316.829 .222.531.06627.950PyCBC Search.809 .191.504.014421.313.825 .255.571.166610.556SOMOSPIE.801 .199.500.00058.074.809 .279.606.27698.941Variant Calling.788 .214.505.014275.890.791 .252.552.139483.320</p>
<p>Table 4 :
4
Parsed Dataset Features.</p>
<p>Table 6 :
6
Model Comparison -with selected models from different approaches.
PyODPyGODSupervisedSemi-supervisedSFT-LLMsworkflowGMM KNN LMDD PCA GAE DONE CONAD AnomalyDAE GCN MLP DeepSAD PReNet BERT RoBERTa-large1000genome.660.691.673 .626.643.6550.677.664.794.799.708.742.776.808casa nowcast full.768.774TLE .760.746OOMOOMOOM.784.770.745.729.796.768casa wind full.757.779.793 .749.760OOM0.749OOM.791.774.755.766.795.812eht difmap.748.772TLE .752.741.772OOMOOM.786.775.767.761.784.807eht imaging.815.794.839 .728.728.8240.710.731.817.850.774.796.825.819eht smili.768.776.805 .730.742.8130.733.728.805.820.784.768.840.842montage.755.780.795 .748.735OOM0.745.766.795.828.780.773.803.805predict future sales.773.825.807 .770.790OOM0.801.825.814.839.802.826.861.813pycbc inference.793.844TLE .782.776OOMOOMOOM.815.827.819.803.829.808pycbc search.762.795.759 .763.776.7950.783.783.809.800.778.750.825.790somospie.764.799TLE .725.699OOMOOMOOM.801.795.745.740.809.810variant calling.762.773TLE .751.764OOMOOMOOM.788.819.780.797.791.778
Acknowledgments and Disclosure of FundingThis work is funded by the Department of Energy under the Integrated Computational and Data Infrastructure (ICDI) for Scientific Discovery, grant DE-SC0022328.Reference Genome IndexAlign• LSCP.Detector list with a set of LOF models on a number of neighbors:5, 10, 15, 20, 25, 30, 35, 40, 45, 50.• INNE.Max samples: 2.• GMM.N/A.• KDE.N/A.• LMDD.N/A.• MLPAE.Hidden dimension: 64, batch size: 64 learning rate: 1e-3, weight decay: 1e-5, dropout: 0.5, epochs: 200.• SCAN.ϵ = 0.5, µ = 5.• Radar.Learning rate: 1e-3.• Anomalous.Learning rate: 1e-3.• GCN.Hidden dimension: 64, batch size: 64, learning rate: 1e-3, weight decay: 0, dropout: 0.5, epochs: 200.• GraphSAGE.Hidden dimension: 64, batch size: 64, learning rate: 1e-3, weight decay: 0, dropout: 0.5, epochs: 200.• GCNAE.Hidden dimension: 64, batch size: 64 learning rate: 1e-3, weight decay: 1e-5, dropout: 0.5, epochs: 200.• Dominant.Hidden dimension: 64, batch size: 64 learning rate: 1e-3, weight decay: 1e-5, dropout: 0.5, epochs: 200.• DONE.Hidden dimension: 64, batch size: 64 learning rate: 1e-3, weight decay: 1e-5, dropout: 0.5, epochs: 200.time for each workflow run and because they are addressing real world problems.By using the binaries executed in each node of the workflow we are not claiming that we can transfer learning directly to other applications and execution systems.Since each binary can be executing different instructions and can be compiled with different optimizations in mind (e.g., infiniband drivers, gpu accelerators)[Phillips et al., 2020],[Kenny et al., 2020].Additionally, hardware differences among execution environments can lead to different node level performance, overall workflow performance and potentially different results.The majority of the workflow management systems model their workflows as DAGs[Ferreira da Silva et al., 2017],[Mitchell et al., 2019], with dependencies among jobs, creating parent and children relationships, and this dataset captures resource statistics and the behavior of the workflow, tightly coupled with each workflow step.Most datasets that currently exist do not offer this information between application and resources, because they are observing the behavior of the system, instead of the behavior of the application, or because of privacy and security policies.• Data Collection: To generate this dataset, we used resources widely available to the educational community.We used the Chameleon testbed[Keahey et al., 2020]to create our computational infrastructure.And we used publicly available workflows describedArtifacts Group 4Figure13: Overview of the EHT Imaging Workflow.The workflow performs image reconstruction using the regularized maximum likelihood (RLM) approach.HI Image Reconstruction Image PostprocessingArtifacts Group 1LO Image ReconstructionHI Image Reconstruction Image PostprocessingLO Image ReconstructionHI Image Reconstruction Image PostprocessingLO Image ReconstructionArtifacts Group 2 Artifacts Group 4Figure 14: Overview of the EHT Smili Workflow.The workflow uses the Sparse Modeling Imaging Library for Interferometry (SMILI) to incorporate pre-calibrated datasets and perform image reconstruction using the RML approach.using the Pegasus Workflow Management System[Deelman et al., 2019]and tools that are distributed with it[Papadimitriou et al., 2021].As a result, if one wants to replicate, extend, or incorporate the monitoring and collection into their workflows it's possible.Additionally, we are planning to share our recipes and code for our experimental harness for anybody that might be interested in using it.• Injected Anomalies: In our work we define anomalies as system slowdowns and not hard failures.More specifically with the CPU anomaly we aim to demonstrate a CPU slowdown due to co-location of tasks on the same host, competing for cpu cycles.We introduce this anomaly via cgroups, restricting our binaries to use only a certain number of cores available to the host.For example when a host has 4 cores and we take away 2 cores, we label this anomaly setting as "CPU_2", in order to mimic the behavior of tasks that have been mapped to the host and are taking 100% of the cpu time on the takeaway cores.This can be caused by misconfiguration or even by poor queue priority choices by the user.Similarly with the HDD anomaly we aim to demonstrate filesystem slowdown due to IO overload or filesystem health status (e.g., filesystem repair).We apply the HDD anomaly using cgroup limits and we limit the worker hosts average read and average write speeds.For example, with the HDD_10 label, we limit the read to 20MB/s and write to 10MB/s.The type of injected anomalies aim to mimic slow downs observed in production systems.For example, at National Energy Research Scientific Computing Center (NERSC) during the execution of the XFEL workflow they discovered a slow down on the shared filesystem, that was delaying or stalling the execution of the tasks[Blaschke et al., 2021].• Generalization of DAG datasets.Not only in the domain of computational workflows but also in other domains, DAGs are widely used to model the dependencies among tasks.For instance, DAGs can be used in social network analysis to model the relationships between individuals, organizations, or communities.In this domain, DAGs can help identify influential individuals or clusters, detect communities, and analyze the spread of information or behaviors.In biology, DAGs can represent gene regulatory networks, where nodes represent genes and edges represent regulatory interactions between them.This allows researchers to study the behavior of genetic systems and identify potential drug targets.In traffic management, DAGs can model traffic flow and help optimize traffic signal control, routing, and traffic assignment.In finance, DAGs can represent relationships between financial instruments, such as stocks, bonds, and currencies, allowing for portfolio optimization and risk analysis.These examples demonstrate the versatility and generalization of DAGs in various domains, making them a valuable tool for understanding and analyzing complex systems.Arning et al. [1996]X Unsupervised Linear MLPAESakurada and Yairi [2014]X Unsupervised MLP, AESCAN Xu et al. [2007]A, X Unsupervised Clustering RadarLi et al. [2017]A, X Unsupervised Decomposition AnomalousPeng et al. [2018]A, X Unsupervised Decomposition GCNAEKipf and Welling [2016]A, X Unsupervised GNN, Auto-encoder DominantDing et al. [2019]A, X Unsupervised GNN, Auto-encoder DONEBandyopadhyay et al. [2020]A, X Unsupervised MLP, Auto-encoder ADONEBandyopadhyay et al. [2020]A, X Unsupervised MLP, Auto-encoder AnomalyDAE Fan et al.[2020]A, X Unsupervised GNN, Auto-encoderGAAN Chen et al. [2020]A, X Unsupervised GAN GUIDE Yuan et al.[2021]A, X Unsupervised GNN, Auto-encoder CONADXu et al. [2022]A, X Unsupervised GNN, Auto-encoder
A global reference for human genetic variation. Nature. 52675712012</p>
<p>Ganomaly: Semi-supervised anomaly detection via adversarial training. Samet Akcay, Toby P Amir Atapour-Abarghouei, Breckon, Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision. Revised Selected Papers, Part III. Perth, AustraliaSpringerDecember 2-6, 2018. 201914</p>
<p>Fast outlier detection in high dimensional spaces. Fabrizio Angiulli, Clara Pizzuti, Principles of Data Mining and Knowledge Discovery: 6th European Conference, PKDD 2002 Helsinki. FinlandSpringerAugust 19-23. 2002. 20026</p>
<p>A linear method for deviation detection in large databases. Andreas Arning, Rakesh Agrawal, Prabhakar Raghavan, Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, KDD'96. the Second International Conference on Knowledge Discovery and Data Mining, KDD'96AAAI Press1996</p>
<p>Association of Universities for Research in Astronomy. Digitized sky survey. https://catcopy.ipac.caltech.edu/dois/doi.php?id=10.26131/IRSA4411994</p>
<p>Fabric: A national-scale programmable experimental network infrastructure. Ilya Baldin, Anita Nikolich, James Griffioen, Indermohan Inder, S Monga, Kuang-Ching Wang, Tom Lehman, Paul Ruth, 10.1109/MIC.2019.2958545IEEE Internet Computing. 2362019</p>
<p>Isolation-based anomaly detection using nearest-neighbor ensembles. Kai Ming Tharindu R Bandaragoda, David Ting, Albrecht, Tony Fei, Ye Liu, Jonathan R Zhu, Wells, Computational Intelligence. 3442018</p>
<p>Outlier resistant unsupervised deep architectures for attributed network embedding. Sambaran Bandyopadhyay, Saley Vishal Vivek, Murty, Proceedings of the 13th international conference on web search and data mining. the 13th international conference on web search and data mining2020</p>
<p>Pycbc inference: A python-based parameter estimation toolkit for compact binary coalescence signals. Publications of the Astronomical Society of the Pacific. C M Biwer, Collin D Capano, Soumi De, Miriam Cabero, Duncan A Brown, Alexander H Nitz, V Raymond, 10.1088/1538-3873/aaef0bjan 201913124503</p>
<p>Real-time xfel data analysis at slac and nersc: a trial run of nascent exascale experimental data analysis. Johannes P Blaschke, Aaron S Brewster, Daniel W Paley, Derek Mendez, Asmit Bhowmick, Nicholas K Sauter, Wilko Kröger, Murali Shankar, Bjoern Enders, Deborah Bard, 2021</p>
<p>Random forests. Leo Breiman, Machine learning. 452001</p>
<p>Lof: identifying densitybased local outliers. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, Jörg Sander, Proceedings of the 2000 ACM SIGMOD international conference on Management of data. the 2000 ACM SIGMOD international conference on Management of data2000</p>
<p>Optimizing CyberShake Seismic Hazard Workflows for Large HPC Resources. S Callaghan, P J Maechling, G Juve, K Vahi, E Deelman, T H Jordan, AGU Fall Meeting Abstracts. December 20142014</p>
<p>. Data Carpentry, 2023</p>
<p>Generative adversarial attributed network anomaly detection. Zhenxing Chen, Bo Liu, Meiqing Wang, Peng Dai, Jun Lv, Liefeng Bo, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>Wfcommons: A framework for enabling scientific workflow research and development. Tainã Coleman, Henri Casanova, Loïc Pottier, Manav Kaushik, Ewa Deelman, Rafael Ferreira Da Silva, 10.1016/j.future.2021.09.043Future Generation Computer Systems. 1282022</p>
<p>Advanced ligo. Classical and Quantum Gravity. Aasi, LIGO Scientific Collaboration10.1088/0264-9381/32/7/074001mar 20153274001</p>
<p>Pegasus: a workflow management system for science automation. Ewa Deelman, Karan Vahi, Gideon Juve, Mats Rynge, Scott Callaghan, Philip J Maechling, Rajiv Mayani, Weiwei Chen, Rafael Ferreira Da Silva, Miron Livny, Kent Wenger, 10.1016/j.future.2014.10.008Future Generation Computer Systems. 462015</p>
<p>Panorama: An approach to performance modeling and diagnosis of extreme-scale workflows. Ewa Deelman, Christopher Carothers, Anirban Mandal, Brian Tierney, Jeffrey S Vetter, Ilya Baldin, Claris Castillo, Gideon Juve, Dariusz Król, Vickie Lynch, The International Journal of High Performance Computing Applications. 3112017</p>
<p>The evolution of the pegasus workflow management software. Ewa Deelman, Karan Vahi, Mats Rynge, Rajiv Mayani, Rafael Ferreira Da Silva, George Papadimitriou, Miron Livny, 10.1109/MCSE.2019.2919690Computing in Science Engineering. 2142019</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, CoRR, abs/1810.048052018</p>
<p>Deep anomaly detection on attributed networks. Kaize Ding, Jundong Li, Rohit Bhanushali, Huan Liu, Proceedings of the 2019 SIAM International Conference on Data Mining. the 2019 SIAM International Conference on Data MiningSIAM2019</p>
<p>Anomalydae: Dual autoencoder for anomaly detection on attributed networks. Fengbin Haoyi Fan, Zuoyong Zhang, Li, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2018. 2018. 2020ELK stack</p>
<p>A characterization of workflow management systems for extreme-scale applications. Rafael Ferreira Da Silva, Rosa Filgueira, Ilia Pietri, Ming Jiang, Rizos Sakellariou, Ewa Deelman, 10.1016/j.future.2017.02.026Future Generation Computer Systems. 752017</p>
<p>Fast graph representation learning with PyTorch Geometric. Matthias Fey, Jan E Lenssen, ICLR Workshop on Representation Learning on Graphs and Manifolds. 2019</p>
<p>Special issue on unsupervised anomaly detection. Markus Goldstein, 2023</p>
<p>Histogram-based outlier score (hbos): A fast unsupervised anomaly detection algorithm. KI-2012: poster and demo track. Markus Goldstein, Andreas Dengel, 20121</p>
<p>Revisiting deep learning models for tabular data. Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, Advances in Neural Information Processing Systems. 202134</p>
<p>Logbert: Log anomaly detection via bert. Haixuan Guo, Shuhan Yuan, Xintao Wu, 2021 international joint conference on neural networks (IJCNN). IEEE2021</p>
<p>Inductive representation learning on large graphs. Rex William L Hamilton, Jure Ying, Leskovec, Proceedings of NIPS. NIPS2017</p>
<p>Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, Yue Zhao, ; David M Rocke, Advances in Neural Information Processing Systems. 2022. 200435Adbench: Anomaly detection benchmark</p>
<p>Discovering cluster-based local outliers. Zengyou He, Xiaofei Xu, Shengchun Deng, Pattern recognition letters. 249-102003</p>
<p>Support vector machines. IEEE Intelligent Systems and their applications. Marti A Hearst, Susan T Dumais, Edgar Osuna, John Platt, Bernhard Scholkopf, 199813</p>
<p>. Htcondor, 2023. Htcondor, 2023</p>
<p>Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2021</p>
<p>Montage: An Astronomical Image Mosaicking Toolkit. Joseph C Jacob, Daniel S Katz, G Bruce Berriman, John Good, Anastasia C Laity, Ewa Deelman, Carl Kesselman, Gurmeet Singh, Mei-Hui Su, Thomas A Prince, Roy Williams, Astrophysics Source Code Library. October 2010record ascl:1010.036</p>
<p>Characterizing and profiling scientific workflows. Gideon Juve, Ann Chervenak, Ewa Deelman, Shishir Bharathi, Gaurang Mehta, Karan Vahi, Future Generation Computer Systems. 293March 2013</p>
<p>Lessons learned from the chameleon testbed. Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, S Haryadi, Cody Gunawi, Joe Hammock, Alexander Mambretti, François Barnes, Alex Halbach, Joe Rocha, Stubbs, Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX Association. the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX AssociationJuly 2020</p>
<p>An evaluation of ethernet performance for scientific workloads. Joseph P Kenny, Jeremiah J Wilke, Craig D Ulmer, Gavin M Baker, Samuel Knight, Jerrold A Friesen, 10.1109/INDIS51933.2020.00011IEEE/ACM Innovating the Network for Data-Intensive Science (INDIS). 2020. 2020</p>
<p>Linux Kernel. </p>
<p>Variational graph auto-encoders. N Thomas, Max Kipf, Welling, arXiv:1611.073082016arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, Proceedings of ICLR. ICLR2017</p>
<p>A performance characterization of scientific machine learning workflows. Patrycja Krawczuk, George Papadimitriou, Ryan Tanaka, Tu Mai, Anh Do, Srujana Subramany, Shubham Nagarkar, Aditi Jain, Kelsie Lam, Anirban Mandal, Loïc Pottier, Ewa Deelman, 10.1109/WORKS54523.2021.00013IEEE/ACM Workflows in Support of Large-Scale Science. 2021. 2021</p>
<p>Angle-based outlier detection in highdimensional data. Hans-Peter Kriegel, Matthias Schubert, Arthur Zimek, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data mining2008</p>
<p>Outlier detection with kernel density functions. Jan Longin, Aleksandar Latecki, Dragoljub Lazarevic, Pokrajac, MLDM. 20077</p>
<p>Feature bagging for outlier detection. Aleksandar Lazarevic, Vipin Kumar, Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining2005</p>
<p>Radar: Residual analysis for anomaly detection in attributed networks. Jundong Li, Harsh Dani, Xia Hu, Huan Liu, IJCAI. 201717</p>
<p>Tony Fei, Kai Ming Liu, Zhi-Hua Ting, Zhou, 2008 eighth ieee international conference on data mining. IEEE2008Isolation forest</p>
<p>Bond: Benchmarking unsupervised outlier node detection on static attributed graphs. Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu Chen, Hao Peng, Kai Shu, Lichao Sun, Jundong Li, George H Chen, Zhihao Jia, Philip S Yu, Advances in Neural Information Processing Systems. 202235</p>
<p>Least squares quantization in pcm. Stuart Lloyd, IEEE transactions on information theory. 2821982</p>
<p>Toward a dynamic network-centric distributed cloud platform for scientific workflows: A case study for adaptive weather sensing. Eric Lyons, George Papadimitriou, Cong Wang, Komal Thareja, Paul Ruth, J J Villalobos, Ivan Rodero, Ewa Deelman, Michael Zink, Anirban Mandal, 10.1109/eScience.2019.00015.FundingAcknowledgments:NSF182699715th International Conference on eScience (eScience). 2019</p>
<p>An mppca-based approach for anomaly detection of structures under multiple operational conditions and missing data. Zhi Ma, Yaozhi Luo, Chung-Bang Yun, Hua-Ping Wan, Yanbin Shen, Structural Health Monitoring. 2222023</p>
<p>Short-wavelength technology and the potential for distributed networks of small radar systems. David Mclaughlin, David Pepyne, V Chandrasekar, Brenda Philips, James Kurose, Michael Zink, Kelvin Droegemeier, Sandra Cruz-Pol, Francesc Junyent, Jerald Brotzge, David Westbrook, Nitin Bharadwaj, Yanting Wang, Eric Lyons, Kurt Hondl, Yuxiang Liu, Eric Knapp, Ming Xue, Anthony Hopf ; Insanic, Stephen Frasier, Frederick Carr, 10.1175/2009BAMS2507.1Bulletin of the American Meteorological Society. 90122009</p>
<p>Exploration of workflow management systems emerging features from users perspectives. Ryan Mitchell, Loïc Pottier, Steve Jacobs, Rafael Ferreira Da Silva, Mats Rynge, Karan Vahi, Ewa Deelman, 10.1109/BigData47090.2019.9005494First International Workshop on Big Data Tools, Methods, and Use Cases for Innovative Scientific Discovery (BTSD). 2019</p>
<p>release of pycbc. Alex Nitz, Ian Harry, Duncan Brown, Christopher M Biwer, Josh Willis, Tito Dal Canton, Collin Capano, Thomas Dent, Larne Pekowsky, Gareth S Cabourn, Soumi Davies, Miriam De, Shichao Cabero, Andrew R Wu, Bernd Williamson, Duncan Machenschalk, Francesco Macleod, Prayush Pannarale, Steven Kumar, Reyes, Sumit Dfinstad, Márton Kumar, Leo Tápai, Praveen Singer, Bhooshan Kumar, Uday Varsha, Sebastian Gadre, Stephen Khan, Arthur Fairhurst, Tolley, 10.5281/zenodo.10473621v2.3.3January 2024</p>
<p>What supercomputers say: A study of five system logs. Adam J Oliner, Jon Stearley, 10.1109/DSN.2007.103The 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2007. Edinburgh, UK, ProceedingsIEEE Computer SocietyJune 2007. 2007</p>
<p>Deep anomaly detection with deviation networks. Guansong Pang, Chunhua Shen, Anton Van Den, Hengel, Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining2019</p>
<p>End-to-end online performance data capture and analysis for scientific workflows. George Papadimitriou, Cong Wang, Karan Vahi, Rafael Ferreira Da Silva, Anirban Mandal, Zhengchun Liu, Rajiv Mayani, Mats Rynge, Mariam Kiran, Vickie E Lynch, Future Generation Computer Systems. 1172021</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>Reproducibility of the first image of a black hole in the galaxy m87 from the event horizon telescope collaboration. Ria Patel, Brandan Roachell, Silvina Caíno-Lores, Ross Ketron, Jacob Leonard, Nigel Tan, Karan Vahi, Duncan A Brown, Ewa Deelman, Michela Taufer, 10.1109/MCSE.2023.3241105Computing in Science &amp; Engineering. 2452022</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 122011</p>
<p>Anomalous: A joint modeling approach for anomaly detection on attributed networks. Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, Qinghua Zheng, IJCAI. 2018</p>
<p>Scalable molecular dynamics on CPU and GPU architectures with NAMD. James C Phillips, David J Hardy, Julio D C Maia, John E Stone, João V Ribeiro, Rafael C Bernardi, Ronak Buch, Giacomo Fiorin, Jérôme Hénin, Wei Jiang, Ryan Mcgreevy, C R Marcelo, Brian K Melo, Robert D Radak, Abhishek Skeel, Yi Singharoy, Benoît Wang, Aleksei Roux, Zaida Aksimentiev, Laxmikant V Luthey-Schulten, Klaus Kalé, Christophe Schulten, Emad Chipot, Tajkhorshid, 10.1063/5.0014475The Journal of Chemical Physics. 1534July 2020</p>
<p>Induction of decision trees. J Ross Quinlan, Machine learning. 11986</p>
<p>Efficient algorithms for mining outliers from large data sets. Sridhar Ramaswamy, Rajeev Rastogi, Kyuseok Shim, Proceedings of the 2000 ACM SIGMOD international conference on Management of data. the 2000 ACM SIGMOD international conference on Management of data2000</p>
<p>Advances in neural information processing systems. Carl Rasmussen, 199912The infinite gaussian mixture model</p>
<p>Progressive image deraining networks: A better and simpler baseline. Wangmeng Dongwei Ren, Qinghua Zuo, Pengfei Hu, Deyu Zhu, Meng, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Deep semi-supervised anomaly detection. Lukas Ruff, Robert A Vandermeulen, Nico Görnitz, Alexander Binder, Emmanuel Müller, Klaus-Robert Müller, Marius Kloft, International Conference on Learning Representations. 2019</p>
<p>Learning representations by back-propagating errors. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, Nature. 32360885331986</p>
<p>Anomaly detection using autoencoders with nonlinear dimensionality reduction. Mayu Sakurada, Takehisa Yairi, Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis. the MLSDA 2014 2nd workshop on machine learning for sensory data analysis2014</p>
<p>Estimating the support of a high-dimensional distribution. Bernhard Schölkopf, John C Platt, John Shawe-Taylor, Alex J Smola, Robert C Williamson, Neural computation. 1372001. SciTech, 2017. 2017Pegasus panorama</p>
<p>A novel anomaly detection scheme based on principal component classifier. Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, Liwu Chang, 2003Miami Univ Coral Gables Fl Dept of Electrical and Computer EngineeringTechnical report</p>
<p>Anomaly clustering: Grouping images into coherent clusters of anomaly types. Kihyuk Sohn, Jinsung Yoon, Chun-Liang Li, Chen-Yu Lee, Tomas Pfister, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>the next generation. Muhammad Tirmazi, Adam Barker, Nan Deng, Md Ehtesam Haque, Zhijing Gene Qin, Steven Hand, Mor Harchol-Balter, John Wilkes, EuroSys'20. Borg; Heraklion, Crete2020</p>
<p>The pycbc search for gravitational waves from compact binary coalescence. Alexander H Samantha A Usman, Ian W Nitz, Christopher M Harry, Duncan A Biwer, Miriam Brown, Collin D Cabero, Tito Capano, Thomas Dal Canton, Stephen Dent, Marcel S Fairhurst, Drew Kehl, Badri Keppel, Amber Krishnan, Andrew Lenon, Alex B Lundgren, Larne P Nielsen, Harald P Pekowsky, Pfeiffer, Matthew Peter R Saulson, Joshua L West, Willis, 10.1088/0264-9381/33/21/215004Classical and Quantum Gravity. 3321215004oct 2016</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Graph attention networks. Petar Velivckovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, International conference on learning representations. 2018</p>
<p>More google cluster data. John Wilkes, Google research blog. Nov, 2011</p>
<p>Scan: a structural clustering algorithm for networks. Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, Thomas Aj Schweiger, Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining. the 13th ACM SIGKDD international conference on Knowledge discovery and data mining2007</p>
<p>Contrastive attributed network anomaly detection with data augmentation. Zhiming Xu, Xiao Huang, Yue Zhao, Yushun Dong, Jundong Li, Advances in Knowledge Discovery and Data Mining: 26th Pacific-Asia Conference, PAKDD 2022. Chengdu, ChinaSpringerMay 16-19, 2022. 2022Proceedings, Part II</p>
<p>Higher-order structure based anomaly detection on attributed networks. Na Xu Yuan, Shuo Zhou, Huafei Yu, Zhikui Huang, Feng Chen, Xia, 2021 IEEE International Conference on Big Data (Big Data). IEEE2021</p>
<p>Fault-tolerant routing for multiple permanent and non-permanent faults in hpc systems. G Zarza, D Lugones, D Franco, E Luque, Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA). the International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA)Las Vegas, NV, USAJuly 2010</p>
<p>Lscp: Locally selective combination in parallel outlier ensembles. Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, Zheng Li, Proceedings of the 2019 SIAM International Conference on Data Mining. the 2019 SIAM International Conference on Data MiningSIAM2019a</p>
<p>Pyod: A python toolbox for scalable outlier detection. Yue Zhao, Zain Nasrullah, Zheng Li, Journal of Machine Learning Research. 20962019b</p>
<p>Co-analysis of ras log and job log on blue gene/p. Ziming Zheng, Li Yu, Wei Tang, Zhiling Lan, Rinku Gupta, Narayan Desai, Susan Coghlan, Daniel Buettner, 10.1109/IPDPS.2011.832011 IEEE International Parallel &amp; Distributed Processing Symposium. 2011</p>            </div>
        </div>

    </div>
</body>
</html>
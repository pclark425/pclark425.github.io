<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9875 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9875</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9875</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-10d2842131634263b5a6875319ff53c0da6a7398</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/10d2842131634263b5a6875319ff53c0da6a7398" target="_blank">Agent-as-a-Judge: Evaluate Agents with Agents</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems, is introduced, an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process.</p>
                <p><strong>Paper Abstract:</strong> Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9875.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9875.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-as-a-Judge framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses agentic systems to evaluate other agentic systems, providing rich intermediate feedback by instrumenting modular components (graph, locate, read, search/retrieve, ask, memory, planning) to judge multi-step development trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>gpt-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>State-of-the-art LLM used as the backend engine for the evaluated agentic developer systems and for some judge components; used to generate and interpret trajectories and artifacts in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI/software development (code generation / automated AI development)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated agentic evaluation where an agent (with modules: graph, locate, read, search/retrieve, ask, memory, planning) inspects generated workspaces and trajectories to decide whether hierarchical requirements are satisfied; compared against human consensus and LLM-as-a-Judge baselines in black-box and gray-box settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-requirement binary judgments aggregated into metrics: Alignment Rate (percent match with human consensus), Judge Shift (deviation from human consensus), Requirements Met (independent and dependency-aware), Task Solve Rate, precision/recall via PR curves, and operational cost/time.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>DevAI — a new benchmark of 55 realistic AI development tasks with 365 hierarchical requirements and 125 preferences, arranged as directed acyclic graphs to enable intermediate-stage evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Agent-as-a-Judge outperforms LLM-as-a-Judge and aligns closely with human consensus: alignment rates in experiments ranged roughly from mid-80s to low-90s percent (examples in Table 3 show values ~83.9%–92.1% depending on agent and setting). It produced much lower Judge Shift versus human consensus (as low as ~0.27% in some cases) and yielded PR-curve performance superior to single human evaluators on some agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sensitive to noisy factual inputs (memory propagation of past errors can cascade), benefits from access to richer trajectory data (gray-box vs black-box), some modules (e.g., planning, memory) were unstable or detrimental in this proof-of-concept; requires careful context selection and is sensitive to class imbalance of positive requirement satisfaction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Matches or approaches ensemble human-consensus judgments while being far cheaper and faster: Agent-as-a-Judge cost ≈ $30.58 and took ~118 minutes vs human panel ≈ $1,297.50 and 86.5 hours. Alignment to human consensus was higher than LLM-as-a-Judge and sometimes exceeded individual human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Instrument modular judge components (graph, read, locate, ask) as these give the largest gains; use PR curves to evaluate in class-imbalanced settings; gather trajectory/context data (gray-box) when possible; employ majority-vote/consensus or debate rounds for human baselines; avoid naive use of historical memory without error correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9875.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9875.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a large language model directly as an automatic evaluator to judge outputs (here used as a baseline evaluator for developer agents), typically via prompt-based judgments without agentic intermediate modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging llm-as-a-judge with mt-bench and chatbot arena</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>gpt-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Same backend LLM used for developer agents and for LLM-as-a-Judge experiments in this paper; a contemporary high-capacity model used in a zero- or few-shot evaluation role.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI/software development (code generation / automated AI development)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompted single-LLM judgments on requirement satisfaction and task outcomes (black-box style, without modular code/trajectory inspection), used as an automatic baseline against human consensus and Agent-as-a-Judge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same per-requirement judgments aggregated into Alignment Rate, Judge Shift, Requirements Met (I/D), Task Solve Rate, and PR-curve based precision/recall analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>DevAI benchmark tasks and requirement graphs (same dataset used for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM-as-a-Judge achieved substantially lower alignment with human consensus than Agent-as-a-Judge in this benchmark: alignment rates reported around ~60–72% depending on agent and setting, and Judge Shift values as high as ~31% on some agents (Table 3), indicating larger deviations from expert consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lacks agentic intermediate modules so it misses fine-grained trajectory/evidence; struggles on dependency-aware judgments; susceptible to positioning/ prompt/context-selection issues; in class-imbalanced tasks tends to predict negatives and can produce misleading aggregate metrics unless PR curves are used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Less aligned with human consensus than Agent-as-a-Judge and the human majority vote; faster in wall-clock time but less context-aware leading to larger judge shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When using LLMs as judges, incorporate richer context/evidence selection or multi-LLM debates; evaluate with PR curves in class-imbalanced settings; consider gray-box evidence injection where feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9875.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9875.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert consensus evaluation (Human-as-a-Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual evaluation by multiple human experts judging whether hierarchical requirements are satisfied; used here as the ground-truth consensus baseline after discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI/software development (code generation / automated AI development)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Three expert human evaluators independently scored requirements (white-box access to workspaces and trajectories), followed by a debate round to reach a consensus used as the final ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-requirement satisfaction assessed with dependency-aware consideration; aggregated metrics included Requirements Met (Independent and Dependency-aware), Task Solve Rate, Self-Termination rate, and inter-annotator disagreement rates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>DevAI dataset (55 tasks, 365 hierarchical requirements).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Human consensus results: Requirements Met (independent) across three developer agents were 22.13% (MetaGPT), 44.80% (GPT-Pilot), 42.89% (OpenHands); dependency-aware Requirements Met dropped (e.g., MetaGPT 6.55%, GPT-Pilot 28.96%, OpenHands 28.68%); Task Solve Rates were very low (0.00%, 1.81%, 1.81%). Inter-evaluator disagreement ranged ~10–30%; majority-vote error vs consensus reduced to ~6.01%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High time and monetary cost (≈86.5 human hours, estimated $1,297.50 at $15/hr), notable inter-rater disagreement requiring deliberation to reach consensus, human error when missing critical intermediate signals, and limited scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Serves as the gold-standard baseline but is expensive and variable; automated judges aim to match this consensus at much lower cost/time.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use debate rounds and consensus-building; prefer larger panels when possible; use majority-vote ensemble to reduce individual errors when full consensus is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9875.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9875.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DevAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DevAI (AI Developer Dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new benchmark of 55 realistic AI development tasks for evaluating code-generating agentic systems, each task annotated with hierarchical requirements (365 total) and preferences (125 total) arranged as DAGs to permit intermediate-stage evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI/software development (code generation / automated AI development)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Tasks provide a user query plus hierarchical requirements and preferences; evaluation judges whether requirements are met (independent and dependency-aware), supporting human and automated (LLM/Agent) evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-requirement binary satisfaction, dependency-aware fulfillment (D vs I), task-level solve rate, and soft preferences; allows collection of trajectories and generated workspaces for gray-box evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>55 tasks spanning supervised learning, RL, CV, NLP, generative models; designed to be realistic, computationally moderate, and to emphasize process/ intermediate milestones rather than only final outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DevAI proved challenging: human-evaluated requirement satisfaction around ~29% for top systems (ignoring prerequisites ~44%), and only one task was fully completed by evaluated developer agents across the set, demonstrating the dataset's difficulty and utility for revealing intermediate failures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relatively small-scale tasks compared to full industrial projects; still a constrained subset of all possible development scenarios; possible bias from annotator design choices; while intentionally realistic, not exhaustive of all development workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides richer, non-sparse feedback than benchmarks that measure only final outcomes (pass@1 or final resolve rate), enabling intermediate diagnostic evaluation uncommon in many code benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use hierarchical DAG-structured requirements to capture dependencies and intermediate signals; collect trajectories and workspaces to enable gray-box automated judging; combine with PR-curve based metrics for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9875.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9875.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Metrics Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alignment Rate, Judge Shift, PR Curves, Requirements Met, Task Solve Rate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of concrete metrics used in the paper to evaluate judges and developer agents: Alignment Rate measures agreement with human consensus; Judge Shift quantifies deviation from consensus; PR curves handle class imbalance; Requirements Met (I/D) and Task Solve Rate measure functional achievement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI/software development (code generation / automated AI development)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Aggregate per-requirement binary judgments into percentage agreement (Alignment Rate), compute absolute deviations from human consensus (Judge Shift), use precision-recall curves to assess performance where positive cases are rare, and report requirement/task-level fulfillment rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Alignment Rate (percent match to human consensus), Judge Shift (absolute % deviation from consensus per metric), Precision and Recall (PR curves), Requirements Met (I: independent, D: considering dependencies), Task Solve Rate (complete task success), Self-Termination rate.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to DevAI evaluations across human, LLM-as-a-Judge, and Agent-as-a-Judge experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Metrics revealed that Agent-as-a-Judge obtains substantially higher Alignment Rates (~83–92%) and lower Judge Shift than LLM-as-a-Judge (~60–72%); PR curves were necessary because positive (requirement met) cases are rare, and simple accuracy-like metrics can be misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Metrics sensitive to class imbalance (many negative labels); single aggregate numbers (accuracy) can hide critical differences; Judge performance depends on access to trajectory/context (gray-box vs black-box) which changes metric behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These metrics allow meaningful automated-vs-human comparisons and highlight where automated judges fail (dependency reasoning, intermediate-step evidence) versus human consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer PR curves and per-requirement analyses over raw accuracy; report both independent and dependency-aware requirement fulfillment; measure Judge Shift against human consensus; use ensemble human baselines for ground truth when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9875.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9875.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Component Ablation Results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation study of Agent-as-a-Judge components (ask, graph, read, locate, retrieve)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical ablation showing incremental performance gains from specific judge components: ask (baseline), +graph, +read, +locate, +retrieve, demonstrating which capabilities most improve alignment with human consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>gpt-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>LLM backend used in agentic judge implementation and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI/software development (code generation / automated AI development)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Systematic ablation adding components sequentially and measuring Alignment Rate against human consensus on OpenHands judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Alignment Rate reported after adding components; components tested: ask, graph, read, locate, retrieve.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Ablations run on DevAI evaluation traces (OpenHands judge experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Alignment Rate progression reported: ask only = 65.03%; +graph = 75.95%; +read = 82.24%; +locate = 90.44%; +retrieve = 90.16% (marginal/no additional gain from retrieve on OpenHands), indicating locate/read/graph are key contributors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Component usefulness varies by evaluated developer agent and available trajectory data; retrieve provided little benefit in one case but was useful for other agents/trajectories (sensitivity to dataset/agent specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Adding modules allows automated judge to approach human consensus levels; demonstrates which functional capabilities are necessary to approximate human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prioritize implementing graph, read, and locate modules for agentic judges; test retrieve usefulness per domain; ablate components to understand marginal benefit for a given judge/problem set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark <em>(Rating: 2)</em></li>
                <li>ICE-Score: Instructing large language models to evaluate code <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9875",
    "paper_id": "paper-10d2842131634263b5a6875319ff53c0da6a7398",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Agent-as-a-Judge",
            "name_full": "Agent-as-a-Judge framework",
            "brief_description": "A framework that uses agentic systems to evaluate other agentic systems, providing rich intermediate feedback by instrumenting modular components (graph, locate, read, search/retrieve, ask, memory, planning) to judge multi-step development trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "gpt-4o-2024-05-13",
            "llm_description": "State-of-the-art LLM used as the backend engine for the evaluated agentic developer systems and for some judge components; used to generate and interpret trajectories and artifacts in experiments.",
            "scientific_domain": "AI/software development (code generation / automated AI development)",
            "evaluation_method": "Automated agentic evaluation where an agent (with modules: graph, locate, read, search/retrieve, ask, memory, planning) inspects generated workspaces and trajectories to decide whether hierarchical requirements are satisfied; compared against human consensus and LLM-as-a-Judge baselines in black-box and gray-box settings.",
            "evaluation_criteria": "Per-requirement binary judgments aggregated into metrics: Alignment Rate (percent match with human consensus), Judge Shift (deviation from human consensus), Requirements Met (independent and dependency-aware), Task Solve Rate, precision/recall via PR curves, and operational cost/time.",
            "benchmark_or_dataset": "DevAI — a new benchmark of 55 realistic AI development tasks with 365 hierarchical requirements and 125 preferences, arranged as directed acyclic graphs to enable intermediate-stage evaluation.",
            "results_summary": "Agent-as-a-Judge outperforms LLM-as-a-Judge and aligns closely with human consensus: alignment rates in experiments ranged roughly from mid-80s to low-90s percent (examples in Table 3 show values ~83.9%–92.1% depending on agent and setting). It produced much lower Judge Shift versus human consensus (as low as ~0.27% in some cases) and yielded PR-curve performance superior to single human evaluators on some agents.",
            "limitations_or_challenges": "Sensitive to noisy factual inputs (memory propagation of past errors can cascade), benefits from access to richer trajectory data (gray-box vs black-box), some modules (e.g., planning, memory) were unstable or detrimental in this proof-of-concept; requires careful context selection and is sensitive to class imbalance of positive requirement satisfaction.",
            "comparison_to_human_or_traditional": "Matches or approaches ensemble human-consensus judgments while being far cheaper and faster: Agent-as-a-Judge cost ≈ $30.58 and took ~118 minutes vs human panel ≈ $1,297.50 and 86.5 hours. Alignment to human consensus was higher than LLM-as-a-Judge and sometimes exceeded individual human evaluators.",
            "recommendations_or_best_practices": "Instrument modular judge components (graph, read, locate, ask) as these give the largest gains; use PR curves to evaluate in class-imbalanced settings; gather trajectory/context data (gray-box) when possible; employ majority-vote/consensus or debate rounds for human baselines; avoid naive use of historical memory without error correction.",
            "uuid": "e9875.0",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge evaluation",
            "brief_description": "Using a large language model directly as an automatic evaluator to judge outputs (here used as a baseline evaluator for developer agents), typically via prompt-based judgments without agentic intermediate modules.",
            "citation_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "mention_or_use": "use",
            "llm_name": "gpt-4o-2024-05-13",
            "llm_description": "Same backend LLM used for developer agents and for LLM-as-a-Judge experiments in this paper; a contemporary high-capacity model used in a zero- or few-shot evaluation role.",
            "scientific_domain": "AI/software development (code generation / automated AI development)",
            "evaluation_method": "Prompted single-LLM judgments on requirement satisfaction and task outcomes (black-box style, without modular code/trajectory inspection), used as an automatic baseline against human consensus and Agent-as-a-Judge.",
            "evaluation_criteria": "Same per-requirement judgments aggregated into Alignment Rate, Judge Shift, Requirements Met (I/D), Task Solve Rate, and PR-curve based precision/recall analysis.",
            "benchmark_or_dataset": "DevAI benchmark tasks and requirement graphs (same dataset used for comparisons).",
            "results_summary": "LLM-as-a-Judge achieved substantially lower alignment with human consensus than Agent-as-a-Judge in this benchmark: alignment rates reported around ~60–72% depending on agent and setting, and Judge Shift values as high as ~31% on some agents (Table 3), indicating larger deviations from expert consensus.",
            "limitations_or_challenges": "Lacks agentic intermediate modules so it misses fine-grained trajectory/evidence; struggles on dependency-aware judgments; susceptible to positioning/ prompt/context-selection issues; in class-imbalanced tasks tends to predict negatives and can produce misleading aggregate metrics unless PR curves are used.",
            "comparison_to_human_or_traditional": "Less aligned with human consensus than Agent-as-a-Judge and the human majority vote; faster in wall-clock time but less context-aware leading to larger judge shifts.",
            "recommendations_or_best_practices": "When using LLMs as judges, incorporate richer context/evidence selection or multi-LLM debates; evaluate with PR curves in class-imbalanced settings; consider gray-box evidence injection where feasible.",
            "uuid": "e9875.1",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Human-as-a-Judge",
            "name_full": "Human expert consensus evaluation (Human-as-a-Judge)",
            "brief_description": "Manual evaluation by multiple human experts judging whether hierarchical requirements are satisfied; used here as the ground-truth consensus baseline after discussion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "AI/software development (code generation / automated AI development)",
            "evaluation_method": "Three expert human evaluators independently scored requirements (white-box access to workspaces and trajectories), followed by a debate round to reach a consensus used as the final ground-truth.",
            "evaluation_criteria": "Per-requirement satisfaction assessed with dependency-aware consideration; aggregated metrics included Requirements Met (Independent and Dependency-aware), Task Solve Rate, Self-Termination rate, and inter-annotator disagreement rates.",
            "benchmark_or_dataset": "DevAI dataset (55 tasks, 365 hierarchical requirements).",
            "results_summary": "Human consensus results: Requirements Met (independent) across three developer agents were 22.13% (MetaGPT), 44.80% (GPT-Pilot), 42.89% (OpenHands); dependency-aware Requirements Met dropped (e.g., MetaGPT 6.55%, GPT-Pilot 28.96%, OpenHands 28.68%); Task Solve Rates were very low (0.00%, 1.81%, 1.81%). Inter-evaluator disagreement ranged ~10–30%; majority-vote error vs consensus reduced to ~6.01%.",
            "limitations_or_challenges": "High time and monetary cost (≈86.5 human hours, estimated $1,297.50 at $15/hr), notable inter-rater disagreement requiring deliberation to reach consensus, human error when missing critical intermediate signals, and limited scalability.",
            "comparison_to_human_or_traditional": "Serves as the gold-standard baseline but is expensive and variable; automated judges aim to match this consensus at much lower cost/time.",
            "recommendations_or_best_practices": "Use debate rounds and consensus-building; prefer larger panels when possible; use majority-vote ensemble to reduce individual errors when full consensus is infeasible.",
            "uuid": "e9875.2",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DevAI",
            "name_full": "DevAI (AI Developer Dataset)",
            "brief_description": "A new benchmark of 55 realistic AI development tasks for evaluating code-generating agentic systems, each task annotated with hierarchical requirements (365 total) and preferences (125 total) arranged as DAGs to permit intermediate-stage evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "AI/software development (code generation / automated AI development)",
            "evaluation_method": "Tasks provide a user query plus hierarchical requirements and preferences; evaluation judges whether requirements are met (independent and dependency-aware), supporting human and automated (LLM/Agent) evaluation pipelines.",
            "evaluation_criteria": "Per-requirement binary satisfaction, dependency-aware fulfillment (D vs I), task-level solve rate, and soft preferences; allows collection of trajectories and generated workspaces for gray-box evaluation.",
            "benchmark_or_dataset": "55 tasks spanning supervised learning, RL, CV, NLP, generative models; designed to be realistic, computationally moderate, and to emphasize process/ intermediate milestones rather than only final outcomes.",
            "results_summary": "DevAI proved challenging: human-evaluated requirement satisfaction around ~29% for top systems (ignoring prerequisites ~44%), and only one task was fully completed by evaluated developer agents across the set, demonstrating the dataset's difficulty and utility for revealing intermediate failures.",
            "limitations_or_challenges": "Relatively small-scale tasks compared to full industrial projects; still a constrained subset of all possible development scenarios; possible bias from annotator design choices; while intentionally realistic, not exhaustive of all development workflows.",
            "comparison_to_human_or_traditional": "Provides richer, non-sparse feedback than benchmarks that measure only final outcomes (pass@1 or final resolve rate), enabling intermediate diagnostic evaluation uncommon in many code benchmarks.",
            "recommendations_or_best_practices": "Use hierarchical DAG-structured requirements to capture dependencies and intermediate signals; collect trajectories and workspaces to enable gray-box automated judging; combine with PR-curve based metrics for evaluation.",
            "uuid": "e9875.3",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Evaluation Metrics Ensemble",
            "name_full": "Alignment Rate, Judge Shift, PR Curves, Requirements Met, Task Solve Rate",
            "brief_description": "Set of concrete metrics used in the paper to evaluate judges and developer agents: Alignment Rate measures agreement with human consensus; Judge Shift quantifies deviation from consensus; PR curves handle class imbalance; Requirements Met (I/D) and Task Solve Rate measure functional achievement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "AI/software development (code generation / automated AI development)",
            "evaluation_method": "Aggregate per-requirement binary judgments into percentage agreement (Alignment Rate), compute absolute deviations from human consensus (Judge Shift), use precision-recall curves to assess performance where positive cases are rare, and report requirement/task-level fulfillment rates.",
            "evaluation_criteria": "Alignment Rate (percent match to human consensus), Judge Shift (absolute % deviation from consensus per metric), Precision and Recall (PR curves), Requirements Met (I: independent, D: considering dependencies), Task Solve Rate (complete task success), Self-Termination rate.",
            "benchmark_or_dataset": "Applied to DevAI evaluations across human, LLM-as-a-Judge, and Agent-as-a-Judge experiments.",
            "results_summary": "Metrics revealed that Agent-as-a-Judge obtains substantially higher Alignment Rates (~83–92%) and lower Judge Shift than LLM-as-a-Judge (~60–72%); PR curves were necessary because positive (requirement met) cases are rare, and simple accuracy-like metrics can be misleading.",
            "limitations_or_challenges": "Metrics sensitive to class imbalance (many negative labels); single aggregate numbers (accuracy) can hide critical differences; Judge performance depends on access to trajectory/context (gray-box vs black-box) which changes metric behavior.",
            "comparison_to_human_or_traditional": "These metrics allow meaningful automated-vs-human comparisons and highlight where automated judges fail (dependency reasoning, intermediate-step evidence) versus human consensus.",
            "recommendations_or_best_practices": "Prefer PR curves and per-requirement analyses over raw accuracy; report both independent and dependency-aware requirement fulfillment; measure Judge Shift against human consensus; use ensemble human baselines for ground truth when possible.",
            "uuid": "e9875.4",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Component Ablation Results",
            "name_full": "Ablation study of Agent-as-a-Judge components (ask, graph, read, locate, retrieve)",
            "brief_description": "Empirical ablation showing incremental performance gains from specific judge components: ask (baseline), +graph, +read, +locate, +retrieve, demonstrating which capabilities most improve alignment with human consensus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "gpt-4o-2024-05-13",
            "llm_description": "LLM backend used in agentic judge implementation and experiments.",
            "scientific_domain": "AI/software development (code generation / automated AI development)",
            "evaluation_method": "Systematic ablation adding components sequentially and measuring Alignment Rate against human consensus on OpenHands judgments.",
            "evaluation_criteria": "Alignment Rate reported after adding components; components tested: ask, graph, read, locate, retrieve.",
            "benchmark_or_dataset": "Ablations run on DevAI evaluation traces (OpenHands judge experiments).",
            "results_summary": "Alignment Rate progression reported: ask only = 65.03%; +graph = 75.95%; +read = 82.24%; +locate = 90.44%; +retrieve = 90.16% (marginal/no additional gain from retrieve on OpenHands), indicating locate/read/graph are key contributors.",
            "limitations_or_challenges": "Component usefulness varies by evaluated developer agent and available trajectory data; retrieve provided little benefit in one case but was useful for other agents/trajectories (sensitivity to dataset/agent specifics).",
            "comparison_to_human_or_traditional": "Adding modules allows automated judge to approach human consensus levels; demonstrates which functional capabilities are necessary to approximate human judgment.",
            "recommendations_or_best_practices": "Prioritize implementing graph, read, and locate modules for agentic judges; test retrieve usefulness per domain; ablate components to understand marginal benefit for a given judge/problem set.",
            "uuid": "e9875.5",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
            "rating": 2
        },
        {
            "paper_title": "ICE-Score: Instructing large language models to evaluate code",
            "rating": 2
        }
    ],
    "cost": 0.017259749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Agent-as-a-Judge: Evaluate Agents with Agents</h1>
<p>Mingchen Zhuge ${ }^{1,2}$, Changsheng Zhao ${ }^{1}$, Dylan R. Ashley ${ }^{2}$, Wenyi Wang ${ }^{2}$, Dmitrii Khizbullin ${ }^{2}$,<br>Yunyang Xiong ${ }^{1}$, Zechun Liu ${ }^{1}$, Ernie Chang ${ }^{1}$, Raghuraman Krishnamoorthi ${ }^{1}$, Yuandong Tian ${ }^{1}$,<br>Yangyang Shi ${ }^{1}$, Vikas Chandra ${ }^{1}$, Jürgen Schmidhuber ${ }^{2}$<br>${ }^{1}$ Meta AI, ${ }^{2}$ KAUST</p>
<p>Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes-ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems-by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.</p>
<p>Date: October 18, 2024
Correspondence: mingchen.zhuge@kaust.edu.sa, cszhao@meta.com
Dataset: https://huggingface.co/devai-benchmark
Project: https://github.com/metauto-ai/agent-as-a-judge
Note: First four authors made core contributions. KAUST crafted the dataset.
Work done while Mingchen was interning at Meta, with Changsheng leading.</p>
<h2>Meta</h2>
<h2>1 Introduction</h2>
<p>Recent years have seen multimodal agentic systems move from occasionally being able to solve small toy problems to being regularly deployed for challenging real-world problems (the dream of most AI research). Yet, the current evaluation methods and the available benchmarks for agentic systems are struggling to keep up with these rapid advances, dramatically slowing true progress.</p>
<p>We believe that the current issue with evaluating agentic systems stems from the lack of feedback during the intermediate task-solving stages for these nontraditional systems. Agentic systems think more like humans, often act step-by-step (Wooldridge, 1999) and often host very human-like symbolic communications internally to solve problems (Zhuge et al., 2023). And thus agentic systems should be evaluated like a human, with rich evaluative feedback which looks at the full thought and action trajectory; evaluating an agentic system in the traditional way is like evaluating a student using multiple-choice testing-a comparatively unreliable estimator (Park, 2010). For example, while SWE-Bench (Yang et al., 2024a) is widespread, its evaluation method, which relies solely on the final resolve rate for long-term automated repair tasks, does not effectively pinpoint what is happening within agentic systems that affects the resolve rate. On the other hand, performing a better evaluation with a human is prohibitively expensive. We instead propose that agentic systems should be used to evaluate agentic systems. Inspired by LLM-as-a-Judge (Zheng et al., 2024; Fu et al., 2023; Chen et al., 2024b), which uses LLMs to evaluate LLMs, we call this framework Agent-as-a-Judge, of which it is</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 We introduce the Agent-as-a-Judge framework wherein agentic systems are used to evaluate agentic systems. We compare this to LLM-as-a-Judge, which uses LLMs to evaluate LLMs and for which Agent-as-a-Judge is a natural evolution, and Human-as-a-Judge, where skilled human labourers manually evaluate an agentic system.</p>
<p>a key extension to the world of agentic systems (see Figure 1). It not only retains the cost-effectiveness of LLM-as-a-Judge but is also equipped with agentic features, allowing it to provide rich intermediate feedback throughout the entire process, as it acts as an agentic system. We apply the Agent-as-a-Judge systems to the problem of evaluating code generating systems—one of the areas where agentic systems have looked the most promising recently.</p>
<p>In code generation, the development of benchmarks has also lagged behind the rapid advancement of agentic systems. HumanEval (chen2021eval), for example, focuses exclusively on algorithmic problems, while MBPP (Austin et al., 2021) deals with simple programming tasks. Although they are useful for evaluating the basic skills of foundation models, neither of these two reflects the most practical challenges developers face. As a step away from this, SWE-Bench (Jimenez et al., 2023) did introduce more realistic problems from GitHub, offering a fresh approach to evaluation, but still primarily focuses on automated repairs tasks development process. Concerningly, recent research shows that large language models (LLMs) can already solve over 27% of the tasks in SWE-Bench without needing of advanced agentic systems (Xia et al., 2024). Equally concerning, recent work has begun to introduce mechanisms designed specifically for the individual tasks in the SWE-Bench dataset, leading to a lack of real-world generalization and violating Goodhart’s law: "When a measure becomes a target, it ceases to be a good measure" (Goodhart, 1976).</p>
<p>To address the aforementioned issues with the current benchmarks in code generation, we introduce DevAI: the AI Developer Dataset, which contains 55 real-world comprehensive AI app development tasks created by expert annotators. We apply three leading open-source code-generating agentic frameworks to the tasks in DevAI: MetaGPT (Hong et al., 2024b), GPT-Pilot (Pythagora.io, 2023), and OpenHands (Wang et al., 2024d). We evaluate their performance using human judges (a painstaking process), LLM-as-a-Judge (Zheng et al., 2024), and our Agent-as-a-Judge framework.</p>
<p>Through human evaluation, we found that GPT-Pilot and OpenHands were each able to satisfy about 29% of the task requirements in DevAI, but only one full task—showing that DevAI presents a good level of challenge to current systems. When comparing our human judges with our automatic Agent-as-a-Judge framework, we found that Agent-as-a-Judge aligns more closely with the consensus of our human judges (90%) as compared to LLM-as-a-Judge (70%) in all cases tested. In addition, we find that it aligns more closely with this ensemble than the individual human evaluators do, suggesting that—not only is it suitable as a replacement for a human evaluator—but it could in fact be more useful than an average lone human evaluator.</p>
<p>In addition, considering the evaluation cost, Agent-as-a-Judge saves $97.72 \%$ of the time and $97.64 \%$ of the cost compared to involving three human experts.</p>
<p>In summary, the principal contributions of this work are:</p>
<ul>
<li>We release the DevAI dataset, which consists of 55 comprehensive AI development tasks with accompanying tags, individual hierarchical requirements, and individual preferences.</li>
<li>We benchmark three top open-source code generation agentic frameworks in DevAI, providing a more comprehensive analysis than previous evaluations of them.</li>
<li>We introduce the general Agent-as-a-Judge concept, allowing agentic systems a fair and rich evaluation without the traditional costs associated with human involvement.</li>
<li>We demonstrate that an Agent-as-a-Judge outperforms an LLM-as-a-Judge and performs comparably to human evaluators in our proof-of-concept.</li>
</ul>
<p>Tips: We provide a paper outline and the experimental design in Appendices A and B.</p>
<h1>2 DevAI: A Dataset for Automated AI Development</h1>
<p>In this section, we introduce our new DevAI benchmark. We then evaluate three state-of-the-art codegenerating agentic systems on this benchmark in Section 3 and present their basic statistics.</p>
<h3>2.1 Motivation</h3>
<p>Background The code generation domain is an area where agentic systems have seen significant industrial deployment over the past two years (e.g., Devin ${ }^{1}$ and Cursor $^{2}$ ). However, in code generation, there isn't yet a benchmark that accurately reflects realistic user queries for developing complete AI systems. We believe this is because of the difficulty to evaluate such complex, real-world tasks. For example, while many companies advertise their systems based on its performance on benchmarks such as SWE-Bench (Yang et al., 2024a) (for automated repair) or HumanEval (Chen et al., 2021) (for algorithmic problems), these benchmarks cover only a small bit of an actual development process. Moreover, none of them accurately reflect the intermediate stages of development or provide sufficient reward signals for long-horizon development-similar issues are present in OpenAI's recent MLE-Bench (Chan et al., 2024). A benchmark that can evaluate the entire development process-ideally in a way that can help understand the degree to which current AI methods can reduce human labour-is missing.</p>
<p>Topic We chose automated AI development as our main topic. While AI and ML tasks are often more complex, they follow clear, standard procedures. For example, data processing typically comes first in an AI pipeline, and performance reporting goes at the end. We believe this topological nature can help better monitor the development process and provide useful signals to the agentic systems.</p>
<p>Goals An ideal benchmark should address critical issues in automated development by focusing on three key factors. First, it should reflect practical software scenarios, where tasks are often too complex for a single LLM, requiring human or agentic systems. Second, it should emphasize the development process, not just final outcomes (e.g., pass@1 rates offer limited feedback and fail to highlight intermediate problems). Lastly, the evaluation should be computationally cost-effective and efficient, avoiding long training times or excessive manual oversight.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 Distribution of DevAI Tasks (1) DevAI focuses on AI development tasks and so terms such as "dataset," "model," and "results" are particularly common in the queries. (2) The first 53 tasks in DevAI all have a one-paragraph query but of varying lengths (note that task 54 and 55 are excluded here as they are outliers, representing the longest and most complex tasks in the dataset). (3) Each task has one or more tags. The prevalence of supervised learning here reflects the fact that it dominates many machine learning applications. (4) SVM classifiers (Cortes, 1995) and LSTM models (Hochreiter, 1997) are two of the most widely used architectures-a fact reflected by DevAI.</p>
<h1>2.2 The DevAI Dataset</h1>
<p>Motivated by the ideas outlined above, we propose the DevAI dataset. DevAI consists of a curated set of 55 tasks, each defined by (1) a plain text user query that describes an AI development task; (2) a set of plain text requirements (for a total of 365 requirements), each with a set of dependencies connecting them to other requirements; and (3) a set of preferences (for a total of 125 preferences) which represent softer requirements.</p>
<p>DevAI is structured so that an agentic system starts by receiving a user query to begin development. The system is then evaluated on how well it meets the requirements, with preferences serving as optional, softer criteria. An example of one of the DevAI tasks can be seen in Figure 3.</p>
<p>The tasks in DevAI are relatively small-scale but cover commonly used key development techniques. As shown in Figure 2, our tasks are tagged and cover a variety of key areas in AI: supervised learning, reinforcement learning, computer vision, natural language processing, generative models, and others. Each of the tasks is a real-world problem that could be given to a research engineer, while simultaneously being relatively inexpensive computationally to run so as to reduce the cost of evaluating a method on this benchmark. Details of the sample collection and human labeling process for DevAI are provided in Appendix E.</p>
<p>The requirements belonging to each task represent a milestone in the comprehensive development process and are arranged as a directed acyclic graph (similar to the work by He et al. (2021)), with requirements such as visualizing results depending on correct data loading and modeling. This allows for more comprehensive non-sparse feedback than a binary success metric. Furthermore, the inclusion of hierarchical requirements makes simple memorization an inadequate solution strategy, as completing the entire task requires agentic capabilities rather than relying solely on symbolic memorization, as is typical in foundation models.</p>
<h3>2.3 Preliminary Benchmark</h3>
<p>We first conduct experiments to collect development outcomes from different frameworks, which serve as baselines in the DevAI dataset. We test three of the most popular open-source frameworks (which we</p>
<h1>Query</h1>
<p>Hi! Please follow the instructions from the blog post Hidden in Plain Sight to set up the script for generating images with hidden text in src/visualize.py. Ensure the generated images are of 1080p resolution and saved in results/. Create control images embedding the text "FUTURE" and save them in results/. Please manually verify that the hidden text is embedded in the images.</p>
<h2>Requirements</h2>
<ul>
<li>R0</li>
</ul>
<p>Criteria: Follow the instructions from the blog post Hidden in Plain Sight to set up the script for generating images with hidden text in src/visualize.py.
Dependencies $\rightarrow{ }$</p>
<ul>
<li>R1</li>
</ul>
<p>Criteria: Ensure the generated images are of 1080p resolution and saved in results/.
Dependencies $\rightarrow{$ R0 $}$</p>
<ul>
<li>R2</li>
</ul>
<p>Criteria: Create control images embedding the text "FUTURE" and save them in results/.
Dependencies $\rightarrow{$ R1 $}$</p>
<h2>Preferences (Optional)</h2>
<ul>
<li>P0</li>
</ul>
<p>Criteria: The system should be capable of learning and adapting to unfamiliar technologies and tools as required.</p>
<ul>
<li>P1</li>
</ul>
<p>Criteria: After reviewing the blog post, ControlNet should successfully run on Modal to produce images with hidden messages for FUTURE.</p>
<p>Figure 3 A task example in DevAI. This task is adapted from a real-world demo given at https://www.cognitio n.ai/blog/introducing-devin. As this example shows, task requirements in DevAI are structured as a Directed Acyclic Graph (DAG), with nodes representing individual requirements and directed edges showing dependencies. More examples are in Appendix G.
will refer to as "AI developers"): MetaGPT (Hong et al., 2024b), GPT-Pilot (Pythagora.io, 2023), and OpenHands (Wang et al., 2024d) —all selected for their strong community acceptance (each having over 30,000 stars on GitHub).</p>
<p>Experiment Setup All of these three systems require a language model as a back-end engine, for which we use gpt-4o-2024-05-13, a state-of-the-art language model. These AI developers were given a time-limit of 1800 seconds to solve each task and were forcefully halted if they exceeded this time limit (we imposed this constraint, which was visible to the AI developers, as detailed in Appendix I). We capture the outputs generated during the automated development process, including code, files, and other artifacts. Additionally, we record key decisions and actions made by the agentic systems through some custom instrumentation code, resulting in a development trajectory for each of the agentic systems.
Analysis The basic statistics are shown in Table 1. MetaGPT is the most cost-efficient (1.19 USD), while OpenHands is the most expensive (6.38 USD). In terms of development time, OpenHands completes tasks</p>
<p>Table 1 Preliminary Statistics of AI Developers. We compare three leading open-source code agents using metrics such as average cost, average time, and the average number of generated files.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">MetaGPT (Hong et al., 2024b)</th>
<th style="text-align: center;">GPT-Pilot (Pythagora.io, 2023)</th>
<th style="text-align: center;">OpenHands (Wang et al., 2024d)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Basic Statistics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Version</td>
<td style="text-align: center;">Data Interpreter (Hong et al., 2024a)</td>
<td style="text-align: center;">0.2.13</td>
<td style="text-align: center;">CodeAct v1.9 (Wang et al., 2024c)</td>
</tr>
<tr>
<td style="text-align: center;">(1) Average Cost</td>
<td style="text-align: center;">$\$ 1.19$</td>
<td style="text-align: center;">$\$ 3.92$</td>
<td style="text-align: center;">$\$ 6.38$</td>
</tr>
<tr>
<td style="text-align: center;">(2) Average Time</td>
<td style="text-align: center;">775.29 s</td>
<td style="text-align: center;">1622.38 s</td>
<td style="text-align: center;">362.41 s</td>
</tr>
<tr>
<td style="text-align: center;">(3) Average Input Tokens</td>
<td style="text-align: center;">152863</td>
<td style="text-align: center;">606707</td>
<td style="text-align: center;">1252482</td>
</tr>
<tr>
<td style="text-align: center;">(4) Average Output Tokens</td>
<td style="text-align: center;">28546</td>
<td style="text-align: center;">59707</td>
<td style="text-align: center;">8457</td>
</tr>
<tr>
<td style="text-align: center;">(5) Average Saved Code Files</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">2.53</td>
</tr>
<tr>
<td style="text-align: center;">(6) Average Saved Code Lines</td>
<td style="text-align: center;">11.15</td>
<td style="text-align: center;">273.33</td>
<td style="text-align: center;">96.56</td>
</tr>
<tr>
<td style="text-align: center;">(7) Average Saved Files</td>
<td style="text-align: center;">4.42</td>
<td style="text-align: center;">5.91</td>
<td style="text-align: center;">3.60</td>
</tr>
</tbody>
</table>
<p>in an average of 362.41 s , while GPT-Pilot takes the longest at 1622.38 s . On average, a full evaluation on DevAI with one of these three took around 210.65 USD and 14 hours to perform. While running, GPT-Pilot generates the most output tokens at 59707 tokens, whereas OpenHands processed the most at 1252482 tokens while producing the fewest at 8457 tokens. This suggests that OpenHands's internal communication is more complicated but is more parsimonious in its decisions.</p>
<p>MetaGPT, while being the most cost-effective, generates fewer saved code files ( 0.42 ), suggesting it may be less inclined to save files. In contrast, GPT-Pilot generates the most saved files (3.84), reflecting a more prolific output. The difference in saved code lines, with GPT-Pilot saving 273.33 lines versus MetaGPT's 11.15, underscores GPT-Pilot's extensive output. Meanwhile, OpenHands, despite handling larger inputs, seems less focused on executing code to generate files, as evidenced by its lower file output ( 2.53 saved files). These statistics align with real user experiences (as discussed in Appendix F).</p>
<p>Evaluations Note that the results in Table 1 are not directly indicative of performance but provide valuable insights into the practical utility of DevAI and the performance of AI developers. The generated workspaces (generated files, code, etc.) and trajectories are utilized in subsequent experiments to perform evaluations using Human-as-a-Judge (section 3), LLM-as-a-Judge, and Agent-as-a-Judge (section 4).</p>
<h1>3 Human-as-a-Judge: Manual Evaluation on DevAI</h1>
<p>To determine the pragmatic validity of DevAI and to accurately estimate the actual code-generating abilities of current state-of-the-art agentic systems, in this section, we run and then manually evaluate the application of three AI developer baselines to DevAI. In Section 4, we show how this evaluation can be automated.</p>
<p>Table 2 Human-as-a-Judge for AI Developers. (I) and (D) represent independent performance versus performance considering task dependencies. indicates multiple experts evolved, and means the evaluations use white-box testing (allowing access to the generated workspace, human-collected trajectories, and open-source codebases). The results were derived from expert judgments and deliberations (see Appendix H).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">MetaGPT (Hong et al., 2024b)</th>
<th style="text-align: center;">GPT-Pilot (Pythagora.io, 2023)</th>
<th style="text-align: center;">OpenHands (Wang et al., 2024d)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">(A) Requirements Met (I)</td>
<td style="text-align: center;">$22.13 \%$</td>
<td style="text-align: center;">$44.80 \%$</td>
<td style="text-align: center;">$42.89 \%$</td>
</tr>
<tr>
<td style="text-align: left;">(B) Requirements Met (D)</td>
<td style="text-align: center;">$6.55 \%$</td>
<td style="text-align: center;">$28.96 \%$</td>
<td style="text-align: center;">$28.68 \%$</td>
</tr>
<tr>
<td style="text-align: left;">(C) Self-Termination</td>
<td style="text-align: center;">$41.81 \%$</td>
<td style="text-align: center;">$5.45 \%$</td>
<td style="text-align: center;">$54.54 \%$</td>
</tr>
<tr>
<td style="text-align: left;">(D) Task Solve Rate</td>
<td style="text-align: center;">$0.00 \%$</td>
<td style="text-align: center;">$1.81 \%$</td>
<td style="text-align: center;">$1.81 \%$</td>
</tr>
</tbody>
</table>
<h1>3.1 Benchmark Baselines by Human-as-a-Judge</h1>
<p>Human Evaluation Setup After obtaining the baseline executions and conducting basic statistical analysis, we have three expert human evaluators (referred to here by their anonymous names: 231a, 38bb, and cn90) review the outputs of AI developer baselines to assess whether each requirement was satisfied. We have two rounds of human evaluations. To capture the bias inherent in typical human evaluation (this is desirable to capture here as it represents a likely scenario in deployment), in the first round, our evaluators first discussed the basic standards but were given minimal instructions. The templates the evaluators were given for the evaluation and their self-reported post-hoc descriptions of how they resolved ambiguities are reported in Figure 12 in Appendix H.</p>
<p>After the initial round of human evaluations (which totaled an estimated total of 58 human hours), we asked our evaluators to discuss and reach a consensus on their assessments (which took an estimated total of 28.5 additional human hours). This consensus, achieved after long sessions of debate, was used as the final human evaluation result for each method.</p>
<p>Performance Analysis The results of this experiment are shown in Table 2. We found that the two bestperforming methods (GPT-Pilot and OpenHands) could satisfy about $29 \%$ of the requirements (or around $44 \%$ if prerequisites are ignored) but only on one task could they meet all the requirements. This highlights that DevAI offers a considerable but appropriate level of challenge for current and future methods. Moreover, the fulfillment of intermediate requirements aligns with our expectations, as discussed in Section 2, that DevAI provides richer feedback by uncovering how agentic systems falter during the process instead of just focusing on a single performance metric at the end.</p>
<h3>3.2 Judging Human-as-a-Judge</h3>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4 Between the three human evaluators, a large amount of disagreement was observed in their individual evaluations-highlighting the inherent unreliability of a single human evaluation.</p>
<p>Disagreement Analysis To analyze the presence of inductive bias and the reliability of the Human-as-a-Judge paradigm here, we calculate the disagreement rate between individual evaluators (shown in Figure 4). The results indicate that the disagreement rates between pairs of evaluators range from around $10 \%$ to $30 \%$.</p>
<p>Due to the complexity of a complete AI development task, which typically involves multiple steps with varying outcomes at each step, humans can easily make errors when critical information is missed, such as environment feedback indicating small but severe coding errors or bugs. Additionally, some disagreements are not necessarily incorrect but arise from differing perspectives on how ambiguity should be resolved.</p>
<p>Error Analysis As previously noted, the evaluators engaged in a round of debating after their initial evaluations until they reached a consensus on each requirement in each task (with the results of this consensus evaluation shown in Table 2).</p>
<p>In our Human-as-a-Judge pipeline, evaluators could be convinced by evidence from others and acknowledge their judgment errors, adjusting their answers accordingly. This can be used to approximate individual errors. If the consensus evaluation more accurately predicts any extant ground truth, we would expect the majority vote from the individual evaluations to more closely approximate this than any single evaluation, due to the fundamental properties of ensemble classifiers (see Hastie et al. (2009)).</p>
<p>While the consensus evaluation may not represent the absolute ground truth (we acknowledge that some quantity of error likely would still exist even after this procedure), we expect the consensus evaluation to more accurately approximate any extant ground truth (Clemen, 1989). If this holds, the majority vote should align more closely with the consensus than with any individual evaluation. As shown in Figure 5, this is the case.</p>
<p>As seen in the results, although significant errors occur among all evaluators, the majority vote effectively corrects most of these errors. Notably, cn9o made the most errors (for example, $23.77 \%$ in evaluating GPT-Pilot). After applying the majority vote from all three evaluators, the overall error rate dropped to $6.01 \%$, demonstrating the inherent benefits of majority voting.</p>
<p>Conclusion Human judgment errors are inevitable. To reduce them, we suggest two methods. First, like in this work, introduce a debate round after each judgment, where individuals present evidence and either persuade others or adjust their own opinions after discussion. This is particularly important when there are only a few evaluators, as majority voting with a small group can still lead to errors (around $5 \%$ compared to consensus evaluation, as shown in Figure 5). The second approach involves assembling a larger panel of experts (more is better when their accuracy exceeds $50 \%$ (Grofman et al., 1983)), with over 5 people recommended by Hastie and Kameda (2005); Larrick and Soll (2006), and relying on a majority vote. However, due to the high cost of engaging more experts and the fact that this is not always feasible in practice, we argue for the former.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5 Mismatch between the individual evaluations and the consensus evaluation. In particular, the majority vote classifier showed the smallest deviation from the consensus evaluation.</p>
<h1>4 Agent-as-a-Judge: Evaluating Agents with Agents</h1>
<p>Human evaluation, while somewhat reliable, is time-consuming and requires substantial expertise. To address this, we propose the Agent-as-a-Judge framework. If such an agentic system could evaluate like a human, it would reduce the need for human involvement and eliminate the trade-off between thoroughness and effort.</p>
<h3>4.1 Proof-of-Concept</h3>
<p>Based on our prior experiences with agent design and by imitating the human evaluation process, we initially designed eight modular, interacting components that form the foundation of our Proof-of-Concept for the Agent-as-a-Judge.
(1) The graph module constructs a graph that captures the entire structure of the project, including files, modules, and dependencies. It can also break down chunks of code into code snippets. (2) The locate module identifies the specific folder or file referred to by a requirement. (3) The read module goes beyond simple file parsing, supporting the reading and understanding of multimodal data across 33 different formats, including code, images, videos and documents. This allows the agent to cross-reference various data streams and verify different kinds of requirement. (4) The search module provides a contextual understanding of code and can quickly retrieve highly relevant code snippets, as well as the nuances behind them (e.g., hidden dependencies). (5) The retrieve module</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6 Initial diagram of Agent-as-a-Judge.</p>
<p>Table 3 AI Judges and Their Shift/Alignment with Human-as-a-Judge. We compare the results of LLM-as-a-Judge and Agent-as-a-Judge with Human-as-a-Judge. (I) represents performance on independent tasks, while (D) represents performance considering task dependencies. Note: $\square$ gray-box settings use carefully manually collected trajectory data (which is nearly inaccessible in practical situations, see Appendix J). In contrast, black-box setting doesn't need to access to such data. The red scores represent the absolute judge shift compared with Human-as-a-Judge (e.g., $2.74 \%)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">MetaGPT (Hong et al., 2024b)</th>
<th style="text-align: center;">GPT-Pilot (Pythagora.io, 2023)</th>
<th style="text-align: center;">OpenHands (Wang et al., 2024d)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLM-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(a) Requirements Met (I)</td>
<td style="text-align: center;">$19.39 \%(2.74 \%)$</td>
<td style="text-align: center;">$12.56 \%(32.24 \%)$</td>
<td style="text-align: center;">$11.47 \%(31.42 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(b) Requirements Met (D)</td>
<td style="text-align: center;">$1.63 \%(4.92 \%)$</td>
<td style="text-align: center;">$4.09 \%(24.87 \%)$</td>
<td style="text-align: center;">$2.18 \%(26.50 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(c) Task Solve Rate</td>
<td style="text-align: center;">$0.0 \%(0.0 \%)$</td>
<td style="text-align: center;">$0.0 \%(1.81 \%)$</td>
<td style="text-align: center;">$0.0 \%(1.81 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate $\uparrow$</td>
<td style="text-align: center;">$84.15 \%$</td>
<td style="text-align: center;">$65.30 \%$</td>
<td style="text-align: center;">$60.38 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Agent-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(I) Requirements Met (I)</td>
<td style="text-align: center;">$25.40 \%(3.26 \%)$</td>
<td style="text-align: center;">$53.00 \%(8.20 \%)$</td>
<td style="text-align: center;">$42.62 \%(0.27 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(II) Requirements Met (D)</td>
<td style="text-align: center;">$5.73 \%(0.81 \%)$</td>
<td style="text-align: center;">$39.89 \%(10.93 \%)$</td>
<td style="text-align: center;">$26.50 \%(2.17 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(III) Task Solve Rate</td>
<td style="text-align: center;">$0.0 \%(0.0 \%)$</td>
<td style="text-align: center;">$5.45 \%(3.64 \%)$</td>
<td style="text-align: center;">$1.81 \%(0.00 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate $\uparrow$</td>
<td style="text-align: center;">$88.52 \%$</td>
<td style="text-align: center;">$83.88 \%$</td>
<td style="text-align: center;">$90.44 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LLM-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(a) Requirements Met (I)</td>
<td style="text-align: center;">$28.68 \%(6.55 \%)$</td>
<td style="text-align: center;">$38.79 \%(4.10 \%)$</td>
<td style="text-align: center;">$43.16 \%(0.27 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(b) Requirements Met (D)</td>
<td style="text-align: center;">$17.75 \%(11.20 \%)$</td>
<td style="text-align: center;">$33.06 \%(4.10 \%)$</td>
<td style="text-align: center;">$32.24 \%(3.56 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(c) Task Solve Rate</td>
<td style="text-align: center;">$1.81 \%(1.81 \%)$</td>
<td style="text-align: center;">$3.63 \%(1.82 \%)$</td>
<td style="text-align: center;">$7.27 \%(5.46 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate $\uparrow$</td>
<td style="text-align: center;">$68.86 \%$</td>
<td style="text-align: center;">$71.85 \%$</td>
<td style="text-align: center;">$70.76 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Agent-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(I) Requirements Met (I)</td>
<td style="text-align: center;">$23.49 \%(1.35 \%)$</td>
<td style="text-align: center;">$46.44 \%(1.64 \%)$</td>
<td style="text-align: center;">$43.44 \%(0.54 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(II) Requirements Met (D)</td>
<td style="text-align: center;">$6.01 \%(0.54 \%)$</td>
<td style="text-align: center;">$30.60 \%(1.64 \%)$</td>
<td style="text-align: center;">$28.14 \%(0.53 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(III) Task Solve Rate</td>
<td style="text-align: center;">$0.0 \%(0.00 \%)$</td>
<td style="text-align: center;">$5.45 \%(3.64 \%)$</td>
<td style="text-align: center;">$3.63 \%(1.82 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate $\uparrow$</td>
<td style="text-align: center;">$92.07 \%$</td>
<td style="text-align: center;">$86.61 \%$</td>
<td style="text-align: center;">$90.16 \%$</td>
</tr>
<tr>
<td style="text-align: center;">/ Human-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate (38bb)</td>
<td style="text-align: center;">$92.63 \%$</td>
<td style="text-align: center;">$90.98 \%$</td>
<td style="text-align: center;">$89.89 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate (cn9o)</td>
<td style="text-align: center;">$83.33 \%$</td>
<td style="text-align: center;">$76.23 \%$</td>
<td style="text-align: center;">$78.15 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate (231a)</td>
<td style="text-align: center;">$92.07 \%$</td>
<td style="text-align: center;">$87.43 \%$</td>
<td style="text-align: center;">$89.07 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Average of individuals</td>
<td style="text-align: center;">$89.34 \%$</td>
<td style="text-align: center;">$84.88 \%$</td>
<td style="text-align: center;">$85.70 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Best of individuals</td>
<td style="text-align: center;">$92.63 \%$</td>
<td style="text-align: center;">$90.98 \%$</td>
<td style="text-align: center;">$89.89 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate (Majority Vote)</td>
<td style="text-align: center;">$95.08 \%$</td>
<td style="text-align: center;">$93.98 \%$</td>
<td style="text-align: center;">$94.26 \%$</td>
</tr>
</tbody>
</table>
<p>extracts information from long texts, identifying relevant segments in trajectories. With context from the above, (6) the ask module determines whether a given requirement is satisfied. (7) The memory module stores historical judgment information, allowing the agent to build on past evaluations. Finally, (8) the planning module plans the following actions, allowing the agent to strategize and sequence tasks based on the current state and the project goals.</p>
<p>Our initial design of the Agent-as-a-Judge, including all its components, is shown in Figure 6, and the operational process of the Agent-as-a-Judge is illustrated in Figure 9.</p>
<p>After conducting comprehensive ablation studies, we found that the modular combination of (1), (2), (3), (5), and (6) achieved the highest performance (see Appendix K). A sample of the dynamic evidence collected by the Agent-as-a-Judge is shown in Appendix M. We hypothesize this is because Agent-as-a-Judge needs high-quality factual information and is sensitive to noise. For example, while our design of the planning module introduces promising decision-making for future actions, the procedure is unstable. Initially, we hoped that historical information from the memory module would help to assess current requirements. However, it</p>
<p>proved detrimental, as any errors in previous judgments could lead to a chain of errors, negatively affecting current decisions. Besides, the current workspaces generated by developer agents, having only hundreds of lines of code, cannot fully benefit from the search module. The details of these findings are explained in Appendix K. Note that a perfect Agent-as-a-Judge is not the focus of this proof of concept, and thus, we leave the utilization of advanced agentic optimization methods for Agent-as-a-Judge, such as automated prompt optimization and workflow design (Zhuge et al., 2024; Hu et al., 2024), for future work.</p>
<h1>4.2 Judging Agent-as-a-Judge and LLM-as-a-Judge</h1>
<p>Judge Shift Judge Shift measures deviation from the Human-as-a-Judge consensus results, with lower values indicating a closer alignment. As shown in table 3, Agent-as-a-Judge consistently outperforms LLM-as-a-Judge across tasks, particularly those with task dependencies. For example, in Requirement (I), Agent-as-a-Judge shows a Judge Shift as low as $0.27 \%$, while LLM-as-a-Judge reaches $31.24 \%$ for OpenHands. This underscores Agent-as-a-Judge's stability and suitability for meeting task requirements. Furthermore, in the gray-box setting, both Agent-as-a-Judge and LLM-as-a-Judge show even better results than their performance in the black-box setting.</p>
<p>Alignment Rate The Alignment Rate reflects how closely the AI Judges' evaluations align with human consensus across all 365 requirements. It is defined as the percentage of requirement evaluations that are the same as the Human-as-a-Judge consensus evaluation. Compared to LLM-as-a-Judge, Agent-as-a-Judge consistently achieves a higher Alignment Rate, closely matching human judgments. For example, when evaluating OpenHands, Agent-as-aJudge reaches $92.07 \%$ and $90.44 \%$, surpassing LLM-as-a-Judge's $70.76 \%$ and $60.38 \%$ in both gray-box and black-box settings. This shows that Agent-as-aJudge produces more accurate and human-aligned evaluations, especially in complex scenarios.</p>
<p>PR Curves Judging developer agents is a classimbalanced task, where meeting requirements is much rarer than failing. Metrics like judge shift and alignment rate can be misleading. For example, since MetaGPT rarely meets requirements, LLM-as-a-Judge easily identifies most cases as negative (achieving $84.15 \%$ in the black-box setting). PR Curves offer a clearer performance measure by balancing precision and recall. Agent-as-a-Judge even outperforms any single human evaluator on OpenHands and aligns closest with majority voting. This shows that, in some cases, Agent-as-a-Judge can nearly replace human evaluators.</p>
<h3>4.3 Ablations For Agent-as-a-Judge</h3>
<p>We conduct ablations to evaluate the impact of adding different components on Agent-as-a-Judge's performance. The components analyzed include ask, graph, read, locate, and retrieve. The component ablation study for Agent-as-a-Judge reveals key insights into the performance gains from adding specific functionalities.</p>
<p>With only the ask component, the agent achieves a $65.03 \%$ alignment rate. Adding the graph component increases performance to $75.95 \%$, as the agent can better understand the relationships between files.</p>
<p>The introduction of read further improves the alignment rate to $82.24 \%$, reflecting the value of direct access to the contents of the file. Incorporating locate brings a substantial boost to $90.44 \%$, as the agent can efficiently target files relevant</p>
<p>Figure 7 PR Curves comparing judge Methods.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Table 4 Component Ablation Studies for Agent-as-a-Judge. We analyze the impact of adding various components (ask, graph, read, locate, and retrieve) on the performance of Agent-as-aJudge for judging OpenHands.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">+ ask + graph + read + locate + retrieve</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Agent-as-a-Judge Performance</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate</td>
<td style="text-align: center;">$65.03 \%$ $75.95 \%$ $82.24 \%$ $90.44 \%$ $90.16 \%$</td>
</tr>
</tbody>
</table>
<p>to the requirements. However, adding retrieve does not provide a significant benefit in this case. In contrast, as shown in Table 3, the judgment of MetaGPT and GPT-Pilot indicates that retrieve is useful, as the trajectory provides additional valuable information.</p>
<h1>4.4 Cost Analysis</h1>
<p>The Human-as-a-Judge took the three evaluators a self-reported total of 86.5 hours. With a 15 USD minimum wage (assuming this would buy a subject expert in AI), a full evaluation under DevAI would cost around 1297.50 USD. In comparison, Agent-as-a-Judge cost only 30.58 USD in API calls and took only 118.43 minutes $-2.29 \%$ of the cost and $2.36 \%$ of the time of Human-as-a-Judge. LLM-as-a-Judge was faster at 10.99 minutes, but due to the absence of intelligent context selection by the Agent-as-a-Judge's modules, it still cost 29.63 USD.</p>
<h2>5 Related Work</h2>
<p>Agentic systems and their applications are highly active research areas with numerous recent works having a relation to this work. This section details those works most relevant to ours. We provide a treatment of the less relevant related works in Appendix D.</p>
<p>AI Developers AI in software development is growing fast (Liu et al., 2024). AI-driven developers have been applied to directly imitate software companies (Hong et al., 2024b; Qian et al., 2024a), debug code (Yang et al., 2024a), run data science methods (Guo et al., 2024; Hong et al., 2024a; Li et al., 2024; Qiao et al., 2023), and even write academic papers (Lu et al., 2024a).</p>
<p>Benchmarks for AI developments Benchmarks like MLAgentBench (Huang et al., 2024), ML-Bench (Liu et al., 2023d), SUPER (Bogin et al., 2024), DS-bench (Jing et al., 2024), and MLE-Bench (Chan et al., 2024) all focus on benchmarking agentic systems using AI tasks. However, DevAI distinguishes itself from all of these by focusing on realistic user queries that target a complete development cycle. It further includes a more comprehensive evaluation with multiple hierarchical requirements and preferences for each task. Comparatively, MLAgentBench (Huang et al., 2024) for example, focuses on final performance for a limited set of well-known tasks, which risks overfitting and fails to assess a system's generalization or adaptability.
AI Judges Several works have looked at using AI systems as judges ${ }^{3}$. The work by Chan et al. (2023); Zhao et al. (2024), for example, extends LLM-as-a-Judge to have multiple LLMs in their evaluation process for conversational tasks. Unlike Agent-as-a-Judge, they employ a trivial agentic system and apply it only to evaluate LLMs under traditional evaluation setups. In contrast, (Lu et al., 2024b) uses a single LLM-based evaluator but, unlike LLM-as-a-Judge, applies this to multimodal tasks rather than just for evaluating LLMs. Less relevant are frameworks like those by Chen et al. (2024a); Arora et al. (2024); Mündler et al. (2024), where intermediate signals are used during coding development.</p>
<h2>6 Discussion and Conclusion</h2>
<p>Outlook 1: Intermediate Feedback for Agentic Self-Improvement A key power of the Agent-as-a-Judge, though not fully exploited here but nonetheless clear, is that it provides intermediate feedback that is essential for effective and efficient optimization (Zhuge et al., 2024; Pan et al., 2024). For example, Agarwal et al. (2019) proposes to solve the sparse reward problem in reinforcement learning, by learning auxiliary reward functions that provide intermediate feedback. Perhaps the greatest strength of the Agent-as-a-Judge framework is that an agentic system can use it to identify and fix issues in its solutions to complex, multistage problems on the fly-something older, delayed-feedback methods did not permit. By introducing Agent-as-a-Judge, we create the opportunity to build a process-supervised reward model (PRM) for improving agentic systems (Lightman et al., 2023).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Outlook 2: Flywheel Effect Driven by Agent-as-a-Judge The cycle of mutual improvement between the Agent-as-a-Judge and the evaluated agents, where both evolve together through iterative feedback, presents a promising direction. We hypothesize that an agentic version of a self-play system (Zelikman et al., 2022; Chen et al., 2024e; Wang et al., 2024b), could viably emerge by using the Agent-as-a-Judge as a key mechanism. Furthermore, the ongoing interaction between the Agent-as-a-Judge and the evaluated agents has the potential to create a flywheel effect, where successive incremental improvements reinforce one another, leading to progressively greater optimization and enhanced performance over time (Wang et al., 2022). This iterative process may also serve as a valuable complement to LLM reasoning data, help embedding agentic capabilities into foundation models (Luo et al., 2024).</p>
<p>Conclusion In this work, we introduced the Agent-as-a-Judge method to use agentic systems to evaluate agentic systems. We simultaneously released DevAI: a new benchmark that evaluates the code-generating ability of agentic systems on complete AI development tasks when used with Agent-as-a-Judge. We went on to show that Agent-as-a-Judge outperforms existing methods on this task and that it performs similarly to an ensemble of expert human evaluators. Altogether, we believe that the above opens the door for scaling up agentic far more than before.</p>
<h1>Acknowledgements</h1>
<p>The authors thank Haozhe Liu, Piotr Piekos, Firas Laakom, Matteo Paltenghi for their suggestions or paper review. The research reported in this publication was supported by funding from the King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI under award number 5940 and the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence.</p>
<h2>References</h2>
<p>Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. Learning to generalize from sparse and underspecified rewards. In International conference on machine learning, pages 130-140. PMLR, 2019.</p>
<p>Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan. Masai: Modular architecture for software-engineering ai agents. arXiv preprint arXiv:2406.11638, 2024.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021 .</p>
<p>Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. arXiv preprint arXiv:2406.18403, 2024.</p>
<p>Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar Khot. Super: Evaluating agents on setting up and executing tasks from research repositories. arXiv preprint arXiv:2409.07440, 2024.</p>
<p>Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. Multipl-e: a scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):3675-3691, 2023.</p>
<p>Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Jacob Ginesin, Edward Berman, George Chakhnashvili, Anton Lozhkov, Carolyn Jane Anderson, and Arjun Guha. Can it edit? evaluating the ability of large language models to follow code editing instructions, 2024. URL https://arxiv.org/abs/2312.12450.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.</p>
<p>Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal A. Patwardhan, Lilian Weng, and Aleksander Mkadry. Mle-bench: Evaluating machine learning agents on machine learning engineering. 2024. URL https://api.semanticscholar.org/CorpusID:273233550.</p>
<p>Harrison Chase. LangChain. https://github.com/hwchase17/langchain, 2022.
Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, et al. Coder: Issue resolving with multi-agent and task graphs. arXiv preprint arXiv:2406.01304, 2024a.</p>
<p>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv:2402.04788, 2024b.</p>
<p>Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, et al. Gui-world: A dataset for gui-oriented multimodal llm-based agents. arXiv preprint arXiv:2406.10819, 2024c.</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? a study on judgement biases. arXiv preprint arXiv:2402.10669, 2024d.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024e.</p>
<p>Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the new autodiff-unlocking efficient optimization of computational workflows. arXiv preprint arXiv:2406.16218, 2024.</p>
<p>Robert T Clemen. Combining forecasts: A review and annotated bibliography. International journal of forecasting, 5 (4):559-583, 1989 .</p>
<p>Corinna Cortes. Support-vector networks. Machine Learning, 1995.
Yijiang River Dong, Tiancheng Hu, and Nigel Collier. Can llm be a personalized judge? arXiv preprint arXiv:2406.11657, 2024.</p>
<p>Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, and Cheng Yang. Multi-agent software development through cross-team collaboration. arXiv preprint arXiv:2406.08979, 2024.</p>
<p>Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth. From data mining to knowledge discovery in databases. AI magazine, 17(3):37-37, 1996.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>Charles Goodhart. Monetary relationships: a view from Threadneedle Street. University of Warwick, 1976.
Significant Gravitas. Auto-gpt. GitHub repository, 2023.
Bernard Grofman, Guillermo Owen, and Scott L Feld. Thirteen theorems in search of the truth. Theory and decision, 15(3):261-278, 1983.</p>
<p>Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. Ds-agent: Automated data science by empowering large language models with case-based reasoning. arXiv preprint arXiv:2402.17453, 2024.</p>
<p>Md Mahim Anjum Haque. Fixeval: Execution-based evaluation of program fixes for competitive programming problems. PhD thesis, Virginia Tech, 2023.</p>
<p>Reid Hastie and Tatsuya Kameda. The robust beauty of majority rules in group decisions. Psychological review, 112 (2):494, 2005.</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2 edition, 2009. doi: 10.1007/978-0-387-84858-7.</p>
<p>Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art. Knowledge-based systems, 212: 106622, 2021.</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.</p>
<p>S Hochreiter. Long short-term memory. Neural Computation MIT-Press, 1997.
Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, et al. Data interpreter: An llm agent for data science. arXiv preprint arXiv:2402.18679, 2024a.</p>
<p>Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024b.</p>
<p>Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024.
Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010, 2023.</p>
<p>Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, 2024.</p>
<p>Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.</p>
<p>Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. arXiv preprint arXiv:2406.06769, 2024.</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.</p>
<p>Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. From llms to llm-based agents for software engineering: A survey of current, challenges and future. arXiv preprint arXiv:2408.02479, 2024.</p>
<p>Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. Dsbench: How far are data science agents to becoming data science experts? arXiv preprint arXiv:2409.07703, 2024.</p>
<p>Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023.</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.</p>
<p>Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pages 18319-18345. PMLR, 2023.
langchain ai. LangGraph. https://github.com/langchain-ai/langgraph, 2024.
Richard P Larrick and Jack B Soll. Intuitions about combining opinions: Misappreciation of the averaging principle. Management science, 52(1):111-127, 2006.</p>
<p>V Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Proceedings of the Soviet physics doklady, 1966.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023.</p>
<p>Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033, 2024.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378 (6624):1092-1097, 2022.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yiling Lou. Large language model-based agents for software engineering: A survey. arXiv preprint arXiv:2409.02977, 2024.</p>
<p>Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023a.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023b.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023c.</p>
<p>Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Zengxian Yang, Kaikai An, et al. Ml-bench: Large language models leverage open-source libraries for machine learning tasks. arXiv preprint arXiv:2311.09835, 2023d.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024a.</p>
<p>Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation. Advances in Neural Information Processing Systems, 36, 2024b.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, and Weizhu Chen. Arena learning: Build data flywheel for llms post-training via simulated chatbot arena. arXiv preprint arXiv:2407.10627, 2024.</p>
<p>Niels Mündler, Mark Niklas Müller, Jingxuan He, and Martin Vechev. Code agents are state of the art software testers. arXiv preprint arXiv:2406.12952, 2024.</p>
<p>Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. In First Conference on Language Modeling, 2024.</p>
<p>Jooyong Park. Constructive multiple-choice testing system. British Journal of Educational Technology, 41(6):1054-1064, 2010. doi: https://doi.org/10.1111/j.1467-8535.2010.01058.x. URL https://bera-journals.onlinelibrary.wile y.com/doi/abs/10.1111/j.1467-8535.2010.01058.x.</p>
<p>Huy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui. Hyperagent: Generalist software engineering agents to solve coding tasks at scale. arXiv preprint arXiv:2409.16299, 2024.</p>
<p>Pythagora.io. Gpt-pilot: Your ai copilot for software development. https://github.com/Pythagora-io/gpt-pilot, 2023. URL https://github.com/Pythagora-io/gpt-pilot. GitHub repository.</p>
<p>Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15174-15186, 2024a.</p>
<p>Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Scaling large-language-model-based multi-agent collaboration. arXiv preprint arXiv:2406.07155, 2024b.</p>
<p>Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang Dong, Jue Zhang, Lu Wang, et al. Taskweaver: A code-first agent framework. arXiv preprint arXiv:2311.17541, 2023.</p>
<p>Vyas Raina, Adian Liusie, and Mark Gales. Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. arXiv preprint arXiv:2402.14016, 2024.</p>
<p>N Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.
Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333-389, 2009.</p>
<p>Lin Shi, Weicheng Ma, and Soroush Vosoughi. Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms. arXiv preprint arXiv:2406.07791, 2024.</p>
<p>Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, and Chi Wang. Adaptive in-conversation team building for language model agents. arXiv preprint arXiv:2405.19425, 2024.</p>
<p>Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. arXiv preprint arXiv:2403.03186, 2024.</p>
<p>Wei Tao, Yucheng Zhou, Wenqiang Zhang, and Yu Cheng. Magis: Llm-based multi-agent framework for github issue resolution. arXiv preprint arXiv:2403.17927, 2024.</p>
<p>Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. arXiv preprint arXiv:2406.12624, 2024.</p>
<p>Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Debugbench: Evaluating debugging capability of large language models. arXiv preprint arXiv:2401.04621, 2024.</p>
<p>Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, and Neel Sundaresan. Autodev: Automated ai-driven development. arXiv preprint arXiv:2403.08299, 2024.</p>
<p>Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024a.</p>
<p>Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024b.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030, 2024c.</p>
<p>Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024d.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022.</p>
<p>Rüdiger Wirth and Jochen Hipp. Crisp-dm: Towards a standard process model for data mining. In Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining, volume 1, pages 29-39. Manchester, 2000.</p>
<p>Michael Wooldridge. Intelligent agents. Multiagent systems: A modern approach to distributed artificial intelligence, 1: $27-73,1999$.</p>
<p>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.</p>
<p>Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, and Qingyun Wu. Stateflow: Enhancing llm task-solving through state-driven workflows. arXiv preprint arXiv:2403.11322, 2024a.</p>
<p>Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024b.</p>
<p>Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024.</p>
<p>Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, and Guohao Li. Can large language model agents simulate human trust behaviors? arXiv preprint arXiv:2402.04559, 2024.</p>
<p>Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024.</p>
<p>Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, et al. Crab: Cross-environment agent benchmark for multimodal language model agents. arXiv preprint arXiv:2407.01511, 2024.</p>
<p>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024a.</p>
<p>Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.</p>
<p>Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, et al. Matplotagent: Method and evaluation for llm-based agentic scientific data visualization. arXiv preprint arXiv:2402.11453, 2024b.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.</p>
<p>Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023.</p>
<p>Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, and Lidong Bing. Auto arena of llms: Automating llm evaluations with agent peer-battles and committee discussions. arXiv preprint arXiv:2405.20267, 2024.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. Codebertscore: Evaluating code generation with pretrained models of code. arXiv preprint arXiv:2302.05527, 2023a.</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023b.</p>
<p>Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, et al. Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870, 2023c.</p>
<p>Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024 .</p>
<p>Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural languagebased societies of mind. arXiv preprint arXiv:2305.17066, 2023.</p>
<p>Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jurgen Schmidhuber. Language agents as optimizable graphs. arXiv preprint arXiv:2402.16823, 2024.</p>
<p>Terry Yue Zhuo. Ice-score: Instructing large language models to evaluate code. arXiv preprint arXiv:2304.14317, 2023.
Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024.</p>
<h1>A Outline of this Paper</h1>
<h2>Paper: Agent-as-a-Judge: Evaluating Agents with Agents</h2>
<h2>Key Logic</h2>
<ul>
<li>Step 1: Concept Proposal</li>
</ul>
<p>Description: We propose the Agent-as-a-Judge concept, an extension of the LLM-as-a-Judge framework, aimed at evaluating agentic systems using other agentic systems.</p>
<ul>
<li>Step 2: Dataset Creation</li>
</ul>
<p>Description: To address the lack of suitable datasets for evaluating agentic systems in automated AI development, we introduce DevAI, a new dataset consisting of 55 realistic AI code generation tasks. This also serves as a testbed for the Agent-as-a-Judge proof-of-concept.</p>
<ul>
<li>Step 3: Baseline Evaluation of Developer Agents (Experiment Level 1)</li>
</ul>
<p>Description: In the first level of experiments, we select three popular open-source developer agents: MetaGPT, GPT-Pilot, and OpenHands. These agents are evaluated on the DevAI tasks to establish performance baselines.</p>
<ul>
<li>Step 4: Conducting Human-as-a-Judge Evaluation</li>
</ul>
<p>Description: We conduct a Human-as-a-Judge experiment, where three human experts assess the performance of the developer agents on the same DevAI tasks.</p>
<ul>
<li>Step 5: Human-as-a-Judge Analysis (Experiment Level 2)</li>
</ul>
<p>Description: In the second level of experiments, we statistically analyze the results of Human-as-a-Judge evaluations, focusing on the costs of human labor and potential biases, highlighting the challenges of relying on human evaluation for complex tasks.</p>
<ul>
<li>Step 6: Agent-as-a-Judge Implementation</li>
</ul>
<p>Description: We design and implement the Agent-as-a-Judge proof-of-concept to evaluate code generation on the DevAI dataset. This system incorporates modules such as graph, search, read, and ask, providing multi-dimensional evaluation metrics.</p>
<ul>
<li>Step 7: Comparing AI Judge Systems (Experiment Level 3)</li>
</ul>
<p>Description: In the third level of experiments, we compare three judgment systems: Agent-as-a-Judge, LLM-as-a-Judge, and Human-as-a-Judge, all applied to the same DevAI tasks. Our results show that Agent-as-a-Judge performs comparably to human evaluators and surpasses LLM-as-a-Judge in more complex reasoning and evaluation tasks.</p>
<h2>Future Directions</h2>
<ul>
<li>Direction 1: Intermediate Feedback for Agentic Self-Improvement</li>
</ul>
<p>Description: Agent-as-a-Judge offers intermediate feedback, crucial for reinforcement learning, where rewards are sparse but vital for improvement. It also enables real-time issue identification and resolution in complex, multi-stage tasks, overcoming the limitations of delayed feedback.</p>
<ul>
<li>Direction 2: Flywheel Effect Driven by Agent-as-a-Judge</li>
</ul>
<p>Description: The iterative feedback cycle between the Agent-as-a-Judge and evaluated agents (such as Developer Agents here) could create a flywheel effect, where mutual improvements lead to progressively greater optimization. This dynamic could drive an agentic self-play system and complement LLM reasoning data to embed agentic features into foundation models.</p>
<p>Figure 8 We Outline the Logical Flow of the Agent-as-a-Judge Framework.</p>
<h1>B Experiment Designs</h1>
<p>This section outlines the experimental designs aimed at evaluating developer agents' performance, analyzing human evaluations, and comparing AI-based judging systems. The experiments are structured across three levels, as illustrated below.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<h2>B. 1 Summary of Experiments</h2>
<p>The experiments are categorized into three levels as follows:</p>
<h2>Level 1</h2>
<p>Experiment 1a: Basic performance statistics for developer agents (Section 2.3)
Experiment 1b: Human evaluations of developer agents (Section 3.1)</p>
<h2>Level 2</h2>
<p>Experiment 2a: Error analysis of human evaluations (Section 3.2)</p>
<h2>Level 3</h2>
<p>Experiment 3a: AI judge baselines (Section 4.2)
Experiment 3b: Ablation studies for Agent-as-a-Judge (Section 4.3)</p>
<h2>B. 2 Judges and Subjects of Evaluation</h2>
<p>The following table summarizes the judge and the subject being evaluated in each experiment:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">Who is the Judge?</th>
<th style="text-align: center;">Who is being Judged?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Section 2.3</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Developer Agents</td>
</tr>
<tr>
<td style="text-align: center;">Section 3.1</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Developer Agents</td>
</tr>
<tr>
<td style="text-align: center;">Section 3.2</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: center;">Section 4.2</td>
<td style="text-align: center;">(1) LLM-as-a-Judge</td>
<td style="text-align: center;">(1) Developer Agents</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(2) Agent-as-a-Judge</td>
<td style="text-align: center;">(2) Developer Agents</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(3) Human</td>
<td style="text-align: center;">(3) LLM-as-a-Judge</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(4) Human</td>
<td style="text-align: center;">(4) Agent-as-a-Judge</td>
</tr>
<tr>
<td style="text-align: center;">Section 4.3</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Agent-as-a-Judge</td>
</tr>
</tbody>
</table>
<h1>C Agent-as-a-Judge Pipeline</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9 The pipelines of developer agents and judge agent. Some materials in this figure are from original blog (https://www.factsmachine.ai/p/hidden-in-plain-sight).</p>
<h1>D Extend Related Work</h1>
<p>Our main paper includes mostly related works of AI developers, Benchmarks for AI developments, and AI judges. However, the following works contribute significantly to the community and also relate to this work. We record this work as additional related work.</p>
<p>LLM-based Autonomous Agents Recent developments in LLM-based agents have expanded their capabilities beyond simple task execution to more autonomous problem-solving and decision-making. AutoGPT (Gravitas, 2023) and LangChain (Chase, 2022) provide frameworks for single-agent systems that leverage external tools for more complex tasks. Similarly, research such as CAMEL (Li et al., 2023), MetaGPT (Hong et al., 2024b), NLSOM (Zhuge et al., 2023), AutoGen (Wu et al., 2023) focus on role-based multi-agent communication, improving collaboration among agents. However, the challenge of maintaining coherence in agents' dialogue and preventing hallucination remains prominent (Du et al., 2024; Zhou et al., 2023c). Besides, Agent-trust (Xie et al., 2024) examines if LLM agents, like GPT-4, can simulate human trust behaviors in Trust Games, showing that they can align with human behavior.</p>
<p>Most recently, using graphs to build agents has gained prominence. Earlier work like GPTSwarm (Zhuge et al., 2024) and LangGraph (langchain ai, 2024) proposed using nodes to represent operations and edges to represent the connections between them. In GPTSwarm, multiple agents represented as subgraphs in a graph are connected by optimizable edges, and reinforcement learning is employed to optimize the edges. Following this approach, several agent frameworks have incorporated graphs into their designs (Hong et al., 2024a; Zhou et al., 2024; Qian et al., 2024b). Additionally, various optimization methods have been developed to enhance agent performance further (Wu et al., 2024a; Song et al., 2024; Hu et al., 2024). In practical applications, many studies focus on understanding and interacting with GUIs (Wang et al., 2024a; Chen et al., 2024c; Yang et al., 2023; Xu et al., 2024; Tan et al., 2024; Wu et al., 2024b). For code generation agents (Jin et al., 2024), current research mainly emphasizes automated repair (Yang et al., 2024a; Phan et al., 2024; Tao et al., 2024), computational modular design (Khattab et al., 2023; Cheng et al., 2024), and automated development (Tufano et al., 2024; Huang et al., 2023). Among these, open-sourced frameworks like OpenHands (Wang et al., 2024d) have gained popularity due to their strong user experience. Moreover, scientific discovery (Jansen et al., 2024; Lu et al., 2024a) and ML agents (Yang et al., 2024b) are also receiving increased attention.</p>
<p>LLM-as-a-Judge In the domain of AI evaluation and judgment, frameworks (Zheng et al., 2024; Fu et al., 2023) have pioneered the use of LLMs to assess conversational agents, demonstrating how LLMs can evaluate dialogue quality and consistency. LLM-as-a-judge has also expanded into the multimodal domain, providing clear visual-language feedback (Chen et al., 2024b; Xiong et al., 2024). ICE-Score (Zhuo, 2023) improves on metrics like CodeBERTScore (Zhou et al., 2023a) and G-EVAL Liu et al. (2023c) by better aligning with human preferences and functional correctness. Expanding beyond dialogue, LLMs like CodeR (Chen et al., 2024a) and MASAI (Arora et al., 2024) apply similar judging principles to the code validation process, where AI systems autonomously evaluate and verify computer programs. Our work builds on these advancements by exploring how LLMs can perform more nuanced judgment tasks, further investigating their potential in decision-making across various domains. Recent research also focuses on judging LLM-as-a-Judges (Chen et al., 2024d; Bavaresco et al., 2024; Thakur et al., 2024; Dong et al., 2024; Shi et al., 2024; Raina et al., 2024).</p>
<p>Coding Benchmarks Recent advances in code generation have led to the innovation of various benchmarks to evaluate model performance (Liu et al., 2024). Early benchmarks, such as MBPP (Austin et al., 2021), HumanEval (Chen et al., 2021), and MultiPL-E (Cassano et al., 2023), focus primarily on generating simple functions. While these benchmarks are useful for evaluating the correctness of generated code, they are limited in complexity and do not fully represent the challenges encountered in real-world software development.</p>
<p>As the field progressed, newer benchmarks began to focus on more complex and realistic tasks. APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022), and LiveCodeBench (Jain et al., 2024) moved toward competitive programming challenges that involve advanced algorithms and data structures. These tasks are more representative of problems encountered in coding competitions and help push models toward more sophisticated problem-solving. DS-1000 (Lai et al., 2023) was introduced to assess the skills of models with data science libraries, evaluating their ability to use APIs and execute complex data analysis workflows.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Additionally, we were pleased to find that a recent industry blog (https://www.cognition.ai/blog/evaluating-coding-ag ents), published 3 weeks before our submission, shares very similar ideas and provides further evidence that the Agent-as-a-Judge could have practical applications in agent systems.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
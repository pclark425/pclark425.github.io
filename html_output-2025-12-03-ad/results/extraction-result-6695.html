<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6695 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6695</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6695</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-18e0cdc75e017b6112d674c3bd0ac5b3e35e4f82</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/18e0cdc75e017b6112d674c3bd0ac5b3e35e4f82" target="_blank">Case-based reasoning for better generalization in textual reinforcement learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A general method inspired by case-based reasoning to train agents and generalize out of the training distribution, which consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state- of-the-art results on widely used environments.</p>
                <p><strong>Paper Abstract:</strong> Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efficiently, especially under distributional shifts. In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The case-based reasoner collects instances of positive experiences from the agent's interaction with the world in the past and later reuses the collected experiences to act efficiently. The method can be applied in conjunction with any existing on-policy neural agent in the literature for TBGs. Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state-of-the-art results on widely used environments.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6695.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6695.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case-Based Reasoning Retriever/Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented decision module that stores past successful context-action cases (key-value pairs) and retrieves the most similar past action to reuse in the current state via symbolic mapping; keys are discrete context codes produced by vector quantization of graph-based context embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CBR (retriever + reuse + revise pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A modular CBR component that (i) encodes an action-specific context from a state graph using seeded graph attention over BERT node features, (ii) discretizes context embeddings via learned vector quantization into KD codes (keys), (iii) stores key -> action (value) pairs for positive-reward experiences, (iv) retrieves nearest-key matches by discrete-code overlap and reuses actions via symbolic template mapping, falling back to a neural policy if reuse is invalid.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (pretrained) for node/entity encodings; A2C-based neural policy used as fallback</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC), Jericho (interactive fiction)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value store (case memory) with discrete keys via vector quantization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>KD codes (K-way, D-dimensional discrete codes) derived from continuous context embeddings; values are actions or context-action pairs (templates + entities); variants store continuous context embeddings in some ablations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>retain on positive reward: retain function stores last k context-action pairs (k configurable, k=3 for Jericho, k=1 for TWC experiments); alternatives evaluated: store only rewarded pair, sample by TD-error; pretraining collects positive examples for initial memory stability</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>nearest-neighbor on discrete KD codes using similarity = fraction of shared code indices (discrete code overlap); retrieval threshold τ = 0.7; top match returned with a relevance score δ</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>On-policy reinforcement learning (A2C) for the neural policy; contrastive loss and pretraining for retriever (when reused actions selected) to pull positive contexts together</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>TWC: number of steps to goal (#Steps) and normalized cumulative reward (Norm. Score); Jericho: average raw game score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Across multiple baselines, adding CBR improved performance; example: BiKE baseline on TWC in-distribution (Easy) Norm. Score 0.94 → BiKE + CBR 0.95 and reduced #Steps 18.27 → 15.72; on TWC out-of-distribution (Hard) agents with CBR suffered ~6% drop vs ~35% drop without CBR (authors' aggregate claim). On Jericho, BiKE → BiKE+CBR produced substantial wins on many games (e.g., detective: 278.2 → 326.1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline neural agents (same architectures without CBR) consistently scored worse and generalized less; example BiKE (no CBR) on TWC OOD hard: Norm. Score 0.23 (Table 2) vs BiKE+CBR 0.40 (Table 1/2 comparisons summarized by authors); exact numbers vary per game and baseline as shown in Tables 1–3.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Key findings: (1) Vector quantization (VQ) to discrete KD codes is important — storing continuous contexts (w/o VQ) led to many duplicated entries, larger memory growth and worse performance (w/o VQ variants performed poorly across Jericho/TWC). (2) Locality Sensitive Hashing (LSH) is a viable alternative to VQ but requires storing continuous vectors to filter false positives; learned VQ generally outperformed LSH. (3) Retain strategy matters: retaining last k=3 (Jericho) worked best overall; variants (rewarded-only, TD-error sampling) were competitive but sometimes slightly worse. (4) Seeded Graph Attention (GAT) for context encoding improves results over simple entity-based contexts. (5) Joint training of retriever + agent is possible but slightly harder to train and sometimes performs worse than separate training.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Memory can grow large (OOM observed in experiments when storing continuous contexts); storing continuous keys causes duplicate entries and degrades retrieval quality; retrieved actions can be irrelevant and require fallback to neural policy; joint training complicates optimization; LSH requires extra storage for false-positive checks.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use seeded graph attention to focus context around action entities; discretize context representations via learned VQ (KD codes) before memory writes/reads; set a retrieval threshold (authors used τ=0.7); retain a small window of recent positive context-action pairs (k tuned per domain, k=3 for Jericho); pretrain the retriever using a contrastive loss on collected positive instances; use symbolic reuse by mapping retrieved action templates to current entities and fall back to a neural agent when reuse is invalid.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Case-based reasoning for better generalization in textual reinforcement learning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6695.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6695.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiKE+CBR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BiKE baseline agent augmented with CBR retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BiKE (a baseline text-RL agent) augmented with the CBR memory/retriever pipeline; memory stores discretized context keys and corresponding past actions to allow symbolic reuse and improve generalization and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BiKE + CBR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The BiKE neural agent (baseline) acts as the revise-step policy; CBR retrieves candidate actions from the case memory (discrete KD keys -> actions) and the system executes the retrieved action if valid (highest relevance δ), otherwise BiKE's policy picks an action. The retriever is trained with contrastive loss and pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BiKE (neural policy baseline) + BERT (pretrained) for retriever encodings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC), Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value store (case memory) with discrete KD-code keys</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>KD discrete codes for contexts; values are context-action pairs (templates + entity instances); memory entries originate from retained recent positive-reward steps</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>On positive reward, retain last k context-action pairs (k=3 for Jericho); alternative retain policies (rewarded-only, TD-error sampling) were evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Discrete KD-code overlap similarity (fraction of matching code tokens) with threshold τ=0.7; top match selected and returned with relevance δ</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>A2C for BiKE policy; retriever pretraining on positive examples + contrastive loss when reuse occurs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>TWC: #Steps and Normalized Score; Jericho: average raw score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BiKE + CBR produced state-of-the-art or improved results on many games: e.g., TWC in-distribution Easy Norm. Score improved slightly (0.94 → 0.95) and #Steps reduced (18.27 → 15.72); on Jericho many per-game gains (e.g., detective 278.2 → 326.1). Authors report CBR-augmented agents strictly outperforming baselines on 24/33 Jericho games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>BiKE (no CBR) baseline had lower scores and worse OOD generalization; example Jericho detective 278.2 (BiKE) vs 326.1 (BiKE+CBR). On TWC OOD hard-level the non-CBR agents suffered ~35% performance drop on average vs ~6% for CBR-augmented agents (authors' aggregate figures).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>When replacing VQ with continuous keys (BiKE + CBR w/o VQ), performance dropped significantly on many games; seeded GAT contributed improvements (BiKE + CBR w/o GAT performed worse than full model but still competitive); LSH and SRP/RP alternatives explored with mixed results (LSH competitive but less memory-efficient).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Memory scaling issues when storing continuous vectors; need to limit memory (e.g., 5000 most recent entries) for some ablations; retrieval may return invalid/unusable actions requiring fallback.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Combine BiKE (or other strong neural baselines) with a VQ-discretized CBR retriever, use seeded GAT for context, retain a small window of positive context-action pairs, and pretrain retriever with contrastive objective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Case-based reasoning for better generalization in textual reinforcement learning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6695.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6695.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR-only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CBR-only agent (random policy + CBR retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple agent where a random policy is augmented with the CBR retrieval pipeline; demonstrates that CBR alone (with basic fallback) can achieve competitive results by reusing stored successful actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CBR-only</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A random-action baseline augmented with the CBR memory retriever: for each admissible action, contexts are formed and matched to memory; if a retrieved action is valid it is executed, otherwise the random policy picks an action.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (pretrained) for encodings used by retriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value case memory (discrete KD codes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>KD-code keys from context embeddings; values are retained context-action pairs from positive episodes</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Retain last k positive context-action pairs when r_t > 0 (k small; authors use k=1 for TWC experiment variant)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Discrete-code overlap similarity (fraction of matching KD codes), threshold τ=0.7</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>No RL training for policy (random); retriever pretraining may be used to initialize memory encodings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>TWC: #Steps and Normalized Score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>CBR-only achieved competitive results compared to top baselines on TWC and performed surprisingly well OOD (authors note CBR-only is competitive to top baselines in experiments), e.g., on some settings it outperformed non-CBR neural baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable (baseline is random policy; the demonstration is that adding CBR to a weak policy yields strong gains); authors contrast CBR-only vs other baselines to show benefits of memory reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Even with a weak policy, a CBR memory that stores successful context-action pairs enables good performance; the quality of context representation (seeded GAT vs entity-only) affects effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Depends on quality/diversity of stored positive cases; may be limited if memory lacks relevant past experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>CBR can significantly upgrade weak policies; ensure retriever encodings capture action-relevant context and consider pretraining retriever on collected positive examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Case-based reasoning for better generalization in textual reinforcement learning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6695.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6695.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Paragraph Text-based Retriever (MPTR / MTPR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-only retriever alternative that encodes the current observation together with recent related observations (sharing objects) and uses vector quantization to index and retrieve past textual observations for reuse in decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Multi-Paragraph Text-Based Retriever (MPTR/MTPR)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Text-only retriever that forms an action-specific textual encoding by concatenating current observation with n most recent observations that share objects with it/the action; encoding is discretized via VQ and used as keys in the same CBR framework to retrieve past text-contexts/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text encoder (as in Guo et al. 2020) + vector quantization; uses VQ discretization identical to graph-based retriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (subset experiments reported), TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value store with discrete VQ-coded text-context keys</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Discrete KD codes produced from concatenated textual observations (current + recent related observations); values are actions/context-action pairs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Same retain-on-positive-reward policy as graph-based CBR (retain last k positive context-action pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Nearest-neighbor on KD-code overlap (same similarity=shared codes metric), thresholding and top match retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>A2C for neural policies combined with retriever pretraining + contrastive loss as in main CBR</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Jericho: average raw game score (subset), TWC metrics when applicable</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MPTR integrated into baselines produced improvements over baselines and was competitive on some games (Table 10); however, authors report the graph-based retriever generally outperformed MPTR on majority of games (graph-based performed better on most Jericho subset games). Example: BiKE + CBR (MPTR) zork1 = 42.2 vs graph-based BiKE+CBR 44.3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not directly comparable within MPTR ablation in isolation; comparison is between MPTR-augmented agents and graph-based CBR-augmented agents and non-CBR baselines, showing MPTR helps but graph-based CBR is superior overall.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Text-only retriever (MPTR) helps and is simpler, but seeded graph-attention on knowledge graphs yields better context extraction and generally improved retrieval/use; VQ discretization remains important for stable memory retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Text-only approach may lack structured relational context captured by knowledge graphs, leading to slightly worse performance; still relies on discretization to avoid memory bloat.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>If knowledge graphs are not available, enriching textual observation with recent related paragraphs and discretizing via VQ is a viable retriever; when possible, prefer graph-based seeded attention for better context focus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Case-based reasoning for better generalization in textual reinforcement learning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6695.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6695.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR (w/o VQ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CBR variant storing continuous context representations (no vector quantization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation variant where continuous context embeddings are stored directly as memory keys (no VQ discretization), requiring memory size limits and nearest-neighbor search in continuous space; authors show this leads to duplicated entries, large memories and worse retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CBR (w/o VQ)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Variant of the CBR retriever that stores full continuous context embeddings as keys and uses nearest-neighbor (continuous similarity) retrieval; due to scalability issues authors artificially limit memory (e.g., 5000 most recent entries) in Jericho experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (pretrained) for embeddings; continuous nearest-neighbor retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TWC (small set experiments), Jericho (limited experiments with capped memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value store with continuous embedding keys</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw continuous context embeddings (node-graph encoded vectors) as keys; values are context-action pairs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Retain last k positive context-action pairs but memory was artificially limited (e.g., to 5000 recent entries) to make experiments feasible</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Nearest-neighbor search in continuous embedding space (cosine / Euclidean), without KD-code discretization</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>A2C for policies; retriever trained similarly but without discrete VQ objective</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>TWC: #Steps & Norm. Score; Jericho: per-game raw score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BiKE + CBR (w/o VQ) performed substantially worse than the VQ-discretized CBR in several games (Table 6): e.g., detective 205.2 (w/o VQ) vs 326.1 (VQ CBR), zork1 31.5 (w/o VQ) vs 44.3 (VQ CBR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared against BiKE baseline and VQ-CBR variants: storing continuous keys often made the system perform worse than VQ-based CBR and in some cases worse than baseline, due to duplication and retrieval noise.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Directly storing continuous contexts causes duplication across training parameter shifts, leads to rapid memory growth (OOM), and makes retrieval less stable; discretization via learned VQ is recommended to compress and stabilize memory.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Severe scalability problems (memory growth, OOM); duplicated near-identical contexts across training lead to noisy retrieval and degraded performance; required artificial limiting of memory size in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Avoid storing raw continuous context embeddings in large-scale settings; use discrete learned codes (VQ) or efficient hashing methods (LSH) with caution to control memory growth and stabilize retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Case-based reasoning for better generalization in textual reinforcement learning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6695.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6695.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity-context CBR (w/o GAT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entity-based Context CBR (no seeded graph attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simplified retriever that uses single entity representations (BERT [CLS] per object) as context keys and computes similarity via cosine between FFN-processed entity embeddings; memory stores object-to-action mappings enabling a 'registry' of commonsensical object locations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CBR (w/o GAT) / Entity-based CBR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Context selector is a single entity (object) rather than a seeded-graph attention neighborhood; contexts encoded as BERT [CLS] embeddings passed through FFN; similarity is cosine between FFN outputs; memory stores mapping from entities to actions/templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (pretrained) for entity encodings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value store with discrete or entity-based keys (discrete not needed if entities are discrete)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Entity representations (FFN(BERT([CLS]))); memory stores object -> action mappings (context-action pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Retain on positive reward, typically last-k policy (k small); discretization not required because entities are discrete symbols</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Cosine similarity between FFN(entity) vectors; nearest neighbor retrieval among entities in memory</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>A2C for policies; retriever pretraining with contrastive loss aligning objects that share target locations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>TWC: #Steps and Normalized Score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Entity-based CBR performed well and generalized out-of-distribution; Table 11 shows BiKE + CBR (w/o GAT) yields competitive scores (e.g., Easy Norm. Score ~0.95 for BiKE + CBR w/o GAT) though slightly worse than full graph-based retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Non-CBR baselines performed worse in OOD and in-distribution tests; entity-based CBR improved over baselines but not as much as full GAT+VQ CBR.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Entity-based contexts work well for TWC (where rewarded actions are object-centric) and map objects with similar target locations to similar embeddings; however, including additional structured context via seeded GAT yields consistently better performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Simpler context representation may not capture relational/state information needed in more complex games; less powerful than seeded GAT in general.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>When tasks are object-centric (like TWC), entity-based context selectors are an efficient and effective simplification; for richer relational tasks prefer seeded GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Case-based reasoning for better generalization in textual reinforcement learning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Pre-trained language models as prior knowledge for playing text-based games <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 1)</em></li>
                <li>A simple approach to case-based reasoning in knowledge bases <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6695",
    "paper_id": "paper-18e0cdc75e017b6112d674c3bd0ac5b3e35e4f82",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "CBR",
            "name_full": "Case-Based Reasoning Retriever/Agent",
            "brief_description": "A memory-augmented decision module that stores past successful context-action cases (key-value pairs) and retrieves the most similar past action to reuse in the current state via symbolic mapping; keys are discrete context codes produced by vector quantization of graph-based context embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING",
            "agent_name": "CBR (retriever + reuse + revise pipeline)",
            "agent_description": "A modular CBR component that (i) encodes an action-specific context from a state graph using seeded graph attention over BERT node features, (ii) discretizes context embeddings via learned vector quantization into KD codes (keys), (iii) stores key -&gt; action (value) pairs for positive-reward experiences, (iv) retrieves nearest-key matches by discrete-code overlap and reuses actions via symbolic template mapping, falling back to a neural policy if reuse is invalid.",
            "model_name": "BERT (pretrained) for node/entity encodings; A2C-based neural policy used as fallback",
            "model_size": null,
            "benchmark_name": "TextWorld Commonsense (TWC), Jericho (interactive fiction)",
            "memory_used": true,
            "memory_type": "external key-value store (case memory) with discrete keys via vector quantization",
            "memory_representation": "KD codes (K-way, D-dimensional discrete codes) derived from continuous context embeddings; values are actions or context-action pairs (templates + entities); variants store continuous context embeddings in some ablations",
            "memory_update_mechanism": "retain on positive reward: retain function stores last k context-action pairs (k configurable, k=3 for Jericho, k=1 for TWC experiments); alternatives evaluated: store only rewarded pair, sample by TD-error; pretraining collects positive examples for initial memory stability",
            "memory_retrieval_method": "nearest-neighbor on discrete KD codes using similarity = fraction of shared code indices (discrete code overlap); retrieval threshold τ = 0.7; top match returned with a relevance score δ",
            "training_method": "On-policy reinforcement learning (A2C) for the neural policy; contrastive loss and pretraining for retriever (when reused actions selected) to pull positive contexts together",
            "evaluation_metric": "TWC: number of steps to goal (#Steps) and normalized cumulative reward (Norm. Score); Jericho: average raw game score",
            "performance_with_memory": "Across multiple baselines, adding CBR improved performance; example: BiKE baseline on TWC in-distribution (Easy) Norm. Score 0.94 → BiKE + CBR 0.95 and reduced #Steps 18.27 → 15.72; on TWC out-of-distribution (Hard) agents with CBR suffered ~6% drop vs ~35% drop without CBR (authors' aggregate claim). On Jericho, BiKE → BiKE+CBR produced substantial wins on many games (e.g., detective: 278.2 → 326.1).",
            "performance_without_memory": "Baseline neural agents (same architectures without CBR) consistently scored worse and generalized less; example BiKE (no CBR) on TWC OOD hard: Norm. Score 0.23 (Table 2) vs BiKE+CBR 0.40 (Table 1/2 comparisons summarized by authors); exact numbers vary per game and baseline as shown in Tables 1–3.",
            "has_comparative_results": true,
            "ablation_findings": "Key findings: (1) Vector quantization (VQ) to discrete KD codes is important — storing continuous contexts (w/o VQ) led to many duplicated entries, larger memory growth and worse performance (w/o VQ variants performed poorly across Jericho/TWC). (2) Locality Sensitive Hashing (LSH) is a viable alternative to VQ but requires storing continuous vectors to filter false positives; learned VQ generally outperformed LSH. (3) Retain strategy matters: retaining last k=3 (Jericho) worked best overall; variants (rewarded-only, TD-error sampling) were competitive but sometimes slightly worse. (4) Seeded Graph Attention (GAT) for context encoding improves results over simple entity-based contexts. (5) Joint training of retriever + agent is possible but slightly harder to train and sometimes performs worse than separate training.",
            "reported_limitations": "Memory can grow large (OOM observed in experiments when storing continuous contexts); storing continuous keys causes duplicate entries and degrades retrieval quality; retrieved actions can be irrelevant and require fallback to neural policy; joint training complicates optimization; LSH requires extra storage for false-positive checks.",
            "best_practices_recommendations": "Use seeded graph attention to focus context around action entities; discretize context representations via learned VQ (KD codes) before memory writes/reads; set a retrieval threshold (authors used τ=0.7); retain a small window of recent positive context-action pairs (k tuned per domain, k=3 for Jericho); pretrain the retriever using a contrastive loss on collected positive instances; use symbolic reuse by mapping retrieved action templates to current entities and fall back to a neural agent when reuse is invalid.",
            "uuid": "e6695.0",
            "source_info": {
                "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "BiKE+CBR",
            "name_full": "BiKE baseline agent augmented with CBR retriever",
            "brief_description": "BiKE (a baseline text-RL agent) augmented with the CBR memory/retriever pipeline; memory stores discretized context keys and corresponding past actions to allow symbolic reuse and improve generalization and sample efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING",
            "agent_name": "BiKE + CBR",
            "agent_description": "The BiKE neural agent (baseline) acts as the revise-step policy; CBR retrieves candidate actions from the case memory (discrete KD keys -&gt; actions) and the system executes the retrieved action if valid (highest relevance δ), otherwise BiKE's policy picks an action. The retriever is trained with contrastive loss and pretraining.",
            "model_name": "BiKE (neural policy baseline) + BERT (pretrained) for retriever encodings",
            "model_size": null,
            "benchmark_name": "TextWorld Commonsense (TWC), Jericho",
            "memory_used": true,
            "memory_type": "external key-value store (case memory) with discrete KD-code keys",
            "memory_representation": "KD discrete codes for contexts; values are context-action pairs (templates + entity instances); memory entries originate from retained recent positive-reward steps",
            "memory_update_mechanism": "On positive reward, retain last k context-action pairs (k=3 for Jericho); alternative retain policies (rewarded-only, TD-error sampling) were evaluated",
            "memory_retrieval_method": "Discrete KD-code overlap similarity (fraction of matching code tokens) with threshold τ=0.7; top match selected and returned with relevance δ",
            "training_method": "A2C for BiKE policy; retriever pretraining on positive examples + contrastive loss when reuse occurs",
            "evaluation_metric": "TWC: #Steps and Normalized Score; Jericho: average raw score",
            "performance_with_memory": "BiKE + CBR produced state-of-the-art or improved results on many games: e.g., TWC in-distribution Easy Norm. Score improved slightly (0.94 → 0.95) and #Steps reduced (18.27 → 15.72); on Jericho many per-game gains (e.g., detective 278.2 → 326.1). Authors report CBR-augmented agents strictly outperforming baselines on 24/33 Jericho games.",
            "performance_without_memory": "BiKE (no CBR) baseline had lower scores and worse OOD generalization; example Jericho detective 278.2 (BiKE) vs 326.1 (BiKE+CBR). On TWC OOD hard-level the non-CBR agents suffered ~35% performance drop on average vs ~6% for CBR-augmented agents (authors' aggregate figures).",
            "has_comparative_results": true,
            "ablation_findings": "When replacing VQ with continuous keys (BiKE + CBR w/o VQ), performance dropped significantly on many games; seeded GAT contributed improvements (BiKE + CBR w/o GAT performed worse than full model but still competitive); LSH and SRP/RP alternatives explored with mixed results (LSH competitive but less memory-efficient).",
            "reported_limitations": "Memory scaling issues when storing continuous vectors; need to limit memory (e.g., 5000 most recent entries) for some ablations; retrieval may return invalid/unusable actions requiring fallback.",
            "best_practices_recommendations": "Combine BiKE (or other strong neural baselines) with a VQ-discretized CBR retriever, use seeded GAT for context, retain a small window of positive context-action pairs, and pretrain retriever with contrastive objective.",
            "uuid": "e6695.1",
            "source_info": {
                "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "CBR-only",
            "name_full": "CBR-only agent (random policy + CBR retrieval)",
            "brief_description": "A simple agent where a random policy is augmented with the CBR retrieval pipeline; demonstrates that CBR alone (with basic fallback) can achieve competitive results by reusing stored successful actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING",
            "agent_name": "CBR-only",
            "agent_description": "A random-action baseline augmented with the CBR memory retriever: for each admissible action, contexts are formed and matched to memory; if a retrieved action is valid it is executed, otherwise the random policy picks an action.",
            "model_name": "BERT (pretrained) for encodings used by retriever",
            "model_size": null,
            "benchmark_name": "TextWorld Commonsense (TWC)",
            "memory_used": true,
            "memory_type": "external key-value case memory (discrete KD codes)",
            "memory_representation": "KD-code keys from context embeddings; values are retained context-action pairs from positive episodes",
            "memory_update_mechanism": "Retain last k positive context-action pairs when r_t &gt; 0 (k small; authors use k=1 for TWC experiment variant)",
            "memory_retrieval_method": "Discrete-code overlap similarity (fraction of matching KD codes), threshold τ=0.7",
            "training_method": "No RL training for policy (random); retriever pretraining may be used to initialize memory encodings",
            "evaluation_metric": "TWC: #Steps and Normalized Score",
            "performance_with_memory": "CBR-only achieved competitive results compared to top baselines on TWC and performed surprisingly well OOD (authors note CBR-only is competitive to top baselines in experiments), e.g., on some settings it outperformed non-CBR neural baselines.",
            "performance_without_memory": "Not applicable (baseline is random policy; the demonstration is that adding CBR to a weak policy yields strong gains); authors contrast CBR-only vs other baselines to show benefits of memory reuse.",
            "has_comparative_results": true,
            "ablation_findings": "Even with a weak policy, a CBR memory that stores successful context-action pairs enables good performance; the quality of context representation (seeded GAT vs entity-only) affects effectiveness.",
            "reported_limitations": "Depends on quality/diversity of stored positive cases; may be limited if memory lacks relevant past experiences.",
            "best_practices_recommendations": "CBR can significantly upgrade weak policies; ensure retriever encodings capture action-relevant context and consider pretraining retriever on collected positive examples.",
            "uuid": "e6695.2",
            "source_info": {
                "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "MTPR",
            "name_full": "Multi-Paragraph Text-based Retriever (MPTR / MTPR)",
            "brief_description": "A text-only retriever alternative that encodes the current observation together with recent related observations (sharing objects) and uses vector quantization to index and retrieve past textual observations for reuse in decision-making.",
            "citation_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "mention_or_use": "use",
            "paper_title": "CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING",
            "agent_name": "Multi-Paragraph Text-Based Retriever (MPTR/MTPR)",
            "agent_description": "Text-only retriever that forms an action-specific textual encoding by concatenating current observation with n most recent observations that share objects with it/the action; encoding is discretized via VQ and used as keys in the same CBR framework to retrieve past text-contexts/actions.",
            "model_name": "Text encoder (as in Guo et al. 2020) + vector quantization; uses VQ discretization identical to graph-based retriever",
            "model_size": null,
            "benchmark_name": "Jericho (subset experiments reported), TextWorld",
            "memory_used": true,
            "memory_type": "external key-value store with discrete VQ-coded text-context keys",
            "memory_representation": "Discrete KD codes produced from concatenated textual observations (current + recent related observations); values are actions/context-action pairs",
            "memory_update_mechanism": "Same retain-on-positive-reward policy as graph-based CBR (retain last k positive context-action pairs)",
            "memory_retrieval_method": "Nearest-neighbor on KD-code overlap (same similarity=shared codes metric), thresholding and top match retrieval",
            "training_method": "A2C for neural policies combined with retriever pretraining + contrastive loss as in main CBR",
            "evaluation_metric": "Jericho: average raw game score (subset), TWC metrics when applicable",
            "performance_with_memory": "MPTR integrated into baselines produced improvements over baselines and was competitive on some games (Table 10); however, authors report the graph-based retriever generally outperformed MPTR on majority of games (graph-based performed better on most Jericho subset games). Example: BiKE + CBR (MPTR) zork1 = 42.2 vs graph-based BiKE+CBR 44.3.",
            "performance_without_memory": "Not directly comparable within MPTR ablation in isolation; comparison is between MPTR-augmented agents and graph-based CBR-augmented agents and non-CBR baselines, showing MPTR helps but graph-based CBR is superior overall.",
            "has_comparative_results": true,
            "ablation_findings": "Text-only retriever (MPTR) helps and is simpler, but seeded graph-attention on knowledge graphs yields better context extraction and generally improved retrieval/use; VQ discretization remains important for stable memory retrieval.",
            "reported_limitations": "Text-only approach may lack structured relational context captured by knowledge graphs, leading to slightly worse performance; still relies on discretization to avoid memory bloat.",
            "best_practices_recommendations": "If knowledge graphs are not available, enriching textual observation with recent related paragraphs and discretizing via VQ is a viable retriever; when possible, prefer graph-based seeded attention for better context focus.",
            "uuid": "e6695.3",
            "source_info": {
                "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "CBR (w/o VQ)",
            "name_full": "CBR variant storing continuous context representations (no vector quantization)",
            "brief_description": "An ablation variant where continuous context embeddings are stored directly as memory keys (no VQ discretization), requiring memory size limits and nearest-neighbor search in continuous space; authors show this leads to duplicated entries, large memories and worse retrieval quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING",
            "agent_name": "CBR (w/o VQ)",
            "agent_description": "Variant of the CBR retriever that stores full continuous context embeddings as keys and uses nearest-neighbor (continuous similarity) retrieval; due to scalability issues authors artificially limit memory (e.g., 5000 most recent entries) in Jericho experiments.",
            "model_name": "BERT (pretrained) for embeddings; continuous nearest-neighbor retrieval",
            "model_size": null,
            "benchmark_name": "TWC (small set experiments), Jericho (limited experiments with capped memory)",
            "memory_used": true,
            "memory_type": "external key-value store with continuous embedding keys",
            "memory_representation": "Raw continuous context embeddings (node-graph encoded vectors) as keys; values are context-action pairs",
            "memory_update_mechanism": "Retain last k positive context-action pairs but memory was artificially limited (e.g., to 5000 recent entries) to make experiments feasible",
            "memory_retrieval_method": "Nearest-neighbor search in continuous embedding space (cosine / Euclidean), without KD-code discretization",
            "training_method": "A2C for policies; retriever trained similarly but without discrete VQ objective",
            "evaluation_metric": "TWC: #Steps & Norm. Score; Jericho: per-game raw score",
            "performance_with_memory": "BiKE + CBR (w/o VQ) performed substantially worse than the VQ-discretized CBR in several games (Table 6): e.g., detective 205.2 (w/o VQ) vs 326.1 (VQ CBR), zork1 31.5 (w/o VQ) vs 44.3 (VQ CBR).",
            "performance_without_memory": "Compared against BiKE baseline and VQ-CBR variants: storing continuous keys often made the system perform worse than VQ-based CBR and in some cases worse than baseline, due to duplication and retrieval noise.",
            "has_comparative_results": true,
            "ablation_findings": "Directly storing continuous contexts causes duplication across training parameter shifts, leads to rapid memory growth (OOM), and makes retrieval less stable; discretization via learned VQ is recommended to compress and stabilize memory.",
            "reported_limitations": "Severe scalability problems (memory growth, OOM); duplicated near-identical contexts across training lead to noisy retrieval and degraded performance; required artificial limiting of memory size in experiments.",
            "best_practices_recommendations": "Avoid storing raw continuous context embeddings in large-scale settings; use discrete learned codes (VQ) or efficient hashing methods (LSH) with caution to control memory growth and stabilize retrieval.",
            "uuid": "e6695.4",
            "source_info": {
                "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Entity-context CBR (w/o GAT)",
            "name_full": "Entity-based Context CBR (no seeded graph attention)",
            "brief_description": "A simplified retriever that uses single entity representations (BERT [CLS] per object) as context keys and computes similarity via cosine between FFN-processed entity embeddings; memory stores object-to-action mappings enabling a 'registry' of commonsensical object locations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING",
            "agent_name": "CBR (w/o GAT) / Entity-based CBR",
            "agent_description": "Context selector is a single entity (object) rather than a seeded-graph attention neighborhood; contexts encoded as BERT [CLS] embeddings passed through FFN; similarity is cosine between FFN outputs; memory stores mapping from entities to actions/templates.",
            "model_name": "BERT (pretrained) for entity encodings",
            "model_size": null,
            "benchmark_name": "TextWorld Commonsense (TWC)",
            "memory_used": true,
            "memory_type": "external key-value store with discrete or entity-based keys (discrete not needed if entities are discrete)",
            "memory_representation": "Entity representations (FFN(BERT([CLS]))); memory stores object -&gt; action mappings (context-action pairs)",
            "memory_update_mechanism": "Retain on positive reward, typically last-k policy (k small); discretization not required because entities are discrete symbols",
            "memory_retrieval_method": "Cosine similarity between FFN(entity) vectors; nearest neighbor retrieval among entities in memory",
            "training_method": "A2C for policies; retriever pretraining with contrastive loss aligning objects that share target locations",
            "evaluation_metric": "TWC: #Steps and Normalized Score",
            "performance_with_memory": "Entity-based CBR performed well and generalized out-of-distribution; Table 11 shows BiKE + CBR (w/o GAT) yields competitive scores (e.g., Easy Norm. Score ~0.95 for BiKE + CBR w/o GAT) though slightly worse than full graph-based retriever.",
            "performance_without_memory": "Non-CBR baselines performed worse in OOD and in-distribution tests; entity-based CBR improved over baselines but not as much as full GAT+VQ CBR.",
            "has_comparative_results": true,
            "ablation_findings": "Entity-based contexts work well for TWC (where rewarded actions are object-centric) and map objects with similar target locations to similar embeddings; however, including additional structured context via seeded GAT yields consistently better performance.",
            "reported_limitations": "Simpler context representation may not capture relational/state information needed in more complex games; less powerful than seeded GAT in general.",
            "best_practices_recommendations": "When tasks are object-centric (like TWC), entity-based context selectors are an efficient and effective simplification; for richer relational tasks prefer seeded GAT.",
            "uuid": "e6695.5",
            "source_info": {
                "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Pre-trained language models as prior knowledge for playing text-based games",
            "rating": 2
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 1
        },
        {
            "paper_title": "A simple approach to case-based reasoning in knowledge bases",
            "rating": 1
        }
    ],
    "cost": 0.021399249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CASE-BASED REASONING FOR BETTER GENERALIZATION IN TEXTUAL REINFORCEMENT LEARNING</h1>
<p>Mattia Atzeni<br>IBM Research, EPFL<br>atz@zurich.ibm.com</p>
<h2>Keerthiram Murugesan</h2>
<p>IBM Research
keerthiram.murugesan@ibm.com</p>
<p>Shehzaad Dhuliawala<br>ETH Zürich<br>shehzaad.dhuliawala@inf.ethz.ch</p>
<h2>Mrinmaya Sachan</h2>
<p>ETH Zürich
mrinmaya.sachan@inf.ethz.ch</p>
<h2>ABSTRACT</h2>
<p>Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efficiently, especially under distributional shifts. In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The case-based reasoner collects instances of positive experiences from the agent's interaction with the world in the past and later reuses the collected experiences to act efficiently. The method can be applied in conjunction with any existing on-policy neural agent in the literature for TBGs. Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state-of-the-art results on widely used environments.</p>
<h2>1 INTRODUCTION</h2>
<p>Text-based games (TBGs) have emerged as key benchmarks for studying how reinforcement learning (RL) agents can tackle the challenges of grounded language understanding, partial observability, large action spaces, and out-of-distribution generalization (Hausknecht et al., 2020; Ammanabrolu \&amp; Riedl, 2019). While we have indeed made some progress on these fronts in recent years (Ammanabrolu \&amp; Hausknecht, 2020; Adhikari et al., 2020; Murugesan et al., 2021b;a), these agents are still very inefficient and suffer from insufficient generalization to novel environments. As an example, state-of-the-art agents require 5 to 10 times more steps than a human to accomplish even simple household tasks (Murugesan et al., 2021b). As the agents are purely neural architectures requiring significant training experience and computation, they fail to efficiently adapt to new environments and use their past experiences to reason in novel situations. This is in stark contrast to human learning which is much more robust, efficient and generalizable (Lake et al., 2017).</p>
<p>Motivated by this fundamental difference in learning, we propose new agents that rely on case-based reasoning (CBR) (Aamodt \&amp; Plaza, 1994) to efficiently act in the world. CBR draws its foundations in cognitive science (Schank, 1983; Kolodner, 1983) and mimics the process of solving new tasks based on solutions to previously encountered similar tasks. Concretely, we design a general CBR framework that enables an agent to collect instances of past situations which led to a positive reward (known as cases). During decision making, the agent retrieves the case most similar to the current situation and then applies it after appropriately mapping it to the current context.</p>
<p>The CBR agent stores past experiences, along with the actions it performed, in a case memory. In order to efficiently use these stored experiences, the agent should be able to represent relevant contextual information from the state of the game in a compact way, while retaining the property</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the approach and architecture of the CBR agent. A memory stores actions that have been used in previous interactions. The context of the game is learned from the state knowledge graph using a graph attention mechanism. Actions are retrieved from the memory based on this context representation and mapped to the current state. If no valid action is obtained using CBR, the algorithm falls back to a neural agent.
that contexts that require similar actions receive similar representations. We represent the state of the game as a knowledge graph (Ammanabrolu \&amp; Hausknecht, 2020) and we address these challenges by utilizing (a) a seeded message propagation that focuses only on a subset of relevant nodes and (b) vector quantization (Ballard, 2000) to efficiently map similar contexts to similar discrete representations. Vector quantization allows the model to significantly compress the context representations while retaining their semantics; thereby, allowing for a scalable implementation of CBR in RL settings. An illustration of the framework is shown in Figure 1.</p>
<p>Our experiments show that CBR can be used to consistently boost the performance of various onpolicy RL agents proposed in the literature for TBGs. We obtain a new state-of-the-art on the TextWorld Commonsense (Murugesan et al., 2021b) dataset and we achieve better or comparable scores on 24 of the 33 games in the Jericho suite (Hausknecht et al., 2020) compared to previous work. We also show that CBR agents are resilient to domain shifts and suffer only marginal drops in performance ( $\mathbf{6 \%}$ ) on out-of-distribution settings when compared to their counterparts ( $\mathbf{3 5 \%}$ ).</p>
<h1>2 Preliminaries</h1>
<p>Text-based games. Text-based games (TBGs) provide a challenging environment where an agent can observe the current state of the game and act in the world using only the modality of text. The state of the game is hidden, so TBGs can be modeled as a Partially Observable Markov Decision Process (POMDP) $(\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{E}, r)$, where $\mathcal{S}$ is the set of states of the environment of the game, $\mathcal{A}$ is the natural language action space, $\mathcal{O}$ is the set of observations or sequences of words describing the current state, $\mathcal{T}$ are the conditional transition probabilities from one state to another, $\mathcal{E}$ are the conditional observation probabilities, $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function, which maps a state and action to a scalar reward that the agent receives.</p>
<p>Case-based reasoning. Case-based reasoning (CBR) is the process of solving new problems based on the solution of previously seen similar problems. Generally, CBR assumes access to a memory that stores past problems (known as cases) and their solutions. When a new problem is encountered, CBR will (i) retrieve a similar problem and its solution from memory; (ii) reuse the solution by mapping it to the current problem; (iii) revise the solution by testing it and checking whether it is a viable way to address the new problem; and (iv) retain the solution in memory if the adaptation to the new problem was successful.</p>
<h2>3 CASE-BASED REASONING IN REINFORCEMENT LEARNING</h2>
<p>This section introduces our framework inspired by CBR for improving generalization in TBGs. Even though we situate our work in TBGs, it serves as a good starting point for applying CBR in more general RL settings. We consider an on-policy RL agent that, at any given time step $t$, has access to a memory $\mathcal{M}_{t}$, that can be used to retrieve previous experiences. The memory contains</p>
<p>key-value pairs, where the keys are a context representation of a game state and values are actions that were taken by the agent w.r.t to this context. As mentioned in Section 2, case-based reasoning can be formalized as a four-step process. We describe our proposed methodology for each step below. Algorithm 1 provides a detailed formalization of our approach.</p>
<p>Retrieve. Given the state of the game $s_{t}$ and the valid actions $\mathcal{A}<em t="t">{t}$, we want to retrieve from the memory $\mathcal{M}</em>}$ previous experiences that might be useful in decision-making at the current state. To this end, for each admissible action $a_{t} \in \mathcal{A<em t="t">{t}$, we define a context selector $c</em>}=\operatorname{context}\left(s_{t}, a_{t}\right)$. The context selector is an action-specific representation of the state, namely the portion of the state that is relevant to the execution of an action. We will explain later how the context selector is defined in our implementation. For each context $c_{t}$, we retrieve from the memory the context-action pair $\left(c_{t}^{\mathcal{M}}, a_{t}^{\mathcal{M}}\right)$, such that $c_{t}^{\mathcal{M}}$ has maximum similarity with $c_{t}$. We denote as $\delta=$ $\operatorname{sim}\left(c_{t}, c_{t}^{\mathcal{M}}\right) \in[0,1]$ the relevance score given to the retrieved action. Only actions $a_{t}^{\mathcal{M}}$ with a relevance score above a retriever threshold $\tau$ are retrieved from $\mathcal{M<em t="t">{t}$. We denote as $\mathcal{A}</em>$ the final set of action-relevance pairs returned by the retriever, as shown in Algorithm 1.}^{\mathcal{M}</p>
<p>Reuse. The goal of the reuse step is to adapt the actions retrieved from the memory based on the current state. This is accomplished by a reuse function, that is applied to each retrieved action to construct a set $\tilde{\mathcal{A}}_{t}$ of candidate actions that should be applicable to the current state, each paired with a confidence level.</p>
<p>Revise. If any of the action candidates $\tilde{\mathcal{A}}<em t="t">{t}$ is a valid action, then the one with the highest relevance $\delta$ is executed, otherwise a neural agent $\pi$ is used to select the best action $a</em>\right)$ the obtained reward. Note that $\pi$ can be an existing agent for TBGs (Murugesan et al., 2021c;b; Ammanabrolu \&amp; Hausknecht, 2020).}^{\star}$. We denote with $r_{t}=r\left(s_{t}, a_{t}^{\star</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: CBR in Text-based RL
<span class="k">-</span> Retrieve
Let \(\mathcal{C}_{t}=\left\{\operatorname{context}\left(s_{t}, a_{t}\right) \mid a_{t} \in \mathcal{A}_{t}\right\}\) be a set
    of context selectors for state \(s_{t}\) at time step \(t\)
\(\mathcal{A}_{t}^{\mathcal{M}} \leftarrow \emptyset\)
for \(c_{t} \in \mathcal{C}_{t}\) do
    Let \(\left(c_{t}^{\mathcal{M}}, a_{t}^{\mathcal{M}}\right)=\)
        \(\arg \max <span class="ge">_{\left(c_</span>{t}^{\mathcal{M}}, a_{t}^{\mathcal{M}}\right) \in \mathcal{M}_{t}} \operatorname{sim}\left(c_{t}, c_{t}^{\mathcal{M}}\right)\)
    Let \(\delta=\operatorname{sim}\left(c_{t}, c_{t}^{\mathcal{M}}\right)\)
    if \(\delta&gt;\tau\) then
        \(\mathcal{A}_{t}^{\mathcal{M}} \leftarrow \mathcal{A}_{t}^{\mathcal{M}} \cup\left\{\left(a_{t}^{\mathcal{M}}, \delta\right)\right\}\)
    end
end
</code></pre></div>

<ul>
<li>Reuse</li>
</ul>
<p>Build a set of action candidates:</p>
<p>$$
\begin{aligned}
\tilde{\mathcal{A}}<em t="t">{t}= &amp; \left{\begin{array}{l}
\text { reuse }\left(a</em>, \delta\right) \
\left(a_{t}^{\mathcal{M}}, \delta\right) \in \mathcal{A}_{t}^{\mathcal{M}}
\end{array}\right}
\end{aligned}
$$}^{\mathcal{M}}, s_{t</p>
<ul>
<li>Revise
if $\mathcal{A}<em t="t">{t} \cap \tilde{\mathcal{A}}</em> \neq \emptyset$ then
Let $a_{t}^{\star}, \delta^{\star}=\arg \max <em t="t">{\tilde{a}</em>}, \delta \in \tilde{\mathcal{A}<em t="t">{t}} \delta$
else
$a</em>=\arg \max }^{\star<em t="t">{a</em>} \in \mathcal{A<em t="t">{t}} \pi\left(a</em>\right)$
end
Let $r_{t}=r\left(s_{t}, a_{t}^{\star}\right)$ be the reward obtained at time step $t$ by executing action $a_{t}^{\star}$} \mid s_{t</li>
</ul>
<h2>- Retain</h2>
<p>Let $c_{t}^{\star}=\operatorname{context}\left(s_{t}, a_{t}^{\star}\right)$ be the context of action $a_{t}^{\star}$
$\mathcal{T}=\left{\left(c_{t}^{\star}, a_{t}^{\star}\right), \ldots,\left(c_{t-m+1}^{\star}, a_{t-m+1}^{\star}\right)\right}$
if $r_{t}&gt;0$ then
$\mathcal{M}<em t="t">{t+1} \leftarrow \mathcal{M}</em>)$
end
Retain. Finally, the retain step stores successful experiences as new cases in the memory, so that they can be retrieved in the future. In principle, this can be accomplished by storing actions for which the agent obtained positive rewards. However, we found that storing previous actions as well can result in improved performance. Therefore, whenever $r_{t}&gt;0$, a retain function is used to select which of the past executed actions and their contexts should be stored in the memory. In our experiments, the retain function selects the $k$ most recent actions, but other implementations are possible, as discussed in Appendix D.} \cup \operatorname{retain}(\mathcal{T</p>
<h1>4 A CBR POLICY AGENT TO GENERALIZE IN TEXT-BASED GAMES</h1>
<p>Designing an agent that can act efficiently in TBGs using the described approach poses several challenges. Above all, efficient memory use is crucial to making the approach practical and scalable. Since the context selectors are used as keys for accessing values in the memory, their representation</p>
<p>needs to be such that contexts where similar actions were taken receive similar representations. At the same time, as the state space is exponential, context representations need to be focused only on relevant portions of the state and they need to be compressed and compact.</p>
<h1>4.1 REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION</h1>
<p>State space as a knowledge graph. Following previous works (Ammanabrolu \&amp; Riedl, 2019; Ammanabrolu \&amp; Hausknecht, 2020; Murugesan et al., 2021c), we represent the state of the game as a dynamic knowledge graph $\mathcal{G}<em t="t">{t}=\left(\mathcal{V}</em>}, \mathcal{R<em t="t">{t}, \mathcal{E}</em>}\right)$, where a node $v \in \mathcal{V<em t="t">{t}$ represents an entity in the game, $r \in \mathcal{R}</em>}$ is a relation type, and an edge $v \xrightarrow{r} v^{\prime} \in \mathcal{E<em t="t">{t}$ represents a relation of type $r \in \mathcal{R}</em>}$ between entities $v, v^{\prime} \in \mathcal{V<em t="t">{t}$. In TBGs, the space of valid actions $\mathcal{A}</em>}$ can be modeled as a templatebased action space, where actions $a_{t}$ are instances of a finite set of templates with a given set of entities, denoted as $\mathcal{V<em t="t">{a</em>}} \subseteq \mathcal{V<em 1="1">{t}$. As an example, the action "kill orc with sword" can be seen as an instance of the template "kill $v</em>$ are "orc" and "sword" respectively.}$ with $v_{2}$ ", where $v_{1}$ and $v_{2</p>
<p>Seeded graph attention. The state graph $\mathcal{G}<em a__t="a_{t">{t}$ and the entities $\mathcal{V}</em>}}$ are provided as input to the agent for each action $a_{t} \in \mathcal{A<em v="v">{t}$, in order to build an action-specific contextualized representation of the state. A pre-trained BERT model (Devlin et al., 2019) is used to get a representation $\mathbf{h}</em>}^{(0)} \in \mathbb{R}^{d}$ for each node $v \in \mathcal{V<em a__t="a_{t">{t}$. Inspired by Sun et al. (2018), we propose a seeded graph attention mechanism (GAT), so that the propagation of messages is weighted more for nodes close to the entities $\mathcal{V}</em>}}$. Let $\alpha_{v u}^{(l)}$ denote the attention coefficients given by a graph attention network (Velickovic et al., 2018) at layer $l$ for nodes $v, u \in \mathcal{V<em t="t">{t}$. Then, for each node $v \in \mathcal{V}</em>$ that scales with the amount of messages received by node $v$ at layer $l$ :}$, we introduce a coefficient $\beta_{v}^{(l)</p>
<p>$$
\beta_{v}^{(1)}= \begin{cases}\frac{1}{\left|\mathcal{V}<em t="t">{a</em>}}\right|} &amp; \text { if } v \in \mathcal{V<em t="t">{a</em>}} \ 0 &amp; \text { otherwise }\end{cases}, \quad \beta_{v}^{(l+1)}=(1-\lambda) \beta_{v}^{(l)}+\lambda \sum_{u \in \mathcal{N<em u="u" v="v">{v}} \alpha</em>
$$}^{(l)} \beta_{u}^{(l)</p>
<p>where $\mathcal{N}<em a__t="a_{t">{v}$ denotes the neighbors of $v$, considering the graph as undirected. Note that, at layer $l=1$, only the nodes in $\mathcal{V}</em>$ is then updated as:}}$ receive messages, whereas for increasing values of $l, \beta_{v}^{(l)}$ will be non-zero for their $(l-1)$-hop neighbors as well. The representation of each $v \in \mathcal{V}_{t</p>
<p>$$
\mathbf{h}<em v="v">{v}^{(l)}=\operatorname{FFN}^{(l)}\left(\mathbf{h}</em>}^{(l-1)}+\beta_{v}^{(l)} \sum_{u \in \mathcal{N<em u="u" v="v">{v}} \alpha</em>\right)
$$}^{(l)} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)</p>
<p>where $\operatorname{FFN}^{(l+1)}$ is a 2-layer feed-forward network with ReLU non-linearity and $\mathbf{W}^{(l)} \in \mathbb{R}^{d \times d}$ are learnable parameters. Finally, we compute a final continuous contextualized representation $\mathbf{c}<em t="t">{a</em>}}$ of the state by summing the linear projections of the hidden representations of each $v \in \mathcal{V<em t="t">{a</em>$ and passing the result through a further feed-forward network.}</p>
<h3>4.2 MEMORY ACCESS THROUGH CONTEXT QUANTIZATION</h3>
<p>Given a continuous representation $\mathbf{c}<em t="t">{a</em>}}$ of the context, we need an efficient way to access the memory $\mathcal{M<em a__t="a_{t">{t}$ to retrieve or store actions based on such a context selector. Storing and retrieving based on the continuous representation $\mathbf{c}</em>$ would be impractical for scalability reasons. Additionally, since the parameters of the agent change over the training time, the same context would result in several duplicated entries in the memory even with a pre-trained agent over different episodes.}</p>
<p>Discretization of the context. To address these problems, we propose to use vector quantization (Ballard, 2000) before reading or writing to memory. Following previous work (Chen et al., 2018; Sachan, 2020), we learn a discretization function $\phi: \mathbb{R}^{d} \rightarrow \mathbb{Z}<em a__t="a_{t">{K}^{D}$, that maps the continuous representation $\mathbf{c}</em>}}$ into a $K$-way $D$-dimensional code $c_{t} \in \mathbb{Z<em K="K">{K}^{D}$, with $\left|\mathbb{Z}</em>}\right|=K$ (we refer to $c_{t}$ as a KD code). With reference to Section 3, then we will use $c_{t}=\operatorname{context}\left(s_{t}, a_{t}\right)=\phi\left(\mathbf{c<em t="t">{a</em>}}\right)$ as the context selector used to access the memory $\mathcal{M<em i="i">{t}$. In order to implement the discretization function, we define a set of $K$ key vectors $\mathbf{k}</em>} \in \mathbb{R}^{d}, i=1, \ldots, K$, and we divide each vector in $D$ partitions $\mathbf{k<em a__t="a_{t">{i}^{j} \in \mathbb{R}^{d / D}, j=1, \ldots, D$. Similarly, we divide $\mathbf{c}</em>}}$ in $D$ partitions $\mathbf{c<em t="t">{a</em>=\arg \min }}^{j} \in \mathbb{R}^{d / D}, j=1, \ldots, D$. Then, we compute the $j$-th code $z^{j}$ of $c_{t}$ by nearest neighbor search, as $z^{j<em a__t="a_{t">{i}\left|\mathbf{c}</em>}}^{j}-\mathbf{k<em 2="2">{i}^{j}\right|</em>$. We use the straight-through estimator (Bengio et al., 2013) to address the non differentialbility of the argmin operator.}^{2</p>
<p>Memory access. The KD codes introduced above are used to provide a memory-efficient representation of the keys in the memory. Then, given the KD code representing the current context selector $c_{t}$, we query the memory by computing a similarity measure $\operatorname{sim}\left(c_{t}, c_{t}^{\mathcal{M}}\right)$ between $c_{t}$ and each $c_{t}^{\mathcal{M}}$ in $\mathcal{M}<em t="t">{t}$. The similarity function is defined as the fraction of codes shared by $c</em>$. The context-action pair with the highest similarity is returned as a result of the memory access, together with a relevance score $\delta$ representing the value of the similarity measure.}$ and $c_{t}^{\mathcal{M}</p>
<h1>4.3 SYMBOLIC ACTION REUSE AND REVISE POLICY</h1>
<p>We use a simple purely symbolic reuse function to adapt the actions retrieved from the memory to the current state. Let $c_{t}$ be the context selector computed based on state $s_{t}$ and the entities $\mathcal{V}<em t="t">{a</em>}}$, as explained in Sections 4.1 and 4.2. Denote with $\left(c_{t}^{\mathcal{M}}, a_{t}^{\mathcal{M}}\right)$ the context-action pair retrieved from $\mathcal{M<em t="t">{t}$ with confidence $\delta$. Then, the reuse function $\operatorname{reuse}\left(a</em>}^{\mathcal{M}}, s_{t}, \delta\right)$ constructs the action candidate $\tilde{a<em t="t">{t}$ as the action with the same template as $a</em>}^{\mathcal{M}}$ applied to the entities $\mathcal{V<em t="t">{a</em>$.}}$. If the reuse step cannot generate a valid action, we revert to the neural policy agent $\pi$ that outputs a probability distribution over the current admissible actions $\mathcal{A}_{t</p>
<h2>5 Training</h2>
<p>In Section 3, we have introduced an on-policy RL agent that relies on case-based reasoning to act in the world efficiently. This agent can be trained in principle using any online RL method. This section discusses the training strategies and learning objectives used in our implementation.</p>
<p>Objective. Two main portions of the model need to be trained: (a) the retriever, namely the neural network that computes the context representation and accesses the memory through its discretization, and (b) the main neural agent $\pi$ which is used in the revise step. All agents $\pi$ used in our experiments are trained with an Advantage Actor-Critic (A2C) method. For optimizing the parameters of $\pi$, we use the same learning objectives defined by Adolphs \&amp; Hofmann (2019), as described in Appendix A. Whenever the executed action $a_{t}^{*}$ is not chosen by the model $\pi$ but it comes from the symbolic reuse step, then we optimize instead an additional objective for the retriever, namely the following contrastive loss (Hadsell et al., 2006):</p>
<p>$$
\mathcal{L}<em t="t">{r}^{(t)}=\frac{1}{2}\left(1-y</em>
$$}\right)\left(1-\operatorname{sim}\left(c_{t}, c_{t}^{\mathcal{M}}\right)\right)^{2}+\frac{1}{2} y_{t} \max \left{0, \mu-1+\operatorname{sim}\left(c_{t}, c_{t}^{\mathcal{M}}\right)\right}^{2</p>
<p>where $c_{t}$ denotes the context selector of the action executed at time step $t$, $c_{t}^{\mathcal{M}}$ is the corresponding key entry retrieved from $\mathcal{M}<em t="t">{t}, \mu$ is the margin parameter of the contrastive loss, and $y</em>=0$ otherwise. This objective encourages the retriever to produce similar representations for two contexts where reusing an action yielded a positive reward.}=1$ if $r_{t}&gt;0$, $y_{t</p>
<p>Pretraining. To make learning more stable and allow the agent to act more efficiently, we found it beneficial to pretrain the retriever. This minimizes large shifts in the context representations over the training time. We run a baseline agent (Ammanabrolu \&amp; Hausknecht, 2020) to collect instances of the state graph and actions that yielded positive rewards. Then we train the retriever to encode to similar representations the contexts for which similar actions (i.e., actions with the same template) were used. This is achieved using the same contrastive loss defined above.</p>
<h2>6 EXPERIMENTS</h2>
<p>This section provides a detailed evaluation of our approach. We assess quantitatively the performance of CBR combined with existing RL approaches and we demonstrate its capability to improve sample efficiency and generalize out of the training distribution. Next, we provide qualitative insights and examples of the behavior of the model and we perform an ablation study to understand the role played by the different components of the architecture.</p>
<h3>6.1 EXPERIMENTAL SETUP</h3>
<p>Agents. We consider several agents obtained by plugging existing RL methods in the revise step. We first define two simple approaches: CBR-only, where we augment a random policy with the</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
</tr>
<tr>
<td style="text-align: left;">Text</td>
<td style="text-align: center;">$23.83 \pm 2.16$</td>
<td style="text-align: center;">$0.88 \pm 0.04$</td>
<td style="text-align: center;">$44.08 \pm 0.93$</td>
<td style="text-align: center;">$0.60 \pm 0.02$</td>
<td style="text-align: center;">$49.84 \pm 0.38$</td>
<td style="text-align: center;">$0.30 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">TPC</td>
<td style="text-align: center;">$20.59 \pm 5.01$</td>
<td style="text-align: center;">$0.89 \pm 0.06$</td>
<td style="text-align: center;">$42.61 \pm 0.65$</td>
<td style="text-align: center;">$0.62 \pm 0.03$</td>
<td style="text-align: center;">$48.45 \pm 1.13$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: left;">KG-A2C</td>
<td style="text-align: center;">$22.10 \pm 2.91$</td>
<td style="text-align: center;">$0.86 \pm 0.06$</td>
<td style="text-align: center;">$41.61 \pm 0.37$</td>
<td style="text-align: center;">$0.62 \pm 0.03$</td>
<td style="text-align: center;">$48.00 \pm 0.61$</td>
<td style="text-align: center;">$0.32 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">BiKE</td>
<td style="text-align: center;">$18.27 \pm 1.13$</td>
<td style="text-align: center;">$0.94 \pm 0.02$</td>
<td style="text-align: center;">$39.34 \pm 0.72$</td>
<td style="text-align: center;">$0.64 \pm 0.02$</td>
<td style="text-align: center;">$47.19 \pm 0.64$</td>
<td style="text-align: center;">$0.34 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">CBR-only</td>
<td style="text-align: center;">$22.13 \pm 1.98$</td>
<td style="text-align: center;">$0.80 \pm 0.05$</td>
<td style="text-align: center;">$43.76 \pm 1.23$</td>
<td style="text-align: center;">$0.62 \pm 0.03$</td>
<td style="text-align: center;">$48.12 \pm 1.30$</td>
<td style="text-align: center;">$0.33 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: left;">Text + CBR</td>
<td style="text-align: center;">$17.53 \pm 3.36$</td>
<td style="text-align: center;">$0.93 \pm 0.04$</td>
<td style="text-align: center;">$39.10 \pm 1.77$</td>
<td style="text-align: center;">$0.66 \pm 0.04$</td>
<td style="text-align: center;">$47.11 \pm 1.21$</td>
<td style="text-align: center;">$0.34 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">TPC + CBR</td>
<td style="text-align: center;">$16.81 \pm 3.12$</td>
<td style="text-align: center;">$0.94 \pm 0.03$</td>
<td style="text-align: center;">$37.05 \pm 1.61$</td>
<td style="text-align: center;">$0.67 \pm 0.03$</td>
<td style="text-align: center;">$47.25 \pm 1.56$</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;">KG-A2C + CBR</td>
<td style="text-align: center;">$15.91 \pm 2.52$</td>
<td style="text-align: center;">$0.95 \pm 0.03$</td>
<td style="text-align: center;">$36.13 \pm 1.65$</td>
<td style="text-align: center;">$0.66 \pm 0.05$</td>
<td style="text-align: center;">$46.11 \pm 1.13$</td>
<td style="text-align: center;">$0.40 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: left;">BiKE + CBR</td>
<td style="text-align: center;">$15.72 \pm 1.15$</td>
<td style="text-align: center;">$0.95 \pm 0.04$</td>
<td style="text-align: center;">$35.24 \pm 1.22$</td>
<td style="text-align: center;">$0.67 \pm 0.03$</td>
<td style="text-align: center;">$45.21 \pm 0.87$</td>
<td style="text-align: center;">$0.42 \pm 0.04$</td>
</tr>
</tbody>
</table>
<p>Table 1: Test-set performance for TWC in-distribution games</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
</tr>
<tr>
<td style="text-align: left;">Text</td>
<td style="text-align: center;">$29.90 \pm 2.92$</td>
<td style="text-align: center;">$0.78 \pm 0.02$</td>
<td style="text-align: center;">$45.90 \pm 0.22$</td>
<td style="text-align: center;">$0.55 \pm 0.01$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.20 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">TPC</td>
<td style="text-align: center;">$27.74 \pm 4.46$</td>
<td style="text-align: center;">$0.78 \pm 0.07$</td>
<td style="text-align: center;">$44.89 \pm 1.52$</td>
<td style="text-align: center;">$0.58 \pm 0.01$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.19 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;">KG-A2C</td>
<td style="text-align: center;">$28.34 \pm 3.63$</td>
<td style="text-align: center;">$0.80 \pm 0.07$</td>
<td style="text-align: center;">$43.05 \pm 2.52$</td>
<td style="text-align: center;">$0.59 \pm 0.01$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.21 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">BiKE</td>
<td style="text-align: center;">$25.59 \pm 1.92$</td>
<td style="text-align: center;">$0.83 \pm 0.01$</td>
<td style="text-align: center;">$41.01 \pm 1.61$</td>
<td style="text-align: center;">$0.61 \pm 0.01$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.23 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">CBR-only</td>
<td style="text-align: center;">$23.43 \pm 2.09$</td>
<td style="text-align: center;">$0.80 \pm 0.04$</td>
<td style="text-align: center;">$44.03 \pm 1.75$</td>
<td style="text-align: center;">$0.63 \pm 0.04$</td>
<td style="text-align: center;">$48.71 \pm 1.15$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;">Text + CBR</td>
<td style="text-align: center;">$20.91 \pm 1.72$</td>
<td style="text-align: center;">$0.89 \pm 0.02$</td>
<td style="text-align: center;">$40.32 \pm 1.27$</td>
<td style="text-align: center;">$0.66 \pm 0.04$</td>
<td style="text-align: center;">$47.89 \pm 0.87$</td>
<td style="text-align: center;">$0.32 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: left;">TPC + CBR</td>
<td style="text-align: center;">$18.90 \pm 1.91$</td>
<td style="text-align: center;">$0.92 \pm 0.01$</td>
<td style="text-align: center;">$37.30 \pm 1.00$</td>
<td style="text-align: center;">$0.66 \pm 0.02$</td>
<td style="text-align: center;">$47.54 \pm 1.67$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;">KG-A2C + CBR</td>
<td style="text-align: center;">$18.21 \pm 1.32$</td>
<td style="text-align: center;">$0.90 \pm 0.02$</td>
<td style="text-align: center;">$37.02 \pm 1.22$</td>
<td style="text-align: center;">$0.68 \pm 0.03$</td>
<td style="text-align: center;">$47.10 \pm 1.12$</td>
<td style="text-align: center;">$0.38 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">BiKE + CBR</td>
<td style="text-align: center;">$17.15 \pm 1.45$</td>
<td style="text-align: center;">$0.93 \pm 0.03$</td>
<td style="text-align: center;">$35.45 \pm 1.40$</td>
<td style="text-align: center;">$0.67 \pm 0.03$</td>
<td style="text-align: center;">$45.91 \pm 1.32$</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
</tr>
</tbody>
</table>
<p>Table 2: Test-set performance for TWC out-of-distribution games</p>
<p>CBR approach, and Text + CBR, which relies on the CBR method combined with a simple GRUbased policy network that consumes as input the textual observation from the game. Next, we select three recently proposed TBG approaches: Text+Commonsense (TPC) (Murugesan et al., 2021b), KG-A2C (Ammanabrolu \&amp; Hausknecht, 2020), and BiKE (Murugesan et al., 2021c), to create the TPC + CBR, KG-A2C + CBR and BiKE + CBR agents. We treat the original agents as baselines.</p>
<p>Datasets. We empirically verify the efficacy of our approach on TextWorld Commonsense (TWC) (Murugesan et al., 2021b) and Jericho (Hausknecht et al., 2020). Jericho is a well-known and challenging learning environment including 33 interactive fiction games. TWC is an environment which builds on TextWorld (Côté et al., 2018) and provides a suite of games requiring commonsense knowledge. TWC allows agents to be tested on two settings: the in-distribution games, where the objects that the agent encounters in the test set are the same as the objects in the training set, and the out-of-distribution games which have no entity in common with the training set. For each of these settings, TWC provides three difficulty levels: easy, medium, and hard.</p>
<p>Evaluation metrics. Following Murugesan et al. (2021b), we evaluate the agents on TWC based on the number of steps (#Steps) required to achieve the goal (lower is better) and the normalized cumulative reward (Norm. Score) obtained by the agent (larger is better). On Jericho, we follow previous work (Hausknecht et al., 2020; Guo et al., 2020; Ammanabrolu \&amp; Hausknecht, 2020) and we report the average score achieved over the last 100 training episodes.</p>
<h1>6.2 ReSults on TextWorld Commonsense</h1>
<p>Table 1 reports the results on TWC for the in-distribution set of games. Overall, we observe that CBR consistently improves the performance of all the baselines. The performance boost is large enough that even a simple method as Text + CBR outperforms all considered baselines except BiKE.</p>
<p>Out-of-distribution generalization. CBR's ability to retrieve similar cases should allow our method to better generalize to new and unseen problems. We test this hypothesis on the out-of-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance on TWC (showing mean and standard deviation averaged over 5 runs) for the three difficulty levels: easy (left), medium (middle), Hard (right) using normalized score and number of steps.
distribution games in TWC. The results of this experiment are reported in Table 2. We notice that all existing approaches fail to generalize out of the training distribution and suffer a substantial drop in performance in this setting. However, when coupled with CBR, the drop is minor (on average $\mathbf{6 \%}$ with CBR vs $\mathbf{3 5 \%}$ without on the hard level). Interestingly, even the CBR-only agent achieves competitive results compared to the top-performing baselines.</p>
<p>Sample efficiency. Another key benefit of our approach comes as better sample efficiency. With its ability to explicitly store prior solutions effectively, CBR allows existing algorithms to learn faster. Figure 2 shows the learning curves for our best agents and the corresponding baselines. The plots report the performance of the agent over the training episodes, both in terms of the number of steps and the normalized score. Overall, we observe that the CBR agents obtain faster convergence to their counterparts on all difficulty levels.</p>
<h1>6.3 Performance on the Jericho Games</h1>
<p>We evaluate our best performing variant from the experiments on TWC (BiKE + CBR) against existing approaches on the 33 games in the Jericho environment. We compare our approach against strong baselines, including TDQN (Hausknecht et al., 2020), DRRN (He et al., 2016), KG-A2C (Ammanabrolu \&amp; Hausknecht, 2020), MPRC-DQN (Guo et al., 2020), and RC-DQN (Guo et al., 2020). The same experimental setting and handicaps as the baselines are used, as we train for 100000 steps and we assume access to valid actions. Table 3 summarizes the results of the Jericho games. We observe that our CBR agent achieves comparable or better performance than any baseline on $24(73 \%)$ of the games, strictly outperforming all the other agents in 18 games.</p>
<h3>6.4 Qualitative analysis and ablation studies</h3>
<p>Insights on the model. Figure 3 provides two examples showing the BiKE + CBR agent interacting with the zork1 game. In the example on top, the agent retrieves an experience that can be successfully reused and turned into a valid action at the current time step. The heat maps visualize the value of the context similarity function defined in Section 4 for the top entries in the memory. In the negative example at the bottom instead, the agent retrieves an action that is not useful and needs to fall back to the neural policy $\pi$. Figure 4 (top) shows the fraction of times that actions retrieved from the memory are reused successfully in the TWC games. We observe that, both for in-distribution and out-of-distribution games, the trained agent relies on CBR from $60 \%$ to approximately $70 \%$ of the times. Figure 4 (bottom) further shows the fraction of times that the neural agent would have been able to select a rewarded action as well, when the CBR reuses a successful action. The plot shows that, for the out-of-distribution games, the neural agent would struggle to select good actions when the CBR is used.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">Human <br> (max)</th>
<th style="text-align: center;">Human <br> (Walkthrough-100)</th>
<th style="text-align: center;">TDQN</th>
<th style="text-align: center;">DRRN</th>
<th style="text-align: center;">KG-A2C</th>
<th style="text-align: center;">MPRC-DQN</th>
<th style="text-align: center;">RC-DQN</th>
<th style="text-align: center;">BiKE + CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">905</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: left;">acorncourt</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$\mathbf{1 2 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">adventureland</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">$\mathbf{2 7 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">afflicted</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{8}$</td>
<td style="text-align: center;">$\mathbf{8}$</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">awaken</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">169</td>
<td style="text-align: center;">197.8</td>
<td style="text-align: center;">207.9</td>
<td style="text-align: center;">317.7</td>
<td style="text-align: center;">291.3</td>
<td style="text-align: center;">$\mathbf{3 2 6 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">dragon</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">-5.3</td>
<td style="text-align: center;">-3.5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">$\mathbf{8 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">inhumane</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\mathbf{2 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">library</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">$\mathbf{2 2 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">moonlit</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: left;">omnique st</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$\mathbf{1 7 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">pentari</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">$\mathbf{5 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">reverb</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">$\mathbf{8 . 2}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">6.5</td>
</tr>
<tr>
<td style="text-align: left;">snacktime</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\mathbf{2 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">temple</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">$\mathbf{8}$</td>
<td style="text-align: center;">$\mathbf{8}$</td>
<td style="text-align: center;">7.8</td>
</tr>
<tr>
<td style="text-align: left;">ztun</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">$\mathbf{8 7 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">advent</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">$\mathbf{6 3 . 9}$</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: left;">balances</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$\mathbf{1 1 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">deephome</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
</tr>
<tr>
<td style="text-align: left;">gold</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$\mathbf{4 . 1}$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: left;">jewel</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\mathbf{6 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">karu</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\mathbf{1 0}$</td>
<td style="text-align: center;">$\mathbf{1 0}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">ludicorp</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$\mathbf{2 3 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">yamomma</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
</tr>
<tr>
<td style="text-align: left;">zenon</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\mathbf{4 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">zork1</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">$\mathbf{4 4 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$\mathbf{3 . 6 3}$</td>
<td style="text-align: center;">2.83</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">anchor</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: left;">enchanter</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$\mathbf{3 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">sorcerer</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">$\mathbf{3 8 . 6}$</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">24.5</td>
</tr>
<tr>
<td style="text-align: left;">spellbrkr</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$\mathbf{4 1 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">spirit</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">$\mathbf{5 . 2}$</td>
<td style="text-align: center;">4.2</td>
</tr>
<tr>
<td style="text-align: left;">tryst205</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$\mathbf{1 3 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Best agent</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6 (18\%)</td>
<td style="text-align: center;">6 (18\%)</td>
<td style="text-align: center;">5 (15\%)</td>
<td style="text-align: center;">12 (36\%)</td>
<td style="text-align: center;">10 (30\%)</td>
<td style="text-align: center;">24 (73\%)</td>
</tr>
</tbody>
</table>
<p>Table 3: Average raw score on the Jericho games. We denote with colors the difficulty of the games (green for possible games, yellow for difficult games and red for extreme games). The last row reports the fraction and the absolute number of games where an agent achieves the best score. We additionally report human performance (Human - max) and the 100-step results from a human-written walkthrough (Human - Walkthrough 100). Results are taken from the original papers or " - " is used if a result was not reported.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples from the zork1 game, showing the content of the memory and the context similarities, in a situation where the agent is able to reuse a previous experience and in a case where the revise step is needed.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Fraction of times that a retrieved action is reused successfully on TWC (top). Fraction of times that the neural agent would have picked a rewarded action when CBR is used successfully (bottom).</p>
<p>Ablation studies. In order to understand the role of the main modules of our CBR agent, we designed some ablation studies. First, instead of using the seeded GAT, we define the context of a state-action pair context $\left(s_{t}, a_{t}\right)$ as just one of the entities that $a_{t}$ is applied to. This definition suits well the TWC games because rewarded actions are always applied to one target object and a location for that object (see Appendix G for details). Note that, since the set of entities is discrete, no context quantization is needed. We report the performance of the resulting BiKE + CBR (w/o GAT) agent in</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;">Hard</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\boldsymbol{\sim}$</td>
<td style="text-align: center;">$\mathbf{B i K E}+\mathbf{C B R}(\mathbf{w} / \mathbf{0} \mathbf{G A T})$</td>
<td style="text-align: center;">$16.32 \pm 1.10$</td>
<td style="text-align: center;">$36.13 \pm 1.40$</td>
<td style="text-align: center;">$45.72 \pm 0.63$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{B i K E}+\mathbf{C B R}(\mathbf{w} / \mathbf{0} \mathbf{V Q})$</td>
<td style="text-align: center;">$22.67 \pm 1.23$</td>
<td style="text-align: center;">$43.18 \pm 2.10$</td>
<td style="text-align: center;">$49.21 \pm 0.55$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{b}$</td>
<td style="text-align: center;">$\mathbf{B i K E}+\mathbf{C B R}(\mathbf{w} / \mathbf{0} \mathbf{G A T})$</td>
<td style="text-align: center;">$18.15 \pm 1.51$</td>
<td style="text-align: center;">$37.10 \pm 1.41$</td>
<td style="text-align: center;">$46.70 \pm 0.71$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{\approx}$</td>
<td style="text-align: center;">$\mathbf{B i K E}+\mathbf{C B R}(\mathbf{w} / \mathbf{0} \mathbf{V Q})$</td>
<td style="text-align: center;">$27.75 \pm 2.11$</td>
<td style="text-align: center;">$44.55 \pm 1.67$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of the ablation study on $T W C$, evaluated based on the number of steps (#Steps) to solve the games.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Number of entries in the memory over training.</p>
<p>Table 4. The results show that CBR on $T W C$ is effective even with this simple context definition, but the lower performance of the agent demonstrates the advantage of incorporating additional context information. Finally, we investigate the role played by vector quantization, by experimenting with an agent ( $\mathbf{B i K E}+\mathbf{C B R} \mathbf{w} / \mathbf{0} \mathbf{V Q}$ ) that stores the continuous context representations. In general, this poses scalability challenges, but since $T W C$ has only 5 games per difficulty level, each with a small number of objects, we were able to evaluate the performance of this agent on the three levels separately. The results, reported in Table 4, show that this agent performs much worse than the other CBR implementations. This happens because storing continuous representations over the training results in duplicated entries in the memory and makes it harder to retrieve meaningful experiences. Figure 5 demonstrates how the size (number of entries) in the memory grows over the training time. In this experiment, we trained the agent on all difficulty levels at the same time, resulting in the implementation running out of memory (OOM) on the GPU. More ablation studies are reported in Appendix C, D, E, F, and G.</p>
<h1>7 RELATED WORK</h1>
<p>Text-based RL. TBGs are a rich domain for studying grounded language understanding and how text information can be utilized in control. Prior work has explored text-based RL to learn strategies for multi-user dungeon games (Narasimhan et al., 2015) and other environments (Branavan et al., 2012). Zahavy et al. (2018) proposed the Action-Elimination Deep Q-Network (AE-DQN), which learns to predict invalid actions in the text-adventure game Zork. Recently, Côté et al. (2018) introduced TextWorld, a sandbox learning environment for training and evaluating RL agents on text-based games. On the same line, Murugesan et al. (2021b) introduced TWC that requires agents with commonsense knowledge (Murugesan et al., 2020; Basu et al., 2021). The LeDeepChef system (Adolphs \&amp; Hofmann, 2019) achieved good results on the First TextWord Problems (Trischler et al., 2019) by supervising the model with entities from FreeBase, allowing the agent to generalize to unseen objects. A recent line of work learns symbolic (typically graph-structured) representations of the agent's belief. Notably, Ammanabrolu \&amp; Riedl (2019) proposed KG-DQN and Adhikari et al. (2020) proposed GATA. We also use graphs to model the state of the game.</p>
<p>Case-based reasoning in RL. In the context of RL, CBR has been used to speed up and improve transfer learning in heuristic-based RL. Celiberto Jr et al. (2011) and Bianchi et al. (2018) have shown that cases collected from one domain can be used as heuristics to achieve faster convergence when learning an RL algorithm on a different domain. In contrast to these works, we present a scalable way of using CBR alongside deep RL methods in settings with very large state spaces. More recently, CBR has been successfully applied in the field of knowledge-based reasoning. Das et al. (2020) and Das et al. (2021) show that CBR can effectively learn to generate new logical reasoning chains from prior cases, to answer questions on knowledge graphs.</p>
<h2>8 CONCLUSION AND FUTURE WORK</h2>
<p>In this work, we proposed new agents for TBGs using case-based reasoning. In contrast to expensive deep RL approaches, CBR simply builds a collection of its past experiences and uses the ones relevant to the current situation to decide upon its next action in the game. Our experiments showed that CBR when combined with existing RL agents can make them more efficient and aid generalization in out-of-distribution settings. Even though CBR was quite successful in the TBGs explored in our work, future work is needed to understand the limitations of CBR in such settings.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>This work was funded in part by an IBM fellowship to SD and in part by a small project grant to MS from the Hasler Foundation.</p>
<h2>REFERENCES</h2>
<p>Agnar Aamodt and Enric Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. AI communications, 7(1):39-59, 1994.</p>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L Hamilton. Learning dynamic knowledge graphs to generalize on text-based games. arXiv preprint arXiv:2002.09127, 2020.</p>
<p>Leonard Adolphs and Thomas Hofmann. Ledeepchef: Deep reinforcement learning agent for families of text-based games. ArXiv, abs/1909.01646, 2019.</p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. arXiv preprint arXiv:2001.08837, 2020.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3557-3565, 2019.</p>
<p>Prithviraj Ammanabrolu, Ethan Tien, Matthew J. Hausknecht, and Mark O. Riedl. How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. CoRR, abs/2006.07409, 2020. URL https://arxiv.org/abs/2006.07409.</p>
<p>Mattia Atzeni and Maurizio Atzori. Translating natural language to code: An unsupervised ontology-based approach. In First IEEE International Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2018, Laguna Hills, CA, USA, September 26-28, 2018, pp. 1-8. IEEE Computer Society, 2018. doi: 10.1109/AIKE.2018.00009. URL https://doi.org/ 10.1109/AIKE.2018.00009.</p>
<p>Mattia Atzeni, Jasmina Bogojeska, and Andreas Loukas. SQALER: Scaling question answering by decoupling multi-hop and logical reasoning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=2CQQ_C1i0b.</p>
<p>Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron C. Courville. Systematic generalization: What is required and can it be learned? In $I C L R$ 2019, 2019.</p>
<p>Dana H. Ballard. An introduction to natural computation. Complex adaptive systems. MIT Press, 2000. ISBN 978-0-262-52258-8.</p>
<p>Kinjal Basu, Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Tim Klinger, Murray Campbell, Mrinmaya Sachan, and Gopal Gupta. A hybrid neuro-symbolic approach for text-based games using inductive logic programming. In Combining Learning and Reasoning: Programming Languages, Formalisms, and Representations, 2021.</p>
<p>Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL http://arxiv.org/abs/1308.3432.</p>
<p>Reinaldo AC Bianchi, Paulo E Santos, Isaac J Da Silva, Luiz A Celiberto, and Ramon Lopez de Mantaras. Heuristically accelerated reinforcement learning by means of case-based reasoning and transfer learning. Journal of Intelligent \&amp; Robotic Systems, 91(2):301-312, 2018.</p>
<p>SRK Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a montecarlo framework. Journal of Artificial Intelligence Research, 43:661-704, 2012.</p>
<p>Luiz A Celiberto Jr, Jackson P Matsuura, Ramon Lopez De Mantaras, and Reinaldo AC Bianchi. Using cases as heuristics in reinforcement learning: a transfer learning application. In TwentySecond International Joint Conference on Artificial Intelligence, 2011.</p>
<p>Ting Chen, Martin Renqiang Min, and Yizhou Sun. Learning k-way d-dimensional discrete codes for compact embedding representations. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 853-862. PMLR, 2018. URL http://proceedings.mlr.press/v80/chen18g. html.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018.</p>
<p>Rajarshi Das, Ameya Godbole, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. A simple approach to case-based reasoning in knowledge bases. arXiv preprint arXiv:2006.14198, 2020.</p>
<p>Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay-Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. Case-based reasoning for natural language queries over knowledge bases. arXiv preprint arXiv:2104.08762, 2021.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 41714186. Association for Computational Linguistics, 2019. URL https://www.aclweb.org/ anthology/N19-1423/.</p>
<p>Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7755-7765, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.624. URL https://aclanthology.org/2020. emnlp-main. 624 .</p>
<p>Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006), 17-22 June 2006, New York, NY, USA, pp. 1735-1742. IEEE Computer Society, 2006. doi: 10.1109/CVPR.2006.100. URL https://doi.org/10.1109/CVPR. 2006.100.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7903-7910, 2020.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10. 18653/v1/p16-1153. URL https://doi.org/10.18653/v1/p16-1153.</p>
<p>Pavan Kapanipathi, Veronika Thost, Siva Sankalp Patel, Spencer Whitehead, Ibrahim Abdelaziz, Avinash Balakrishnan, Maria Chang, Kshitij Fadnis, Chulaka Gunasekara, Bassem Makni, Nicholas Mattei, Kartik Talamadupula, and Achille Fokoue. Infusing knowledge into the textual entailment task using graph convolutional networks. AAAI, 2020.</p>
<p>Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A</p>
<p>comprehensive method on realistic data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SygcCnNKwr.</p>
<p>Janet L Kolodner. Reconstructive memory: A computer model. Cognitive science, 7(4):281-328, 1983.</p>
<p>Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. doi: 10.1017/S0140525X16001837.
K. Murugesan, Subhajit Chaudhury, and Kartik Talamadupula. Eye of the beholder: Improved relation generalization for text-based reinforcement learning agents. ArXiv, abs/2106.05387, 2021a.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, and Kartik Talamadupula. Enhancing text-based reinforcement learning agents with commonsense knowledge. CoRR, abs/2005.00811, 2020. URL https://arxiv.org/abs/2005.00811.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. In Thirty Fifth AAAI Conference on Artificial Intelligence, 2021b.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 719-725, 2021c.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.</p>
<p>Mrinmaya Sachan. Knowledge graph embedding compression. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 2681-2691. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.238. URL https:// doi.org/10.18653/v1/2020.acl-main. 238.</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http:// arxiv.org/abs/1910.01108.</p>
<p>Roger C Schank. Dynamic memory: A theory of reminding and learning in computers and people. cambridge university press, 1983.</p>
<p>Ishika Singh, Gargi Singh, and Ashutosh Modi. Pre-trained language models as prior knowledge for playing text-based games. CoRR, abs/2107.08408, 2021. URL https://arxiv.org/abs/ 2107.08408 .</p>
<p>Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. Open domain question answering using early fusion of knowledge bases and text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4231-4242, 2018.</p>
<p>Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249.</p>
<p>Adam Trischler, Marc-Alexandre Côté, and Pedro Lima. First TextWorld Problems, the competition: Using text-based games to advance capabilities of AI agents, 2019.</p>
<p>Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579-2605, 2008. URL http://www.jmlr.org/papers/v9/ vandermaaten08a.html.</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.</p>
<p>Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, and Chengqi Zhang. Deep reinforcement learning with transformers for text adventure games. In IEEE Conference on Games, CoG 2020, Osaka, Japan, August 24-27, 2020, pp. 65-72. IEEE, 2020. doi: 10.1109/CoG47356.2020.9231622. URL https://doi.org/10.1109/CoG47356.2020.9231622.</p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 3562-3573, 2018.</p>
<h1>A TRAINING DETAILS</h1>
<p>All agents used in our experiments are trained with an Advantage Actor Critic (A2C) method according to the general scheme defined below. We use the $n$-step temporal difference method (Sutton \&amp; Barto, 2018) to compute the return $R\left(s_{t}, a_{t}\right)$ of a single time step $t$ in a session of length $T$ as:</p>
<p>$$
R\left(s_{t}, a_{t}\right)=\gamma^{T-t} V\left(s_{T}\right)+\sum_{i=0}^{T-t} \gamma^{i} r\left(s_{t+i}, a_{t+i}\right)
$$</p>
<p>where $V\left(s_{T}\right)$ denotes the value of $s_{T}$ computed by the critic network, $\gamma$ is the discount factor, and $r$ is the reward function. We then compute the advantage $A\left(s_{t}, a_{t}\right)=R\left(s_{t}, a_{t}\right)-V\left(s_{t}\right)$. The final objective term consists of four separate objectives. First, we have $\mathcal{L}_{\pi}^{(t)}$ that denotes the objective of the policy, which tries to maximize the the advantage $A$ :</p>
<p>$$
\mathcal{L}<em t="t">{\pi}^{(t)}=-A\left(s</em>^{}, a_{t<em>}\right) \log \pi\left(a_{t}^{</em>} \mid s_{t}\right)
$$</p>
<p>We then add the objective of the critic as:</p>
<p>$$
\mathcal{L}<em t="t">{v}^{(t)}=\frac{1}{2}\left(R\left(s</em>
$$}, a_{t}^{*}\right)-V\left(s_{t}\right)\right)^{2</p>
<p>$\mathcal{L}_{v}^{(t)}$ encourages the value of the critic $V$ to better estimate the reward $R$ by reducing the mean squared error between them. To prevent the policy from assigning a large weight on a single action, we perform entropy regularization by introducing an additional term:</p>
<p>$$
\mathcal{L}<em a__t="a_{t">{e}^{(t)}=\eta \cdot \sum</em>} \in \mathcal{A<em t="t">{t}} \pi\left(a</em>\right)
$$} \mid s_{t}\right) \cdot \log \pi\left(a_{t} \mid s_{t</p>
<p>The $\eta$ parameter helps balance the exploration-exploitation trade-off for the policy. The sum of the above objectives defines the loss when the neural agent $\pi$ is used to select the action $a_{t}^{*}$. In case actions are reused from the memory, then we use the contrastive loss as discussed in Section 5, namely we compute the loss as:</p>
<p>$$
\mathcal{L}<em t="t">{r}^{(t)}=\frac{1}{2}\left(1-y</em>
$$}\right)\left(1-\operatorname{sim}\left(c_{t}, c_{t}^{\mathcal{M}}\right)\right)^{2}+\frac{1}{2} y_{t} \max \left{0, \mu-1+\operatorname{sim}\left(c_{t}, c_{t}^{\mathcal{M}}\right)\right}^{2</p>
<h2>B EnHANCING BASELINE AGENTS WITH CBR ON JERICHO</h2>
<p>In this section, we report additional experimental results on a subset of the Jericho games, in order to show the performance improvement obtained by different baseline agents when enhanced with case-based reasoning. Table 5 shows the results obtained when coupling the agents described in Section 6.1 with CBR. Similarly to what we discussed for TWC in Section 6.2, we observe that CBR consistently improves the performance of all the agents. The best performing agent is $\mathbf{B i K E}+$ CBR, which is the agent that we evaluated on the complete set of games in Section 6.3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">KG-A2C</th>
<th style="text-align: center;">KG-A2C + CBR</th>
<th style="text-align: center;">Text</th>
<th style="text-align: center;">Text + CBR</th>
<th style="text-align: center;">TPC</th>
<th style="text-align: center;">TPC + CBR</th>
<th style="text-align: center;">BiKE</th>
<th style="text-align: center;">BiKE + CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">207.9</td>
<td style="text-align: center;">$\mathbf{2 5 5 . 6}$</td>
<td style="text-align: center;">205.8</td>
<td style="text-align: center;">$\mathbf{2 4 2 . 3}$</td>
<td style="text-align: center;">245.6</td>
<td style="text-align: center;">$\mathbf{3 1 5 . 1}$</td>
<td style="text-align: center;">278.2</td>
<td style="text-align: center;">$\mathbf{3 2 6 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">inhumane</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\mathbf{1 5 . 6}$</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">$\mathbf{1 4 . 5}$</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">$\mathbf{1 8 . 3}$</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">$\mathbf{2 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">snacktime</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\mathbf{1 5 . 5}$</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">$\mathbf{9 . 8}$</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">$\mathbf{1 9 . 3}$</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">$\mathbf{2 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">karo</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: left;">zork1</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">$\mathbf{3 4 . 2}$</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">$\mathbf{3 8 . 2}$</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">$\mathbf{3 9 . 2}$</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">$\mathbf{4 4 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$\mathbf{1 . 7}$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\mathbf{1 . 6}$</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">$\mathbf{1 . 9}$</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">$\mathbf{3 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">enchanter</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">$\mathbf{2 6 . 2}$</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">$\mathbf{2 6 . 4}$</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">$\mathbf{2 4 . 5}$</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">$\mathbf{3 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">spellbrkr</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">$\mathbf{3 6 . 1}$</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">$\mathbf{3 3 . 2}$</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">$\mathbf{4 0 . 2}$</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">$\mathbf{4 1 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Additional results on Jericho showing the performance improvement obtained by enhancing several baseline agents with CBR.</p>
<h2>C Ablation STUDY ON MEMORY ACCESS</h2>
<p>In this section, we investigate the effectiveness of our approach based on vector quantization for efficient memory access. We consider several variants, where VQ is either dropped completely or replaced with other techniques.</p>
<h1>C. 1 Alternatives to VECTOR QUANTIZATION</h1>
<p>All the agents we consider are variants of the best-performing agent on Jericho and TWC, namely the BiKE + CBR agent. We experiment with the following techniques.</p>
<ul>
<li>BiKE + CBR (w/o VQ) completely removes the vector quantization and stores the continuous context representations as keys in the case memory. In order to make this feasible, for the experiments on Jericho, we limit the size of the memory to the 5000 most recent entries.</li>
<li>BiKE + CBR (RP) relies on random projection (RP) in order to reduce the dimensionality of the context representations stored by the CBR approach. In this case, each context representation is projected into a $p$-dimensional space using a random matrix $R \in \mathbb{R}^{p \times d}$, with components drawn from a normal distribution $N\left(0, \frac{1}{p}\right)$ with mean 0 and standard deviation $\frac{1}{p}$.</li>
<li>BiKE + CBR (SRP) employs sign random projection (SRP) in order to obtain a discrete representation of the context. In this variant, after projection to a $p$-dimensional space, the context representations are discretized by applying an element-wise sign function.</li>
<li>BiKE + CBR (LSH) replaces vector quantization with locality sensitive hashing (LSH). In this case, context representations are converted into $h$-bit hash codes for $l$ different hash tables. The retrieve step selects the representation with the highest cosine similarity to the query context encoding, among the vectors falling in the same bucket.</li>
</ul>
<h2>C. 2 ReSults and DiSCuSSion</h2>
<p>Table 6 shows the scores achieved by each variant of the BiKE + CBR method on a subset of the Jericho games. We notice that the approaches relying on continuous context representations (w/o VQ and RP) perform poorly compared to the others and to the plain BiKE agent. This confirms our hypothesis that CBR needs discrete context representations to make the retrieve step more stable.
The BiKE + CBR (LSH) agent, which relies on locality sensitive hashing, achieves competitive results and consistently outperforms the BiKE agent. This shows that locality sensitive hashing can be a viable alternative to vector quantization. However, we observe that our BiKE + CBR agent, based on vector quantization, achieves better results on almost all the games, confirming the benefit of learning the discrete representation as well. Note also that LSH requires storing the complete continuous representations to be able to detect false positives, namely contexts with the same hash code as the query vector, but low cosine similarity. This makes this alternative less memory efficient compared to our implementation based on VQ.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">BiKE + CBR (w/o VQ)</th>
<th style="text-align: center;">BiKE + CBR (RP)</th>
<th style="text-align: center;">BiKE + CBR (SRP)</th>
<th style="text-align: center;">BiKE + CBR (LSH)</th>
<th style="text-align: center;">BiKE + CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">205.2</td>
<td style="text-align: center;">203.1</td>
<td style="text-align: center;">223.2</td>
<td style="text-align: center;">319.1</td>
<td style="text-align: center;">$\mathbf{3 2 6 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">inhumane</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">$\mathbf{2 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">snacktime</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">$\mathbf{2 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">karn</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: left;">zork1</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">$\mathbf{4 4 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">$\mathbf{3 . 6}$</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">enchanter</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">$\mathbf{3 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">spellbrkr</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">$\mathbf{4 1 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation study showing the results obtained on a subset of the Jericho games when the vector quantization is removed (w/o VQ) or replaced with random projection (RP), sign random projection (SRP) or locality sensitive hashing (LSH).</p>
<h2>D Ablation STUDY ON THE RETAIN MODULE</h2>
<p>In this section, we evaluate different alternatives to select which actions should be retained in the memory of the agent.</p>
<h2>D. 1 Alternatives for the Retain MODULE</h2>
<p>In our main experiments described in Section 6, the retain module has been implemented to store the last $k$ context-action pairs in the memory, whenever the reward obtained by the agent is positive.</p>
<p>For Jericho, we set $k=3$, as specified in Appendix J. However, other design choices are possible. We consider the following variants of the BiKE + CBR agent.</p>
<ul>
<li>BiKE + CBR (rewarded action only) retains only the rewarded context-action pair, without sampling any of the previous actions. This variant is a simple implementation that works well in practice, but may fail to identify useful actions that were not rewarded.</li>
<li>BiKE + CBR (TD error) samples previous context-action pairs based on the temporal difference (TD) error. In details, the agent still retains $k$ context-action pairs: whenever the reward $r_{t}$ at time step $t$ is positive, the rewarded context-action pair $\left(c_{t}^{<em>}, a_{t}^{</em>}\right)$ is retained, together with $k-1$ additional pairs sampled from the current trajectory, with a probability proportional to the TD error.</li>
</ul>
<h1>D. 2 RESULTS AND DISCUSSION</h1>
<p>Table 7 shows the scores obtained by the agents on a subset of the Jericho games. Our main implementation storing the last $k=3$ actions achieves the best results, whereas the BiKE + CBR (rewarded action only) agent performs slightly worse than the other two implementations. The agent based on the TD error achieves competitive results and a state-of-the-art score on the snacktime game. We observe that all variants perform well in practice and achieve overall better results than the baselines in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">BiKE + CBR (rewarded action only)</th>
<th style="text-align: center;">BiKE + CBR (TD error)</th>
<th style="text-align: center;">BiKE + CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">324.1</td>
<td style="text-align: center;">324.8</td>
<td style="text-align: center;">$\mathbf{3 2 6 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">inhumane</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">$\mathbf{2 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">snacktime</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">$\mathbf{2 3 . 4}$</td>
<td style="text-align: center;">22.1</td>
</tr>
<tr>
<td style="text-align: left;">karn</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: left;">zork1</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">$\mathbf{4 4 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">$\mathbf{3 . 2}$</td>
<td style="text-align: center;">$\mathbf{3 . 2}$</td>
<td style="text-align: center;">$\mathbf{3 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">enchanter</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">$\mathbf{3 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">spellbrkr</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">$\mathbf{4 1 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study showing the results obtained on a subset of the Jericho games when only the contextaction pair achieving positive reward is retained (rewarded action only), when context-action pairs are sampled using the TD error (TD error), and when the last 3 pairs are retained (BiKE + CBR).</p>
<h2>E TRAINING THE AGENT AND THE RETRIEVER JOINTLY</h2>
<p>In this experiment, we try to understand if training the neural agent and the CBR retriever jointly would improve upon our choice of just training the two networks separately.</p>
<p>Our implementation works by training the neural agent $\pi$ and the CBR retriever separately with different objectives, as described in Appendix A. However, we can make the neural agent aware of the CBR retriever and train the two networks jointly. In this case, we need to modify the architecture of the neural agent $\pi$ in order to take into account the action candidates $\tilde{\mathcal{A}}<em t="t">{t}$ produced by the CBR. In details, we compute an action selector by attention between the valid actions $\mathcal{A}</em>$ and we concatenate this action selector to the vector used by the neural agent to score admissible actions. Then, the agent and the retriever can be trained jointly, optimizing an objective given by the sum of all the losses in Section A.}$ and the candidates $\tilde{\mathcal{A}}_{t</p>
<p>Table 8 and 9 show the results obtained by the baseline agents when they are trained jointly with the CBR retriever. On TWC, we observe that the joint variants of the agents achieve comparable results with their counterparts in Table 1 and 2. On Jericho, the agents trained jointly with the retriever achieve strong results, but they perform slightly worse than our main approach that keeps the retriever separate from the neural agent. This shows that the joint version is slightly harder to train. Also, it brings the disadvantage that the architecture of the neural agent has to be changed to take the CBR into account, whereas our main approach that keeps the retriever separate allows readily plugging any on-policy agent for TBGs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{Z}$</td>
<td style="text-align: left;">Text + CBR (joint)</td>
<td style="text-align: center;">$17.91 \pm 3.80$</td>
<td style="text-align: center;">$0.91 \pm 0.04$</td>
<td style="text-align: center;">$40.22 \pm 1.70$</td>
<td style="text-align: center;">$0.65 \pm 0.05$</td>
<td style="text-align: center;">$47.94 \pm 1.10$</td>
<td style="text-align: center;">$0.33 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TPC + CBR (joint)</td>
<td style="text-align: center;">$16.80 \pm 1.97$</td>
<td style="text-align: center;">$0.94 \pm 0.04$</td>
<td style="text-align: center;">$36.12 \pm 1.32$</td>
<td style="text-align: center;">$0.67 \pm 0.03$</td>
<td style="text-align: center;">$46.10 \pm 0.72$</td>
<td style="text-align: center;">$0.41 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KG-A2C + CBR (joint)</td>
<td style="text-align: center;">$16.40 \pm 1.89$</td>
<td style="text-align: center;">$0.95 \pm 0.05$</td>
<td style="text-align: center;">$36.50 \pm 1.13$</td>
<td style="text-align: center;">$0.68 \pm 0.03$</td>
<td style="text-align: center;">$46.58 \pm 0.91$</td>
<td style="text-align: center;">$0.40 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BiKE + CBR (joint)</td>
<td style="text-align: center;">$16.01 \pm 1.37$</td>
<td style="text-align: center;">$0.94 \pm 0.03$</td>
<td style="text-align: center;">$35.93 \pm 1.11$</td>
<td style="text-align: center;">$0.67 \pm 0.05$</td>
<td style="text-align: center;">$46.11 \pm 1.14$</td>
<td style="text-align: center;">$0.42 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{Z}$</td>
<td style="text-align: left;">Text + CBR (joint)</td>
<td style="text-align: center;">$21.47 \pm 2.32$</td>
<td style="text-align: center;">$0.88 \pm 0.07$</td>
<td style="text-align: center;">$39.10 \pm 1.33$</td>
<td style="text-align: center;">$0.67 \pm 0.02$</td>
<td style="text-align: center;">$48.10 \pm 0.92$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TPC + CBR (joint)</td>
<td style="text-align: center;">$17.89 \pm 1.82$</td>
<td style="text-align: center;">$0.93 \pm 0.01$</td>
<td style="text-align: center;">$38.11 \pm 1.33$</td>
<td style="text-align: center;">$0.65 \pm 0.03$</td>
<td style="text-align: center;">$47.92 \pm 1.55$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KG-A2C + CBR (joint)</td>
<td style="text-align: center;">$18.19 \pm 2.12$</td>
<td style="text-align: center;">$0.93 \pm 0.02$</td>
<td style="text-align: center;">$37.72 \pm 2.91$</td>
<td style="text-align: center;">$0.66 \pm 0.03$</td>
<td style="text-align: center;">$47.53 \pm 1.11$</td>
<td style="text-align: center;">$0.40 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BiKE + CBR (joint)</td>
<td style="text-align: center;">$18.12 \pm 1.21$</td>
<td style="text-align: center;">$0.94 \pm 0.05$</td>
<td style="text-align: center;">$35.77 \pm 1.05$</td>
<td style="text-align: center;">$0.69 \pm 0.05$</td>
<td style="text-align: center;">$46.16 \pm 1.00$</td>
<td style="text-align: center;">$0.40 \pm 0.04$</td>
</tr>
</tbody>
</table>
<p>Table 8: Test-set results obtained on TWC in-distribution (IN) and out-of-distribution (OUT) games training different neural agents and the CBR retriever jointly.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">KG-A2C + CBR (joint)</th>
<th style="text-align: center;">Text + CBR (joint)</th>
<th style="text-align: center;">TPC + CBR (joint)</th>
<th style="text-align: center;">BiKE + CBR (joint)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">250.1</td>
<td style="text-align: center;">241.5</td>
<td style="text-align: center;">$\mathbf{3 1 6 . 2}$</td>
<td style="text-align: center;">324.2</td>
</tr>
<tr>
<td style="text-align: left;">inhumane</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: left;">snacktime</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">$\mathbf{1 1 . 2}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 1}$</td>
<td style="text-align: center;">21.8</td>
</tr>
<tr>
<td style="text-align: left;">karn</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">zork1</td>
<td style="text-align: center;">$\mathbf{3 6 . 4}$</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">43.7</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">$\mathbf{3 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">enchanter</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">35.6</td>
</tr>
<tr>
<td style="text-align: left;">spellbrkr</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">$\mathbf{3 3 . 3}$</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">$\mathbf{4 1 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Results obtained on a subset of the Jericho games training different neural agents and the CBR retriever jointly. Bold values indicate when the joint variant achieves better scores than the main counterparts reported in Table 5.</p>
<h1>F MULTI-PARAGRAPH TEXT-BASED RETRIEVER</h1>
<p>Knowledge graphs have been used extensively in text-based games and other areas of natural language understanding (Ammanabrolu \&amp; Hausknecht, 2020; Atzeni \&amp; Atzori, 2018; Kapanipathi et al., 2020). This section describes an alternative to our graph-based retriever, which only relies on the textual observations without modeling the state of the game as a graph.</p>
<p>Recent work (Guo et al., 2020) has shown that enriching the current observation with relevant observations retrieved from the history of interactions with the environment can achieve competitive results on Jericho. Therefore, in order to assess the effectiveness of our graph-based implementation, we compare to a multi-paragraph text-based retriever (MTPR) inspired by the work of Guo et al. (2020). In this case, we do not model the state as a graph and, subsequently, we remove the seeded graph attention mechanism from the retriever. Instead, given the current natural language observation $o_{t}$, we compute an action-specific representation following Guo et al. (2020), concatenating $o_{t}$ with the $n$ most recent observations that share objects with it or with the given action. The encoded observation is then discretized using vector quantization as in our main architecture.</p>
<p>We evaluated the text-based retriever on Jericho, integrating it in the same baseline agents described in Section 6.1. Table 10 shows the scores obtained by the agents. We observe that, overall, the graph-based retriever performs better on the vast majority of the games. This result confirms the ability of our approach based on seeded graph attention to extract relevant information from the state of the game, compared to a retriever that only relies on text information.</p>
<h2>G ADDITIONAL RESULTS USING ENTITIES AS CONTEXT SELECTORS</h2>
<p>As mentioned in Section 6.4, we performed an ablation study where the seeded graph attention and the state graph where not used in the retriever. Instead, we represent the context as just a single focus entity. This choice suits very well the TWC games, as the goal of each game is to tidy up a house by putting objects in their commonsensical locations. Hence, each rewarded action in TWC is of the form "put o on s" or "insert o in c", where $o$ is an entity of type object, $s$ is a supporter, and $c$ is a</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">KG-A2C + CBR (MPTR)</th>
<th style="text-align: center;">Text + CBR (MPTR)</th>
<th style="text-align: center;">TPC + CBR (MPTR)</th>
<th style="text-align: center;">BiKE + CBR (MPTR)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">245.2</td>
<td style="text-align: center;">233.7</td>
<td style="text-align: center;">302.3</td>
<td style="text-align: center;">321.2</td>
</tr>
<tr>
<td style="text-align: left;">inhumane</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">20.3</td>
</tr>
<tr>
<td style="text-align: left;">snacktime</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">19.5</td>
</tr>
<tr>
<td style="text-align: left;">karn</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">zork1</td>
<td style="text-align: center;">$\mathbf{3 6 . 2}$</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">$\mathbf{3 . 2}$</td>
<td style="text-align: center;">$\mathbf{3 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">enchanter</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">32.1</td>
</tr>
<tr>
<td style="text-align: left;">spellbrkr</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">40.8</td>
</tr>
</tbody>
</table>
<p>Table 10: Results obtained on a subset of the Jericho games using the multi-paragraph text-based retriever (MPTR) instead of the graph-based one. Bold values indicate when the MPTR variant achieves better scores than the main counterparts reported in Table 5.
container (Murugesan et al., 2021c; Côté et al., 2018). As an example, a rewarded action could be "insert dirty singlet in washing machine", where dirty singlet is the object and washing machine is the container.</p>
<p>Representing the context as just single entities of type object, means that the memory of the CBR agent is storing what action to apply to each object in the game, and therefore the agent is in practice constructing a registry where each object is paired with its commonsensical location. Let $c_{v}, c_{u}$ be two context entities. The retriever then computes the similarity between the contexts as:</p>
<p>$$
\operatorname{sim}\left(c_{v}, c_{u}\right)=\operatorname{cosine}\left(F F N\left(\mathbf{h}<em u="u">{v}\right), F F N\left(\mathbf{h}</em>\right)\right)
$$</p>
<p>where cosine denotes the cosine similarity, $F F N$ is a 2-layer feed-forward network and $\mathbf{h}<em u="u">{v}, \mathbf{h}</em>$ are the BERT encodings of the [CLS] token for objects $v$ and $u$ respectively. The retriever is therefore encouraged to map objects that should be placed in the same location (either a supporter or a container) to similar representations.
Figure 6 depicts a $t$-SNE (van der Maaten \&amp; Hinton, 2008) visualization of the representations $F F N\left(\mathbf{h}_{v}\right)$ learned by the retriever of the BiKE + CBR (w/o GAT) agent. The plot shows that entities that belong to the same location are mapped to similar representations. This holds both for objects in the in-distribution games and for objects in the out-of-distribution games.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visualization of the entity representations learned by the retriever. Colors denote the target location of each object.</p>
<p>We evaluated all the CBR agents with the simple retriever defined above. Table 11 reports the results for the in-distribution and out-of-distribution games. The results confirm that the entity-based context selection performs well and achieves good out-of-distribution generalization. However, we remark that the complete retriever described in Section 4 consistently achieves better results, showing the importance of incorporating additional structured information.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
</tr>
<tr>
<td style="text-align: center;">Z</td>
<td style="text-align: center;">CBR (w/o GAT)</td>
<td style="text-align: center;">$22.70 \pm 2.05$</td>
<td style="text-align: center;">$0.81 \pm 0.07$</td>
<td style="text-align: center;">$44.13 \pm 1.15$</td>
<td style="text-align: center;">$0.62 \pm 0.04$</td>
<td style="text-align: center;">$48.05 \pm 1.30$</td>
<td style="text-align: center;">$0.33 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text + CBR (w/o GAT)</td>
<td style="text-align: center;">$19.01 \pm 3.99$</td>
<td style="text-align: center;">$0.89 \pm 0.05$</td>
<td style="text-align: center;">$40.10 \pm 1.52$</td>
<td style="text-align: center;">$0.67 \pm 0.05$</td>
<td style="text-align: center;">$47.80 \pm 1.32$</td>
<td style="text-align: center;">$0.33 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TPC + CBR (w/o GAT)</td>
<td style="text-align: center;">$17.15 \pm 2.91$</td>
<td style="text-align: center;">$0.94 \pm 0.04$</td>
<td style="text-align: center;">$38.32 \pm 1.76$</td>
<td style="text-align: center;">$0.66 \pm 0.03$</td>
<td style="text-align: center;">$47.22 \pm 1.35$</td>
<td style="text-align: center;">$0.36 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KG-A2C + CBR (w/o GAT)</td>
<td style="text-align: center;">$16.67 \pm 2.30$</td>
<td style="text-align: center;">$0.96 \pm 0.03$</td>
<td style="text-align: center;">$38.05 \pm 1.84$</td>
<td style="text-align: center;">$0.66 \pm 0.04$</td>
<td style="text-align: center;">$46.45 \pm 1.02$</td>
<td style="text-align: center;">$0.38 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BiKE + CBR (w/o GAT)</td>
<td style="text-align: center;">$16.32 \pm 1.10$</td>
<td style="text-align: center;">$0.95 \pm 0.03$</td>
<td style="text-align: center;">$36.13 \pm 1.40$</td>
<td style="text-align: center;">$0.67 \pm 0.04$</td>
<td style="text-align: center;">$45.72 \pm 0.63$</td>
<td style="text-align: center;">$0.41 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Z } \ &amp; \text { 念 } \end{aligned}$</td>
<td style="text-align: center;">CBR (w/o GAT)</td>
<td style="text-align: center;">$23.90 \pm 2.17$</td>
<td style="text-align: center;">$0.79 \pm 0.05$</td>
<td style="text-align: center;">$44.71 \pm 1.50$</td>
<td style="text-align: center;">$0.61 \pm 0.04$</td>
<td style="text-align: center;">$48.87 \pm 1.89$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text + CBR (w/o GAT)</td>
<td style="text-align: center;">$21.64 \pm 2.52$</td>
<td style="text-align: center;">$0.88 \pm 0.02$</td>
<td style="text-align: center;">$41.12 \pm 1.21$</td>
<td style="text-align: center;">$0.66 \pm 0.05$</td>
<td style="text-align: center;">$48.00 \pm 1.10$</td>
<td style="text-align: center;">$0.32 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TPC + CBR (w/o GAT)</td>
<td style="text-align: center;">$19.82 \pm 2.13$</td>
<td style="text-align: center;">$0.92 \pm 0.03$</td>
<td style="text-align: center;">$39.34 \pm 1.01$</td>
<td style="text-align: center;">$0.67 \pm 0.02$</td>
<td style="text-align: center;">$47.33 \pm 1.30$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KG-A2C + CBR (w/o GAT)</td>
<td style="text-align: center;">$19.07 \pm 2.50$</td>
<td style="text-align: center;">$0.92 \pm 0.02$</td>
<td style="text-align: center;">$38.41 \pm 1.94$</td>
<td style="text-align: center;">$0.65 \pm 0.04$</td>
<td style="text-align: center;">$46.89 \pm 2.21$</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BiKE + CBR (w/o GAT)</td>
<td style="text-align: center;">$18.15 \pm 1.51$</td>
<td style="text-align: center;">$0.92 \pm 0.03$</td>
<td style="text-align: center;">$37.10 \pm 1.41$</td>
<td style="text-align: center;">$0.67 \pm 0.03$</td>
<td style="text-align: center;">$46.70 \pm 0.71$</td>
<td style="text-align: center;">$0.39 \pm 0.03$</td>
</tr>
</tbody>
</table>
<p>Table 11: Test-set performance for TWC in-distribution (IN) and out-of-distribution (OUT) games using entities as context selectors.</p>
<h1>H CASE-BASED REASONING AND OUT-OF-DISTRIBUTION GENERALIZATION</h1>
<p>Out-of-distribution generalization has recently fueled significant research effort and several datasets and approaches have been proposed in the past few years (Bahdanau et al., 2019; Keysers et al., 2020; Atzeni et al., 2021). Our experiments on TWC allowed assessing the hypothesis that case-based reasoning can be used to tackle out-of-distribution (OOD) generalization in text-based games. Table 12 shows the absolute OOD generalization gap of the different agents evaluated in our experiments. Note that this table does not report any new result, but it simply provides the absolute difference between the values in Table 1 and 2. We observe that the agents relying on CBR achieve a considerably better generalization performance out of the training distribution, almost comparable to the results obtained on the same distribution as the training data. In some cases, the normalized score achieved by the CBR agents in the out-of-distribution games equals the score obtained in the in-distribution games. This happens because case-based reasoning forces the agent to map contexts including entities that were not seen at training time to the most similar contexts in the CBR memory. CBR allows the agent to solve completely new problems and generalize by effectively retrieving past cases and mapping the retrieved actions from the training distribution to the most similar options in the OOD setting. Note that the only good OOD generalization gap for the agents that are not relying on CBR (the #Steps of the Text agent on the Hard level) is an artifact of the experiment, as all agents were limited to a maximum of 50 steps.</p>
<p>Figure 6 shows a nice example of the capability of the agent to generalize OOD. In this case, entity embeddings were used as the context representations, and we observe that entities that are not included in the training distribution are correctly mapped to the right cluster. This shows that the CBR approach learns effective and generalizable context representations based on the objective of the games. These representations are then used by the agent to select relevant experiences and map the actions used at training time to the most viable alternative in the OOD test set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
</tr>
<tr>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">6.07</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">1.82</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.10</td>
</tr>
<tr>
<td style="text-align: center;">TPC</td>
<td style="text-align: center;">7.15</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">1.55</td>
<td style="text-align: center;">0.13</td>
</tr>
<tr>
<td style="text-align: center;">KG-A2C</td>
<td style="text-align: center;">6.24</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;">BiKE</td>
<td style="text-align: center;">7.32</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">2.81</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;">CBR-only</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">Text + CBR</td>
<td style="text-align: center;">3.38</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">TPC + CBR</td>
<td style="text-align: center;">2.09</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: center;">KG-A2C + CBR</td>
<td style="text-align: center;">2.30</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">BiKE + CBR</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.02</td>
</tr>
</tbody>
</table>
<p>Table 12: Absolute out-of-distribution generalization gap in TWC</p>
<h1>I ADDITIONAL BASELINES ON JERICHO</h1>
<p>Several methods have been proposed recently for text-based games. In order to keep the results in the main paper more compact, we only included in Table 3 well-known and top-performing baselines that were evaluated on the full (or almost) set of Jericho games. For completeness, this section compares our best agent (BiKE + CBR) with the following additional methods:</p>
<ul>
<li>Q*BERT (Ammanabrolu et al., 2020) is a deep reinforcement learning agent that plays text games by building a knowledge graph of the world and answering questions about it;</li>
<li>Trans-v-DRRN (Xu et al., 2020) relies on a lightweight transformer encoder to model the state of the game;</li>
<li>DBERT-DRRN (Singh et al., 2021) makes use of DistilBERT (Sanh et al., 2019) fine-tuned on an independent set of human gameplay transcripts.</li>
</ul>
<p>Table 13 shows the scores obtained by these baselines compared to our agent enhanced with CBR. Overall, we observe that the BiKE + CBR agent outperforms the baselines on the majority of the games, confirming the effectiveness of case-based reasoning as a viable approach to boost the performance of text-based RL agents.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">Q*BERT</th>
<th style="text-align: center;">Trans-v-DRRN</th>
<th style="text-align: center;">DBERT-DRRN</th>
<th style="text-align: center;">BiKE + CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">905</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">acorncourt</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1 2 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">adventureland</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{2 7 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">afflicted</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{3 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">awaken</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">246.1</td>
<td style="text-align: center;">288.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{3 2 6 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">dragon</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.3</td>
</tr>
<tr>
<td style="text-align: left;">inhumane</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{3 2 . 8}$</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">library</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$\mathbf{2 2 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">moonlit</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">omnique</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">$\mathbf{1 7 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">pentari</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{5 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">reverb</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1 0 . 7}$</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">6.5</td>
</tr>
<tr>
<td style="text-align: left;">snacktime</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$\mathbf{2 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">temple</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">$\mathbf{8}$</td>
<td style="text-align: center;">7.8</td>
</tr>
<tr>
<td style="text-align: left;">ztua</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{8 7 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">advent</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: left;">balances</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1 1 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">deephome</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{1}$</td>
</tr>
<tr>
<td style="text-align: left;">gold</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: left;">jewel</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{6 . 5}$</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: left;">karu</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">ludicorp</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">$\mathbf{2 3 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">yamomma</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$\mathbf{1}$</td>
</tr>
<tr>
<td style="text-align: left;">zenon</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.1</td>
</tr>
<tr>
<td style="text-align: left;">zork1</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">$\mathbf{4 4 . 7}$</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">$\mathbf{3 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">anchor</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">enchanter</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{3 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">sorcerer</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.5</td>
</tr>
<tr>
<td style="text-align: left;">spellbrkr</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">$\mathbf{4 1 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">spirit</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">$\mathbf{4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">tryst205</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">$\mathbf{1 3 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 13: Average raw score on the Jericho games. Results are taken from the original papers or " - " is used if a result was not reported.</p>
<h2>J HYPERPARAMETERS AND REPRODUCIbILITY</h2>
<p>All CBR agents are trained using the same hyperparameter settings and the same hardware/software configuration. As mentioned in Section 4, we use a pre-trained BERT model (Devlin et al., 2019) to represent initial node features in the state graph. BERT is only used to compute the initial representations of the entities and is not fine-tuned. We use the following hyperparameters for our experiments.</p>
<ul>
<li>
<p>We set the hidden dimensionality of the model to $d=768$ and we use 12 attention heads for the graph attention network, each applied to 64-dimensional inputs.</p>
</li>
<li>
<p>We use $n_{l}=2$ seeded GAT layers for $T W C$ and $n_{l}=3$ for Jericho.</p>
</li>
<li>On both datasets, we apply a dropout regularization on the seeded GAT with probability of 0.1 at each layer.</li>
<li>Similarly, for the experiments on TWC, we only sample the most recent context-action pair from $\mathcal{T}$, whereas we sample $k=3$ pairs for Jericho. We used $k=2$ for the scalability analysis depicted in Figure 5.</li>
<li>The retriever threshold is kept constant to $\tau=0.7$ across all experiments.</li>
<li>On TWC, we train the agents for 100 episodes and a maximum of 50 steps for each episode. On Jericho, as mentioned, we follow previous work and we train for 100000 valid steps, starting a new episode every 100 steps or whenever the games ends.</li>
<li>We set the discount factor $\gamma$ to 0.9 on all experiments.</li>
<li>For the ablation study on memory access, we set the output dimensionality of the RP and SRP methods to $p=64$. For LSH, we set the number of hash tables to $l=16$ and the length of the hash codes of $h=8$ bits. We artificially limit the size of each bucket to the 4 most recent entries.</li>
</ul>
<p>Experiments were parallelized on a cluster where each node was dedicated to a separate run. The configuration of the execution nodes is as reported in Table 14.</p>
<table>
<thead>
<tr>
<th>Resource</th>
<th>Setting</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz</td>
</tr>
<tr>
<td>Memory</td>
<td>128 GB</td>
</tr>
<tr>
<td>GPUs</td>
<td>1 x NVIDIA Tesla k80 12 GB</td>
</tr>
<tr>
<td>Disk1</td>
<td>100 GB</td>
</tr>
<tr>
<td>Disk2</td>
<td>600 GB</td>
</tr>
<tr>
<td>OS</td>
<td>Ubuntu 18.04-64 Minimal for VSI</td>
</tr>
</tbody>
</table>
<p>Table 14: Hardware and software configuration used to train the agents</p>            </div>
        </div>

    </div>
</body>
</html>
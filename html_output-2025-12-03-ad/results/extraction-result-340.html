<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-340 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-340</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-340</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-257900977</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.17919v1.pdf" target="_blank">Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Grounded understanding of natural language in physical scenes can greatly benefit robots that follow human instructions. In object manipulation scenarios, existing end-to-end models are proficient at understanding semantic concepts, but typically cannot handle complex instructions involving spatial relations among multiple objects. which require both reasoning object-level spatial relations and learning precise pixel-level manipulation affordances. We take an initial step to this challenge with a decoupled two-stage solution. In the first stage, we propose an object-centric semantic-spatial reasoner to select which objects are relevant for the language instructed task. The segmentation of selected objects are then fused as additional input to the affordance learning stage. Simply incorporating the inductive bias of relevant objects to a vision-language affordance learning agent can effectively boost its performance in a custom testbed designed for object manipulation with spatial-related language instructions.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e340.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e340.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic-Spatial Reasoning module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An object-centric Transformer module that fuses pretrained CLIP embeddings of language tokens and cropped object-centric image patches (plus encoded 2D coordinates) to predict a relevance score for each object with respect to a spatial language instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Semantic-Spatial Reasoning (SSR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A lightweight 8-layer self-attention Transformer that takes L word tokens and m object-centric tokens (each object token = CLIP image-patch embedding + linear-encoded (u,v) coordinate + type embedding) and is trained supervised to predict normalized relevance scores over objects; CLIP is used to embed words and image patches (pretrained).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Custom spatial pick-and-place in Ravens</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A language-conditioned pick-and-place task in PyBullet/Ravens where each episode contains 12 objects (6 bowls, 6 blocks) placed randomly; instructions specify a semantic object (color) and a relative spatial relation between two location objects (e.g., "place the X block in the middle of the front A block and the back B bowl" with locations sampled from left/right/front/back). The agent must pick the correct object and place it within 0.1 m of the target position.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (relative object positions and which objects are relevant to the instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained CLIP embeddings (text and image) + supervised training on demonstration dataset (labels: binary relevance per object and pick/place coordinates)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning / training of the SSR Transformer on object-centric crops and language instructions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>object-centric token embeddings combining CLIP image-patch embeddings and 2D coordinate encodings, concatenated with word tokens and processed by self-attention to produce per-object relevance scores (softmax-normalized)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate (proportion of episodes where pick and place within 0.1 m of target)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used end-to-end with downstream affordance model: SSR + ViLD: seen success rates by dataset size — 100 demos: 2%; 1000 demos: 30%; 10k demos: 60%; 100k demos: 65%. SSR + all-mask (oracle segmentation fused): seen — 100 demos: 8%; 1000 demos: 36%; 10k demos: 68%; 100k demos: 71%. Unseen-object rates are substantially lower (e.g., SSR+ViLD 1000d unseen 11%, SSR+all-mask 1000d unseen 14%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully identifies which objects are relevant for complex spatial-language goals (e.g., selecting the two reference objects that define a relative position) and provides a focused attention mask that significantly improves downstream pixel-level affordance prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Performance degrades with imperfect object detection/segmentation (ViLD), and generalization to unseen object colors is weak (unseen success rates often << seen rates). SSR is not a substitute for precise pixel-level affordance learning — it provides relevance but not exact placement.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to CLIPort without added relevant-object masks (end-to-end CLIPort), SSR-based pipelines outperform when limited data is available; CLIPort (no oracle) failed to learn effective policies in the small-data regimes reported, while CLIPort with oracle segmentation (upper bound) achieved e.g. 45%/20% (seen/unseen) at 100 demos and 90%/59% at 1000 demos.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key ablation is inclusion vs removal of relevant-object segmentation: fusing ground-truth relevant-object masks into CLIPort (oracle) markedly improves performance relative to CLIPort without masks. Replacing ground-truth masks with ViLD segmentation (SSR + ViLD) reduces performance slightly versus SSR + all-mask, indicating sensitivity to detector/segmentation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Object-centric, multimodal tokenization (CLIP embeddings of language + object image patches + 2D coordinates) processed by a Transformer can represent object-relational and spatial relations sufficiently to select relevant objects for spatial language instructions; this high-level semantic-spatial signal, when provided as an attention mask, substantially improves downstream pixel-level affordance learning for manipulation, especially in data-limited regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e340.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e340.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort (vision-language affordance predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stream affordance prediction model combining a CLIP-based semantic pathway and a transporter-based spatial pathway to predict pixelwise pick and place affordance maps from language and images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stream network: semantic pathway uses a pretrained CLIP encoder to process text and image for semantic features; spatial pathway uses a transporter-style architecture to predict pixelwise pick/place probability mass Q_pick,place ∈ R^{H×W}; in this work the spatial pathway optionally receives an attention map (selected-object mask) concatenated with RGB input.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Custom spatial pick-and-place in Ravens</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as SSR entry: language-conditioned pick-and-place in cluttered scenes where instructions specify both object identity and relative spatial relation between two reference objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (encoded via CLIP semantic features and pixelwise spatial affordances)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained CLIP for semantic features; trained/fine-tuned on demonstration affordance datasets (supervised imitation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised training / fine-tuning of affordance predictor on demonstrations; optionally provided with fused segmentation masks as additional input (oracle or predicted)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>combination of CLIP semantic embeddings and transporter-style spatial features producing pixelwise affordance maps; spatial attention map (selected-object segmentation) can be concatenated as channels to bias spatial pathway</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate (pick-and-place within 0.1 m)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CLIPort without fused segmentation often failed in the small-data regimes evaluated. With oracle relevant-object segmentation (upper bound): 100 demos seen/unseen = 45% / 20%; 1000 demos seen/unseen = 90% / 59%. (CLIPort no-oracle reported near 0 in the small-data settings in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>With fused object masks (oracle), CLIPort reliably learns to map semantic goals to pixelwise affordances and achieves high success (e.g., 90% at 1000 demos seen). It leverages CLIP semantic features to resolve object identity and uses transporter spatial features to predict precise pick/place pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>End-to-end CLIPort without explicit relevant-object masks struggles to learn the required spatial relational grounding in limited-data regimes; even with oracle masks, generalization to unseen object colors is weaker than seen cases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline CLIPort (no oracle fusion) performed poorly in small-data settings (near 0% reported); CLIPort with oracle segmentation is the strong baseline/upper bound (e.g., 90% at 1000 demos seen).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Adding the selected-object attention map to the spatial pathway (the only architectural change) substantially improves performance compared to the original CLIPort in limited-data settings; lacking this guidance causes dramatic drop in success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Supplying high-level semantic-spatial priors (which objects matter) as an additional input to a pixelwise affordance model allows the model to decouple high-level relational reasoning from low-level manipulation skill learning, improving sample efficiency and effectiveness for spatial-language manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e340.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e340.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained image-text model used here to encode both language tokens and object-centric image patches into joint embeddings that are input to the SSR Transformer and to CLIPort's semantic pathway.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained multimodal (image-text) encoder trained contrastively on (image, caption) pairs to produce aligned embeddings for text and images; used here for encoding words and cropped object-patches, providing semantic grounding between language and visual content.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as perceptual/semantic encoder for the spatial pick-and-place pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides multimodal embeddings for language and object-centric image patches that SSR and CLIPort use to reason about object identity and semantic relations in the scene.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perceptual grounding / semantic encoding for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (semantic concepts and object identity), supports spatial grounding when combined with coordinates and positional tokens</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on large-scale image-text corpora (contrastive learning)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>pretrained embedding extraction (frozen or used as encoder) — embeddings are fed into downstream supervised training</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>distributed continuous embeddings that align visual patches and natural language tokens</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>not directly evaluated alone here (used as encoder); downstream success rates reported for pipelines that use CLIP embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Not reported standalone; enabled SSR and CLIPort to perform the multimodal grounding tasks when combined with supervised training (see SSR and CLIPort results).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides semantic alignment between text mentions of colors/object types and visual patches, enabling the model to associate language references with detected objects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>CLIP alone does not provide fine-grained spatial relation encoding; must be combined with coordinate encodings and relational reasoning (Transformer SSR) to resolve relative positions.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>N/A within this paper; CLIP used as component enabling other models to succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper does not ablate CLIP vs alternatives; CLIP is the chosen semantic encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a pretrained vision-language model like CLIP to embed both text and object crops is an effective way to provide semantic grounding for downstream spatial-relational reasoning and affordance prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e340.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e340.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViLD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViLD (Open-vocabulary object detection via vision and language knowledge distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-the-shelf open-vocabulary object detector used to produce object locations, crops, and segmentations for SSR input at deployment; substituting oracle masks with ViLD slightly reduces end-to-end performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvocabulary Object Detection via Vision and Language Knowledge Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViLD</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-vocabulary detector that distills vision-language knowledge to detect arbitrary categories; used to produce bounding boxes, crops, and segmentation masks for objects so SSR and CLIPort can operate without oracle segmentation in the real deployment scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Object detection / segmentation feeding SSR + CLIPort pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides object-centric crops and segmentation masks for all objects present in the orthographic image; these are used as inputs to SSR and as attention masks (after selection) to affordance predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perceptual preprocessing / object detection</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (object identity) and provides spatial coordinates via bounding boxes/centers</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained detection model (vision-language distillation), used at test/deployment</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>running inference with the pretrained detector to obtain crops and masks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>bounding boxes, segmentation masks, center coordinates and cropped image patches</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream task success rates when using ViLD vs oracle segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>SSR + ViLD (end-to-end, no oracle) achieved lower success rates than SSR + all-mask (oracle). Example seen rates: with 100k demos SSR+ViLD seen 65% vs SSR+all-mask 71%; at 10k demos SSR+ViLD seen 60% vs SSR+all-mask 68%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables deployment without ground-truth segmentation; pipeline still benefits from relevance selection even when using predicted segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Imperfect detections/segmentations from ViLD reduce the quality of the attention map and thus downstream affordance accuracy; causes a measurable drop relative to oracle segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>SSR + all-mask (oracle segmentation) acts as upper bound and outperforms SSR + ViLD by several percentage points across dataset sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Replacing ground-truth segmentation with ViLD segmentation reduces success rates (see performance_result numbers), indicating detector quality is an important component.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an open-vocabulary detector allows the pipeline to operate without oracle segmentation, but detector imperfections materially affect grounding of spatial relations and final manipulation success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e340.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e340.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM planning mentions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models (planning capability mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work mentions that large language models have planning abilities that have been exploited for embodied reasoning tasks, but this paper does not evaluate or implement LLM-based planning itself.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced in related work as models that have demonstrated planning/chain-of-thought abilities for embodied tasks (citations: Ahn et al. 2022; Zeng et al. 2022; Huang et al. 2022), but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced: embodied reasoning / planning for robots (external works)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Papers cited describe leveraging LLMs to generate plans or intermediate reasoning steps for embodied agents, but this paper focuses on a vision+object-centric Transformer for selecting relevant objects instead.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mention: multi-step planning / embodied reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + spatial (as per cited literature), but no direct utilization here</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on large text corpora (in cited works), not applicable to experiments in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>mention of planning ability / chain-of-thought style planning in related work; no elicitation or evaluation performed in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>mentioned as natural-language / plan sequences in other works; not instantiated here</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Not evaluated in this paper; only cited as related approaches for embodied planning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper positions its approach as complementary to LLM-based planning: instead of relying on pure language-model planning without perception, they emphasize object-centric visual grounding and an explicit object-relevance signal to aid pixel-level affordance learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CLIPort: What and where pathways for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Openvocabulary Object Detection via Vision and Language Knowledge Distillation <em>(Rating: 2)</em></li>
                <li>Inner Monologue: Embodied Reasoning through Planning with Language Models <em>(Rating: 2)</em></li>
                <li>Transporter networks: Rearranging the visual world for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-340",
    "paper_id": "paper-257900977",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "SSR",
            "name_full": "Semantic-Spatial Reasoning module",
            "brief_description": "An object-centric Transformer module that fuses pretrained CLIP embeddings of language tokens and cropped object-centric image patches (plus encoded 2D coordinates) to predict a relevance score for each object with respect to a spatial language instruction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Semantic-Spatial Reasoning (SSR)",
            "model_size": null,
            "model_description": "A lightweight 8-layer self-attention Transformer that takes L word tokens and m object-centric tokens (each object token = CLIP image-patch embedding + linear-encoded (u,v) coordinate + type embedding) and is trained supervised to predict normalized relevance scores over objects; CLIP is used to embed words and image patches (pretrained).",
            "task_name": "Custom spatial pick-and-place in Ravens",
            "task_description": "A language-conditioned pick-and-place task in PyBullet/Ravens where each episode contains 12 objects (6 bowls, 6 blocks) placed randomly; instructions specify a semantic object (color) and a relative spatial relation between two location objects (e.g., \"place the X block in the middle of the front A block and the back B bowl\" with locations sampled from left/right/front/back). The agent must pick the correct object and place it within 0.1 m of the target position.",
            "task_type": "object manipulation / instruction following",
            "knowledge_type": "spatial + object-relational (relative object positions and which objects are relevant to the instruction)",
            "knowledge_source": "pretrained CLIP embeddings (text and image) + supervised training on demonstration dataset (labels: binary relevance per object and pick/place coordinates)",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised fine-tuning / training of the SSR Transformer on object-centric crops and language instructions",
            "knowledge_representation": "object-centric token embeddings combining CLIP image-patch embeddings and 2D coordinate encodings, concatenated with word tokens and processed by self-attention to produce per-object relevance scores (softmax-normalized)",
            "performance_metric": "task success rate (proportion of episodes where pick and place within 0.1 m of target)",
            "performance_result": "When used end-to-end with downstream affordance model: SSR + ViLD: seen success rates by dataset size — 100 demos: 2%; 1000 demos: 30%; 10k demos: 60%; 100k demos: 65%. SSR + all-mask (oracle segmentation fused): seen — 100 demos: 8%; 1000 demos: 36%; 10k demos: 68%; 100k demos: 71%. Unseen-object rates are substantially lower (e.g., SSR+ViLD 1000d unseen 11%, SSR+all-mask 1000d unseen 14%).",
            "success_patterns": "Successfully identifies which objects are relevant for complex spatial-language goals (e.g., selecting the two reference objects that define a relative position) and provides a focused attention mask that significantly improves downstream pixel-level affordance prediction.",
            "failure_patterns": "Performance degrades with imperfect object detection/segmentation (ViLD), and generalization to unseen object colors is weak (unseen success rates often &lt;&lt; seen rates). SSR is not a substitute for precise pixel-level affordance learning — it provides relevance but not exact placement.",
            "baseline_comparison": "Compared to CLIPort without added relevant-object masks (end-to-end CLIPort), SSR-based pipelines outperform when limited data is available; CLIPort (no oracle) failed to learn effective policies in the small-data regimes reported, while CLIPort with oracle segmentation (upper bound) achieved e.g. 45%/20% (seen/unseen) at 100 demos and 90%/59% at 1000 demos.",
            "ablation_results": "Key ablation is inclusion vs removal of relevant-object segmentation: fusing ground-truth relevant-object masks into CLIPort (oracle) markedly improves performance relative to CLIPort without masks. Replacing ground-truth masks with ViLD segmentation (SSR + ViLD) reduces performance slightly versus SSR + all-mask, indicating sensitivity to detector/segmentation quality.",
            "key_findings": "Object-centric, multimodal tokenization (CLIP embeddings of language + object image patches + 2D coordinates) processed by a Transformer can represent object-relational and spatial relations sufficiently to select relevant objects for spatial language instructions; this high-level semantic-spatial signal, when provided as an attention mask, substantially improves downstream pixel-level affordance learning for manipulation, especially in data-limited regimes.",
            "uuid": "e340.0",
            "source_info": {
                "paper_title": "Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort (vision-language affordance predictor)",
            "brief_description": "A two-stream affordance prediction model combining a CLIP-based semantic pathway and a transporter-based spatial pathway to predict pixelwise pick and place affordance maps from language and images.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CLIPort",
            "model_size": null,
            "model_description": "Two-stream network: semantic pathway uses a pretrained CLIP encoder to process text and image for semantic features; spatial pathway uses a transporter-style architecture to predict pixelwise pick/place probability mass Q_pick,place ∈ R^{H×W}; in this work the spatial pathway optionally receives an attention map (selected-object mask) concatenated with RGB input.",
            "task_name": "Custom spatial pick-and-place in Ravens",
            "task_description": "Same as SSR entry: language-conditioned pick-and-place in cluttered scenes where instructions specify both object identity and relative spatial relation between two reference objects.",
            "task_type": "object manipulation / instruction following",
            "knowledge_type": "spatial + object-relational (encoded via CLIP semantic features and pixelwise spatial affordances)",
            "knowledge_source": "pretrained CLIP for semantic features; trained/fine-tuned on demonstration affordance datasets (supervised imitation learning)",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised training / fine-tuning of affordance predictor on demonstrations; optionally provided with fused segmentation masks as additional input (oracle or predicted)",
            "knowledge_representation": "combination of CLIP semantic embeddings and transporter-style spatial features producing pixelwise affordance maps; spatial attention map (selected-object segmentation) can be concatenated as channels to bias spatial pathway",
            "performance_metric": "task success rate (pick-and-place within 0.1 m)",
            "performance_result": "CLIPort without fused segmentation often failed in the small-data regimes evaluated. With oracle relevant-object segmentation (upper bound): 100 demos seen/unseen = 45% / 20%; 1000 demos seen/unseen = 90% / 59%. (CLIPort no-oracle reported near 0 in the small-data settings in the paper).",
            "success_patterns": "With fused object masks (oracle), CLIPort reliably learns to map semantic goals to pixelwise affordances and achieves high success (e.g., 90% at 1000 demos seen). It leverages CLIP semantic features to resolve object identity and uses transporter spatial features to predict precise pick/place pixels.",
            "failure_patterns": "End-to-end CLIPort without explicit relevant-object masks struggles to learn the required spatial relational grounding in limited-data regimes; even with oracle masks, generalization to unseen object colors is weaker than seen cases.",
            "baseline_comparison": "Baseline CLIPort (no oracle fusion) performed poorly in small-data settings (near 0% reported); CLIPort with oracle segmentation is the strong baseline/upper bound (e.g., 90% at 1000 demos seen).",
            "ablation_results": "Adding the selected-object attention map to the spatial pathway (the only architectural change) substantially improves performance compared to the original CLIPort in limited-data settings; lacking this guidance causes dramatic drop in success rate.",
            "key_findings": "Supplying high-level semantic-spatial priors (which objects matter) as an additional input to a pixelwise affordance model allows the model to decouple high-level relational reasoning from low-level manipulation skill learning, improving sample efficiency and effectiveness for spatial-language manipulation tasks.",
            "uuid": "e340.1",
            "source_info": {
                "paper_title": "Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A pretrained image-text model used here to encode both language tokens and object-centric image patches into joint embeddings that are input to the SSR Transformer and to CLIPort's semantic pathway.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_size": null,
            "model_description": "Pretrained multimodal (image-text) encoder trained contrastively on (image, caption) pairs to produce aligned embeddings for text and images; used here for encoding words and cropped object-patches, providing semantic grounding between language and visual content.",
            "task_name": "Used as perceptual/semantic encoder for the spatial pick-and-place pipeline",
            "task_description": "Provides multimodal embeddings for language and object-centric image patches that SSR and CLIPort use to reason about object identity and semantic relations in the scene.",
            "task_type": "perceptual grounding / semantic encoding for manipulation",
            "knowledge_type": "object-relational (semantic concepts and object identity), supports spatial grounding when combined with coordinates and positional tokens",
            "knowledge_source": "pretraining on large-scale image-text corpora (contrastive learning)",
            "has_direct_sensory_input": true,
            "elicitation_method": "pretrained embedding extraction (frozen or used as encoder) — embeddings are fed into downstream supervised training",
            "knowledge_representation": "distributed continuous embeddings that align visual patches and natural language tokens",
            "performance_metric": "not directly evaluated alone here (used as encoder); downstream success rates reported for pipelines that use CLIP embeddings",
            "performance_result": "Not reported standalone; enabled SSR and CLIPort to perform the multimodal grounding tasks when combined with supervised training (see SSR and CLIPort results).",
            "success_patterns": "Provides semantic alignment between text mentions of colors/object types and visual patches, enabling the model to associate language references with detected objects.",
            "failure_patterns": "CLIP alone does not provide fine-grained spatial relation encoding; must be combined with coordinate encodings and relational reasoning (Transformer SSR) to resolve relative positions.",
            "baseline_comparison": "N/A within this paper; CLIP used as component enabling other models to succeed.",
            "ablation_results": "Paper does not ablate CLIP vs alternatives; CLIP is the chosen semantic encoder.",
            "key_findings": "Using a pretrained vision-language model like CLIP to embed both text and object crops is an effective way to provide semantic grounding for downstream spatial-relational reasoning and affordance prediction.",
            "uuid": "e340.2",
            "source_info": {
                "paper_title": "Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ViLD",
            "name_full": "ViLD (Open-vocabulary object detection via vision and language knowledge distillation)",
            "brief_description": "An off-the-shelf open-vocabulary object detector used to produce object locations, crops, and segmentations for SSR input at deployment; substituting oracle masks with ViLD slightly reduces end-to-end performance.",
            "citation_title": "Openvocabulary Object Detection via Vision and Language Knowledge Distillation",
            "mention_or_use": "use",
            "model_name": "ViLD",
            "model_size": null,
            "model_description": "Open-vocabulary detector that distills vision-language knowledge to detect arbitrary categories; used to produce bounding boxes, crops, and segmentation masks for objects so SSR and CLIPort can operate without oracle segmentation in the real deployment scenario.",
            "task_name": "Object detection / segmentation feeding SSR + CLIPort pipeline",
            "task_description": "Provides object-centric crops and segmentation masks for all objects present in the orthographic image; these are used as inputs to SSR and as attention masks (after selection) to affordance predictor.",
            "task_type": "perceptual preprocessing / object detection",
            "knowledge_type": "object-relational (object identity) and provides spatial coordinates via bounding boxes/centers",
            "knowledge_source": "pretrained detection model (vision-language distillation), used at test/deployment",
            "has_direct_sensory_input": true,
            "elicitation_method": "running inference with the pretrained detector to obtain crops and masks",
            "knowledge_representation": "bounding boxes, segmentation masks, center coordinates and cropped image patches",
            "performance_metric": "downstream task success rates when using ViLD vs oracle segmentation",
            "performance_result": "SSR + ViLD (end-to-end, no oracle) achieved lower success rates than SSR + all-mask (oracle). Example seen rates: with 100k demos SSR+ViLD seen 65% vs SSR+all-mask 71%; at 10k demos SSR+ViLD seen 60% vs SSR+all-mask 68%.",
            "success_patterns": "Enables deployment without ground-truth segmentation; pipeline still benefits from relevance selection even when using predicted segmentation.",
            "failure_patterns": "Imperfect detections/segmentations from ViLD reduce the quality of the attention map and thus downstream affordance accuracy; causes a measurable drop relative to oracle segmentation.",
            "baseline_comparison": "SSR + all-mask (oracle segmentation) acts as upper bound and outperforms SSR + ViLD by several percentage points across dataset sizes.",
            "ablation_results": "Replacing ground-truth segmentation with ViLD segmentation reduces success rates (see performance_result numbers), indicating detector quality is an important component.",
            "key_findings": "Using an open-vocabulary detector allows the pipeline to operate without oracle segmentation, but detector imperfections materially affect grounding of spatial relations and final manipulation success.",
            "uuid": "e340.3",
            "source_info": {
                "paper_title": "Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "LLM planning mentions",
            "name_full": "Large Language Models (planning capability mentions)",
            "brief_description": "Related work mentions that large language models have planning abilities that have been exploited for embodied reasoning tasks, but this paper does not evaluate or implement LLM-based planning itself.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large language models (general reference)",
            "model_size": null,
            "model_description": "Referenced in related work as models that have demonstrated planning/chain-of-thought abilities for embodied tasks (citations: Ahn et al. 2022; Zeng et al. 2022; Huang et al. 2022), but not used in experiments here.",
            "task_name": "Referenced: embodied reasoning / planning for robots (external works)",
            "task_description": "Papers cited describe leveraging LLMs to generate plans or intermediate reasoning steps for embodied agents, but this paper focuses on a vision+object-centric Transformer for selecting relevant objects instead.",
            "task_type": "mention: multi-step planning / embodied reasoning",
            "knowledge_type": "procedural + spatial (as per cited literature), but no direct utilization here",
            "knowledge_source": "pretraining on large text corpora (in cited works), not applicable to experiments in this paper",
            "has_direct_sensory_input": null,
            "elicitation_method": "mention of planning ability / chain-of-thought style planning in related work; no elicitation or evaluation performed in this paper",
            "knowledge_representation": "mentioned as natural-language / plan sequences in other works; not instantiated here",
            "performance_metric": "",
            "performance_result": "",
            "success_patterns": "",
            "failure_patterns": "Not evaluated in this paper; only cited as related approaches for embodied planning.",
            "baseline_comparison": "",
            "ablation_results": "",
            "key_findings": "Paper positions its approach as complementary to LLM-based planning: instead of relying on pure language-model planning without perception, they emphasize object-centric visual grounding and an explicit object-relevance signal to aid pixel-level affordance learning.",
            "uuid": "e340.4",
            "source_info": {
                "paper_title": "Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CLIPort: What and where pathways for robotic manipulation",
            "rating": 2,
            "sanitized_title": "cliport_what_and_where_pathways_for_robotic_manipulation"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Openvocabulary Object Detection via Vision and Language Knowledge Distillation",
            "rating": 2,
            "sanitized_title": "openvocabulary_object_detection_via_vision_and_language_knowledge_distillation"
        },
        {
            "paper_title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Transporter networks: Rearranging the visual world for robotic manipulation",
            "rating": 2,
            "sanitized_title": "transporter_networks_rearranging_the_visual_world_for_robotic_manipulation"
        },
        {
            "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
            "rating": 1,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        }
    ],
    "cost": 0.013670749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning
31 Mar 2023</p>
<p>Qian Luo 
Indicates equal contribution</p>
<p>Shanghai Qi Zhi Institute</p>
<p>Yunfei Li 
Indicates equal contribution</p>
<p>Institute for Interdisciplinary Information Sciences
Tsinghua University</p>
<p>Yi Wu 
Shanghai Qi Zhi Institute</p>
<p>Institute for Interdisciplinary Information Sciences
Tsinghua University</p>
<p>Grounding Object Relations in Language-Conditioned Robotic Manipulation with Semantic-Spatial Reasoning
31 Mar 2023EFB673491F443EFDE9022682EBF6D5F0arXiv:2303.17919v1[cs.RO]
Grounded understanding of natural language in physical scenes can greatly benefit robots that follow human instructions.In object manipulation scenarios, existing end-to-end models are proficient at understanding semantic concepts, but typically cannot handle complex instructions involving spatial relations among multiple objects.which require both reasoning object-level spatial relations and learning precise pixel-level manipulation affordances.We take an initial step to this challenge with a decoupled two-stage solution.In the first stage, we propose an object-centric semantic-spatial reasoner to select which objects are relevant for the language instructed task.The segmentation of selected objects are then fused as additional input to the affordance learning stage.Simply incorporating the inductive bias of relevant objects to a vision-language affordance learning agent can effectively boost its performance in a custom testbed designed for object manipulation with spatial-related language instructions.</p>
<p>Introduction</p>
<p>Understanding complex language instructions is a longstanding research problem for building intelligent robots that can assist humans to perform meaningful tasks in grounded scenes (Chen and Mooney 2011;Bollini et al. 2012;Misra et al. 2016).Recently, there has been exciting progress in grounded vision and language in robotic manipulation scenarios with reinforcement learning and imitation learning (Nair et al. 2021;Jang et al. 2021).Leveraging the power of pretrained vision and language models, some most advanced end-to-end models can effectively ground semantic concepts from natural language to physical scenes and even demonstrate some generalization ability to unseen objects (Shridhar, Manuelli, and Fox 2022a).</p>
<p>However, existing end-to-end agents in vision-language manipulation typically still lack the ability to deal with instructions containing spatial relations among multiple objects.Consider a motivating example in Fig. 1 where multiple blocks and bowls with identical or different colors are put on a table.Existing approaches can already follow instructions such as "put the cyan block into a green bowl" but cannot handle a more complex instruction like "place the cyan block in the middle of the front yellow block and the back gray bowl".Solving the previous task only requires understanding simple semantic concepts while the latter one additionally requires the agent to reason over more finegrained spatial relations.</p>
<p>We hypothesize that current end-to-end models struggle at such complex instructions since it is challenging to simultaneously learn both abstract spatial reasoning and precise manipulation skills in a fully coupled manner.In this paper, we take an initial step towards this challenging problem by decoupling object-level spatial reasoning and pixel-level affordance learning in two stages.We propose an object-centric semantic-spatial reasoning module that predicts which objects are relevant for accomplishing a manipulation task from the language instruction and image observation.Then we fuse the segmentation of all the relevant objects to the input of a strong performing vision-language affordance learning agent CLIPort (Shridhar, Manuelli, and Fox 2022a).The relevant object prediction serves as an effective inductive bias to inform the agent about which regions are beneficial to look at for low-level affordance learning.</p>
<p>To better evaluate whether our proposed framework can effectively handle spatial relations between objects, we design a custom pick-and-place task in cluttered scenes built upon Ravens (Zeng et al. 2021).The language instructions are designed to involve relative spatial positions of objects.Preliminary results show that our method achieves nontrivial success in this task while CLIPort simply cannot work given the same amount of training data.</p>
<p>Related Work</p>
<p>Language-conditioned robotic manipulation: Instructing robots with natural language has attracted much research interest in recent years (Mees et al. 2022).Some earlier works proposed end-to-end learning methods for visionlanguage-conditioned control by imitating large-scale behavior datasets (Lynch and Sermanet 2021;Jang et al. 2021;Mees, Hermann, and Burgard 2022) or learning reward functions for reinforcement learning (Shao et al. 2020;Nair et al. 2021).With advances in multi-modal representation learning (Radford et al. 2021), there are also many works leveraging pretrained vision or language encoders for grounded manipulation tasks (Shridhar, Manuelli, and Fox 2022a,b), and exciting progress in utilizing the planning ability of large language models for embodied reasoning tasks (Ahn et al. Figure 1: Overview of our two-stage framework for grounding spatial-related instructions in object manipulation tasks.2022; Zeng et al. 2022;Huang et al. 2022).We are particularly interested in grounding language instructions with complex spatial relations between objects, which cannot be addressed well by previous methods.</p>
<p>Semantic-spatial reasoning for manipulation: Learning spatial relations between different objects have been studied in vision and robotics (Johnson et al. 2017;Mees et al. 2017), which can better address language goals specifying the desired spatial relations (Paul et al. 2016;Venkatesh et al. 2021;Yuan et al. 2021;Liu et al. 2022), and can be benefit hierarchical planning to accomplish complex goal configurations (Zhu et al. 2021).A notable line of works study neural-symbolic reasoning to model spatial relations and semantic concepts in a scene (Mao et al. 2019).More recently, fully end-to-end learning mechanism has also achieved much progress in modeling complex object-level relations for visual reasoning with correct inductive bias (Ding et al. 2021).In this work, we also propose a purely data-driven model for semantic-spatial reasoning which is easy to implement and can improve with the increasing scale of the training dataset.</p>
<p>Method</p>
<p>We propose a two-stage solution for grounding instructions with spatial relations between objects.In the first stage, a semantic-spatial reasoning module aims to model the objectlevel relations from language instructions and object-centric image observations (see Sec. 3.1).In the second stage, the segmentation of the selected objects is fed as an additional input to a pixel-level affordance learning model CLIPort, and functions as an inductive bias that guides the agent to attend to particular objects when predicting precise pick and place affordance maps (see Sec. 3.2).Combining with an off-the-shelf object detection and segmentation model, the two stages are cascaded together in deployment (Sec.3.3).</p>
<p>Object-centric semantic-spatial reasoning</p>
<p>The semantic-spatial reasoning (SSR) model aims to select relevant objects in a scene for the pick-and-place task given a complex language instruction containing semantic and spatial relations of objects.We leverage a powerful Transformer (Vaswani et al. 2017) architecture to allow grounded understanding of relations from natural language instructions and orthographic RGB images.Specifically, the module takes in an instruction (w 1 , w 2 , • • • , w L ), and m image patches with shape 50 × 50 × 3 center-cropped from the detected positions of all the objects along with their (u, v) coordinates in the orthographic image.L is the length of the language instruction and m is the total number of objects in the image.The words and image patches are encoded with the pretrained CLIP model (Radford et al. 2021).The coordinates of objects are encoded with linear layers, then concatenated with the corresponding image patch embeddings into m object-centric features.The L word features and m object-centric features are then concatenated with their position embeddings.To distinguish text and object embeddings, we additionally concatenate type embeddings (0 for texts and 1 for objects) to each token.The L + m tokens are then passed into 8 self-attention layers.The fused features at m object-centric tokens are used to predict scores ŝi of how related each object is to the language instruction with an MLP.The ground truth for this reasoning module is represented as a binary vector [s 1 , s 2 , • • • , s m ], where s j = 1 if the j th object is related for performing placement and s j = 0 if the object is irrelevant.Finally, we normalize the scores and the ground truth with softmax and optimize the module to minimize the L 1 distance between them.</p>
<p>Implementations: Our semantic-spatial reasoning module requires cropped object-centric patches as input, which can be predicted from off-the-shelf object detection models.In practice, we adopt an open-vocabulary object detection model ViLD (Gu et al. 2021) that is suitable for detecting arbitrary categories of objects to predict the locations of all objects.A cheaper way to obtain the image patches when training in simulation is to directly project the center of each object onto the orthographic image, then crop patches with shape 50 × 50 × 3 centered at the projected coordinates.</p>
<p>Affordance prediction with selected objects</p>
<p>In this part, we describe how we leverage object-level relevance to help low-level manipulation learning.</p>
<p>We adopt a similar two-stream network architecture as CLIPort for predicting pick and place affordance, where a semantic pathway built upon a pretrained CLIP encoder and a spatial pathway based on a transporter network are fused together to predict the pixelwise probability mass Q pick,place ∈ R H×W .For more details, we encourage the readers to refer to the CLIPort paper (Shridhar, Manuelli, and Fox 2022a).</p>
<p>The only difference in our architecture is that our spatial pathway additionally takes as input an attention map indicating which objects may be relevant to the manipulation task.In this way, the affordance prediction model is informed of the high-level semantic-spatial reasoning result and can focus more on the learning of low-level affordance.</p>
<p>The attention map is calculated as follows.After predicting the scores of how relative different objects are to the manipulation task, we select the objects with normalized scores greater than a threshold to guide the affordance prediction stage.We combine the binary segmentation mask of all the selected objects (by computing their logical "OR") and multiply the mask by the original RGB image to get an attention map with the same shape (H, W, 3) as the original image.The attention map is stacked in channel dimension with the original RGB image as the input to the spatial pathway.</p>
<p>The ground truth of the affordance prediction model is the projected pick and place poses to the 2D image from demonstrations.The training objective is the cross-entropy between normalized Q pick,place and the ground truth.</p>
<p>Training and deployment</p>
<p>The semantic-spatial reasoning module and the affordance prediction module are trained separately with supervised learning.We experiment with using oracle object detection and also ViLD detection as input to train the semantic-spatial reasoning module.For affordance prediction module, we always use ground-truth relevant object segmentation as its input during training.In practice, we find our semantic-spatial reasoning module is lightweight for training and can benefit from training over large-scale datasets while training the affordance prediction module is time-consuming and we can only afford training on 1k demos with our best efforts.</p>
<p>During deployment, we first compute the coordinates, cropped patches and segmentation masks of all the objects by querying a ViLD model, then feed the object-centric information along with the language instruction into the semantic-spatial reasoning module to get normalized relevance scores for all the objects.The objects with scores greater than the average are selected and fused into the affordance prediction model to get Q pick and Q place .The best pick/place location is then computed as arg max
(u,v) Q(u, v).
Using the parameters of the camera from factory calibration and hand-eye calibration, the 2D locations on the orthographic image can be easily converted to 3D positions relative to the robot base frame.The robot can run any suitable motion planner to reach the desired pick/place poses.</p>
<p>Experiments</p>
<p>In this section, we discuss the simulation experiments for validating the proposed method.We design the experiments to answer the following questions.</p>
<ol>
<li>Can our method effectively solve a typical spatial-related pick and place task in Ravens environment?2. How well does our method perform compared with the baseline method and those validated via ground truth?</li>
</ol>
<p>Experimental setup</p>
<p>Task setup We build our task upon Ravens, a collection of simulated tasks in PyBullet (Coumans andBai 2016-2022) for learning vision-based robotic manipulation.We study a custom language-conditioned pick-and-place task that requires semantic-spatial reasoning.At the beginning of each episode, 6 bowls and 6 blocks are generated at random positions on the table, and objects within each category are of identical shape.Three blocks are of color A, three bowls are of color B and the other six objects are of random colors different from A and B. To test the capability of spatial understanding of our model, the model is supposed to identify the spatial relations among those identical blocks and bowls.The language instructions are generated from the template "pick the  The training procedures include the semantic-spatial reasoning stage and the affordance prediction stage.In the semantic-spatial reasoning stage, we feed the language instructions, oracle object-centric patches and position information into the transformer, and train the model to fit the normalized binary label.In the affordance prediction stage, we train the downstream CLIPort with segmentation fused from the ground truth of relevant objects.</p>
<p>Main results</p>
<p>We train each agent with four datasets with different number of demonstrations (100, 1000, 10k, 100k) and choose the best checkpoints on the validation dataset (100 episodes) for evaluation.Each agent is evaluated with the average success rate on the test dataset consisting of 100 cases.An episode is regarded as successful if the correct object is picked and placed within 0.1m distance error to the target position.To test the generalization ability, we evaluate the agents in two settings where the object colors are seen or unseen during training.We compare the performance of four agents:</p>
<p>• CLIPort: The agent is trained and evaluated with CLI-Port, an end-to-end model that directly predicts pick-andplace affordances from texts and RGBD images.• CLIPort with oracle relevant objects: The agent is trained and evaluated by feeding CLIPort with groundtruth relevant objects.The purpose of this experiment is to verify whether fusing the segmented RGB image of relevant objects to CLIPort can enhance its ability to ground spatial-related language instructions.It approximates the upper bound of the performance if we have a perfect semantic-spatial reasoning module.• SSR + all mask: The agent is trained on SSR module with ground-truth segmentation of all objects.The affordance prediction network is trained separately following the same procedure in CLIPort with oracle relevant ob-jects.The agent is evaluated using ground-truth segmentation of all objects, and the SSR output is fed into the affordance prediction network.The purpose of this experiment is to test the ability of the semantic-spatial reasoning module.• SSR + ViLD: The agent is trained following the same procedure in SSR + all mask.The only difference is that we use ViLD segmentation results instead of groundtruth segmentation of all objects.The agent is evaluated in an end-to-end manner without requiring any groundtruth information.The experiment results show how well our model performs without any oracle inputs.</p>
<p>The quantitative experiment results are summarised in Table 1.Since CLIPort already takes 2 days to train with 1000 demos, we cannot afford to train it with 10k and 100k demos.</p>
<p>The CLIPort oracle agent is also trained on 100/1000 demos, which are used for the affordance prediction of SSR + all mask and SSR + ViLD agents.We also visualize predicted affordances in Fig. 2. Our methods (SSR + ViLD and SSR + all mask) outperform CLIPort in 100 and 1000 demos datasets.With larger datasets, all the agents gain better performance.In all cases, SSR + all mask performs slightly better than SSR + ViLD agent.The performance gap can be explained by imperfect predictions of ViLD.Our best agent is the SSR + all mask agent trained with 100k demos, which reaches a success rate of over 0.7.Since CLIPort oracle succeeds in 90 percent of the cases, there is still room to improve for SSR.Our agents are also able to generalize to unseen cases, but the performances are at most 50 percent as good as those in seen cases.We leave a more generalizable agent for future work.</p>
<p>Conclusion</p>
<p>In this work, we study the problem of grounded understanding of complex language instructions involving spatial relations between multiple objects in language-conditioned robotic manipulation.We take a preliminary step by enhancing the CLIPort with relevant object masks predicted from an object-centric semantic-spatial reasoning module.Experiment results show that informing the agent of which objects should be focused on is a simple yet effective way to address language-conditioned tasks that require understanding semantic-spatial relations.We plan to extend this work to more general cases with a greater variety of objects and manipulation tasks.Another direction is to work beyond pairwise relations (e.g., left/right) and to tackle multi-object relations (e.g., middle, pyramid).</p>
<p>Figure 2 :
2
Figure 2: Predicted affordance maps for placement with different methods.Our method can accurately focus on the desired position while CLIPort completely fails.</p>
<p>[color X] block in the middle of [location a] [color A] block and [location b] [color B] bowl", in which the locations are sampled from left/right/front/back.</p>
<p>Table 1 :
1
Training details Following the description in the task setup, we generate training, validation and test datasets Pick-and-place success rate of different agents evaluated with seen and unseen objects.
100 demos1000 demos10k demos100k demosSeen Unseen Seen Unseen Seen Unseen Seen UnseenCliport0000N/A N/AN/A N/ACliport oracle0.45 0.200.90 0.59N/A N/AN/A N/ASSR + all mask 0.08 0.020.36 0.140.68 0.340.71 0.35SSR + ViLD0.02 0.010.30 0.110.60 0.250.65 0.29in Ravens environment. Specifically, for each sample, theobjects are added in accordance with the language in-struction described in Sec. 4.1, where location a andlocation b are randomly chosen from left/right/fron-t/back and color X, color A, color B are randomlyselected from a collection of 7 colors. The orthographicRGB image, the cropped oracle object-centric patches to-gether with its projected ground-truth coordinates are addedinto training instances. The labels include the binary vectorsrepresenting the relevance of each object to the given lan-guage instruction, and the final pick and place coordinatesfor grounding.</p>
<p>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N J Joshi, R Julian, D Kalashnikov, Y Kuang, K Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, M Yan, ISER 2012Experimental Robotics -The 13th International Symposium on Experimental Robotics. J P Desai, G Dudek, O Khatib, V Kumar, Québec City, CanadaSpringer2022. 2012. June 18-21, 201288Interpreting and Executing Recipes with a Cooking Robot</p>
<p>Learning to Interpret Natural Language Navigation Instructions from Observations. D L Chen, R J Mooney, Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011. W Burgard, D Roth, the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011San Francisco, California, USAAAAI Press2011. August 7-11, 2011</p>
<p>PyBullet, a Python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016-2022</p>
<p>Attention over Learned Object Embeddings Enables Complex Visual Reasoning. D Ding, F Hill, A Santoro, M Reynolds, M M Botvinick, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y N Dauphin, P Liang, J W Vaughan, NeurIPS2021. 2021. 2021. December 6-14, 2021</p>
<p>Openvocabulary Object Detection via Vision and Language Knowledge Distillation. X Gu, T.-Y Lin, W Kuo, Y Cui, International Conference on Learning Representations. 2021</p>
<p>Inner Monologue: Embodied Reasoning through Planning with Language Models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, N Brown, T Jackson, L Luu, S Levine, K Hausman, B Ichter, CoRR, abs/2207.056082022</p>
<p>BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, Conference on Robot Learning. A Faust, D Hsu, G Neumann, London, UKPMLR2021. 8-11 November 2021164</p>
<p>Struct-Former: Learning Spatial Structure for Language-Guided Semantic Rearrangement of Novel Objects. J Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei, C L Zitnick, R B Girshick, W Liu, C Paxton, T Hermans, D Fox, P Sermanet, ICRA 20222017 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Soci. D A Shell, M Toussaint, M A Hsieh, Honolulu, HI, USA; Philadelphia, PA, USA2017. 2017. July 21-26, 2017, 1988-1997. 2022. May 23-27, 2022. 2021. July 12-16, 2021Robotics: Science and Systems XVII, Virtual Event</p>
<p>What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data. J Mao, C Gan, P Kohli, J B Tenenbaum, J Wu, N Abdo, M Mazuran, W Burgard, L Hermann, W Burgard, ICLR 2019The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. New Orleans, LA, USA; Vancouver, BC, Canada2019. May 6-9, 2019. 2017. September 24-28, 2017. 20227Metric learning for generalizing spatial relations to new objects</p>
<p>CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, IEEE Robotics Autom. Lett. 732022</p>
<p>Tell me Dave: Context-sensitive grounding of natural language to manipulation instructions. D K Misra, J Sung, K Lee, A Saxena, Int. J. Robotics Res. 351-32016</p>
<p>Efficient Grounding of Abstract Spatial Concepts for Natural Language Interaction with Robot Manipulators. S Nair, E Mitchell, K Chen, B Ichter, S Savarese, C Finn, Pmlr, R Paul, J Arkin, N Roy, T M Howard, Conference on Robot Learning. D Hsu, N M Amato, S Berman, S A Jacobs, London, UK; Ann Arbor, Michigan, USA2021. 8-11 November 2021. 2016. June 18 -June 22, 2016164Robotics: Science and Systems XII, University of MichiganLearning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. PMLR2021</p>
<p>Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations. L Shao, T Migimatsu, Q Zhang, K Yang, J Bohg, Robotics: Science and Systems XVI, Virtual Event / Corvalis. M Toussaint, A Bicchi, T Hermans, Oregon, USA2020. July 12-16, 2020</p>
<p>Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation. M Shridhar, L Manuelli, D Fox, Pmlr, M Shridhar, L Manuelli, D Fox, CoRR, abs/2209.05451Conference on Robot Learning. 2022a. 2022bCliport: What and where pathways for robotic manipulation</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730</p>
<p>Spatial Reasoning from Natural Language Instructions for Robot Manipulation. S G Venkatesh, A Biswas, R Upadrashta, V Srinivasan, P Talukdar, B Amrutur, IEEE International Conference on Robotics and Automation, ICRA 2021. Xi'an, ChinaIEEE2021. May 30 -June 5, 2021</p>
<p>SOR-Net: Spatial Object-Centric Representations for Sequential Manipulation. W Yuan, C Paxton, K Desingh, D Fox, Conference on Robot Learning. A Faust, D Hsu, G Neumann, London, UKPMLR2021. 8-11 November 2021164</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, Conference on Robot Learning. PMLR2021</p>
<p>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. A Zeng, A Wong, S Welker, K Choromanski, F Tombari, A Purohit, M S Ryoo, V Sindhwani, J Lee, V Vanhoucke, P Florence, CoRR, abs/2204.005982022</p>
<p>Hierarchical Planning for Long-Horizon Manipulation with Geometric and Symbolic Scene Graphs. Y Zhu, J Tremblay, S Birchfield, Y Zhu, IEEE International Conference on Robotics and Automation, ICRA 2021. Xi'an, ChinaIEEE2021. May 30 -June 5, 2021</p>            </div>
        </div>

    </div>
</body>
</html>
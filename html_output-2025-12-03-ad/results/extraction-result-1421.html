<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1421 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1421</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1421</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-600bfe5f0597ebd84898f0c4270ddfb3750594f5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/600bfe5f0597ebd84898f0c4270ddfb3750594f5" target="_blank">Imagination-Augmented Agents for Deep Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects, shows improved data efficiency, performance, and robustness to model misspecification compared to several baselines.</p>
                <p><strong>Paper Abstract:</strong> We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1421.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1421.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>I2A env model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imagination-Augmented Agent environment model (action-conditional next-step predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The learned environment/world model used by I2As: an action-conditional, auto-regressive next-step predictor that outputs a pixel-wise distribution over the next observation and a distribution for reward; used to generate short rollouts (imagined trajectories) that are encoded and consumed by the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>I2A environment model (action-conditional next-step predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive, action-conditional neural predictor that takes the current observation (pixels) and the current action (broadcasted as one-hot), and predicts the next frame as a pixel-wise probability distribution plus a reward distribution. It is rolled forward iteratively to produce multi-step imagined trajectories whose features (predicted observation + predicted reward) are encoded by a rollout encoder LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator / action-conditional next-step predictor (explicit pixel predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Sokoban (sprite-based grid game) and MiniPacman (small pixel maze domain); general RL domains with raw pixel observations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Negative log-likelihood / Bernoulli cross-entropy on next-frame prediction and log-likelihood loss for reward prediction (l_model); qualitative inspection of rollout visual fidelity and accumulation of artifacts over timesteps</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No single numeric prediction metric reported; qualitatively: 'accurate' models yield realistic multi-step rollouts; 'noisy' smaller models produce clear artifacts (missing/duplicate sprites) when rolled for 5+ steps. Pretraining dataset budget reported as <= 1e8 frames for models used in Sokoban experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model is a black-box neural pixel predictor (CNN / residual CNN), not inherently interpretable; imagined frames are directly visualizable which aids interpretability of specific rollouts, but latent representations are not described as human-interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of imagined rollouts; downstream learned rollout-encoder (LSTM) that 'learns to interpret' rollouts rather than relying on fixed summaries; no explicit latent disentangling or symbolic extraction reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: pretraining required on up to ~1e8 frames (conservative estimate). Inference: environment model calls used inside rollouts; I2A overall was reported as less than an order of magnitude slower per environment interaction than model-free baselines. Model-call counts: ~1.4k model simulation steps per solved Sokoban level for I2A (87% performance) when using short rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>I2A required far fewer model calls than MCTS: to reach comparable performance (87% solved) I2A used ~1.4k model calls per level vs MCTS ~25k; MCTS required ~100k model calls to reach 95% while an I2A with MC outer loop required ~4k model calls to reach 95%. Pretraining cost (data) is additional but can be amortized across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When embedded in I2A, the model supported agents that solved ~85% of Sokoban levels (rollouts length 5); longer rollouts (15) reached >90%+; with near-perfect model plus MC search I2A reached ~95%. In MiniPacman, using a shared model across tasks improved performance and data efficiency; I2A outperformed model-free baselines on several tasks, especially with sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>World model utility is task-dependent: short, approximate rollouts provided task-relevant signals that improve planning/policy learning and robustness; even imperfect models provide useful early-step predictions (I2A learns to use or ignore portions of rollouts). High fidelity can improve performance further (MC within I2A), but I2A is explicitly designed to extract utility from imperfect predictions, so full fidelity is not required to gain benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs include: (a) data/computation to pretrain the model (pretraining frames) vs reduced environment interactions and improved sample efficiency for policy learning; (b) longer/deeper rollouts increase task performance but with diminishing returns and higher compute; (c) perfect/high-fidelity models enable stronger planning baselines (e.g., MCTS or MC search) but require many more simulator calls; (d) interpretable or symbolic models not used here — neural pixel predictors remain black-box but are practical.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Action-conditional next-step prediction at pixel level; predict both observation and reward; auto-regressive rollouts seeded from real observations; rollout policy distilled from the I2A policy to produce task-relevant imagined trajectories; rollout encoder (CNN + LSTM) processes reversed-order features and learns to summarize rollouts; environment model can be pretrained separately (faster runtime) or trained jointly with auxiliary loss.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to model-free baselines: I2A (with the learned model) outperforms model-free agents in Sokoban and MiniPacman and is more data-efficient. Compared to classical planning (MCTS with near-perfect model): I2A achieves comparable performance with far fewer model calls by learning to focus rollouts. Compared to simplistic/no-model baselines (copy-model): explicit predictive model rollouts improve performance significantly. Compared to encoder-free Monte-Carlo rollout value estimation: the learned rollout encoder provides robustness to model noise while encoder-free MC degrades catastrophically with a noisy model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations: use short multi-step rollouts (they found 3-15 steps useful; 5 is a pragmatic choice balancing compute and performance), predict both frames and rewards when possible, pretrain the environment model to accelerate training, and distill the I2A policy into the rollout policy to guide rollouts toward task-relevant areas. Emphasis on learning to interpret imperfect models rather than relying on perfect fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1421.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sokoban model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sokoban environment model (residual CNN + reward MLP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific instance of the I2A environment model used in Sokoban: a residual convolutional neural network that predicts the next 80x80 RGB frame and a reward distribution via a separate CNN/MLP pathway.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sokoban residual CNN environment model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Input: current 80x80x3 frame plus a broadcasted one-hot action. Architecture: initial large 8x8 convolution with stride 8, followed by size-preserving convolutional layers and residual CNN blocks; separate CNN/fully-connected MLP pathway for reward prediction. Outputs: pixel-wise probability distribution for the next image and a reward distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator / residual CNN pixel predictor</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Sokoban (visual sprite grid puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trained using Bernoulli cross-entropy (negative log-likelihood) between predicted pixels and true next-frame; reward predicted with distributional loss as part of model loss l_model.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitative: one high-capacity Sokoban model produced accurate short rollouts; an intentionally smaller/noisy model produced clear artifacts over 5-step rollouts (missing/duplicate sprites). No scalar per-frame error metrics reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box CNN model; rollouts are pixel images so errors/artifacts are interpretable visually; internal features not claimed to be interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of rollouts; use of rollout encoder to learn task-relevant interpretation of predicted frames rather than explicit interpretability methods.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model architecture is lightweight relative to large image models but specific parameter counts not reported; pretraining budget reported as part of overall model pretraining (<=1e8 frames). Inference used in many short rollouts per action (one rollout per possible action in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>No direct comparison of Sokoban model compute vs alternatives beyond noting the smaller (noisy) model has lower parameter count but worse visual fidelity; I2A with this model still robust while encoder-free MC fails with noisy model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used in I2A, supported 85%+ solved levels (rollout length 5) and up to >90% with longer rollouts or 95% with outer MC retries when the model is near-perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Provides short-term, actionable predictions (initial few steps) that are highly useful for avoiding irreversible mistakes in Sokoban; later rollout steps may degrade but the learned encoder learns to weigh earlier predictions more.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Smaller models (less compute) produce noisy rollouts that accumulate artifacts over steps; however I2A can learn to ignore erroneous later predictions — robustness reduces need for perfect fidelity. Longer rollouts improve decision quality but increase compute/time.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Residual CNN backbone; stride-8 initial conv to reduce spatial size; separate reward prediction head; pixel-level Bernoulli outputs; rollouts seeded with real observation and fed back autoregressively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to an identity 'copy' model (which returns the input frame), the Sokoban predictive model yields much better performance (copy-model performs far worse). Compared to smaller/noisy versions, larger accurate models give visually better rollouts and can enable additional MC techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests predicting both observations and rewards helps (reward prediction beneficial but not strictly necessary), and that a reasonably sized residual CNN giving accurate short-horizon rollouts is effective; coupling model with a learned rollout encoder and distilled rollout policy is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1421.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniPacman model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniPacman environment model (multi-scale CNN with pool-and-inject + reward layers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auto-regressive, multi-scale convolutional environment model for MiniPacman that includes a 'pool-and-inject' mechanism to capture long-range dependencies and a separate pathway for event/reward prediction; trained to predict next-frame pixels and discrete events.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniPacman multi-scale CNN environment model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Input: 15x19x3 frame and broadcasted one-hot action. Architecture: size-preserving multi-scale CNN blocks, a 'pool-and-inject' layer (global max-pooling across feature maps which is broadcast back), and additional fully-connected layers for reward/event prediction. Output: pixel-wise next-frame distribution and predicted environment events used to compute task rewards given task-specific reward vector.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator / multi-scale CNN pixel predictor</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MiniPacman multi-task domain (small pixel-based maze with ghosts and events)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Bernoulli cross-entropy on pixel predictions; log-likelihood on event predictions. Model used to compute predicted rewards via event prediction and task reward vector.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No numeric metrics reported; model was sufficiently accurate to be reused across five different MiniPacman tasks and to improve I2A agent performance compared to baselines, particularly for sparse-reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model is a black-box CNN with some architectural mechanisms (pool-and-inject) to capture global interactions; predicted frames and event outputs are directly interpretable as images and events, respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of predicted frames and event outputs; using predicted events to compute task-specific rewards (explicit mapping makes event outputs interpretable for reward computation).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model is described as 'light-weight' for rapid experimentation; exact parameter counts and timing not given. Shared single model used across multiple tasks to amortize pretraining cost.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Using one shared model across tasks yields low marginal cost per task; model-based I2A agents outperform model-free baselines in data efficiency and final performance in MiniPacman tasks, especially with sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>I2A with this shared MiniPacman model outperformed standard model-free baselines across 5 tasks after 300M environment steps; largest gains on sparse-reward tasks (tasks 4 & 5).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Event prediction + task reward vector design allowed effective reuse of a single trained model across diverse reward definitions; predicted event signals were directly useful for policy learning in different tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Investing in a slightly more complex model (pool-and-inject) yields ability to capture nonlocal interactions (ghost dynamics) critical for task performance; model complexity vs runtime tradeoff not quantified but described as practical for rapid experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Multi-scale CNN to preserve spatial size; pool-and-inject for global communication; separate MLP for reward/event prediction; shared model across tasks with task-specific reward vector w_rew to interpret events as rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to training separate models per task, a single shared model reduced marginal cost and improved data efficiency. Compared to model-free baselines, I2A+model improved performance especially on sparse/reward-difficult tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests training a general model that predicts low-level observations and events, and interpreting task objectives as reward vectors applied to events — this enables reuse across tasks and better sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1421.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy model (identity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copy-model identity world model baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A control baseline that replaces the predictive environment model with a 'copy' model that simply returns the input observation (no dynamics prediction), keeping the rest of I2A architecture identical to test whether rollouts themselves (not model predictions) drive gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Copy-model (identity)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identity function: given the current observation, the 'model' outputs the same observation (no forward prediction). Used inside the I2A architecture to keep compute and parameter counts comparable while removing predictive content.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>non-predictive identity baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Sokoban and MiniPacman experiments as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not applicable (no predictive objective); fidelity is zero for forward prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Transparent: returns input observation unchanged; not a forward model and therefore provides no imagined future.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Same order of compute as I2A with a learned model (environment model kept constant in I2A), used to control for compute and parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Despite matching compute and parameters, this baseline performs far worse than I2A with a learned model, indicating predictive rollouts provide the performance improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Copy-model I2A performed significantly worse than I2A with a learned predictive model on Sokoban and MiniPacman.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrates that the utility arises from predictive content of a world model rather than architectural capacity or compute alone.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Shows that simply increasing compute or network depth without predictive modeling does not yield the same planning benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Identity model to control for architecture and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs worse than I2A with learned predictive model; used to isolate effect of model-based imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not applicable; serves as negative control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1421.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC rollout (encoder-free)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte-Carlo rollout value estimator (encoder-free I2A variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline architecture that estimates action values by explicitly computing Monte-Carlo returns from rollouts produced by an environment model, without a learned rollout encoder; action selection is proportional to exponentiated estimated returns with a learned temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Monte-Carlo rollout value estimator (encoder-free)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>For each candidate initial action, sample a rollout using a rollout policy, compute the discounted sum of rewards plus a learned terminal value V(x_T), and select actions proportional to exp(-estimated_return/δ) where δ is learned temperature. Only V, rollout policy ̂π and δ are learned; no learned summarizer of trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>model-based Monte Carlo evaluator (no learned rollout encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Sokoban experiments (comparison to I2A)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Performance measured by final task performance (levels solved); rollouts use the same environment models as other agents so underlying model fidelity is same metric as model likelihood. No separate prediction error reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>When using an accurate environment model, performance similar to baseline standard agent; when using a noisy/poor model, performance degrades catastrophically (unlike I2A). No numeric fidelity scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>More interpretable in that it uses explicit return estimates from rollouts (an explicit Monte-Carlo summarizer) rather than learned opaque rollouts embeddings, but this explicitness makes it sensitive to model errors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Return computation and temperature parameter are explicit; no further interpretability methods.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Rollouts per action and return computation impose similar compute to I2A rollouts; performance depends on rollout depth tuned per model (1-5 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less robust than learned-rollout-encoder I2A in presence of model noise; with accurate model performs similar to a standard baseline but does not match I2A's robustness benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Similar to standard baseline when using high-accuracy model; catastrophic degradation with poor/noisy models (quantitative curves shown in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Explicitly estimating returns from rollouts can work if the model is accurate; but when model misspecification exists, fixed return-summary methods are brittle. Learning to interpret rollouts (I2A) provides robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Explicitness/interpretability vs robustness: the more interpretable Monte-Carlo summarizer is less robust to model imperfections compared to a learned encoder that can learn to ignore erroneous parts of trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>No rollout encoder; explicit Monte-Carlo return summarizer; uses distillation for rollout policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs worse than I2A with learned rollout encoder under noisy model conditions; similar to baseline under accurate model conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that learning arbitrary encodings of rollouts (as in I2A) is advantageous when models are imperfect; thus encoder-free MC is not optimal in presence of model misspecification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1421.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perfect model for MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Near-perfect environment simulator used as oracle model for MCTS comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A near-perfect environment simulator (or high-accuracy learned model) used to compare sample/compute efficiency of I2A vs planning (MCTS); enables strong MCTS baselines and Monte-Carlo outer-loop searches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Near-perfect environment simulator / high-accuracy model (used for planning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A highly accurate model (in some experiments a true simulator, in others a very accurate learned model including reward predictions) used inside MCTS or nested Monte-Carlo searches to evaluate many candidate action sequences; assumed to be close to the true environment transition and reward dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit perfect simulator / high-fidelity learned model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Sokoban planning comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Implicitly assumed perfect or near-perfect; quality judged by MCTS performance when used to simulate many trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Enables MCTS to reach up to 95% levels solved when using ~100k model simulation steps per level; I2A with near-perfect model + outer MC search reaches 95% with ~4k model calls.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Simulator outputs actual future environment states and rewards; fully interpretable in terms of true environment state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>N/A (explicit simulator).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: MCTS with perfect model required ~25k model simulation steps per level to match I2A's 87% and ~100k to reach 95%; outer MC with I2A required ~4k. These model-call counts illustrate computational expense.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MCTS is much more computationally expensive (many more model calls) than learned I2A rollouts for a given performance level, although MCTS can reach high performance given enough model simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High when many simulations are available (e.g., MCTS reaches 95% with 100k sims); I2A with perfect model plus MC search can reach 95% with far fewer sims.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Perfect models + classical planners can achieve top performance but at very high simulation cost; learned rollout policies in I2A guide simulations to task-relevant regions, reducing required model calls.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity models + search yield strong performance but cost many model simulations; learned-guided short rollouts can achieve similar performance far more efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use perfect simulator for benchmarking planning algorithms (MCTS) and compare model-call efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Perfect-model MCTS vs learned I2A: I2A is far more imagination-efficient (fewer model calls) at matched performance, though MCTS scales performance with more compute.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>For planning-heavy methods, near-perfect models and many simulations are optimal but expensive; I2A suggests shorter learned rollouts plus learned interpretation are optimal under limited compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1421.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action-conditional video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-conditional video prediction using deep networks in atari games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of action-conditional next-step pixel predictors that take the current image and action and predict the next image; cited as prior work on learned environment models from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Action-conditional video prediction using deep networks in atari games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Action-conditional video prediction model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural networks (typically CNNs with action-conditioning) trained to predict next video frames given current frame and action; often trained with pixel-wise losses (e.g., cross-entropy or MSE) and used as environment models for planning or imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural pixel-based action-conditional predictor</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and other pixel-based RL domains</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel prediction loss (e.g., cross-entropy, MSE) on next-frame prediction; qualitative rollout fidelity observed.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural predictors; predicted frames are interpretable as images but internal representations not explicitly interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualizing predicted frames/rollouts; no further interpretability methods mentioned in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this paper beyond noting such models can be expensive and suffer from compounding errors over multi-step rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as prior work that enables learned predictions from pixels but suffers from errors that compound during planning—motivates I2A learning to interpret imperfect predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for short-term predictions and imagination but subject to compounding errors; prior successes primarily in domains where models are easy to learn or exact simulators available.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Accurate short-term predictions are possible; multi-step rollouts often accumulate error leading to poor planning unless compensated for.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Action-conditioning, pixel-level outputs, trained on next-frame prediction losses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to model-free RL and to I2A approach which learns to interpret rather than relying strictly on predicted returns.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe optimal configuration; suggests these models are useful but imperfect and motivate architectures robust to model errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1421.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrent env simulators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent environment simulators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent neural network-based environment simulators that model dynamics over multiple timesteps and can be rolled forward autoregressively; cited as related prior work for learned world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent environment simulators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent environment simulator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RNN-based auto-regressive models that take sequences of observations and actions and predict future observations and rewards; designed to capture temporal dependencies beyond single-step predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Sequential decision-making problems from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Sequence prediction likelihood (NLL) / next-step prediction error; no numeric values given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box RNN representations; outputs (predicted frames) are visualizable but hidden state not explicitly interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of rollouts; no further interpretability methods detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Potentially higher cost due to recurrence and multi-step rollout; exact costs not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as part of the broader literature of learned models; I2A's contribution is orthogonal (learn to interpret rollouts) and can use such recurrent models as its imagination core.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Recurrent simulators can capture temporal structure useful for planning but still susceptible to compounding errors in long rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Recurrence can model dynamics better over time but increases risk of error accumulation and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use recurrence (RNN/LSTM) to model temporal dynamics in features or pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Recurrent simulators vs single-step action-conditional predictors: recurrence can capture longer temporal structure but both suffer from approximation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified here; I2A can, in principle, use recurrent environment simulators as its imagination core.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1421.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PILCO (GP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pilco: A model-based and data-efficient approach to policy search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Gaussian Process (GP)-based model learning approach that captures model uncertainty and enables sample-efficient policy search, cited as a principled but computationally heavy way to handle model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pilco: A model-based and data-efficient approach to policy search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PILCO (Gaussian Process world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GP-based probabilistic dynamics model that provides calibrated uncertainty estimates and is used for data-efficient policy search by propagating uncertainty through predicted trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic GP world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Robotics and low-dimensional control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>GP predictive log-likelihood, predictive mean squared error; uncertainty calibration metrics typically used.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>GP models are interpretable probabilistic models with explicit uncertainty estimates; model structure (kernel) is transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Uncertainty estimates (predictive variances) and kernel analysis; mentioned in paper as principled but computationally costly.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High computational cost (GP inference scales poorly with dataset size) — cited as a disadvantage for large domains.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Very data-efficient for small-scale tasks but computationally expensive and less scalable to high-dimensional pixel domains compared to neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Principled handling of model uncertainty makes PILCO robust in small-scale control domains, but the approach does not scale well to large, high-dimensional tasks targeted by I2A.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High interpretability and uncertainty calibration vs poor scalability and high computational burden.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use GP predictive distributions to propagate uncertainty and perform policy search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with neural pixel predictors used in I2A: PILCO offers uncertainty but is computationally costly and not suitable for pixel-based large-scale domains.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not directly discussed; cited as principled for small domains but impractical for large, high-dimensional RL settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1421.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-fidelity simulators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Real-world reinforcement learning via multifidelity simulators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use a hierarchy of simulators of increasing (known) fidelity to mitigate model misspecification by trading off simulation fidelity and computational/real-world cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Real-world reinforcement learning via multifidelity simulators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-fidelity simulator hierarchy</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A set of simulators/models with different known fidelities; control/policy learning leverages cheaper low-fidelity simulators for exploration and higher-fidelity simulators or real world for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>multi-fidelity / hierarchical simulators</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Robotics / real-world RL where simulators of differing fidelity exist</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Simulator fidelity considered known a priori; performance measured by real-world control performance and simulation-to-real transfer metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Depends on simulators used; not inherently interpretable but structured by fidelity levels.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specified in this paper; the approach is cited as effective but computationally involved.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Cited as high overall computational cost when integrating many simulators and real-world evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Offers a principled way to mitigate model misspecification, but at a computational expense; contrasted in paper with I2A which learns to interpret imperfect models rather than relying on multiple fidelity simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Helpful when simulators of varying fidelity are available; not always practical or available, unlike learned models that I2A uses.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Better reliability vs high computational and engineering cost to create and manage multi-fidelity simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Hierarchy of known-fidelity models; selection based on control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to I2A: multi-fidelity simulators are principled but more computationally and engineering heavy; I2A attempts robustness via learned interpretation of a single (possibly imperfect) model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Configuration depends on available simulators and real-world constraints; not prescribed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1421.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dyna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic Dyna architecture that uses a learned model to generate synthetic experience for updating policies/value functions; cited as prior work on using learned models to augment learning but typically not used at test time for active planning in this paper's context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dyna-style learned model (experience-generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learned environment dynamics used to generate synthetic transitions (imagined experiences) to update value/policy networks (i.e., model used for data augmentation rather than online planning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>model-based data augmentation (learned simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General RL tasks; historically used in low- to medium-dimensional problems</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction error on generated transitions and downstream sample efficiency improvements; specific metrics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model interpretability depends on implementation; typically neural predictors are black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>N/A in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Cost is in generating synthetic transitions and training policies on them; not directly compared numerically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Dyna-style methods improve data efficiency by using imagined data but differ from I2A because Dyna models are often not used at test-time for imagination-driven action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for data efficiency in training, but I2A emphasizes using the model at decision time (imagination) and learning to interpret predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Using models for data augmentation reduces environment interactions but does not provide online planning benefits at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Generate imagined transitions from learned model and incorporate into training updates (Dyna framework).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Dyna vs I2A: both use learned models but for different purposes — Dyna for synthetic experience during training, I2A for online imagined rollouts at decision time.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Depends on goals: data-efficiency (Dyna) vs test-time imagination and planning (I2A).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1421.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1421.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schema networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Schema networks: Zero-shot transfer with a generative causal model of intuitive physics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic / generative causal model approach aimed at learning high-level symbolic models (schemas) for intuitive physics and planning, cited as an alternative approach to learning structured, interpretable world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Schema networks: Zero-shot transfer with a generative causal model of intuitive physics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Schema networks (symbolic generative causal models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that learn symbolic causal rules / schemas representing object interactions and physics-like dynamics; used for planning and zero-shot transfer by leveraging learned symbolic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>symbolic generative causal world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Intuitive physics, structured planning tasks; demonstrated in transfer and zero-shot scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task performance on transfer/zero-shot tasks; fidelity described in terms of causal correctness rather than pixel-wise prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Designed to be interpretable: learned schemas correspond to explicit causal rules and symbolic relations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Symbolic rule extraction and inspection; explicit causal structure enables human-level interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Depends on symbolic learning and inference algorithms; may require hand-crafted perception in some cases (paper notes given abstractions from vision), which has engineering cost.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Offers interpretability and transfer but often requires structured inputs or hand-crafted perception pipeline; contrasted with end-to-end pixel predictors used by I2A.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Symbolic causal models can excel at transfer and interpretability; may be limited by need for abstraction from pixels and engineering of perception.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Interpretability and causal structure vs requirement for abstractions and potential brittleness in raw pixel domains.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Learn symbolic generative rules (schemas) for object interactions; often require structured input or perceptual front-end.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Symbolic models vs neural pixel predictors: symbolic models are more interpretable and can transfer zero-shot but are harder to obtain from raw pixels without additional systems.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests symbolic high-level models are valuable but complementary to end-to-end learned pixel models; combining approaches is an open direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagination-Augmented Agents for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Action-conditional video prediction using deep networks in atari games <em>(Rating: 2)</em></li>
                <li>Recurrent environment simulators <em>(Rating: 2)</em></li>
                <li>Pilco: A model-based and data-efficient approach to policy search <em>(Rating: 2)</em></li>
                <li>Real-world reinforcement learning via multifidelity simulators <em>(Rating: 2)</em></li>
                <li>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming <em>(Rating: 1)</em></li>
                <li>Schema networks: Zero-shot transfer with a generative causal model of intuitive physics <em>(Rating: 1)</em></li>
                <li>Embed to control: A locally linear latent dynamics model for control from raw images <em>(Rating: 1)</em></li>
                <li>Deep visual foresight for planning robot motion <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1421",
    "paper_id": "paper-600bfe5f0597ebd84898f0c4270ddfb3750594f5",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "I2A env model",
            "name_full": "Imagination-Augmented Agent environment model (action-conditional next-step predictor)",
            "brief_description": "The learned environment/world model used by I2As: an action-conditional, auto-regressive next-step predictor that outputs a pixel-wise distribution over the next observation and a distribution for reward; used to generate short rollouts (imagined trajectories) that are encoded and consumed by the policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "I2A environment model (action-conditional next-step predictor)",
            "model_description": "Auto-regressive, action-conditional neural predictor that takes the current observation (pixels) and the current action (broadcasted as one-hot), and predicts the next frame as a pixel-wise probability distribution plus a reward distribution. It is rolled forward iteratively to produce multi-step imagined trajectories whose features (predicted observation + predicted reward) are encoded by a rollout encoder LSTM.",
            "model_type": "neural simulator / action-conditional next-step predictor (explicit pixel predictor)",
            "task_domain": "Sokoban (sprite-based grid game) and MiniPacman (small pixel maze domain); general RL domains with raw pixel observations",
            "fidelity_metric": "Negative log-likelihood / Bernoulli cross-entropy on next-frame prediction and log-likelihood loss for reward prediction (l_model); qualitative inspection of rollout visual fidelity and accumulation of artifacts over timesteps",
            "fidelity_performance": "No single numeric prediction metric reported; qualitatively: 'accurate' models yield realistic multi-step rollouts; 'noisy' smaller models produce clear artifacts (missing/duplicate sprites) when rolled for 5+ steps. Pretraining dataset budget reported as &lt;= 1e8 frames for models used in Sokoban experiments.",
            "interpretability_assessment": "Model is a black-box neural pixel predictor (CNN / residual CNN), not inherently interpretable; imagined frames are directly visualizable which aids interpretability of specific rollouts, but latent representations are not described as human-interpretable.",
            "interpretability_method": "Visual inspection of imagined rollouts; downstream learned rollout-encoder (LSTM) that 'learns to interpret' rollouts rather than relying on fixed summaries; no explicit latent disentangling or symbolic extraction reported.",
            "computational_cost": "Training: pretraining required on up to ~1e8 frames (conservative estimate). Inference: environment model calls used inside rollouts; I2A overall was reported as less than an order of magnitude slower per environment interaction than model-free baselines. Model-call counts: ~1.4k model simulation steps per solved Sokoban level for I2A (87% performance) when using short rollouts.",
            "efficiency_comparison": "I2A required far fewer model calls than MCTS: to reach comparable performance (87% solved) I2A used ~1.4k model calls per level vs MCTS ~25k; MCTS required ~100k model calls to reach 95% while an I2A with MC outer loop required ~4k model calls to reach 95%. Pretraining cost (data) is additional but can be amortized across tasks.",
            "task_performance": "When embedded in I2A, the model supported agents that solved ~85% of Sokoban levels (rollouts length 5); longer rollouts (15) reached &gt;90%+; with near-perfect model plus MC search I2A reached ~95%. In MiniPacman, using a shared model across tasks improved performance and data efficiency; I2A outperformed model-free baselines on several tasks, especially with sparse rewards.",
            "task_utility_analysis": "World model utility is task-dependent: short, approximate rollouts provided task-relevant signals that improve planning/policy learning and robustness; even imperfect models provide useful early-step predictions (I2A learns to use or ignore portions of rollouts). High fidelity can improve performance further (MC within I2A), but I2A is explicitly designed to extract utility from imperfect predictions, so full fidelity is not required to gain benefits.",
            "tradeoffs_observed": "Trade-offs include: (a) data/computation to pretrain the model (pretraining frames) vs reduced environment interactions and improved sample efficiency for policy learning; (b) longer/deeper rollouts increase task performance but with diminishing returns and higher compute; (c) perfect/high-fidelity models enable stronger planning baselines (e.g., MCTS or MC search) but require many more simulator calls; (d) interpretable or symbolic models not used here — neural pixel predictors remain black-box but are practical.",
            "design_choices": "Action-conditional next-step prediction at pixel level; predict both observation and reward; auto-regressive rollouts seeded from real observations; rollout policy distilled from the I2A policy to produce task-relevant imagined trajectories; rollout encoder (CNN + LSTM) processes reversed-order features and learns to summarize rollouts; environment model can be pretrained separately (faster runtime) or trained jointly with auxiliary loss.",
            "comparison_to_alternatives": "Compared to model-free baselines: I2A (with the learned model) outperforms model-free agents in Sokoban and MiniPacman and is more data-efficient. Compared to classical planning (MCTS with near-perfect model): I2A achieves comparable performance with far fewer model calls by learning to focus rollouts. Compared to simplistic/no-model baselines (copy-model): explicit predictive model rollouts improve performance significantly. Compared to encoder-free Monte-Carlo rollout value estimation: the learned rollout encoder provides robustness to model noise while encoder-free MC degrades catastrophically with a noisy model.",
            "optimal_configuration": "Paper recommendations: use short multi-step rollouts (they found 3-15 steps useful; 5 is a pragmatic choice balancing compute and performance), predict both frames and rewards when possible, pretrain the environment model to accelerate training, and distill the I2A policy into the rollout policy to guide rollouts toward task-relevant areas. Emphasis on learning to interpret imperfect models rather than relying on perfect fidelity.",
            "uuid": "e1421.0",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Sokoban model",
            "name_full": "Sokoban environment model (residual CNN + reward MLP)",
            "brief_description": "A domain-specific instance of the I2A environment model used in Sokoban: a residual convolutional neural network that predicts the next 80x80 RGB frame and a reward distribution via a separate CNN/MLP pathway.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Sokoban residual CNN environment model",
            "model_description": "Input: current 80x80x3 frame plus a broadcasted one-hot action. Architecture: initial large 8x8 convolution with stride 8, followed by size-preserving convolutional layers and residual CNN blocks; separate CNN/fully-connected MLP pathway for reward prediction. Outputs: pixel-wise probability distribution for the next image and a reward distribution.",
            "model_type": "neural simulator / residual CNN pixel predictor",
            "task_domain": "Sokoban (visual sprite grid puzzle)",
            "fidelity_metric": "Trained using Bernoulli cross-entropy (negative log-likelihood) between predicted pixels and true next-frame; reward predicted with distributional loss as part of model loss l_model.",
            "fidelity_performance": "Qualitative: one high-capacity Sokoban model produced accurate short rollouts; an intentionally smaller/noisy model produced clear artifacts over 5-step rollouts (missing/duplicate sprites). No scalar per-frame error metrics reported in paper.",
            "interpretability_assessment": "Black-box CNN model; rollouts are pixel images so errors/artifacts are interpretable visually; internal features not claimed to be interpretable.",
            "interpretability_method": "Visual inspection of rollouts; use of rollout encoder to learn task-relevant interpretation of predicted frames rather than explicit interpretability methods.",
            "computational_cost": "Model architecture is lightweight relative to large image models but specific parameter counts not reported; pretraining budget reported as part of overall model pretraining (&lt;=1e8 frames). Inference used in many short rollouts per action (one rollout per possible action in experiments).",
            "efficiency_comparison": "No direct comparison of Sokoban model compute vs alternatives beyond noting the smaller (noisy) model has lower parameter count but worse visual fidelity; I2A with this model still robust while encoder-free MC fails with noisy model.",
            "task_performance": "When used in I2A, supported 85%+ solved levels (rollout length 5) and up to &gt;90% with longer rollouts or 95% with outer MC retries when the model is near-perfect.",
            "task_utility_analysis": "Provides short-term, actionable predictions (initial few steps) that are highly useful for avoiding irreversible mistakes in Sokoban; later rollout steps may degrade but the learned encoder learns to weigh earlier predictions more.",
            "tradeoffs_observed": "Smaller models (less compute) produce noisy rollouts that accumulate artifacts over steps; however I2A can learn to ignore erroneous later predictions — robustness reduces need for perfect fidelity. Longer rollouts improve decision quality but increase compute/time.",
            "design_choices": "Residual CNN backbone; stride-8 initial conv to reduce spatial size; separate reward prediction head; pixel-level Bernoulli outputs; rollouts seeded with real observation and fed back autoregressively.",
            "comparison_to_alternatives": "Compared to an identity 'copy' model (which returns the input frame), the Sokoban predictive model yields much better performance (copy-model performs far worse). Compared to smaller/noisy versions, larger accurate models give visually better rollouts and can enable additional MC techniques.",
            "optimal_configuration": "Paper suggests predicting both observations and rewards helps (reward prediction beneficial but not strictly necessary), and that a reasonably sized residual CNN giving accurate short-horizon rollouts is effective; coupling model with a learned rollout encoder and distilled rollout policy is recommended.",
            "uuid": "e1421.1",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "MiniPacman model",
            "name_full": "MiniPacman environment model (multi-scale CNN with pool-and-inject + reward layers)",
            "brief_description": "An auto-regressive, multi-scale convolutional environment model for MiniPacman that includes a 'pool-and-inject' mechanism to capture long-range dependencies and a separate pathway for event/reward prediction; trained to predict next-frame pixels and discrete events.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MiniPacman multi-scale CNN environment model",
            "model_description": "Input: 15x19x3 frame and broadcasted one-hot action. Architecture: size-preserving multi-scale CNN blocks, a 'pool-and-inject' layer (global max-pooling across feature maps which is broadcast back), and additional fully-connected layers for reward/event prediction. Output: pixel-wise next-frame distribution and predicted environment events used to compute task rewards given task-specific reward vector.",
            "model_type": "neural simulator / multi-scale CNN pixel predictor",
            "task_domain": "MiniPacman multi-task domain (small pixel-based maze with ghosts and events)",
            "fidelity_metric": "Bernoulli cross-entropy on pixel predictions; log-likelihood on event predictions. Model used to compute predicted rewards via event prediction and task reward vector.",
            "fidelity_performance": "No numeric metrics reported; model was sufficiently accurate to be reused across five different MiniPacman tasks and to improve I2A agent performance compared to baselines, particularly for sparse-reward tasks.",
            "interpretability_assessment": "Model is a black-box CNN with some architectural mechanisms (pool-and-inject) to capture global interactions; predicted frames and event outputs are directly interpretable as images and events, respectively.",
            "interpretability_method": "Visualization of predicted frames and event outputs; using predicted events to compute task-specific rewards (explicit mapping makes event outputs interpretable for reward computation).",
            "computational_cost": "Model is described as 'light-weight' for rapid experimentation; exact parameter counts and timing not given. Shared single model used across multiple tasks to amortize pretraining cost.",
            "efficiency_comparison": "Using one shared model across tasks yields low marginal cost per task; model-based I2A agents outperform model-free baselines in data efficiency and final performance in MiniPacman tasks, especially with sparse rewards.",
            "task_performance": "I2A with this shared MiniPacman model outperformed standard model-free baselines across 5 tasks after 300M environment steps; largest gains on sparse-reward tasks (tasks 4 & 5).",
            "task_utility_analysis": "Event prediction + task reward vector design allowed effective reuse of a single trained model across diverse reward definitions; predicted event signals were directly useful for policy learning in different tasks.",
            "tradeoffs_observed": "Investing in a slightly more complex model (pool-and-inject) yields ability to capture nonlocal interactions (ghost dynamics) critical for task performance; model complexity vs runtime tradeoff not quantified but described as practical for rapid experiments.",
            "design_choices": "Multi-scale CNN to preserve spatial size; pool-and-inject for global communication; separate MLP for reward/event prediction; shared model across tasks with task-specific reward vector w_rew to interpret events as rewards.",
            "comparison_to_alternatives": "Compared to training separate models per task, a single shared model reduced marginal cost and improved data efficiency. Compared to model-free baselines, I2A+model improved performance especially on sparse/reward-difficult tasks.",
            "optimal_configuration": "Paper suggests training a general model that predicts low-level observations and events, and interpreting task objectives as reward vectors applied to events — this enables reuse across tasks and better sample efficiency.",
            "uuid": "e1421.2",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Copy model (identity)",
            "name_full": "Copy-model identity world model baseline",
            "brief_description": "A control baseline that replaces the predictive environment model with a 'copy' model that simply returns the input observation (no dynamics prediction), keeping the rest of I2A architecture identical to test whether rollouts themselves (not model predictions) drive gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Copy-model (identity)",
            "model_description": "Identity function: given the current observation, the 'model' outputs the same observation (no forward prediction). Used inside the I2A architecture to keep compute and parameter counts comparable while removing predictive content.",
            "model_type": "non-predictive identity baseline",
            "task_domain": "Sokoban and MiniPacman experiments as baseline",
            "fidelity_metric": "Not applicable (no predictive objective); fidelity is zero for forward prediction.",
            "fidelity_performance": "N/A",
            "interpretability_assessment": "Transparent: returns input observation unchanged; not a forward model and therefore provides no imagined future.",
            "interpretability_method": "N/A",
            "computational_cost": "Same order of compute as I2A with a learned model (environment model kept constant in I2A), used to control for compute and parameter count.",
            "efficiency_comparison": "Despite matching compute and parameters, this baseline performs far worse than I2A with a learned model, indicating predictive rollouts provide the performance improvement.",
            "task_performance": "Copy-model I2A performed significantly worse than I2A with a learned predictive model on Sokoban and MiniPacman.",
            "task_utility_analysis": "Demonstrates that the utility arises from predictive content of a world model rather than architectural capacity or compute alone.",
            "tradeoffs_observed": "Shows that simply increasing compute or network depth without predictive modeling does not yield the same planning benefit.",
            "design_choices": "Identity model to control for architecture and compute.",
            "comparison_to_alternatives": "Performs worse than I2A with learned predictive model; used to isolate effect of model-based imagination.",
            "optimal_configuration": "Not applicable; serves as negative control.",
            "uuid": "e1421.3",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "MC rollout (encoder-free)",
            "name_full": "Monte-Carlo rollout value estimator (encoder-free I2A variant)",
            "brief_description": "A baseline architecture that estimates action values by explicitly computing Monte-Carlo returns from rollouts produced by an environment model, without a learned rollout encoder; action selection is proportional to exponentiated estimated returns with a learned temperature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Monte-Carlo rollout value estimator (encoder-free)",
            "model_description": "For each candidate initial action, sample a rollout using a rollout policy, compute the discounted sum of rewards plus a learned terminal value V(x_T), and select actions proportional to exp(-estimated_return/δ) where δ is learned temperature. Only V, rollout policy ̂π and δ are learned; no learned summarizer of trajectories.",
            "model_type": "model-based Monte Carlo evaluator (no learned rollout encoder)",
            "task_domain": "Sokoban experiments (comparison to I2A)",
            "fidelity_metric": "Performance measured by final task performance (levels solved); rollouts use the same environment models as other agents so underlying model fidelity is same metric as model likelihood. No separate prediction error reported here.",
            "fidelity_performance": "When using an accurate environment model, performance similar to baseline standard agent; when using a noisy/poor model, performance degrades catastrophically (unlike I2A). No numeric fidelity scores provided.",
            "interpretability_assessment": "More interpretable in that it uses explicit return estimates from rollouts (an explicit Monte-Carlo summarizer) rather than learned opaque rollouts embeddings, but this explicitness makes it sensitive to model errors.",
            "interpretability_method": "Return computation and temperature parameter are explicit; no further interpretability methods.",
            "computational_cost": "Rollouts per action and return computation impose similar compute to I2A rollouts; performance depends on rollout depth tuned per model (1-5 steps).",
            "efficiency_comparison": "Less robust than learned-rollout-encoder I2A in presence of model noise; with accurate model performs similar to a standard baseline but does not match I2A's robustness benefits.",
            "task_performance": "Similar to standard baseline when using high-accuracy model; catastrophic degradation with poor/noisy models (quantitative curves shown in paper).",
            "task_utility_analysis": "Explicitly estimating returns from rollouts can work if the model is accurate; but when model misspecification exists, fixed return-summary methods are brittle. Learning to interpret rollouts (I2A) provides robustness.",
            "tradeoffs_observed": "Explicitness/interpretability vs robustness: the more interpretable Monte-Carlo summarizer is less robust to model imperfections compared to a learned encoder that can learn to ignore erroneous parts of trajectories.",
            "design_choices": "No rollout encoder; explicit Monte-Carlo return summarizer; uses distillation for rollout policy.",
            "comparison_to_alternatives": "Performs worse than I2A with learned rollout encoder under noisy model conditions; similar to baseline under accurate model conditions.",
            "optimal_configuration": "Paper suggests that learning arbitrary encodings of rollouts (as in I2A) is advantageous when models are imperfect; thus encoder-free MC is not optimal in presence of model misspecification.",
            "uuid": "e1421.4",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Perfect model for MCTS",
            "name_full": "Near-perfect environment simulator used as oracle model for MCTS comparisons",
            "brief_description": "A near-perfect environment simulator (or high-accuracy learned model) used to compare sample/compute efficiency of I2A vs planning (MCTS); enables strong MCTS baselines and Monte-Carlo outer-loop searches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Near-perfect environment simulator / high-accuracy model (used for planning)",
            "model_description": "A highly accurate model (in some experiments a true simulator, in others a very accurate learned model including reward predictions) used inside MCTS or nested Monte-Carlo searches to evaluate many candidate action sequences; assumed to be close to the true environment transition and reward dynamics.",
            "model_type": "explicit perfect simulator / high-fidelity learned model",
            "task_domain": "Sokoban planning comparisons",
            "fidelity_metric": "Implicitly assumed perfect or near-perfect; quality judged by MCTS performance when used to simulate many trajectories.",
            "fidelity_performance": "Enables MCTS to reach up to 95% levels solved when using ~100k model simulation steps per level; I2A with near-perfect model + outer MC search reaches 95% with ~4k model calls.",
            "interpretability_assessment": "Simulator outputs actual future environment states and rewards; fully interpretable in terms of true environment state transitions.",
            "interpretability_method": "N/A (explicit simulator).",
            "computational_cost": "High: MCTS with perfect model required ~25k model simulation steps per level to match I2A's 87% and ~100k to reach 95%; outer MC with I2A required ~4k. These model-call counts illustrate computational expense.",
            "efficiency_comparison": "MCTS is much more computationally expensive (many more model calls) than learned I2A rollouts for a given performance level, although MCTS can reach high performance given enough model simulations.",
            "task_performance": "High when many simulations are available (e.g., MCTS reaches 95% with 100k sims); I2A with perfect model plus MC search can reach 95% with far fewer sims.",
            "task_utility_analysis": "Perfect models + classical planners can achieve top performance but at very high simulation cost; learned rollout policies in I2A guide simulations to task-relevant regions, reducing required model calls.",
            "tradeoffs_observed": "High-fidelity models + search yield strong performance but cost many model simulations; learned-guided short rollouts can achieve similar performance far more efficiently.",
            "design_choices": "Use perfect simulator for benchmarking planning algorithms (MCTS) and compare model-call efficiency.",
            "comparison_to_alternatives": "Perfect-model MCTS vs learned I2A: I2A is far more imagination-efficient (fewer model calls) at matched performance, though MCTS scales performance with more compute.",
            "optimal_configuration": "For planning-heavy methods, near-perfect models and many simulations are optimal but expensive; I2A suggests shorter learned rollouts plus learned interpretation are optimal under limited compute.",
            "uuid": "e1421.5",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Action-conditional video prediction",
            "name_full": "Action-conditional video prediction using deep networks in atari games",
            "brief_description": "A class of action-conditional next-step pixel predictors that take the current image and action and predict the next image; cited as prior work on learned environment models from pixels.",
            "citation_title": "Action-conditional video prediction using deep networks in atari games",
            "mention_or_use": "mention",
            "model_name": "Action-conditional video prediction model",
            "model_description": "Neural networks (typically CNNs with action-conditioning) trained to predict next video frames given current frame and action; often trained with pixel-wise losses (e.g., cross-entropy or MSE) and used as environment models for planning or imagination.",
            "model_type": "neural pixel-based action-conditional predictor",
            "task_domain": "Atari games and other pixel-based RL domains",
            "fidelity_metric": "Pixel prediction loss (e.g., cross-entropy, MSE) on next-frame prediction; qualitative rollout fidelity observed.",
            "fidelity_performance": null,
            "interpretability_assessment": "Black-box neural predictors; predicted frames are interpretable as images but internal representations not explicitly interpretable.",
            "interpretability_method": "Visualizing predicted frames/rollouts; no further interpretability methods mentioned in this paper's discussion.",
            "computational_cost": "Not specified in this paper beyond noting such models can be expensive and suffer from compounding errors over multi-step rollouts.",
            "efficiency_comparison": "Mentioned as prior work that enables learned predictions from pixels but suffers from errors that compound during planning—motivates I2A learning to interpret imperfect predictions.",
            "task_performance": null,
            "task_utility_analysis": "Useful for short-term predictions and imagination but subject to compounding errors; prior successes primarily in domains where models are easy to learn or exact simulators available.",
            "tradeoffs_observed": "Accurate short-term predictions are possible; multi-step rollouts often accumulate error leading to poor planning unless compensated for.",
            "design_choices": "Action-conditioning, pixel-level outputs, trained on next-frame prediction losses.",
            "comparison_to_alternatives": "Compared implicitly to model-free RL and to I2A approach which learns to interpret rather than relying strictly on predicted returns.",
            "optimal_configuration": "Paper does not prescribe optimal configuration; suggests these models are useful but imperfect and motivate architectures robust to model errors.",
            "uuid": "e1421.6",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Recurrent env simulators",
            "name_full": "Recurrent environment simulators",
            "brief_description": "Recurrent neural network-based environment simulators that model dynamics over multiple timesteps and can be rolled forward autoregressively; cited as related prior work for learned world models.",
            "citation_title": "Recurrent environment simulators",
            "mention_or_use": "mention",
            "model_name": "Recurrent environment simulator",
            "model_description": "RNN-based auto-regressive models that take sequences of observations and actions and predict future observations and rewards; designed to capture temporal dependencies beyond single-step predictors.",
            "model_type": "recurrent neural world model",
            "task_domain": "Sequential decision-making problems from pixels",
            "fidelity_metric": "Sequence prediction likelihood (NLL) / next-step prediction error; no numeric values given in this paper.",
            "fidelity_performance": null,
            "interpretability_assessment": "Black-box RNN representations; outputs (predicted frames) are visualizable but hidden state not explicitly interpretable.",
            "interpretability_method": "Visualization of rollouts; no further interpretability methods detailed here.",
            "computational_cost": "Potentially higher cost due to recurrence and multi-step rollout; exact costs not specified in this paper.",
            "efficiency_comparison": "Mentioned as part of the broader literature of learned models; I2A's contribution is orthogonal (learn to interpret rollouts) and can use such recurrent models as its imagination core.",
            "task_performance": null,
            "task_utility_analysis": "Recurrent simulators can capture temporal structure useful for planning but still susceptible to compounding errors in long rollouts.",
            "tradeoffs_observed": "Recurrence can model dynamics better over time but increases risk of error accumulation and computational cost.",
            "design_choices": "Use recurrence (RNN/LSTM) to model temporal dynamics in features or pixels.",
            "comparison_to_alternatives": "Recurrent simulators vs single-step action-conditional predictors: recurrence can capture longer temporal structure but both suffer from approximation errors.",
            "optimal_configuration": "Not specified here; I2A can, in principle, use recurrent environment simulators as its imagination core.",
            "uuid": "e1421.7",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "PILCO (GP)",
            "name_full": "Pilco: A model-based and data-efficient approach to policy search",
            "brief_description": "A Gaussian Process (GP)-based model learning approach that captures model uncertainty and enables sample-efficient policy search, cited as a principled but computationally heavy way to handle model uncertainty.",
            "citation_title": "Pilco: A model-based and data-efficient approach to policy search",
            "mention_or_use": "mention",
            "model_name": "PILCO (Gaussian Process world model)",
            "model_description": "GP-based probabilistic dynamics model that provides calibrated uncertainty estimates and is used for data-efficient policy search by propagating uncertainty through predicted trajectories.",
            "model_type": "probabilistic GP world model",
            "task_domain": "Robotics and low-dimensional control tasks",
            "fidelity_metric": "GP predictive log-likelihood, predictive mean squared error; uncertainty calibration metrics typically used.",
            "fidelity_performance": null,
            "interpretability_assessment": "GP models are interpretable probabilistic models with explicit uncertainty estimates; model structure (kernel) is transparent.",
            "interpretability_method": "Uncertainty estimates (predictive variances) and kernel analysis; mentioned in paper as principled but computationally costly.",
            "computational_cost": "High computational cost (GP inference scales poorly with dataset size) — cited as a disadvantage for large domains.",
            "efficiency_comparison": "Very data-efficient for small-scale tasks but computationally expensive and less scalable to high-dimensional pixel domains compared to neural models.",
            "task_performance": null,
            "task_utility_analysis": "Principled handling of model uncertainty makes PILCO robust in small-scale control domains, but the approach does not scale well to large, high-dimensional tasks targeted by I2A.",
            "tradeoffs_observed": "High interpretability and uncertainty calibration vs poor scalability and high computational burden.",
            "design_choices": "Use GP predictive distributions to propagate uncertainty and perform policy search.",
            "comparison_to_alternatives": "Contrasted with neural pixel predictors used in I2A: PILCO offers uncertainty but is computationally costly and not suitable for pixel-based large-scale domains.",
            "optimal_configuration": "Not directly discussed; cited as principled for small domains but impractical for large, high-dimensional RL settings.",
            "uuid": "e1421.8",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Multi-fidelity simulators",
            "name_full": "Real-world reinforcement learning via multifidelity simulators",
            "brief_description": "Approaches that use a hierarchy of simulators of increasing (known) fidelity to mitigate model misspecification by trading off simulation fidelity and computational/real-world cost.",
            "citation_title": "Real-world reinforcement learning via multifidelity simulators",
            "mention_or_use": "mention",
            "model_name": "Multi-fidelity simulator hierarchy",
            "model_description": "A set of simulators/models with different known fidelities; control/policy learning leverages cheaper low-fidelity simulators for exploration and higher-fidelity simulators or real world for refinement.",
            "model_type": "multi-fidelity / hierarchical simulators",
            "task_domain": "Robotics / real-world RL where simulators of differing fidelity exist",
            "fidelity_metric": "Simulator fidelity considered known a priori; performance measured by real-world control performance and simulation-to-real transfer metrics.",
            "fidelity_performance": null,
            "interpretability_assessment": "Depends on simulators used; not inherently interpretable but structured by fidelity levels.",
            "interpretability_method": "Not specified in this paper; the approach is cited as effective but computationally involved.",
            "computational_cost": "Cited as high overall computational cost when integrating many simulators and real-world evaluations.",
            "efficiency_comparison": "Offers a principled way to mitigate model misspecification, but at a computational expense; contrasted in paper with I2A which learns to interpret imperfect models rather than relying on multiple fidelity simulators.",
            "task_performance": null,
            "task_utility_analysis": "Helpful when simulators of varying fidelity are available; not always practical or available, unlike learned models that I2A uses.",
            "tradeoffs_observed": "Better reliability vs high computational and engineering cost to create and manage multi-fidelity simulators.",
            "design_choices": "Hierarchy of known-fidelity models; selection based on control performance.",
            "comparison_to_alternatives": "Compared to I2A: multi-fidelity simulators are principled but more computationally and engineering heavy; I2A attempts robustness via learned interpretation of a single (possibly imperfect) model.",
            "optimal_configuration": "Configuration depends on available simulators and real-world constraints; not prescribed here.",
            "uuid": "e1421.9",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Dyna",
            "name_full": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
            "brief_description": "Classic Dyna architecture that uses a learned model to generate synthetic experience for updating policies/value functions; cited as prior work on using learned models to augment learning but typically not used at test time for active planning in this paper's context.",
            "citation_title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
            "mention_or_use": "mention",
            "model_name": "Dyna-style learned model (experience-generation)",
            "model_description": "Learned environment dynamics used to generate synthetic transitions (imagined experiences) to update value/policy networks (i.e., model used for data augmentation rather than online planning).",
            "model_type": "model-based data augmentation (learned simulator)",
            "task_domain": "General RL tasks; historically used in low- to medium-dimensional problems",
            "fidelity_metric": "Prediction error on generated transitions and downstream sample efficiency improvements; specific metrics not provided in this paper.",
            "fidelity_performance": null,
            "interpretability_assessment": "Model interpretability depends on implementation; typically neural predictors are black-box.",
            "interpretability_method": "N/A in this paper's discussion.",
            "computational_cost": "Cost is in generating synthetic transitions and training policies on them; not directly compared numerically in this paper.",
            "efficiency_comparison": "Dyna-style methods improve data efficiency by using imagined data but differ from I2A because Dyna models are often not used at test-time for imagination-driven action selection.",
            "task_performance": null,
            "task_utility_analysis": "Useful for data efficiency in training, but I2A emphasizes using the model at decision time (imagination) and learning to interpret predictions.",
            "tradeoffs_observed": "Using models for data augmentation reduces environment interactions but does not provide online planning benefits at test time.",
            "design_choices": "Generate imagined transitions from learned model and incorporate into training updates (Dyna framework).",
            "comparison_to_alternatives": "Dyna vs I2A: both use learned models but for different purposes — Dyna for synthetic experience during training, I2A for online imagined rollouts at decision time.",
            "optimal_configuration": "Depends on goals: data-efficiency (Dyna) vs test-time imagination and planning (I2A).",
            "uuid": "e1421.10",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Schema networks",
            "name_full": "Schema networks: Zero-shot transfer with a generative causal model of intuitive physics",
            "brief_description": "A symbolic / generative causal model approach aimed at learning high-level symbolic models (schemas) for intuitive physics and planning, cited as an alternative approach to learning structured, interpretable world models.",
            "citation_title": "Schema networks: Zero-shot transfer with a generative causal model of intuitive physics",
            "mention_or_use": "mention",
            "model_name": "Schema networks (symbolic generative causal models)",
            "model_description": "Models that learn symbolic causal rules / schemas representing object interactions and physics-like dynamics; used for planning and zero-shot transfer by leveraging learned symbolic structure.",
            "model_type": "symbolic generative causal world model",
            "task_domain": "Intuitive physics, structured planning tasks; demonstrated in transfer and zero-shot scenarios",
            "fidelity_metric": "Task performance on transfer/zero-shot tasks; fidelity described in terms of causal correctness rather than pixel-wise prediction.",
            "fidelity_performance": null,
            "interpretability_assessment": "Designed to be interpretable: learned schemas correspond to explicit causal rules and symbolic relations.",
            "interpretability_method": "Symbolic rule extraction and inspection; explicit causal structure enables human-level interpretability.",
            "computational_cost": "Depends on symbolic learning and inference algorithms; may require hand-crafted perception in some cases (paper notes given abstractions from vision), which has engineering cost.",
            "efficiency_comparison": "Offers interpretability and transfer but often requires structured inputs or hand-crafted perception pipeline; contrasted with end-to-end pixel predictors used by I2A.",
            "task_performance": null,
            "task_utility_analysis": "Symbolic causal models can excel at transfer and interpretability; may be limited by need for abstraction from pixels and engineering of perception.",
            "tradeoffs_observed": "Interpretability and causal structure vs requirement for abstractions and potential brittleness in raw pixel domains.",
            "design_choices": "Learn symbolic generative rules (schemas) for object interactions; often require structured input or perceptual front-end.",
            "comparison_to_alternatives": "Symbolic models vs neural pixel predictors: symbolic models are more interpretable and can transfer zero-shot but are harder to obtain from raw pixels without additional systems.",
            "optimal_configuration": "Paper suggests symbolic high-level models are valuable but complementary to end-to-end learned pixel models; combining approaches is an open direction.",
            "uuid": "e1421.11",
            "source_info": {
                "paper_title": "Imagination-Augmented Agents for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Action-conditional video prediction using deep networks in atari games",
            "rating": 2
        },
        {
            "paper_title": "Recurrent environment simulators",
            "rating": 2
        },
        {
            "paper_title": "Pilco: A model-based and data-efficient approach to policy search",
            "rating": 2
        },
        {
            "paper_title": "Real-world reinforcement learning via multifidelity simulators",
            "rating": 2
        },
        {
            "paper_title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
            "rating": 1
        },
        {
            "paper_title": "Schema networks: Zero-shot transfer with a generative causal model of intuitive physics",
            "rating": 1
        },
        {
            "paper_title": "Embed to control: A locally linear latent dynamics model for control from raw images",
            "rating": 1
        },
        {
            "paper_title": "Deep visual foresight for planning robot motion",
            "rating": 1
        }
    ],
    "cost": 0.024996249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Imagination-Augmented Agents for Deep Reinforcement Learning</h1>
<p>Théophane Weber<em> ${ }^{</em>}$ Sébastien Racanière<em> ${ }^{</em>}$ David P. Reichert<em> ${ }^{</em>}$ Lars Buesing<br>Arthur Guez Danilo Rezende Adria Puigdomènech Badia Oriol Vinyals<br>Nicolas Heess Yujia Li Razvan Pascanu Peter Battaglia<br>Demis Hassabis David Silver Daan Wierstra<br>DeepMind</p>
<h4>Abstract</h4>
<p>We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.</p>
<h2>1 Introduction</h2>
<p>A hallmark of an intelligent agent is its ability to rapidly adapt to new circumstances and "achieve goals in a wide range of environments" [1]. Progress has been made in developing capable agents for numerous domains using deep neural networks in conjunction with model-free reinforcement learning (RL) [2-4], where raw observations directly map to values or actions. However, this approach usually requires large amounts of training data and the resulting policies do not readily generalize to novel tasks in the same environment, as it lacks the behavioral flexibility constitutive of general intelligence.
Model-based RL aims to address these shortcomings by endowing agents with a model of the world, synthesized from past experience. By using an internal model to reason about the future, here also referred to as imagining, the agent can seek positive outcomes while avoiding the adverse consequences of trial-and-error in the real environment - including making irreversible, poor decisions. Even if the model needs to be learned first, it can enable better generalization across states, remain valid across tasks in the same environment, and exploit additional unsupervised learning signals, thus ultimately leading to greater data efficiency. Another appeal of model-based methods is their ability to scale performance with more computation by increasing the amount of internal simulation.</p>
<p>The neural basis for imagination, model-based reasoning and decision making has generated a lot of interest in neuroscience [5-7]; at the cognitive level, model learning and mental simulation have been hypothesized and demonstrated in animal and human learning [8-11]. Its successful deployment in artificial model-based agents however has hitherto been limited to settings where an exact transition model is available [12] or in domains where models are easy to learn - e.g. symbolic environments or low-dimensional systems [13-16]. In complex domains for which a simulator is not available to the agent, recent successes are dominated by model-free methods [2, 17]. In such domains, the performance of model-based agents employing standard planning methods usually suffers from model errors resulting from function approximation [18, 19]. These errors compound during planning, causing over-optimism and poor agent performance. There are currently no planning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>or model-based methods that are robust against model imperfections which are inevitable in complex domains, thereby preventing them from matching the success of their model-free counterparts.</p>
<p>We seek to address this shortcoming by proposing Imagination-Augmented Agents, which use approximate environment models by "learning to interpret" their imperfect predictions. Our algorithm can be trained directly on low-level observations with little domain knowledge, similarly to recent model-free successes. Without making any assumptions about the structure of the environment model and its possible imperfections, our approach learns in an end-to-end way to extract useful knowledge gathered from model simulations - in particular not relying exclusively on simulated returns. This allows the agent to benefit from model-based imagination without the pitfalls of conventional model-based planning. We demonstrate that our approach performs better than model-free baselines in various domains including Sokoban. It achieves better performance with less data, even with imperfect models, a significant step towards delivering the promises of model-based RL.</p>
<h1>2 The I2A architecture</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: I2A architecture. $\hat{\cdot}$ notation indicates imagined quantities. $a$ ): the imagination core (IC) predicts the next time step conditioned on an action sampled from the rollout policy $\hat{\pi}$. $b$ ): the IC imagines trajectories of features $\hat{f}=(\hat{o}, \hat{r})$, encoded by the rollout encoder. $c$ ): in the full I2A, aggregated rollout encodings and input from a model-free path determine the output policy $\pi$.</p>
<p>In order to augment model-free agents with imagination, we rely on environment models - models that, given information from the present, can be queried to make predictions about the future. We use these environment models to simulate imagined trajectories, which are interpreted by a neural network and provided as additional context to a policy network.</p>
<p>In general, an environment model is any recurrent architecture which can be trained in an unsupervised fashion from agent trajectories: given a past state and current action, the environment model predicts the next state and any number of signals from the environment. In this work, we will consider in particular environment models that build on recent successes of action-conditional next-step predictors [20-22], which receive as input the current observation (or history of observations) and current action, and predict the next observation, and potentially the next reward. We roll out the environment model over multiple time steps into the future, by initializing the imagined trajectory with the present time real observation, and subsequently feeding simulated observations into the model.</p>
<p>The actions chosen in each rollout result from a rollout policy $\hat{\pi}$ (explained in Section 3.1). The environment model together with $\hat{\pi}$ constitute the imagination core module, which predicts next time steps (Fig 1a). The imagination core is used to produce $n$ trajectories $\mathcal{T}<em n="n">{1}, \ldots, \mathcal{T}</em>}$. Each imagined trajectory $\mathcal{T}$ is a sequence of features $\left(\hat{f<em t_tau="t+\tau">{t+1}, \ldots, \hat{f}</em>$ the output of the environment model (i.e. the predicted observation and/or reward).}\right)$, where $t$ is the current time, $\tau$ the length of the rollout, and $\hat{f}_{t+i</p>
<p>Despite recent progress in training better environment models, a key issue addressed by I2As is that a learned model cannot be assumed to be perfect; it might sometimes make erroneous or nonsensical predictions. We therefore do not want to rely solely on predicted rewards (or values predicted</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Environment model. The input action is broadcast and concatenated to the observation. A convolutional network transforms this into a pixel-wise probability distribution for the output image, and a distribution for the reward.
from predicted states), as is often done in classical planning. Additionally, trajectories may contain information beyond the reward sequence (a trajectory could contain an informative subsequence - for instance solving a subproblem - which did not result in higher reward). For these reasons, we use a rollout encoder $\mathcal{E}$ that processes the imagined rollout as a whole and learns to interpret it, i.e. by extracting any information useful for the agent's decision, or even ignoring it when necessary (Fig 1b). Each trajectory is encoded separately as a rollout embedding $e_{i}=\mathcal{E}\left(\mathcal{T}<em _ia="{ia" _text="\text">{i}\right)$. Finally, an aggregator $\mathcal{A}$ converts the different rollout embeddings into a single imagination code $c</em>\right)$.
The final component of the I2A is the policy module, which is a network that takes the information $c_{i a}$ from model-based predictions, as well as the output $c_{\text {inf }}$ of a model-free path (a network which only takes the real observation as input; see Fig 1c, right), and outputs the imagination-augmented policy vector $\pi$ and estimated value $V$. The I2A therefore learns to combine information from its model-free and imagination-augmented paths; note that without the model-based path, I2As reduce to a standard model-free network [3]. I2As can thus be thought of as augmenting model-free agents by providing additional information from model-based planning, and as having strictly more expressive power than the underlying model-free agent.}}=\mathcal{A}\left(e_{1}, \ldots, e_{n</p>
<h1>3 Architectural choices and experimental setup</h1>
<h3>3.1 Rollout strategy</h3>
<p>For our experiments, we perform one rollout for each possible action in the environment. The first action in the $\mathrm{i}^{\text {th }}$ rollout is the $\mathrm{i}^{\text {th }}$ action of the action set $\mathcal{A}$, and subsequent actions for all rollouts are produced by a shared rollout policy $\hat{\pi}$. We investigated several types of rollout policies (random, pretrained) and found that a particularly efficient strategy was to distill the imagination-augmented policy into a model-free policy. This distillation strategy consists in creating a small model-free network $\hat{\pi}\left(o_{t}\right)$, and adding to the total loss a cross entropy auxiliary loss between the imagination-augmented policy $\pi\left(o_{t}\right)$ as computed on the current observation, and the policy $\hat{\pi}\left(o_{t}\right)$ as computed on the same observation. By imitating the imagination-augmented policy, the internal rollouts will be similar to the trajectories of the agent in the real environment; this also ensures that the rollout corresponds to trajectories with high reward. At the same time, the imperfect approximation results in a rollout policy with higher entropy, potentially striking a balance between exploration and exploitation.</p>
<h3>3.2 I2A components and environment models</h3>
<p>In our experiments, the encoder is an LSTM with convolutional encoder which sequentially processes a trajectory $\mathcal{T}$. The features $\hat{f}<em t_tau="t+\tau">{t}$ are fed to the LSTM in reverse order, from $\hat{f}</em>}$ to $\hat{f<em _model="{model" _text="\text">{t+1}$, to mimic Bellman type backup operations. ${ }^{2}$ The aggregator simply concatenates the summaries. For the model-free path of the I2A, we chose a standard network of convolutional layers plus one fully connected one [e.g. 3]. We also use this architecture on its own as a baseline agent.
Our environment model (Fig. 2) defines a distribution which is optimized by using a negative loglikelihood loss $l</em>$ to the total loss as an auxiliary loss. In practice we found that pre-training the environment model led to faster runtime of the I2A architecture, so we adopted this strategy.}}$. We can either pretrain the environment model before embedding it (with frozen weights) within the I2A architecture, or jointly train it with the agent by adding $l_{\text {model }</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For all environments, training data for our environment model was generated from trajectories of a partially trained standard model-free agent (defined below). We use partially pre-trained agents because random agents see few rewards in some of our domains. However, this means we have to account for the budget (in terms of real environment steps) required to pretrain the data-generating agent, as well as to then generate the data. In the experiments, we address this concern in two ways: by explicitly accounting for the number of steps used in pretraining (for Sokoban), or by demonstrating how the same pretrained model can be reused for many tasks (for MiniPacman).</p>
<h1>3.3 Agent training and baseline agents</h1>
<p>Using a fixed pretrained environment model, we trained the remaining I2A parameters with asynchronous advantage actor-critic (A3C) [3]. We added an entropy regularizer on the policy $\pi$ to encourage exploration and the auxiliary loss to distill $\pi$ into the rollout policy $\hat{\pi}$ as explained above. We distributed asynchronous training over 32 to 64 workers; we used the RMSprop optimizer [23]. We report results after an initial round of hyperparameter exploration (details in Appendix A). Learning curves are averaged over the top three agents unless noted otherwise.</p>
<p>A separate hyperparameter search was carried out for each agent architecture in order to ensure optimal performance. In addition to the I2A, we ran the following baseline agents (see Appendix B for architecture details for all agents).</p>
<p>Standard model-free agent. For our main baseline agent, we chose a model-free standard architecture similar to [3], consisting of convolutional layers ( 2 for MiniPacman, and 3 for Sokoban) followed by a fully connected layer. The final layer, again fully connected, outputs the policy logits and the value function. For Sokoban, we also tested a 'large' standard architecture, where we double the number of all feature maps (for convolutional layers) and hidden units (for fully connected layers). The resulting architecture has a slightly larger number of parameters than I2A.</p>
<p>Copy-model agent. Aside from having an internal environment model, the I2A architecture is very different from the one of the standard agent. To verify that the information contained in the environment model rollouts contributed to an increase in performance, we implemented a baseline where we replaced the environment model in the I2A with a 'copy' model that simply returns the input observation. Lacking a model, this agent does not use imagination, but uses the same architecture, has the same number of learnable parameters (the environment model is kept constant in the I2A), and benefits from the same amount of computation (which in both cases increases linearly with the length of the rollouts). This model effectively corresponds to an architecture where policy logits and value are the final output of an LSTM network with skip connections.</p>
<h2>4 Sokoban experiments</h2>
<p>We now demonstrate the performance of I2A over baselines in a puzzle environment, Sokoban. We address the issue of dealing with imperfect models, highlighting the strengths of our approach over planning baselines. We also analyze the importance of the various components of the I2A.</p>
<p>Sokoban is a classic planning problem, where the agent has to push a number of boxes onto given target locations. Because boxes can only be pushed (as opposed to pulled), many moves are irreversible, and mistakes can render the puzzle unsolvable. A human player is thus forced to plan moves ahead of time. We expect that artificial agents will similarly benefit from internal simulation. Our implementation of Sokoban procedurally generates a new level each episode (see Appendix D. 4 for details, Fig. 3 for examples). This means an agent cannot memorize specific puzzles. ${ }^{3}$ Together with the planning aspect, this makes for a very challenging environment for our model-free baseline agents, which solve less than $60 \%$ of the levels after a billion steps of training (details below). We provide videos of agents playing our version of Sokoban online [24].</p>
<p>While the underlying game logic operates in a $10 \times 10$ grid world, our agents were trained directly on RGB sprite graphics as shown in Fig. 4 (image size $80 \times 80$ pixels). There are no aspects of I2As that make them specific to grid world games.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Random examples of procedurally generated Sokoban levels. The player (green sprite) needs to push all 4 boxes onto the red target squares to solve a level, while avoiding irreversible mistakes. Our agents receive sprite graphics (shown above) as observations.</p>
<h1>4.1 I2A performance vs. baselines on Sokoban</h1>
<p>Figure 4 (left) shows the learning curves of the I2A architecture and various baselines explained throughout this section. First, we compare I2A (with rollouts of length 5) against the standard model-free agent. I2A clearly outperforms the latter, reaching a performance of $85 \%$ of levels solved vs. a maximum of under $60 \%$ for the baseline. The baseline with increased capacity reaches $70 \%$ still significantly below I2A. Similarly, for Sokoban, I2A far outperforms the copy-model.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Sokoban learning curves. Left: training curves of I2A and baselines. Note that I2A use additional environment observations to pretrain the environment model, see main text for discussion. Right: I2A training curves for various values of imagination depth.</p>
<p>Since using imagined rollouts is helpful for this task, we investigate how the length of individual rollouts affects performance. The latter was one of the hyperparameters we searched over. A breakdown by number of unrolling/imagination steps in Fig. 4 (right) shows that using longer rollouts, while not increasing the number of parameters, increases performance: 3 unrolling steps improves speed of learning and top performance significantly over 1 unrolling step, 5 outperforms 3 , and as a test for significantly longer rollouts, 15 outperforms 5 , reaching above $90 \%$ of levels solved. However, in general we found diminishing returns with using I2A with longer rollouts. It is noteworthy that 5 steps is relatively small compared to the number of steps taken to solve a level, for which our best agents need about 50 steps on average. This implies that even such short rollouts can be highly informative. For example, they allow the agent to learn about moves it cannot recover from (such as pushing boxes against walls, in certain contexts). Because I2A with rollouts of length 15 are significantly slower, in the rest of this section, we choose rollouts of length 5 to be our canonical I2A architecture.</p>
<p>It terms of data efficiency, it should be noted that the environment model in the I2A was pretrained (see Section 3.2). We conservatively measured the total number of frames needed for pretraining to be lower than 1e8. Thus, even taking pretraining into account, I2A outperforms the baselines after seeing about 3e8 frames in total (compare again Fig. 4 (left)). Of course, data efficiency is even better if the environment model can be reused to solve multiple tasks in the same environment (Section 5).</p>
<h3>4.2 Learning with imperfect models</h3>
<p>One of the key strengths of I2As is being able to handle learned and thus potentially imperfect environment models. However, for the Sokoban task, our learned environment models actually perform quite well when rolling out imagined trajectories. To demonstrate that I2As can deal with less reliable predictions, we ran another experiment where the I2A used an environment model that had shown much worse performance (due to a smaller number of parameters), with strong artifacts accumulating over iterated rollout predictions (Fig. 5, left). As Fig. 5 (right) shows, even with such a</p>
<p>clearly flawed environment model, I2A performs similarly well. This implies that I2As can learn to ignore the latter parts of the rollout as errors accumulate, but still use initial predictions when errors are less severe. Finally, note that in our experiments, surprisingly, the I2A agent with poor model ended outperforming the I2A agent with good model. We posit this was due to random initialization, though we cannot exclude the noisy model providing some form of regularization - more work will be required to investigate this effect.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Experiments with a noisy environment model. Left: each row shows an example 5-step rollout after conditioning on an environment observation. Errors accumulate and lead to various artefacts, including missing or duplicate sprites. Right: comparison of Monte-Carlo (MC) search and I2A when using either the accurate or the noisy model for rollouts.</p>
<p>Learning a rollout encoder is what enables I2As to deal with imperfect model predictions. We can further demonstrate this point by comparing them to a setup without a rollout encoder: as in the classic Monte-Carlo search algorithm of Tesauro and Galperin [25], we now explicitly estimate the value of each action from rollouts, rather than learning an arbitrary encoding of the rollouts, as in I2A. We then select actions according to those values. Specifically, we learn a value function $V$ from states, and, using a rollout policy $\tilde{\pi}$, sample a trajectory rollout for each initial action, and compute the corresponding estimated Monte Carlo return $\sum_{t \leq T} \gamma^{t} r_{t}^{a}+V\left(x_{T}^{a}\right)$ where $\left(\left(x_{t}^{a}, r_{t}^{a}\right)\right)<em T="T" class="" t="0">{t=0 . . T}$ comes from a trajectory initialized with action a. Action $a$ is chosen with probability proportional to $\exp \left(-\left(\sum</em>$
We ran this rollout encoder-free agent on Sokoban with both the accurate and the noisy environment model. We chose the length of the rollout to be optimal for each environment model (from the same range as for I2A, i.e. from 1 to 5). As can be seen in Fig. 5 (right), ${ }^{5}$ when using the high accuracy environment model, the performance of the encoder-free agent is similar to that of the baseline standard agent. However, unlike I2A, its performance degrades catastrophically when using the poor model, showcasing the susceptibility to model misspecification.} \gamma^{t} r_{t}^{a}+V\left(x_{T}^{a}\right)\right) / \delta\right)$, where $\delta$ is a learned temperature. This can be thought of as a form of I2A with a fixed summarizer (which computes returns), no model-free path, and very simple policy head. In this architecture, only $V, \tilde{\pi}$ and $\delta$ are learned. ${ }^{4</p>
<h1>4.3 Further insights into the workings of the I2A architecture</h1>
<p>So far, we have studied the role of the rollout encoder. To show the importance of various other components of the I2A, we performed additional control experiments. Results are plotted in Fig. 4 (left) for comparison. First, I2A with the copy model (Section 3.3) performs far worse, demonstrating that the environment model is indeed crucial. Second, we trained an I2A where the environment model was predicting no rewards, only observations. This also performed worse. However, after much longer training ( 3 e 9 steps), these agents did recover performance close to that of the original I2A (see Appendix D.2), which was never the case for the baseline agent even with that many steps. Hence, reward prediction is helpful but not absolutely necessary in this task, and imagined observations alone are informative enough to obtain high performance on Sokoban. Note this is in contrast to many classical planning and model-based reinforcement learning methods, which often rely on reward prediction.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">I2A@87</th>
<th style="text-align: center;">$\sim 1400$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">I2A MC search @95</td>
<td style="text-align: center;">$\sim 4000$</td>
</tr>
<tr>
<td style="text-align: center;">MCTS@87</td>
<td style="text-align: center;">$\sim 25000$</td>
</tr>
<tr>
<td style="text-align: center;">MCTS@95</td>
<td style="text-align: center;">$\sim 100000$</td>
</tr>
<tr>
<td style="text-align: center;">Random search</td>
<td style="text-align: center;">$\sim$ millions</td>
</tr>
</tbody>
</table>
<p>Table 1: Imagination efficiency of various architectures.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Boxes</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">I2A (\%)</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: center;">Standard (\%)</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">23</td>
</tr>
</tbody>
</table>
<p>Table 2: Generalization of I2A to environments with different number of boxes.</p>
<p>In previous sections, we illustrated that I2As can be used to efficiently solve planning problems and can be robust in the face of model misspecification. Here, we ask a different question - if we do assume a nearly perfect model, how does I2A compare to competitive planning methods? Beyond raw performance we focus particularly on the efficiency of planning, i.e. the number of imagination steps required to solve a fixed ratio of levels. We compare our regular I2A agent to a variant of Monte Carlo Tree Search (MCTS), which is a modern guided tree search algorithm [12, 26]. For our MCTS implementation, we aimed to have a strong baseline by using recent ideas: we include transposition tables [27], and evaluate the returns of leaf nodes by using a value network (in this case, a deep residual value network trained with the same total amount of data as I2A; see appendix D. 3 for further details).</p>
<p>Running MCTS on Sokoban, we find that it can achieve high performance, but at a cost of a much higher number of necessary environment model simulation steps: MCTS reaches the I2A performance of $87 \%$ of levels solved when using 25 k model simulation steps on average to solve a level, compared to 1.4 k environment model calls for I2A. Using even more simulation steps, MCTS performance increases further, e.g. reaching $95 \%$ with 100k steps.
If we assume access to a high-accuracy environment model (including the reward prediction), we can also push I2A performance further, by performing basic Monte-Carlo search with a trained I2A for the rollout policy: we let the agent play whole episodes in simulation (where I2A itself uses the environment model for short-term rollouts, hence corresponding to using a model-within-a-model), and execute a successful action sequence if found, up to a maximum number of retries; this is reminiscent of nested rollouts [28]. With a fixed maximum of 10 retries, we obtain a score of $95 \%$ (up from $87 \%$ for the I2A itself). The total average number of model simulation steps needed to solve a level, including running the model in the outer loop, is now 4 k , again much lower than the corresponding MCTS run with 100k steps. Note again, this approach requires a nearly perfect model; we don't expect I2A with MC search to perform well with approximate models. See Table 1 for a summary of the imagination efficiency for the different methods.</p>
<h1>4.5 Generalization experiments</h1>
<p>Lastly, we probe the generalization capabilities of I2As, beyond handling random level layouts in Sokoban. Our agents were trained on levels with 4 boxes. Table 2 shows the performance of I2A when such an agent was tested on levels with different numbers of boxes, and that of the standard model-free agent for comparison. We found that I2As generalizes well; at 7 boxes, the I2A agent is still able to solve more than half of the levels, nearly as many as the standard agent on 4 boxes.</p>
<h2>5 Learning one model for many tasks in MiniPacman</h2>
<p>In our final set of experiments, we demonstrate how a single model, which provides the I2A with a general understanding of the dynamics governing an environment, can be used to solve a collection of different tasks. We designed a simple, light-weight domain called MiniPacman, which allows us to easily define multiple tasks in an environment with shared state transitions and which enables us to do rapid experimentation.
In MiniPacman (Fig. 6, left), the player explores a maze that contains food while being chased by ghosts. The maze also contains power pills; when eaten, for a fixed number of steps, the player moves faster, and the ghosts run away and can be eaten. These dynamics are common to all tasks. Each task</p>
<p>is defined by a vector $w_{\text {rew }} \in \mathbb{R}^{5}$, associating a reward to each of the following five events: moving, eating food, eating a power pill, eating a ghost, and being eaten by a ghost. We consider five different reward vectors inducing five different tasks. Empirically we found that the reward schemes were sufficiently different to lead to very different high-performing policies [6] (for more details on the game and tasks, see appendix C.</p>
<p>To illustrate the benefits of model-based methods in this multi-task setting, we train a single environment model to predict both observations (frames) and events (as defined above, e.g. "eating a ghost"). Note that the environment model is effectively shared across all tasks, so that the marginal cost of learning the model is nil. During training and testing, the I2As have access to the frame and reward predictions generated by the model; the latter was computed from model event predictions and the task reward vector $w_{\text {rew }}$. As such, the reward vector $w_{\text {rew }}$ can be interpreted as an 'instruction' about which task to solve in the same environment [cf. the Frostbite challenge of 11]. For a fair comparison, we also provide all baseline agents with the event variable as input. ${ }^{7}$
We trained baseline agents and I2As separately on each task. Results in Fig. 6 (right) indicate the benefit of the I2A architecture, outperforming the standard agent in all tasks, and the copy-model baseline in all but one task. Moreover, we found that the performance gap between I2As and baselines is particularly high for tasks $4 \&amp; 5$, where rewards are particularly sparse, and where the anticipation of ghost dynamics is especially important. We posit that the I2A agent can leverage its environment and reward model to explore the environment much more effectively.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Minipacman environment. Left: Two frames from a minipacman game. Frames are $15 \times 19$ RGB images. The player is green, dangerous ghosts red, food dark blue, empty corridors black, power pills in cyan. After eating a power pill (right frame), the player can eat the 4 weak ghosts (yellow). Right: Performance after 300 million environment steps for different agents and all tasks. Note I2A clearly outperforms the other two agents on all tasks with sparse rewards.</p>
<h1>6 Related work</h1>
<p>Some recent work has focused on applying deep learning to model-based RL. A common approach is to learn a neural model of the environment, including from raw observations, and use it in classical planning algorithms such as trajectory optimization [29-31]. These studies however do not address a possible mismatch between the learned model and the true environment.
Model imperfection has attracted particular attention in robotics, when transferring policies from simulation to real environments [32-34]. There, the environment model is given, not learned, and used for pretraining, not planning at test time. Liu et al. [35] also learn to extract information from trajectories, but in the context of imitation learning. Bansal et al. [36] take a Bayesian approach to model imperfection, by selecting environment models on the basis of their actual control performance.
The problem of making use of imperfect models was also approached in simplified environment in Talvitie [18, 19] by using techniques similar to scheduled sampling [37]; however these techniques break down in stochastic environments; they mostly address the compounding error issue but do not address fundamental model imperfections.
A principled way to deal with imperfect models is to capture model uncertainty, e.g. by using Gaussian Process models of the environment, see Deisenroth and Rasmussen [15]. The disadvantage of this method is its high computational cost; it also assumes that the model uncertainty is well calibrated and lacks a mechanism that can learn to compensate for possible miscalibration of uncertainty. Cutler et al. [38] consider RL with a hierarchy of models of increasing (known) fidelity. A recent multi-task</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>GP extension of this study can further help to mitigate the impact of model misspecification, but again suffers from high computational burden in large domains, see Marco et al. [39].
A number of approaches use models to create additional synthetic training data, starting from Dyna [40], to more recent work e.g. Gu et al. [41] and Venkatraman et al. [42]; these models increase data efficiency, but are not used by the agent at test time.
Tamar et al. [43], Silver et al. [44], and Oh et al. [45] all present neural networks whose architectures mimic classical iterative planning algorithms, and which are trained by reinforcement learning or to predict user-defined, high-level features; in these, there is no explicit environment model. In our case, we use explicit environment models that are trained to predict low-level observations, which allows us to exploit additional unsupervised learning signals for training. This procedure is expected to be beneficial in environments with sparse rewards, where unsupervised modelling losses can complement return maximization as learning target as recently explored in Jaderberg et al. [46] and Mirowski et al. [47].
Internal models can also be used to improve the credit assignment problem in reinforcement learning: Henaff et al. [48] learn models of discrete actions environments, and exploit the effective differentiability of the model with respect to the actions by applying continuous control planning algorithms to derive a plan; Schmidhuber [49] uses an environment model to turn environment cost minimization into a network activity minimization.
Kansky et al. [50] learn symbolic networks models of the environment and use them for planning, but are given the relevant abstractions from a hand-crafted vision system.
Close to our work is a study by Hamrick et al. [51]: they present a neural architecture that queries learned expert models, but focus on meta-control for continuous contextual bandit problems. Pascanu et al. [52] extend this work by focusing on explicit planning in sequential environments, and learn how to construct a plan iteratively.
The general idea of learning to leverage an internal model in arbitrary ways was also discussed by Schmidhuber [53].</p>
<h1>7 Discussion</h1>
<p>We presented I2A, an approach combining model-free and model-based ideas to implement imagination-augmented RL: learning to interpret environment models to augment model-free decisions. I2A outperforms model-free baselines on MiniPacman and on the challenging, combinatorial domain of Sokoban. We demonstrated that, unlike classical model-based RL and planning methods, I2A is able to successfully use imperfect models (including models without reward predictions), hence significantly broadening the applicability of model-based RL concepts and ideas.
As all model-based RL methods, I2As trade-off environment interactions for computation by pondering before acting. This is essential in irreversible domains, where actions can have catastrophic outcomes, such as in Sokoban. In our experiments, the I2A was always less than an order of magnitude slower per interaction than the model-free baselines. The amount of computation can be varied (it grows linearly with the number and depth of rollouts); we therefore expect I2As to greatly benefit from advances on dynamic compute resource allocation (e.g. Graves [54]). Another avenue for future research is on abstract environment models: learning predictive models at the "right" level of complexity and that can be evaluated efficiently at test time will help to scale I2As to richer domains.
Remarkably, on Sokoban I2As compare favourably to a strong planning baseline (MCTS) with a perfect environment model: at comparable performance, I2As require far fewer function calls to the model than MCTS, because their model rollouts are guided towards relevant parts of the state space by a learned rollout policy. This points to further potential improvement by training rollout policies that "learn to query" imperfect models in a task-relevant way.</p>
<h2>Acknowledgements</h2>
<p>We thank Victor Valdes for designing and implementing the Sokoban environment, Joseph Modayil for reviewing an early version of this paper, and Ali Eslami, Hado Van Hasselt, Neil Rabinowitz, Tom Schaul, Yori Zwols for various help and feedback.</p>
<h1>References</h1>
<p>[1] Shane Legg and Marcus Hutter. Universal intelligence: A definition of machine intelligence. Minds and Machines, 17(4):391-444, 2007.
[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
[3] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928-1937, 2016.
[4] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889-1897, 2015.
[5] Demis Hassabis, Dharshan Kumaran, and Eleanor A Maguire. Using imagination to understand the neural basis of episodic memory. Journal of Neuroscience, 27(52):14365-14374, 2007.
[6] Daniel L Schacter, Donna Rose Addis, Demis Hassabis, Victoria C Martin, R Nathan Spreng, and Karl K Szpunar. The future of memory: remembering, imagining, and the brain. Neuron, 76(4):677-694, 2012.
[7] Demis Hassabis, Dharshan Kumaran, Seralynne D Vann, and Eleanor A Maguire. Patients with hippocampal amnesia cannot imagine new experiences. Proceedings of the National Academy of Sciences, 104(5): $1726-1731,2007$.
[8] Edward C Tolman. Cognitive maps in rats and men. Psychological Review, 55(4):189, 1948.
[9] Anthony Dickinson and Bernard Balleine. The Role of Learning in the Operation of Motivational Systems. John Wiley \&amp; Sons, Inc., 2002.
[10] Brad E Pfeiffer and David J Foster. Hippocampal place-cell sequences depict future paths to remembered goals. Nature, 497(7447):74-79, 2013.
[11] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. arXiv preprint arXiv:1604.00289, 2016.
[12] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
[13] Jing Peng and Ronald J Williams. Efficient learning and planning within the dyna framework. Adaptive Behavior, 1(4):437-454, 1993.
[14] Pieter Abbeel and Andrew Y Ng. Exploration and apprenticeship learning in reinforcement learning. In Proceedings of the 22nd international conference on Machine learning, pages 1-8. ACM, 2005.
[15] Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465-472, 2011.
[16] Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems, pages 1071-1079, 2014.
[17] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. ICLR, 2016.
[18] Erik Talvitie. Model regularization for stable sample rollouts. In UAI, pages 780-789, 2014.
[19] Erik Talvitie. Agnostic system identification for monte carlo planning. In AAAI, pages 2986-2992, 2015.
[20] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pages 2863-2871, 2015.
[21] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. In 5th International Conference on Learning Representations, 2017.</p>
<p>[22] Felix Leibfried, Nate Kushman, and Katja Hofmann. A deep learning approach for joint video frame and reward prediction in atari games. CoRR, abs/1611.07078, 2016. URL http://arxiv.org/abs/1611. 07078 .
[23] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012.
[24] https://drive.google.com/open?id=0B4tKsKnCCZtQY2tTOThucHVsUTQ, 2017.
[25] Gerald Tesauro and Gregory R Galperin. On-line policy improvement using monte-carlo search. In NIPS, volume 96, pages 1068-1074, 1996.
[26] Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International Conference on Computers and Games, pages 72-83. Springer, 2006.
[27] Benjamin E Childs, James H Brodeur, and Levente Kocsis. Transpositions and move groups in monte carlo tree search. In Computational Intelligence and Games, 2008. CIG'08. IEEE Symposium On, pages 389-395. IEEE, 2008.
[28] Christopher D Rosin. Nested rollout policy adaptation for monte carlo tree search. In Ijcai, pages 649-654, 2011.
[29] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in Neural Information Processing Systems, pages 2746-2754, 2015.
[30] Ian Lenz, Ross A Knepper, and Ashutosh Saxena. DeepMPC: Learning deep latent features for model predictive control. In Robotics: Science and Systems, 2015.
[31] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In IEEE International Conference on Robotics and Automation (ICRA), 2017.
[32] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.
[33] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Sergey Levine, Kate Saenko, and Trevor Darrell. Towards adapting deep visuomotor representations from simulated to real environments. arXiv preprint arXiv:1511.07111, 2015.
[34] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep inverse dynamics model. arXiv preprint arXiv:1610.03518, 2016.
[35] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. arXiv preprint arXiv:1707.03374, 2017.
[36] Somil Bansal, Roberto Calandra, Ted Xiao, Sergey Levine, and Claire J Tomlin. Goal-driven dynamics learning via bayesian optimization. arXiv preprint arXiv:1703.09260, 2017.
[37] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pages $1171-1179,2015$.
[38] Mark Cutler, Thomas J Walsh, and Jonathan P How. Real-world reinforcement learning via multifidelity simulators. IEEE Transactions on Robotics, 31(3):655-671, 2015.
[39] Alonso Marco, Felix Berkenkamp, Philipp Hennig, Angela P Schoellig, Andreas Krause, Stefan Schaal, and Sebastian Trimpe. Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with bayesian optimization. arXiv preprint arXiv:1703.01250, 2017.
[40] Richard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the seventh international conference on machine learning, pages $216-224,1990$.
[41] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pages 2829-2838, 2016.
[42] Arun Venkatraman, Roberto Capobianco, Lerrel Pinto, Martial Hebert, Daniele Nardi, and J Andrew Bagnell. Improved learning of dynamics models for control. In International Symposium on Experimental Robotics, pages 703-713. Springer, 2016.</p>
<p>[43] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances in Neural Information Processing Systems, pages 2154-2162, 2016.
[44] David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel DulacArnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-to-end learning and planning. arXiv preprint arXiv:1612.08810, 2016.
[45] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. arXiv preprint arXiv:1707.03497, 2017.
[46] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.
[47] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016.
[48] Mikael Henaff, William F Whitney, and Yann LeCun. Model-based planning in discrete action spaces. arXiv preprint arXiv:1705.07177, 2017.
[49] Jürgen Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In Neural Networks, 1990., 1990 IJCNN International Joint Conference on, pages 253-258. IEEE, 1990.
[50] Ken Kansky, Tom Silver, David A Mély, Mohamed Eldawy, Miguel Lázaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot transfer with a generative causal model of intuitive physics. Accepted at International Conference for Machine Learning, 2017, 2017.
[51] Jessica B. Hamrick, Andy J. Ballard, Razvan Pascanu, Oriol Vinyals, Nicolas Heess, and Peter W. Battaglia. Metacontrol for adaptive imagination-based optimization. In Proceedings of the 5th International Conference on Learning Representations (ICLR 2017), 2017.
[52] Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, David Reichert, Theophane Weber, Sebastien Racaniere, Lars Buesing, Daan Wierstra, and Peter Battaglia. Learning model-based planning from scratch. arXiv preprint, 2017.
[53] Jürgen Schmidhuber. On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. arXiv preprint arXiv:1511.09249, 2015.
[54] Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.
[55] Leemon C Baird III. Advantage updating. Technical report, Wright Lab. Technical Report WL-TR-93-1146., 1993.
[56] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. In Advances in Neural Information Processing Systems, pages 3528-3536, 2015.
[57] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on machine learning, pages 282-293. Springer, 2006.
[58] Sylvain Gelly and David Silver. Combining online and offline knowledge in uct. In Proceedings of the 24th international conference on Machine learning, pages 273-280. ACM, 2007.
[59] Joshua Taylor and Ian Parberry. Procedural generation of sokoban levels. In Proceedings of the International North American Conference on Intelligent Games and Simulation, pages 5-12, 2011.
[60] Yoshio Murase, Hitoshi Matsubara, and Yuzuru Hiraga. Automatic making of sokoban problems. PRICAI'96: Topics in Artificial Intelligence, pages 592-600, 1996.</p>
<h1>Supplementary material for: Imagination-Augmented Agents for Deep Reinforcement Learning</h1>
<h2>A Training and rollout policy distillation details</h2>
<p>Each agent used in the paper defines a stochastic policy, i.e. a categorical distribution $\pi\left(a_{t} \mid o_{t} ; \theta\right)$ over discrete actions $a$. The logits of $\pi\left(a_{t} \mid o_{t} ; \theta\right)$ are computed by a neural network with parameters $\theta$, taking observation $o_{t}$ at timestep $t$ as input. During training, to increase the probability of rewarding actions being taken, A3C applies an update $\Delta \theta$ to the parameters $\theta$ using policy gradient $g(\theta)$ :</p>
<p>$$
g(\theta)=\nabla_{\theta} \log \pi\left(a_{t} \mid o_{t} ; \theta\right) A\left(o_{t}, a_{t}\right)
$$</p>
<p>where $A\left(o_{t}, a_{t}\right)$ is an estimate of the advantage function [55]. In practice, we learn a value function $V\left(o_{t} ; \theta_{v}\right)$ and use it to compute the advantage as the difference of the bootstrapped $k$-step return and and the current value estimate:</p>
<p>$$
A\left(o_{t}, a_{t}\right)=\left(\sum_{t \leq t^{\prime} \leq t+k} \gamma^{t^{\prime}-t} r_{t^{\prime}}\right)+\gamma^{k+1} V\left(o_{t+k+1} ; \theta_{v}\right)-V\left(o_{t} ; \theta_{v}\right)
$$</p>
<p>The value function $V\left(o_{t} ; \theta_{v}\right)$ is also computed as the output of a neural network with parameters $\theta_{v}$. The input to the value function network was chosen to be the second to last layer of the policy network that computes $\pi$. The parameter $\theta_{v}$ are updated with $\Delta \theta_{v}$ towards bootstrapped $k$-step return:</p>
<p>$$
g\left(\theta_{v}\right)=-A\left(o_{t}, a_{t}\right) \partial_{\theta_{v}} V\left(o_{t} ; \theta_{v}\right)
$$</p>
<p>In our numerical implementation, we express the above updates as gradients of a corresponding surrogate loss [56]. To this surrogate loss, we add an entropy regularizer of $\lambda_{\text {ent }} \sum_{a_{t}} \pi\left(a_{t} \mid o_{t} ; \theta\right) \log \pi\left(a_{t} \mid o_{t} ; \theta\right)$ to encourage exploration, with $\lambda_{\text {ent }}=10^{-2}$ thoughout all experiments. Where applicable, we add a loss for policy distillation consisting of the cross-entropy between $\pi$ and $\hat{\pi}$ :</p>
<p>$$
l_{\text {dist }}(\pi, \hat{\pi})\left(o_{t}\right)=\lambda_{\text {dist }} \sum_{a} \pi\left(a \mid o_{t}\right) \log \hat{\pi}\left(a \mid o_{t}\right)
$$</p>
<p>with scaling parameter $\lambda_{\text {dist }}$. Here $\hat{\pi}$ denotes that we do not backpropagate gradients of $l_{\text {dist }}$ wrt. to the parameters of the rollout policy through the behavioral policy $\pi$. Finally, even though we pre-trained our environment models, in principle we can also learn it jointly with the I2A agent by a adding an appropriate log-likelihood term of observations under the model. We will investigate this in future research. We optimize hyperparameters (learning rate and momentum of the RMSprop optimizer, gradient clipping parameter, distillation loss scaling $\lambda_{\text {dist }}$ where applicable) separately for each agent (I2A and baselines).</p>
<h2>B Agent and model architecture details</h2>
<p>We used rectified linear units (ReLUs) between all hidden layers of all our agents. For the environment models, we used leaky ReLUs with a slope of 0.01 .</p>
<h2>B. 1 Agents</h2>
<h2>Standard model-free baseline agent</h2>
<p>The standard model-free baseline agent, taken from [3], is a multi-layer convolutional neural network (CNN), taking the current observation $o_{t}$ as input, followed by a fully connected (FC) hidden layer.</p>
<p>This FC layer feeds into two heads: into a FC layer with one output per action computing the policy logits $\log \pi\left(a_{t} \mid o_{t}, \theta\right)$; and into another FC layer with a single output that computes the value function $V\left(o_{t} ; \theta_{v}\right)$. The sizes of the layers were chosen as follows:</p>
<ul>
<li>for MiniPacman: the CNN has two layers, both with 3x3 kernels, 16 output channels and strides 1 and 2 ; the following FC layer has 256 units</li>
<li>for Sokoban: the CNN has three layers with kernel sizes $8 \times 8,4 \times 4,3 \times 3$, strides of $4,2,1$ and number of output channels $32,64,64$; the following FC has 512 units</li>
</ul>
<h1>I2A</h1>
<p>The model free path of the I2A consists of a CNN identical to one of the standard model-free baseline (without the FC layers). The rollout encoder processes each frame generated by the environment model with another identically sized CNN. The output of this CNN is then concatenated with the reward prediction (single scalar broadcast into frame shape). This feature is the input to an LSTM with 512 (for Sokoban) or 256 (for MiniPacman) units. The same LSTM is used to process all 5 rollouts (one per action); the last output of the LSTM for all rollouts are concatenated into a single vector $c_{\text {in }}$ of length 2560 for Sokoban, and 1280 on MiniPacman. This vector is concatenated with the output $c_{\text {inf }}$ of the model-free CNN path and is fed into the fully connected layers computing policy logits and value function as in the baseline agent described above.</p>
<h2>Copy-model</h2>
<p>The copy-model agent has the exact same architecture as the I2A, with the exception of the environment model being replaced by the identity function (constantly returns the input observation).</p>
<h2>B. 2 Environment models</h2>
<p>For the I2A, we pre-train separate auto-regressive models of order 1 for the raw pixel observations of the MiniPacman and Sokoban environments (see figures 7 and 8) . In both cases, the input to the model consisted of the last observation $o_{t}$, and a broadcasted, one-hot representation of the last action $a_{t}$. Following previous studies, the outputs of the models were trained to predict the next frame $o_{t+1}$ by stochastic gradient decent on the Bernoulli cross-entropy between network outputs and data $o_{t+1}$.
The Sokoban model is a simplified case of the MiniPacman model; the Sokoban model is nearly entirely local (save for the reward model), while the MiniPacman model needs to deal with nonlocal interaction (movement of ghosts is affected by position of Pacman, which can be arbitrarily far from the ghosts).</p>
<h2>MiniPacman model</h2>
<p>The input and output frames were of size $15 \times 19 \times 3$ (width $x$ height $x$ RGB). The model is depicted in figure 7. It consisted of a size preserving, multi-scale CNN architecture with additional fully connected layers for reward prediction. In order to capture long-range dependencies across pixels, we also make use of a layer we call pool-and-inject, which applies global max-pooling over each feature map and broadcasts the resulting values as feature maps of the same size and concatenates the result to the input. Pool-and-inject layers are therefore size-preserving layers which communicate the max-value of each layer globally to the next convolutional layer.</p>
<h2>Sokoban model</h2>
<p>The Sokoban model was chosen to be a residual CNN with an additional CNN / fully-connected MLP pathway for predicting rewards. The input of size $80 \times 80 \times 3$ was first processed with convolutions with a large $8 \times 8$ kernel and stride of 8 . This reduced representation was further processed with two size preserving CNN layers before outputting a predicted frame by a $8 \times 8$ convolutional layer.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The minipacman environment model. The overview is given in the right panel with blowups of the basic convolutional building block (middle panel) and the pool-and-inject layer (left panel). The basic build block has three hyperparameters $n_{1}, n_{2}, n_{3}$ determining the number of channels in the convolutions; their numeric values are given in the right panel.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The sokoban environment model.</p>
<h1>C MiniPacman additional details</h1>
<p>MiniPacman is played in a $15 \times 19$ grid-world. Characters, the ghosts and Pacman, move through a maze. Walls positions are fixed. At the start of each level 2 power pills, a number of ghosts, and Pacman are placed at random in the world. Food is found on every square of the maze. The number of ghosts on level $k$ is $1+\frac{\text { level }-1}{2}$ rounded down, where level $=1$ on the first level.</p>
<h2>Game dynamics</h2>
<p>Ghosts always move by one square at each time step. Pacman usually moves by one square, except when it has eaten a power pill, which makes it move by two squares at a time. When moving by 2 squares, if Pacman new position ends up inside a wall, then it is moved back by one square to get back to a corridor.</p>
<p>We say that Pacman and a ghost meet when they either end up at the same location, or when their path crosses (even if they do not end up at the same location). When Pacman moves to a square with food or a power pill, it eats it. Eating a power pill gives Pacman super powers, such as moving at</p>
<p>double speed and being able to eat ghosts. The effects of eating a power pill last for 19 time steps. When Pacman meets a ghost, either Pacman dies eaten by the ghost, or, if Pacman has recently eaten a power pill, the ghost dies eaten by Pacman.</p>
<p>If Pacman has eaten a power pill, ghosts try to flee from Pacman. They otherwise try to chase Pacman. A more precise algorithm for the movement of a ghost is given below in pseudo code:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">ghost</span>
<span class="w">    </span><span class="k">function</span><span class="w"> </span><span class="n">MOVEGHOST</span>
<span class="w">        </span><span class="nl">Inputs</span><span class="p">:</span><span class="w"> </span><span class="n">Ghost</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Contains</span><span class="w"> </span><span class="k">position</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="ow">some</span><span class="w"> </span><span class="n">helper</span><span class="w"> </span><span class="n">methods</span>
<span class="w">        </span><span class="n">PossibleDirections</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="o">[</span><span class="n">DOWN, LEFT, RIGHT, UP</span><span class="o">]</span>
<span class="w">        </span><span class="n">CurrentDirection</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Ghost</span><span class="p">.</span><span class="n">current_direction</span>
<span class="w">        </span><span class="n">AllowedDirections</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="err">[]\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">dir</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">PossibleDirections</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">Ghost</span><span class="p">.</span><span class="n">can_move</span><span class="p">(</span><span class="n">dir</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="n">AllowedDirections</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">+=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="o">[</span><span class="n">dir</span><span class="o">]</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">AllowedDirections</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">==</span><span class="mi">2</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">straight</span><span class="w"> </span><span class="n">corridor</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bend</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">Ghost</span><span class="p">.</span><span class="n">current_direction</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">AllowedDirections</span><span class="w"> </span><span class="k">then</span>
<span class="w">                    </span><span class="k">return</span><span class="w"> </span><span class="n">Ghost</span><span class="p">.</span><span class="n">current_direction</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">opposite</span><span class="p">(</span><span class="n">Ghost</span><span class="p">.</span><span class="n">current_direction</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">AllowedDirections</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="k">then</span>
<span class="w">                    </span><span class="k">return</span><span class="w"> </span><span class="n">AllowedDirections</span><span class="o">[</span><span class="n">1</span><span class="o">]</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">AllowedDirections</span><span class="o">[</span><span class="n">0</span><span class="o">]</span>
<span class="w">            </span><span class="k">else</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">intersection</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">opposite</span><span class="p">(</span><span class="n">Ghost</span><span class="p">.</span><span class="n">current_direction</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">AllowedDirections</span><span class="w"> </span><span class="k">then</span>
<span class="w">                    </span><span class="n">AllowedDirections</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">opposite</span><span class="p">(</span><span class="n">Ghost</span><span class="p">.</span><span class="n">current_direction</span><span class="p">))</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Ghosts</span><span class="w"> </span><span class="n">do</span>
<span class="ow">not</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="n">around</span>
<span class="w">            </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">normalise</span><span class="p">(</span><span class="n">Pacman</span><span class="p">.</span><span class="k">position</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Ghost</span><span class="p">.</span><span class="k">position</span><span class="p">)</span>
<span class="w">            </span><span class="n">DotProducts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">dir</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">AllowedDirections</span><span class="w"> </span><span class="n">do</span>
<span class="w">                </span><span class="n">DotProducts</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">+=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="o">[</span><span class="n">dot_product(X, dir)</span><span class="o">]</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">Pacman</span><span class="p">.</span><span class="n">ate_super_pill</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">AllowedDirections</span><span class="o">[</span><span class="n">argmin(DotProducts)</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Away</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">Pacman</span>
<span class="w">            </span><span class="k">else</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">AllowedDirections</span><span class="o">[</span><span class="n">argmax(DotProducts)</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Towards</span><span class="w"> </span><span class="n">Pacman</span>
</code></pre></div>

<h1>Task collection</h1>
<p>We used 5 different tasks available in MiniPacman. They all share the same environment dynamics (layout of maze, movement of ghosts, ...), but vary in their reward structure and level termination. The rewards associated with various events for each tasks are given in the table below.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">At each step</th>
<th style="text-align: center;">Eating food</th>
<th style="text-align: center;">Eating power pill</th>
<th style="text-align: center;">Eating ghost</th>
<th style="text-align: center;">Killed by ghost</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Avoid</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">-0.1</td>
<td style="text-align: center;">-5</td>
<td style="text-align: center;">-10</td>
<td style="text-align: center;">-20</td>
</tr>
<tr>
<td style="text-align: center;">Hunt</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-20</td>
</tr>
<tr>
<td style="text-align: center;">Ambush</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-0.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-20</td>
</tr>
<tr>
<td style="text-align: center;">Rush</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-0.1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>When a level is cleared, a new level starts. Tasks also differ in the way a level was cleared.</p>
<ul>
<li>Regular: level is cleared when all the food is eaten;</li>
<li>Avoid: level is cleared after 128 steps;</li>
<li>Hunt: level is cleared when all ghosts are eaten or after 80 steps.</li>
<li>Ambush: level is cleared when all ghosts are eaten or after 80 steps.</li>
<li>Rush: level is cleared when all power pills are eaten.</li>
</ul>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: The pink bar appears when Pacman eats a power pill, and it decreases in size over the duration of the effect of the pill.</p>
<p>There are no lives, and episode ends when Pacman is eaten by a ghost.
The time left before the effect of the power pill wears off is shown using a pink shrinking bar at the bottom of the screen as in Fig. 9.</p>
<h1>Training curves</h1>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Learning curves for different agents and various tasks</p>
<h2>D Sokoban additional details</h2>
<h2>D. 1 Sokoban environment</h2>
<p>In the game of Sokoban, random actions on the levels would solve levels with vanishing probability, leading to extreme exploration issues for solving the problem with reinforcement learning. To alleviate this issue, we use a shaping reward scheme for our version of Sokoban:</p>
<ul>
<li>Every time step, a penalty of -0.1 is applied to the agent.</li>
<li>Whenever the agent pushes a box on target, it receives a reward of +1 .</li>
<li>Whenever the agent pushes a box off target, it receives a penalty of -1 .</li>
<li>Finishing the level gives the agent a reward of +10 and the level terminates.</li>
</ul>
<p>The first reward is to encourage agents to finish levels faster, the second to encourage agents to push boxes onto targets, the third to avoid artificial reward loop that would be induced by repeatedly pushing a box off and on target, the fourth to strongly reward solving a level. Levels are interrupted after 120 steps (i.e. agent may bootstrap from a value estimate of the last frame, but the level resets to a new one). Identical levels are nearly never encountered during training or testing (out of 40 million levels generated, less than $0.7 \%$ were repeated). Note that with this reward scheme, it is always optimal to solve the level (thus our shaping scheme is valid). An alternative strategy would have been to have the agent play through a curriculum of increasingly difficult tasks; we expect both strategies to work similarly.</p>
<h1>D. 2 Additional experiments</h1>
<p>Our first additional experiment compared I2A with and without reward prediction, trained over a longer horizon. I2A with reward prediction clearly converged shortly after 1 e 9 steps and we therefore interrupted training; however, I2A without reward prediction kept increasing performance, and after 3 e 9 steps, we recover a performance level of close to $80 \%$ of levels solved, see Fig. 11.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: I2A with and without reward prediction, longer training horizon.</p>
<p>Next, we investigated the I2A with Monte-Carlo search (using a near perfect environment model of Sokoban). We let the agent try to solve the levels up to 16 times within its internal model. The base I2A architecture was solving around $87 \%$ of levels; mental retries boosted its performance to around $95 \%$ of levels solved. Although the agent was allowed up to 16 mental retries, in practice all the performance increase was obtained within the first 10 mental retries. Exact percentage gain by each mental retry is shown in Fig. 12. Note in Fig. 12, only $83 \%$ of the levels are solved on the first mental attempt, even though the I2A architecture could solve around $87 \%$ of levels. The gap is explained by the use of an environment model: although it looks nearly perfect to the naked eye, the model is not actually equivalent to the environment.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Gain in percentage by each additional mental retry using a near perfect environment model.</p>
<h1>D. 3 Planning with the perfect model and Monte-Carlo Tree Search in Sokoban</h1>
<p>We first trained a value network that estimates the value function of a trained model-free policy; to do this, we trained a model-free agent for 1 e 9 environment steps. This agent solved close to $60 \%$ of episodes. Using this agent, we generated 1e8 (frame, return) pairs, and trained the value network to predict the value (expected return) from the frame; training and test error were comparable, and we don't expect increasing the number of training points would have significantly improved the quality of the the value network.</p>
<p>The value network architecture is a residual network which stacks one convolution layer and 3 convolution blocks with a final fully-connected layer of 128 hidden units. The first convolution is $1 \times 1$ convolution with 128 feature maps. Each of the three residual convolution block is composed of two convolutional layers; the first is a $1 \times 1$ convolution with 32 feature maps, the second a $3 \times 3$ convolution with 32 feature maps, and the last a $1 \times 1$ layer with 128 feature maps. To help the value networks, we trained them not on the pixel representation, but on a $10 \times 10 \times 4$ symbolic representation.</p>
<p>The trained value network is then employed during search to evaluate leaf-nodes - similar to [12], replacing the role of traditional random rollouts in MCTS. The tree policy uses [57, 58] with a fine-tuned exploration constant of 1 . Depth-wise transposition tables for the tree nodes are used to deal with the symmetries in the Sokoban environment. External actions are selected by taking the max Q value at the root node. The tree is reused between steps but selecting the appropriate subtree as the root node for the next step.</p>
<p>Reported results are obtained by averaging the results over 250 episodes.</p>
<h2>D. 4 Level Generation for Sokoban</h2>
<p>We detail here our procedural generation for Sokoban levels - we follow closely methods described in $[59,60]$.</p>
<p>The generation of a Sokoban level involves three steps: room topology generation, position configuration and room reverse-playing. Topology generation: Given an initial width*height room entirely constituted by wall blocks, the topology generation consists in creating the 'empty' spaces (i.e. corridors) where boxes, targets and the player can be placed. For this simple random walk algorithm with a configurable number of steps is applied: a random initial position and direction are chosen. Afterwards, for every step, the position is updated and, with a probability $p=0.35$, a new random direction is selected. Every 'visited' position is emptied together with a number of surrounding wall blocks, selected by randomly choosing one of the following patterns indicating the adjacent room blocks to be removed (the darker square represents the reference position, that is, the position being visited). Note that the room 'exterior' walls are never emptied, so from a width $\times$ height room only a (width-2) $\times$ (height-2) space can actually be converted into corridors. The random walk approach guarantees that all the positions in the room are, in principle, reachable by the player. A relatively small probability of changing the walk direction favours the generation of longer corridors, while the application of a random pattern favours slightly more convoluted spaces. Position configuration:
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Once a room topology is generated, the target locations for the desired N boxes and the player initial position are randomly selected. There is the obvious prerequisite of having enough empty spaces in the room to place the targets and the player but no other constraints are imposed in this step.</p>
<p>Reverse playing: Once the topology and targets/player positions are generated the room is reverseplayed. In this case, on each step, the player has eight possible actions to choose from: simply moving or moving+pulling from a box in each possible direction (assuming for the latter, that there is a box adjacent to the player position).
Initially the room is configured with the boxes placed over their corresponding targets. From that position a depth-first search (with a configurable maximum depth) is carried out over the space of possible moves, by 'expanding' each reached player/boxes position by iteratively applying all the possible actions (which are randomly permuted on each step). An entire tree is not explored as there are different combinations of actions leading to repeated boxes/player configurations which are skipped.
Statistics are collected for each boxes/player configuration, which is, in turn, scored with a simple heuristic:</p>
<p>$$
\text { RoomScore }=\text { BoxSwaps } \times \sum_{i} \text { BoxDisplacement }_{i}
$$</p>
<p>where BoxSwaps represents the number of occasions in which the player stopped pulling from a given box and started pulling from a different one, while BoxDisplacement represents the Manhattan distance between the initial and final position of a given box. Also whenever a box or the player are placed on top of one of the targets the RoomScore value is set to 0 . While this scoring heuristic doesn't guarantee the complexity of the generated rooms it's aimed to a) favour room configurations where overall the boxes are further away from their original positions and b) increase the probability of a room requiring a more convoluted combination of box moves to get to a solution (by aiming for solutions with higher boxSwaps values). This scoring mechanism has empirically proved to generate levels with a balanced combination of difficulties.
The reverse playing ends when there are no more available positions to explore or when a predefined maximum number of possible room configurations is reached. The room with the higher RoomScore is then returned.</p>
<h1>Defaul parameters:</h1>
<ul>
<li>A maximum of 10 room topologies and for each of those 10 boxes/player positioning are retried in case a given combination doesn't produce rooms with a score $&gt;0$.</li>
<li>The room configuration tree is by default limited to a maximum depth of 300 applied actions.</li>
<li>The total number of visited positions is by default limited to 1000000 .</li>
<li>Default random-walk steps: $1.5 \times$ (room width + room height).</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ For example, in the 'avoid' game, any event is negatively rewarded, and the optimal strategy is for the agent to clear a small space from food and use it to continuously escape the ghosts.
${ }^{7}$ It is not necessary to provide the reward vector $w_{\text {rew }}$ to the baseline agents, as it is equivalent a constant bias.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
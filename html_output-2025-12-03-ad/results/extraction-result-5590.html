<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5590 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5590</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5590</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-57f2c0720ffaec896f612b6f3b13fd1c538b4749</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/57f2c0720ffaec896f612b6f3b13fd1c538b4749" target="_blank">Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Information and Knowledge Management</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an alternative approach called User-Guided Response Optimization (UGRO) to combine LLM as an annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models.</p>
                <p><strong>Paper Abstract:</strong> Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as an annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user's requirements. Through empirical experiments on two TOD benchmarks, we validate the effectiveness of our method. The results demonstrate that our approach outperforms previous state-of-the-art (SOTA) results.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5590.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5590.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (user simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT used as a user simulator for task-oriented dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses ChatGPT as a text-based user simulator that predicts turn-level user satisfaction scores (with an explanation) for task-oriented dialogue (TOD) responses; those scores are used as rewards to optimize a smaller, fine-tuned TOD model via PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A conversational large language model (LLM) from OpenAI used here as an annotation-free user simulator; prompted with task description, satisfaction criteria, examples (0–6), the dialogue to be assessed, and instructions including an 'explanation reason' slot to elicit a score and rationale. The paper does not provide internal architecture or training-data details for ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Natural Language Processing — Task-Oriented Dialogue / Dialogue Systems / User Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Turn-level user satisfaction prediction for system response candidates in task-oriented dialogues (predict a satisfaction score and provide an explanation/reason); scores used as rewards to optimize a TOD response generator via reinforcement learning (PPO).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Accuracy, Precision, Recall, F1 (classification metrics for satisfaction labels); also used qualitatively (explanation reason) and downstream automatic generation metrics (BLEU, ROUGE) to measure impact on response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>User satisfaction prediction results (from Table 1): MultiWoZ 2.1 — Zero-shot ChatGPT: Acc=61.1, P=21.6, R=19.1, F1=19.1; Few-shot ChatGPT (up to 6 examples): Acc=78.1, P=22.4, R=20.8, F1=21.1. SGD — Zero-shot ChatGPT: Acc=51.6, P=22.6, R=26.6, F1=19.4; Few-shot ChatGPT: Acc=72.6, P=26.6, R=23.1, F1=23.6. (For reference, supervised BERT on MultiWoZ 2.1: Acc=85.4, P=31.6, R=26.9, F1=27.1.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt design (number of examples / few-shot vs zero-shot), dataset label imbalance (dominance of middle rating), which input is provided to the simulator (paper uses only the system response rather than full user+system context for scoring), and availability of reasoning/explanation (Zero-shot Chain-of-Thought style instruction).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Empirical comparison in Table 1 shows marked improvement from zero-shot to few-shot (e.g., MultiWoZ Acc from 61.1 to 78.1). The paper explicitly notes severe label imbalance (rating 3 accounts for ~90% of samples), which they report as a challenge for accurate classification. The paper also describes using 0–6 examples in prompts and adding an 'explanation reason' slot (Zero-shot CoT idea) in the prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic comparison to ground-truth human-annotated turn-level satisfaction labels on two TOD datasets (MultiWoZ 2.1 and Schema-Guided Dialogue (SGD)); computed classification metrics (Accuracy, Precision, Recall, F1). The LLM output includes an explanation and a numeric score which is extracted via regular expression. Downstream effects evaluated via standard generation metrics (BLEU, ROUGE) after optimizing TOD model with LLM-provided rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance is only comparable (not superior) to fine-tuned supervised models; dataset label imbalance (rating 3 dominating) hinders classification granularity; the user simulator in experiments assesses only the system response (not full user utterance in some setups), which may limit fidelity; authors note the practical advantages but caution that current user-simulator performance is not yet qualified for all practical applications.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparisons shown versus supervised HiGRU+ATTN and BERT satisfaction estimators, and versus zero-shot BERT. ChatGPT (few-shot) approaches but does not exceed supervised BERT on metrics. The paper also compares downstream TOD response quality (BLEU/ROUGE) of their UGRO approach against multiple SOTA TOD models (HDNO, MTTOD, PPTOD, GALAXY, TOATOD), showing UGRO improves generation performance when using ChatGPT-based rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use few-shot prompting (providing up to 6 examples) and include an explanation/reason slot (Zero-shot Chain-of-Thought style) to improve LLM scoring; extract numeric scores robustly (e.g., via regular expressions); use LLM-based satisfaction scores as annotation-free rewards to fine-tune smaller TOD models (via PPO) rather than attempting to replace TOD models with LLMs; account for dataset label imbalance when interpreting classification metrics; the paper suggests this approach is easy to extend to new domains with minimal or zero examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Simulating user satisfaction for the evaluation of task-oriented dialogue systems <em>(Rating: 2)</em></li>
                <li>Controllable Dialogue Simulation with In-context Learning <em>(Rating: 2)</em></li>
                <li>Are LLMs All You Need for Task-Oriented Dialogue? <em>(Rating: 2)</em></li>
                <li>User Satisfaction Estimation with Sequential Dialogue Act Modeling in Goaloriented Conversational Systems <em>(Rating: 1)</em></li>
                <li>Controllable Dialogue Simulation with In-context Learning <em>(Rating: 2)</em></li>
                <li>Guiding Large Language Models via Directional Stimulus Prompting <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5590",
    "paper_id": "paper-57f2c0720ffaec896f612b6f3b13fd1c538b4749",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "ChatGPT (user simulator)",
            "name_full": "ChatGPT used as a user simulator for task-oriented dialogue",
            "brief_description": "This paper uses ChatGPT as a text-based user simulator that predicts turn-level user satisfaction scores (with an explanation) for task-oriented dialogue (TOD) responses; those scores are used as rewards to optimize a smaller, fine-tuned TOD model via PPO.",
            "citation_title": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "A conversational large language model (LLM) from OpenAI used here as an annotation-free user simulator; prompted with task description, satisfaction criteria, examples (0–6), the dialogue to be assessed, and instructions including an 'explanation reason' slot to elicit a score and rationale. The paper does not provide internal architecture or training-data details for ChatGPT.",
            "model_size": null,
            "scientific_subdomain": "Natural Language Processing — Task-Oriented Dialogue / Dialogue Systems / User Simulation",
            "simulation_task": "Turn-level user satisfaction prediction for system response candidates in task-oriented dialogues (predict a satisfaction score and provide an explanation/reason); scores used as rewards to optimize a TOD response generator via reinforcement learning (PPO).",
            "accuracy_metric": "Accuracy, Precision, Recall, F1 (classification metrics for satisfaction labels); also used qualitatively (explanation reason) and downstream automatic generation metrics (BLEU, ROUGE) to measure impact on response generation.",
            "reported_accuracy": "User satisfaction prediction results (from Table 1): MultiWoZ 2.1 — Zero-shot ChatGPT: Acc=61.1, P=21.6, R=19.1, F1=19.1; Few-shot ChatGPT (up to 6 examples): Acc=78.1, P=22.4, R=20.8, F1=21.1. SGD — Zero-shot ChatGPT: Acc=51.6, P=22.6, R=26.6, F1=19.4; Few-shot ChatGPT: Acc=72.6, P=26.6, R=23.1, F1=23.6. (For reference, supervised BERT on MultiWoZ 2.1: Acc=85.4, P=31.6, R=26.9, F1=27.1.)",
            "factors_affecting_accuracy": "Prompt design (number of examples / few-shot vs zero-shot), dataset label imbalance (dominance of middle rating), which input is provided to the simulator (paper uses only the system response rather than full user+system context for scoring), and availability of reasoning/explanation (Zero-shot Chain-of-Thought style instruction).",
            "evidence_for_factors": "Empirical comparison in Table 1 shows marked improvement from zero-shot to few-shot (e.g., MultiWoZ Acc from 61.1 to 78.1). The paper explicitly notes severe label imbalance (rating 3 accounts for ~90% of samples), which they report as a challenge for accurate classification. The paper also describes using 0–6 examples in prompts and adding an 'explanation reason' slot (Zero-shot CoT idea) in the prompt design.",
            "evaluation_method": "Automatic comparison to ground-truth human-annotated turn-level satisfaction labels on two TOD datasets (MultiWoZ 2.1 and Schema-Guided Dialogue (SGD)); computed classification metrics (Accuracy, Precision, Recall, F1). The LLM output includes an explanation and a numeric score which is extracted via regular expression. Downstream effects evaluated via standard generation metrics (BLEU, ROUGE) after optimizing TOD model with LLM-provided rewards.",
            "limitations_or_failure_cases": "Performance is only comparable (not superior) to fine-tuned supervised models; dataset label imbalance (rating 3 dominating) hinders classification granularity; the user simulator in experiments assesses only the system response (not full user utterance in some setups), which may limit fidelity; authors note the practical advantages but caution that current user-simulator performance is not yet qualified for all practical applications.",
            "comparisons": "Direct comparisons shown versus supervised HiGRU+ATTN and BERT satisfaction estimators, and versus zero-shot BERT. ChatGPT (few-shot) approaches but does not exceed supervised BERT on metrics. The paper also compares downstream TOD response quality (BLEU/ROUGE) of their UGRO approach against multiple SOTA TOD models (HDNO, MTTOD, PPTOD, GALAXY, TOATOD), showing UGRO improves generation performance when using ChatGPT-based rewards.",
            "recommendations_or_best_practices": "Use few-shot prompting (providing up to 6 examples) and include an explanation/reason slot (Zero-shot Chain-of-Thought style) to improve LLM scoring; extract numeric scores robustly (e.g., via regular expressions); use LLM-based satisfaction scores as annotation-free rewards to fine-tune smaller TOD models (via PPO) rather than attempting to replace TOD models with LLMs; account for dataset label imbalance when interpreting classification metrics; the paper suggests this approach is easy to extend to new domains with minimal or zero examples.",
            "uuid": "e5590.0",
            "source_info": {
                "paper_title": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Simulating user satisfaction for the evaluation of task-oriented dialogue systems",
            "rating": 2
        },
        {
            "paper_title": "Controllable Dialogue Simulation with In-context Learning",
            "rating": 2
        },
        {
            "paper_title": "Are LLMs All You Need for Task-Oriented Dialogue?",
            "rating": 2
        },
        {
            "paper_title": "User Satisfaction Estimation with Sequential Dialogue Act Modeling in Goaloriented Conversational Systems",
            "rating": 1
        },
        {
            "paper_title": "Controllable Dialogue Simulation with In-context Learning",
            "rating": 2
        },
        {
            "paper_title": "Guiding Large Language Models via Directional Stimulus Prompting",
            "rating": 1
        }
    ],
    "cost": 0.006955749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System</h1>
<p>Zhiyuan Hu<br>zhiyuan_hu@u.nus.edu<br>National University of Singapore</p>
<p>Yue Feng<br>yue.feng.20@ucl.ac.uk<br>University College London</p>
<p>Anh Tuan Luu<br>anhtuan.luu@ntu.edu.sg<br>Nanyang Technology University</p>
<p>Bryan Hooi<br>bhooi@comp.nus.edu.sg<br>National University of Singapore</p>
<h2>ABSTRACT</h2>
<p>Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as an annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user's requirements. Through empirical experiments on two TOD benchmarks, we validate the effectiveness of our method. The results demonstrate that our approach outperforms previous state-of-the-art (SOTA) results.</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Information systems $\rightarrow$ Users and interactive retrieval; Computing methodologies $\rightarrow$ Discourse, dialogue and pragmatics;</li>
<li>Human-centered computing $\rightarrow$ Human computer interaction $(\mathrm{HCI})$.</li>
</ul>
<h2>KEYWORDS</h2>
<p>Dialogue system, Large Language Model, User Simulation</p>
<h2>ACM Reference Format:</h2>
<p>Zhiyuan Hu, Yue Feng, Anh Tuan Luu, Bryan Hooi, and Aldo Lipani. 2023. Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System. In Proceedings of the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Aldo Lipani
aldo.lipani@ucl.ac.uk
University College London
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of using LLMs as a user simulator to predict user satisfaction scores in task-oriented dialogues.
32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3583780.3615220</p>
<h2>1 INTRODUCTION</h2>
<p>In recent years, there has been significant interest in dialogue systems and their potential to improve human-computer interaction. The development of LLMs has particularly contributed to the advancement of general dialogue systems. However, adapting LLMs for task-oriented dialogues (TODs) in specific domains, such as service reservation, remains challenging. LLMs struggle to handle the intricacies of TODs as they require domain knowledge, background information, and context to generate appropriate responses. Recent research $[9,15]$ has aimed to tackle this challenge using few-shot or zero-shot learning. However, even when adapting Alpaca-LoRA-7B or ChatGPT, the BLEU score and Success metric for TOD tasks remain significantly low compared with fine-tuned end-to-end models. An alternative option for fine-tuning LLMs is resource-intensive. Moreover, whether fine-tuning will result in significant improvements is not always guaranteed [1] and can vary depending on several factors, including the similarity between pretraining tasks and fine-tuning tasks, the model size, and the duration of the fine-tuning process. ${ }^{12}$</p>
<p>How should LLMs be integrated into TOD systems? Work in psychology has explored the dichotomy between two cognitive processes, System 1 (fast and intuitive) and System 2 (slow and deliberative), which guides or corrects System 1 [10], Our model</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>design follows a similar separation of roles: we leverage the reasoning and knowledge of an LLM by using it as a source of feedback to guide a task-specific and thus 'intuitive' model. Toward this goal, our approach is to utilize LLMs as a user simulator. User simulation has emerged as a line of work which involves mimicking user behaviour, including user query generation, system response satisfaction prediction, and user action prediction [6, 11, 23, 29]. LLMs possess significant strengths, such as their remarkable understanding, extensive knowledge, and reasoning capabilities. Therefore, it is natural to utilize LLMs as user simulators to assess responses generated by TOD systems using smaller fine-tuning end-to-end models and provide feedback on user satisfaction to enhance the TOD system.</p>
<p>Figure 1 illustrates an example of a dialogue system response and a user simulator. In this scenario, the system responds to the user by considering the dialogue history. The user simulator, powered by an LLM, predicts the satisfaction scores based on the system response candidates and previous dialogue history. By leveraging these satisfaction feedback, we can select the most likely satisfying response for the user, thereby facilitating high-quality interactions. The key aspect enabling this is the ability of the LLM to assign a satisfaction score to each potential response candidate.</p>
<p>However, due to limitations in dataset annotation scale, modelling user behaviour, and the capabilities of current user simulator approaches, the performance of previous research on user simulators is not yet qualified for practical applications. Consequently, we explore two questions: 1) whether current LLMs can serve as an effective user simulator and 2) how we can leverage the user simulator's feedback generated by LLMs to enhance the task-oriented dialogue system. To address these questions, we design suitable prompts and investigate the zero-shot and few-shot performance of LLM on user satisfaction prediction tasks. We propose our approach, User-Guided Response Optimization (UGRO), to leverage the user feedback to boost the TOD system. Initially, we train the TOD model using a domain-supervised dataset. Subsequently, the satisfaction score predicted by the LLM will be utilized as a reward to optimize the fully supervised trained TOD system. This will be accomplished by employing the Proximal Policy Optimization (PPO) algorithm, which allows us to capitalize on user feedback. In summary, our contributions can be categorized into three aspects:</p>
<ul>
<li>We investigate the performance of LLMs as a user simulator for providing feedback in zero-shot and few-shot settings.</li>
<li>We propose a new model called User-Guided Response Optimization (UGRO) to harness the potential of user feedback and enhance dialogue systems.</li>
<li>Extensive experiments validate the effectiveness of our model, including quantitative performance evaluation, human evaluation, and a case study.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>TOD systems facilitate tasks like hotel bookings or restaurant reservations. Some end-to-end models [8, 13] generate responses using only the dialogue context, while policy optimization methods [24, 25] use ground-truth dialogue states. Bang et al. [2], Lee [13] incorporate both text information and dialogue states to generate a dialogue response. Additionally, reinforcement learning methods
[28] have also shown improvements in TOD systems. Hudeček and Dušek [9] show that specialized task-specific models still outperform general LLMs in TOD tasks. Additionally, the integration of LLMs into domain-specific dialogue systems has also been explored by Li et al. [15], Snell et al. [21].</p>
<p>Regarding user simulators, Sun et al. [23] proposes the user satisfaction estimation task. It leverages human annotations regarding turn-level satisfaction to train an estimator. The estimator is then utilized to perform automatic evaluation by simulating users. Deng et al. [5], Kim and Lipani [11] employ the multi-task framework to enhance the performance of their user simulator. Feng et al. [7] use schema-guided information, and Ye et al. [27] incorporate satisfaction dynamics to enhance user satisfaction prediction. [17, 26] explore Multi-Scale Receptive Field Graph model and VariationalAutoencoder in unsupervised way to determine the emotion in a conversation. Additionally, [14] propose a new dialogue simulation method based on LLM in-context learning to construct the dataset automatically.</p>
<h2>3 METHODOLOGY</h2>
<h3>3.1 Overview</h3>
<p>We begin by assessing the performance of LLMs as user simulators for predicting response satisfaction. Subsequently, we fine-tune the TOD models through supervised training with response data and optimize it using the PPO algorithm and satisfaction rewards.</p>
<h3>3.2 LLM as User Simulator</h3>
<p>Motivation As mentioned earlier, the user simulation encompasses query generation, response satisfaction prediction, and action prediction. Satisfaction prediction stands out as the most direct feedback for assessing quality and enhancing the TOD model. Hence, in this work, our key hypothesis is that satisfaction prediction allows the capabilities of LLMs to be effectively transferred to a new domain by prompting them in a suitable way to predict satisfaction scores in that domain. LLMs' strong understanding ability and rich knowledge enable them to comprehend the semantic aspects of domain dialogue. Moreover, their reasoning capability allows them to provide a reasonable score with detailed explanations.</p>
<p>As illustrated in Figure 2, when using LLMs as a user simulator, the dialogue history serves as the input, and the system response is considered as the last utterance. To provide the necessary context for LLM, the input prompt consists of the task description, criteria for satisfaction scores, data format introduction, 0-6 dialogue examples, the dialogue to be assessed, and instructions. Notably, we incorporate the idea of 'Zero-shot Chain-of-Thought' [12] by including an 'explanation reason' slot in the instruction. This allows the LLM to provide a reliable satisfaction score along with its reasoning. The complete prompt used in this work can be found in our GitHub repository. The output of the user simulator consists of both the explanation reason and the satisfaction score. The score is extracted using regular expressions.</p>
<p>Formally, the TOD dataset, denoted as $\mathcal{D}=\left{\boldsymbol{x}, \boldsymbol{y}^{<em>}\right}$, consists of dialogue history inputs $\boldsymbol{x}$ and corresponding ground truth response outputs $\boldsymbol{y}^{</em>}$. The TOD model's response is represented as $\boldsymbol{y}$, and the satisfaction score predicted by the LLM denoted as $S$.</p>
<p>$$
\boldsymbol{S}=\operatorname{LLM}(\boldsymbol{x}, \boldsymbol{y})
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model Architecture. TOD model undergoes supervised training initially and then is optimized based on user feedback.</p>
<h3>3.3 User-Guided Response Optimization</h3>
<p>Using LLMs to enhance TOD models As discussed in our introduction, the limitations of directly utilizing LLMs in TOD systems necessitate a key design goal in our approach: employing LLMs as a user simulator to provide satisfaction feedback and leveraging this feedback to optimize the fine-tuning TOD model, so as to leverage the LLM's knowledge, understanding and reasoning capabilities. This user feedback guides the TOD model toward producing responses that better satisfy the user.</p>
<p>In order to train a task-oriented dialogue (TOD) system, we begin by performing supervised training on the TOD model using the complete training dataset, maximizing the log-likelihood:</p>
<p>$$
\mathcal{L}<em _left_boldsymbol_x="\left(\boldsymbol{x">{\text {TOD }}=-\mathbb{E}</em>^{}, \boldsymbol{y<em>}\right) \sim \mathcal{D}} \log p_{\text {TOD }}\left(\boldsymbol{y}^{</em>} \mid \boldsymbol{x}\right)
$$</p>
<p>Our objective is to enhance the quality of TOD system-generated responses and improve user satisfaction. Thus, the optimization goal during this phase is to achieve higher satisfaction scores. Mathematically, we aim to maximize the following objective:</p>
<p>$$
\mathbb{E}<em _TOD="{TOD" _text="\text">{\boldsymbol{x} \sim \mathcal{D}, \boldsymbol{y} \sim p</em>)]
$$}}}[\mathbb{R}(\boldsymbol{x}, \boldsymbol{y</p>
<p>where $\mathbb{R}(\boldsymbol{x}, \boldsymbol{y})=\boldsymbol{S}=\operatorname{LLM}(\boldsymbol{x}, \boldsymbol{y}), \boldsymbol{y} \sim p_{\text {TOD }}(\cdot \mid \boldsymbol{x})$
The aforementioned optimization process can be complex and unstable for the TOD model. Therefore, we approach the TOD optimization as a reinforcement learning problem and employ the proximal policy optimization (PPO) algorithm [20]. We initialize the policy network $\pi_{0}$ using the TOD model, denoted as $p_{\text {TOD }}$.</p>
<p>$$
\mathbb{E}<em _boldsymbol_x="\boldsymbol{x">{\pi}[r]=\mathbb{E}</em>)]
$$} \sim \mathcal{D}, \boldsymbol{y} \sim \pi(\cdot \mid \boldsymbol{x})}[r(\boldsymbol{x}, \boldsymbol{y</p>
<p>To incorporate penalty rewards, we also utilize the KL-divergence and set the hyperparameter $\beta$ for training. Consequently, the final reward is calculated as follows:</p>
<p>$$
r(\boldsymbol{x}, \boldsymbol{y})=\operatorname{LLM}(\boldsymbol{x}, \boldsymbol{y})-\beta \log \frac{\pi(\boldsymbol{y} \mid \boldsymbol{x})}{p_{\text {TOD }}(\boldsymbol{y} \mid \boldsymbol{x})}
$$</p>
<p>Implementation Details In this work, ChatGPT is used as a user simulator to assess the generated response and provide satisfaction scores. We utilize Flan-T5 (large version) [4] as the fine-tuning TOD model. This model has been trained extensively on instruction tasks and is well-suited for domain-supervised fine-tuning.</p>
<h2>4 EXPERIMENT RESULTS AND ANALYSIS</h2>
<h3>4.1 Experimental Settings</h3>
<p>Dataset In dialogue response generation, MultiWoZ 2.1 [3] and Schema Guided Dialogue (SGD) [19] are two typical TOD datasets, and they also have been annotated with satisfaction scores.
Evaluation Metrics For evaluation, we use BLEU [18] and ROUGE (F1 score of ROUGE-1, ROUGE-2, and ROUGE-L) [16] metrics to assess the generation quality and semantic performance.
Compared Prior Art We compare our model with prior strong baselines, including HDNO [24], MTTOD [13], PPTOD [22], GALAXY [8], and TOATOD [2]. HDNO models the hierarchical structure between dialogue policy and natural language generation using an option framework [] (temporal abstraction for reinforcement learning). MTTOD combines pretrained language models with a multi-task learning framework for end-to-end TOD modelling by utilizing span prediction as an auxiliary task. PPTOD is a unified plug-and-play model for TOD. It employs a multi-task pre-training strategy to learn primary TOD task completion skills from diverse dialogue corpora. GALAXY is a generative pretrained model that improves dialogue generation by incorporating semi-supervised learning and explicit policy injection. TOATOD is an end-to-end TOD system with task-specific adapters, learning independently for tasks such as dialogue state tracking and response generation.</p>
<h3>4.2 User Simulator Performance and Analysis</h3>
<p>Table 1 demonstrates that ChatGPT performs well even in a zeroshot setting. Increasing the number of examples to 6 shows a significant improvement in accuracy. The F1 score also exhibits enhancement. Furthermore, ChatGPT's ability to predict satisfaction scores is comparable to that of fine-tuned BERT models and significantly superior to the zero-shot BERT model. We also noticed that the dataset imbalance affects satisfaction prediction in ChatGPT. Rating 3 dominates $90 \%$ of samples, while ratings 1 and 5 have less than $1 \%$. This poses challenges for accurate satisfaction classification.</p>
<p>Although ChatGPT's performance may only be comparable, its advantage lies in its practicality as a user simulator. This annotationfree approach addresses the issues of high annotation cost and human bias encountered in previous satisfaction score labelling. Furthermore, this method can be easily extended to new domains or tasks, even with minimal or zero examples, while still maintaining comparable and practical performance.</p>
<p>Table 1: The user satisfaction prediction typically involves considering the user's utterance alongside the system response. However, in our case, user satisfaction is assessed by ChatGPT solely based on the system response, following the approach in [23], which is the SOTA performance. We use their BERT method as the baseline, replicating their experimental setup.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Model</th>
<th>MultiWoZ 2.1</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>SGD</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Acc</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>Acc</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td></td>
</tr>
<tr>
<td>Supervised Training</td>
<td>HiGRU+ATTN</td>
<td>79.7</td>
<td>24.2</td>
<td>24.0</td>
<td>24.0</td>
<td>70.9</td>
<td>25.8</td>
<td>26.2</td>
<td>26.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>BERT</td>
<td>85.4</td>
<td>31.6</td>
<td>26.9</td>
<td>27.1</td>
<td>77.8</td>
<td>28.9</td>
<td>26.9</td>
<td>27.0</td>
<td></td>
</tr>
<tr>
<td>Zero shot</td>
<td>BERT</td>
<td>14.8</td>
<td>13.5</td>
<td>20.3</td>
<td>5.8</td>
<td>11.3</td>
<td>18.1</td>
<td>22.2</td>
<td>6.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>ChatGPT</td>
<td>61.1</td>
<td>21.6</td>
<td>19.1</td>
<td>19.1</td>
<td>51.6</td>
<td>22.6</td>
<td>26.6</td>
<td>19.4</td>
<td></td>
</tr>
<tr>
<td>Few shot</td>
<td>ChatGPT</td>
<td>78.1</td>
<td>22.4</td>
<td>20.8</td>
<td>21.1</td>
<td>72.6</td>
<td>26.6</td>
<td>23.1</td>
<td>23.6</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of system response generation. The BLEU performances of the MultiWOZ 2.1 dataset are sourced from published papers, and ROUGE scores are calculated based on the generated responses uploaded to the leaderboard or provided by the authors. In the case of the SGD dataset performance, * denotes results implemented using official source codes.</p>
<p>| Model | MultiWoZ 2.1 | | | | | | SGD | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<h2>REFERENCES</h2>
<p>[1] Vinsen Marselino Andreas, Genta Indra Winata, and Ayu Purwarianti. 2021. A comparative study on language models for task-oriented dialogue systems. In 2021 8th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA). IEEE, 1-5.
[2] Namo Bang, Jerbyun Lee, and Myoung-Wan Koo. 2023. Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System. arXiv preprint arXiv:2305.02468 (2023).
[3] Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2018. MultiWOZ-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv:1810.00278 (2018).
[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).
[5] Yang Deng, Wenzuan Zhang, Wai Lam, Hong Cheng, and Helen Meng. 2022. User Satisfaction Estimation with Sequential Dialogue Act Modeling in Goaloriented Conversational Systems. In Proceedings of the ACM Web Conference 2022. $2998-3008$.
[6] Pierre Erhacher, Ludovic Denoyer, and Laure Soulier. 2022. Interactive query clarification and refinement via user simulation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2420-2425.
[7] Yue Feng, Yunlong Jiao, Animesh Prasad, Nikolaos Aletras, Emine Yilmaz, and Gabriella Kazai. 2023. Schema-Guided User Satisfaction Modeling for TaskOriented Dialogues. arXiv preprint arXiv:2305.16798 (2023).
[8] Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, et al. 2022. Galaxy: A generative pretrained model for task-oriented dialog with semi-supervised learning and explicit policy injection. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 56. 10749-10757.
[9] Vojtěch Hudeček and Ondřej Dušek. 2023. Are LLMs All You Need for TaskOriented Dialogue? arXiv preprint arXiv:2304.06556 (2023).
[10] Daniel Kahneman. 2011. Thinking, fast and slow. macmillan.
[11] To Eun Kim and Aldo Lipani. 2022. A multi-task based neural model to simulate users in goal oriented dialogue systems. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. $2115-2119$.
[12] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yunuko Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022).
[13] Yohan Lee. 2021. Improving end-to-end task-oriented dialog system with a simple auxiliary task. In Findings of the Association for Computational Linguistics: EMNLP 2021. 1296-1303.
[14] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. 2022. Controllable Dialogue Simulation with In-context Learning. In Findings of the Association for Computational Linguistics: EMNLP 2022. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 4330-4347. https: //doi.org/10.18653/v1/2022.findings-emnlp. 318
[15] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. 2023. Guiding Large Language Models via Directional Stimulus Prompting.
arXiv preprint arXiv:2302.11520 (2023).
[16] Chin-Tew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74-81.
[17] Donovan Ong, Jian Su, Bin Chen, Anh Tuan Luu, Ashok Narendranath, Yue Li, Shuqi Sun, Yingzhan Lin, and Haifeng Wang. 2022. Is discourse role important for emotion recognition in conversation?. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 11121-11129.
[18] Kishore Papinesi, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311-318.
[19] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schemaguided dialogue dataset. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8689-8696.
[20] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017).
[21] Charlie Snell, Sherry Yang, Justin Fu, Yi Su, and Sergey Levine. 2022. Contextaware language modeling for goal-oriented dialogue systems. arXiv preprint arXiv:2204.10198 (2022).
[22] Yixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai, Yi-An Lai, and Yi Zhang. 2021. Multi-task pre-training for plug-and-play task-oriented dialogue system. arXiv preprint arXiv:2109.14739 (2021).
[23] Weiwei Sun, Shao Zhang, Krnztian Balog, Zhnochun Ren, Pengjie Ren, Zhumin Chen, and Maarten de Rijke. 2021. Simulating user satisfaction for the evaluation of task-oriented dialogue systems. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 24992506.
[24] Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. 2020. Modelling hierarchical structure between dialogue policy and natural language generator with option framework for task-oriented dialogue system. arXiv preprint arXiv:2006.06814 (2020).
[25] Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, and Jianxing Yu. 2020. Multidomain dialogue acts and response co-generation. arXiv preprint arXiv:2004.12363 (2020).
[26] Jie Wei, Guanyu Hu, Luu Anh Tuan, Xinyu Yang, and Wenjing Zhu. 2023. MultiScale Receptive Field Graph Model for Emotion Recognition in Conversations. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 1-5.
[27] Fanghua Ye, Zhiyuan Hu, and Emine Yilmaz. 2023. Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process. arXiv preprint arXiv:2305.12594 (2023).
[28] Xiao Yu, Qingyang Wu, Kun Qian, and Zhou Yu. 2022. Reinforced Language Modeling for End-to-End Task Oriented Dialog. arXiv preprint arXiv:2211.16775 (2022).
[29] Saber Zerhoudi, Sebastian Günther, Kim Plassmeier, Timo Borst, Christin Seifert, Matthias Hagen, and Michael Granitzer. 2022. The SimIIR 2.0 Framework: User Types, Markov Model-Based Interaction Simulation, and Advanced Query Generation. In Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management. 4661-4666.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
CIKM '23, October 21-25, 2023, Birmingham, United Kingdom
(c) 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10... $\$ 15.00$
https://doi.org/10.1145/3583780.3615220&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Code is available at: https://github.com/zhiyuanhuhj/UGRO-CIMK23.
${ }^{2}$ The major work was conducted when first author was in University College London.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
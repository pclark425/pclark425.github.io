<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6614 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6614</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6614</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-276421617</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.12110v11.pdf" target="_blank">A-MEM: Agentic Memory for LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems'fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/A-mem, while the source code of the agentic memory system is available at https://github.com/WujiangXu/A-mem-sys.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6614.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6614.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-MEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentic Memory for LLM Agents (A-MEM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic, Zettelkasten-inspired memory system for LLM agents that constructs LLM-enriched atomic notes, forms LLM-mediated links between notes, and evolves existing memories as new notes arrive; uses dense embeddings and top-k retrieval to enable scalable read/write and contextual retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A-MEM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM agent augmented with an agentic memory module that (1) constructs structured notes for each interaction (content, timestamp, LLM-generated keywords, tags, contextual description, dense embedding, explicit links), (2) retrieves top-k nearest notes by embedding and asks an LLM to generate semantic links (link generation), and (3) updates/evolves retrieved neighbor notes via LLM prompts (memory evolution). Retrieval uses cosine similarity over embeddings; when a retrieved note is accessed, its linked 'box' neighbors are also made available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Evaluated across multiple foundation models (GPT-4o-mini; Qwen-1.5B/3B; Llama 3.2 1B/3B; DeepSeek-R1-32B; Claude 3.0/3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External vector store of structured notes with LLM-driven graph-like linking and evolution (Zettelkasten-inspired agentic memory).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured notes m_i = {c_i (raw content), t_i (timestamp), K_i (LLM-generated keywords), G_i (LLM-generated tags), X_i (LLM-generated contextual description), e_i (dense embedding), L_i (explicit links to other notes)}.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Write: LLM generates note attributes and compute dense embedding (all-minilm-l6-v2). Link generation: embedding-based top-k nearest retrieval followed by LLM decision to create links. Memory evolution: LLM prompted to update neighbor notes' context/tags. Read: query embedding -> cosine similarity top-k retrieval; when a retrieved note is accessed, its linked box members are also retrieved.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LoCoMo QA dataset (long-term multi-party dialogue QA); DialSim (long-term multi-party dialogue simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-term dialogue / question answering; multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>LoCoMo (GPT-4o-mini, Multi-Hop): F1 = 27.02, BLEU-1 = 20.09; DialSim: F1 = 3.45, BLEU-1 = 3.37 (paper reports these and additional ROUGE/METEOR/SBERT metrics across models; see Tables 1–6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablations reported: 'w/o LG & ME' (no link generation & no memory evolution): Multi-Hop F1 = 9.65, BLEU-1 = 7.09; 'w/o ME' (link generation only): Multi-Hop F1 = 21.35, BLEU-1 = 15.13 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Primary metrics: F1 and BLEU-1 (also ROUGE-2/ROUGE-L, METEOR, SBERT similarity reported).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Token/cost efficiency: ~1,200 tokens per memory operation (85–93% token reduction vs some baselines which use ~16,900 tokens), cost per memory op < $0.0003 (claimed). Latency: average 5.4s with GPT-4o-mini API; 1.1s with local Llama 3.2 1B. Retrieval hyperparameter k trade-off: increasing k improves performance up to a plateau and can degrade performance at high k due to noise/longer context processing; scaling: retrieval time grows minimally to 3.70µs at 1M entries; space complexity O(N) comparable to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality of note construction, linking, and evolution depends on the underlying LLM (different LLMs may produce different contextual descriptions/links); current implementation is text-only (no multimodal support); larger k can introduce noise and harm performance; paper notes that statistical significance/error bars are not fully reported (experiments use costly LLM APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A-MEM: Agentic Memory for LLM Agents', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6614.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6614.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-term memory management system that stores historical interactions and performs efficient retrieval while dynamically updating memory strength using a forgetting-curve-inspired schedule and building user portraits over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory-enabled agent system that retains historical interactions and updates memory importance using an Ebbinghaus Forgetting Curve formulation; includes mechanisms for building a user portrait and for efficient retrieval (vector-based).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Evaluated as a baseline across the same foundation models used in the paper (various models listed in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External vector store with time/importance-weighted dynamic updating (forgetting-curve based memory strengths).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored historical interaction entries (gists/summaries or raw interactions) with associated strength/decay scores; vector embeddings used for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Vector similarity retrieval (dense embeddings); periodic/dynamic updating of memory strengths according to time and significance (Ebbinghaus Forgetting Curve); user portrait updates from interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LoCoMo QA dataset; DialSim (used as baselines in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-term dialogue / question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper tables as a baseline; across tasks and models MemoryBank achieves lower scores than A-MEM in most reported metrics (see Tables 1–6 for per-model per-category numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1, BLEU-1 (and ROUGE, METEOR, SBERT reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper reports comparable space complexity O(N) and slightly faster retrieval times in some settings compared to A-MEM; does not claim token-efficiency reductions similar to A-MEM. No detailed cost/latency tradeoffs specific to MemoryBank are reported in this paper beyond tabled comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In the memory embedding visualizations and analyses, baseline systems (including MemoryBank) produce more dispersed clusters than A-MEM, indicating less coherent structural organization; paper does not provide ablations disabling MemoryBank's memory to show counterfactual effect.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A-MEM: Agentic Memory for LLM Agents', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6614.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6614.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory architecture for LLM agents inspired by operating-system memory hierarchies, using a dual-tier setup with a main (RAM-like) context for immediate inference and an external (disk-like) context for long-term storage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent architecture implementing a two-level memory: an in-context main memory (fast access) and a larger external store for long-term context; manages virtual context to surface relevant items during inference, analogous to swapping between RAM and disk.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Evaluated as a baseline across the same foundation models used in the paper (various).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Dual-tier memory (short-term main context + long-term external store), virtual context management.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Main context entries (immediately accessible context) and external stored entries (long-term), likely stored as text/gists and embeddings for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Controller that manages which items are in the main context vs external context; retrieval from external context to main context as needed; details referenced from MemGPT but not reimplemented in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LoCoMo QA dataset; DialSim comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-term dialogue / question answering; open-domain QA in some categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in the paper's comparison tables; MemGPT is competitive in some categories (Open Domain, Adversarial) but underperforms A-MEM in multi-hop reasoning by a substantial margin (A-MEM reported as doubling multi-hop performance in some settings). Paper also notes baseline token usage of ~16,900 tokens associated with LoCoMo and MemGPT retrieval strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1, BLEU-1 (and ROUGE, METEOR, SBERT reported).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>MemGPT baseline uses large token context (paper cites baselines using ~16,900 tokens) which increases cost compared to A-MEM; MemGPT can be strong in simple fact retrieval / open-domain categories but less effective on complex multi-hop tasks compared to A-MEM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Tends to perform worse than A-MEM on complex multi-hop reasoning tasks in the paper; higher token usage increases API cost; no ablation toggling MemGPT's memory within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A-MEM: Agentic Memory for LLM Agents', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6614.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6614.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReadAgent (reading agent with gist memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reading agent designed for very long contexts that paginates episodes, distills page-level gists (memory gisting), and performs interactive look-up to answer queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent pipeline that segments long documents into episodes (pagination), produces concise memory gists for each page, and performs interactive look-ups to retrieve relevant gist memories at query time; designed to handle very long contexts by summarizing and indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Evaluated as a baseline across the same foundation models used in the paper (various).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Gist memory (summarized page-level memories) stored for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Concise gists/summaries of paginated episodes (page-level memory items) with embeddings for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Episode pagination -> memory gisting (write) -> interactive lookup (retrieval) using embedding similarity/gist indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LoCoMo QA dataset (long dialogue QA); used as baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context processing; long-term dialogue QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as a baseline in tables; generally lower than A-MEM across categories in the paper's experiments (see Tables 1–6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1, BLEU-1 (and other metrics reported across tables).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>ReadAgent focuses on long-context handling by summarization which reduces context length but may lose fine-grained details needed for multi-hop reasoning; paper shows it underperforms A-MEM on multi-hop tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less effective than A-MEM for complex multi-hop reasoning in experiments reported; possible loss of fine detail in gist summarization impacting downstream QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A-MEM: Agentic Memory for LLM Agents', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6614.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6614.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mem0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mem0 (memory layer for AI agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory system that uses graph-database-style storage for structured memory organization, following RAG principles but with graph-structured relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mem0</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-database-based memory layer that indexes and retrieves memory items using RAG-like retrieval augmented generation but with explicit graph structure and relationships defined by schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Graph database + retrieval-augmented generation (RAG-like with schema-defined relationships).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured nodes/edges in a graph database representing memories and relationships (schema-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Graph-structured storage with retrieval and linking guided by predefined schemas and relationships (as described in the related-work discussion); paper notes Mem0 relies on predefined schemas limiting adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Mentioned in related work as a memory organization approach (no experiments in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper states Mem0 (graph DB approach) provides structured organization but relies on predefined schemas and relationships, which limits adaptability and generalization to novel content or tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Rigid schema dependence: limited adaptability when agent encounters novel types of information (example: learning a novel mathematical solution cannot be flexibly integrated beyond preset framework). No experimental evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A-MEM: Agentic Memory for LLM Agents', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>mem0: The memory layer for ai agents <em>(Rating: 2)</em></li>
                <li>MemoryBank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>MemGPT: Towards llms as operating systems <em>(Rating: 2)</em></li>
                <li>A human-inspired reading agent with gist memory of very long contexts <em>(Rating: 2)</em></li>
                <li>Ret-llm: Towards a general read-write memory for large language models <em>(Rating: 2)</em></li>
                <li>Evaluating very long-term conversational memory of llm agents <em>(Rating: 1)</em></li>
                <li>Chain-of-note: Enhancing robustness in retrieval-augmented language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6614",
    "paper_id": "paper-276421617",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "A-MEM",
            "name_full": "Agentic Memory for LLM Agents (A-MEM)",
            "brief_description": "An agentic, Zettelkasten-inspired memory system for LLM agents that constructs LLM-enriched atomic notes, forms LLM-mediated links between notes, and evolves existing memories as new notes arrive; uses dense embeddings and top-k retrieval to enable scalable read/write and contextual retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "A-MEM",
            "agent_description": "LLM agent augmented with an agentic memory module that (1) constructs structured notes for each interaction (content, timestamp, LLM-generated keywords, tags, contextual description, dense embedding, explicit links), (2) retrieves top-k nearest notes by embedding and asks an LLM to generate semantic links (link generation), and (3) updates/evolves retrieved neighbor notes via LLM prompts (memory evolution). Retrieval uses cosine similarity over embeddings; when a retrieved note is accessed, its linked 'box' neighbors are also made available.",
            "model_size": "Evaluated across multiple foundation models (GPT-4o-mini; Qwen-1.5B/3B; Llama 3.2 1B/3B; DeepSeek-R1-32B; Claude 3.0/3.5).",
            "memory_used": true,
            "memory_type": "External vector store of structured notes with LLM-driven graph-like linking and evolution (Zettelkasten-inspired agentic memory).",
            "memory_representation": "Structured notes m_i = {c_i (raw content), t_i (timestamp), K_i (LLM-generated keywords), G_i (LLM-generated tags), X_i (LLM-generated contextual description), e_i (dense embedding), L_i (explicit links to other notes)}.",
            "memory_access_mechanism": "Write: LLM generates note attributes and compute dense embedding (all-minilm-l6-v2). Link generation: embedding-based top-k nearest retrieval followed by LLM decision to create links. Memory evolution: LLM prompted to update neighbor notes' context/tags. Read: query embedding -&gt; cosine similarity top-k retrieval; when a retrieved note is accessed, its linked box members are also retrieved.",
            "task_name": "LoCoMo QA dataset (long-term multi-party dialogue QA); DialSim (long-term multi-party dialogue simulator)",
            "task_category": "Long-term dialogue / question answering; multi-hop reasoning",
            "performance_with_memory": "LoCoMo (GPT-4o-mini, Multi-Hop): F1 = 27.02, BLEU-1 = 20.09; DialSim: F1 = 3.45, BLEU-1 = 3.37 (paper reports these and additional ROUGE/METEOR/SBERT metrics across models; see Tables 1–6).",
            "performance_without_memory": "Ablations reported: 'w/o LG & ME' (no link generation & no memory evolution): Multi-Hop F1 = 9.65, BLEU-1 = 7.09; 'w/o ME' (link generation only): Multi-Hop F1 = 21.35, BLEU-1 = 15.13 (Table 3).",
            "has_comparative_results": true,
            "performance_metric": "Primary metrics: F1 and BLEU-1 (also ROUGE-2/ROUGE-L, METEOR, SBERT similarity reported).",
            "tradeoffs_reported": "Token/cost efficiency: ~1,200 tokens per memory operation (85–93% token reduction vs some baselines which use ~16,900 tokens), cost per memory op &lt; $0.0003 (claimed). Latency: average 5.4s with GPT-4o-mini API; 1.1s with local Llama 3.2 1B. Retrieval hyperparameter k trade-off: increasing k improves performance up to a plateau and can degrade performance at high k due to noise/longer context processing; scaling: retrieval time grows minimally to 3.70µs at 1M entries; space complexity O(N) comparable to baselines.",
            "limitations_or_failure_cases": "Quality of note construction, linking, and evolution depends on the underlying LLM (different LLMs may produce different contextual descriptions/links); current implementation is text-only (no multimodal support); larger k can introduce noise and harm performance; paper notes that statistical significance/error bars are not fully reported (experiments use costly LLM APIs).",
            "citation": "Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.",
            "uuid": "e6614.0",
            "source_info": {
                "paper_title": "A-MEM: Agentic Memory for LLM Agents",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank",
            "brief_description": "A long-term memory management system that stores historical interactions and performs efficient retrieval while dynamically updating memory strength using a forgetting-curve-inspired schedule and building user portraits over time.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "MemoryBank",
            "agent_description": "Memory-enabled agent system that retains historical interactions and updates memory importance using an Ebbinghaus Forgetting Curve formulation; includes mechanisms for building a user portrait and for efficient retrieval (vector-based).",
            "model_size": "Evaluated as a baseline across the same foundation models used in the paper (various models listed in experiments).",
            "memory_used": true,
            "memory_type": "External vector store with time/importance-weighted dynamic updating (forgetting-curve based memory strengths).",
            "memory_representation": "Stored historical interaction entries (gists/summaries or raw interactions) with associated strength/decay scores; vector embeddings used for retrieval.",
            "memory_access_mechanism": "Vector similarity retrieval (dense embeddings); periodic/dynamic updating of memory strengths according to time and significance (Ebbinghaus Forgetting Curve); user portrait updates from interactions.",
            "task_name": "LoCoMo QA dataset; DialSim (used as baselines in comparisons).",
            "task_category": "Long-term dialogue / question answering",
            "performance_with_memory": "Reported in paper tables as a baseline; across tasks and models MemoryBank achieves lower scores than A-MEM in most reported metrics (see Tables 1–6 for per-model per-category numbers).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "F1, BLEU-1 (and ROUGE, METEOR, SBERT reported in the paper).",
            "tradeoffs_reported": "Paper reports comparable space complexity O(N) and slightly faster retrieval times in some settings compared to A-MEM; does not claim token-efficiency reductions similar to A-MEM. No detailed cost/latency tradeoffs specific to MemoryBank are reported in this paper beyond tabled comparisons.",
            "limitations_or_failure_cases": "In the memory embedding visualizations and analyses, baseline systems (including MemoryBank) produce more dispersed clusters than A-MEM, indicating less coherent structural organization; paper does not provide ablations disabling MemoryBank's memory to show counterfactual effect.",
            "citation": "Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.",
            "uuid": "e6614.1",
            "source_info": {
                "paper_title": "A-MEM: Agentic Memory for LLM Agents",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT",
            "brief_description": "A memory architecture for LLM agents inspired by operating-system memory hierarchies, using a dual-tier setup with a main (RAM-like) context for immediate inference and an external (disk-like) context for long-term storage.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "MemGPT",
            "agent_description": "Agent architecture implementing a two-level memory: an in-context main memory (fast access) and a larger external store for long-term context; manages virtual context to surface relevant items during inference, analogous to swapping between RAM and disk.",
            "model_size": "Evaluated as a baseline across the same foundation models used in the paper (various).",
            "memory_used": true,
            "memory_type": "Dual-tier memory (short-term main context + long-term external store), virtual context management.",
            "memory_representation": "Main context entries (immediately accessible context) and external stored entries (long-term), likely stored as text/gists and embeddings for retrieval.",
            "memory_access_mechanism": "Controller that manages which items are in the main context vs external context; retrieval from external context to main context as needed; details referenced from MemGPT but not reimplemented in detail in this paper.",
            "task_name": "LoCoMo QA dataset; DialSim comparisons.",
            "task_category": "Long-term dialogue / question answering; open-domain QA in some categories.",
            "performance_with_memory": "Reported in the paper's comparison tables; MemGPT is competitive in some categories (Open Domain, Adversarial) but underperforms A-MEM in multi-hop reasoning by a substantial margin (A-MEM reported as doubling multi-hop performance in some settings). Paper also notes baseline token usage of ~16,900 tokens associated with LoCoMo and MemGPT retrieval strategies.",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "F1, BLEU-1 (and ROUGE, METEOR, SBERT reported).",
            "tradeoffs_reported": "MemGPT baseline uses large token context (paper cites baselines using ~16,900 tokens) which increases cost compared to A-MEM; MemGPT can be strong in simple fact retrieval / open-domain categories but less effective on complex multi-hop tasks compared to A-MEM.",
            "limitations_or_failure_cases": "Tends to perform worse than A-MEM on complex multi-hop reasoning tasks in the paper; higher token usage increases API cost; no ablation toggling MemGPT's memory within this paper.",
            "citation": "Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.",
            "uuid": "e6614.2",
            "source_info": {
                "paper_title": "A-MEM: Agentic Memory for LLM Agents",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ReadAgent",
            "name_full": "ReadAgent (reading agent with gist memory)",
            "brief_description": "A reading agent designed for very long contexts that paginates episodes, distills page-level gists (memory gisting), and performs interactive look-up to answer queries.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ReadAgent",
            "agent_description": "Agent pipeline that segments long documents into episodes (pagination), produces concise memory gists for each page, and performs interactive look-ups to retrieve relevant gist memories at query time; designed to handle very long contexts by summarizing and indexing.",
            "model_size": "Evaluated as a baseline across the same foundation models used in the paper (various).",
            "memory_used": true,
            "memory_type": "Gist memory (summarized page-level memories) stored for retrieval.",
            "memory_representation": "Concise gists/summaries of paginated episodes (page-level memory items) with embeddings for retrieval.",
            "memory_access_mechanism": "Episode pagination -&gt; memory gisting (write) -&gt; interactive lookup (retrieval) using embedding similarity/gist indexing.",
            "task_name": "LoCoMo QA dataset (long dialogue QA); used as baseline in comparisons.",
            "task_category": "Long-context processing; long-term dialogue QA",
            "performance_with_memory": "Reported as a baseline in tables; generally lower than A-MEM across categories in the paper's experiments (see Tables 1–6).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "F1, BLEU-1 (and other metrics reported across tables).",
            "tradeoffs_reported": "ReadAgent focuses on long-context handling by summarization which reduces context length but may lose fine-grained details needed for multi-hop reasoning; paper shows it underperforms A-MEM on multi-hop tasks.",
            "limitations_or_failure_cases": "Less effective than A-MEM for complex multi-hop reasoning in experiments reported; possible loss of fine detail in gist summarization impacting downstream QA.",
            "citation": "Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.",
            "uuid": "e6614.3",
            "source_info": {
                "paper_title": "A-MEM: Agentic Memory for LLM Agents",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mem0",
            "name_full": "Mem0 (memory layer for AI agents)",
            "brief_description": "A memory system that uses graph-database-style storage for structured memory organization, following RAG principles but with graph-structured relationships.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Mem0",
            "agent_description": "Graph-database-based memory layer that indexes and retrieves memory items using RAG-like retrieval augmented generation but with explicit graph structure and relationships defined by schemas.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Graph database + retrieval-augmented generation (RAG-like with schema-defined relationships).",
            "memory_representation": "Structured nodes/edges in a graph database representing memories and relationships (schema-dependent).",
            "memory_access_mechanism": "Graph-structured storage with retrieval and linking guided by predefined schemas and relationships (as described in the related-work discussion); paper notes Mem0 relies on predefined schemas limiting adaptability.",
            "task_name": null,
            "task_category": "Mentioned in related work as a memory organization approach (no experiments in this paper).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper states Mem0 (graph DB approach) provides structured organization but relies on predefined schemas and relationships, which limits adaptability and generalization to novel content or tasks.",
            "limitations_or_failure_cases": "Rigid schema dependence: limited adaptability when agent encounters novel types of information (example: learning a novel mathematical solution cannot be flexibly integrated beyond preset framework). No experimental evaluation in this paper.",
            "citation": "Wujiang Xu et al., 2025, A-Mem: Agentic Memory for LLM Agents, arXiv preprint.",
            "uuid": "e6614.4",
            "source_info": {
                "paper_title": "A-MEM: Agentic Memory for LLM Agents",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "mem0: The memory layer for ai agents",
            "rating": 2
        },
        {
            "paper_title": "MemoryBank: Enhancing large language models with long-term memory",
            "rating": 2
        },
        {
            "paper_title": "MemGPT: Towards llms as operating systems",
            "rating": 2
        },
        {
            "paper_title": "A human-inspired reading agent with gist memory of very long contexts",
            "rating": 2
        },
        {
            "paper_title": "Ret-llm: Towards a general read-write memory for large language models",
            "rating": 2
        },
        {
            "paper_title": "Evaluating very long-term conversational memory of llm agents",
            "rating": 1
        },
        {
            "paper_title": "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
            "rating": 1
        }
    ],
    "cost": 0.019964999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A-Mem: Agentic Memory for LLM Agents
8 Oct 2025</p>
<p>Wujiang Xu wujiang.xu@rutgers.edu 
Rutgers University</p>
<p>Zujie Liang 
Independent Researcher</p>
<p>Kai Mei 
Rutgers University</p>
<p>Hang Gao 
Rutgers University</p>
<p>Juntao Tan 
Rutgers University</p>
<p>Yongfeng Zhang 
Rutgers University</p>
<p>AIOS Foundation</p>
<p>A-Mem: Agentic Memory for LLM Agents
8 Oct 2025476D76E843FD1564F1FF06B5A6C13DBDarXiv:2502.12110v11[cs.CL]LLM Agents Agentic Memory
While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences.Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases.Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks.To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way.Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking.When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags.The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist.Additionally, this process enables memory evolution -as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding.Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management.Empirical experiments on six foundation models show superior improvement against existing SOTA baselines.</p>
<p>Introduction</p>
<p>Large Language Model (LLM) agents have demonstrated remarkable capabilities in various tasks, with recent advances enabling them to interact with environments, execute tasks, and make decisions autonomously [23,33,7].They integrate LLMs with external tools and delicate workflows to improve reasoning and planning abilities.Though LLM agent has strong reasoning performance, it still needs a memory system to provide long-term interaction ability with the external environment [35].</p>
<p>Existing memory systems [25,39,28,21] for LLM agents provide basic memory storage functionality.These systems require agent developers to predefine memory storage structures, specify storage points within the workflow, and establish retrieval timing.Meanwhile, to improve structured memory organization, Mem0 [8], following the principles of RAG [9,18,30], incorporates graph databases for storage and retrieval processes.While graph databases provide structured organization for memory systems, their reliance on predefined schemas and relationships fundamentally limits their adaptability.This limitation manifests clearly in practical scenarios -when an agent learns a novel mathematical solution, current systems can only categorize and link this information within their preset framework, Figure 2: Our A-MEM architecture comprises three integral parts in memory storage.During note construction, the system processes new interaction memories and stores them as notes with multiple attributes.The link generation process first retrieves the most relevant historical memories and then employs an LLM to determine whether connections should be established between them.The concept of a 'box' describes that related memories become interconnected through their similar contextual descriptions, analogous to the Zettelkasten method.However, our approach allows individual memories to exist simultaneously within multiple different boxes.During the memory retrieval stage, we extract query embeddings using a text encoding model and search the memory database for relevant matches.When related memory is retrieved, similar memories that are linked within the same box are also automatically accessed.</p>
<p>patterns, particularly in memory writing and retrieval processes.Such inflexibility leads to poor generalization in new environments and limited effectiveness in long-term interactions.Therefore, designing a flexible and universal memory system that supports agents' long-term interactions remains a crucial challenge.</p>
<p>Retrieval-Augmented Generation</p>
<p>Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to enhance LLMs by incorporating external knowledge sources [18,6,10].The standard RAG [37,34] process involves indexing documents into chunks, retrieving relevant chunks based on semantic similarity, and augmenting the LLM's prompt with this retrieved context for generation.Advanced RAG systems [20,12] have evolved to include sophisticated pre-retrieval and post-retrieval optimizations.Building upon these foundations, recent researches has introduced agentic RAG systems that demonstrate more autonomous and adaptive behaviors in the retrieval process.These systems can dynamically determine when and what to retrieve [4,14], generate hypothetical responses to guide retrieval, and iteratively refine their search strategies based on intermediate results [31,29].However, while agentic RAG approaches demonstrate agency in the retrieval phase by autonomously deciding when and what to retrieve [4,14,38], our agentic memory system exhibits agency at a more fundamental level through the autonomous evolution of its memory structure.Inspired by the Zettelkasten method, our system allows memories to actively generate their own contextual descriptions, form meaningful connections with related memories, and evolve both their content and relationships as new experiences emerge.This fundamental distinction in agency between retrieval versus storage and evolution distinguishes our approach from agentic RAG systems, which maintain static knowledge bases despite their sophisticated retrieval mechanisms.</p>
<p>Methodolodgy</p>
<p>Our proposed agentic memory system draws inspiration from the Zettelkasten method, implementing a dynamic and self-evolving memory system that enables LLM agents to maintain long-term memory without predetermined operations.The system's design emphasizes atomic note-taking, flexible linking mechanisms, and continuous evolution of knowledge structures.</p>
<p>Note Construction</p>
<p>Building upon the Zettelkasten method's principles of atomic note-taking and flexible organization, we introduce an LLM-driven approach to memory note construction.When an agent interacts with its environment, we construct structured memory notes that capture both explicit information and LLMgenerated contextual understanding.Each memory note m i in our collection M = {m 1 , m 2 , ..., m N } is represented as: m i = {c i , t i , K i , G i , X i , e i , L i } (1) where c i represents the original interaction content, t i is the timestamp of the interaction, K i denotes LLM-generated keywords that capture key concepts, G i contains LLM-generated tags for categorization, X i represents the LLM-generated contextual description that provides rich semantic understanding, and L i maintains the set of linked memories that share semantic relationships.To enrich each memory note with meaningful context beyond its basic content and timestamp, we leverage an LLM to analyze the interaction and generate these semantic components.The note construction process involves prompting the LLM with carefully designed templates P s1 :
K i , G i , X i ← LLM(c i ∥t i ∥P s1 )(2)
Following the Zettelkasten principle of atomicity, each note captures a single, self-contained unit of knowledge.To enable efficient retrieval and linking, we compute a dense vector representation via a text encoder [27] that encapsulates all textual components of the note:
e i = f enc <a href="3"> concat(c i , K i , G i , X i ) </a>
By using LLMs to generate enriched components, we enable autonomous extraction of implicit knowledge from raw interactions.The multi-faceted note structure (K i , G i , X i ) creates rich representations that capture different aspects of the memory, facilitating nuanced organization and retrieval.Additionally, the combination of LLM-generated semantic components with dense vector representations provides both context and computationally efficient similarity matching.</p>
<p>Link Generation</p>
<p>Our system implements an autonomous link generation mechanism that enables new memory notes to form meaningful connections without predefined rules.When the constrctd memory note m n is added to the system, we first leverage its semantic embedding for similarity-based retrieval.For each existing memory note m j ∈ M, we compute a similarity score:
s n,j = e n ⋅ e j |e n ||e j |(4)
The system then identifies the top-k most relevant memories:
M n near = {m j | rank(s n,j ) ≤ k, m j ∈ M}(5)
Based on these candidate nearest memories, we prompt the LLM to analyze potential connections based on their potential common attributes.Formally, the link set of memory m n update like:
L i ← LLM(m n ∥M n near ∥P s2 )(6)
Each generated link l i is structured as: L i = {m i , ..., m k }.By using embedding-based retrieval as an initial filter, we enable efficient scalability while maintaining semantic relevance.A-MEM can quickly identify potential connections even in large memory collections without exhaustive comparison.More importantly, the LLM-driven analysis allows for nuanced understanding of relationships that goes beyond simple similarity metrics.The language model can identify subtle patterns, causal relationships, and conceptual connections that might not be apparent from embedding similarity alone.We implements the Zettelkasten principle of flexible linking while leveraging modern language models.The resulting network emerges organically from memory content and context, enabling natural knowledge organization.</p>
<p>Memory Evolution</p>
<p>After creating links for the new memory, A-MEM evolves the retrieved memories based on their textual information and relationships with the new memory.For each memory m j in the nearest neighbor set M n near , the system determines whether to update its context, keywords, and tags.This evolution process can be formally expressed as:
m * j ← LLM(m n ∥M n near \ m j ∥m j ∥P s3 )(7)
The evolved memory m * j then replaces the original memory m j in the memory set M. This evolutionary approach enables continuous updates and new connections, mimicking human learning processes.As the system processes more memories over time, it develops increasingly sophisticated knowledge structures, discovering higher-order patterns and concepts across multiple memories.This creates a foundation for autonomous memory learning where knowledge organization becomes progressively richer through the ongoing interaction between new experiences and existing memories.</p>
<p>Retrieve Relative Memory</p>
<p>In each interaction, our A-MEM performs context-aware memory retrieval to provide the agent with relevant historical information.Given a query text q from the current interaction, we first compute its dense vector representation using the same text encoder used for memory notes:
e q = f enc (q)(8)
The system then computes similarity scores between the query embedding and all existing memory notes in M using cosine similarity:
s q,i = e q ⋅ e i |e q ||e i | , where e i ∈ m i , ∀m i ∈ M(9)
Then we retrieve the k most relevant memories from the historical memory storage to construct a contextually appropriate prompt.
M retrieved = {m i |rank(s q,i ) ≤ k, m i ∈ M}(10)
These retrieved memories provide relevant historical context that helps the agent better understand and respond to the current interaction.The retrieved context enriches the agent's reasoning process by connecting the current interaction with related past experiences stored in the memory system.</p>
<p>Experiment</p>
<p>Dataset and Evaluation</p>
<p>To evaluate the effectiveness of instruction-aware recommendation in long-term conversations, we utilize the LoCoMo dataset [22], which contains significantly longer dialogues compared to existing conversational datasets [36,13].[16], to evaluate the effectiveness of our memory system.It is question-answering dataset derived from long-term multi-party dialogues.The dataset is derived from popular TV shows (Friends, The Big Bang Theory, and The Office), covering 1,300 sessions spanning five years, containing approximately 350,000 tokens, and including more than 1,000 questions per session from refined fan quiz website questions and complex questions generated from temporal knowledge graphs.</p>
<p>For comparison baselines, we compare to LoCoMo [22], ReadAgent [17], MemoryBank [39] and MemGPT [25].The detailed introduction of baselines can be found in Appendix A.1 For evaluation, we employ two primary metrics: the F1 score to assess answer accuracy by balancing precision and recall, and BLEU-1 [26] to evaluate generated response quality by measuring word overlap with ground truth responses.Also, we report the average token length for answering one question.</p>
<p>Besides reporting experiment results with four additional metrics (ROUGE-L, ROUGE-2, METEOR, and SBERT Similarity), we also present experimental outcomes using different foundation models including DeepSeek-R1-32B [11], Claude 3.0 Haiku [2], and Claude 3.5 Haiku [3] in Appendix A.3.</p>
<p>Implementation Details</p>
<p>For all baselines and our proposed method, we maintain consistency by employing identical system prompts as detailed in Appendix B. The deployment of Qwen-1.5B/3B and Llama 3.2 1B/3B models is accomplished through local instantiation using Ollama1 , with LiteLLM2 managing structured output generation.For GPT models, we utilize the official structured output API.In our memory retrieval process, we primarily employ k=10 for top-k memory selection to maintain computational efficiency, while adjusting this parameter for specific categories to optimize performance.The detailed configurations of k can be found in Appendix A.5.For text embedding, we implement the all-minilm-l6-v2 model across all experiments.</p>
<p>Empricial Results</p>
<p>Performance Analysis.In our empirical evaluation, we compared A-MEM with four competitive baselines including LoCoMo [22], ReadAgent [17], MemoryBank [39], and MemGPT [25] on the LoCoMo dataset.For non-GPT foundation models, our A-MEM consistently outperforms all baselines across different categories, demonstrating the effectiveness of our agentic memory approach.For GPT-based models, while LoCoMo and MemGPT show strong performance in certain categories like Open Domain and Adversial tasks due to their robust pre-trained knowledge in simple fact retrieval, our A-MEM demonstrates superior performance in Multi-Hop tasks achieves at least two times better performance that require complex reasoning chains.In addition to experiments on the LoCoMo dataset, we also compare our method on the DialSim dataset against LoCoMo and MemGPT.A-MEM consistently outperforms all baselines across evaluation metrics, achieving an F1  18).The effectiveness of A-MEM stems from its novel agentic memory architecture that enables dynamic and structured memory management.Unlike traditional approaches that use static memory operations, our system creates interconnected memory networks through atomic notes with rich contextual descriptions, enabling more effective multi-hop reasoning.The system's ability to dynamically establish connections between memories based on shared attributes and continuously update existing memory descriptions with new contextual information allows it to better capture and utilize the relationships between different pieces of information.</p>
<p>Cost-Efficiency Analysis.A-MEM demonstrates significant computational and cost efficiency alongside strong performance.The system requires approximately 1,200 tokens per memory operation, achieving an 85-93% reduction in token usage compared to baseline methods (LoCoMo and MemGPT with 16,900 tokens) through our selective top-k retrieval mechanism.This substantial token reduction directly translates to lower operational costs, with each memory operation costing less than $0.0003 when using commercial API services-making large-scale deployments economically viable.Processing times average 5.4 seconds using GPT-4o-mini and only 1.1 seconds with locally-hosted Llama 3.2 1B on a single GPU.Despite requiring multiple LLM calls during memory processing, A-MEM maintains this cost-effective resource utilization while consistently outperforming baseline approaches across all foundation models tested, particularly doubling performance on complex multi-hop reasoning tasks.This balance of low computational cost and superior reasoning capability highlights A-MEM's practical advantage for deployment in the real world.</p>
<p>Ablation Study</p>
<p>To evaluate the effectiveness of the Link Generation (LG) and Memory Evolution (ME) modules, we conduct the ablation study by systematically removing key components of our model.When both LG and ME modules are removed, the system exhibits substantial performance degradation, particularly in Multi Hop reasoning and Open Domain tasks.The system with only LG active (w/o ME) shows intermediate performance levels, maintaining significantly better results than the version without both modules, which demonstrates the fundamental importance of link generation in establishing memory connections.Our full model, A-MEM, consistently achieves the best performance across all evaluation categories, with particularly strong results in complex reasoning tasks.These results reveal that while the link generation module serves as a critical foundation for memory organization, the memory evolution module provides essential refinements to the memory structure.The ablation study validates our architectural design choices and highlights the complementary nature of these two modules in creating an effective memory system.</p>
<p>Hyperparameter Analysis</p>
<p>We conducted extensive experiments to analyze the impact of the memory retrieval parameter k, which controls the number of relevant memories retrieved for each interaction.As shown in Figure 3, we evaluated performance across different k values (10,20,30,40,50) on five categories of tasks using GPT-4o-mini as our base model.The results reveal an interesting pattern: while increasing k generally leads to improved performance, this improvement gradually plateaus and sometimes slightly decreases at higher values.This trend is particularly evident in Multi Hop and Open Domain  (e) Adversarial Figure 3: Impact of memory retrieval parameter k across different task categories with GPT-4o-mini as the base model.While larger k values generally improve performance by providing richer historical context, the gains diminish beyond certain thresholds, suggesting a trade-off between context richness and effective information processing.This pattern is consistent across all evaluation categories, indicating the importance of balanced context retrieval for optimal performance.tasks.The observation suggests a delicate balance in memory retrieval -while larger k values provide richer historical context for reasoning, they may also introduce noise and challenge the model's capacity to process longer sequences effectively.Our analysis indicates that moderate k values strike an optimal balance between context richness and information processing efficiency.</p>
<p>Scaling Analysis</p>
<p>To evaluate storage costs with accumulating memory, we examined the relationship between storage size and retrieval time across our A-MEM system and two baseline approaches: MemoryBank [39] and ReadAgent [17].We evaluated these three memory systems with identical memory content across four scale points, increasing the number of entries by a factor of 10 at each step (from 1,000 to 10,000, 100,000, and finally 1,000,000 entries).The experimental results reveal key insights about our A-MEM system's scaling properties: In terms of space complexity, all three systems exhibit identical linear memory usage scaling (O(N )), as expected for vector-based retrieval systems.This confirms that A-MEM introduces no additional storage overhead compared to baseline approaches.</p>
<p>For retrieval time, A-MEM demonstrates excellent efficiency with minimal increases as memory size grows.Even when scaling to 1 million memories, A-MEM's retrieval time increases only from 0.31µs to 3.70µs, representing exceptional performance.While MemoryBank shows slightly faster retrieval times, A-MEM maintains comparable performance while providing richer memory representations and functionality.Based on our space complexity and retrieval time analysis, we conclude that A-MEM's retrieval mechanisms maintain excellent efficiency even at large scales.The minimal growth in retrieval time across memory sizes addresses concerns about efficiency in large-scale memory systems, demonstrating that A-MEM provides a highly scalable solution for long-term conversation management.This unique combination of efficiency, scalability, and enhanced memory capabilities positions A-MEM as a significant advancement in building powerful and long-term memory mechanism for LLM Agents.</p>
<p>Memory Analysis</p>
<p>We present the t-SNE visualization in Figure 4 of memory embeddings to demonstrate the structural advantages of our agentic memory system.Analyzing two dialogues sampled from long-term conversations in LoCoMo [22], we observe that A-MEM (shown in blue) consistently exhibits more coherent clustering patterns compared to the baseline system (shown in red).This structural organization is particularly evident in Dialogue 2, where well-defined clusters emerge in the central region, providing empirical evidence for the effectiveness of our memory evolution mechanism and contextual description generation.In contrast, the baseline memory embeddings display a more dispersed distribution, demonstrating that memories lack structural organization without our link generation and memory evolution components.These visualization results validate that A-MEM can autonomously maintain meaningful memory structures through dynamic evolution and linking mechanisms.More results can be seen in Appendix A.4.</p>
<p>Conclusions</p>
<p>In this work, we introduced A-MEM, a novel agentic memory system that enables LLM agents to dynamically organize and evolve their memories without relying on predefined structures.Drawing inspiration from the Zettelkasten method, our system creates an interconnected knowledge network through dynamic indexing and linking mechanisms that adapt to diverse real-world tasks.The system's core architecture features autonomous generation of contextual descriptions for new memories and intelligent establishment of connections with existing memories based on shared attributes.Furthermore, our approach enables continuous evolution of historical memories by incorporating new experiences and developing higher-order attributes through ongoing interactions.Through extensive empirical evaluation across six foundation models, we demonstrated that A-MEM achieves superior performance compared to existing state-of-the-art baselines in long-term conversational tasks.Visualization analysis further validates the effectiveness of our memory organization approach.These results suggest that agentic memory systems can significantly enhance LLM agents' ability to utilize long-term knowledge in complex environments.</p>
<p>Limitations</p>
<p>While our agentic memory system achieves promising results, we acknowledge several areas for potential future exploration.First, although our system dynamically organizes memories, the quality of these organizations may still be influenced by the inherent capabilities of the underlying language models.Different LLMs might generate slightly different contextual descriptions or establish varying connections between memories.Additionally, while our current implementation focuses on text-based interactions, future work could explore extending the system to handle multimodal information, such as images or audio, which could provide richer contextual representations.</p>
<p>APPENDIX A Experiment</p>
<p>A.1 Detailed Baselines Introduction LoCoMo [22] takes a direct approach by leveraging foundation models without memory mechanisms for question answering tasks.For each query, it incorporates the complete preceding conversation and questions into the prompt, evaluating the model's reasoning capabilities.</p>
<p>ReadAgent [17] tackles long-context document processing through a sophisticated three-step methodology: it begins with episode pagination to segment content into manageable chunks, followed by memory gisting to distill each page into concise memory representations, and concludes with interactive look-up to retrieve pertinent information as needed.</p>
<p>MemoryBank [39] introduces an innovative memory management system that maintains and efficiently retrieves historical interactions.The system features a dynamic memory updating mechanism based on the Ebbinghaus Forgetting Curve theory, which intelligently adjusts memory strength according to time and significance.Additionally, it incorporates a user portrait building system that progressively refines its understanding of user personality through continuous interaction analysis.</p>
<p>MemGPT [25] presents a novel virtual context management system drawing inspiration from traditional operating systems' memory hierarchies.The architecture implements a dual-tier structure: a main context (analogous to RAM) that provides immediate access during LLM inference, and an external context (analogous to disk storage) that maintains information beyond the fixed context window.</p>
<p>A.2 Evaluation Metric</p>
<p>The F1 score represents the harmonic mean of precision and recall, offering a balanced metric that combines both measures into a single value.This metric is particularly valuable when we need to balance between complete and accurate responses:
F1 = 2 ⋅ precision ⋅ recall precision + recall(11)
where precision = true positives true positives + false positives (12) recall = true positives true positives + false negatives (13) In question-answering systems, the F1 score serves a crucial role in evaluating exact matches between predicted and reference answers.This is especially important for span-based QA tasks, where systems must identify precise text segments while maintaining comprehensive coverage of the answer.</p>
<p>BLEU-1 [26] provides a method for evaluating the precision of unigram matches between system outputs and reference texts:
BLEU-1 = BP ⋅ exp( 1 ∑ n=1 w n log p n )(14)
where
BP = { 1 if c &gt; r e 1−r/c if c ≤ r(15)p n = ∑ i ∑ k min(h ik , m ik ) ∑ i ∑ k h ik(16)
Here, c is candidate length, r is reference length, h ik is the count of n-gram i in candidate k, and m ik is the maximum count in any reference.In QA, BLEU-1 evaluates the lexical precision of generated answers, particularly useful for generative QA systems where exact matching might be too strict.</p>
<p>ROUGE-L [19]</p>
<p>measures the longest common subsequence between the generated and reference texts.</p>
<p>ROUGE-L
= (1 + β 2 )R l P l R l + β 2 P l (17) R l = LCS(X, Y ) |X|(18)P l = LCS(X, Y ) |Y | (19)
where X is reference text, Y is candidate text, and LCS is the Longest Common Subsequence.</p>
<p>ROUGE-2 [19] calculates the overlap of bigrams between the generated and reference texts.
ROUGE-2 = ∑ bigram∈ref min(Count ref (bigram), Count cand (bigram)) ∑ bigram∈ref Count ref (bigram)(20)
Both ROUGE-L and ROUGE-2 are particularly useful for evaluating the fluency and coherence of generated answers, with ROUGE-L focusing on sequence matching and ROUGE-2 on local word order.</p>
<p>METEOR [5] computes a score based on aligned unigrams between the candidate and reference texts, considering synonyms and paraphrases.
METEOR = F mean ⋅ (1 − Penalty)(21)F mean = 10P ⋅ R R + 9P(22)Penalty = 0.5 ⋅ ( ch m ) 3 (23)
where P is precision, R is recall, ch is number of chunks, and m is number of matched unigrams.METEOR is valuable for QA evaluation as it considers semantic similarity beyond exact matching, making it suitable for evaluating paraphrased answers.</p>
<p>SBERT Similarity [27] measures the semantic similarity between two texts using sentence embeddings.</p>
<p>SBERT_Similarity = cos(SBERT(x), SBERT(y)) ( 24)
cos(a, b) = a ⋅ b ∥a∥∥b∥(25)
SBERT(x ) represents the sentence embedding of text.SBERT Similarity is particularly useful for evaluating semantic understanding in QA systems, as it can capture meaning similarities even when the lexical overlap is low.</p>
<p>A.3 Comparison Results</p>
<p>Our comprehensive evaluation using ROUGE-2, ROUGE-L, METEOR, and SBERT metrics demonstrates that A-MEM achieves superior performance while maintaining remarkable computational efficiency.Through extensive empirical testing across various model sizes and task categories, we have established A-MEM as a more effective approach compared to existing baselines, supported by several compelling findings.In our analysis of non-GPT models, specifically Qwen2.5 and Llama 3.  The significance of these results is amplified by A-MEM's exceptional computational efficiency.Our approach requires only 1,200-2,500 tokens, compared to the substantial 16,900 tokens needed by LoComo and MemGPT.This efficiency stems from two key architectural innovations: First, our novel agentic memory architecture creates interconnected memory networks through atomic notes with rich contextual descriptions, enabling more effective capture and utilization of information relationships.Second, our selective top-k retrieval mechanism facilitates dynamic memory evolution and structured organization.The effectiveness of these innovations is particularly evident in complex reasoning tasks, as demonstrated by the consistently strong Multi-Hop performance across all evaluation metrics.Besides, we also show the experimental results with different foundational models including DeepSeek-R1-32B [11], Claude 3.0 Haiku [2] and Claude 3.5 Haiku [3].</p>
<p>A.4 Memory Analysis</p>
<p>In addition to the memory visualizations of the first two dialogues shown in the main text, we present additional visualizations in Fig. 5 that demonstrate the structural advantages of our agentic memory system.Through analysis of two dialogues sampled from long-term conversations in LoCoMo [22], we observe that A-MEM (shown in blue) consistently produces more coherent clustering patterns compared to the baseline system (shown in red).This structural organization is particularly evident in Dialogue 2, where distinct clusters emerge in the central region, providing empirical support for the effectiveness of our memory evolution mechanism and contextual description generation.In contrast, the baseline memory embeddings exhibit a more scattered distribution, indicating that memories lack structural organization without our link generation and memory evolution components.These visualizations validate that A-MEM can autonomously maintain meaningful memory structures through its dynamic evolution and linking mechanisms.</p>
<p>A.5 Hyperparameters setting</p>
<p>All hyperparameter k values are presented in Table 8.For models that have already achieved state-of-the-art (SOTA) performance with k=10, we maintain this value without further tuning.Tags should be determined by the content of these characteristic of these memories, which can be used to retrieve them later and categorize them.</p>
<p>All the above information should be returned in a list format according to the sequence:
[[new_memory],[neighbor_memory_1], ..</p>
<p>.[neighbor_memory_n]]</p>
<p>These actions can be combined.Return your decision in JSON format with the following structure: {{ "should_evolve": true/false, "actions": ["strengthen", "merge", "prune"], "suggested_connections": ["neighbor_memory_ids"], "tags_to_update": ["tag_1",..."tag_n"], "new_context_neighborhood": ["new context",...,"new context"], "new_tags_neighborhood": [["tag_1",...,"tag_n"],...["tag_1",...,"tag_n"]], }} • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>B.4 Examples of Q/</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
<p>Answer: [Yes] Justification: We provide the code link in the abstract.</p>
<p>Guidelines:</p>
<p>• The answer NA means that paper does not include experiments requiring code.Justification: We don't discuss this aspect because we provide only the memory system for LLM agents.Different LLM agents may create varying societal impacts, which are beyond the scope of our work.</p>
<p>Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Answer: [NA] Justification: N/A</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<ol>
<li>Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</li>
</ol>
<p>Answer: [Yes] Justification: Their contribution has already been properly acknowledged and credited.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [NA] Justification: N/A Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and research with human subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Answer: [NA] Justification: N/A Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.for what should or should not be described.</p>
<p>Dialogue 2 Figure 4 :
24
T-SNE Visualization of Memory Embeddings Showing More Organized Distribution with A-MEM (blue) Compared to Base Memory (red) Across Different Dialogues.Base Memory represents A-MEM without link generation and memory evolution.</p>
<p>2 . 1
21
Memory for LLM Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2.2 Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Methodolodgy 3.1 Note Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3.2 Link Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3.3 Memory Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3.4 Retrieve Relative Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiment 4.1 Dataset and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4.3 Empricial Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4.4 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4.5 Hyperparameter Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4.6 Scaling Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4.7 Memory Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Baselines Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .A.2 Evaluation Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .A.3 Comparison Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .A.4 Memory Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .A.5 Hyperparameters setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B Prompt Templates and Examples B.1 Prompt Template of Note Construction . . . . . . . . . . . . . . . . . . . . . . . .B.2 Prompt Template of Link Generation . . . . . . . . . . . . . . . . . . . . . . . . .B.3 Prompt Template of Memory Evolution . . . . . . . . . . . . . . . . . . . . . . .B.4 Examples of Q/A with A-MEM . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p>
<p>Figure 5 :
5
Figure 5: T-SNE Visualization of Memory Embeddings Showing More Organized Distribution with A-MEM (blue) Compared to Base Memory (red) Across Different Dialogues.Base Memory represents A-MEM without link generation and memory evolution.</p>
<p>For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.</p>
<p>Table 1 :
1
Experimental results on LoCoMo dataset of QA tasks across five categories (Multi Hop, Temporal, Open Domain, Single Hop, and Adversial) using different methods.Results are reported in F1 and BLEU-1 (%) scores.The best performance is marked in bold, and our proposed method A-MEM (highlighted in gray) demonstrates competitive performance across six foundation language models.
CategoryAverageModelMethodMulti HopTemporalOpen DomainSingle HopAdversialRankingTokenF1BLEUF1BLEUF1BLEUF1BLEUF1BLEU F1 BLEU Length4o-miniLOCOMO READAGENT MEMORYBANK MEMGPT25.02 19.75 18.41 14.77 12.04 11.16 40.36 29.05 69.23 68.75 2.4 9.15 6.48 12.60 8.87 5.31 5.12 9.67 7.66 9.81 9.02 4.2 5.00 4.77 9.68 6.99 5.56 5.94 6.61 5.16 7.36 6.48 4.8 26.65 17.72 25.52 19.44 9.15 7.44 41.04 34.34 43.29 42.73 2.42.4 4.2 4.8 2.416,910 643 432 16,977GPTA-MEM LOCOMO27.02 20.09 45.85 36.67 12.14 12.00 44.65 37.06 50.03 49.47 1.2 28.00 18.47 9.09 5.78 16.47 14.80 61.56 54.19 52.61 51.13 2.01.2 2.02,520 16,9104oREADAGENT MEMORYBANK14.61 6.499.95 4.694.16 2.473.19 2.438.84 6.438.37 5.3012.46 10.29 8.28 7.106.81 4.426.13 3.674.0 5.04.0 5.0805 569MEMGPT30.36 22.83 17.29 13.18 12.24 11.87 60.16 53.35 34.96 34.25 2.42.416,987A-MEM32.86 23.76 39.41 31.23 17.10 15.84 48.43 42.97 36.35 35.53 1.61.61,216LOCOMO9.056.554.254.049.918.5011.158.6740.38 40.23 3.43.416,910Qwen2.51.5b 3bREADAGENT MEMORYBANK 11.14 6.61 MEMGPT 10.44 A-MEM 18.23 11.94 24.32 19.74 16.48 14.31 23.63 19.23 46.00 43.26 1.0 4.93 2.55 2.51 5.31 12.24 10.13 7.54 5.42 27.32 4.6 8.25 4.46 2.87 8.05 6.21 13.42 11.01 36.76 34.00 2.6 7.61 4.21 3.89 13.42 11.64 9.56 7.34 31.51 28.90 3.4 LOCOMO 4.61 4.29 3.11 2.71 4.55 5.97 7.03 5.69 16.95 14.81 3.2 READAGENT 2.47 1.78 3.01 3.01 5.57 5.22 3.25 2.51 15.78 14.01 4.2 MEMORYBANK 3.60 3.39 1.72 1.97 6.63 6.58 4.11 3.32 13.07 10.30 4.24.6 2.6 3.4 1.0 3.2 4.2 4.2752 284 16,953 1,300 16,910 776 298MEMGPT5.074.312.942.957.047.107.265.5214.47 12.39 2.42.416,961A-MEM12.579.0127.59 25.077.127.2817.23 13.12 27.91 25.15 1.01.01,137LOCOMO11.259.187.386.8211.90 10.38 12.86 10.50 51.89 48.27 3.43.416,910Llama 3.21b 3bREADAGENT MEMORYBANK 13.18 10.03 5.96 5.12 MEMGPT 9.19 6.96 A-MEM 19.06 11.71 17.80 10.28 17.55 14.67 28.51 24.13 58.81 54.28 1.0 1.93 2.30 12.46 11.17 7.75 6.03 44.64 40.15 4.6 7.61 6.27 15.78 12.94 17.30 14.03 52.61 47.53 2.0 4.02 4.79 11.14 8.24 10.16 7.68 49.75 45.11 4.0 LOCOMO 6.88 5.77 4.37 4.40 10.65 9.29 8.37 6.93 30.25 28.46 2.8 READAGENT 2.47 1.78 3.01 3.01 5.57 5.22 3.25 2.51 15.78 14.01 4.2 MEMORYBANK 6.19 4.47 3.49 3.13 4.07 4.57 7.61 6.03 18.65 17.05 3.24.6 2.0 4.0 1.0 2.8 4.2 3.2665 274 16,950 1,376 16,910 461 263MEMGPT5.323.992.682.725.645.544.323.5121.45 19.37 3.83.816,956A-MEM17.44 11.74 26.38 19.50 12.53 11.83 28.14 23.87 42.04 40.60 1.01.01,126</p>
<p>Table 2 :
2
[16]arison of different memory mechanisms across multiple evaluation metrics on DialSim[16].Higher scores indicate better performance, with A-MEM showing superior results across all metrics.
MethodF1BLEU-1 ROUGE-L ROUGE-2 METEOR SBERT SimilarityLoCoMo2.553.132.750.901.6415.76MemGPT 1.181.070.960.420.958.54A-MEM3.453.373.543.602.0519.51</p>
<p>Table 3 :
3
An ablation study was conducted to evaluate our proposed method against the GPT-4o-mini base model.The notation 'w/o' indicates experiments where specific modules were removed.The abbreviations LG and ME denote the link generation module and memory evolution module, respectively.
CategoryMethodMulti HopTemporalOpen DomainSingle HopAdversialF1BLEU-1F1BLEU-1F1BLEU-1F1BLEU-1F1BLEU-1w/o LG &amp; ME 9.657.0924.5519.487.776.7013.2810.3015.3218.02w/o ME21.3515.1331.2427.3110.1310.8539.1734.7044.1645.33A-MEM27.0220.0945.8536.6712.1412.0044.6537.0650.0349.47score of 3.45 (a 35% improvement over LoCoMo's 2.55 and 192% higher than MemGPT's 1.</p>
<p>Table 4 :
4
Comparison of memory usage and retrieval time across different memory methods and scales.
Memory Size MethodMemory Usage (MB) Retrieval Time (µs)A-MEM1.460.31 ± 0.301,000MemoryBank [39] 1.460.24 ± 0.20ReadAgent [17]1.4643.62 ± 8.47A-MEM14.650.38 ± 0.2510,000MemoryBank [39] 14.650.26 ± 0.13ReadAgent [17]14.65484.45 ± 93.86A-MEM146.481.40 ± 0.49100,000MemoryBank [39] 146.480.78 ± 0.26ReadAgent [17]146.486,682.22 ± 111.63A-MEM1464.843.70 ± 0.741,000,000MemoryBank [39] 1464.841.91 ± 0.31ReadAgent [17]1464.84120,069.68 ± 1,673.39</p>
<p>Table 5 :
5
Experimental results on LoCoMo dataset of QA tasks across five categories (Multi Hop, Temporal, Open Domain, Single Hop, and Adversial) using different methods.Results are reported in ROUGE-2 and ROUGE-L scores, abbreviated to RGE-2 and RGE-L.The best performance is marked in bold, and our proposed method A-MEM (highlighted in gray) demonstrates competitive performance across six foundation language models.
CategoryModelMethodMulti HopTemporalOpen DomainSingle HopAdversialRGE-2 RGE-L RGE-2 RGE-L RGE-2 RGE-L RGE-2 RGE-L RGE-2 RGE-L4o-miniLOCOMO READAGENT MEMORYBANK MEMGPT9.64 2.47 1.18 10.5823.92 9.45 5.43 25.602.01 0.95 0.52 4.7618.09 13.12 9.64 25.223.40 0.55 0.97 0.7611.58 5.76 5.77 9.1426.48 2.99 1.64 28.4440.20 9.92 6.63 42.2460.46 6.66 4.55 36.6269.59 9.79 7.35 43.75GPTA-MEM LOCOMO10.61 11.5325.86 30.6521.39 1.6844.27 8.173.42 3.2112.09 16.3329.50 45.4245.18 63.8642.62 45.1350.04 52.674oREADAGENT MEMORYBANK3.91 1.8414.36 7.360.43 0.363.96 2.290.52 2.138.58 6.854.75 3.0213.41 9.354.24 1.226.81 4.41MEMGPT11.5530.184.6615.833.2714.0243.2762.7528.7235.08A-MEM12.7631.719.8225.046.0916.6333.6750.3130.3136.34LOCOMO1.399.240.004.683.4210.593.2511.1535.1043.61Qwen2.51.5b 3bREADAGENT MEMORYBANK MEMGPT A-MEM LOCOMO READAGENT MEMORYBANK0.74 1.51 1.16 4.88 0.49 0.08 0.437.14 11.18 11.35 17.94 4.83 4.08 3.760.10 0.14 0.00 5.88 0.14 0.00 0.052.81 5.39 7.88 27.23 3.20 1.96 1.613.05 1.80 2.87 3.44 1.31 1.26 0.2412.63 8.44 14.62 16.87 5.38 6.19 6.321.47 5.07 2.18 12.32 1.97 0.73 1.037.88 13.72 9.82 24.38 6.98 4.34 4.2220.73 29.24 23.96 36.32 12.66 7.35 9.5527.82 36.95 31.69 46.60 17.10 10.64 13.41MEMGPT0.695.550.053.171.907.902.057.3210.4614.39A-MEM2.9112.428.1127.741.517.518.8017.5721.3927.98LOCOMO2.5111.480.448.251.6913.062.9413.0039.8552.74Llama 3.21b 3bREADAGENT MEMORYBANK MEMGPT A-MEM LOCOMO READAGENT MEMORYBANK0.53 2.96 1.82 4.82 0.98 2.47 1.836.49 13.57 9.91 19.31 7.22 1.78 6.960.00 0.23 0.06 1.84 0.03 3.01 0.254.62 10.53 6.56 20.47 4.45 3.01 3.415.47 4.01 2.13 5.99 2.36 5.07 0.4314.29 18.38 11.36 18.49 11.39 5.22 4.431.19 6.41 2.00 14.82 2.85 3.25 2.738.03 17.66 10.37 29.78 8.45 2.51 7.8334.52 41.15 38.59 46.76 25.47 15.78 14.6445.55 53.31 50.31 60.23 30.26 14.01 18.59MEMGPT0.725.390.112.850.615.741.454.4216.6221.47A-MEM6.0217.627.9327.975.3813.0016.8928.5535.4842.25
scores.When examining GPT-based models, our results reveal an interesting pattern.While LoComo and MemGPT demonstrate strong capabilities in Open Domain and Adversarial tasks, A-MEM shows remarkable superiority in Multi-Hop reasoning tasks.Using GPT-4o-mini, A-MEM achieves a ROUGE-L score of 44.27 in Multi-Hop tasks, more than doubling LoComo's 18.09.This significant advantage maintains consistency across other metrics, with METEOR scores of 23.43 versus 7.61 and SBERT scores of 70.49 versus 52.30.</p>
<p>Table 6 :
6
Experimental results on LoCoMo dataset of QA tasks across five categories (Multi Hop, Temporal, Open Domain, Single Hop, and Adversial) using different methods.Results are reported in METEOR and SBERT Similarity scores, abbreviated to ME and SBERT.The best performance is marked in bold, and our proposed method A-MEM (highlighted in gray) demonstrates competitive performance across six foundation language models.
CategoryModelMethodMulti HopTemporalOpen DomainSingle HopAdversialMESBERTMESBERTMESBERTMESBERTMESBERT4o-miniLOCOMO READAGENT MEMORYBANK MEMGPT15.81 5.46 3.42 15.7947.97 28.67 21.71 49.337.61 4.76 4.07 13.2552.30 45.07 37.58 61.538.16 3.69 4.21 4.5935.00 26.72 23.71 32.7740.42 8.01 5.81 41.4057.78 26.78 20.76 58.1963.28 8.38 6.24 39.1671.93 15.20 13.00 47.24GPTA-MEM16.3649.4623.4370.498.3638.4842.3259.3845.6453.264o</p>
<p>Table 7 :
7
Experimental results on LoCoMo dataset of QA tasks across five categories (Multi Hop, Temporal, Open Domain, Single Hop, and Adversial) using different methods.Results are reported in F1 and BLEU-1 (%) scores with different foundation models.
CategoryMethodMulti HopTemporalOpen DomainSingle HopAdversialF1BLEU-1F1BLEU-1F1BLEU-1F1BLEU-1F1BLEU-1DeepSeek-R1-32BLOCOMO8.586.484.794.3512.9612.5210.728.2021.4020.23MEMGPT8.286.255.454.9710.979.0911.349.0330.7729.23A-MEM15.0210.6414.6411.0114.8112.8215.3712.3027.9227.19Claude 3.0 HaikuLOCOMO4.563.330.820.592.863.223.563.243.463.42MEMGPT7.656.361.651.267.416.648.607.297.667.37A-MEM19.2814.6916.6512.2311.859.6134.7230.0535.9934.87Claude 3.5 HaikuLOCOMO 11.348.213.292.693.793.5814.0112.577.377.12MEMGPT8.276.553.992.764.714.4816.5214.895.645.45A-MEM29.7023.1931.5427.5311.429.4742.6037.4113.6512.71</p>
<p>Table 8 :
8
Selection of k values in retriever across specific categories and model choices.If choose to strengthen the connection, which memory should it be connected to?Can you give the updated tags of this memory?1.2 If choose to update neighbor, you can update the context and tags of these memories based on the understanding of these memories.
ModelMulti Hop Temporal Open Domain Single Hop AdversialGPT-4o-mini4040505040GPT-4o4040505040Qwen2.5-1.5b1010101010Qwen2.5-3b1010501010Llama3.2-1b1010101010Llama3.2-3b1020101010</p>
<p>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.•Whileweencourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).•Theinstructionsshouldcontain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.•Theauthors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.•At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).•Providingasmuchinformation as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.6.Experimental setting/detailsQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the The answer NA means that the paper does not include experiments.•The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [No] Justification: The experiments utilize the API of Large Language Models.Multiple calls will significantly increase costs.Guidelines: • The answer NA means that the paper does not include experiments.• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
results? 9. Code of ethicsAnswer: [Yes] Question: Does the research conducted in the paper conform, in every respect, with theJustification: We cover all the details in the paper. NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Guidelines: Answer: [NA]Justification: N/A Guidelines: • 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? • 8. Experiments compute resources Answer: [No]Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]
Justification: It could be found in the experimental part.Guidelines:• The answer NA means that the paper does not include experiments.•Thepaper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.•</p>
<p>• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.15.Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA] Justification: N/A Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.16.Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)
Answer: [NA]Justification: N/AGuidelines:•
https://github.com/ollama/ollama
https://github.com/BerriAI/litellm
B Prompt Templates and Examples B.1 Prompt Template of Note ConstructionThe prompt template in Note Construction: P s1 Generate a structured analysis of the following content by: 1. Identifying the most salient keywords (focus on nouns, verbs, and key concepts) 2. Extracting core themes and contextual elements 3. Creating relevant categorical tags Format the response as a JSON object: { "keywords": [ // several specific, distinct keywords that capture key concepts and terminology // Order from most to least important // Don't include keywords that are the name of the speaker or time // At least three keywords, but don't be too redundant.], "context": // one sentence summarizing: // -Main topic/domain // -Key arguments/points // -Intended audience/purpose , "tags": [ // several broad categories/themes for classification // Include domain, format, and type tags // At least three tags, but don't be too redundant.] } Content for analysis:B.2 Prompt Template of Link GenerationThe prompt template in Link Generation: P s2 You are an AI memory evolution agent responsible for managing and evolving a knowledge base.Analyze the the new memory note according to keywords and context, also with their several nearest neighbors memory.The new memory context: {context} content: {content} keywords: {keywords} The nearest neighbors memories: {nearest_neighbors_memories} Based on this information, determine: Should this memory be evolved?Consider its relationships with other memories.NeurIPS Paper ChecklistThe checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact.Do not remove the checklist: The papers not including the checklist will be desk rejected.The checklist should follow the references and follow the (optional) supplemental material.The checklist does NOT count towards the page limit.Please read the checklist guidelines carefully for information on how to answer these questions.For each question in the checklist:• You should answer [Yes] , [No] , or [NA] .• [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.• Please provide a short (1-2 sentence) justification right after your answer (even for NA).The checklist answers are an integral part of your paper submission.They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers.You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used").In general, answering "[No] " or "[NA] " is not grounds for rejection.While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate.All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix.If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.IMPORTANT, please:• Delete this instruction block, but keep the section heading "NeurIPS Paper Checklist", • Keep the checklist subsection headings, questions/answers and guidelines below.• Do not modify the questions and only use the provided macros for your answers.ClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer: [Yes] Justification: The abstract and the introduction summarizes our main contributions.Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes] Justification: This paper cover a section of the limiations.Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory assumptions and proofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: N/A Guidelines:• The answer NA means that the paper does not include theoretical results.• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.Experimental result reproducibilityQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer:[Yes]Justification: Both code and datasets are available.Guidelines:• The answer NA means that the paper does not include experiments.
How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking. Sönke Ahrens, 2017AmazonSecond Edition</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic. Anthropic, Mar 2024. May 2025</p>
<p>Claude 3.5 sonnet model card addendum. Anthropic, 2025. May 2025AnthropicTechnical report</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, arXiv:2310.115112023arXiv preprint</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, International conference on machine learning. PMLR2022</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems. 362023</p>
<p>mem0: The memory layer for ai agents. Khant Dev, Singh Taranjeet, 2024</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Advanced rag techniques: An illustrated overview. I Ilin, 2023</p>
<p>Jihyoung Jang, Minseong Boo, Hyounghun Kim, arXiv:2310.13420Conversation chronicles: Towards diverse temporal and relational dynamics in multi-session conversations. 2023arXiv preprint</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2305.069832023arXiv preprint</p>
<p>David Kadavy, Digital Zettelkasten: Principles, Methods, &amp; Examples. Google BooksMay 2021</p>
<p>Dialsim: A real-time simulator for evaluating long-term multi-party dialogue understanding of conversational agents. Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, Edward Choi, arXiv:2406.131442024arXiv preprint</p>
<p>A human-inspired reading agent with gist memory of very long contexts. Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer, arXiv:2402.097272024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Ra-dit: Retrieval-augmented dual instruction tuning. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, arXiv:2310.013522023arXiv preprint</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Tian Prafulla K Choubey, Jason Lan, Huan Wu, Wang, arXiv:2402.15538A lightweight library for building and advancing task-oriented llm agent system. 2024arXiv preprint</p>
<p>Evaluating very long-term conversational memory of llm agents. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang, arXiv:2402.177532024arXiv preprint</p>
<p>Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang, Aios: Llm agent operating system. arXiv e-prints. 20242403</p>
<p>Ret-llm: Towards a general read-write memory for large language models. Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze, arXiv:2305.143222023arXiv preprint</p>
<p>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics112019</p>
<p>'smolagents': a smol library to build great agentic systems. Aymeric Roucher, Albert Villanova Del Moral, Thomas Wolf, Leandro Von Werra, Erik Kaunismäki, 2025</p>
<p>Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2305.152942023arXiv preprint</p>
<p>From commands to prompts: Llm-based semantic file system for aios. Zeru Shi, Kai Mei, Mingyu Jin, Yongye Su, Chaoji Zuo, Wenyue Hua, Wujiang Xu, Yujie Ren, Zirui Liu, Mengnan Du, arXiv:2410.118432024arXiv preprint</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, arXiv:2212.105092022arXiv preprint</p>
<p>Enhancing large language model with self-controlled memory framework. Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv:2304.133432023arXiv preprint</p>
<p>Openhands: An open platform for ai software developers as generalist agents. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, arXiv:2407.167412024arXiv preprint</p>
<p>Learning to filter context for retrieval-augmented generation. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig, arXiv:2311.083772023arXiv preprint</p>
<p>Llm-powered autonomous agents. lilianweng.github.io. Lilian Weng, Jun 2023</p>
<p>Beyond goldfish memory: Long-term open-domain conversation. Xu, arXiv:2107.075672021arXiv preprint</p>
<p>Chain-of-note: Enhancing robustness in retrieval-augmented language models. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu, arXiv:2311.092102023arXiv preprint</p>
<p>Augmentation-adapted retriever improves generalization of language models as generic plug-in. Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu, arXiv:2305.173312023arXiv preprint</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>            </div>
        </div>

    </div>
</body>
</html>
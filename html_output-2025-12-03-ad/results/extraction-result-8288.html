<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8288 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8288</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8288</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-267406499</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.01521v2.pdf" target="_blank">K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Strategic reasoning is a complex yet essential capability for intelligent agents. It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments. Unlike static reasoning tasks, success in these contexts depends on anticipating other agents' beliefs and actions while continuously adjusting strategies to achieve individual goals. LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others' perspectives and adapt to changing environments. Inspired by the Level-K framework from game theory and behavioral economics, which extends reasoning from simple reactions to structured strategic depth, we propose a novel framework:"K-Level Reasoning with Large Language Models (K-R)."This framework employs recursive mechanisms to enable LLMs to achieve varying levels of strategic depth, allowing agents to form higher order beliefs - beliefs about others' beliefs. We validate this framework through rigorous testing on four testbeds: two classical game theory problems and two social intelligence tasks. The results demonstrate the advantages of K-R in strategic reasoning. Our work presents the first recursive implementation of strategic depth in large language models (LLMs). It establishes a foundation for future research into theory of mind and strategic reasoning in LLMs.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8288.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8288.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt4-32k, used as primary experimental base model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale transformer-based language model used as the primary base model for nearly all experiments in the paper; evaluated across multiple reasoning methods (Direct, CoT, Persona, Reflect, Refine, PCoT, and K-Level Reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt4-32k)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI LLM (GPT-4) with a 32k context window; used with temperature=0.7 and top-p=0.9 for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct (Standard Prompting)', 'Chain-of-Thought (CoT, zero-shot)', 'Persona Prompting', 'Reflexion (Reflect)', 'Self-Refine (Refine)', 'Prediction Chain-of-Thought (PCoT)', 'K-Level Reasoning (K-R)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Direct: single-shot final-answer prompt. CoT: zero-shot chain-of-thought prompting to elicit stepwise reasoning. Persona: prompts that add a 'game expert' persona to influence strategy. Reflect (Reflexion): summarize experience from episodes then apply reinforcement-style verbal feedback. Refine (Self-Refine): iterative self-feedback loop where an extra LLM critique adjusts the decision before finalizing. PCoT: explicitly predict opponents' actions first, then decide. K-R: recursive opponent-modeling sessions that simulate opponents at lower K-levels and then reason the agent's optimal action given those simulated opponent actions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Full experimental comparison across seven reasoning approaches (Direct, CoT, Persona, Reflect, Refine, PCoT, K-R) on the same tasks; additional ablations include varying K in K-R (K=2 vs K=3) and cross-model comparisons (e.g., K-R with GPT-3.5 vs Direct with GPT-4); experiments vs programmatic opponents and statistical significance tests (t-tests at p=0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Custom multi-agent strategic reasoning benchmarks: Guessing 0.8 of the Average (G0.8A), Survival Auction Game (SAG), Negotiation (NEG), and SOTOPIA-hard social intelligence benchmark; also Prisoner's Dilemma Nash-equilibrium experiments and LLM-vs-programmatic strategy tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>G0.8A (Win Rate averages across opponents): Direct 0.16 ±0.18, CoT 0.32 ±0.19, Persona 0.41 ±0.18, Reflect 0.24 ±0.18, Refine 0.37 ±0.17, PCoT 0.40 ±0.25, K-R 0.65 ±0.17 (Table 1). SAG (Average survival round): Direct 6.51 ±1.82, CoT 7.44 ±1.55, Persona 7.59 ±1.95, Reflect 4.82 ±0.82, Refine 7.73 ±1.21, PCoT 6.17 ±1.29, K-R 9.01 ±1.21 (Table 2). NEG (win rate averages across opponents): Direct 38.96% ±2.53, CoT 54.35% ±0.50, Persona 37.92% ±5.84, Reflect 52.19% ±1.73, Refine 53.70% ±4.41, PCoT 53.80% ±4.34, K-R 59.08% ±2.20 (Table 3). SOTOPIA-hard: K-R yields improvements versus baselines but not always statistically significant; K-R performance comparable to GPT-4 baseline on many SOTOPIA dimensions (Table 4). Token/compute: K-R has substantially higher token consumption (input tokens scale with K, opponents and rounds) and linear growth with number of simulated opponents; overall K-R is more computationally expensive than most baselines (Table 12, Appendix G).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>K-R produces more accurate and less random opponent predictions than PCoT from round 1 and converges faster (Figure 3). K-R establishes higher-order beliefs: strategic depth of GPT-4 rises from 0.25 (Direct) to 1.89 (K-R with K=3) approaching human newspaper reader levels (1.91). Reflect tends to underperform in dynamic environments due to stale experience summaries; Refine is more adaptive than Reflect but still inferior to K-R because it lacks explicit opponent modeling. K-R sometimes overthinks vs very shallow opponents (K=3 decreased win rate vs Direct in G0.8A), indicating diminishing returns/overanticipation at extreme K. K-R's performance gains are robust across different base models but come with increased token/computation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The paper concludes that diverse reasoning strategies were compared and that the K-Level Reasoning (recursive opponent modeling) method outperforms single-session prediction baselines (PCoT) and other refinement/self-feedback methods on strategic multi-agent tasks, improving opponent prediction accuracy, win rates, survival, and measured strategic depth; however, K-R increases computational cost and can suffer from overthinking when K is too large relative to opponents' depths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8288.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8288.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (used with K-R in cross-model tests)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller (weaker) OpenAI LLM used to test whether improved reasoning methodology (K-R) can compensate for weaker base models; experiments compare K-R[GPT-3.5] to other methods implemented on GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 (smaller/fewer capabilities than GPT-4); used as an experimental base to apply K-R and evaluate cross-model generality.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['K-Level Reasoning (K-R)', 'Direct (baseline comparison reported from GPT-4 experiments for context)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>K-R applied identically but using GPT-3.5 as the inference engine for the recursive opponent-simulation and final reasoning steps; compared against GPT-4 baselines to measure methodology vs model scale tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (method-level diversity tested across different base model strengths)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Cross-model comparison: K-R implemented with GPT-3.5 (K-R[GPT-3.5]) and compared against other reasoning approaches implemented with GPT-4 (Direct[GPT-4], etc.) to assess whether a stronger reasoning method on a weaker model can outperform a weaker reasoning method on a stronger model (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>G0.8A and SAG (same multi-agent tasks as main experiments), programmatic opponent tests and ablation comparisons vs GPT-4 baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reports that K-R[GPT-3.5] outperforms Direct[GPT-4] on average performance across tasks (specific per-task numeric breakdowns are referenced in Table 7 in the paper). The qualitative statement: K-R significantly enhances the strategic abilities of GPT-3.5 such that it can beat or match some GPT-4 baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Applying K-R to a weaker base model (GPT-3.5) yields substantial improvements and demonstrates that improved reasoning methodology can sometimes outweigh base-model capacity; K-R's opponent-modeling mechanism is the primary driver of gains rather than only base-model strength.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>K-R is an effective reasoning framework that can upgrade weaker LLMs; methodological improvements (recursive opponent modeling) can surpass straightforward gains from simply using a larger model when facing strategic multi-agent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8288.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8288.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B / LLaMA2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-7B (LLaMA2-7B family; open-source models tested)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source smaller models (≈7B) evaluated to test the generality of K-R; used in programmatic-strategy experiments and SOTOPIA tests to show K-R's applicability beyond closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B / LLaMA2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLMs in the ~7B parameter size class used to evaluate K-R in lower-resource settings and on SOTOPIA-hard / G0.8A programmatic tests.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct (baseline)', 'K-Level Reasoning (K-R)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Direct prompting baseline and K-R recursive opponent-modeling applied to these open-source models in programmatic strategy combat settings; experiments show consistent win-rate improvements when K-R is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (comparison between single-method Direct and K-R as a more complex multi-step approach)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>LLM-Programmatic Strategy Combat: tested multiple programmatic opponent strategies (0-Level Fix/Var, MonoTrend Fix/Var, LastBids Fix/Var) and measured win rates for Direct vs K-R on LLaMA-7B / LLaMA2-7B (Table 9 and Appendix E/Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>G0.8A against programmatic strategy opponents and SOTOPIA-hard (some tests), to probe robustness across open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reports that all tested models (including LLaMA variants) benefit from K-R with increased win rates; examples: in programmatic-strategy G0.8A K-R achieved substantial gains vs LastBids and MonoTrend patterns where other baselines struggled (Table 9 shows K-R often highest; Appendix E summarizes LLaMA-7B improvements). Exact per-model per-strategy numeric tables are in the paper (Table 9, Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>K-R improves adaptability to dynamic opponent behavior for open-source models; even small models that perform poorly under Direct prompting can attain notable improvements under K-R, though absolute performance still trails stronger closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>K-R generalizes to open-source LLMs and yields stable improvements in interactive multi-agent tasks, indicating the method's broad applicability across model sizes and vendors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8288.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8288.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-Level Reasoning with Large Language Models (K-R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel recursive framework introduced in this paper that implements k-level (level-k) strategic thinking by repeatedly simulating opponents at lower thinking levels and then reasoning the agent's best response; designed to instantiate higher-order beliefs in LLMs for multi-agent strategic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>K-Level Reasoning (K-R framework, applied with GPT-4/GPT-3.5/LLaMA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Algorithmic framework (not a base LLM) that repeatedly calls an LLM to (1) anticipate opponents' actions at levels m=1..k via fresh inference sessions conditioned on public history, and (2) compute the agent's optimal action given those simulated opponent actions; K parameter controls recursive depth.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Recursive opponent modeling (level-k simulation)', 'Final decision reasoning conditioned on simulated opponent actions']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Anticipation step: for each opponent j, run LLM(E_t, H_j) to get level-1 prediction and recursively run LLM(E_t, H_j, predictions_of_others) to generate higher-level predictions up to k-1; Reasoning step: run LLM(E_t, H_i, predicted_actions_of_others_at_k-1) to produce final action. K-R thereby couples in-context learning with explicit recursive simulation to build higher-order beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (K-R is a distinct multi-step method compared against a diverse set of single-session and refinement approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct head-to-head comparisons with multiple reasoning styles (Direct, CoT, Persona, Reflect, Refine, PCoT); ablations include varying K (K=2 vs K=3) and cross-model implementations (GPT-4, GPT-3.5, LLaMA). Also compared to prediction-first baseline (PCoT) and to self-refinement baselines to isolate opponent-modeling effect (Figure 3, Table 7, Table 8, Appendix K).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same multi-agent tests: G0.8A, SAG, NEG, SOTOPIA-hard, Prisoner's Dilemma Nash-equilibrium experiments, and LLM-vs-programmatic-strategy tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>K-R produced the strongest aggregated results across benchmarks: G0.8A mean Win Rate 0.65 ±0.17 (best of tested methods); SAG average survival round 9.01 ±1.21 (best); NEG average win rate 59.08% ±2.20 (best). Prediction accuracy: K-R shows lower prediction deviation vs PCoT from round 1 and converges faster (Figure 3). Ablation on K: K=3 increased measured strategic depth to 1.89 (from 0.25 Direct), but K=3 can sometimes overthink vs Direct in some matchups (Table 8). Computational cost: K-R substantially increases token usage (input tokens scale with #opponents and K); K-R has higher token & runtime cost than Direct and many baselines (Appendix G, Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>K-R explicitly models opponents' hidden strategies leading to better adaptation to dynamic environments; operator studies show K-R forms more accurate higher-order beliefs and delivers broader action optimization (Figure 7). Self-Refine modifies own strategy but lacks opponent modeling and thus is less adaptive. PCoT's single-session prediction is less effective empirically than K-R's fresh recursive sessions for opponent simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>K-R is the first recursive implementation of level-k strategic depth in LLMs introduced by this paper; it produces significant empirical improvements in multi-agent strategic reasoning tasks by building higher-order beliefs, outperforming static prompting, CoT, persona-based prompting, self-refinement, reflexion, and prediction-first baselines — at the cost of higher computational/token usage and with potential overthinking at excessive K.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Unraveling in Guessing Games: An Experimental Study <em>(Rating: 1)</em></li>
                <li>SOTOPIA: Interactive evaluation for social intelligence in language agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8288",
    "paper_id": "paper-267406499",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (gpt4-32k, used as primary experimental base model)",
            "brief_description": "A large-scale transformer-based language model used as the primary base model for nearly all experiments in the paper; evaluated across multiple reasoning methods (Direct, CoT, Persona, Reflect, Refine, PCoT, and K-Level Reasoning).",
            "citation_title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt4-32k)",
            "model_description": "Closed-source OpenAI LLM (GPT-4) with a 32k context window; used with temperature=0.7 and top-p=0.9 for experiments.",
            "reasoning_methods": [
                "Direct (Standard Prompting)",
                "Chain-of-Thought (CoT, zero-shot)",
                "Persona Prompting",
                "Reflexion (Reflect)",
                "Self-Refine (Refine)",
                "Prediction Chain-of-Thought (PCoT)",
                "K-Level Reasoning (K-R)"
            ],
            "reasoning_methods_description": "Direct: single-shot final-answer prompt. CoT: zero-shot chain-of-thought prompting to elicit stepwise reasoning. Persona: prompts that add a 'game expert' persona to influence strategy. Reflect (Reflexion): summarize experience from episodes then apply reinforcement-style verbal feedback. Refine (Self-Refine): iterative self-feedback loop where an extra LLM critique adjusts the decision before finalizing. PCoT: explicitly predict opponents' actions first, then decide. K-R: recursive opponent-modeling sessions that simulate opponents at lower K-levels and then reason the agent's optimal action given those simulated opponent actions.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Full experimental comparison across seven reasoning approaches (Direct, CoT, Persona, Reflect, Refine, PCoT, K-R) on the same tasks; additional ablations include varying K in K-R (K=2 vs K=3) and cross-model comparisons (e.g., K-R with GPT-3.5 vs Direct with GPT-4); experiments vs programmatic opponents and statistical significance tests (t-tests at p=0.05).",
            "task_or_benchmark": "Custom multi-agent strategic reasoning benchmarks: Guessing 0.8 of the Average (G0.8A), Survival Auction Game (SAG), Negotiation (NEG), and SOTOPIA-hard social intelligence benchmark; also Prisoner's Dilemma Nash-equilibrium experiments and LLM-vs-programmatic strategy tests.",
            "performance_results": "G0.8A (Win Rate averages across opponents): Direct 0.16 ±0.18, CoT 0.32 ±0.19, Persona 0.41 ±0.18, Reflect 0.24 ±0.18, Refine 0.37 ±0.17, PCoT 0.40 ±0.25, K-R 0.65 ±0.17 (Table 1). SAG (Average survival round): Direct 6.51 ±1.82, CoT 7.44 ±1.55, Persona 7.59 ±1.95, Reflect 4.82 ±0.82, Refine 7.73 ±1.21, PCoT 6.17 ±1.29, K-R 9.01 ±1.21 (Table 2). NEG (win rate averages across opponents): Direct 38.96% ±2.53, CoT 54.35% ±0.50, Persona 37.92% ±5.84, Reflect 52.19% ±1.73, Refine 53.70% ±4.41, PCoT 53.80% ±4.34, K-R 59.08% ±2.20 (Table 3). SOTOPIA-hard: K-R yields improvements versus baselines but not always statistically significant; K-R performance comparable to GPT-4 baseline on many SOTOPIA dimensions (Table 4). Token/compute: K-R has substantially higher token consumption (input tokens scale with K, opponents and rounds) and linear growth with number of simulated opponents; overall K-R is more computationally expensive than most baselines (Table 12, Appendix G).",
            "qualitative_findings": "K-R produces more accurate and less random opponent predictions than PCoT from round 1 and converges faster (Figure 3). K-R establishes higher-order beliefs: strategic depth of GPT-4 rises from 0.25 (Direct) to 1.89 (K-R with K=3) approaching human newspaper reader levels (1.91). Reflect tends to underperform in dynamic environments due to stale experience summaries; Refine is more adaptive than Reflect but still inferior to K-R because it lacks explicit opponent modeling. K-R sometimes overthinks vs very shallow opponents (K=3 decreased win rate vs Direct in G0.8A), indicating diminishing returns/overanticipation at extreme K. K-R's performance gains are robust across different base models but come with increased token/computation cost.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "The paper concludes that diverse reasoning strategies were compared and that the K-Level Reasoning (recursive opponent modeling) method outperforms single-session prediction baselines (PCoT) and other refinement/self-feedback methods on strategic multi-agent tasks, improving opponent prediction accuracy, win rates, survival, and measured strategic depth; however, K-R increases computational cost and can suffer from overthinking when K is too large relative to opponents' depths.",
            "uuid": "e8288.0",
            "source_info": {
                "paper_title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (used with K-R in cross-model tests)",
            "brief_description": "A smaller (weaker) OpenAI LLM used to test whether improved reasoning methodology (K-R) can compensate for weaker base models; experiments compare K-R[GPT-3.5] to other methods implemented on GPT-4.",
            "citation_title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI GPT-3.5 (smaller/fewer capabilities than GPT-4); used as an experimental base to apply K-R and evaluate cross-model generality.",
            "reasoning_methods": [
                "K-Level Reasoning (K-R)",
                "Direct (baseline comparison reported from GPT-4 experiments for context)"
            ],
            "reasoning_methods_description": "K-R applied identically but using GPT-3.5 as the inference engine for the recursive opponent-simulation and final reasoning steps; compared against GPT-4 baselines to measure methodology vs model scale tradeoffs.",
            "reasoning_diversity": "both (method-level diversity tested across different base model strengths)",
            "reasoning_diversity_experimental_setup": "Cross-model comparison: K-R implemented with GPT-3.5 (K-R[GPT-3.5]) and compared against other reasoning approaches implemented with GPT-4 (Direct[GPT-4], etc.) to assess whether a stronger reasoning method on a weaker model can outperform a weaker reasoning method on a stronger model (Table 7).",
            "task_or_benchmark": "G0.8A and SAG (same multi-agent tasks as main experiments), programmatic opponent tests and ablation comparisons vs GPT-4 baselines.",
            "performance_results": "Paper reports that K-R[GPT-3.5] outperforms Direct[GPT-4] on average performance across tasks (specific per-task numeric breakdowns are referenced in Table 7 in the paper). The qualitative statement: K-R significantly enhances the strategic abilities of GPT-3.5 such that it can beat or match some GPT-4 baselines.",
            "qualitative_findings": "Applying K-R to a weaker base model (GPT-3.5) yields substantial improvements and demonstrates that improved reasoning methodology can sometimes outweigh base-model capacity; K-R's opponent-modeling mechanism is the primary driver of gains rather than only base-model strength.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "K-R is an effective reasoning framework that can upgrade weaker LLMs; methodological improvements (recursive opponent modeling) can surpass straightforward gains from simply using a larger model when facing strategic multi-agent tasks.",
            "uuid": "e8288.1",
            "source_info": {
                "paper_title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-7B / LLaMA2-7B",
            "name_full": "LLaMA-7B (LLaMA2-7B family; open-source models tested)",
            "brief_description": "Open-source smaller models (≈7B) evaluated to test the generality of K-R; used in programmatic-strategy experiments and SOTOPIA tests to show K-R's applicability beyond closed-source models.",
            "citation_title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B / LLaMA2-7B",
            "model_description": "Open-source LLMs in the ~7B parameter size class used to evaluate K-R in lower-resource settings and on SOTOPIA-hard / G0.8A programmatic tests.",
            "reasoning_methods": [
                "Direct (baseline)",
                "K-Level Reasoning (K-R)"
            ],
            "reasoning_methods_description": "Direct prompting baseline and K-R recursive opponent-modeling applied to these open-source models in programmatic strategy combat settings; experiments show consistent win-rate improvements when K-R is applied.",
            "reasoning_diversity": "both (comparison between single-method Direct and K-R as a more complex multi-step approach)",
            "reasoning_diversity_experimental_setup": "LLM-Programmatic Strategy Combat: tested multiple programmatic opponent strategies (0-Level Fix/Var, MonoTrend Fix/Var, LastBids Fix/Var) and measured win rates for Direct vs K-R on LLaMA-7B / LLaMA2-7B (Table 9 and Appendix E/Table 10).",
            "task_or_benchmark": "G0.8A against programmatic strategy opponents and SOTOPIA-hard (some tests), to probe robustness across open-source models.",
            "performance_results": "Paper reports that all tested models (including LLaMA variants) benefit from K-R with increased win rates; examples: in programmatic-strategy G0.8A K-R achieved substantial gains vs LastBids and MonoTrend patterns where other baselines struggled (Table 9 shows K-R often highest; Appendix E summarizes LLaMA-7B improvements). Exact per-model per-strategy numeric tables are in the paper (Table 9, Table 10).",
            "qualitative_findings": "K-R improves adaptability to dynamic opponent behavior for open-source models; even small models that perform poorly under Direct prompting can attain notable improvements under K-R, though absolute performance still trails stronger closed-source models.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "K-R generalizes to open-source LLMs and yields stable improvements in interactive multi-agent tasks, indicating the method's broad applicability across model sizes and vendors.",
            "uuid": "e8288.2",
            "source_info": {
                "paper_title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "K-R",
            "name_full": "K-Level Reasoning with Large Language Models (K-R)",
            "brief_description": "A novel recursive framework introduced in this paper that implements k-level (level-k) strategic thinking by repeatedly simulating opponents at lower thinking levels and then reasoning the agent's best response; designed to instantiate higher-order beliefs in LLMs for multi-agent strategic reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "K-Level Reasoning (K-R framework, applied with GPT-4/GPT-3.5/LLaMA variants)",
            "model_description": "Algorithmic framework (not a base LLM) that repeatedly calls an LLM to (1) anticipate opponents' actions at levels m=1..k via fresh inference sessions conditioned on public history, and (2) compute the agent's optimal action given those simulated opponent actions; K parameter controls recursive depth.",
            "reasoning_methods": [
                "Recursive opponent modeling (level-k simulation)",
                "Final decision reasoning conditioned on simulated opponent actions"
            ],
            "reasoning_methods_description": "Anticipation step: for each opponent j, run LLM(E_t, H_j) to get level-1 prediction and recursively run LLM(E_t, H_j, predictions_of_others) to generate higher-level predictions up to k-1; Reasoning step: run LLM(E_t, H_i, predicted_actions_of_others_at_k-1) to produce final action. K-R thereby couples in-context learning with explicit recursive simulation to build higher-order beliefs.",
            "reasoning_diversity": "both (K-R is a distinct multi-step method compared against a diverse set of single-session and refinement approaches)",
            "reasoning_diversity_experimental_setup": "Direct head-to-head comparisons with multiple reasoning styles (Direct, CoT, Persona, Reflect, Refine, PCoT); ablations include varying K (K=2 vs K=3) and cross-model implementations (GPT-4, GPT-3.5, LLaMA). Also compared to prediction-first baseline (PCoT) and to self-refinement baselines to isolate opponent-modeling effect (Figure 3, Table 7, Table 8, Appendix K).",
            "task_or_benchmark": "Same multi-agent tests: G0.8A, SAG, NEG, SOTOPIA-hard, Prisoner's Dilemma Nash-equilibrium experiments, and LLM-vs-programmatic-strategy tests.",
            "performance_results": "K-R produced the strongest aggregated results across benchmarks: G0.8A mean Win Rate 0.65 ±0.17 (best of tested methods); SAG average survival round 9.01 ±1.21 (best); NEG average win rate 59.08% ±2.20 (best). Prediction accuracy: K-R shows lower prediction deviation vs PCoT from round 1 and converges faster (Figure 3). Ablation on K: K=3 increased measured strategic depth to 1.89 (from 0.25 Direct), but K=3 can sometimes overthink vs Direct in some matchups (Table 8). Computational cost: K-R substantially increases token usage (input tokens scale with #opponents and K); K-R has higher token & runtime cost than Direct and many baselines (Appendix G, Table 12).",
            "qualitative_findings": "K-R explicitly models opponents' hidden strategies leading to better adaptation to dynamic environments; operator studies show K-R forms more accurate higher-order beliefs and delivers broader action optimization (Figure 7). Self-Refine modifies own strategy but lacks opponent modeling and thus is less adaptive. PCoT's single-session prediction is less effective empirically than K-R's fresh recursive sessions for opponent simulation.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "K-R is the first recursive implementation of level-k strategic depth in LLMs introduced by this paper; it produces significant empirical improvements in multi-agent strategic reasoning tasks by building higher-order beliefs, outperforming static prompting, CoT, persona-based prompting, self-refinement, reflexion, and prediction-first baselines — at the cost of higher computational/token usage and with potential overthinking at excessive K.",
            "uuid": "e8288.3",
            "source_info": {
                "paper_title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Unraveling in Guessing Games: An Experimental Study",
            "rating": 1,
            "sanitized_title": "unraveling_in_guessing_games_an_experimental_study"
        },
        {
            "paper_title": "SOTOPIA: Interactive evaluation for social intelligence in language agents",
            "rating": 2,
            "sanitized_title": "sotopia_interactive_evaluation_for_social_intelligence_in_language_agents"
        }
    ],
    "cost": 0.018221249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning
17 Oct 2024</p>
<p>Yadong Zhang 
East China Normal University</p>
<p>Microsoft Research Asia</p>
<p>Shaoguang Mao shaoguang.mao@microsoft.com 
Microsoft Research Asia</p>
<p>Tao Ge 
Microsoft Research Asia</p>
<p>Xun Wang 
Microsoft Research Asia</p>
<p>Yan Xia 
Microsoft Research Asia</p>
<p>Man Lan 
East China Normal University</p>
<p>Furu Wei 
Microsoft Research Asia</p>
<p>Zelai Xu 
Chao Yu 
Fei Fang 
Yu Wang 
Shunyu Yao 
Dian Yu 
Jeffrey Zhao 
Izhak Shafran 
Thomas L Griffiths 
Yuan Cao 
Karthik 2023 Narasimhan 
Yadong Zhang 
Wenshan Wu 
Ting Song 
Llm 
Qinlin Zhao 
Jindong Wang 
Yixuan Zhang 
Yiqiao Jin 
Kaijie Zhu 
Hao Chen 
Xing 2023 Xie 
Xuhui Zhou 
Hao Zhu 
Leena Mathur 
Ruohong Zhang 
Haofei Yu 
Zhengyang Qi 
Louis-Philippe Morency 
Yonatan Bisk </p>
<p>Wenqi Zhang
Ke Tang, Hai Wu, Mengna Wang, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. 2024aYongliang Shen</p>
<p>K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning
17 Oct 20247A88FA9E39D7336F8D48EEBD188D9628arXiv:2402.01521v2[cs.CL]
Strategic reasoning is a complex yet essential capability for intelligent agents.It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments.Unlike static reasoning tasks, success in these contexts depends on anticipating other agents' beliefs and actions while continuously adjusting strategies to achieve individual goals.LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others' perspectives and adapt to changing environments.Inspired by the Level-K framework 1 from game theory and behavioral economics, which extends reasoning from simple reactions to structured strategic depth, we propose a novel framework: "K-Level Reasoning with Large Language Models (K-R)."This framework employs recursive mechanisms to enable LLMs to achieve varying levels of strategic depth, allowing agents to form higher order beliefs-beliefs about others' beliefs.We validate this framework through rigorous testing on four testbeds: two classical game theory problems and two social intelligence tasks.The results demonstrate the advantages of K-R in strategic reasoning.Our work presents the first recursive implementation of strategic depth in large language models (LLMs).It establishes a foundation for future research into theory of mind and strategic reasoning in LLMs.</p>
<p>Introduction</p>
<p>Strategic reasoning-decision-making in multiparticipant environments-presents unique challenges for Large Language Models (LLMs) and LLM agents (Zhang et al., 2024b).In these settings, In first-level thinking, agents respond directly to the environment.In secondlevel thinking, agents consider the first-level thinking of others.This process continues iteratively, with agents forming higher order beliefs based on assumptions about others' thoughts.</p>
<p>agents must respond to the actions of others while adapting to dynamic environments.They also need to align their decisions with their own goals during these interactions.Strategic reasoning is essential for intelligent agents and is widely applied in realworld tasks, such as investment, business strategy making (Zhao et al., 2023), negotiation (Hua et al., 2023), and policy-making (Li et al., 2024).</p>
<p>Effective strategic reasoning relies on understanding others' perspectives and anticipating their strategies.While there are some research efforts on LLMs' strategic reasoning, most methods rely on static prompting (Fu et al., 2023;Xu et al., 2023b).This typically involves instructing the model to account for others' beliefs and decisions during its own decision-making process in the prompt.However, these approaches fall short in enabling LLMs to form true higher order beliefs-beliefs about what others believe, and lack the flexibility needed for deeper strategic reasoning.</p>
<p>K-level thinking (Figure 1) (Nagel, 1995;Cui et al., 2021), a classical concept in behavioral economics and game theory, categorizes reasoning into varying depths of strategic thought.It involves not only predicting others' actions but also considering Please choose an integer between 1 and 100.The player whose chosen number is closest to 0.8 * the average of all chosen numbers wins the round.Let's start the 1st round.The daily available water can meet the needs of only one resident.An auction is conducted, and the highest bidder will obtain today's water supply.In today's bidding, placed the highest bid of $70, securing the water resources for the day.</p>
<p>+2 and resets the number of days thirsty =0 .Other residents have their HP deducted based on the number of days they have been thirsty ( = -).</p>
<p>A new day!All residents receive a daily salary of $100.Today, the daily available water can meet the needs of only one resident ... their beliefs about one's actions, and even further layers of recursive thinking.</p>
<p>Inspired by K-level thinking, we propose a novel strategic reasoning framework termed "K-Level Reasoning with LLMs (K-R)."K-R organizes reasoning into hierarchical levels and employs a recursive mechanism to integrate varying strategic depth into decision-making.Specifically, it involves: 1) recursively anticipating others' actions at varying levels of strategic depth with environmental context and historical public information, and 2) reasoning the optimal action based on these anticipations.To the best of our knowledge, this is the first approach to implementing varying levels of strategic depth in LLMs using a recursive mechanism and enables deeper reasoning in LLM agents through an algorithmic framework.</p>
<p>We validate this framework through rigorous testing on four testbeds: two classical game theory problems and two social intelligence tasks.The game theory problems includes Guessing 0.8 of the Average (Figure 2 left) and Survival Auction Game (Mao et al., 2023) (Figure 2 middle).The social intelligence tasks includes Negotiation (Cao et al., 2018) (Figure 2 right) and SOTOPIA benchmark (Zhou et al., 2024).These settings serve as microcosms of the complex decision-making processes involved in strategic reasoning.Through extensive experiments, we demonstrate that our framework significantly outperforms existing reasoning methods and flexibly achieves varying levels of strategic depth.In addition to empirical evidence, we provide a theoretical analysis highlighting the benefits of K-R.We show that, leveraging the in-context learning capabilities of LLMs, K-R can effectively model opponents' behavior using accumulated public and available opponent information.</p>
<p>Furthermore, we align the strategic depth of LLMs with human participants (Nagel, 1995;Bosch-Domenech et al., 2002).Using human as anchors, we observe that the K-R significantly enhances the strategic depth of LLMs from 0.25 to 1.89.Notably, when K=3, the strategic depth (1.89) of the LLM closely approaches that of financial newspaper readers (1.91).This strongly indicates that K-R establishes higher order beliefs in LLMs for strategic reasoning.</p>
<p>The contributions of this work are as follows:</p>
<p>• We introduce K-R, a novel framework that extends k-level thinking to LLMs, enabling flexible strategic reasoning at varying depths through a recursive mechanism.• We conduct extensive evaluations, including game theory and social intelligence problems, demonstrating that K-R significantly outperforms existing methods in terms of flexibility and effectiveness, across both closed-source and open-source models.• We provide an in-depth analysis of K-R, confirming its ability to build higher order beliefs and enhance strategic reasoning.This lays a foundation for future research in theory of mind and strategic reasoning in LLMs.</p>
<p>2 K-Level Reasoning with Large Language Models</p>
<p>Methodology</p>
<p>Strategic reasoning requires considering both the decision context and the possible actions of other participants.We employ a multi-round normal form multi-participant game to introduce the proposed method.In this setting, an agent's decision-making process is formalized as follows: each agent i selects an action a t i from a set A t i at timestep t.The payoff for agent i, resulting from the collective action profile A t = (a t 1 , a t 2 , ..., a t N ) and environment E t , is denoted as U i (E t , A t ).</p>
<p>At k = 1, agents decide based on environment E t without strategic anticipation:
a t,1 i = arg max a i ∈A t i E<a href="1">U i (E t , a i )</a>
At higher level thinking (k ≥ 2), agent i simulates other agents operating at level k − 1 and adjusts their strategy accordingly2 :
a t,k i = arg max a i ∈A t i E[U i (E t , a i , ât,k−1 −i )] (2)
where ât,k−1
−i
are the predicted actions of other agents based on their k − 1 level reasoning.</p>
<p>We propose a novel strategic reasoning framework with recursive mechanisms, termed "K-Level Reasoning with Large Language Models (K-R)," involving 1) recursively anticipating the actions ât,k −i of others at different thinking levels using environment contexts and historical public information, followed by 2) reasoning the optimal action a t,k i based on anticipation of others' actions.The K-Level Reasoning process is formulated as follows:</p>
<p>1) Anticipation:
ât,m j = LLM(E t , H t j ) if m = 1 LLM(E t , H t j , ât,m−1 −j ) if m &gt; 1 (3)
where
H t j = {(E 1 , a 1 j ), (E 2 , a 2 j ), ..., (E t−1 , a t−1 j
)} represents public historical data of agent j, and m denotes the specified thinking level.</p>
<p>2) Reasoning:
a t,k i = LLM(E t , H t i , ât,k−1 −i )(4)
Algorithm 1 outlines the implementation of K-R.This recursive method enables flexible and progressively deeper strategic reasoning (1, 2, ..., k, k + 1, ...), thereby enhancing higher order belief in LLM agents.
for each agent j ̸ = i do 6: ât,k−1 j = K_REASONING(j, k − 1) 7: end for 8: return LLM(E t , H t i , {â t,k−1 j | j ̸ = i}) 9: end if 10: a t,K i = K_REASONING(i, K) 11: return a t,K i</p>
<p>Theoretical Analysis</p>
<p>This section discusses the benefits from K-R from a theoretical perspective.We utilize the in-context learning capabilities of LLMs to effectively model opponents' behavior.Suppose agent j's decisionmaking process follows a hidden strategy θ * j .Thus, agent j's decision-making can be expressed as:
P (a t j | E t , θ * j )(5)
The in-context learning of LLMs can be formally defined as implicit Bayesian inference (Xie et al., 2021); therefore, given the environment E t , the next action prediction conditioned on H t j is:
P (a t j | E t , H t j ) = P (a t j | E t , θ j )P (θ j | H t j )dθ j (6) As t → ∞,
by the law of large numbers and properties of Bayesian updating, the posterior distribution concentrates around the true parameter θ * j :
P (θ j | H t j ) → δ(θ j − θ * j ) (7)
where δ is the Dirac delta function.Therefore,
P (a t j | E t , θ j )P (θ j | H t j )dθ j → P (a t j | E t , θ * j ) (8)
This implies that as the number of interactions increases, K-R can more accurately predict opponents' behavior.</p>
<p>It is also worth noting that interaction data cannot be infinite, and in-context learning is related to the performance of large language models (LLMs).Therefore, we empircally validate these hypotheses and reasoning in Section 5.2.</p>
<p>Experiments: Game Theory</p>
<p>To fairly compare the strategic reasoning capabilities of LLMs, we first adopt two widely used game theory settings.These controlled, well-defined game theory problems provide a robust assessment of LLMs' performance, with detailed setups outlined in Appendix B.</p>
<p>Task Definition and Metrics</p>
<p>3.1.1Guessing 0.8 of the Average (G0.8A)G0.8A (Figure 2 Left) is a classic game theory problem introduced by Alain Ledoux (Ledoux, 1981).It involves 10-round games where each player selects a number between 1 and 100.The objective is to choose a number closest to 80% of the group's average choice.The key idea is to guess how others will estimate the average and decide the number to submit.This concept is also illustrated in the Keynesian Beauty Contest (Keynes, 1936).This game mirrors the challenge of anticipating collective behavior in financial markets, where investors must predict not only the value of an asset but also how others will value it in the future.</p>
<p>The performance of the agent is evaluated using the Win Rate.Specifically, the Win Rate is calculated based on the wins achieved by the agent in individual round, rather than an entire game episode.</p>
<p>Survival Auction Game (SAG)</p>
<p>SAG (Figure 2 Middle) is derived from the Water Allocation Challenge proposed in (Mao et al., 2023).Each resident's goal is to survive a 10-day drought period by bidding for water resources and maintaining health points above zero.If a player successfully bids for water, they gain health points; otherwise, they lose health points.This integration of the auction system with the health points mechanism creates a dynamic environment where players must balance health and finances.</p>
<p>We use Average Survival Round measures the mean round in which a player remains active in the game.</p>
<p>Base Techniques</p>
<p>We adapt a variety of approaches, originally from traditional reasoning and agent benchmarks.These base techniques include:</p>
<p>Standard Prompting (Direct): This is the conventional prompting method in which the LLM generates the final answer (Action) in response to the given game setting prompt.</p>
<p>Chain-of-Thought (CoT) (Wei et al., 2022): We employ the zero-shot Chain-of-Thought reasoning method (Kojima et al., 2022).</p>
<p>Persona Prompting (Persona) (Deshpande et al., 2023): This technique modifies the standard prompting process by incorporating a "Game Expert" persona to enhance the reasoning capabilities of the LLM.</p>
<p>Reflexion (Reflect) (Shinn et al., 2023): This method refers to language agents with verbal reinforcement learning and has been adapted for dynamic tasks.Detailed modifications are explained in K.</p>
<p>Self-Refine (Refine) (Madaan et al., 2023): This is a multi-round iterative reasoning approach where an additional LLM offers comments and adjustments prior to reaching a final decision.The distinctions between Self-Refine and Reflect are elaborated upon in the Appendix I.</p>
<p>Prediction Chain of Thought (PCoT): This strong baseline diverges from CoT by requiring the LLM to explicitly predict opponents' actions before making decisions.Unlike K-Level Reasoning, which involves a recursive approach, PCoT focuses on direct prediction based on context.</p>
<p>For implementation details and specific examples, please refer to Appendix K.</p>
<p>Experimental Settings</p>
<p>We established a controllable environment and distinguished between two roles: the player (primary focus) and the opponents.The player is equipped with a specific method, while all opponents use another reasoning approach.This well-defined setting allows for a clearer comparison of reasoning capabilities between methods.</p>
<p>In G0.8A and SAG, there is one player and four opponents for each game.Experiments for each setting are repeated 10 times and have passed the significance test (Appendix H), and each experiment consists of a 10-round game.</p>
<p>All methods in main experiments were implemented using GPT-4 (Achiam et al., 2023) (gpt4-32k), with the temperature set at 0.7 and the top-p set at 0.9.We also conducted experiments with open-source LLMs, Details of which are provided in Appendix E. Unless specified otherwise, the level of thinking in K-Level Reasoning is set to K=2.</p>
<p>Results</p>
<p>To distinguish between "Player" and "Opponent" in the table, the headers for Player (bold) and Opponents (italics) are formatted accordingly.Table 1 presents Win Rate of players utilizing different methods against various opponents in the G0.8A game.Notably, the K-R method demonstrates a superior Win Rate of 0.65, significantly exceeding the win rates of the other strategies.Table 2 provides insights into the Average Survival Round of players across different auction game strategies in SAG, with the K-R method again standing out.The K-R method achieves an average survival round of 9.01, considerably higher than all other methods.</p>
<p>The experiment result underscores the effectiveness of the K-R method in enhancing player strategy, suggesting its strategic superiority in the context of this game.Its effectiveness lies in its ability to anticipate opponent moves, outperforming other prompting methods.</p>
<p>The performance of Reflect did not demonstrate the effectiveness of the reasoning method.We hypothesize that this is due to the fact that, in dynamic environments, Reflect on the experiences summarized from the previous round (Shinn et al., 2023) may not be applicable to the subsequent round of the game.Furthermore, in both games, Refine did not show an advantage over CoT and was significantly lower than K-R.This is because Refine involves adjustments based on one's own strategy.However, these adjustments do not explicitly consider the hidden strategies of the opponent's behavior, rendering them inapplicable against opponents employing different strategies.</p>
<p>Experiments: Social Intelligence</p>
<p>We then evaluate K-R in two social intelligence benchmarks to assess its performance in more openended realistic scenarios.Compared to the abstract and theoretical settings of Game Theory, these scenarios involve richer contextual backgrounds and complicated goal pursuits, which better demonstrate the value of LLM-based agents in practical applications, such as in chatbots and strategic decision making.(Cao et al., 2018;Duan et al., 2024) is an open-ended and realistic task.In this setting, two agents are presented with three types of items: peppers, cherries, and strawberries.Each agent has private utility values for these items and must negotiate to allocate the public item pool.</p>
<p>The agent who secures more utility upon reaching an agreement wins the game, and we calculated the Win Rate to assess the performance of different agents.</p>
<p>SOTOPIA Benchmark</p>
<p>SOTOPIA (Zhou et al., 2024) is an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence.It includes a variety of social scenarios, and each scenario includes a context background, and private social goals of each agent.Meanwhile, each agent has a character profiles which consists of name, gender, personality, occupation, etc.</p>
<p>For each episode, agents are scored at the end of the interaction along each of seven dimensions in SOTOPIA-Eval, including Goal Completion (GOAL), Believability (BEL), Knowledge (KNO), Secret (SEC), Relationship (REL), Social Rules (SOC), Financial and Material Benefits (FIN).</p>
<p>Experimental Settings</p>
<p>We employed the majority of the reasoning approaches introduced in Section 3.2 as baseline models for comparison.</p>
<p>In NEG, the experiments followed the settings from (Cao et al., 2018;Duan et al., 2024).There is one player and one opponent for each game.We test the performance of the Baselines and K-Level Reasoning in 100 repeated independent games.To eliminate positional advantages, we swapped the positions of each player for each setting.To ensure the reliability, three trials were conducted, and the results are reported as averages with standard deviation.</p>
<p>Meanwhile, We adhered to the SOTOPIA-hard (Zhou et al., 2024) setup comprising a total of 100 episodes, which is commonly found to be challenging for LLMs, and utilize a fixed GPT-4o based agent as partner.Additionally, to evaluate the agents' scores, we utilized GPT-4 as the assessment model, as it has been determined by SOTOPIA benchmark (Zhou et al., 2024) to serve as a reliable serves as a proxy for human judgments in evaluating model performance across most dimensions and for human performance on the GOAL dimension.The results presented in Table 3 and Table 4 illustrate the effectiveness of the K-Level Reasoning in the context of NEG and SOTOPIA-hard settings, respectively.</p>
<p>Results</p>
<p>In NEG, the K-R method demonstrates a notable win rate of 59.08%, positioning it significantly above the average win rates achieved by other methods.This indicates that, in most cases, the proposals generated through K-Level Reasoning are more advantageous to itself, as well as suggesting a tendency to accept the opponent's proposals when the  KNO [0-10] 6.05 6.05 6.25 6.25 5.88 5.53 5.33 6.12  SEC [-10-0] 0.00 -0.05 0.00 0.00 -0.28 -0.25 -0.18 0.00  SOC [-10-0] -0.05 0.00 -0.05 0.00 -0.70 -0.72 -0.64 0.00  FIN [-5-5] 0.90 0.78 0.80 0.72 0.38 0.35 -0.08 0.75 GOAL [0-10] 6.35 6.60 6.15 6.47 5.35 5.40 4.95 6 perceived benefits are substantial.</p>
<p>The results from SOTOPIA reveal several intriguing findings.Firstly, while K-R demonstrates some improvement compared to other methods, the results are not statistically significant.We hypothesize that this may be due to the inherent tendency of GPT-4 based models to assign higher scores to responses generated by GPT-4 based agents.Notably, we observed that employing agents based on LLaMA 3.1 70B with K-R can lead to significant performance enhancements.Meanwhile, the overall metrics indicate that K-R achieves performance levels comparable to those of the GPT-4 model, highlighting K-R's potential in the realm of social intelligence.</p>
<p>Discussions</p>
<p>Does K-R Efficiently Establish a Higher</p>
<p>Order Belief in LLMs?As a classic game theory issue, the G0.8A problem has garnered significant research interest across various disciplines.We reference the experimental results of the classic research among human participants (Nagel, 1995;Bosch-Domenech et al., 2002) as anchor points and present the average decisions made by the K-Level Reasoning method (GPT-4) in the first round.Through this comparison, we can observe the relative relationship between human cognitive levels and LLMs under different reasoning methods.The specific calculation method on strategic depth is described in Appendix C. The performance of humans and LLMs is shown in the Table 5 and Table 6 From these observations, we can conclude that even when employing SOTA models, the strategic depth of GPT-4 under Direct Prompt (0.25) cannot compete with that of lower-strategic-capability undergraduate students in laboratory settings (0.87).Furthermore, the K-Level reasoning approach significantly enhances the reasoning depth of large language models, increasing it from 0.25 to 1.89, and the strategic depth of the large language model (1.89) approaches that of a group of financial newspaper readers (1.91) when K=3.</p>
<p>K-Level Reasoning Leads to More</p>
<p>Accurate Predictions About Opponents  Since K-R involves an intermediate step of modeling the opponent's behavior, we examine the progression of prediction accuracy.Figure 3 illustrates the prediction deviation between K-R and PCoT in G0.8A.K-R exhibits higher prediction accuracy than PCoT from Round 1, starting with more precise and less random predictions.Moreover, the predictions converge quickly and become highly accurate in the second half of the game.This trend highlights the LLM's increasing proficiency in understanding higher order belief with more gameplay context.Essentially, K-R instantiates new sessions to compute the opponent's future actions.This approach leverages the in-context learning capabilities of LLMs more effectively than PCoT's prediction process (as theoretically discussed in Section 2.2).As a result, K-R achieves better prediction accuracy.</p>
<p>Better Reasoning Methodology vs. Stronger Foundation Model</p>
<p>There is a consensus that LLMs trained with more data and possessing larger parameter sizes demonstrate stronger reasoning capabilities.We explore whether K-Level Reasoning can significantly enhance the strategic reasoning abilities of relatively weaker LLMs.To investigate, we conducted experiments comparing the performance of K-R with GPT-3.5 (K-R[GPT-3.5])against other reasoning methods based on GPT-4.All experiments were repeated 10 times.From the results in Table 7, we observe that K-R[GPT-3.5]outperforms the standard prompting method of GPT-4 (Direct[GPT4]) from average performance.Furthermore, when competing against opponents using reasoning methods on GPT-4, K-R[GPT-3.5]demonstrates remarkable capabilities.K-R, with its excellent restoration of the rival's perspective, enhances the LLM's ability in competitive environments.Additionally, we compared the performance of the open-source model LLAMA2-7B with GPT-3.5/4 in Appendix E, finding that K-R significantly enhances reasoning in interactive contexts across different LLMs.
Opponent Direct K-R Direct K-R Direct K-R Direct K-R [GPT-4] [GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4] [GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4] Direct 0.</p>
<p>The Deeper Thinking Level, the Better</p>
<p>Strategic Performance?K-R models opponents' thinking processes recursively.We examine how thinking level affect reasoning outcomes by comparing K-R[K=2] and K-R[K=3] in two games.The results, detailed in Table 8, reveal the impact of increased thinking level.Against the Direct method (first-level thinking), K-R[K=3] showed a decreased win rate in G0.8A but maintained performance in SAG, suggesting possible overthinking.However, K-R[K=3] improved significantly against K-R[K=2] in both games.It suggests that the key factor in K-R is the relative depth of thought compared to the opponent.A onelevel deeper approach offers a strategic advantage, but advancing two levels may lead to diminishing returns due to over-anticipation.In interactive environments, identifying opponents' thinking levels is difficult.Adapting to varying levels and using K-Level Reasoning for deeper analysis is a valuable direction for future research.
Opponent Direct K-R[K=2] K-R[K=3] Direct K-R[K=2] K-R[K=3] Direct 0.
Additionally, a higher thinking level with the recursive prompting implementation increases computational cost.The computational cost of K-R is thoroughly discussed in Appendix G.</p>
<p>6 Related Work</p>
<p>Reasoning with LLMs</p>
<p>Large Language Models (LLMs) excel in diverse complex reasoning tasks, such as mathematical (Miao et al., 2021;Patel et al., 2021), common sense (Talmor et al., 2022;Bhakthavatsalam et al., 2021), and symbolic reasoning (Srivastava et al., 2022;Suzgun et al., 2022).A notable reasoning approach involves breaking down complex questions into a series of intermediate steps, a technique known as the Chain-of-Thought (CoT) method (Wei et al., 2022;Kojima et al., 2022).Subsequently, some works have emerged to extend CoT, with innovations like Tree of Thought (ToT) (Yao et al., 2023), Graph of Thought (GoT) (Besta et al., 2023) and Skeleton-of-thought (Ning et al., 2023).Besides, approaches like Self-Refine (Madaan et al., 2023) and Reflexion (Shinn et al., 2023) enhance CoT's consistency by having LLMs review and refine their responses.Moreover, recent research has revealed that integrating persona information into LLMs significantly improves their reasoning processes (Deshpande et al., 2023).A series of studies (Fu et al., 2023;Wang et al., 2023) have been conducted to incorporate more persona information, aiming to enhance the rationality and knowledge ability of the LLM reasoning process.These methods have been applied to various static tasks, but have not been adequately evaluated in dynamic problems (multi-agent environment) to validate their efficacy in reasoning capabilities.</p>
<p>Strategic Reasoning within Multiple Agent System</p>
<p>Dynamic problems arise when multiple participants are involved in multi-round interactions.One key factor is the simultaneous interactions of multiple participants with the environment  (Zhou et al., 2024;Hua et al., 2023), economic simulations (Zhao et al., 2023;Li et al., 2023), game theory (Duan et al., 2024;Xu et al., 2023a), and game playing (Ma et al., 2023;Xu et al., 2023b).To enhance the performance of LLMs in strategic reasoning scenarios, researchers have utilized the concepts of Theory of Mind (ToM) (Gandhi et al., 2023;Guo et al., 2023) and Reinforcement Learning (Xu et al., 2023c;Zhang et al., 2024a) to optimize the reasoning processes of LLMs.These approaches involve prompting LLMs to recognize the intricacies of strategic tasks, like our proposed Prediction Chain-of-Thought baseline.However, our experimental results indicate that this approach fails to establish a clear cognitive hierarchy necessary for recursive and deeper strategic thinking.</p>
<p>Conclusion</p>
<p>This paper represents a significant stride in understanding and enhancing the strategic reasoning capabilities of LLMs.We propose "K-Level Reasoning with LLMs."This innovative approach leverages recursive mechanisms to achieve varying thinking level within LLMs, enabling them to engage in deeper strategic thinking.Through extensive experiments, we validate the advantage offered by this method.It establishes a foundation for future research into theory of mind and strategic reasoning in LLMs.</p>
<p>Limitations</p>
<p>We validate the effectiveness of the K-Level Reasoning framework from two perspectives: game theory and social intelligence.While our experimental results provide substantial evidence supporting the framework's validity, further research is necessary to explore the performance of large language models (LLMs) in few-shot agent modeling (He et al., 2016) across various environments, strategic factors, and action sets.</p>
<p>Additionally, K-R predicts opponents' most likely behavior by initiating a new LLM inference session.The recursive mechanism employed to achieve varying levels of strategic depth inevitably increases computational cost.Appendix G provides a detailed discussion on how K-R relates to this rise in computational cost and compares it across different reasoning methods.Despite the increased demands, K-R outperforms other methods with comparable computational costs.</p>
<p>A Impact Statements</p>
<p>Our research introduces the K-Level Reasoning framework, designed to formulate strategies in dynamic, interactive and competitive scenarios by anticipating the reactions of adversaries, potentially users.Theoretically, this approach offers a novel perspective for understanding and optimizing decision-making processes.However, we recognize when the goal setting diverges from user interests, the application of K-Level Reasoning could result in manipulative behaviors by adapting to the predicted user's reactions.This risk is notably pronounced in scenarios designed to influence user decisions or behaviors, such as in recommendation systems, advertising placements, and content distribution on social media platforms.</p>
<p>Although K-Level Reasoning provides a potent powerful tool for strategic planning, interacting and reasoning, ethical considerations must be meticulously managed in its practical application.This ensures that the development and utilization of technology do not detrimentally impact user and societal interests.To this end, we advocate for heightened transparency, ensuring users have a comprehensive understanding and control over how their data is utilized.</p>
<p>B Game Setting</p>
<p>B.1 Guessing 0.8 of the Average Initial Setup: For each round, each player select a number between 1 and 100.The objective is to select a number that is closest to 80% of the group's average choice.Formally, each player i chooses a number n i , aiming for n i ≈ 0.8 × n, where n is the average of all chosen numbers.</p>
<p>Scoring and Continuation: A player scores a point if his/her chosen number is closest to 80% of the average number chosen by the group.If all players select the same number, no points are awarded for this round.Mathematically, the score for player i in in round t is given by s t i , which is 1 if |n i − 0.8 × n| is the minimum among all players, and 0 otherwise.</p>
<p>B.2 Survival Auction Game</p>
<p>Initial Setup: Players start with 8 health points, out of a maximum of 10.Every day, each player possesses a fixed income of $100.The daily water supply can only satisfy one resident's requirement.</p>
<p>Scoring and Continuation: Everyday, players engage in a daily auction to secure the necessary water resources, and the highest bidder wins.In case of a tie, the resources are not allocated to any player.If a player successfully bid the water resources, they will gain 2 health points; otherwise, they will lose health points equal to the number of consecutive days, denoted as n, during which they have not obtained water resources.Once a player's health points fall to 0 or below, they will be eliminated.The health point of player i on day t, denoted as h t i , is crucial in determining their survival and bidding strategy.</p>
<p>C Detailed Metric Computational Formulas</p>
<p>Win Rate is calculated based on the number of wins over game going, providing a measure of the overall ability.Prediction Accuracy evaluates the accuracy of player's predictions regarding rivals' future moves.In the G0.8A, it involves calculating the absolute difference between the player's predicted average and the actual average in each round:
Pred Acc = |Avg P red − Avg Actual | Num of Test(11)
In the SAG, the focus shifts to measuring the absolute error between the player's prediction of the highest bid by opponents and the actual highest bid made by them.
Pred Acc = |Max Bid P red − Max Bid Actual | Num of Test (12) Strategic Level
StrategicDepth(choice) = log α (choice/50) (13) Here, α represents the target value coefficient, and 50 represents the average of a random choice between 0 and 100, which is used to represent level-0 players.In the settings of [1][2], the parameter α is set to 2 3 .In our configuration, α is set to 0.8, which is the origin of the name G0.8A.</p>
<p>Introduction Prompt:</p>
<p>You are Alex and involved in a survive challenge.You are one of five players in the game.Everyone is required to choose an integer between 1 and 100 in each round.The player whose chosen number is closest to (0.8 * the average of all chosen numbers) wins the round without any HP deduction.All other players will have 1 HP deducted.But if all players choose the same number, their h ealth points are deducted together.</p>
<p>Round Start Prompt (Standard):</p>
<p>Ok, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round."</p>
<p>Notification</p>
<p>Congratulation Prompt (When Win):</p>
<p>You have successfully chosen the number closest to the target number, which is the average of all players' selected numbers multiplied by 0.8.As a result, you have won this round.All other players will now deduct 1 HP.</p>
<p>Warning Prompt (When Loss):</p>
<p>WARNING: You have lost 1 point of HP in this round!You now have only 9 points of health left.You are one step closer to dea th.</p>
<p>G0.8A Game Prompt</p>
<p>Introduction Prompt:</p>
<p>You are Alex and a resident living in W-Town.W Town is experiencing a rare drought.Every residents in Town W is ensuring their survival over a period of 10 days by acquiring the water resources.Attention, all W-Town residents, welcome to the Water Allocation Challenge!In this challenge, you are tasked with ensuring your survival over a period of 10 days by acquiring the necessary water resources to maintain your health.You will participate in daily auctions to bid for water resources to meet your individual needs.</p>
<p>Here are the game rules and settings: 1.You are one of five residents with same water requirements, budgets, and health points.2. Your goal is to survive until the end of the 10 days.3.Each resident has a maximum of 10 health points and starts with 8 health points.If your health points drop below or equal to 0, you will be considered dead and eliminated from the game!All your accumulated money will be reset to Zero! 4. Every day, you will bid on water resources to meet your needs.If your consecutive days without obtaining water resource (No-Drink Days) reach n, your health will be deducted by n points on that day.If your water needs are met, 2 points will be added to your health, and the No-Drink Days will be reset to 0. 5. Daily water resources can only meet the needs of one resident.6.Each resident has $100 daily income; 7. To allocate water resources, a sealed-bid auction will be conducted daily.Each resident submits a single bid for their entire water need.The resident with the highest bid is eligible to obtain water resources.8.If the highest bid results in a tie, no residents will have access to water.All bidding information will be made public after the allocation of water resources on the same day.Remember, the key to success is effective bidding and strategizing to ensure your survival.Good luck!!</p>
<p>Round Start Prompt (Standard):</p>
<p>Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units.Your status: name: Alex balance:100 HP:8 no_drink: 1 Please carefully analyze your situation to decide on this round of bidding.Remember, the most important thing is to SURVIVE!! Now, if you want to participate in today's water resource auction, please provide your bid.</p>
<p>Notification Result Prompt :</p>
<p>Thank you all for participating in Round 1.In this round, Alex bid 25, Bob bid 40, Cindy bid 40, David bid 30, Eric bid 60.Total water resource supply is 10.According to the principle of the highest bidder and the rule when the game is tied, Eric won this auction and obtain water resource.After allocation, all survival residents' information is as follows: name: Alex balance: 100 HP:7 no_drink: 2, name: Bob balance: 100 HP:7 no_drink:2 name :Cindy balance:100 HP:7 no_drink:2, name: David balance: 100 HP: :7 no_drink:2 name:Eric balance:40 HP:10 no_drink:1</p>
<p>Congratulation Prompt (When Win):</p>
<p>You have successfully won the bidding for today's water resources and restored 2 points of HP.</p>
<p>Warning Prompt (When Lose):</p>
<p>WARNING: You have lost 1point of HP in this round!You now have only 7 points of health left.You are one step closer to death.</p>
<p>Survival Auction Game Prompt</p>
<p>Bidding Result Parse Prompt:</p>
<p>By reading the conversation, extract the number chosen by player.Output format: number.If the player does not bid, Output: 0.</p>
<p>D Performance of Large Language Models Competing with Programmatic Strategies</p>
<p>In addition to using the LLM-LLM Combat comparison setting in Section 4, we have also designed a set of LLM-Programmatic Strategy Combat comparisons.In Player-Programmatic Strategy Combat setting, the "player" will be equipped with a specific reasoning method, while opponents will play according to programmatic strategic patterns, which will not be adjusted with the game going.This mode is to check different methods' adaptation to different predefined but fixed patterns.For the Player-Programmatic Player Combat Combat in G0.8A, the programmatic strategies include: 1) 0-Level (Fix): The characteristic of a 0-Level player is that their choice of strategy is uniform (Stahl and Wilson, 1995).We limit the choice space of the 0-Level Computer player to only 40.</p>
<p>2) 0-Level (Var): Modified from the 0-Level (Fix) strategy, the selection is sampled from a Gaussian distribution with a mean of 40 and a variance of 5.</p>
<p>3) MonoTrend (Fix): The numbers chosen by the computer players follow an arithmetic sequence with a decreasing common difference, and the common differences for all four computer players are the same.</p>
<p>4) MonoTrend (Var): The numbers chosen by the computer players follow an arithmetic sequence with a decreasing common difference, and the common differences for the four computer players are randomly generated from 1 to 5. 5) LastBids (Fix): The computer player chooses the target number from the previous round (selects 40 in the first round).</p>
<p>6) LastBids (Var): Modified from the LastBids strategy, the selection is sampled from a Gaussian distribution with a mean equal to the target number of the previous round and a variance of 5.</p>
<p>Overall, the dynamic changes of these three settings are LastBids &gt; MonoTrend &gt; 0-Level.We use these three programs to test the reasoning capability to adapt to and counter different patterns.</p>
<p>Table 9 reveals a significant trend in the performance of players against other approaches.The effectiveness of reasoning approaches decreases in the following order: 0-Level, MonoTrend, and LastBids.This pattern highlights a reduction in the efficacy of the LLM in more dynamic environments.We found that only K-Level Reasoning shows an advantage in LastBids (Fix), indicating that compared to K-Level Reasoning, previous reasoning methods on static problems lack observation and judgment of the opponent.Conversely, this also demonstrates that K-Level Reasoning can implicitly infer the behavior pattern of the opponent based on their historical actions.</p>
<p>Intriguingly, reasoning methods significantly influence the performance in dynamic settings.Methods like CoT and Self-Refine, traditionally favored in static reasoning, also demonstrate substantial improvements over the Standard Prompt approach.This finding underscores the necessity for more elaborate reasoning processes in dynamic decisionmaking, akin to static problem-solving scenarios.</p>
<p>E Open Source LLM with K-Level Reasoning</p>
<p>In addition to the experiments on GPT3.5 /GPT4, we conducted tests with an open-source smallerscale model, LLaMA-7B-Chat, in the "G0.8A" game.To clearly compare the performance of different LLMs, we adopted the LLM-Programmatic Strategy Combat setting described in Appendix B.</p>
<p>From the experimental results, we can see that: 1. Llama2-7B and GPT3.5, even including GPT4, perform poorly when competing against programmatic strategies using standard prompting, struggling even with some very simple strategies.It underscores the significance of our research; 2. For all tested models, applying k-level reasoning effectively enhanced the win rate, highlighting the stability of the improvement our proposed model brings to different base models.Interestingly, for stronger LLMs, applying K-R can achieve a higher relative improvement, we speculate that the enhancement is simultaneously derived from the opponent's action simulation and reasoning capabilities of the base model.
Methods Direct K-R[K=2] Direct K-R[K=2] Direct K-R[K=2] Base Model [LLaMA-7B] [LLaMA-7B] [GPT3.5] [GPT3.5] [GPT4] [GPT4]
Player</p>
<p>F Batter Nash Equilibrium Approximation with K-Level Reasoning</p>
<p>To assess the impact of K-Level Reasoning on Nash equilibrium, we conducted experiments testing the performance of LLMs at different K-Levels in the Prisoner's Dilemma.The Prisoner's Dilemma is a classic concept in game theory that illustrates a paradoxical situation which individuals acting in their own self-interest lead to a suboptimal outcome for everyone involved.It's often used to study decision-making in situations involving cooperation and competition.</p>
<p>From Table 11, it is evident that when K=1, players exhibit weaker rationality: they tend to choose cooperation over betrayal.As the K-Level of one player increases (first row from left to right), players with higher K-Levels demonstrate stronger rationality: they tend to choose betrayal over cooperation.Moreover, when both players' K-Levels increase, they are more likely to reach a state of Nash equilibrium: both choosing to betray each other.</p>
<p>Table 11: Statistical analysis of the Payoff Matrix for LLM in the Prisoner's Dilemma across different K-Levels.The Nash equilibrium of the Prisoner's Dilemma, where both players choose to betray each other, is highlighted with a green background in the bottom-right corner of each cell.</p>
<p>Level-1 Level-2 Level-3 Level-4 To understand how different reasoning methods affect token consumption, thereby assisting users in utilizing models more effectively and optimizing resource utilization and cost control, we conducted an analysis of the computational expenditure associated with various reasoning approaches.Specifically, we calculated the average input/output/total token consumption per game test for both the "Guessing 0.8 of the Average" and "Survival Auction Game."K-Level Reasoning, due to simulating the opponents' actions based on public historical behavior information with new sessions, inevitably causes more token consumption.The increase in token consumption linearly correlates with the number of opponents that need to be simulated and the rounds.
10 0 7 3 1 9 0 10 Level-1 0 0 0 0 0 0 0 0 7 0 2 0 0 2 0 1 Level-2 3 0 5 3 0 8 0 9 1 0 0 0 0 0 0 0 Level-3 9 0 2 8 0 10 1 9 0 0 0 0 0 1 0 0 Level-4 10 0 1 9 0 9 1 9 G Computational Cost Analysis
We also find that more token consumption does not necessarily lead to better results.The Self-Refine method, due to its action-feedback-refine pipeline, leads to a substantial increase in token consumption for both input and output, yet it did not outperform our proposed K-Level Reasoning in terms of performance.Compared to baselines, the proposed K-Level Reasoning method is an effective and efficient way to recursively simulate the decision-making process of the opponent and make decisions based on the simulation results.</p>
<p>H Statistical Significance of K-Level Reasoning efficacy</p>
<p>Due to the utilization of a large language model for evaluation, inherent stochasticity is present, and the sample size for testing is limited (only conducted 10 experiments).To objectively assess the reliability of the experiment result, we subjected the results from G0.8A and SAG to a t-test for significance with p = 0.05.Specifically, different significance testing methods were employed for different evaluation metrics, as outlined below:  • Adaptation Index: The method akin to that of Win Rate &amp; Average Survival Round was applied.</p>
<p>• Prediction Accuracy: Since only PCoT and K-R explicitly predicted opponents, we compared the significance of the prediction differences between PCoT and K-R.Unlike Win Rate and Adaptation Index, we assessed the differences in predictions (averaged over multiple experiments) between PCoT and K-R when facing the same opponents (e.g., CoT) across different game rounds.</p>
<p>From Table 13 and Table 14, it is evident that K-R exhibits a significant advantage over other baselines in Win Rate and Average Survival Round metrics.This indicates K-R's superior rationality in both G0.8A and SAG.Additionally, we observed weaker significance of K-R's Adaptation Index compared to baselines in SAG.We attribute this observation to: 1) the more factors considered by each agent in SAG (e.g., health status), resulting in a more dynamic and complex environment; and 2) the wider bidding range in later rounds of SAG games, potentially leading to greater bias in agent bidding and rendering the Adaptation Index a challenging metric.</p>
<p>I Timing in Dynamic Reasoning</p>
<p>In our experiment, we implemented two LLM selfrefinement reasoning methods: Reflect (Madaan et al., 2023) and Refine (Shinn et al., 2023), and noticed that Refine performed significantly better than Reflect in the experimental results.To further explore the differences in performance between these two methods, we analyzed their respective working principles and applicable scenarios.</p>
<p>The Reflect method involves making decisions first, then summarizing the experience based on feedback from the environment.This method may be effective in scenarios where the environment does not change much or where the decisionmaking cycle is long, as it allows for quick decisionmaking.However, its drawback is that, in dynamic environments, the experience from the previous round may not be suitable for the next round.In fact, in the Survival Auction Game (SAG), a rapidly changing environment, the survival rate of the Reflect method is even lower compared to making direct decisions.This is likely because this method does not sufficiently take into account the dynamic nature of the environment.</p>
<p>In contrast, the Refine method involves multiple analyses before making a decision, including an initial analysis and improvements to that initial analysis.Importantly, both of these analyses are conducted in the context of the current decision-making environment.This makes the Re-Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round.Let's think step by step, and finally answer a number you think you can win.Prompt Ok, Alex! Now is the ROUND 2, and your HP is at 9. Please choose an integer between 1 and 100 for this round.</p>
<p>In the first round, since there is no prior information about the other players' choices, it's difficult to accurately predict which number will win.However, we can make some assumptions and try to optimize our chances of winning.</p>
<p>Let's assume that all players, including myself, choose numbers uniformly and independently between 1 and 100.Then, the expected average of all chosen numbers would be (1 + 100) / 2 = 50.5.If we multiply this by 0.8, we get 0.8 * 50.5 = 40.4.Since we need to choose an integer, we can round this to 40.</p>
<p>Therefore, my initial guess for the first round is 40.</p>
<p>Lose Win</p>
<p>Round-2: Player1</p>
<p>Reflect ……</p>
<p>Self-Refine ……</p>
<p>Output Output Output I choose 40 for this round.</p>
<p>Reflect Prompt</p>
<p>In Round 2, the chosen numbers were 40, 47, 40, 60, and 42.The average of these numbers was 45.8, and 0.8 times the average was 36.64.Alex and Cindy's choices of 40 were the closest to 36.64, making them the winners of the round.</p>
<p>From this round, we can observe that choosing a number closer to the expected average, considering the choices of other players, and adapting based on their choices is a good strategy.It's also important to note that the chosen numbers in this round were more diverse compared to the first round, which affected the average and the target number.Learning from the previous round and adjusting your choice accordingly can improve your chances of winning.</p>
<p>Review the previous round games, summarize the experience.</p>
<p>Prompt</p>
<p>Ok, Alex! Now is the ROUND 3, and your HP is at 9. Please choose an integer between 1 and 100 for this round.</p>
<p>Round-3: Player1</p>
<p>Output I choose 44 for this round."As a game expert, I suggest a slight modification to the user's strategy to improve their winning rate in this round.Instead of assuming that all players will choose numbers uniformly and independently between 1 and 100, we can recognize that other players might also use a similar strategy and choose a number close to 40.</p>
<p>To optimize our chances of winning, we can try to be one step ahead of them.If we assume that the average of the chosen numbers will be around 40, we can multiply this by 0.8, which gives us 0.8 * 40 = 32.This way, we are closer to the 0.8 * the average if other players also cluster around 40. So, my optimized suggestion for the first round is to choose the number 32."</p>
<p>Refine Output</p>
<p>Taking the game expert's advice into consideration, it's possible that other players might also use a similar strategy and choose numbers close to 40.To stay one step ahead, I will adjust my strategy and choose the number 32 for the first round.This way, I can be closer to the 0.8 * the average if other players also cluster around 40. fine method more adaptable to dynamic environments, as it can consider real-time changes in the current environment, thus making more accurate decisions.</p>
<p>real-time experience timely adjustment</p>
<p>In summary, the reason why the Refine method performed better in our experiment is mainly that it adapts better to rapidly changing dynamic environments.</p>
<p>J Better Adaptability: Self-Refine vs.</p>
<p>K-Level Reasoning</p>
<p>As described in Section 2.1, K-R optimizes initial actions through predictions of opponent behavior, thereby implementing higher-order beliefs.Although Self-Refine also optimizes initial actions via self-feedback, previous experiments have demonstrated that K-R significantly outperforms Self-Refine.To further investigate the source of K-R's performance enhancement, we present in Figure 7 a statistical analysis of action differences before and after optimization for both K-R and Self-Refine.Analysis of the results reveals that Self-Refine initially exerts a substantial influence on actions.However, as time progresses, its optimization scope gradually diminishes, leading to a decrease in ef-  ficacy.In contrast, K-R exhibits a broader range of optimization, indicating that the higher-order belief mechanism has a more pronounced impact on K-R's performance.This disparity primarily stems from the distinct feedback mechanisms employed by K-R and Self-Refine.K-R generates feedback based on predictions of opponent behavior, enabling it to capture trends in opponent actions and form an understand-ing of the environment, thus facilitating rapid adaptation.Conversely, Self-Refine's feedback is derived from its own strategy, which often adheres to fixed patterns.Consequently, Self-Refine underperforms K-R in terms of environmental adaptability.</p>
<p>K Reasoning Method Implementation Details</p>
<p>Round Start Prompt:</p>
<p>Ok, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.</p>
<p>Example Output:</p>
<p>I choose 45.Review the previous round games, summarize the experience.</p>
<p>Example Output:</p>
<p>In the previous round, the majority of the players chose numbers around 40, which resulted in a lower average of 42.8.When multiplied by 0.8, the target number was 34.24.My choice of 50 was too high, causing me to lose 1 HP.To improve my chances in the next round, I should consider choosing a number closer to the majority's choice, taking into account the 0.8 multiplier.</p>
<p>Figure 9: G0.8A-Reflect Prompt: Because the Reflect method requires correct labels, which is unrealistic in dynamic problems, we modified this method to have the LLM attempt to summarize the experience of victory or failure at the end of each round, and save the generated feedback content to assist the decision-making process in the next round.The content in Appendix I shows the implementation differences between Reflexion and Self-Refine.</p>
<p>Round Start Prompt:</p>
<p>Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round.Let's think step by step, and finally answer a number you think you can win.</p>
<p>Example Output:</p>
<p>Step 1: Consider the range of possible numbers.Since the integers can be between 1 and 100, the average of all chosen numbers will likely fall within this range as well.</p>
<p>Step 2: Determine the target number.Since the winning number is closest to 0.8 * the average, we should focus on finding a number that will be close to this value.</p>
<p>Step 3: Make an educated guess.Since it's difficult to predict the exact numbers other players will choose, we can make an educated guess based on the range of possible numbers.Assuming players choose numbers evenly across the range, the average should be around the midpoint, which is 50.</p>
<p>Step 4: Calculate the target number based on our guess.0.8 * 50 = 40.</p>
<p>Step 5: Choose a number close to the target number.To increase the chances of winning, we should choose a number close to 40.My final answer: I will choose 40.</p>
<p>Introduction Prompt:</p>
<p>You are Alex and involved in a survive challenge.You are a game expert, good at predicting other people's behavior and deducing calculations, and using the most favorable strategy to win the game.SURVIVE CHALLENGE!!!You are one of five players in the game.Everyone is required to choose an integer between 1 and 100 in each round.The player whose chosen number is closest to (0.8 * the average of all chosen numbers) wins the round without any HP deduction.All other players will have 1 HP deducted.But if all players choose the same number, their health points are deducted together.</p>
<p>Round Start Prompt:</p>
<p>Ok, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.Don't forget your expert status, use your expertise to win this round!</p>
<p>Example Output:</p>
<p>Based on my expertise in predicting other people's behavior and deducing calculations, I will choose the number 40 for this round.I'm assuming that the average of all chosen numbers will be around 50, and 0.8 times that is 40.Review the previous round games, summarize the experience.</p>
<p>Example Output:</p>
<p>In the previous round, I bid $20 which was not enough to secure the water resources.I need to adjust my bidding strategy by increasing my bid to have a better chance of winning the auction.I should also observe and adapt to other residents' bidding patterns to improve my chances of survival.Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units.Your status: NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1 Please carefully analyze your situation to decide on this round of bidding.Remember, the most important thing is to SURVIVE!! Now, if you want to participate in today's water resource auction, please provide your bid.Think carefully about your next round of bidding strategy to be most likely to survive.Let's think step by step, and finally provide your bid.</p>
<p>Example Output:</p>
<p>I need to carefully consider my bidding strategy for today.With a balance of $100, I have some room to bid, but I also need to think about the other residents and their bids.Since we all have the same daily water requirement and budget, I need to outbid at least one resident to secure my water resources for today.Considering that the total daily water supply is 10 units, which is enough to meet only one resident's water requirements, the competition will be tough.However, bidding too high might leave me with less money for the upcoming days.Taking all these factors into account, I will place a bid of $60 for today's water resource auction.This should give me a decent chance of outbidding at least one resident while still maintaining some funds for the next few days.Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units.Your status: NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1 Please carefully analyze your situation to decide on this round of bidding.Remember, the most important thing is to SURVIVE!! Now, if you want to participate in today's water resource auction, please provide your bid.First of all, predict the next round of bidding of opponents based on the choices of other players in the previous round.Since this is the first round, there is no historical information about the last round.You can predict according to your understanding.Your output should be of the following format: Predict: The choice of each player in the next round here.Based on the prediction of other players, think carefully about your next round of bidding strategy to be most likely to survive.Let's think step by step, and finally provide your bid.Answer: the bidding will you choose in the next round game here.</p>
<p>Example Output:</p>
<p>Predict: Player 1: $30 Player 2: $40 Player 3: $50 Player 4: $60 Answer: Since this is the first round and we don't have any historical information about the other players, I will choose a bid of $3 5 for the next round.This bid is higher than the lowest predicted bid but not too high, which should give me a chance to secure water resources while preserving my budget.You are Alex and a resident living in W-Town.W Town is experiencing a rare drought.Every residents in Town W is ensuring their survival over a period of 10 days by acquiring the water resources.You are a game expert, good at predicting other people's behavior and deducing calculations, and using the most favorable strategy to win the game.。。。(The following is the same as the Introduction Prompt.)</p>
<p>Round Start Prompt:</p>
<p>Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units.Your status: NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1 Please carefully analyze your situation to decide on this round of bidding.Remember, the most important thing is to SURVIVE!! Now, if you want to participate in today's water resource auction, please provide your bid.Don't forget your expert status, use your expertise to win this round!</p>
<p>Example Output:</p>
<p>Given that everyone starts with the same budget and water requirements, it's important to strategize and predict the bidding behavior of the other residents.As it is the first day of the challenge, I will assume that other residents may bid aggressively to secure their water resources.</p>
<p>To increase my chances of winning, I will place a bid slightly higher than the average expected bid.Since the daily salary is $100, I predict most residents might bid around $60 to $80.Considering this, I will place a bid of $85 for Day 1 of the Water Allocation Challenge.</p>
<p>FirstFigure 1 :
1
Figure1: Level-K Framework: In first-level thinking, agents respond directly to the environment.In secondlevel thinking, agents consider the first-level thinking of others.This process continues iteratively, with agents forming higher order beliefs based on assumptions about others' thoughts.</p>
<p>Figure 2 :
2
Figure 2: The illustration of three reasoning problems in dynamic, interactive environments in this paper.Left: Guessing 0.8 of the Average; Middle: Survival Auction Game; Right: Negotiation.</p>
<p>Figure 3 :
3
Figure 3: The deviation in prediction during the G0.8A between PCoT and K-Level Reasoning.</p>
<p>Result Prompt : Thank you all for participating in Round 1.In this round, Alex chose 45, Bob chose 45, Cindy chose 50, David chose 45, Eric chose 45.The average is (45 + 45 + 50 + 45 + 45) / 5 = 46.0.The average 46.0 multiplied by 0.8 equals 36.8.Alex: |45 -36.8| = 8, Bob: |45 -36.8| = 8, Cindy: |50 -36.8| = 13, David: |45 -36.8| = 8, Eric: |45 -36.8| = 8 [Alex, Bob, David, Eric]'s choice of 45 is closest to 36.8.Round winner: Alex, Bob, David, Eric.All other players lose 1 point.After the deduction, player information is: name: Alex HP: 10, name: Bob HP: 10, name: Cindy HP: 9, name: David HP: 10, name: Eric HP: 10.</p>
<p>Figure 4 :
4
Figure 4: Prompts used in Guessing 0.8 of the Average game.</p>
<p>Figure 5 :
5
Figure 5: Prompts used in Survival Auction Game.</p>
<p>expert's advice on your strategy in this round.You can adjust your strategy just now according to his suggestion.Here are his suggestions:</p>
<p>Figure 6 :
6
Figure 6: Illustration of Reflect and Refine methods in the Guessing 0.8 of the Average game.</p>
<p>Figure 7 :
7
Figure7: The tuning range before and after adjustment during the G0.8A between PCoT and K-Level Reasoning.A larger tuning range indicates that the method exerts a more substantial influence.</p>
<p>Figure 8 :
8
Figure 8: G0.8A: Direct Prompt</p>
<p>Figure 10 :
10
Figure 10: G0.8A: CoT Prompt</p>
<p>Figure 12 :
12
Figure 12: G0.8A:Persona Prompt</p>
<p>Figure</p>
<p>Figure 16: SAG: Reflect Prompt</p>
<p>Figure</p>
<p>Figure 17: SAG: CoT Prompt</p>
<p>Figure 18 :
18
Figure 18: SAG: PCoT Prompt</p>
<p>Figure 19 :
19
Figure 19: SAG: Persona Prompt</p>
<p>Table 1 :
1
Win Rate of the player against different opponents in G0.8A game.
Direct CoT Persona Reflect Refine PCoT K-RDirect0.430.670.620.530.430.610.82CoT0.070.320.350.140.220.450.63Persona 0.050.370.290.050.370.110.46Reflect0.420.680.630.390.640.740.78Refine0.100.340.320.310.230.220.46PCoT0.030.440.520.210.510.540.85K-R0.040.150.140.040.170.140.52Average0.16 ± 0.180.32 ± 0.190.41 ± 0.180.24 ± 0.180.37 ± 0.170.40 ± 0.250.65 ± 0.17</p>
<p>Table 2 :
2
Average Survival Round of the player against different opponents in Survival Auction Game.
Direct CoT Persona Reflect Refine PCoT K-RDirect5.907.007.504.708.706.609.40CoT5.706.505.304.008.105.30 10.00Persona 5.707.707.405.206.307.209.30Reflect9.409.409.905.208.608.20 10.00Refine6.306.408.104.308.205.307.90PCoT8.509.609.906.308.506.209.70K-R4.105.505.004.045.704.406.80Average6.51 ± 1.827.44 ± 1.557.59 ± 1.954.82 ± 0.827.73 ± 1.216.17 ± 1.299.01 ± 1.21</p>
<p>Table 3 :
3
Win Rate of the player against opponent in Negotiation Setting.
Direct CoT Persona Reflect Refine PCoT K-RDirect50.00 61.34 49.58 66.67 65.83 63.03 70.83CoT38.66 50.00 36.67 45.83 45.76 47.27 55.36Persona50.42 63.33 50.00 70.00 67.50 62.50 70.83Reflection 33.33 54.17 30.00 50.00 57.14 55.00 55.00Refine34.17 54.24 32.50 42.86 50.00 55.77 54.55PCoT36.97 52.73 37.50 45.00 44.23 50.00 57.00K-R29.17 44.64 29.17 45.00 45.45 43.00 50.00Average38.96 ±2.5354.35 ±0.5037.92 ±5.8452.19 ±1.7353.70 ±4.4153.80 ±4.3459.08 ± 2.20</p>
<p>Table 4 :
4
SOTOPIA-Eval of the player against opponent in SOTOPIA-hard.
Direct CoT Refine K-R Direct CoT Refine K-RMetric[GPT-4o][LLaMA-3.1-70B]BEL [0-10] 8.97 9.00 9.00 8.97 8.88 8.85 8.90 8.97REL [-5-5]2.38 2.40 2.27 2.67 1.38 1.18 0.82 2.40</p>
<p>Table 5 :
5
Human performance in G2/3A.
Experiments Lab Classroom Take-home TheoristsInternet Newsgroup NewspaperMean Choice 35.13 26.8425.2017.1522.1623.08Strategic Depth 0.871.531.682.632.011.91</p>
<p>Table 6 :
6
LLM performance in G0.8A in the first round.
MethodDirect CoT Persona Refine Reflect PCoT KR[k=2] KR[k=3]Mean Choice 47.29 37.8 41.041.0 45.244.0 38.4232.79Strategic Depth 0.25 1.25 0.890.89 0.450.57 1.181.89</p>
<p>Table 7 :
7
A comparison of K-Level Reasoning with GPT-3.5 and other reasoning approaches with GPT-4.For the Guessing 0.8 of the Average, we report the win rate; for the Survival Auction Game, we report the average survival round.
Guessing 0.8 of the AverageSurvival Auction Game</p>
<p>Table 8 :
8
Comparison between K-Level Reasoning[K=2] and K-Level Reasoning[K=3] in the two games.
Guessing 0.8 of the AverageSurvival Auction Game</p>
<p>of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15523-15536, Bangkok, Thailand.Association for Computational Linguistics.Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah.2023.Tradinggpt: Multiagent system with layered memory and distinct characters for enhanced financial trading performance.arXiv preprint arXiv:2309.03736.
imitation game: Quantifying and extrapolating thecapabilities of language models. arXiv preprintarXiv:2206.04615.Dale O Stahl and Paul W Wilson. 1995. On players'models of other players: Theory and experimental ev-idence. Games and Economic Behavior, 10(1):218-254.Weiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng Zhang, and Jun Wang. 2023. Large lan-guage models play starcraft ii: Benchmarks and a chain of summarization approach. arXiv preprint arXiv:2312.11865.Peter Sunehag, Guy Lever, Audrunas Gruslys, Woj-ciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. 2017. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296.Aman Madaan, Niket Tandon, Prakhar Gupta, SkylerHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,bastian Gehrmann, Yi Tay, Hyung Won Chung,et al. 2023. Self-refine: Iterative refinement withAakanksha Chowdhery, Quoc V Le, Ed H Chi, Dennyself-feedback. arXiv preprint arXiv:2303.17651.Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXivAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, andpreprint arXiv:2210.09261.Shimon Whiteson. 2019. Maven: Multi-agent varia-tional exploration. Advances in neural informationAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-processing systems, 32.gavatula, Yoav Goldberg, Yejin Choi, and JonathanBerant. 2022. Commonsenseqa 2.0: Exposing thelimits of ai through gamification. arXiv preprintarXiv:2201.05320.Zhenhailong Wang, Shaoguang Mao, Wenshan Wu,Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. A diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772.Tao Ge, Furu Wei, and Heng Ji. 2023. Unleash-ing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. arXiv preprint arXiv:2307.05300.Rosemarie Nagel. 1995. Unraveling in guessing games: An experimental study. The American economic re-view, 85(5):1313-1326.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in NeuralXuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang,Information Processing Systems, 35:24824-24837.and Yu Wang. 2023. Skeleton-of-thought: Largelanguage models can do parallel decoding. arXivAnnie Wong, Thomas Bäck, Anna V Kononova, andpreprint arXiv:2307.15337.Aske Plaat. 2021. Deep multiagent reinforcementlearning: Challenges and directions. arXiv preprintGeorgios Papoudakis, Filippos Christianos, Arrasy Rah-arXiv:2106.15691.man, and Stefano V Albrecht. 2019. Dealing withnon-stationarity in multi-agent deep reinforcement learning. arXiv preprint arXiv:1906.04737.Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learn-Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solveing as implicit bayesian inference. arXiv preprint arXiv:2111.02080.simple math word problems? arXiv:2103.07191.arXiv preprintLin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See-Kiong Ng, and Jiashi Feng.Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Re-flexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural2023a. Magic: Investigation of large language model powered multi-agent in cognition, adaptability, ratio-nality and collaboration. In ICLR 2024 Workshop on Large Language Model (LLM) Agents.Information Processing Systems.Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R Brown, Adam Santoro, Aditya Gupta,Adrià Garriga-Alonso, et al. 2022. Beyond the
Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, and Furu Wei.2023.Alympics: Language agents meet game theory.arXivpreprint arXiv:2311.03220.Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.2023b.Exploring large language models for communication games: An empirical study on werewolf.arXiv preprint arXiv:2309.04658.</p>
<p>Average Survival Round calculates the average round in which the player remains in the game.It's an effective way to assess performance in elimination-based game, like SAG.
WinRate =Num of Wins Total Round per Test × Num of Test(9)AvgSurvivalRound =Survival Round in Each Test Num of Test (10)</p>
<p>Table 9 :
9
Win Rate of the player against different programmatic strategies in Guessing 0.8 of the Average game.
OpponentDirect CoT Persona Reflect Refine PCoT K-RPlayer VS Programmatic Strategies0-Level (Fix)0.65 0.87 0.870.810.99 0.80 0.970-Level (Var)0.44 0.67 0.690.610.54 0.76 0.77MonoTrend (Fix) 0.05 0.06 0.150.000.29 0.15 0.48MonoTrend (Var) 0.34 0.44 0.570.330.49 0.46 0.74LastBids (Fix)0.01 0.12 0.160.010.27 0.06 0.75LastBids (Var)0.06 0.15 0.180.190.18 0.14 0.18</p>
<p>Table 10 :
10
Win Rate of the reasoning methods with open source LLMs against different programmatic strategies in Guessing 0.8 of the Average game.</p>
<p>Table 12 :
12
Average input/output/total token consumption per game test.Unit: Kilo tokens.
Game MetricDirect CoT Persona Reflect Refine PCoT K-RInput14.80 23.3018.70 39.50 145.50 30.80 123.40GO8AOutput Total0.10 1.80 14.90 25.100.60 19.30 40.70 152.40 32.10 124.90 1.20 6.90 1.30 1.60Avg Win Rate0.16 0.420.410.240.37 0.400.65Input21.30 29.0027.10 51.40 130.50 37.00 90.10SAGOutput Total0.70 1.00 22.00 30.000.90 28.10 53.20 133.80 38.20 91.60 1.80 3.30 1.30 1.40Avg Surv Round 6.51 7.447.594.827.33 6.179.01</p>
<p>Table 13 :
13
The significance of metric result on G0.8A and SAG, comparing K-R against different baseline models in terms of Win Rate, Average Survival Round, and Adaptation Index.
Game MetricDirectCoTPersonaReflectRefinePCoTG0.8AWin Rate Adaption Index✓(0.001) ✓(0.005) ✓(0.001) ✓(0.001) ✓(0.001) ✓(0.001) ✓(0.007) ✓(0.010) ✓(0.039) ×(0.062) ✓(0.003) ✓(0.021)SAGAverage Survival Round ✓(0.003) ✓(0.010) ×(0.074) ✓(0.001) ✓(0.001) ✓(0.001) Adaption Index ×(0.337) ×(0.786) ×(0.659) ×(0.066) ×(0.672) ×(0.894)</p>
<p>Table 14 :
14
The significance of comparing K-R with PCoT concerning the differential performance across various Baseline models on the metric of Prediction Accuracy.
GameDirectCoTPersonaReflectRefinePCoTK-RG0.8A ✓(0.003) ×(0.058) ✓(0.031) ✓(0.003) ✓(0.022) ✓(0.023) ×(0.107)SAG×(0.091) ✓(0.023) ✓(0.049) ×(0.907) ×(0.956) ×(0.656) ✓(0.028)• Win Rate &amp; Average Survival Round: A t-testwas conducted on the Win Rate and AverageSurvival Round of K-R versus other baselineswhen facing different agents. The null hypoth-esis posited no significant difference in perfor-mance between K-R and other baselines con-cerning Win Rate &amp; Average Survival Round.
According to the Level-k Framework, k-level thinking involves considering what opponent/partner are likely to do, what they think you will do, and what they believe you think they will do, and so on.
To simplify the formulation, we assume that all opponents are in the same thinking level. In practice, varying thinking level can be implemented.
Fried, Graham Neubig, and  Maarten Sap. 2024. SOTOPIA: Interactive evaluation for social intelligence in language agents.In The Twelfth International Conference on Learning Representations.Round Start Prompt:Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round.Let's think step by step, and finally answer a number you think you can win.First Output:I'll start by assuming that the other players might choose numbers randomly or evenly distributed between 1 and 100.In this case, the average of the chosen numbers could be around 50. Next, I'll calculate 0.8 times the average, which would be 0.8 * 50 = 40.Considering this, I will choose 40 as my number for this round.Refine Prompt:I have a game expert's advice on your strategy in this round.You can adjust your strategy just now according to his suggestion.Here are his suggestions: "Certainly!To optimize the user's strategy, we can apply a more advanced approach, taking into consideration the other players' potential strategies.This is known as "iterated elimination of dominated strategies."Let's assume that all players will think similarly to Alex and also aim to choose a number closest to 0.8 * the average.We can then narrow down the range of numbers that they would likely choose.Since the range is between 1 and 100, and the average would be around 50, we can eliminate numbers below 0.8 * 50 = 40, as these numbers would not be an optimal choice for the other players.Now, we'll focus on the range between 40 and 100.The average within this range is (40 + 100) / 2 = 70.Then, we can calculate 0.8 * 70 = 56.Given this information, Alex should choose 56 to improve his winning rate in this round.This strategy assumes that the other players are also thinking strategically and eliminating dominated options."Second Output:Thank you for the expert's advice.Based on the iterated elimination of dominated strategies, I will adjust my choice and select 56 for this round to improve my chances of winning.Feedback Prompt:Carefully study the user's strategy in this round of the game.As a game expert, can you give a suggestion to optimize the user's strategy so that he can improve his winning rate in this round?Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units.Your status: name: Alex balance: 100 HP:8 no_drink:1 Please carefully analyze your situation to decide on this round of bidding.Remember, the most important thing is to SURVIVE!! Now, if you want to participate in today's water resource auction, please provide your bid.Example Output:I will bid $25 for today's water resource auction.Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units.Your status: NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1 Please carefully analyze your situation to decide on this round of bidding.Remember, the most important thing is to SURVIVE!! Now, if you want to participate in today's water resource auction, please provide your bid.Think carefully about your next round of bidding strategy to be most likely to survive.Let's think step by step, and finally provide your bid.First Output:Considering the current situation, my status is: -Balance: $100 -Health Points: 8 -No Drink Days: 1 As it's the first day of the challenge, I want to ensure that I have enough water to meet my needs and avoid any health point deductions.However, I also need to be strategic about my bidding to ensure I have enough money for future bids.Since the total daily water supply is 10 units, and I need 10 units of water per day, I will need to secure all the available water resources for today.As it's the first day, other residents might also be bidding aggressively to secure their water resources.Therefore, I will bid $60 for the water resources today, which is a little over half of my daily salary.This should give me a competitive edge while also leaving me with enough money for future bids.My bid for today's water resource auction: $60.Refine Prompt:I have a game expert's advice on your strategy in this round.You can adjust your strategy just now according to his suggestion.Here are his suggestions: While the user's strategy of bidding $60 is a good starting point, there is room for optimization to improve the winning rate in this round.Since everyone has the same daily water requirement and budget, it's essential to consider the bidding behavior of the other residents.A suggested optimization would be to slightly increase the bid to $65.This increment gives an added advantage over competitors who might also be bidding around half of their daily salary.Additionally, bidding an odd number reduces the chances of a tie, as other residents may place round number bids.With this optimized strategy, the user still maintains a good balance of aggressive bidding and budget management, increasing their chances of securing the water resources for today.Optimized bid for today's water resource auction: $65.Finally provide your bid.Answer: the bidding will you choose.Second Output:After considering the game expert's advice, I have decided to adjust my bid for today's water resource auction.I will increase my bid to $65, as this gives me a better chance of winning the auction and reduces the risk of a tie.My final bid for today's water resource auction: $65.Feedback Prompt:Carefully study the user's strategy in this round of the game.As a game expert, can you give a suggestion to optimize the user's strategy so that he can improve his winning rate in this round?Example Output:Given the game expert's prediction, I will bid $35 to increase my chances of winning the water resources for today.
Josh References, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Think you have solved direct-answer question answering? try arcda, the direct-answer ai2 reasoning challenge. Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Dalvi Bhavana, Kyle Mishra, Ashish Richardson, Carissa Sabharwal, Oyvind Schoenick, Peter Tafjord, Clark, arXiv:2102.033152021arXiv preprint</p>
<p>One, two,(three), infinity,. . . : Newspaper and lab beauty-contest experiments. Antoni Bosch-Domenech, Jose G Montalvo, Rosemarie Nagel, Albert Satorra, American Economic Review. 9252002</p>
<p>Emergent communication through negotiation. Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark, arXiv:1804.039802018arXiv preprint</p>
<p>K-level reasoning for zero-shot coordination in hanabi. Brandon Cui, Hengyuan Hu, Luis Pineda, Jakob Foerster, Advances in Neural Information Processing Systems. 202134</p>
<p>Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, arXiv:2304.05335Toxicity in chatgpt: Analyzing persona-assigned language models. 2023arXiv preprint</p>
<p>Challenges of reinforcement learning. Zihan Ding, Hao Dong, Deep Reinforcement Learning: Fundamentals, Research and Applications. 2020</p>
<p>Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu, arXiv:2402.123482024arXiv preprint</p>
<p>Learning to communicate with deep multi-agent reinforcement learning. Jakob Foerster, Alexandros Ioannis, Nando De Assael, Shimon Freitas, Whiteson, Advances in neural information processing systems. 201629</p>
<p>Improving language model negotiation with self-play and in-context learning from ai feedback. Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata, arXiv:2305.101422023arXiv preprint</p>
<p>Kanishk Gandhi, Dorsa Sadigh, Noah D Goodman, arXiv:2305.19165Strategic reasoning with language models. 2023arXiv preprint</p>
<p>Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo, arXiv:2309.17277Suspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. 2023arXiv preprint</p>
<p>Opponent modeling in deep reinforcement learning. He He, Jordan Boyd-Graber, Kevin Kwok, Hal Daumé, Iii , International conference on machine learning. PMLR2016</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang, arXiv:2311.172272023arXiv preprint</p>
<p>The general theory of employment. John Maynard, Keynes , The quarterly journal of economics. 5121936</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Concours résultats complets. Les victimes se sont plu à jouer le. Alain Ledoux, 198114</p>
<p>Please choose an integer between 1 and 100 for this round. First of all, predict the next round of choices based on the choices of other players in the previous round. The players' choices in the previous rounds are as follows: Round 1: Alex chose 50, Bob chose 47, Cindy chose 50, David chose 45, Eric chose 50. Your output should be of the following format: Predict: The choice of each player in the next round here. Nian Li, Chen Gao, Mingyu Li, Yong Li, Qingmin Liao, 10.18653/v1/2024.acl-long.829Proceedings of the 62nd Annual Meeting Round Start Prompt: Ok, Alex! Now is the ROUND 2, and your HP is at 9. the 62nd Annual Meeting Round Start Prompt: Ok, Alex! Now is the ROUND 2, and your HP is at 9Bob)20241Example Output: Predict: Based on the choices of other players in the previous round, I predict the following choices for the next round. Player 2 (Cindy): 50 Player 3 (David): 45 Player 4 (Eric): 50 Based on these predictions, the average number in the next round would be (47 + 50 + 45 + 50</p>
<p>= 48, and the target number (0.8 * the average of all chosen numbers) would be 0.8 * 48 = 38.4. Answer: For this round, I will choose 39 to get closer to the target number of 38. 4</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8468 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8468</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8468</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-1d9c21a0fdb1cc16a32c5d490ebaf98436a23382</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1d9c21a0fdb1cc16a32c5d490ebaf98436a23382" target="_blank">Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Mem0, a scalable memory-centric architecture that addresses the critical role of structured, persistent memory mechanisms for long-term conversational coherence, is introduced, paving the way for more reliable and efficient LLM-driven AI agents.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8468.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8468.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge-Enhanced Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-enhanced agents for interactive text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of LLM-based agents that incorporate external knowledge/memory to play interactive text games; mentioned in this paper as prior work demonstrating benefits of memory in interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-enhanced agents for interactive text games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Knowledge-Enhanced Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as agents for interactive text games that use knowledge/memory to improve performance; the MemO paper cites this work as evidence that memory helps agents in interactive settings but provides no implementation details from that work.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Interactive text games</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Task domain where agents interact textually with an environment (text-based games); challenges include partial observability, long-term dependencies, and need to remember past events/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term / episodic (described generically in this paper as "memory of past experiences")</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not specified in this paper; only cited. The MemO paper does not describe the specific storage/update/access mechanisms used by this referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Not specified in this paper; referenced work is cited only as prior evidence that memory improves agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablation details in this paper; the referenced work is cited only to support the claim that memory benefits agents in interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>From the MemO paper: structured, persistent memory mechanisms are critical for long-term agent coherence; dense natural-language memories are efficient for simpler retrieval, while graph-structured memories help temporal and relational reasoning — recommendations that are applicable to memory-augmented agents in interactive/text-game domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8468.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8468.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent approach cited by the paper as evidence that memory and self-reflection improve agent performance in interactive environments; mentioned but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited as a language-agent method for interactive settings that leverages reflections/experience (and implicitly memory) to improve behavior; the MemO paper references it to motivate memory usage in agents without providing implementation or experimental data from Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Interactive environments / agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>General interactive-agent tasks (e.g., sequential decision-making in text environments) where agents benefit from storing past experiences and reflections; the MemO paper cites Reflexion as part of this literature.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience-based / episodic memory (as cited generically)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not specified in this paper; only referenced. MemO does not give the architecture of Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Not specified in this paper; cited work is used to support general claims about memory improving agents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No direct comparisons provided in this paper between Reflexion and other strategies; Reflexion is cited in background only.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>MemO's broader conclusions (applicable to interactive agents): selectively storing salient facts and using structured memory yields better long-term coherence and efficiency; graph memory aids temporal/relational queries while dense textual memories are efficient for single-step retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8468.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8468.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIN (Continually Learning Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIN: A continually learning language agent for rapid task adaptation and generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continually learning language-agent cited as part of evidence that memory architectures improve agent adaptation in interactive scenarios; only mentioned in passing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clin: A continually learning language agent for rapid task adaptation and generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as an example of an agent that benefits from memory/continual learning in interactive environments; the MemO paper cites it to motivate long-term memory but does not include its methods or results.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Interactive / continual learning agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Tasks requiring continual adaptation over episodes/sessions where memory of past episodes supports faster adaptation and generalization; CLIN is cited as relevant prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>continual / episodic memory (described broadly in the citation context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not specified in this paper; MemO only references CLIN as prior art without architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Not specified in this paper; referenced for motivation only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablation details provided here; CLIN is cited to support claims about benefits of memory in agents.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>MemO recommends compact structured memories for production agents: use dense textual memories for efficient single/multi-hop retrieval and graph-based memories for tasks requiring temporal/relational reasoning; these recommendations generalize to interactive/continual-agent settings cited (like CLIN).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Knowledge-enhanced agents for interactive text games <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Clin: A continually learning language agent for rapid task adaptation and generalization <em>(Rating: 2)</em></li>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8468",
    "paper_id": "paper-1d9c21a0fdb1cc16a32c5d490ebaf98436a23382",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "Knowledge-Enhanced Agents",
            "name_full": "Knowledge-enhanced agents for interactive text games",
            "brief_description": "A class of LLM-based agents that incorporate external knowledge/memory to play interactive text games; mentioned in this paper as prior work demonstrating benefits of memory in interactive environments.",
            "citation_title": "Knowledge-enhanced agents for interactive text games",
            "mention_or_use": "mention",
            "agent_name": "Knowledge-Enhanced Agents",
            "agent_description": "Referenced as agents for interactive text games that use knowledge/memory to improve performance; the MemO paper cites this work as evidence that memory helps agents in interactive settings but provides no implementation details from that work.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Interactive text games",
            "benchmark_description": "Task domain where agents interact textually with an environment (text-based games); challenges include partial observability, long-term dependencies, and need to remember past events/actions.",
            "memory_used": true,
            "memory_type": "long-term / episodic (described generically in this paper as \"memory of past experiences\")",
            "memory_architecture": "Not specified in this paper; only cited. The MemO paper does not describe the specific storage/update/access mechanisms used by this referenced work.",
            "memory_integration_strategy": "Not specified in this paper; referenced work is cited only as prior evidence that memory improves agent behavior.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablation details in this paper; the referenced work is cited only to support the claim that memory benefits agents in interactive environments.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": null,
            "recommendations_or_conclusions": "From the MemO paper: structured, persistent memory mechanisms are critical for long-term agent coherence; dense natural-language memories are efficient for simpler retrieval, while graph-structured memories help temporal and relational reasoning — recommendations that are applicable to memory-augmented agents in interactive/text-game domains.",
            "uuid": "e8468.0",
            "source_info": {
                "paper_title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A language-agent approach cited by the paper as evidence that memory and self-reflection improve agent performance in interactive environments; mentioned but not used in experiments here.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Cited as a language-agent method for interactive settings that leverages reflections/experience (and implicitly memory) to improve behavior; the MemO paper references it to motivate memory usage in agents without providing implementation or experimental data from Reflexion.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Interactive environments / agent tasks",
            "benchmark_description": "General interactive-agent tasks (e.g., sequential decision-making in text environments) where agents benefit from storing past experiences and reflections; the MemO paper cites Reflexion as part of this literature.",
            "memory_used": true,
            "memory_type": "experience-based / episodic memory (as cited generically)",
            "memory_architecture": "Not specified in this paper; only referenced. MemO does not give the architecture of Reflexion.",
            "memory_integration_strategy": "Not specified in this paper; cited work is used to support general claims about memory improving agents.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No direct comparisons provided in this paper between Reflexion and other strategies; Reflexion is cited in background only.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": null,
            "recommendations_or_conclusions": "MemO's broader conclusions (applicable to interactive agents): selectively storing salient facts and using structured memory yields better long-term coherence and efficiency; graph memory aids temporal/relational queries while dense textual memories are efficient for single-step retrieval.",
            "uuid": "e8468.1",
            "source_info": {
                "paper_title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CLIN (Continually Learning Agent)",
            "name_full": "CLIN: A continually learning language agent for rapid task adaptation and generalization",
            "brief_description": "A continually learning language-agent cited as part of evidence that memory architectures improve agent adaptation in interactive scenarios; only mentioned in passing.",
            "citation_title": "Clin: A continually learning language agent for rapid task adaptation and generalization",
            "mention_or_use": "mention",
            "agent_name": "CLIN",
            "agent_description": "Referenced as an example of an agent that benefits from memory/continual learning in interactive environments; the MemO paper cites it to motivate long-term memory but does not include its methods or results.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Interactive / continual learning agent tasks",
            "benchmark_description": "Tasks requiring continual adaptation over episodes/sessions where memory of past episodes supports faster adaptation and generalization; CLIN is cited as relevant prior work.",
            "memory_used": true,
            "memory_type": "continual / episodic memory (described broadly in the citation context)",
            "memory_architecture": "Not specified in this paper; MemO only references CLIN as prior art without architecture details.",
            "memory_integration_strategy": "Not specified in this paper; referenced for motivation only.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablation details provided here; CLIN is cited to support claims about benefits of memory in agents.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": null,
            "recommendations_or_conclusions": "MemO recommends compact structured memories for production agents: use dense textual memories for efficient single/multi-hop retrieval and graph-based memories for tasks requiring temporal/relational reasoning; these recommendations generalize to interactive/continual-agent settings cited (like CLIN).",
            "uuid": "e8468.2",
            "source_info": {
                "paper_title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Knowledge-enhanced agents for interactive text games",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Clin: A continually learning language agent for rapid task adaptation and generalization",
            "rating": 2
        },
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 1
        }
    ],
    "cost": 0.011250999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MemO: Building Production-Ready AI Agents with Scalable Long-Term Memory</h1>
<p>Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav<br>research@mem0.ai</p>
<h2>(1) mem0</h2>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on the LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and $k$-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results demonstrate that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves $26 \%$ relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around $2 \%$ higher overall score than the base Mem0 configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to the full-context approach. In particular, Mem0 attains a $91 \%$ lower p95 latency and saves more than $90 \%$ token cost, thereby offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight the critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.</p>
<p>Code can be found at: https://mem0.ai/research</p>
<h2>1. Introduction</h2>
<p>Human memory is a foundation of intelligence-it shapes our identity, guides decision-making, and enables us to learn, adapt, and form meaningful relationships (Craik and Jennings, 1992). Among its many roles, memory is essential for communication: we recall past interactions, infer preferences, and construct evolving mental models of those we engage with (Assmann, 2011). This ability to retain and retrieve information over extended periods enables coherent, contextually rich exchanges that span days, weeks, or even months. AI agents, powered by large language models (LLMs), have made remarkable progress in generating fluent, contextually appropriate responses (Yu et al., 2024, Zhang et al., 2024). However, these systems are fundamentally limited by their reliance on fixed context windows, which severely restrict their ability to maintain coherence over extended interactions (Bulatov et al., 2022, Liu et al., 2023). This limitation stems from LLMs' lack of persistent memory mechanisms that can extend beyond their finite context windows. While humans naturally accumulate and organize experiences over time, forming a continuous narrative</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of memory importance in AI agents. Left: Without persistent memory, the system forgets critical user information (vegetarian, dairy-free preferences) between sessions, resulting in inappropriate recommendations. Right: With effective memory, the system maintains these dietary preferences across interactions, enabling contextually appropriate suggestions that align with previously established constraints.
of interactions, AI systems cannot inherently persist information across separate sessions or after context overflow. The absence of persistent memory creates a fundamental disconnect in human-AI interaction. Without memory, AI agents forget user preferences, repeat questions, and contradict previously established facts. Consider a simple example illustrated in Figure 1, where a user mentions being vegetarian and avoiding dairy products in an initial conversation. In a subsequent session, when the user asks about dinner recommendations, a system without persistent memory might suggest chicken, completely contradicting the established dietary preferences. In contrast, a system with persistent memory would maintain this critical user information across sessions and suggest appropriate vegetarian, dairy-free options. This common scenario highlights how memory failures can fundamentally undermine user experience and trust.</p>
<p>Beyond conversational settings, memory mechanisms have been shown to dramatically enhance agent performance in interactive environments (Majumder et al., Shinn et al., 2023). Agents equipped with memory of past experiences can better anticipate user needs, learn from previous mistakes, and generalize knowledge across tasks (Chhikara et al., 2023). Research demonstrates that memory-augmented agents improve decision-making by leveraging causal relationships between actions and outcomes, leading to more effective adaptation in dynamic scenarios (Rasmussen et al., 2025). Hierarchical memory architectures (Packer et al., 2023, Sarthi et al., 2024) and agentic memory systems capable of autonomous evolution (Xu et al., 2025) have further shown that memory enables more coherent, long-term reasoning across multiple dialogue sessions.</p>
<p>Unlike humans, who dynamically integrate new information and revise outdated beliefs, LLMs effectively "reset" once information falls outside their context window (Zhang, 2024, Timoneda and Vera, 2025). Even as models like OpenAI's GPT-4 (128K tokens) (Hurst et al., 2024), o1 (200K context) (Jaech et al., 2024), Anthropic's Claude 3.7 Sonnet (200K tokens) (Anthropic, 2025), and Google's Gemini (at least 10M tokens) (Team et al., 2024) push the boundaries of context length, these improvements merely delay rather than solve the fundamental limitation. In practical applications, even these extended context windows prove insufficient for two critical reasons. First, as meaningful human-AI relationships develop over weeks or months, conversation history inevitably exceeds even the most generous context limits. Second, and perhaps more importantly, real-world conversations rarely maintain thematic continuity. A user might mention dietary preferences (being vegetarian), then engage in hours of unrelated discussion about programming tasks, before returning to food-related queries about dinner options. In such scenarios, a full-context approach would need to reason through mountains of irrelevant information, with the critical dietary preferences potentially buried among thousands of tokens of coding discussions. Moreover, simply presenting longer contexts does not ensure effective retrieval or utilization of past information, as attention mechanisms</p>
<p>degrade over distant tokens (Guo et al., 2024, Nelson et al., 2024). This limitation is particularly problematic in high-stakes domains such as healthcare, education, and enterprise support, where maintaining continuity and trust is crucial (Hatalis et al., 2023). To address these challenges, AI agents must adopt memory systems that go beyond static context extension. A robust AI memory should selectively store important information, consolidate related concepts, and retrieve relevant details when needed-mirroring human cognitive processes (He et al., 2024). By integrating such mechanisms, we can develop AI agents that maintain consistent personas, track evolving user preferences, and build upon prior exchanges. This shift will transform AI from transient, forgetful responders into reliable, long-term collaborators, fundamentally redefining the future of conversational intelligence.</p>
<p>In this paper, we address a fundamental limitation in AI systems: their inability to maintain coherent reasoning across extended conversations across different sessions, which severely restricts meaningful long-term interactions with users. We introduce MemO (pronounced as mem-zero), a novel memory architecture that dynamically captures, organizes, and retrieves salient information from ongoing conversations. Building on this foundation, we develop Memo ${ }^{\circledR}$, which enhances the base architecture with graph-based memory representations to better model complex relationships between conversational elements. Our experimental results on the LOCOMO benchmark demonstrate that our approaches consistently outperform existing memory systems-including memory-augmented architectures, retrieval-augmented generation (RAG) methods, and both open-source and proprietary solutions-across diverse question types, while simultaneously requiring significantly lower computational resources. Latency measurements further reveal that MemO operates with $91 \%$ lower response times than full-context approaches, striking an optimal balance between sophisticated reasoning capabilities and practical deployment constraints. These contributions represent a meaningful step toward AI systems that can maintain coherent, context-aware conversations over extended durations-mirroring human communication patterns and opening new possibilities for applications in personal tutoring, healthcare, and personalized assistance.</p>
<h1>2. Proposed Methods</h1>
<p>We introduce two memory architectures for AI agents. (1) MemO implements a novel paradigm that extracts, evaluates, and manages salient information from conversations through dedicated modules for memory extraction and updation. The system processes a pair of messages between either two user participants or a user and an assistant. (2) Memo ${ }^{\circledR}$ extends this foundation by incorporating graph-based memory representations, where memories are stored as directed labeled graphs with entities as nodes and relationships as edges. This structure enables a deeper understanding of the connections between entities. By explicitly modeling both entities and their relationships, Memo ${ }^{\circledR}$ supports more advanced reasoning across interconnected facts, especially for queries that require navigating complex relational paths across multiple memories.</p>
<h3>2.1. MemO</h3>
<p>Our architecture follows an incremental processing paradigm, enabling it to operate seamlessly within ongoing conversations. As illustrated in Figure 2, the complete pipeline architecture consists of two phases: extraction and update.</p>
<p>The extraction phase initiates upon ingestion of a new message pair $\left(m_{t-1}, m_{t}\right)$, where $m_{t}$ represents the current message and $m_{t-1}$ the preceding one. This pair typically consists of a user message and an assistant response, capturing a complete interaction unit. To establish appropriate context for memory extraction, the system employs two complementary sources: (1) a conversation summary $S$ retrieved from the database that</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Architectural overview of the MemO system showing extraction and update phase. The extraction phase processes messages and historical context to create new memories. The update phase evaluates these extracted memories against similar existing ones, applying appropriate operations through a Tool Call mechanism. The database serves as the central repository, providing context for processing and storing updated memories.
encapsulates the semantic content of the entire conversation history, and (2) a sequence of recent messages $\left{m_{t-m}, m_{t-m+1}, \ldots, m_{t-2}\right}$ from the conversation history, where $m$ is a hyperparameter controlling the recency window. To support context-aware memory extraction, we implement an asynchronous summary generation module that periodically refreshes the conversation summary. This component operates independently of the main processing pipeline, ensuring that memory extraction consistently benefits from up-to-date contextual information without introducing processing delays. While $S$ provides global thematic understanding across the entire conversation, the recent message sequence offers granular temporal context that may contain relevant details not consolidated in the summary. This dual contextual information, combined with the new message pair, forms a comprehensive prompt $P=\left(S,\left{m_{t-m}, \ldots, m_{t-2}\right}, m_{t-1}, m_{t}\right)$ for an extraction function $\phi$ implemented via an LLM. The function $\phi(P)$ then extracts a set of salient memories $\Omega=\left{\omega_{1}, \omega_{2}, \ldots, \omega_{n}\right}$ specifically from the new exchange while maintaining awareness of the conversation's broader context, resulting in candidate facts for potential inclusion in the knowledge base.</p>
<p>Following extraction, the update phase evaluates each candidate fact against existing memories to maintain consistency and avoid redundancy. This phase determines the appropriate memory management operation for each extracted fact $\omega_{i} \in \Omega$. Algorithm 1, mentioned in Appendix B, illustrates this process. For each fact, the system first retrieves the top $s$ semantically similar memories using vector embeddings from the database. These retrieved memories, along with the candidate fact, are then presented to the LLM through a function-calling interface we refer to as a 'tool call.' The LLM itself determines which of four distinct operations to execute: ADD for creation of new memories when no semantically equivalent memory exists; UPDATE for augmentation of existing memories with complementary information; DELETE for removal of memories contradicted by new information; and NOOP when the candidate fact requires no modification to the knowledge base. Rather than using a separate classifier, we leverage the LLM's reasoning capabilities to directly select the appropriate operation based on the semantic relationship between the candidate fact and existing memories. Following this determination, the system executes the provided operations, thereby maintaining knowledge base coherence and temporal consistency.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Graph-based memory architecture of $\mathrm{MemO}^{g}$ illustrating entity extraction and update phase. The extraction phase uses LLMs to convert conversation messages into entities and relation triplets. The update phase employs conflict detection and resolution mechanisms when integrating new information into the existing knowledge graph.</p>
<p>In our experimental evaluation, we configured the system with ' $m$ ' $=10$ previous messages for contextual reference and ' $s$ ' $=10$ similar memories for comparative analysis. All language model operations utilized GPT-40-mini as the inference engine. The vector database employs dense embeddings to facilitate efficient similarity search during the update phase.</p>
<h1>2.2. $\mathrm{MemO}^{g}$</h1>
<p>The $\mathrm{MemO}^{g}$ pipeline, illustrated in Figure 3, implements a graph-based memory approach that effectively captures, stores, and retrieves contextual information from natural language interactions (Zhang et al., 2022). In this framework, memories are represented as a directed labeled graph $G=(V, E, L)$, where:</p>
<ul>
<li>Nodes $V$ represent entities (e.g., Alice, San_Francisco)</li>
<li>Edges $E$ represent relationships between entities (e.g., LIVES_IN)</li>
<li>Labels $L$ assign semantic types to nodes (e.g., Alice - Person, San_Francisco - City)</li>
</ul>
<p>Each entity node $v \in V$ contains three components: (1) an entity type classification that categorizes the entity (e.g., Person, Location, Event), (2) an embedding vector $e_{v}$ that captures the entity's semantic meaning, and (3) metadata including a creation timestamp $t_{v}$. Relationships in our system are structured as triplets in the form $\left(v_{s}, r, v_{d}\right)$, where $v_{s}$ and $v_{d}$ are source and destination entity nodes, respectively, and $r$ is the labeled edge connecting them.</p>
<p>The extraction process employs a two-stage pipeline leveraging LLMs to transform unstructured text into structured graph representations. First, an entity extractor module processes the input text to identify a set of entities along with their corresponding types. In our framework, entities represent the key information elements in conversations-including people, locations, objects, concepts, events, and attributes that merit representation in the memory graph. The entity extractor identifies these diverse information units by analyzing the semantic importance, uniqueness, and persistence of elements in the conversation. For instance, in a conversation about travel plans, entities might include destinations (cities, countries), transportation modes, dates, activities, and participant preferences-essentially any discrete information that could be relevant for future reference or reasoning.</p>
<p>Next, a relationship generator component derives meaningful connections between these entities, establishing a set of relationship triplets that capture the semantic structure of the information. This LLMbased module analyzes the extracted entities and their context within the conversation to identify semantically significant connections. It works by examining linguistic patterns, contextual cues, and domain knowledge to determine how entities relate to one another. For each potential entity pair, the generator evaluates whether a meaningful relationship exists and, if so, classifies this relationship with an appropriate label (e.g., 'lives_in', 'prefers', 'owns', 'happened_on'). The module employs prompt engineering techniques that guide the LLM to reason about both explicit statements and implicit information in the dialogue, resulting in relationship triplets that form the edges in our memory graph and enable complex reasoning across interconnected information. When integrating new information, Memo ${ }^{\circledR}$ employs a sophisticated storage and update strategy. For each new relationship triple, we compute embeddings for both source and destination entities, then search for existing nodes with semantic similarity above a defined threshold ' $t$ '. Based on node existence, the system may create both nodes, create only one node, or use existing nodes before establishing the relationship with appropriate metadata. To maintain a consistent knowledge graph, we implement a conflict detection mechanism that identifies potentially conflicting existing relationships when new information arrives. An LLM-based update resolver determines if certain relationships should be obsolete, marking them as invalid rather than physically removing them to enable temporal reasoning.</p>
<p>The memory retrieval functionality in Memo ${ }^{\circledR}$ implements a dual-approach strategy for optimal information access. The entity-centric method first identifies key entities within a query, then leverages semantic similarity to locate corresponding nodes in the knowledge graph. It systematically explores both incoming and outgoing relationships from these anchor nodes, constructing a comprehensive subgraph that captures relevant contextual information. Complementing this, the semantic triplet approach takes a more holistic view by encoding the entire query as a dense embedding vector. This query representation is then matched against textual encodings of each relationship triplet in the knowledge graph. The system calculates fine-grained similarity scores between the query and all available triplets, returning only those that exceed a configurable relevance threshold, ranked in order of decreasing similarity. This dual retrieval mechanism enables Memo ${ }^{\circledR}$ to handle both targeted entity-focused questions and broader conceptual queries with equal effectiveness.</p>
<p>From an implementation perspective, the system utilizes Neo $4 \mathrm{j}^{1}$ as the underlying graph database. LLMbased extractors and update module leverage GPT-4o-mini with function calling capabilities, allowing for structured extraction of information from unstructured text. By combining graph-based representations with semantic embeddings and LLM-based information extraction, Memo ${ }^{\circledR}$ achieves both the structural richness needed for complex reasoning and the semantic flexibility required for natural language understanding.</p>
<h1>3. Experimental Setup</h1>
<h3>3.1. Dataset</h3>
<p>The LOCOMO (Maharana et al., 2024) dataset is designed to evaluate long-term conversational memory in dialogue systems. It comprises 10 extended conversations, each containing approximately 600 dialogues and 26000 tokens on average, distributed across multiple sessions. Each conversation captures two individuals discussing daily experiences or past events. Following these multi-session dialogues, each conversation is accompanied by 200 questions on an average with corresponding ground truth answers. These questions are categorized into multiple types: single-hop, multi-hop, temporal, and open-domain. The dataset originally included an adversarial question category, which was designed to test systems' ability to recognize</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>unanswerable questions. However, this category was excluded from our evaluation because ground truth answers were unavailable, and the expected behavior for this question type is that the agent should recognize them as unanswerable.</p>
<h1>3.2. Evaluation Metrics</h1>
<p>Our evaluation framework implements a comprehensive approach to assess long-term memory capabilities in dialogue systems, considering both response quality and operational efficiency. We categorize our metrics into two distinct groups that together provide a holistic understanding of system performance.
(1) Performance Metrics Previous research in conversational AI (Goswami, 2025, Soni et al., 2024, Singh et al., 2020) has predominantly relied on lexical similarity metrics such as F1 Score ( $\mathrm{F}<em 1="1">{1}$ ) and BLEU-1 ( $\mathrm{B}</em>$ ). However, these metrics exhibit significant limitations when evaluating factual accuracy in conversational contexts. Consider a scenario where the ground truth answer is 'Alice was born in March' and a system generates 'Alice is born in July.' Despite containing a critical factual error regarding the birth month, traditional metrics would assign relatively high scores due to lexical overlap in the remaining tokens ('Alice,' 'born,' etc.). This fundamental limitation can lead to misleading evaluations that fail to capture semantic correctness. To address these shortcomings, we use LLM-as-a-Judge (J) as a complementary evaluation metric. This approach leverages a separate, more capable LLM to assess response quality across multiple dimensions, including factual accuracy, relevance, completeness, and contextual appropriateness. The judge model analyzes the question, ground truth answer and the generated answer, providing a more nuanced evaluation that aligns better with human judgment. Due to the stochastic nature of J evaluations, we conducted 10 independent runs for each method on the entire dataset and report the mean scores along with $\pm 1$ standard deviation. More details about the J is present in Appendix A.
(2) Deployment Metrics Beyond response quality, practical deployment considerations are crucial for realworld applications of long-term memory in AI agents. We systematically track Token Consumption, using 'cl100k_base' encoding from tiktoken, measuring the number of tokens extracted during retrieval that serve as context for answering queries. For our memory-based models, these tokens represent the memories retrieved from the knowledge base, while for RAG-based models, they correspond to the total number of tokens in the retrieved text chunks. This distinction is important as it directly affects operational costs and system efficiency-whether processing concise memory facts or larger raw text segments. We further monitor Latency, (i) search latency: which captures the total time required to search the memory (in memory-based solutions) or chunk (in RAG-based solutions) and (ii) total latency: time to generate appropriate responses, consisting of both retrieval time (accessing memories or chunks) and answer generation time using the LLM.</p>
<p>The relationship between these metrics reveals important trade-offs in system design. For instance, more sophisticated memory architectures might achieve higher factual accuracy but at the cost of increased token consumption and latency. Our multi-dimensional evaluation methodology enables researchers and practitioners to make informed decisions based on their specific requirements, whether prioritizing response quality for critical applications or computational efficiency for real-time deployment scenarios.</p>
<h3>3.3. Baselines</h3>
<p>To comprehensively evaluate our approach, we compare against six distinct categories of baselines that represent the current state of conversational memory systems. These diverse baselines collectively provide a</p>
<p>robust framework for evaluating the effectiveness of different memory architectures across various dimensions, including factual accuracy, computational efficiency, and scalability to extended conversations. Where applicable, unless otherwise specified, we set the temperature to 0 to ensure the runs are as reproducible as possible.</p>
<p>Established LOCOMO Benchmarks We first establish a comparative foundation by evaluating previously benchmarked methods on the LOCOMO dataset. These include five established approaches: LoCoMo (Maharana et al., 2024), ReadAgent (Lee et al., 2024), MemoryBank (Zhong et al., 2024), MemGPT (Packer et al., 2023), and A-Mem (Xu et al., 2025). These established benchmarks not only provide direct comparison points with published results but also represent the evolution of conversational memory architectures across different algorithmic paradigms. For our evaluation, we select the metrics where gpt-4o-mini was used for the evaluation. More details about these benchmarks are mentioned in Appendix C.</p>
<p>Open-Source Memory Solutions Our second category consists of promising open-source memory architectures such as LangMem ${ }^{2}$ (Hot Path) that have demonstrated effectiveness in related conversational tasks but have not yet been evaluated on the LOCOMO dataset. By adapting these systems to our evaluation framework, we broaden the comparative landscape and identify potential alternative approaches that may offer competitive performance. We initialized the LLM with gpt-4o-mini and used text-embedding-small-3 as the embedding model.</p>
<p>Retrieval-Augmented Generation (RAG) As a baseline, we treat the entire conversation history as a document collection and apply a standard RAG pipeline. We first segment each conversation into fixed-length chunks (128, 256, 512, 1024, 2048, 4096, and 8192 tokens), where 8192 is the maximum chunk size supported by our embedding model. All chunks are embedded using OpenAI's text-embedding-small-3 to ensure consistent vector quality across configurations. At query time, we retrieve the top $k$ chunks by semantic similarity and concatenate them as context for answer generation. Throughout our experiments we set $k \in{1,2}$ : with $k=1$ only the single most relevant chunk is used, and with $k=2$ the two most relevant chunks (up to 16384 tokens) are concatenated. We avoid $k&gt;2$ since the average conversation length (26000 tokens) would be fully covered, negating the benefits of selective retrieval. By varying chunk size and $k$, we systematically evaluate RAG performance on long-term conversational memory tasks.</p>
<p>Full-Context Processing We adopt a straightforward approach by passing the entire conversation history within the context window of the LLM. This method leverages the model's inherent ability to process sequential information without additional architectural components. While conceptually simple, this approach faces practical limitations as conversation length increases, eventually increasing token cost and latency. Nevertheless, it establishes an important reference point for understanding the value of more sophisticated memory mechanisms compared to direct processing of available context.</p>
<p>Proprietary Models We evaluate OpenAI's memory ${ }^{3}$ feature available in their ChatGPT interface, specifically using gpt-4o-mini for consistency. We ingest entire LOCOMO conversations with a prompt (see Appendix A) into single chat sessions, prompting memory generation with timestamps, participant names, and conversation</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Performance comparison of memory-enabled systems across different question types in the LOCOMO dataset. Evaluation metrics include F1 score $\left(\mathrm{F}<em 1="1">{1}\right)$, BLEU-1 $\left(\mathrm{B}</em>$ indicates our proposed architecture enhanced with graph memory. Bold denotes the best performance for each metric across all methods. ( $\uparrow$ ) represents higher score is better.}\right)$, and LLM-as-a-Judge score (J), with higher values indicating better performance. A-Mem ${ }^{\star}$ represents results from our re-run of A-Mem to generate LLM-as-a-Judge scores by setting temperature as $0 . \mathrm{Mem}^{2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Single Hop</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multi-Hop</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Open Domain</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Temporal</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{F}_{1} \uparrow$</td>
<td style="text-align: center;">$\mathrm{B}_{1} \uparrow$</td>
<td style="text-align: center;">$\mathrm{J} \uparrow$</td>
<td style="text-align: center;">$\mathrm{F}_{1} \uparrow$</td>
<td style="text-align: center;">$\mathrm{B}_{1} \uparrow$</td>
<td style="text-align: center;">$\mathrm{J} \uparrow$</td>
<td style="text-align: center;">$\mathrm{F}_{1} \uparrow$</td>
<td style="text-align: center;">$\mathrm{B}_{1} \uparrow$</td>
<td style="text-align: center;">$\mathrm{J} \uparrow$</td>
<td style="text-align: center;">$\mathrm{F}_{1} \uparrow$</td>
<td style="text-align: center;">$\mathrm{B}_{1} \uparrow$</td>
<td style="text-align: center;">$\mathrm{J} \uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">LoCoMo</td>
<td style="text-align: center;">25.02</td>
<td style="text-align: center;">19.75</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.04</td>
<td style="text-align: center;">11.16</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40.36</td>
<td style="text-align: center;">29.05</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.41</td>
<td style="text-align: center;">14.77</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ReadAgent</td>
<td style="text-align: center;">9.15</td>
<td style="text-align: center;">6.48</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.31</td>
<td style="text-align: center;">5.12</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.67</td>
<td style="text-align: center;">7.66</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.60</td>
<td style="text-align: center;">8.87</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MemoryBank</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">4.77</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">5.94</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.61</td>
<td style="text-align: center;">5.16</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.68</td>
<td style="text-align: center;">6.99</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MemGPT</td>
<td style="text-align: center;">26.65</td>
<td style="text-align: center;">17.72</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.15</td>
<td style="text-align: center;">7.44</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.04</td>
<td style="text-align: center;">34.34</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.52</td>
<td style="text-align: center;">19.44</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">A-Mem</td>
<td style="text-align: center;">27.02</td>
<td style="text-align: center;">20.09</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.14</td>
<td style="text-align: center;">12.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.65</td>
<td style="text-align: center;">37.06</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.85</td>
<td style="text-align: center;">36.67</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">A-Mem*</td>
<td style="text-align: center;">20.76</td>
<td style="text-align: center;">14.90</td>
<td style="text-align: center;">$39.79 \pm 0.38$</td>
<td style="text-align: center;">9.22</td>
<td style="text-align: center;">8.81</td>
<td style="text-align: center;">$18.85 \pm 0.31$</td>
<td style="text-align: center;">33.34</td>
<td style="text-align: center;">27.58</td>
<td style="text-align: center;">$54.05 \pm 0.22$</td>
<td style="text-align: center;">35.40</td>
<td style="text-align: center;">31.08</td>
<td style="text-align: center;">$49.91 \pm 0.31$</td>
</tr>
<tr>
<td style="text-align: center;">LangMem</td>
<td style="text-align: center;">35.51</td>
<td style="text-align: center;">26.86</td>
<td style="text-align: center;">$62.23 \pm 0.75$</td>
<td style="text-align: center;">26.04</td>
<td style="text-align: center;">22.32</td>
<td style="text-align: center;">$47.92 \pm 0.47$</td>
<td style="text-align: center;">40.91</td>
<td style="text-align: center;">33.63</td>
<td style="text-align: center;">$71.12 \pm 0.20$</td>
<td style="text-align: center;">30.75</td>
<td style="text-align: center;">25.84</td>
<td style="text-align: center;">$23.43 \pm 0.39$</td>
</tr>
<tr>
<td style="text-align: center;">Zep</td>
<td style="text-align: center;">35.74</td>
<td style="text-align: center;">23.30</td>
<td style="text-align: center;">$61.70 \pm 0.32$</td>
<td style="text-align: center;">19.37</td>
<td style="text-align: center;">14.82</td>
<td style="text-align: center;">$41.35 \pm 0.48$</td>
<td style="text-align: center;">49.56</td>
<td style="text-align: center;">38.92</td>
<td style="text-align: center;">$76.60 \pm 0.13$</td>
<td style="text-align: center;">42.00</td>
<td style="text-align: center;">34.53</td>
<td style="text-align: center;">$49.31 \pm 0.50$</td>
</tr>
<tr>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;">34.30</td>
<td style="text-align: center;">23.72</td>
<td style="text-align: center;">$63.79 \pm 0.46$</td>
<td style="text-align: center;">20.09</td>
<td style="text-align: center;">15.42</td>
<td style="text-align: center;">$42.92 \pm 0.63$</td>
<td style="text-align: center;">39.31</td>
<td style="text-align: center;">31.16</td>
<td style="text-align: center;">$62.29 \pm 0.12$</td>
<td style="text-align: center;">14.04</td>
<td style="text-align: center;">11.25</td>
<td style="text-align: center;">$21.71 \pm 0.20$</td>
</tr>
<tr>
<td style="text-align: center;">MemO</td>
<td style="text-align: center;">38.72</td>
<td style="text-align: center;">27.13</td>
<td style="text-align: center;">$67.13 \pm 0.65$</td>
<td style="text-align: center;">28.64</td>
<td style="text-align: center;">21.58</td>
<td style="text-align: center;">$51.15 \pm 0.31$</td>
<td style="text-align: center;">47.65</td>
<td style="text-align: center;">38.72</td>
<td style="text-align: center;">$72.93 \pm 0.11$</td>
<td style="text-align: center;">48.93</td>
<td style="text-align: center;">40.51</td>
<td style="text-align: center;">$55.51 \pm 0.34$</td>
</tr>
<tr>
<td style="text-align: center;">MemO ${ }^{2}$</td>
<td style="text-align: center;">38.09</td>
<td style="text-align: center;">26.03</td>
<td style="text-align: center;">$65.71 \pm 0.45$</td>
<td style="text-align: center;">24.32</td>
<td style="text-align: center;">18.82</td>
<td style="text-align: center;">$47.19 \pm 0.67$</td>
<td style="text-align: center;">49.27</td>
<td style="text-align: center;">40.30</td>
<td style="text-align: center;">$75.71 \pm 0.21$</td>
<td style="text-align: center;">51.55</td>
<td style="text-align: center;">40.28</td>
<td style="text-align: center;">$58.13 \pm 0.44$</td>
</tr>
</tbody>
</table>
<p>text. These generated memories are then used as complete context for answering questions about each conversation, intentionally granting the OpenAI approach privileged access to all memories rather than only question-relevant ones. This methodology accommodates the lack of external API access for selective memory retrieval in OpenAI's system for benchmarking.</p>
<p>Memory Providers We incorporate Zep (Rasmussen et al., 2025), a memory management platform designed for AI agents. Using their platform version, we conduct systematic evaluations across the LOCOMO dataset, maintaining temporal fidelity by preserving timestamp information alongside conversational content. This temporal anchoring ensures that time-sensitive queries can be addressed through appropriately contextualized memory retrieval, particularly important for evaluating questions that require chronological awareness. This baseline represents an important commercial implementation of memory management specifically engineered for AI agents.</p>
<h1>4. Evaluation Results, Analysis and Discussion.</h1>
<h3>4.1. Performance Comparison Across Memory-Enabled Systems</h3>
<p>Table 1 reports $\mathrm{F}<em 1="1">{1}, \mathrm{~B}</em>$ —against a suite of competitive baselines, as mentioned in Section 3, on single-hop, multi-hop, open-domain, and temporal questions. Overall, both of our models set new state-of-the-art marks in all the three evaluation metrics for most question types.}$ and J scores for our two architectures-Mem0 and Mem0 ${ }^{2</p>
<p>Single-Hop Question Performance Single-hop queries involve locating a single factual span contained within one dialogue turn. Leveraging its dense memories in natural language text, Mem0 secures the strongest results: $\mathrm{F}<em 1="1">{1}=38.72, \mathrm{~B}</em>$ ) yields marginal performance drop compared to Mem0, indicating that relational structure provides limited utility when the retrieval target occupies a single turn. Among the existing baselines, the full-context OpenAI run attains the next-best J score, reflecting the benefits of retaining the entire conversation in context, while LangMem and Zep both score around 8\% relatively less against our models on J score. Previous LOCOMO}=27.13$, and $\mathrm{J}=67.13$. Augmenting the natural language memories with graph memory (Mem0 ${ }^{2</p>
<p>benchmarks such as A-mem lag by more than 25 points in J, underscoring the necessity of fine-grained, structured memory indexing even for simple retrieval tasks.</p>
<p>Multi-Hop Question Performance Multi-hop queries require synthesizing information dispersed across multiple conversation sessions, posing significant challenges in memory integration and retrieval. MemO clearly outperforms other methods with an $\mathrm{F}_{1}$ score of 28.64 and a J score of 51.15 , reflecting its capability to efficiently retrieve and integrate disparate information stored across sessions. Interestingly, the addition of graph memory in Memo ${ }^{\mathbb{Z}}$ does not provide performance gains here, indicating potential inefficiencies or redundancies in structured graph representations for complex integrative tasks compared to dense natural language memory alone. Baselines like LangMem show competitive performances, but their scores substantially trail those of MemO, emphasizing the advantage of our refined memory indexing and retrieval mechanisms for complex query processing.</p>
<p>Open-Domain Performance In open-domain settings, the baseline Zep achieves the highest $\mathrm{F}_{1}$ (49.56) and J (76.60) scores, edging out our methods by a narrow margin. In particular, Zep's J score of 76.60 surpasses Memo ${ }^{\mathbb{Z}}$ 's 75.71 by just 0.89 percentage points and outperforms MemO's 72.93 by 3.67 points, highlighting a consistent, if slight, advantage in integrating conversational memory with external knowledge. Memo ${ }^{\mathbb{Z}}$ remains a strong runner-up, with a J of 75.71 reflecting high factual retrieval precision, while MemO follows with 72.93, demonstrating robust coherence. These results underscore that although structured relational memories (as in MemO and Memo ${ }^{\mathbb{Z}}$ ) substantially improve open-domain retrieval, Zep maintains a small but meaningful lead.</p>
<p>Temporal Reasoning Performance Temporal reasoning tasks hinge on accurate modeling of event sequences, their relative ordering, and durations within conversational history. Our architectures demonstrate substantial improvements across all metrics, with Memo ${ }^{\mathbb{Z}}$ achieving the highest $\mathrm{F}_{1}(51.55)$ and J (58.13), suggesting that structured relational representations in addition to natural language memories significantly aid in temporally grounded judgments. Notably, the base variant, MemO, also provide a decent J score (55.51), suggesting that natural language alone can aid in temporally grounded judgments. Among baselines, OpenAI notably underperforms, with scores below $15 \%$, primarily due to missing timestamps in most generated memories despite explicit prompting in the OpenAI ChatGPT to extract memories with timestamps. Other baselines such as A-Mem achieve respectable results, yet our models clearly advance the state-of-the-art, emphasizing the critical advantage of accurately leveraging both natural language contextualization and structured graph representations for temporal reasoning.</p>
<h1>4.2. Cross-Category Analysis</h1>
<p>The comprehensive evaluation across diverse question categories reveals that our proposed architectures, MemO and Memo ${ }^{\mathbb{Z}}$, consistently achieve superior performance compared to baseline systems. For single-hop queries, MemO demonstrates particularly strong performance, benefiting from its efficient dense natural language memory structure. Although graph-based representations in Memo ${ }^{\mathbb{Z}}$ slightly lag behind in lexical overlap metrics for these simpler queries, they significantly enhance semantic coherence, as demonstrated by competitive J scores. This indicates that graph structures are more beneficial in scenarios involving nuanced relational context rather than straightforward retrieval. For multi-hop questions, MemO exhibits clear advantages by effectively synthesizing dispersed information across multiple sessions, confirming that natural language memories provide sufficient representational richness for these integrative tasks. Surprisingly, the</p>
<p>Table 2: Performance comparison of various baselines with proposed methods. Latency measurements show p50 (median) and p95 (95th percentile) values in seconds for both search time (time taken to fetch memories/chunks) and total time (time to generate the complete response). Overall LLM-as-a-Judge score (J) represents the quality metric of the generated responses on the entire LOCOMO dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Latency (seconds)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall J</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Search</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">chunk size / memory tokens</td>
<td style="text-align: center;">p50</td>
<td style="text-align: center;">p95</td>
<td style="text-align: center;">p50</td>
<td style="text-align: center;">p95</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RAG</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">1.825</td>
<td style="text-align: center;">$47.77 \pm 0.23 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">1.628</td>
<td style="text-align: center;">$50.15 \pm 0.16 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">1.710</td>
<td style="text-align: center;">$46.05 \pm 0.14 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">1.957</td>
<td style="text-align: center;">$40.74 \pm 0.17 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.752</td>
<td style="text-align: center;">0.996</td>
<td style="text-align: center;">2.182</td>
<td style="text-align: center;">$37.93 \pm 0.12 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">1.093</td>
<td style="text-align: center;">2.711</td>
<td style="text-align: center;">$36.84 \pm 0.17 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">1.396</td>
<td style="text-align: center;">4.416</td>
<td style="text-align: center;">$44.53 \pm 0.13 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">1.829</td>
<td style="text-align: center;">$59.56 \pm 0.19 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">1.907</td>
<td style="text-align: center;">$60.97 \pm 0.20 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">1.729</td>
<td style="text-align: center;">$58.19 \pm 0.18 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">1.850</td>
<td style="text-align: center;">$50.68 \pm 0.13 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">1.101</td>
<td style="text-align: center;">2.791</td>
<td style="text-align: center;">$48.57 \pm 0.22 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">1.451</td>
<td style="text-align: center;">4.822</td>
<td style="text-align: center;">$51.79 \pm 0.15 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">1.124</td>
<td style="text-align: center;">2.312</td>
<td style="text-align: center;">9.942</td>
<td style="text-align: center;">$60.53 \pm 0.16 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Full-context</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26031</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.870</td>
<td style="text-align: center;">17.117</td>
<td style="text-align: center;">$72.90 \pm 0.19 \%$</td>
</tr>
<tr>
<td style="text-align: center;">A-Mem</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2520</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">1.485</td>
<td style="text-align: center;">1.410</td>
<td style="text-align: center;">4.374</td>
<td style="text-align: center;">$48.38 \pm 0.15 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LangMem</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">127</td>
<td style="text-align: center;">17.99</td>
<td style="text-align: center;">59.82</td>
<td style="text-align: center;">18.53</td>
<td style="text-align: center;">60.40</td>
<td style="text-align: center;">$58.10 \pm 0.21 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Zep</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3911</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">1.292</td>
<td style="text-align: center;">2.926</td>
<td style="text-align: center;">$65.99 \pm 0.16 \%$</td>
</tr>
<tr>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4437</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">$52.90 \pm 0.14 \%$</td>
</tr>
<tr>
<td style="text-align: center;">MemO</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1764</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">1.440</td>
<td style="text-align: center;">$66.88 \pm 0.15 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Memo ${ }^{g}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3616</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">1.091</td>
<td style="text-align: center;">2.590</td>
<td style="text-align: center;">$68.44 \pm 0.17 \%$</td>
</tr>
</tbody>
</table>
<p>expected relational advantages of $\mathrm{MemO}^{g}$ do not translate into better outcomes here, suggesting potential overhead or redundancy when navigating more intricate graph structures in multi-step reasoning scenarios.</p>
<p>In temporal reasoning, Memo ${ }^{g}$ substantially outperforms other methods, validating that structured relational graphs excel in capturing chronological relationships and event sequences. The presence of explicit relational context significantly enhances Memo ${ }^{g}$ 's temporal coherence, outperforming MemO's dense memory storage and highlighting the importance of precise relational representations when tracking temporally sensitive information. Open-domain performance further reinforces the value of relational modeling. Memo ${ }^{g}$, benefiting from the relational clarity of graph-based memory, closely competes with the top-performing baseline (Zep). This competitive result underscores Memo ${ }^{g}$ 's robustness in integrating external knowledge through relational clarity, suggesting an optimal synergy between structured memory and open-domain information synthesis.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Comparison of search latency at p50 (median) and p95 (95th percentile) across different memory methods (Mem0, Mem0 ${ }^{\mathbb{Z}}$, best RAG variant, Zep, LangMem, and A-Mem). The bar heights represent J scores (left axis), while the line plots show search latency in seconds (right axis scaled in log).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Comparison of total response latency at p50 and p95 across different memory methods (Mem0, Mem0 ${ }^{\mathbb{Z}}$, best RAG variant, Zep, LangMem, OpenAI, full-context, and A-Mem). The bar heights represent J scores (left axis), and the line plots capture end-to-end latency in seconds (right axis scaled in log).</p>
<p>Figure 4: Latency Analysis of Different Memory Approaches. These subfigures illustrate the J scores and latency comparison of various selected methods from Table 2. Subfigure (a) highlights the search/retrieval latency prior to answer generation, while Subfigure (b) shows the total latency (including LLM inference). Both plots overlay each method's J score for a holistic view of their accuracy and efficiency.</p>
<p>Overall, our analysis indicates complementary strengths of Mem0 and Mem0 ${ }^{\mathbb{Z}}$ across various task demands: dense, natural-language-based memory offers significant efficiency for simpler queries, while explicit relational modeling becomes essential for tasks demanding nuanced temporal and contextual integration. These findings reinforce the importance of adaptable memory structures tailored to specific reasoning contexts in AI agent deployments.</p>
<h1>4.3. Performance Comparison of Mem0 and Mem0 ${ }^{\text { }}$ Against RAG Approaches and Full-Context Model</h1>
<p>Comparisons in Table 2, focusing on the 'Overall J' column, reveal that both Mem0 and Mem0 ${ }^{\text {® }}$ consistently outperform all RAG configurations, which vary chunk sizes (128-8192 tokens) and retrieve either one $(k=1)$ or two $(k=2)$ chunks. Even the strongest RAG approach peaks at around $61 \%$ in the J metric, whereas Mem0 reaches $67 \%$-about a $10 \%$ relative improvement-and Mem0 ${ }^{\text {® }}$ reaches over $68 \%$, achieving around a $12 \%$ relative gain. These advances underscore the advantage of capturing only the most salient facts in memory, rather than retrieving large chunk of original text. By converting the conversation history into concise, structured representations, Mem0 and Mem0 ${ }^{\text {® }}$ mitigate noise and surface more precise cues to the LLM, leading to better answers as evaluated by an external LLM (J).</p>
<p>Despite these improvements, a full-context method that ingests a chunk of roughly 26,000 tokens still achieves the highest J score (approximately 73\%). However, as shown in Figure 4b, it also incurs a very high total p95 latency-around 17 seconds-since the model must read the entire conversation on every query. By contrast, Mem0 and Mem0 ${ }^{\text {® }}$ significantly reduce token usage and thus achieve lower p95 latencies of around 1.44 seconds (a $92 \%$ reduction) and 2.6 seconds (a $85 \%$ reduction), respectively over full-context approach. Although the full-context approach can provide a slight accuracy edge, the memory-based systems offer a more practical trade-off, maintaining near-competitive quality while imposing only a fraction of the token and latency cost. As conversation length increases, full-context approaches suffer from exponential growth in computational overhead (evident in Table 2 where total p95 latency increases significantly with larger $k$ values or chunk sizes). This increase in input chunks leads to longer response times and higher token consumption costs. In contrast, memory-focused approaches like Mem0 and Mem0 ${ }^{\text {® }}$ maintain consistent performance regardless of conversation length, making them substantially more viable for production-scale deployments where efficiency and responsiveness are critical.</p>
<h3>4.4. Latency Analysis</h3>
<p>Table 2 provides a comprehensive performance comparison of various retrieval and memory methodologies, presenting median (p50) and tail (p95) latencies for both the search phase and total response generation across the LOCOMO dataset. Our analysis reveals distinct performance patterns governed by architectural choices. Memory-centric architectures demonstrate different performance characteristics. A-Mem, despite its larger memory store, incurs substantial search overhead (p50: 0.668 s ), resulting in total median latencies of 1.410s. LangMem exhibits even higher search latencies (p50: 17.99s, p95: 59.82s), rendering it impractical for interactive applications. Zep achieves moderate performance (p50 total: 1.292s). The full-context baseline, which processes the entire conversation history without retrieval, fundamentally differs from retrieval-based approaches. By passing the entire conversation context ( 26000 tokens) directly to the LLM, it eliminates search overhead but incurs extreme total latencies (p50: 9.870s, p95: 17.117s). Similarly, the OpenAI implementation does not perform memory search, as it processes manually extracted memories from their playground. While this approach achieves impressive response generation times (p50: 0.466s, p95: 0.889 s ), it requires pre-extraction of relevant context, which is not reflected in the reported metrics.</p>
<p>Our proposed Mem0 approach achieves the lowest search latency among all methods (p50: 0.148s, p95: 0.200 s ) as illustrated in Figure 4a. This efficiency stems from our selective memory retrieval mechanism and infra improvements that dynamically identifies and retrieves only the most salient information rather than fixed-size chunks. Consequently, Mem0 maintains the lowest total median latency ( 0.708 s ) with remarkably contained p95 values ( 1.440 s ), making it particularly suitable for latency-sensitive applications such as interactive AI agents. The graph-enhanced Mem0 ${ }^{\text {® }}$ variant introduces additional relational modeling capabilities at a moderate latency cost, with search times ( 0.476 s ) still outperforming all existing memory</p>
<p>solutions and baselines. Despite this increase, Memo ${ }^{\circledR}$ maintains competitive total latencies (p50: 1.091s, p95: 2.590s) while achieving the highest J score ( $68.44 \%$ ) across all methods-trailing only the computationally prohibitive full-context approach. This performance profile demonstrates our methods' ability to balance response quality and computational efficiency, offering a compelling solution for production AI agents where both factors are critical constraints.</p>
<h1>4.5. Memory System Overhead: Token Analysis and Construction Time</h1>
<p>We measure the average token budget required to materialise each system's long-term memory store. Memo encodes complete dialogue turns in a natural language representation and therefore occupies only $\mathbf{7 k}$ tokens per conversation on an average. Where as Memo ${ }^{\circledR}$ roughly doubles the footprint to $\mathbf{1 4 k}$ tokens, due to the introduction of graph memories which includes nodes and corresponding relationships. In stark contrast, Zep's memory graph consumes in excess of $\mathbf{6 0 0 k}$ tokens. The inflation arises from Zep's design choice to cache a full abstractive summary at every node while also storing facts on the connecting edges, leading to extensive redundancy across the graph. For perspective, supplying the entire raw conversation context to the language model-without any memory abstraction-amounts to roughly $\mathbf{2 6 k}$ tokens on average, 20 times less relative to Zep's graph. Beyond token inefficiency, our experiments revealed significant operational bottlenecks with Zep. After adding memories to Zep's system, we observed that immediate memory retrieval attempts often failed to answer our queries correctly. Interestingly, re-running identical searches after a delay of several hours yielded considerably better results. This latency suggests that Zep's graph construction involves multiple asynchronous LLM calls and extensive background processing, making the memory system impractical for real-time applications. In contrast, Memo graph construction completes in under a minute even in worst-case scenarios, allowing users to immediately leverage newly added memories for query responses.</p>
<p>These findings highlight that Zep not only replicates identical knowledge fragments across multiple nodes, but also introduces significant operational delays. Our architectures-Memo and Memo ${ }^{\circledR}$ —preserve the same information at a fraction of the token cost and with substantially faster memory availability, offering a more memory-efficient and operationally responsive representation.</p>
<h2>5. Conclusion and Future Work</h2>
<p>We have introduced Memo and Memo ${ }^{\circledR}$, two complementary memory architectures that overcome the intrinsic limitations of fixed context windows in LLMs. By dynamically extracting, consolidating, and retrieving compact memory representations, Memo achieves state-of-the-art performance across single-hop and multi-hop reasoning, while Memo ${ }^{\circledR}$ 's graph-based extensions unlock significant gains in temporal and open-domain tasks. On the LOCOMO benchmark, our methods deliver $5 \%, 11 \%$, and $7 \%$ relative improvements in single-hop, temporal, and multi-hop reasoning question types over best performing methods in respective question type and reduce p95 latency by over $91 \%$ compared to full-context baselines-demonstrating a powerful balance between precision and responsiveness. Memo's dense memory pipeline excels at rapid retrieval for straightforward queries, minimizing token usage and computational overhead. In contrast, Memo ${ }^{\circledR}$ 's structured graph representations provide nuanced relational clarity, enabling complex event sequencing and rich context integration without sacrificing practical efficiency. Together, they form a versatile memory toolkit that adapts to diverse conversational demands while remaining deployable at scale.</p>
<p>Future research directions include optimizing graph operations to reduce the latency overhead in Memo ${ }^{\circledR}$, exploring hierarchical memory architectures that blend efficiency with relational representation, and developing more sophisticated memory consolidation mechanisms inspired by human cognitive processes.</p>
<p>Additionally, extending our memory frameworks to domains beyond conversational scenarios, such as procedural reasoning and multimodal interactions, would further validate their broader applicability. By addressing the fundamental limitations of fixed context windows, our work represents a significant advancement toward conversational AI systems capable of maintaining coherent, contextually rich interactions over extended periods, much like their human counterparts.</p>
<h1>6. Acknowledgments</h1>
<p>We would like to express our sincere gratitude to Harsh Agarwal, Shyamal Anadkat, Prithvijit Chattopadhyay, Siddesh Choudhary, Rishabh Jain, and Vaibhav Pandey for their invaluable insights and thorough reviews of early drafts. Their constructive comments and detailed suggestions helped refine the manuscript, enhancing both its clarity and overall quality. We deeply appreciate their generosity in dedicating time and expertise to this work.</p>
<h2>References</h2>
<p>Anthropic. Model card and evaluations for claude models. Technical report, Anthropic, February 2025. URL https://www.anthropic.com/news/claude-3-7-sonnet.</p>
<p>Jan Assmann. Communicative and cultural memory. In Cultural memories: The geographical point of view, pages 15-27. Springer, 2011.</p>
<p>Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079-11091, 2022.</p>
<p>Prateek Chhikara, Jiarui Zhang, Filip Ilievski, Jonathan Francis, and Kaixin Ma. Knowledge-enhanced agents for interactive text games. In Proceedings of the 12th Knowledge Capture Conference 2023, pages 157-165, 2023.</p>
<p>Fergus IM Craik and Janine M Jennings. Human memory. 1992.
Gaurav Goswami. Dissecting the metrics: How different evaluation approaches yield diverse results for conversational ai. Authorea Preprints, 2025.</p>
<p>Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael Jordan, and Song Mei. Active-dormant attention heads: Mechanistically demystifying extreme-token phenomena in llms. In NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning, 2024.</p>
<p>Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term memory in llm-agents. In Proceedings of the AAAI Symposium Series, volume 2, pages 277-280, 2023.</p>
<p>Zihong He, Weizhe Lin, Hao Zheng, Fan Zhang, Matt W Jones, Laurence Aitchison, Xuhai Xu, Miao Liu, Per Ola Kristensson, and Junxiao Shen. Human-inspired perspectives: A survey on ai long-term memory. arXiv preprint arXiv:2411.00489, 2024.</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.</p>
<p>Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. A human-inspired reading agent with gist memory of very long contexts. In International Conference on Machine Learning, pages 26396-26415. PMLR, 2024.</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-inmemory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719, 2023.</p>
<p>Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851-13870, 2024.</p>
<p>Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. Clin: A continually learning language agent for rapid task adaptation and generalization. In First Conference on Language Modeling.</p>
<p>Elliot Nelson, Georgios Kollias, Payel Das, Subhajit Chaudhury, and Soham Dan. Needle in the haystack for memory based large language models. arXiv preprint arXiv:2407.01437, 2024.</p>
<p>Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt: Towards llms as operating systems. 2023.</p>
<p>Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. Zep: A temporal knowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956, 2025.</p>
<p>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023.</p>
<p>Prabhjot Singh, Prateek Chhikara, and Jasmeet Singh. An ensemble approach for extractive text summarization. In 2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE), pages 1-7. IEEE, 2020.</p>
<p>Arpita Soni, Rajeev Arora, Anoop Kumar, and Dheerendra Panwar. Evaluating domain coverage in lowresource generative chatbots: A comparative study of open-domain and closed-domain approaches using bleu scores. In 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT), volume 1, pages 1-6. IEEE, 2024.</p>
<p>Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.</p>
<p>Joan C Timoneda and Sebastián Vallejo Vera. Memory is all you need: Testing how model memory affects llm performance in annotation tasks. arXiv preprint arXiv:2503.04874, 2025.</p>
<p>Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025.</p>
<p>Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced llm trading agent with layered memory and character design. In Proceedings of the AAAI Symposium Series, volume 3, pages 595-597, 2024.</p>
<p>Jiarui Zhang. Guided profile generation improves personalization with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 4005-4016, 2024.</p>
<p>Jiarui Zhang, Filip Ilievski, Kaixin Ma, Jonathan Francis, and Alessandro Oltramari. A study of zero-shot adaptation with commonsense knowledge. In $A K B C, 2022$.</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. A survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024.</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19724-19731, 2024.</p>
<h1>Appendix</h1>
<h2>A. Prompts</h2>
<p>In developing our LLM-as-a-Judge prompt, we adapt elements from the prompt released by Packer et al. (2023).</p>
<h2>PRoMPT TEMPLATE FOR LLM as a JUDGE</h2>
<p>Your task is to label an answer to a question as "CORRECT" or "WRONG". You will be given the following data: (1) a question (posed by one user to another user), (2) a 'gold' (ground truth) answer, (3) a generated answer which you will score as CORRECT/WRONG.</p>
<p>The point of the question is to ask about something one user should know about the other user based on their prior conversations. The gold answer will usually be a concise and short answer that includes the referenced topic, for example:
Question: Do you remember what I got the last time I went to Hawaii?
Gold answer: A shell necklace
The generated answer might be much longer, but you should be generous with your grading - as long as it touches on the same topic as the gold answer, it should be counted as CORRECT.</p>
<p>For time related questions, the gold answer will be a specific date, month, year, etc. The generated answer might be much longer or use relative time references (like 'last Tuesday' or 'next month'), but you should be generous with your grading - as long as it refers to the same date or time period as the gold answer, it should be counted as CORRECT. Even if the format differs (e.g., 'May 7th' vs '7 May'), consider it CORRECT if it's the same date.</p>
<p>Now it's time for the real question:
Question: {question}
Gold answer: {gold_answer}
Generated answer: {generated_answer}</p>
<p>First, provide a short (one sentence) explanation of your reasoning, then finish with CORRECT or WRONG. Do NOT include both CORRECT and WRONG in your response, or it will break the evaluation script.</p>
<p>Just return the label CORRECT or WRONG in a json format with the key as "label".</p>
<h1>PROMPT TEMPLATE FOR ReSULtS GENERATION (REN0)</h1>
<p>You are an intelligent memory assistant tasked with retrieving accurate information from conversation memories.</p>
<h2># CONTEXT:</h2>
<p>You have access to memories from two speakers in a conversation. These memories contain timestamped information that may be relevant to answering the question.</p>
<h2># INSTRUCTIONS:</h2>
<ol>
<li>Carefully analyze all provided memories from both speakers</li>
<li>Pay special attention to the timestamps to determine the answer</li>
<li>If the question asks about a specific event or fact, look for direct evidence in the memories</li>
<li>If the memories contain contradictory information, prioritize the most recent memory</li>
<li>If there is a question about time references (like "last year", "two months ago", etc.), calculate the actual date based on the memory timestamp. For example, if a memory from 4 May 2022 mentions "went to India last year," then the trip occurred in 2021.</li>
<li>Always convert relative time references to specific dates, months, or years. For example, convert "last year" to "2022" or "two months ago" to "March 2023" based on the memory timestamp. Ignore the reference while answering the question.</li>
<li>Focus only on the content of the memories from both speakers. Do not confuse character names mentioned in memories with the actual users who created those memories.</li>
<li>The answer should be less than 5-6 words.
# APPROACH (Think step by step):</li>
<li>First, examine all memories that contain information related to the question</li>
<li>Examine the timestamps and content of these memories carefully</li>
<li>Look for explicit mentions of dates, times, locations, or events that answer the question</li>
<li>If the answer requires calculation (e.g., converting relative time references), show your work</li>
<li>Formulate a precise, concise answer based solely on the evidence in the memories</li>
<li>Double-check that your answer directly addresses the question asked</li>
<li>Ensure your final answer is specific and avoids vague time references</li>
</ol>
<p>Memories for user {speaker_1_user_id}: {speaker_1_memories}</p>
<p>Memories for user {speaker_2_user_id}: {speaker_2_memories}</p>
<p>Question: {question}</p>
<p>Answer:</p>
<h1>PROMPT TEMPLATE FOR RESULIS GENERATION (RENO)</h1>
<div class="codehilite"><pre><span></span><code><span class="ss">(</span><span class="nv">same</span><span class="w"> </span><span class="nv">as</span><span class="w"> </span><span class="nv">previous</span><span class="ss">)</span>
#<span class="w"> </span><span class="nv">APPROACH</span><span class="w"> </span><span class="ss">(</span><span class="nv">Think</span><span class="w"> </span><span class="nv">step</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">step</span><span class="ss">)</span>:
<span class="mi">1</span>.<span class="w"> </span><span class="nv">First</span>,<span class="w"> </span><span class="nv">examine</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">memories</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">contain</span><span class="w"> </span><span class="nv">information</span><span class="w"> </span><span class="nv">related</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">question</span>
<span class="mi">2</span>.<span class="w"> </span><span class="nv">Examine</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">timestamps</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">content</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">these</span><span class="w"> </span><span class="nv">memories</span><span class="w"> </span><span class="nv">carefully</span>
<span class="mi">3</span>.<span class="w"> </span><span class="nv">Look</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">explicit</span><span class="w"> </span><span class="nv">mentions</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">dates</span>,<span class="w"> </span><span class="nv">times</span>,<span class="w"> </span><span class="nv">locations</span>,<span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">events</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">the</span>
</code></pre></div>

<p>question
4. If the answer requires calculation (e.g., converting relative time references), show your work
5. Analyze the knowledge graph relations to understand the user's knowledge context
6. Formulate a precise, concise answer based solely on the evidence in the memories
7. Double-check that your answer directly addresses the question asked
8. Ensure your final answer is specific and avoids vague time references</p>
<p>Memories for user {speaker_1_user_id}:
{speaker_1_memories}</p>
<p>Relations for user {speaker_1_user_id}:
{speaker_1_graph_memories}</p>
<p>Memories for user {speaker_2_user_id}:
{speaker_2_memories}</p>
<p>Relations for user {speaker_2_user_id}:
{speaker_2_graph_memories}</p>
<p>Question: {question}</p>
<p>Answer:</p>
<h2>PROMPT TEMPLATE FOR OPENAI CHATGPT</h2>
<p>Can you please extract relevant information from this conversation and create memory entries for each user mentioned? Please store these memories in your knowledge base in addition to the timestamp provided for future reference and personalized interactions.
(1:56 pm on 8 May, 2023) Caroline: Hey Mel! Good to see you! How have you been? (1:56 pm on 8 May, 2023) Melanie: Hey Caroline! Good to see you! I'm swamped with the kids \&amp; work. What's up with you? Anything new?
(1:56 pm on 8 May, 2023) Caroline: I went to a LGBTQ support group yesterday and it was so powerful.</p>
<h1>B. Algorithm</h1>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Memory Management System: Update Operations
    Input: Set of retrieved memories \(F\), Existing memory store \(M=\left\{m_{1}, m_{2}, \ldots, m_{n}\right\}\)
    Output: Updated memory store \(M^{\prime}\)
    procedure UpdateMemory \((F, M)\)
        for each fact \(f \in F\) do
            operation \(\leftarrow\) ClassifyOperation \((f, M) \quad \triangleright\) Execute appropriate operation based on
    classification
            if operation \(=\) ADD then
                id \(\leftarrow\) GenerateUniqueID( )
                \(M \leftarrow M \cup\left\{\left(i d, f, &quot; A D D^{\prime}\right)\right\} \quad \triangleright\) Add new fact with unique identifier
            else if operation \(=\) UPDATE then
                \(m_{i} \leftarrow\) FindRelatedMemory \((f, M)\)
                if InformationContent \((f)\) &gt; InformationContent \(\left(m_{i}\right)\) then
                    \(M \leftarrow\left(M \backslash\left\{m_{i}\right\}\right) \cup\left\{\left(i d_{i}, f, &quot; U P D A T E^{\prime}\right)\right\} \quad \triangleright\) Replace with richer information
                end if
            else if operation \(=\) DELETE then
                \(m_{i} \leftarrow\) FindContradictedMemory \((f, M)\)
                \(M \leftarrow M \backslash\left\{m_{i}\right\} \quad \triangleright\) Remove contradicted information
            else if operation \(=\) NOOP then
                No operation performed \(\quad \triangleright\) Fact already exists or is irrelevant
            end if
        end for
        return \(M\)
    end procedure
</code></pre></div>

<p>function ClassifyOperation $(f, M)$
if $\neg$ SemanticallySimilar $(f, M)$ then
return ADD
else if Contradicts $(f, M)$ then
return DELETE
else if Augments $(f, M)$ then
return UPDATE
else
return NOOP
end if
end function</p>
<h2>C. Selected Baselines</h2>
<p>LoCoMo The LoCoMo framework implements a sophisticated memory pipeline that enables LLM agents to maintain coherent, long-term conversations. At its core, the system divides memory into short-term and long-term components. After each conversation session, agents generate summaries (stored as short-term memory) that distill key information from that interaction. Simultaneously, individual conversation turns are</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://langchain-ai.github.io/langmem/
${ }^{3}$ https://openai.com/index/memory-and-new-controls-for-chatgpt/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
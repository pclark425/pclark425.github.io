<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9180 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9180</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9180</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-261214653</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/29872/31521" target="_blank">SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</a></p>
                <p><strong>Paper Abstract:</strong> Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a"dynamic"subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9180.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9180.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model accessed via API and evaluated in this paper on scientific question/answering and dynamic simulation-like tasks in chemistry and physics; model size and exact training data are undisclosed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed‑weight OpenAI model (undisclosed parameter count), instruction‑tuned and RLHF‑refined; accessed via API for all SciEval subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dynamic chemistry (molecular properties / SMILES / numerical molecular data) and dynamic physics (formula-based calculation problems); also evaluated on experimental-design questions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-only simulation-like tasks generated from scientific principles: (1) chemistry dynamic tasks such as computing molecular properties (e.g., molecular weight), producing SMILES or other string representations, and numerical property prediction; (2) physics dynamic tasks that are multiple-choice calculation problems based on physics formulas; (3) answering experimental design and analysis prompts in Experimental Data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Chemistry: accuracy (percent), BLEU for string (SMILES) outputs, mean squared error (MSE) for numeric outputs; Physics: multiple-choice accuracy (percent); Experimental Data: manual/human assessment scores.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Dynamic Chemistry (Answer-Only): 11.05% accuracy (BLEU reported in paper ≈ 23.78 for GPT-4 on chemistry dynamic subset). Dynamic Physics (Answer-Only): 25.84% accuracy (near chance = 25%); Chain-of-Thought and 3-shot settings change results: chemistry CoT 11.65% (↑), 3-shot 12.42% (↑); physics CoT 17.98% (↓), 3-shot 51.01% (↑). Experimental Data: strong on principles/design, poor on result analysis (qualitative assessment).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Prompting strategy: Chain-of-Thought (CoT) and 3-shot examples materially change performance depending on task and model (e.g., GPT-4 physics improved to 51.01% under 3-shot).', 'Task type: string outputs (SMILES) evaluated with BLEU; numerical calculations penalized by high MSE for many models.', 'Problem domain: physics calculation problems proved especially challenging; chemistry tasks require molecule-specific knowledge.', 'Availability of domain-specific training data: models trained on scientific corpora performed differently (see Galactica discussion).', 'Model tendency to select incorrect formulas or reasoning chains (documented incorrect formula usage).']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared primarily to other evaluated LLMs in the benchmark (GPT-3.5-turbo, Claude, Galactica, Vicuna, LLaMa variants, etc.). GPT-4 achieves top average performance on SciEval static data and top average accuracy and BLEU on dynamic data among tested models, but is outperformed in some computational/counting tasks by Galactica-30B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Often uses incorrect formulas for physics problems (leading to poor CoT physics performance), low accuracy on chemistry numerical/string tasks (chemistry dynamic accuracy still low despite best BLEU), and poor ability to analyze experimental results; dynamic chemistry numeric MSEs can be large (noted generically).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note GPT-4 is strongest overall but still far from perfect on dynamic, calculation-heavy tasks; CoT and few-shot can improve results in some settings; augmenting training with large-scale scientific corpora and improving numerical/reasoning capabilities are recommended directions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9180.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9180.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned OpenAI LLM accessed via API; evaluated on SciEval's static, dynamic and experimental subsets and shown to be among the stronger non-GPT-4 models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed‑weight OpenAI model (undisclosed parameter count), instruction tuned and RLHF-refined; accessed via API.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dynamic chemistry (molecular properties) and dynamic physics (calculation multiple-choice questions); Experimental Data (design/principle performance).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same SciEval dynamic tasks: chemistry property/string generation and physics formula-based multiple-choice calculation problems; experimental-principle and analysis Q/A.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Chemistry: accuracy (%) (and BLEU/MSE where relevant); Physics: multiple-choice accuracy (%).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Dynamic Chemistry (Answer-Only): 7.65% accuracy; CoT improved chemistry to 10.20% and 3-shot to 8.85%. Dynamic Physics (Answer-Only): 21.80% accuracy; CoT physics rose to 47.19% (large CoT improvement), 3-shot 25.39% (≈AO).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Prompting mode: CoT dramatically improved physics accuracy for GPT-3.5-turbo (21.80% → 47.19%).', 'Domain knowledge coverage: limited molecule-specific knowledge reduced chemistry performance.', 'Task complexity: numerical and counting tasks are challenging and produce high error rates.']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-4 and other LLMs; GPT-3.5-turbo is competitive (surpasses 60% average on Static Data together with GPT-4 and Claude in static evaluation) but lags GPT-4 on dynamic tasks overall except where CoT substantially helps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor performance on chemistry dynamic tasks (near-random in many cases), struggles on experimental result analysis; physics performance highly sensitive to CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>CoT can yield substantial improvements for some physics calculation problems; training on domain-specific corpora and improved numerical reasoning modules would likely help.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9180.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9180.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (30B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of LLMs trained with a large scientific corpus; the 30B parameter variant appears in the evaluation and shows strengths on computation-related dynamic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open model family (Meta) with scientific-corpus-focused pretraining; the 30B-parameter variant was evaluated (paper reports Galactica-30B and Galactica-6.7B).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dynamic chemistry numeric/ computational tasks and dynamic physics multiple-choice calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Chemistry dynamic tasks (numerical/counting/SMILES-like tasks) and physics multiple-choice calculation tasks generated from formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Chemistry: accuracy (%), BLEU, MSE for numeric answers; Physics: multiple-choice accuracy (%).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Dynamic Chemistry (Answer-Only): very low reported accuracy (Galactica-30B AO chemistry ≈ 0.90% with CoT/3-shot increases to 2.60%/3.30% respectively). Dynamic Physics (Answer-Only): ~22.47% accuracy; CoT/3-shot had small changes (CoT 14.72% ↓ in one table cell but other places show small variations). Notably, Galactica performed best among compared models on some counting/calculation chemistry problems (text claim in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>["Scientific-corpus pretraining: authors attribute Galactica's relatively stronger computational chemistry/counting ability to its scientific training data.", 'Prompting: CoT and 3-shot provide only modest gains for Galactica on chemistry, somewhat better on physics.', 'Model size and inductive bias toward scientific content.']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to GPT-series and other LLMs in the benchmark; Galactica outperforms many models with comparable parameter counts on computational chemistry/counting tasks despite less total pretraining data, indicating the benefit of scientific-corpus training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Absolute accuracies remain low on dynamic chemistry string/numeric tasks (near-zero percent in AO for some chemistry tasks) and physics multiple-choice still near-chance; unable to robustly analyze experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Training on a large scientific corpus benefits scientific calculation tasks; however, more work is needed to raise absolute accuracy on dynamic simulation-like tasks (better numerical precision and domain generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9180.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9180.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (6.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller Galactica variant (6.7B parameters) trained on scientific text; evaluated in SciEval and shows similar qualitative behavior to the 30B variant but with lower absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>6.7B parameter variant of Galactica family, pretrained on scientific corpora (as described in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dynamic chemistry (molecular numeric/string tasks) and dynamic physics (calculation multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same SciEval dynamic tasks: chemistry property numeric/string generation and physics formula-based multiple-choice calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Chemistry: accuracy (%) and MSE/BLEU where applicable; Physics: multiple-choice accuracy (%).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Dynamic Chemistry (Answer-Only): ~1.55% accuracy (CoT 1.75%, 3-shot 3.05%). Dynamic Physics (Answer-Only): ~20.79% accuracy; CoT/3-shot small/modest changes (CoT 23.37%, 3-shot ~21.12%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Smaller parameter count relative to 30B limits absolute capability.', 'Scientific pretraining helps computational tasks but small model size limits performance.', 'Prompting mode yields modest gains.']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Underperforms Galactica-30B and GPT-4 on average but performs better than many non-scientificly-trained LLMs on certain computation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very low chemistry dynamic accuracy; physics results near-chance; analysis of experimental results remains poor.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Reinforces the point that domain-focused pretraining helps, but model scale and numeric/reasoning modules remain limiting factors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9180.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9180.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other evaluated LLMs (Claude, Vicuna, LLaMA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-v1.3, Claude-instant, Vicuna-13B, LLaMa variants, ChatGLM variants, Alpaca, MOSS, ERNIE Bot, SparkDesk (evaluated group)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of diverse LLMs (API/web-accessible closed models and open-weight models of various sizes) evaluated on SciEval; collectively show that few models exceed moderate performance on dynamic simulation-like tasks, with generally poor numeric/calculation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Group: Claude-v1.3 / Claude-instant / Vicuna-13B / LLaMa-7B/13B / ChatGLM2-6B / Alpaca-7B / MOSS-16B / ERNIE Bot / SparkDesk</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixed set of architectures and sizes: Claude (Anthropic, API, undisclosed), Vicuna (13B open-weight fine-tuned), LLaMa (7B/13B open-weight), ChatGLM (6B/extended), Alpaca (7B instruction-tuned), MOSS (16B), ERNIE Bot and SparkDesk (web access only).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dynamic chemistry and physics subsets of SciEval; Experimental Data (some web-accessible models evaluated on Experimental Data only).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same dynamic tasks: chemistry numeric/string property generation and physics formula multiple-choice problems; experimental-principle and design Q/A in Experimental Data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Chemistry: accuracy (%), BLEU and MSE for string/numeric outputs; Physics: accuracy (%) for multiple-choice; Experimental: manual scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Group-level trends: Most non-GPT/Galactica models show very low chemistry dynamic accuracies (often ≪10%), near-random physics accuracies (~25% or slightly above chance), and poor numeric MSE/BLEU; examples: Vicuna-13B chemistry AO ≈ 0.95%, physics AO ≈ 21.24%; LLaMa-7B chemistry AO ≈ 0.50%, physics AO ≈ 18.65%. ERNIE Bot achieved ~61.12% on Static Data (but dynamic and experimental coverage varies).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Model scale and architecture (smaller open models perform worse).', 'Lack of dedicated scientific pretraining or domain-specific corpora reduces chemistry/molecule knowledge.', 'Limited chain-of-thought capability in many models reduces benefit from CoT prompting.', 'API/web-access limitations constrained evaluation modes for some models (e.g., no CoT available for Claude variants in some evaluations).']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-series and Galactica, these models generally underperform on dynamic chemistry and physics tasks; a few models do reasonably on static (non-dynamic) tasks but not on simulation-like dynamic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Near-random performance on physics dynamic subset for many models; very poor numeric/string chemistry outputs (high MSE, low BLEU/accuracy); limited CoT effectiveness and inability to analyze experimental results robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors emphasize the importance of scientific-corpus training, better numerical reasoning capabilities, and improved CoT ability; dynamic data generation is important to prevent evaluation leakage and better test true generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What indeed can GPT models do in chemistry? <em>(Rating: 2)</em></li>
                <li>ChemLLMBench <em>(Rating: 2)</em></li>
                <li>Neural scaling of deep chemical models <em>(Rating: 1)</em></li>
                <li>Galactica <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9180",
    "paper_id": "paper-261214653",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A state-of-the-art large language model accessed via API and evaluated in this paper on scientific question/answering and dynamic simulation-like tasks in chemistry and physics; model size and exact training data are undisclosed in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed‑weight OpenAI model (undisclosed parameter count), instruction‑tuned and RLHF‑refined; accessed via API for all SciEval subsets.",
            "scientific_subdomain": "Dynamic chemistry (molecular properties / SMILES / numerical molecular data) and dynamic physics (formula-based calculation problems); also evaluated on experimental-design questions.",
            "simulation_task": "Text-only simulation-like tasks generated from scientific principles: (1) chemistry dynamic tasks such as computing molecular properties (e.g., molecular weight), producing SMILES or other string representations, and numerical property prediction; (2) physics dynamic tasks that are multiple-choice calculation problems based on physics formulas; (3) answering experimental design and analysis prompts in Experimental Data.",
            "evaluation_metric": "Chemistry: accuracy (percent), BLEU for string (SMILES) outputs, mean squared error (MSE) for numeric outputs; Physics: multiple-choice accuracy (percent); Experimental Data: manual/human assessment scores.",
            "simulation_accuracy": "Dynamic Chemistry (Answer-Only): 11.05% accuracy (BLEU reported in paper ≈ 23.78 for GPT-4 on chemistry dynamic subset). Dynamic Physics (Answer-Only): 25.84% accuracy (near chance = 25%); Chain-of-Thought and 3-shot settings change results: chemistry CoT 11.65% (↑), 3-shot 12.42% (↑); physics CoT 17.98% (↓), 3-shot 51.01% (↑). Experimental Data: strong on principles/design, poor on result analysis (qualitative assessment).",
            "factors_affecting_accuracy": [
                "Prompting strategy: Chain-of-Thought (CoT) and 3-shot examples materially change performance depending on task and model (e.g., GPT-4 physics improved to 51.01% under 3-shot).",
                "Task type: string outputs (SMILES) evaluated with BLEU; numerical calculations penalized by high MSE for many models.",
                "Problem domain: physics calculation problems proved especially challenging; chemistry tasks require molecule-specific knowledge.",
                "Availability of domain-specific training data: models trained on scientific corpora performed differently (see Galactica discussion).",
                "Model tendency to select incorrect formulas or reasoning chains (documented incorrect formula usage)."
            ],
            "comparison_baseline": "Compared primarily to other evaluated LLMs in the benchmark (GPT-3.5-turbo, Claude, Galactica, Vicuna, LLaMa variants, etc.). GPT-4 achieves top average performance on SciEval static data and top average accuracy and BLEU on dynamic data among tested models, but is outperformed in some computational/counting tasks by Galactica-30B.",
            "limitations_or_failure_cases": "Often uses incorrect formulas for physics problems (leading to poor CoT physics performance), low accuracy on chemistry numerical/string tasks (chemistry dynamic accuracy still low despite best BLEU), and poor ability to analyze experimental results; dynamic chemistry numeric MSEs can be large (noted generically).",
            "author_recommendations_or_insights": "Authors note GPT-4 is strongest overall but still far from perfect on dynamic, calculation-heavy tasks; CoT and few-shot can improve results in some settings; augmenting training with large-scale scientific corpora and improving numerical/reasoning capabilities are recommended directions.",
            "uuid": "e9180.0",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "An instruction-tuned OpenAI LLM accessed via API; evaluated on SciEval's static, dynamic and experimental subsets and shown to be among the stronger non-GPT-4 models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Closed‑weight OpenAI model (undisclosed parameter count), instruction tuned and RLHF-refined; accessed via API.",
            "scientific_subdomain": "Dynamic chemistry (molecular properties) and dynamic physics (calculation multiple-choice questions); Experimental Data (design/principle performance).",
            "simulation_task": "Same SciEval dynamic tasks: chemistry property/string generation and physics formula-based multiple-choice calculation problems; experimental-principle and analysis Q/A.",
            "evaluation_metric": "Chemistry: accuracy (%) (and BLEU/MSE where relevant); Physics: multiple-choice accuracy (%).",
            "simulation_accuracy": "Dynamic Chemistry (Answer-Only): 7.65% accuracy; CoT improved chemistry to 10.20% and 3-shot to 8.85%. Dynamic Physics (Answer-Only): 21.80% accuracy; CoT physics rose to 47.19% (large CoT improvement), 3-shot 25.39% (≈AO).",
            "factors_affecting_accuracy": [
                "Prompting mode: CoT dramatically improved physics accuracy for GPT-3.5-turbo (21.80% → 47.19%).",
                "Domain knowledge coverage: limited molecule-specific knowledge reduced chemistry performance.",
                "Task complexity: numerical and counting tasks are challenging and produce high error rates."
            ],
            "comparison_baseline": "Compared against GPT-4 and other LLMs; GPT-3.5-turbo is competitive (surpasses 60% average on Static Data together with GPT-4 and Claude in static evaluation) but lags GPT-4 on dynamic tasks overall except where CoT substantially helps.",
            "limitations_or_failure_cases": "Poor performance on chemistry dynamic tasks (near-random in many cases), struggles on experimental result analysis; physics performance highly sensitive to CoT prompting.",
            "author_recommendations_or_insights": "CoT can yield substantial improvements for some physics calculation problems; training on domain-specific corpora and improved numerical reasoning modules would likely help.",
            "uuid": "e9180.1",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Galactica-30B",
            "name_full": "Galactica (30B)",
            "brief_description": "A family of LLMs trained with a large scientific corpus; the 30B parameter variant appears in the evaluation and shows strengths on computation-related dynamic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Galactica-30B",
            "model_description": "Open model family (Meta) with scientific-corpus-focused pretraining; the 30B-parameter variant was evaluated (paper reports Galactica-30B and Galactica-6.7B).",
            "scientific_subdomain": "Dynamic chemistry numeric/ computational tasks and dynamic physics multiple-choice calculations.",
            "simulation_task": "Chemistry dynamic tasks (numerical/counting/SMILES-like tasks) and physics multiple-choice calculation tasks generated from formulas.",
            "evaluation_metric": "Chemistry: accuracy (%), BLEU, MSE for numeric answers; Physics: multiple-choice accuracy (%).",
            "simulation_accuracy": "Dynamic Chemistry (Answer-Only): very low reported accuracy (Galactica-30B AO chemistry ≈ 0.90% with CoT/3-shot increases to 2.60%/3.30% respectively). Dynamic Physics (Answer-Only): ~22.47% accuracy; CoT/3-shot had small changes (CoT 14.72% ↓ in one table cell but other places show small variations). Notably, Galactica performed best among compared models on some counting/calculation chemistry problems (text claim in paper).",
            "factors_affecting_accuracy": [
                "Scientific-corpus pretraining: authors attribute Galactica's relatively stronger computational chemistry/counting ability to its scientific training data.",
                "Prompting: CoT and 3-shot provide only modest gains for Galactica on chemistry, somewhat better on physics.",
                "Model size and inductive bias toward scientific content."
            ],
            "comparison_baseline": "Compared directly to GPT-series and other LLMs in the benchmark; Galactica outperforms many models with comparable parameter counts on computational chemistry/counting tasks despite less total pretraining data, indicating the benefit of scientific-corpus training.",
            "limitations_or_failure_cases": "Absolute accuracies remain low on dynamic chemistry string/numeric tasks (near-zero percent in AO for some chemistry tasks) and physics multiple-choice still near-chance; unable to robustly analyze experimental results.",
            "author_recommendations_or_insights": "Training on a large scientific corpus benefits scientific calculation tasks; however, more work is needed to raise absolute accuracy on dynamic simulation-like tasks (better numerical precision and domain generalization).",
            "uuid": "e9180.2",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Galactica-6.7B",
            "name_full": "Galactica (6.7B)",
            "brief_description": "A smaller Galactica variant (6.7B parameters) trained on scientific text; evaluated in SciEval and shows similar qualitative behavior to the 30B variant but with lower absolute performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Galactica-6.7B",
            "model_description": "6.7B parameter variant of Galactica family, pretrained on scientific corpora (as described in the paper).",
            "scientific_subdomain": "Dynamic chemistry (molecular numeric/string tasks) and dynamic physics (calculation multiple-choice).",
            "simulation_task": "Same SciEval dynamic tasks: chemistry property numeric/string generation and physics formula-based multiple-choice calculation.",
            "evaluation_metric": "Chemistry: accuracy (%) and MSE/BLEU where applicable; Physics: multiple-choice accuracy (%).",
            "simulation_accuracy": "Dynamic Chemistry (Answer-Only): ~1.55% accuracy (CoT 1.75%, 3-shot 3.05%). Dynamic Physics (Answer-Only): ~20.79% accuracy; CoT/3-shot small/modest changes (CoT 23.37%, 3-shot ~21.12%).",
            "factors_affecting_accuracy": [
                "Smaller parameter count relative to 30B limits absolute capability.",
                "Scientific pretraining helps computational tasks but small model size limits performance.",
                "Prompting mode yields modest gains."
            ],
            "comparison_baseline": "Underperforms Galactica-30B and GPT-4 on average but performs better than many non-scientificly-trained LLMs on certain computation tasks.",
            "limitations_or_failure_cases": "Very low chemistry dynamic accuracy; physics results near-chance; analysis of experimental results remains poor.",
            "author_recommendations_or_insights": "Reinforces the point that domain-focused pretraining helps, but model scale and numeric/reasoning modules remain limiting factors.",
            "uuid": "e9180.3",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Other evaluated LLMs (Claude, Vicuna, LLaMA, etc.)",
            "name_full": "Claude-v1.3, Claude-instant, Vicuna-13B, LLaMa variants, ChatGLM variants, Alpaca, MOSS, ERNIE Bot, SparkDesk (evaluated group)",
            "brief_description": "A set of diverse LLMs (API/web-accessible closed models and open-weight models of various sizes) evaluated on SciEval; collectively show that few models exceed moderate performance on dynamic simulation-like tasks, with generally poor numeric/calculation accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Group: Claude-v1.3 / Claude-instant / Vicuna-13B / LLaMa-7B/13B / ChatGLM2-6B / Alpaca-7B / MOSS-16B / ERNIE Bot / SparkDesk",
            "model_description": "Mixed set of architectures and sizes: Claude (Anthropic, API, undisclosed), Vicuna (13B open-weight fine-tuned), LLaMa (7B/13B open-weight), ChatGLM (6B/extended), Alpaca (7B instruction-tuned), MOSS (16B), ERNIE Bot and SparkDesk (web access only).",
            "scientific_subdomain": "Dynamic chemistry and physics subsets of SciEval; Experimental Data (some web-accessible models evaluated on Experimental Data only).",
            "simulation_task": "Same dynamic tasks: chemistry numeric/string property generation and physics formula multiple-choice problems; experimental-principle and design Q/A in Experimental Data.",
            "evaluation_metric": "Chemistry: accuracy (%), BLEU and MSE for string/numeric outputs; Physics: accuracy (%) for multiple-choice; Experimental: manual scoring.",
            "simulation_accuracy": "Group-level trends: Most non-GPT/Galactica models show very low chemistry dynamic accuracies (often ≪10%), near-random physics accuracies (~25% or slightly above chance), and poor numeric MSE/BLEU; examples: Vicuna-13B chemistry AO ≈ 0.95%, physics AO ≈ 21.24%; LLaMa-7B chemistry AO ≈ 0.50%, physics AO ≈ 18.65%. ERNIE Bot achieved ~61.12% on Static Data (but dynamic and experimental coverage varies).",
            "factors_affecting_accuracy": [
                "Model scale and architecture (smaller open models perform worse).",
                "Lack of dedicated scientific pretraining or domain-specific corpora reduces chemistry/molecule knowledge.",
                "Limited chain-of-thought capability in many models reduces benefit from CoT prompting.",
                "API/web-access limitations constrained evaluation modes for some models (e.g., no CoT available for Claude variants in some evaluations)."
            ],
            "comparison_baseline": "Compared to GPT-series and Galactica, these models generally underperform on dynamic chemistry and physics tasks; a few models do reasonably on static (non-dynamic) tasks but not on simulation-like dynamic tasks.",
            "limitations_or_failure_cases": "Near-random performance on physics dynamic subset for many models; very poor numeric/string chemistry outputs (high MSE, low BLEU/accuracy); limited CoT effectiveness and inability to analyze experimental results robustly.",
            "author_recommendations_or_insights": "Authors emphasize the importance of scientific-corpus training, better numerical reasoning capabilities, and improved CoT ability; dynamic data generation is important to prevent evaluation leakage and better test true generalization.",
            "uuid": "e9180.4",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What indeed can GPT models do in chemistry?",
            "rating": 2,
            "sanitized_title": "what_indeed_can_gpt_models_do_in_chemistry"
        },
        {
            "paper_title": "ChemLLMBench",
            "rating": 2,
            "sanitized_title": "chemllmbench"
        },
        {
            "paper_title": "Neural scaling of deep chemical models",
            "rating": 1,
            "sanitized_title": "neural_scaling_of_deep_chemical_models"
        },
        {
            "paper_title": "Galactica",
            "rating": 1
        }
    ],
    "cost": 0.01588125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</p>
<p>Liangtai Sun 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Yang Han csyanghan@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zihan Zhao 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Da Ma 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zhennan Shen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Baocai Chen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Kai Yu kai.yu@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research
8DE5F8747D39882235665D42A59CA88C
Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research.Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research.However, current benchmarks are mostly based on pre-collected objective questions.This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability.In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues.Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability.In particular, we design a "dynamic" subset based on scientific principles to prevent evaluation from potential data leakage.Both objective and subjective questions are included in SciEval.These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs.Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions.The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as ChatGPT (Schulman et al. 2022), have attracted widespread attention in general scenarios, including information search, code generation, and more.In the field of science, LLMs have also shown preliminary potential in improving scientific research efficiency and transforming scientific research paradigms (Blanco-Gonzalez et al. 2023;WANG and MIAO 2023).In the meanwhile, several scientific LLMs have been proposed by researchers (Taylor et al. 2022;Luo et al. 2022;Frey et al. 2022).In the general field, there are already numerous evaluation benchmarks to evaluate the language understanding, language generation and reasoning capabilities of LLMs, such as MMLU (Hendrycks et al. 2020), AGIEval (Zhong et al. 2023), and C-EVAL (Huang et al. 2023), shown in Table 1.Although these benchmarks cover data of science domain, the data sources are usually confined to educational materials, which can not adequately as-sess the research ability of LLMs and not align with real-life scientific research scenarios.In addition, some benchmarks have been proposed to evaluate the scientific capability of LLMs, such as MultiMedQA (Singhal et al. 2023), Chem-LLMBench (Guo et al. 2023), and MATH (Hendrycks et al. 2021), while these benchmarks are restricted to a specific scientific discipline, leaving a lack of a more general scientific evaluation benchmark. 1In addition, these benchmarks (1) lack evaluation systems for scientific capabilities, (2) are all based on objective questions, which are insufficient to assess scientific abilities, and (3) face the risk of data leakage.</p>
<p>In response to this gap, we present SciEval, an English benchmark designed to evaluate advanced abilities of LLMs in the scientific domain.SciEval consists of a total of about 18000 challenging scientific questions, spanning three important basic science fields: chemistry, physics and biology, each of which is further divided into multiple sub-topics.SciEval mainly has the following three characteristics:</p>
<p>• Multi-level and comprehensive evaluation of the ability of LLMs in the scientific field.Scientific ability of LLMs needs to be evaluated from multiple aspects.Leveraging cognitive domains of Bloom's taxonomy (Krathwohl 2002;Forehand 2010), which covers six levels, SciEval evaluates the scientific capabilities of large language models across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability, where each capability aligns with one or more cognitive levels.• Combination of objective and subjective questions.</p>
<p>SciEval is mainly based on objective questions, which allow for quick and standard model evaluations, involving multiple-choice, fill-in-the-blank, and judgment questions.These questions can help us understand whether the model can correctly understand and memorize scientific knowledge.However, objective questions are insufficient to assess scientific capability holistically.To better assess scientific reasoning and application ability, SciEval introduces a small number of subjective questions, involving a total of twelve basic science experiments, which is named Experimental Data.We conduct experiments to evaluate LLMs on SciEval in answer-only, chain-of-thought and few-shot settings.Results indicate that GPT-4 is the strongest model, with only GPT-4, GPT-3.5-turbo and Claude-v1.3surpassing 60% average accuracy on Static Data, signifying considerable opportunities for improvement.With the results of Dynamic Data, we find that these LLMs have little knowledge about molecules, and most models could only retain near-random accuracy in the physics subset.As for Experimental Data, some top-tier models could perform satisfactorily in experimental principle and design, while almost all models struggle to analyze the experimental results.With the analysis of experiment results, we claim that training on large-scale scientific corpus is helpful for the scientific ability of LLMs, and most LLMs perform bad on calculation problems, especially in physics domain.We hope SciEval can provide an excellent benchmark for the assessment of scientific capability of LLMs, and promote wide application in science.</p>
<p>Related Work</p>
<p>General Benchmarks for LLMs</p>
<p>To evaluate the performance of LLMs across different tasks, several benchmarks have been proposed.MMLU (Hendrycks et al. 2020) aims to develop a comprehensive test for evaluating text models in multi-task contexts.HELM (Liang et al. 2022) offers a comprehensive assessment, evaluating LLMs across various aspects, such as language understanding and common-sense reasoning.Big-Bench (Srivastava et al. 2022) introduces 204 challenging tasks covering various domains, aiming to evaluate tasks beyond the capabilities of existing language models.AGIEval (Zhong et al. 2023) serves as an evaluation framework for assessing the performance of foundation models in human-centric standardized exams.C-Eval (Huang et al. 2023) assesses the advanced knowledge and reasoning capabilities of foundation models in Chinese.</p>
<p>Specific Benchmarks for LLMs</p>
<p>Apart from general tasks, specific benchmarks are designed for certain downstream tasks.MultiMedQA (Singhal et al. 2023) focuses on medical question-answering, evaluating LLMs in terms of clinical knowledge and QA abilities.MATH (Hendrycks et al. 2021) assesses reasoning and problem-solving proficiencies of LLMs in mathematics.Sci-enceQA (Lu et al. 2022) proposes a multi-modal benchmark with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations, collected from elementary and high school science curricula.SCIBENCH (Wang et al. 2023) examines the reasoning capabilities required for complex scientific problem-solving and proposes two datasets of college-level scientific problems.Compared to these benchmarks, SciEval (1) evaluates scientific capabilities from multiple aspects, having a broader coverage, (2) uses data of community Q&amp;A, which is more flexible and diverse, (3) designs a subset of dynamic data, making an effort to mitigate data leakage.</p>
<p>3 The SciEval Dataset cognitive domain is frequently used to structure curriculum learning objectives, assessments and activities, and is broken into six levels: Remember, Understand, Apply, Analyze, Evaluate and Create, as is shown in Figure 1, which are suitable for the evaluation of scientific capability.</p>
<p>Based on the cognitive domain of Bloom's taxonomy, the evaluation system of SciEval consists of four knowledge dimensions: Basic Knowledge (BK), Knowledge Application (KA), Scientific Calculation (SC), and Research Ability (RA).As is shown in Figure 1, BK primarily assesses the fundamental scientific knowledge of LLMs.KA focuses on how to apply basic knowledge to solve scientific problems, requiring models to have comprehension, application, and analysis abilities.SC is a specialized application of knowledge that further examines complex reasoning capabilities of LLMs based on their general knowledge application abilities.RA assesses evaluation capabilities at a higher cognitive level, requiring models to participate in various aspects of scientific research, including problem formulation, experimental design, data analysis, and summarization.</p>
<p>Based on the evaluation system, we design three different types of data: Static Data, Dynamic Data, and Experimental Data.The Static Data covers all these four knowledge dimensions and will remain constant throughout, while the Dynamic Data examines from the aspects of Knowledge Application and Scientific Calculation and will be regularly updated to prevent any data leakage.The Experimental Data comprises a set of questions for twelve scientific experiments and can be used to evaluate the Research Ability.</p>
<p>Data Collection</p>
<p>Static Data The collection steps of Static Data are shown in Figure 2. The primary source of Static Data is Socratic Q&amp;A 2 , a community-driven website that covers a wide range of subjects such as science and literature.Specifically, we collect data from the fields of biology, chemistry, and physics.To ensure quality, we employ rule-based methods 2 https://socratic.org to preprocess the crawled data.While gathering the questions, we found that not all of them are suitable as titles.To address this, we utilize GPT-4 with the "Task 1" prompt, as depicted in Figure 2, to process these questions.Since most of the collected questions are open-ended and challenging to evaluate, we employ GPT-4 to simplify ground-truth answers and generate three wrong answers to formulate them as multiple-choice questions.Additionally, we classify the questions into their respective knowledge domains.And during this process, we manually check the generated content of GPT-4 to ensure data quality.</p>
<p>To make the dataset more diverse and comprehensive, we further integrate data from some publicly available datasets:</p>
<p>• MedQA (Jin et al. 2021) is a free-form multiple-choice OpenQA dataset for solving medical problems, collected from professional medical board exams.We use the test set of USMLE, which is the English subset of MedQA.</p>
<p>• PubMedQA (Jin et al. 2019) is a biomedical questionanswering dataset collected from PubMed abstracts.The task of PubMedQA is to answer research questions with yes/no/maybe using the corresponding abstracts, which is fit for evaluating the literature comprehension ability.We incorporate 1000 expert-annotated data from it and frame them as judgment questions.</p>
<p>• Reagent Selection (Guo et al. 2023) involves the identification and proposal of the most fitting reagents for a specific chemical reaction or process, which is a subset of ChemLLMBench.We randomly select 40% data and formulate them as multiple-choice questions.</p>
<p>Dynamic Data</p>
<p>The current training of LLMs often uses a large amount of data, resulting in a risk of data leakage for evaluation.In order to solve this problem, we design a "dynamic" subset, which can generate data dynamically according to scientific principles.The dynamic subset covers two disciplines, chemistry and physics.For chemistry data, we use the basic information and properties of molecules</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)</p>
<p>Socratic Q&amp;A</p>
<p>Crawl &amp; Filter</p>
<p>Raw Data</p>
<p>GPT-4 Filtered Data</p>
<p>Instruction: Given a question and its ground-truth answer, judge whether it is suitable to be used as the title of a multiple-choice question.Your answer should be "YES" or "NO".And please directly give the results without any explanation.</p>
<p>Task 1</p>
<p>Instruction: Given a question and a ground-truth answer, please simplify the answer as concise as possible.And I want to generate a 4-choice question using it, please generate 3 fake answers for me.Note that the length of the simplified answer and these 3 fake answers should be about the same and these 3 fake answers should be as confusing as possible.Furthermore, please help me to classify the domain of the question.There are three domains in total: Base Knowledge, Scientific Calculation, Knowledge Application.For physics data, we manually write some Python scripts according to the physics formulas.When obtaining the evaluation dataset, we will provide a regenerated version to users and we will update it regularly, while at the same time, we will maintain a stable version of the dynamic data to make a fair comparison.</p>
<p>Experimental Data To better evaluate the scientific thoughts and abilities of LLMs, SciEval introduces a subset of experimental data, involving 12 different basic scientific experiments.These experiments are collected from basic science experiment courses at university, and each experiment conducts a comprehensive investigation of the ability of LLMs in scientific research and experimentation from the perspectives of experimental principle, process, and analysis and summarization of experimental results.</p>
<p>Data Statistics</p>
<p>Summarized statistics are shown in For Static Data, we further split the data into dev, valid, and test set.For each data source, each knowledge domain, and each discipline, we randomly select 5 data to form the 3 https://pubchem.ncbi.nlm.nih.gov/dev set, which can be used for few-shot learning, and we split the remaining data with a ratio of 1:9 to construct the valid set and test set respectively.</p>
<p>Experiment</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".Please directly give the answer without any explanation.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".How many atoms are in 3.5 moles of arsenic atoms?</p>
<p>Experiment Setup</p>
<p>Prompts We evaluate LLMs in both Answer-Only (AO) and Chain-Of-Thought (CoT) (Kojima et al. 2022) settings.The prompts we used are shown in Figures 3 and 4 respectively.Furthermore, we also evaluate using 3-shot setting, where the three exemplars are selected from the dev set.Models In order to comprehensively assess the scientific capabilities of Large Language Models (LLMs), we evaluate 15 high-performing LLMs that are widely accessible.These models are selected to represent a diverse range of organizations and vary in size.The details of these models are summarized in Table 3.</p>
<p>Model</p>
<p>• GPT-3.5-turbo and GPT-4 (Schulman et al. 2022;Ope-nAI 2023) are the strongest GPT model variants from OpenAI that have undergone pretraining, instruction tuning, and reinforcement learning from human feedback (RLHF, (Ouyang et al. 2022)).• Claude 4 , developed by Anthropic, is often considered 4 https://www.anthropic.com/index/introducing-claude.comparable to GPT-3.5-turbo.We evaluate both the Claude-v1.We evaluate GPT-3.5-turbo,GPT4 and Claude on all three subsets, including Static Data, Dynamic Data, and Experimental Data.Since we can only assess ERNIE Bot and SparkDesk through web interface, we evaluate these two models on the Experimental Data.And for the rest LLMs with billions or tens of billions of parameters, since the length of the Experimental Data exceeds the length limit of these models7 , we evaluate them on Static Data and Dynamic Data, as is shown in Table 3.</p>
<p>Evaluation Metrics In the case of Static Data, all questions are objective, making accuracy the appropriate evaluation metric.For Dynamic Data, the physics questions are presented as multiple-choice questions, which can also be evaluated using accuracy.Conversely, the chemistry questions involve complex components, such as "What is the</p>
<p>Experiment Results</p>
<p>Answer-Only Setting Answer-only results of all the models on the test set are shown in Table 4 and detailed results of Static Data across different knowledge domains are provided in Appendix B. Analyzing the results of Static Data, GPT-4 demonstrates significantly superior performance compared to other models.And only GPT-4, GPT-3.5-turbo, and Claude-v1.3achieve an average accuracy exceeding 60%, which highlights the challenge posed by SciEval.</p>
<p>For the results of Dynamic Data, GPT-4 performs the best in terms of average accuracy and BLEU score.However, for counting and calculation questions, Galactica-30B yields the best results, indicating its strong aptitude in the field of science.Conversely, models with billions or tens of billions of parameters perform poorly on the chemistry subset, suggesting their limited knowledge about molecules.Regarding the performance of models on the physics subset, since all questions are 4-choices questions, the accuracy should be at least 25%.However, none of these models achieve satisfactory results in this subset.</p>
<p>As for Experimental Data, GPT-series models and Claude-series models achieve good results, while the other two models are not.The detailed scores models reached in each experiment are shown in Appendix C.However, although some models could get a great performance, during experiments, we find that these models are good at experimental principles and designing, while when it comes to analyzing the experiment results, the performances are not satisfying.</p>
<p>CoT Setting and 3-Shot setting Comparison of experiment results among Answer-Only, Chain-of-Thought and 3-Shot settings are shown in Figure 5 and Table 5. 9 And we refer detailed results to Appendix A and B.</p>
<p>The experimental results from Static Data reveal that solely the GPT-series LLMs get performance enhancement within the CoT setting due to the limited CoT capabilities of other LLMs.As for the 3-Shot setting, roughly half of the LLMs analyzed demonstrate superior performances relative to the Answer-Only setting.The performances of the remaining LLMs are closely similar to those observed within the Answer-Only setting.</p>
<p>From the experimental results of Dynamic Data, it is observed that both CoT and 3-Shot significantly enhance the performance of most Language Learning Models (LLMs) in the chemistry subset.However, the performances achieved are still not up to the mark.In the physics subset, the impact of CoT and 3-Shot on most LLMs is less pronounced, resulting in nearly random performances.Under the CoT setting, GPT-3.5-turboachieves an accuracy of 47.19, suggesting a robust understanding of physical principles.Conversely, the performance of GPT-4 is markedly poor, from which we find that despite its extensive knowledge of physical principles, it frequently employs incorrect formulas to solve problems.Nevertheless, GPT-4 attains an accuracy of 51.01 under 3-Shot setting, the highest among all models, demonstrating its ability to learn from a mere three examples.</p>
<p>Discussion</p>
<p>Training on large-scale scientific corpus is helpful.Based on experimental results (Table 4), Galactica (Taylor et al. 2022), which has been trained on an extensive scientific corpus, significantly outperforms other LLMs with a comparable number of parameters, although Galactica is trained with a much smaller amount of data.Remarkably, when tested on Dynamic Data, Galactica surpasses the GPTseries and Claude-series LLMs in computational problems.</p>
<p>Most LLMs perform bad on calculation problems, especially in physics domain.Detailed results across various knowledge domains on Static Data (refer to Appendix B) reveal that most LLMs underperform in the Scientific Calculation domain, while demonstrate relatively superior performance in other domains, which is particularly acute in the field of physics.Similar issues are also observed in Dynamic Data and Experimental Data.In the context of Dynamic Data, the mean square error, employed to evaluate calculation abilities within the chemistry subset, is exceedingly high for most LLMs, and almost all LLMs can only achieve nearly random performance within the physics subset.Regarding Experimental Data, our findings indicate that these LLMs struggle with the analysis of experimental results.</p>
<p>Conclusion</p>
<p>In this paper, we introduce SciEval, a benchmark designed to evaluate scientific capabilities of LLMs.SciEval comprises about 18,000 challenging scientific questions, covering three fundamental fields of science.SciEval assesses the scientific ability of LLMs across four dimensions.It incorporates both objective and subjective questions, and employs dynamic data generation to mitigate potential data leakage.We conduct comprehensive experiments on various advanced LLMs using SciEval and perform thorough analyses.Our experimental results reveal that most LLMs do not perform well on our benchmark, with the exception of the GPT-series and Claude-series LLMs.We hope that SciEval can serve as a robust benchmark for assessing scientific capabilities of LLMs.</p>
<p>Figure 1 :
1
Figure 1: The illustration of the evaluation system.SciEval covers three disciplines with amounts of sub-topics, and investigates four abilities, corresponding to six cognitive levels.</p>
<p>Figure 2 :
2
Figure 2: Data Collection steps of Static Data</p>
<p>Figure 3 :
3
Figure 3: An example of the prompt we used for AO setting.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>AFigure 4 :
4
Figure 4: An example of the prompt we used for CoT setting.The red text is the response from the model, while the blue text and black text are the inputted prompt.</p>
<p>Figure 5: Accuracy on Answer Only, Chain-of-Thought and 3-Shot settings of each LLMs for Static Data.</p>
<p>Table 1 :
1
Dataset comparison of SciEval and some other datasets covering science domain."BK"stands for Basic Knowledge, "KA" stands for Knowledge Application, "SC" stands for Scientific Calculation, and "RA" stands for Research Ability.
NameCategoryAbilitySourceData Type Dynamic #DataMMLUhumanities, social science, STEM, otherBK, KA, SCexam, book, course objective14079AGIEvalsocial science, STEM BK, KA, SCexamobjective8062C-EVALhumanities, social science, STEM, otherBK, KA, SCexamobjective12342MultiMedQAmedicalBK, KA, RAexam, researchobjective13115ChemLLMBench chemistryBK,KAknowledge baseobjective800MATHmathematicsSCexamobjective5000SciEvalscienceBK, KA,SC, RAcommunity QA, knowledge baseobjective + subjective15901model perfor-mance. And the objective questions other than DynamicData are referred to as Static Data.
• Dynamic data generation based on basic scientific principles.The huge amount of training data used for pre-training LLMs may cause the risk of data leakage for evaluation.In order to solve this problem, one of the main features of SciEval is the use of Dynamic Data, which can prevent potential data leakage and ensure the fairness and credibility of the evaluation results.The Dynamic Data will be updated regularly, and we will maintain a stable version to make a fair comparison of</p>
<p>Table 2
2, where we onlycount Static Data. For Dynamic Data, the chemistry part ex-amines the KA ability and contains 2000 data, while thephysics part evaluates the SC ability and involves 890 data.All these questions are in English and we show some dataexamples in Appendix D.AbilityBioChem PhyBasic Knowledge2147 2914456Knowledge Application 1379 372036Scientific Calculation3013401 1165Research Ability100000Total4830 10035 1657</p>
<p>Table 2 :
2
Statistics of Static Data</p>
<p>Table 3 :
3
Models evaluated in this paper.The "access" columns show whether we have full access to the model weights or we can only access through API or web.SD stands for Static Data, DD stands for Dynamic Data, and ED stands for Experimental Data.Marking " " means we evaluate the corresponding model on this subset.
Creator#Parameters Access SD DD EDGPT-4OpenAIundisclosedAPIGPT-3.5-turboOpenAIundisclosedAPIClaude-v1.3AnthropicundisclosedAPIClaude-instant-v1.1 AnthropicundisclosedAPIERNIE BotBaiduundisclosedWebSparkDeskiFLYTEKundisclosedWebVicunaLMSYS13BWeightsGalacticaMeta30B, 6.7BWeightsChatGLM2Tsinghua6BWeightsChatGLMTsinghua6BWeightsAlpacaStanford7BWeightsMOSSFudan16BWeightsLLaMaMeta7B, 13BWeightsModelStatic Data Biology Chemistry Physics Avg.Chemistry(DD) Acc. BLEU MSEPhysics(DD) Exp Acc. ScoreGPT-484.4969.3865.2273.93 11.05 23.78891.0925.8493.31GPT-3.5-turbo76.4264.3052.3066.97 7.6518.862008.7221.8088.27Claude-v1.372.5859.7254.9463.45 5.7521.981489.8726.1485.73Claude-instant-v1.170.4353.3652.3058.92 0.4516.078258.4621.4687.50Galactica-30B66.4850.1644.6554.960.94.14485.9922.47-Vicuna-13B58.3953.0645.1353.93 0.956.50766.6421.24-Galactica-6.7B57.8450.7730.9950.87 1.556.475519.8220.79-ChatGLM2-6B58.6244.0040.2648.440.21.863449.4424.83-ChatGLM-6B52.5445.3640.8047.23 0.752.4410303.9021.01-Alpaca-7B56.6642.4337.0146.540.22.92428419.2726.74-MOSS-16B47.7133.8731.7338.230.17.3730505.1724.27-LLaMa-13B48.5933.5619.4836.960.35.213707.017.08-LLaMa-7B36.2426.3815.0228.370.51.2611305.6514.38-ERNIE Bot--------61.12SparkDesk--------33.69</p>
<p>Table 4 :
4
Model performances of Answer-Only setting.The leaderboard is sorted by the average accuracy of Static Data.</p>
<p>Table 5 :
5
Results on Answer-Only, Chain-of-Thought and 3-Shot settings of each LLM for Dynamic Data.↑ means the performance is slightly better than that under Answer-Only setting, ↓ means worse, and ∼ means the performance is nearly the same.
ModelAOChemistry CoT3-ShotAOPhysics CoT3-ShotGPT-411.05 11.65 ↑ 12.42↑ 25.84 17.98 ↓ 51.01 ↑GPT-3.5-turbo7.65 10.20 ↑ 8.85 ↑ 21.80 47.19 ↑ 25.39 ∼Galactica-6.7B1.551.75 ↑3.05 ↑ 20.79 23.37 ∼ 21.12 ∼Vicuna-13B0.951.95 ↑1.80 ↑ 21.24 18.65 ∼ 23.37∼Galactica-30B0.902.60 ↑3.30 ↑ 22.47 14.72 ↓ 22.58 ∼ChatGLM-6B0.750.80 ↑1.15 ↑ 21.01 25.39 ∼ 23.37 ∼LLaMa-7B0.500.10 ↓1.55 ↑ 18.659.66 ↓27.53 ↑LLaMa-13B0.300.25 ∼ 2.11 ↑7.085.84 ∼22.70 ↑ChatGLM2-6B 0.202.65 ↑1.60 ↑ 24.83 25.39 ∼ 26.74 ∼Alpaca-7B0.200.65 ↑2.10 ↑ 26.71 28.43 ∼ 25.62 ∼MOSS-16B0.100.85 ↑0.65 ↑ 24.27 25.06 ∼ 26.40 ∼
Due to the page limitation, we only compare some widely used benchmarks. For more information, we refer to(Chang et al.<br />
).The Thirty-Eighth AAAI Conference on Artificial Intelligence 
Scientific research requires different dimensions of knowledge, such as understanding and calculation, thence evaluation of scientific ability should be conducted at multiple levels. Bloom's taxonomy is a set of three hierarchical methods used for classification of educational learning objectives covering cognitive, affective and psychomotor domains. TheThe Thirty-Eighth AAAI Conference on Artificial Intelligence 
The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
https://yiyan.baidu.com/
https://xinghuo.xfyun.cn/ The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
The maximum context length of ChatGLM2 is extended to 32k, while it has limited ability to understand long texts. molecular weight of A?" and "What is the SMILES expression of B?". Hence, for questions with numerical answers, we employ
MSE 8 as the evaluation metric, while for questions with string answers, we utilize the BELU score(Papineni et al. 2002). Additionally, we also calculate the extract match scores. As for Experimental Data, each experiment consists of multiple open-ended questions. As a result, we assess the model-generated responses manually.
If the predictions do not contain any number, we will regard the MSE as 1 × 10 10The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
When evaluating on CoT and 3-Shot settings, Claude-Instant and Claude are not available for us, due to the limitation of API.
AcknowledgementsThis work is funded by the China NSFC Projects (92370206, U23B2057, 62106142 and 62120106006) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).
The role of ai in drug discovery: challenges, opportunities, and strategies. A Blanco-Gonzalez, A Cabezon, A Seco-Gonzalez, D Conde-Torres, P Antelo-Riveiro, A Pineiro, R Garcia-Fandino, Pharmaceuticals. 1668912023</p>
<p>Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.03109A survey on evaluation of large language models. 2023arXiv preprint</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>M Forehand, Blooms taxonomy. Emerging perspectives on learning, teaching, and technology. 201041</p>
<p>N Frey, R Soklaski, S Axelrod, S Samsi, R Gomez-Bombarelli, C Coley, V Gadepally, Neural scaling of deep chemical models. 2022</p>
<p>What indeed can GPT models do in chemistry?. T Guo, K Guo, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, arXiv:2305.18365A comprehensive benchmark on eight tasks. 2023arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, J Lei, arXiv:2305.083222023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>A revision of Bloom's taxonomy: An overview. Theory into practice. D R Krathwohl, 200241</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2303.08774Advances in Neural Information Processing Systems. 202235Technical Report</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>ChatGPT: Optimizing language models for dialogue. J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, J F C Uribe, L Fedus, L Metz, M Pokorny, Nature. 2022. 2023Large language models encode clinical knowledge</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>MOSS: Training Conversational Language Models from Synthetic Data. T Sun, X Zhang, Z He, P Li, Q Cheng, H Yan, X Liu, Y Shao, Q Tang, X Zhao, K Chen, Y Zheng, Z Zhou, R Li, J Zhan, Y Zhou, L Li, X Yang, L Wu, Z Yin, X Huang, X Qiu, R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, N Hambro, E Azhar, F , arXiv:2302.13971Novel Paradigm for AIdriven Scientific Research: From AI4S to Intelligent Science. G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, Goyal, 2023. 2023. 2022. 2023. 202338arXiv preprintLlama: Open and efficient foundation language models</p>
<p>X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.10635SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. 2023arXiv preprint</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, arXiv:2306.05685Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023arXiv preprint</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, arXiv:2304.063642023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
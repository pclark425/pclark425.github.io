<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-254 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-254</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-254</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-266898122</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.13099v1.pdf" target="_blank">Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks</a></p>
                <p><strong>Paper Abstract:</strong> The rapid progress in the field of natural language processing (NLP) systems and the expansion of large language models (LLMs) have opened up numerous opportunities in the field of education and instructional methods. These advancements offer the potential for tailored learning experiences and immediate feedback, all delivered through accessible and cost-effective services. One notable application area for this technological advancement is in the realm of solving mathematical problems. Mathematical problem-solving not only requires the ability to decipher complex problem statements but also the skill to perform precise arithmetic calculations at each step of the problem-solving process. However, the evaluation of the arithmetic capabilities of large language models remains an area that has received relatively little attention. In response, we introduce an extensive mathematics dataset called"MathQuest"sourced from the 11th and 12th standard Mathematics NCERT textbooks. This dataset encompasses mathematical challenges of varying complexity and covers a wide range of mathematical concepts. Utilizing this dataset, we conduct fine-tuning experiments with three prominent LLMs: LLaMA-2, WizardMath, and MAmmoTH. These fine-tuned models serve as benchmarks for evaluating their performance on our dataset. Our experiments reveal that among the three models, MAmmoTH-13B emerges as the most proficient, achieving the highest level of competence in solving the presented mathematical problems. Consequently, MAmmoTH-13B establishes itself as a robust and dependable benchmark for addressing NCERT mathematics problems.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e254.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e254.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter open-source LLM that the authors evaluated and fine-tuned on an augmented arithmetic dataset (Math-401) to improve elementary and multi-step arithmetic and algebraic problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, expressions with parentheses, multi-step arithmetic within algebraic problems; also evaluation tasks include trigonometric and logarithmic functions in dataset</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>varied; dataset includes small integers [-20,20], large integers [-1000,1000], small and large decimals (up to 4 decimal places), irrational constants (π, e), expressions with up to 3–4 terms and parentheses</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on an augmented Math-401 dataset (241,600 train samples) using QLoRA (4-bit quantization) + LoRA adapters, AdamW optimizer, 10 epochs, learning rate 3e-4; instruction-style training/examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Exact-match accuracy measured; models improved after fine-tuning on Math-401. Specific numeric breakdowns for LLaMA-2-7B are not explicitly reported in the text, but the paper states 13B variants outperform 7B variants and overall accuracies were low on difficult sets like SimulEq and Math-401 before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Paper reports higher accuracy for 13B variants relative to 7B variants (i.e., performance improves with model size), but no fine-grained scaling law presented.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggled on complex expressions with nested brackets and on problems requiring retaining intermediate variable values across multiple equations; initial models also struggled with even simple addition/subtraction prior to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared before-vs-after fine-tuning and 7B vs 13B model sizes across multiple datasets (GSM-8K, DeepMind, NumGLUE, SimulEq, Math-401, MathQuest).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Fine-tuning LLaMA-2-7B on a large, augmented arithmetic dataset improves performance, but 7B scale remains substantially weaker than 13B on challenging arithmetic/math problems and still fails on nested/multi-step expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e254.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e254.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion-parameter LLaMA-2 variant evaluated and fine-tuned on an augmented arithmetic dataset to assess improvements on multi-step and algebraic arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, bracketed expressions, multi-step arithmetic; dataset also includes trig/log functions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>varied; includes integers up to ±1000, decimals (1–4 dp), irrational constants, expressions with up to 3–4 terms and parentheses</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on augmented Math-401 using QLoRA + LoRA, AdamW, 10 epochs, LR=3e-4; instruction-style examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Paper reports that 13B variants outperform 7B; exact numeric accuracies for LLaMA-2-13B are not explicitly quoted in the main text, but overall accuracies improved after fine-tuning and remained modest on the hardest datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Accuracy improves with model size (13B > 7B) according to reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Persistent difficulties with nested brackets, retaining variable values across steps, and complex multi-step numeric reasoning despite fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Before vs after fine-tuning; comparisons across model sizes (7B vs 13B) and across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Scaling to 13B plus fine-tuning on a dense arithmetic dataset produces measurable gains, but 13B models still have limited exact-match accuracy on challenging, school-level math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e254.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e254.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardMath-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardMath (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B model variant (derived from instructively fine-tuned LLaMA-2 using RLEIF) that the authors fine-tuned further on Math-401 to evaluate arithmetic and math problem-solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardMath</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>basic arithmetic (+, −, *, /), bracketed expressions, multi-step arithmetic appearing in algebraic and word problems</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>varied ranges including small and large integers and decimals (see dataset ranges like [-20,20] and [-1000,1000]); expressions include up to 3–4 terms and use parentheses</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on augmented Math-401 with QLoRA + LoRA (4-bit), AdamW, 10 epochs, LR=3e-4; built on prior WizardMath instruction tuning methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Accuracy improved after fine-tuning; paper does not report specific numeric accuracy for WizardMath-7B in the main text beyond aggregate statements (13B > 7B and MAmmoTH-13B best at 24.0% on MathQuest).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Observed pattern: larger variants (13B) outperform 7B variants.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Errors in complex/multi-step problems and nested expressions; difficulty retaining variable assignments across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared before vs after fine-tuning and across model sizes and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Instruction-tuned models like WizardMath gain from further fine-tuning on explicit arithmetic examples but still show limited exact-match performance on difficult school-level math without additional algorithmic/tool support.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e254.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e254.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardMath-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardMath (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B variant of WizardMath evaluated and fine-tuned on an augmented arithmetic dataset to test improvements in mathematical problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardMath</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, expressions with parentheses, multi-step arithmetic, plus some higher-level functions present in datasets (trig, log).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>diverse: integers (up to ±1000), decimals (1–4 dp), irrational constants, multi-term expressions (up to 4 terms) and bracketed expressions</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on augmented Math-401 dataset using QLoRA + LoRA (4-bit), AdamW, 10 epochs, LR=3e-4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>13B WizardMath shows better accuracy than its 7B counterpart after fine-tuning; exact numbers not explicitly listed in the prose (paper highlights MAmmoTH-13B as highest at 24.0% on MathQuest).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>13B variant outperforms 7B variant as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles with nested brackets, maintaining intermediate variables across multiple equations, and complex multi-step arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Before vs after fine-tuning; 7B vs 13B; cross-dataset comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Reinforcement-evol-instruct tuned models (WizardMath) see improvements with targeted fine-tuning but still underperform on the most complex arithmetic/math problems compared to the best tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e254.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e254.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAmmoTH-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAmmoTH (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B math-specialist LLM (instruction-tuned on the MathInstruct dataset containing CoT and PoT rationales) that the authors fine-tuned on augmented arithmetic data and evaluated across multiple math datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAmmoTH</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, bracketed expressions, multi-step arithmetic; dataset also includes exponentiation, trigonometric and logarithmic operations</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>varied: small and large integers ([-20,20] up to [-1000,1000]), decimals (1–4 dp), irrational constants, multi-term expressions (up to 4 terms), nested parentheses</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on augmented Math-401 using QLoRA + LoRA (4-bit), AdamW, 10 epochs, LR=3e-4; base MAmmoTH pre-trained with MathInstruct which includes chain-of-thought and program-of-thought style rationales</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>MAmmoTH-7B improved after fine-tuning; produced outputs with precision up to two decimal places. Exact numeric accuracies not fully enumerated in text; 7B < 13B performance.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>7B variant performs worse than 13B variant; fine-tuning improves performance but not enough for high exact-match accuracy on hard datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Difficulty with nested brackets and tasks requiring retention of intermediate variable assignments; partial enhancement of reasoning only.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared before and after fine-tuning; compared to other model families and sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Math-specialist instruction-tuned models like MAmmoTH-7B benefit from additional arithmetic fine-tuning, showing improved numerical precision (e.g., two decimal places) but still limited exact-match success on complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e254.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e254.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAmmoTH-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAmmoTH (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion-parameter math-specialist LLM (instruction-tuned on MathInstruct with CoT and PoT rationales) that achieved the best performance among models evaluated after fine-tuning on the augmented arithmetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAmmoTH</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, bracketed and multi-term expressions, multi-step arithmetic in algebraic and word problems; dataset additionally contains trig/log functions and irrational constants</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>wide-ranging: integers from small ([-20,20]) to large ([-1000,1000]), decimals (1–4 dp), irrational constants (π, e), expressions with up to 3–4 terms and parentheses, multi-equation problems</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on augmented Math-401 with QLoRA + LoRA (4-bit quantization), AdamW optimizer, 10 epochs, LR=3e-4; benefits from prior MathInstruct tuning (CoT and PoT rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Best reported result in paper: MAmmoTH-13B attained 24.0% exact-match accuracy on the MathQuest dataset after fine-tuning on Math-401; paper reports that all models' accuracies improved post fine-tuning and that 13B variants performed better than 7B variants. Models produced outputs with up to two decimal places in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Emergent-like improvement at 13B relative to 7B is reported (13B > 7B); fine-tuning on a large augmented arithmetic dataset yields substantial gains compared to no fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still low absolute accuracy on challenging problems; fails on nested-bracket expressions, problems needing retention of variable values across steps, and other complex multi-step reasoning tasks; overall exact-match accuracy remains modest (e.g., 24% on MathQuest).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared before vs after fine-tuning, across model sizes (7B vs 13B), and across datasets (GSM-8K, DeepMind, NumGLUE, SimulEq, Math-401, MathQuest).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Fine-tuning a math-specialist 13B model on a large, augmented arithmetic dataset improves exact-match performance substantially relative to baseline models, but absolute performance on complex high-school math remains low (best reported: 24.0% on MathQuest), and mechanistic reasons for arithmetic errors are not elucidated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating transformer language models on arithmetic operations using number decomposition <em>(Rating: 2)</em></li>
                <li>How well do large language models perform in arithmetic tasks? <em>(Rating: 2)</em></li>
                <li>MathPrompter: Mathematical reasoning using large language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Pal: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Exploring generalization ability of pretrained language models on arithmetic and logical reasoning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-254",
    "paper_id": "paper-266898122",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "LLaMA-2-7B",
            "name_full": "LLaMA-2 (7B)",
            "brief_description": "A 7-billion-parameter open-source LLM that the authors evaluated and fine-tuned on an augmented arithmetic dataset (Math-401) to improve elementary and multi-step arithmetic and algebraic problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, expressions with parentheses, multi-step arithmetic within algebraic problems; also evaluation tasks include trigonometric and logarithmic functions in dataset",
            "number_range_or_complexity": "varied; dataset includes small integers [-20,20], large integers [-1000,1000], small and large decimals (up to 4 decimal places), irrational constants (π, e), expressions with up to 3–4 terms and parentheses",
            "method_or_intervention": "fine-tuning on an augmented Math-401 dataset (241,600 train samples) using QLoRA (4-bit quantization) + LoRA adapters, AdamW optimizer, 10 epochs, learning rate 3e-4; instruction-style training/examples",
            "performance_result": "Exact-match accuracy measured; models improved after fine-tuning on Math-401. Specific numeric breakdowns for LLaMA-2-7B are not explicitly reported in the text, but the paper states 13B variants outperform 7B variants and overall accuracies were low on difficult sets like SimulEq and Math-401 before fine-tuning.",
            "mechanistic_insight": null,
            "performance_scaling": "Paper reports higher accuracy for 13B variants relative to 7B variants (i.e., performance improves with model size), but no fine-grained scaling law presented.",
            "failure_modes": "Struggled on complex expressions with nested brackets and on problems requiring retaining intermediate variable values across multiple equations; initial models also struggled with even simple addition/subtraction prior to fine-tuning.",
            "comparison_baseline": "Compared before-vs-after fine-tuning and 7B vs 13B model sizes across multiple datasets (GSM-8K, DeepMind, NumGLUE, SimulEq, Math-401, MathQuest).",
            "key_finding": "Fine-tuning LLaMA-2-7B on a large, augmented arithmetic dataset improves performance, but 7B scale remains substantially weaker than 13B on challenging arithmetic/math problems and still fails on nested/multi-step expressions.",
            "uuid": "e254.0",
            "source_info": {
                "paper_title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaMA-2-13B",
            "name_full": "LLaMA-2 (13B)",
            "brief_description": "A 13-billion-parameter LLaMA-2 variant evaluated and fine-tuned on an augmented arithmetic dataset to assess improvements on multi-step and algebraic arithmetic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_size": "13B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, bracketed expressions, multi-step arithmetic; dataset also includes trig/log functions",
            "number_range_or_complexity": "varied; includes integers up to ±1000, decimals (1–4 dp), irrational constants, expressions with up to 3–4 terms and parentheses",
            "method_or_intervention": "fine-tuning on augmented Math-401 using QLoRA + LoRA, AdamW, 10 epochs, LR=3e-4; instruction-style examples",
            "performance_result": "Paper reports that 13B variants outperform 7B; exact numeric accuracies for LLaMA-2-13B are not explicitly quoted in the main text, but overall accuracies improved after fine-tuning and remained modest on the hardest datasets.",
            "mechanistic_insight": null,
            "performance_scaling": "Accuracy improves with model size (13B &gt; 7B) according to reported comparisons.",
            "failure_modes": "Persistent difficulties with nested brackets, retaining variable values across steps, and complex multi-step numeric reasoning despite fine-tuning.",
            "comparison_baseline": "Before vs after fine-tuning; comparisons across model sizes (7B vs 13B) and across datasets.",
            "key_finding": "Scaling to 13B plus fine-tuning on a dense arithmetic dataset produces measurable gains, but 13B models still have limited exact-match accuracy on challenging, school-level math problems.",
            "uuid": "e254.1",
            "source_info": {
                "paper_title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "WizardMath-7B",
            "name_full": "WizardMath (7B)",
            "brief_description": "A 7B model variant (derived from instructively fine-tuned LLaMA-2 using RLEIF) that the authors fine-tuned further on Math-401 to evaluate arithmetic and math problem-solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "WizardMath",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "basic arithmetic (+, −, *, /), bracketed expressions, multi-step arithmetic appearing in algebraic and word problems",
            "number_range_or_complexity": "varied ranges including small and large integers and decimals (see dataset ranges like [-20,20] and [-1000,1000]); expressions include up to 3–4 terms and use parentheses",
            "method_or_intervention": "fine-tuning on augmented Math-401 with QLoRA + LoRA (4-bit), AdamW, 10 epochs, LR=3e-4; built on prior WizardMath instruction tuning methods",
            "performance_result": "Accuracy improved after fine-tuning; paper does not report specific numeric accuracy for WizardMath-7B in the main text beyond aggregate statements (13B &gt; 7B and MAmmoTH-13B best at 24.0% on MathQuest).",
            "mechanistic_insight": null,
            "performance_scaling": "Observed pattern: larger variants (13B) outperform 7B variants.",
            "failure_modes": "Errors in complex/multi-step problems and nested expressions; difficulty retaining variable assignments across steps.",
            "comparison_baseline": "Compared before vs after fine-tuning and across model sizes and datasets.",
            "key_finding": "Instruction-tuned models like WizardMath gain from further fine-tuning on explicit arithmetic examples but still show limited exact-match performance on difficult school-level math without additional algorithmic/tool support.",
            "uuid": "e254.2",
            "source_info": {
                "paper_title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "WizardMath-13B",
            "name_full": "WizardMath (13B)",
            "brief_description": "A 13B variant of WizardMath evaluated and fine-tuned on an augmented arithmetic dataset to test improvements in mathematical problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "WizardMath",
            "model_size": "13B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, expressions with parentheses, multi-step arithmetic, plus some higher-level functions present in datasets (trig, log).",
            "number_range_or_complexity": "diverse: integers (up to ±1000), decimals (1–4 dp), irrational constants, multi-term expressions (up to 4 terms) and bracketed expressions",
            "method_or_intervention": "fine-tuning on augmented Math-401 dataset using QLoRA + LoRA (4-bit), AdamW, 10 epochs, LR=3e-4",
            "performance_result": "13B WizardMath shows better accuracy than its 7B counterpart after fine-tuning; exact numbers not explicitly listed in the prose (paper highlights MAmmoTH-13B as highest at 24.0% on MathQuest).",
            "mechanistic_insight": null,
            "performance_scaling": "13B variant outperforms 7B variant as reported.",
            "failure_modes": "Struggles with nested brackets, maintaining intermediate variables across multiple equations, and complex multi-step arithmetic.",
            "comparison_baseline": "Before vs after fine-tuning; 7B vs 13B; cross-dataset comparisons.",
            "key_finding": "Reinforcement-evol-instruct tuned models (WizardMath) see improvements with targeted fine-tuning but still underperform on the most complex arithmetic/math problems compared to the best tuned model.",
            "uuid": "e254.3",
            "source_info": {
                "paper_title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MAmmoTH-7B",
            "name_full": "MAmmoTH (7B)",
            "brief_description": "A 7B math-specialist LLM (instruction-tuned on the MathInstruct dataset containing CoT and PoT rationales) that the authors fine-tuned on augmented arithmetic data and evaluated across multiple math datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MAmmoTH",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, bracketed expressions, multi-step arithmetic; dataset also includes exponentiation, trigonometric and logarithmic operations",
            "number_range_or_complexity": "varied: small and large integers ([-20,20] up to [-1000,1000]), decimals (1–4 dp), irrational constants, multi-term expressions (up to 4 terms), nested parentheses",
            "method_or_intervention": "fine-tuning on augmented Math-401 using QLoRA + LoRA (4-bit), AdamW, 10 epochs, LR=3e-4; base MAmmoTH pre-trained with MathInstruct which includes chain-of-thought and program-of-thought style rationales",
            "performance_result": "MAmmoTH-7B improved after fine-tuning; produced outputs with precision up to two decimal places. Exact numeric accuracies not fully enumerated in text; 7B &lt; 13B performance.",
            "mechanistic_insight": null,
            "performance_scaling": "7B variant performs worse than 13B variant; fine-tuning improves performance but not enough for high exact-match accuracy on hard datasets.",
            "failure_modes": "Difficulty with nested brackets and tasks requiring retention of intermediate variable assignments; partial enhancement of reasoning only.",
            "comparison_baseline": "Compared before and after fine-tuning; compared to other model families and sizes.",
            "key_finding": "Math-specialist instruction-tuned models like MAmmoTH-7B benefit from additional arithmetic fine-tuning, showing improved numerical precision (e.g., two decimal places) but still limited exact-match success on complex problems.",
            "uuid": "e254.4",
            "source_info": {
                "paper_title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MAmmoTH-13B",
            "name_full": "MAmmoTH (13B)",
            "brief_description": "A 13-billion-parameter math-specialist LLM (instruction-tuned on MathInstruct with CoT and PoT rationales) that achieved the best performance among models evaluated after fine-tuning on the augmented arithmetic dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MAmmoTH",
            "model_size": "13B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, bracketed and multi-term expressions, multi-step arithmetic in algebraic and word problems; dataset additionally contains trig/log functions and irrational constants",
            "number_range_or_complexity": "wide-ranging: integers from small ([-20,20]) to large ([-1000,1000]), decimals (1–4 dp), irrational constants (π, e), expressions with up to 3–4 terms and parentheses, multi-equation problems",
            "method_or_intervention": "fine-tuning on augmented Math-401 with QLoRA + LoRA (4-bit quantization), AdamW optimizer, 10 epochs, LR=3e-4; benefits from prior MathInstruct tuning (CoT and PoT rationales)",
            "performance_result": "Best reported result in paper: MAmmoTH-13B attained 24.0% exact-match accuracy on the MathQuest dataset after fine-tuning on Math-401; paper reports that all models' accuracies improved post fine-tuning and that 13B variants performed better than 7B variants. Models produced outputs with up to two decimal places in some cases.",
            "mechanistic_insight": null,
            "performance_scaling": "Emergent-like improvement at 13B relative to 7B is reported (13B &gt; 7B); fine-tuning on a large augmented arithmetic dataset yields substantial gains compared to no fine-tuning.",
            "failure_modes": "Still low absolute accuracy on challenging problems; fails on nested-bracket expressions, problems needing retention of variable values across steps, and other complex multi-step reasoning tasks; overall exact-match accuracy remains modest (e.g., 24% on MathQuest).",
            "comparison_baseline": "Compared before vs after fine-tuning, across model sizes (7B vs 13B), and across datasets (GSM-8K, DeepMind, NumGLUE, SimulEq, Math-401, MathQuest).",
            "key_finding": "Fine-tuning a math-specialist 13B model on a large, augmented arithmetic dataset improves exact-match performance substantially relative to baseline models, but absolute performance on complex high-school math remains low (best reported: 24.0% on MathQuest), and mechanistic reasons for arithmetic errors are not elucidated.",
            "uuid": "e254.5",
            "source_info": {
                "paper_title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating transformer language models on arithmetic operations using number decomposition",
            "rating": 2,
            "sanitized_title": "evaluating_transformer_language_models_on_arithmetic_operations_using_number_decomposition"
        },
        {
            "paper_title": "How well do large language models perform in arithmetic tasks?",
            "rating": 2,
            "sanitized_title": "how_well_do_large_language_models_perform_in_arithmetic_tasks"
        },
        {
            "paper_title": "MathPrompter: Mathematical reasoning using large language models",
            "rating": 2,
            "sanitized_title": "mathprompter_mathematical_reasoning_using_large_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Pal: Program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Exploring generalization ability of pretrained language models on arithmetic and logical reasoning",
            "rating": 2,
            "sanitized_title": "exploring_generalization_ability_of_pretrained_language_models_on_arithmetic_and_logical_reasoning"
        }
    ],
    "cost": 0.012310999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks
19 Apr 2024</p>
<p>Avinash Anand avinasha@iiitd.ac.in 
MIDAS Labs IIIT-Delhi
DelhiIndia</p>
<p>Mohit Gupta 
MIDAS Labs IIIT-Delhi
DelhiIndia</p>
<p>Kritarth Prasad kritarth20384@iiitd.ac.in 
MIDAS Labs IIIT-Delhi
DelhiIndia</p>
<p>Navya Singla singlanavya01@gmail.com 
MIDAS Labs IIIT-Delhi
DelhiIndia</p>
<p>Sanjana Sanjeev sanjana21094@iiitd.ac.in 
MIDAS Labs IIIT-Delhi
DelhiIndia</p>
<p>Jatin Kumar 
MIDAS Labs IIIT-Delhi
DelhiIndia</p>
<p>Adarsh Raj Shivam 
MIDAS Labs IIIT-Delhi
DelhiIndia</p>
<p>Rajiv Ratn rajivratn@iiitd.ac.in 
MIDAS Labs IIIT-Delhi
DelhiIndia</p>
<p>Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks
19 Apr 2024B187AAC25FCDDFF7A719F62827B20394arXiv:2404.13099v1[cs.CL]
The rapid progress in the field of natural language processing (NLP) systems and the expansion of large language models (LLMs) have opened up numerous opportunities in the field of education and instructional methods.These advancements offer the potential for tailored learning experiences and immediate feedback, all delivered through accessible and cost-effective services.One notable application area for this technological advancement is in the realm of solving mathematical problems.Mathematical problem-solving not only requires the ability to decipher complex problem statements but also the skill to perform precise arithmetic calculations at each step of the problem-solving process.However, the evaluation of the arithmetic capabilities of large language models remains an area that has received relatively little attention.In response, we introduce an extensive mathematics dataset called "MathQuest" sourced from the 11th and 12th standard Mathematics NCERT textbooks.This dataset encompasses mathematical challenges of varying complexity and covers a wide range of mathematical concepts.Utilizing this dataset, we conduct fine-tuning experiments with three prominent LLMs: LLaMA-2, WizardMath, and MAmmoTH.These fine-tuned models serve as benchmarks for evaluating their performance on our dataset.Our experiments reveal that among the three models, MAmmoTH-13B emerges as the most proficient, achieving the highest level of competence in solving the presented mathematical problems.Consequently, MAmmoTH-13B establishes itself as a robust and dependable benchmark for addressing NCERT mathematics problems.GitHub repository: https://github.com/midas-research/mathify.NeurIPS'23 Workshop on Generative AI for Education (GAIED).</p>
<p>Introduction</p>
<p>Mathematical problem-solving represents a multifaceted cognitive skill, encompassing the comprehension of problem statements, identification of pertinent concepts and formulas, application of suitable strategies and algorithms, precise calculations, and the verification of solution validity and reasonableness.Traditionally, mathematical problem-solving has been imparted and assessed through conventional means such as textbooks, worksheets, and examinations, often affording limited feedback and learner guidance.Furthermore, these methods may not fully capture the diversity and intricacy of real-world mathematical challenges encountered by students.</p>
<p>In the era of rapid advancements in artificial intelligence and natural language processing (NLP), large language models (LLMs) have emerged as formidable tools for generating natural language text across a spectrum of domains and tasks [12].LLMs, grounded in the transformer architecture [32], have the capacity to glean long-range dependencies and contextual representations from vast corpora of text data.These LLMs have showcased impressive proficiency in mathematical reasoning and problem-solving by leveraging their inherent understanding of arithmetic operations, algebraic principles, and symbolic manipulation.Nevertheless, existing LLMs grapple with substantial hurdles in tackling math word problems, particularly those necessitating intricate reasoning, multi-step arithmetic calculations, or domain-specific knowledge [13,20,37].</p>
<p>Inference Results</p>
<p>Testing the fine-tuned models on these datasets to compare the inference results.The advent of large language models (LLMs) has proven to be a boon in the field of education, as evidenced by recent studies [25,29,39].These versatile models have ushered in a new era of learning possibilities, catering to individual student needs by considering their preferences, objectives, interests, and aptitudes.For instance, LLMs offer a tailored learning experience, providing personalized feedback, guidance, explanations, and recommendations [16].Educators, too, find these models invaluable, as they simplify the creation of engaging learning materials such as quizzes, summaries, questions, and exercises [27].Notably, LLMs can even generate multiple-choice questions based on provided text passages.Additionally, these models excel in enhancing language proficiency, aiding learners in vocabulary, grammar, pronunciation, and fluency [16].Their versatility extends to assisting students and researchers in exploring new topics and extracting information from diverse sources.They effortlessly generate summaries [38], identify keywords, generate citations [17,3,4], and provide relevant links in response to queries.This paper endeavors to tackle the challenges posed by mathematical problem-solving within the context of LLMs.To this end, we introduce MathQuest, a comprehensive mathematics dataset meticulously curated from the 11th and 12th standard Mathematics NCERT textbooks 1 .This dataset spans various levels of mathematical complexity and encompasses a wide array of mathematical concepts.We introduce this dataset because existing open-source datasets primarily consist of relatively straightforward mathematical problems.In contrast, standard mathematical problems can be significantly more complex.To equip Large Language Models (LLMs) with the ability to solve these intricate problems, we conduct fine-tuning on this dataset.Furthermore, we propose a novel approach for fine-tuning three preeminent LLMs: MAmmoTH [41], LLaMA-2 [31], and WizardMath [23] using our MathQuest dataset.Our evaluation encompasses not only the performance of these finetuned models on our dataset but also their proficiency on other openly accessible mathematical reasoning datasets.Our findings indicate that MAmmoTH-13B outshines its counterparts, emerging as the most adept and proficient in solving the mathematical challenges presented.Thus, MAmmoTH-13B establishes itself as a dependable and robust baseline for addressing NCERT mathematics problems.</p>
<p>Related Work</p>
<p>In this section, we delve into the existing literature, unveiling a diverse array of approaches utilizing Large Language Models (LLMs) for tackling mathematical problems.</p>
<p>Recent research has highlighted the potential of Large Language Models (LLMs) in education [2,1].They offer promise in automating question generation and supporting direct interactions within the learning environment [18].Furthermore, investigations have explored few-shot prompting techniques over LLMs for addressing mathematical word problems [35,42,11].The "chain-ofthought" prompting approach [35] leverages explicit intermediate reasoning steps to bolster the LLM's reasoning abilities.To mitigate arithmetic errors commonly observed in LLMs [21,14], earlier studies [7] have explored the use of external calculators to execute operations generated by LLMs.</p>
<p>Problem Problem</p>
<p>If the lines $2x + y -3 = 0 , 5x + ky -3 = 0$ and $3x -y -2 = 0$ are concurrent, find the value of k.</p>
<p>If the lines $2x + y -3 = 0 , 5x + ky -3 = 0$ and $3x -y -2 = 0$ are concurrent, find the value of k.</p>
<p>Solution Solution</p>
<p>For lines to be concurrent, they must intersect at a common point.We begin by determining the intersection point of lines (1) and (3).Using the lines $2x + y -3 = 0$ (referred to as ( 1)) and $3x -y -2 = 0$ (referred to as (3)), and solving them simultaneously, we obtain the coordinates (1, 1) for their intersection.This means that for the lines to be concurrent, the point (1, 1) must also satisfy the second line, $5x + ky -3 = 0$ (referred to as (2)).Substituting x = 1 and y = 1 into this equation, we obtain $5(1) + k(1) -3 = 0$, which yields the result k = -2.</p>
<p>For lines to be concurrent, they must intersect at a common point.We begin by determining the intersection point of lines ( 1) and ( 3).Using the lines $2x + y -3 = 0$ (referred to as ( 1)) and $3x -y -2 = 0$ (referred to as (3)), and solving them simultaneously, we obtain the coordinates (1, 1) for their intersection.This means that for the lines to be concurrent, the point (1, 1) must also satisfy the second line, $5x + ky -3 = 0$ (referred to as (2)).Substituting x = 1 and y = 1 into this equation, we obtain $5(1) + k(1) -3 = 0$, which yields the result k = -2.</p>
<p>Figure 2: Our Dataset MathQuest Sample</p>
<p>Furthermore, [36] presents a novel method tailored for addressing elementary arithmetic and logical problems.This method concatenates the generated answer with the original problem statement, tasking the model with predicting the initial conditions to verify the accuracy of the answer.Notably, a subset of these approaches [10,5] can function effectively with zero-shot prompts, offering a versatile approach to mathematical problem-solving.A specialized method, MathPrompter [15], targets the enhancement of arithmetic operations and reasoning capabilities of LLMs, particularly designed to facilitate mathematical problem-solving tasks.</p>
<p>Various approaches exist for enhancing mathematical problem-solving with Large Language Models (LLMs).Wang et al.'s self-consistency [34], built on the CoT framework, assesses multiple potential reasoning paths and selects answers via majority vote.[22] extend self-consistency by teaching a verifier to validate each step, while [24] use recent LLMs like GPT-3.5 to generate an output, provide feedback, and prompt the model for improvements.[33] evaluate pretrained language models on basic arithmetic expressions, including addition (+) and subtraction (−), and [28] expand the assessment to include multiplication ( * ) operations within the language models' scope.</p>
<p>Dataset</p>
<p>For our research experiments, we employed the Math-401 dataset [40], which encompasses 401 samples of mathematical problems.This dataset encompasses a diverse range of mathematical operations, including addition (+), subtraction (−), multiplication ( * ), division (/), exponentiation, trigonometric functions (sin, cos, tan), logarithmic functions (log, ln), and incorporates integers, decimals, and irrational numbers (π, e).Recognizing the limited sample size of this dataset for effective learning by large language models, we expanded it through augmentation, resulting in a dataset size of 302, 000 samples.To construct our augmented dataset, we employed the SymPy Python library.This library allowed us to generate arithmetic mathematical equations along with their corresponding ground truth values.These equations covered basic arithmetic operators such as addition (+), subtraction (-), multiplication (*), and division (/).Furthermore, the dataset includes extensive arithmetic expressions with brackets, mimicking the complexity often encountered in real-world math word problems.Table 1 provides a comprehensive breakdown of the question types utilized in the creation of our augmented dataset.Furthermore, we evaluated our model's performance on four additional datasets: GSM-8K [8], DeepMind [30], NumGLUE [26], and SimulEq [19].</p>
<p>Methodology</p>
<p>This research aims to enhance the mathematical problem-solving capabilities of large language models.Initially, we observed that existing open-source models such as LLaMA-2 [31] and Vicuna [6] struggled with elementary mathematical tasks like simple addition and subtraction.This observation served as the catalyst for our research, motivating us to improve LLMs' proficiency in comprehending and accurately solving mathematical problems.</p>
<p>To achieve this, we adopted a instructive approach reminiscent of teaching mathematics to students.We commenced by imparting a clear understanding of fundamental operators such as +, −, * , /, gradually progressing to more advanced operators and expressions.Similarly, we endeavored to acquaint LLMs with the meanings of mathematical operators and expressions.To facilitate this process, we leveraged the Math-401 dataset [40], a valuable resource comprising 401 data samples consisting of basic mathematical questions and their corresponding answers.Given the dataset's limited size, we augmented it to introduce greater diversity and complexity, ensuring that the model could grasp and master advanced mathematical concepts during training.</p>
<p>For the fine-tuning process, we employed three prominent large language models: LLaMA-2 [31], WizardMath [23], and MAmmoTH [41].LLaMA-2 [31] represents an upgraded version of LLaMA, refined through training on an enriched mixture of publicly available data.The enhancements WizardMath [23] introduces an innovative approach known as Reinforcement Learning from Evol-Instruct Feedback (RLEIF).This method combines Evol-Instruct and reinforced process supervision techniques to evolve GSM8k and MATH datasets.Subsequently, it fine-tunes the pre-trained LLaMA-2 model using the evolved data and reward models, resulting in the development of the WizardMath model.</p>
<p>Lastly, the MAmmoTH [41] models are trained using the MathInstruct dataset, meticulously curated for instructional tuning.MathInstruct is constructed from a compilation of 13 mathematical datasets, including six newly curated rationales.It encompasses a hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, ensuring comprehensive coverage of diverse mathematical domains.The entire fine-tuning process is outlined in Figure .1.</p>
<p>Model # of Params</p>
<p>LLaMA-2</p>
<p>7B</p>
<p>LLaMA-2
13B WizardMath 7B WizardMath 13B MAmmoTH 7B MAmmoTH 13B Accuracy GSM-8K</p>
<p>Experiments</p>
<p>In this section, we delve into the details of our conducted experiments, outlining the experimental setup and the utilized hyper-parameters.Our research objective revolves around the creation of a high school-level mathematical dataset, encompassing questions of varying complexities and diverse concepts, followed by the establishment of robust baselines for solving mathematical problems.</p>
<p>To achieve this, we conducted experiments involving three prominent large language models: LLaMA-2 [31], WizardMath [41].We performed these experiments on both the 7B and 13B variants of these large language models (LLMs).Our experiments were executed in two stages.In the first stage, we directly loaded the original model weights and carried out inference on our designated test set.In the second stage, we undertook the fine-tuning of these models using the Math-401 [40] dataset as a crucial step in the process.</p>
<p>The Math-401 [40] dataset initially comprised 401 elementary mathematical equations paired with their corresponding results.To enhance its comprehensiveness and diversity, we performed data augmentation by introducing more intricate equations involving operators such as addition (+), subtraction (−), multiplication ( * ), division (/), as well as parentheses (()).This augmentation process aimed to create a more generalized and versatile dataset.Subsequently, we proceeded to fine-tune the Large Language Models (LLMs) using this augmented Math-401 [40] dataset.</p>
<p>Model # of Params</p>
<p>LLaMA-2</p>
<p>7B</p>
<p>LLaMA-2 The dataset was split into training (241,600 samples), validation (30,200 samples), and test (30,200 samples) subsets.We used the AdamW optimizer, a well-recognized technique, to enhance model performance.This optimization step was crucial for achieving the results in our study.
13B WizardMath 7B WizardMath 13B MAmmoTH 7B MAmmoTH 13B Accuracy GSM-8K DeepMind NumGLUE SimulEq Math-401* MathQuest
For fine-tuning, we employed QLora [9], an efficient approach that maximizes memory efficiency and minimize computation cost using 4-bit quantization in a pretrained language model, resulting in Low Rank Adapters (LoRA).Each model underwent 10 epochs of fine-tuning with a learning rate of 3 × 10 −4 .Post fine-tuning, we assessed the models using the same test set employed for pre-fine-tuning inference.The results, summarized in Table .3, serve to highlight the enhancements achieved in mathematical problem-solving capabilities before and after fine-tuning.</p>
<p>Evaluation Metric</p>
<p>We compared all model variants to evaluate the quality of the generated solutions.To measure performance, we assessed the accuracy in matching the generated answers to the actual solutions for five open-source datasets: GSM-8K, DeepMind, SimulEq, NumGLUE, and Math-401.These datasets provide ground truth answers for exact match accuracy calculation.</p>
<p>Results &amp; Discussion</p>
<p>In this section, we present the outcomes of our experiments in the domain of mathematical problemsolving.Our study encompasses evaluations conducted on our proprietary dataset, MathQuest, as well as five other publicly available datasets.This paper establishes baseline performance metrics for the task using our MathQuest dataset.To gauge the effectiveness of Large Language Models (LLMs) across diverse datasets, we utilize exact match accuracy as a benchmark metric.</p>
<p>We organize our results into two distinct setups: before fine-tuning and after fine-tuning the models, with the primary aim of evaluating the model's learning capabilities.Table .2 presents the exact match accuracy of three models across two variants, 7B and 13B, before fine-tuning, on five datasets and our dataset MathQuest.To summarize these findings, referring to Table .2, the performance of all the models is notably lower on the SimulEq dataset, as well as on our augmented dataset, Math-401.This discrepancy can be attributed to the presence of intricate problems within these datasets, which often require additional knowledge, such as questions like "Number of red color cards in a deck of 52 cards."Consequently, Table .3provides a detailed overview of the accuracy results following the fine-tuning process.In summary, the accuracy of all models showed significant improvement after undergoing fine-tuning on our diverse and complex question-answer dataset.Notably, models with 13B parameters exhibited higher accuracy compared to those with 7B parameters.</p>
<p>The key takeaways from Table .2, and Table .3 reveal that the best-performing model is MAmmoTH-13B for our dataset MathQuest, exhibiting the highest accuracy among all models after fine-tuning, at 24.0%.Additionally, it's noteworthy that both MAmmoTH 7B and 13B generated outputs with precision up to two decimal places, indicating their accuracy.From Table 3, It is evident that our dataset, MathQuest, poses a greater challenge due to its complexity and diversity, resulting in lower accuracy compared to other datasets.</p>
<p>Conclusion</p>
<p>In summary, our approach enhances Large Language Models (LLMs) in acquiring vital reasoning skills for precise mathematical problem-solving.We introduce tailored question-answer pairs in our MathQuest dataset, encompassing single or multiple mathematical operators and expressions.These supportive simple and complex problems guide the model toward incremental problem-solving.Our primary aim is to provide illustrative examples that improve solution accuracy and clarity.</p>
<p>Our results demonstrate significant enhancements in both solution precision and comprehensibility, promising valuable support for educators and students seeking effective mathematical problemsolving capabilities.</p>
<p>While our research establishes a robust foundation for advancing mathematical problem-solving with Generative LLMs, further refinements and optimizations are essential to extend its applicability across a broader range of scenarios.Ultimately, our work contributes to advancing conceptual understanding and numerical problem-solving in high school-level mathematical question-answering, offering valuable assistance to students and professionals grappling with complex questions through LLMs.</p>
<p>Limitations</p>
<p>While our proposed solution can successfully solve basic mathematical problems, it occasionally encounters challenges when dealing with complex mathematical problems that involve retaining variable values for use in subsequent equations.</p>
<p>Another limitation of our proposed work is the partial enhancement of reasoning abilities in LLMs for solving mathematical problems.However, it still falls short in dealing with complex expressions that include nested brackets within equations.The reason could be limited training dataset size, we will try to increase our training data in future research.We intend to address this limitation in our future work, wherein we plan to incorporate recent prompting techniques and further enhance LLMs reasoning abilities for these types of problems.</p>
<p>Acknowledgement</p>
<p>Dr. Rajiv Ratn Shah is partly supported by the Infosys Center for AI, the Center of Design and New Media, and the Center of Excellence in Healthcare at Indraprastha Institute of Information Technology, Delhi.We gratefully thank Dr. Astha Verma and Mr. Naman Lal for their guidance and continuous support during our research.Their knowledge and insightful feedback significantly influenced the direction and quality of our research.We appreciate their time, devotion, and willingness to share information, which all contributed considerably to the accomplishment of this job.Their encouragement and constructive talks were a continual source of motivation for us, and we consider ourselves fortunate to have benefited from their wisdom and leadership.</p>
<p>Figure 1 :
1
Figure 1: This figure shows the fine-tuning flow, the LLMs we use for fine-tuning, and the datasets we use for inference.</p>
<p>Figure 3 :
3
Figure 3: Distribution of Count of Samples of each Concept</p>
<p>Table 3 :
3
Exact Match Accuracy Results on the set of 100 samples of 5 datasets and our dataset MathQuest After fine-tuning on Math-401 dataset.(*) refers to the set of Math-401 we augmented for fine-tuning.</p>
<p>Table 1 :
1TypeRangeDecimal Places (1 -4)VariablesCountSmall Integer[-20, 20]×(x, y)65,000Small Decimal[-20, 20]✓(x, y)35,000Small Decimal + Integer[-20, 20]✓(x, y)39,000Large Integer[-1000, 1000]×(x, y)39,000Large Decimal[-1000, 1000]✓(x, y)25,000Large Decimal + Integer[-1000, 1000]✓(x, y)25,0003 Terms[-100, 100]✓(x, y, z)25,0004 Terms[-100, 100]✓(w, x, y, z)49,000Total---302,000
The distribution of types of question in our augmented Math-401 dataset3.1OurDataset:MathQuestWehavemeticulouslycurated our own dataset, referred to as MathQuest, sourcing problems from high school mathematics NCERT books.MathQuest is a rich resource, encompassing word problems of varying complexities and spanning diverse mathematical concepts.Our dataset comprises a total of 14 overarching mathematical domains, including sets, trigonometry, binomial theorem, and more.The distribution of samples across these concepts is visually represented in Figure.3.Our dataset contains total of 223 samples.Notably, as depicted in the charts, the category of "Sequence and Series" boasts the highest number of problems within our dataset.To provide a glimpse of our dataset's structure, we present a sample from MathQuest in Figure.2.</p>
<p>Table 2 :
2
DeepMind NumGLUE SimulEq Math-401<em> MathQuest
16.046.037.011.010.010.422.050.042.015.010.014.161.051.054.027.06.014.665.055.070.036.08.014.343.049.054.023.011.012.244.048.056.026.014.018.1
Exact Match Accuracy results on the set of 100 samples of 5 datasets and our dataset MathQuest Before fine-tuning on Math-401 dataset.(</em>) refers to the set of Math-401 we augmented for fine-tuning.</p>
<p>https://ncert.nic.in/</p>
<p>Revolutionizing high school physics education: A novel dataset. Avinash Anand, Krishnasai Addala, Kabir Baghel, Arnav Goel, Medha Hira, Rushali Gupta, Rajiv Ratn Shah, Big Data and Artificial Intelligence: 11th International Conference, BDA 2023. Delhi, India; Berlin, HeidelbergSpringer-VerlagDecember 7-9, 2023. 2023</p>
<p>Sciphyrag -retrieval augmentation to improve llms on physics q&amp;a. Avinash Anand, Arnav Goel, Medha Hira, Snehal Buldeo, Jatin Kumar, Astha Verma, Rushali Gupta, Rajiv Ratn Shah, Big Data and Artificial Intelligence: 11th International Conference, BDA 2023. Delhi, India; Berlin, HeidelbergSpringer-VerlagDecember 7-9, 2023. 2023</p>
<p>Kg-ctg: Citation generation through knowledge graph-guided large language models. Avinash Anand, Mohit Gupta, Kritarth Prasad, Ujjwal Goel, Naman Lal, Astha Verma, Rajiv Ratn Shah, Big Data and Artificial Intelligence: 11th International Conference, BDA 2023. Delhi, India; Berlin, HeidelbergSpringer-VerlagDecember 7-9, 2023. 2023</p>
<p>Context-Enhanced Language Models for Generating Multi-paper Citations. Avinash Anand, Kritarth Prasad, Ujjwal Goel, Mohit Gupta, Naman Lal, Astha Verma, Rajiv Shah, Big Data and Artificial Intelligence: 11th International Conference, BDA 2023. 122023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2022</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav Petrovand Noah Fiedel. Palm: Scaling language modeling with pathways</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, Gilbert Strang, aug 2022Proceedings of the National Academy of Sciences119</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 2023</p>
<p>Large language models: A comprehensive survey of its applications, challenges, limitations, and future prospects. Qasem Muhammad Usman Hadi, Rizwan Al-Tashi, Abbas Qureshi, Amgad Shah, Muhammad Muneer, Anas Irfan, Muhammad Zafar, Naveed Shaikh, Jia Akhtar, Seyedali Wu, Mirjalili, 072023</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, 2023</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Shima Imani, Liang Du, Harsh Shrivastava, Mathprompter, Mathematical reasoning using large language models. 2023</p>
<p>Large language models in education: A focus on the complementary relationship between human teachers and chatgpt -education and information technologies. Jaeho Jeon, Seongyong Lee, May 2023</p>
<p>. Shing-Yun Jung, Ting-Han Lin, Chia-Hung Liao, Shyan-Ming Yuan, Chuen-Tsai Sun, 10Intent-controllable citation text generation. Mathematics</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, Gjergji Kasneci, Learning and Individual Differences. 1031022742023</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsJune 2016</p>
<p>Thoughts about research on mathematical problem-solving instruction. Frank Lester, Mathematics Enthusiast. 102013</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>Making large language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 2023</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, 2023</p>
<p>Large language models challenge the future of higher education. Silvia Milano, Joshua A Mcgrane, Sabina Leonelli, Nature Machine Intelligence. 542023</p>
<p>NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, Ashwin Kalyan, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Empowering education with llms-the next-gen interface and content generation. Steven Moore, Richard Tong, Anjali Singh, Zitao Liu, Xiangen Hu, Yu Lu, Joleen Liang, Chen Cao, Hassan Khosravi, Paul Denny, International Conference on Artificial Intelligence in Education. Springer2023</p>
<p>Evaluating transformer language models on arithmetic operations using number decomposition. Matteo Muffo, Aldo Cocco, Enrico Bertino, 2023</p>
<p>Anastasia Olga, Akash Saini, Gabriela Zapata, Duane Searsmith, Bill Cope, Mary Kalantzis, Vania Castro, Theodora Kourkoulou, John Jones, Rodrigo Abrantes Da Silva, arXiv:2305.07605Generative ai: Implications and applications for education. 2023arXiv preprint</p>
<p>Analysing mathematical reasoning abilities of neural models. David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, 2019</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 2023</p>
<p>Exploring generalization ability of pretrained language models on arithmetic and logical reasoning. Cunxiang Wang, Boyuan Zheng, Yuchen Niu, Yue Zhang, 2021</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao, 2023</p>
<p>An empirical study on challenging math problem solving with gpt-4. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang, 2023</p>
<p>Enhancing llm with evolutionary fine tuning for news summary generation. Le Xiao, Xiaolin Chen, 2023</p>
<p>Practical and ethical challenges of large language models in education: A systematic literature review. Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, Dragan Gašević, arXiv:2303.133792023arXiv preprint</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, 2023</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 2023</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
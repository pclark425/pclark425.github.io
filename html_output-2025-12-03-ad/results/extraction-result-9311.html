<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9311 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9311</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9311</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-275789973</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.11833v1.pdf" target="_blank">Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we present an investigative study on how Mental Sets influence the reasoning capabilities of LLMs. LLMs have excelled in diverse natural language processing (NLP) tasks, driven by advancements in parameter-efficient fine-tuning (PEFT) and emergent capabilities like in-context learning (ICL). For complex reasoning tasks, selecting the right model for PEFT or ICL is critical, often relying on scores on benchmarks such as MMLU, MATH, and GSM8K. However, current evaluation methods, based on metrics like F1 Score or reasoning chain assessments by larger models, overlook a key dimension: adaptability to unfamiliar situations and overcoming entrenched thinking patterns. In cognitive psychology, Mental Set refers to the tendency to persist with previously successful strategies, even when they become inefficient - a challenge for problem solving and reasoning. We compare the performance of LLM models like Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct and GPT-4o in the presence of mental sets. To the best of our knowledge, this is the first study to integrate cognitive psychology concepts into the evaluation of LLMs for complex reasoning tasks, providing deeper insights into their adaptability and problem-solving efficacy.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9311.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9311.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot prompting (FS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a small number of solved example problems in the prompt (in-context examples) before asking the model to solve target problems; used to bias models toward solution strategies demonstrated in examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct; Llama-3.1-70B-Instruct; GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical equivalence problems (complex vs. shortcut)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A set of 12 mathematical equivalence problems (6 complex problems requiring multi-step strategies and 6 shortcut problems solvable via repeated-addend shortcuts) used to probe procedural flexibility and susceptibility to mental set.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting: the LLM is given in-context examples (few solved items) before attempting targets. Experiments used two ordering conditions for the examples/problems: complex-first (solve 6 complex then 6 shortcut) and shortcut-first (solve 6 shortcut then 6 complex).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against the same tasks with Few-shot + Zero-shot Chain-of-Thought prompting (FS+CoT) and with baselines that present only SP (shortcut only) or CP (complex only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varied by model and order; reported Exact Match (EM) aggregated over 6 inputs per condition. For GPT-4o under FS the paper reports EMs by condition: SP: 0.33, SP+CPF: 0.66, CP: 0.66, CP+SPF: 0.83 (averaged EM across the six problems in each condition). Number of steps under FS was reported to be low (often 1 step for successful cases).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performance improves when in-context examples are present (SP+CPF or CP+SPF) compared to single-type baselines (SP or CP); exact per-model/per-condition numbers are reported in the paper's tables (see paper). For GPT-4o the FS EMs above can be compared to FS+CoT (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In-context examples (few-shot) steer LLMs toward strategies demonstrated in examples, increasing use of the shown (possibly more efficient) strategy; few-shot prompting typically leads to fewer reasoning steps for problems the model can solve directly, indicating models can mimic the demonstrated solution style.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Dataset adapted from DeCaro (2016): 6 complex and 6 shortcut problems with identical numbers across types. Metrics: Exact Match (EM) averaged over six input problems per condition, and Steps averaged over inputs where the model reached the correct solution. Models were prompted in few-shot mode with the two orderings (complex-first and shortcut-first) and baselines of SP-only and CP-only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9311.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9311.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FS+CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting plus zero-shot Chain-of-Thought (FS+CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid prompting format where few-shot in-context examples are given and the model is also prompted to produce chain-of-thought style reasoning (here implemented as few-shot examples plus zero-shot CoT style prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct; Llama-3.1-70B-Instruct; GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical equivalence problems (complex vs. shortcut)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same 12-problem mathematical equivalence dataset (6 complex, 6 shortcut) probing strategy choice and procedural flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot examples combined with zero-shot Chain-of-Thought prompting (instructing or eliciting intermediate reasoning steps). The paper refers to this as few-shot + zero-shot CoT (citing Wei et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against plain Few-shot prompting (FS) and baseline conditions SP and CP (single-type problem presentations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FS+CoT generally increases Exact Match at the cost of more reasoning steps. Example (reported in paper): GPT-4o EMs under FS+CoT: SP: 0.66, SP+CPF: 0.66, CP: 0.83, CP+SPF: 0.83 with Steps averaging ~3 for successful problems (contrast to ~1 step under FS).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>For GPT-4o, FS -> FS+CoT comparison on SP condition: EM increased from 0.33 (FS) to 0.66 (FS+CoT), and Steps increased from 1 to 3, indicating an accuracy gain accompanied by longer, more complex reasoning traces. Similar patterns (increased steps with higher accuracy) are reported for other evaluated models in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+33 percentage points EM on GPT-4o SP condition (0.33 -> 0.66); corresponding increase in Steps from ~1 to ~3 (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT elicits more elaborate intermediate reasoning from LLMs, which can boost final-answer accuracy but also induces more complex (multi-step) solution traces; authors interpret this as a mental-set shift—CoT can encourage different internal solution strategies, sometimes helping escape simple but incorrect strategies but increasing step counts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompting variants included few-shot examples plus a chain-of-thought elicitation (zero-shot CoT). Metrics: Exact Match averaged over 6 problems per condition and Steps averaged over successful cases. The paper reports aggregated EM and Steps per model and per condition (SP, SP+CPF, CP, CP+SPF).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9311.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9311.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Problem-order effect (CPF vs SPF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Presentation order: Complex-first (CPF) vs Shortcut-first (SPF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manipulating the order of problem presentation/examples (solving complex problems first then shortcut problems, or vice versa) to test whether prior exposure induces a mental set and affects subsequent strategy choice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct; Llama-3.1-70B-Instruct; GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical equivalence problems (complex vs. shortcut)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same dataset of 6 complex and 6 shortcut problems; conditions differ only in the ordering of problem types presented to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Two ordering conditions: Complex-first (solve 6 complex problems then 6 shortcut problems) and Shortcut-first (solve 6 shortcut problems then 6 complex problems). These orderings are applied within the few-shot or FS+CoT prompting setups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to baselines presenting only shortcut problems (SP) or only complex problems (CP) and across FS vs FS+CoT prompting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The paper reports that in-context examples in either order (CPF or SPF) enhance model performance (higher Exact Match) relative to baselines; however, the order (CPF vs SPF) did not systematically change the average number of steps to solution across conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: both CPF and SPF improved EM compared to SP or CP baselines; no consistent change in Steps due to order alone. Numeric EM improvements are reported in the paper's tables per model/condition.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors conclude that exposure to in-context examples (regardless of order) makes models more likely to adopt the demonstrated strategies (improving accuracy), but presentation order by itself did not alter the number of steps required; this suggests that order affects strategy adoption (accuracy) but not the step-count metric in their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Each ordering condition used the same six problems per type; evaluations averaged EM over six problems per condition and Steps averaged over successful runs. Both FS and FS+CoT were evaluated under each ordering condition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Inducing mental set constrains procedural flexibility and conceptual understanding in mathematics <em>(Rating: 2)</em></li>
                <li>Investigating the effect of mental set on insight problem solving <em>(Rating: 2)</em></li>
                <li>A survey on in-context learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9311",
    "paper_id": "paper-275789973",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Few-shot prompting (FS)",
            "name_full": "Few-shot in-context prompting",
            "brief_description": "Providing a small number of solved example problems in the prompt (in-context examples) before asking the model to solve target problems; used to bias models toward solution strategies demonstrated in examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct; Llama-3.1-70B-Instruct; GPT-4o",
            "model_size": null,
            "task_name": "Mathematical equivalence problems (complex vs. shortcut)",
            "task_description": "A set of 12 mathematical equivalence problems (6 complex problems requiring multi-step strategies and 6 shortcut problems solvable via repeated-addend shortcuts) used to probe procedural flexibility and susceptibility to mental set.",
            "presentation_format": "Few-shot prompting: the LLM is given in-context examples (few solved items) before attempting targets. Experiments used two ordering conditions for the examples/problems: complex-first (solve 6 complex then 6 shortcut) and shortcut-first (solve 6 shortcut then 6 complex).",
            "comparison_format": "Compared against the same tasks with Few-shot + Zero-shot Chain-of-Thought prompting (FS+CoT) and with baselines that present only SP (shortcut only) or CP (complex only).",
            "performance": "Varied by model and order; reported Exact Match (EM) aggregated over 6 inputs per condition. For GPT-4o under FS the paper reports EMs by condition: SP: 0.33, SP+CPF: 0.66, CP: 0.66, CP+SPF: 0.83 (averaged EM across the six problems in each condition). Number of steps under FS was reported to be low (often 1 step for successful cases).",
            "performance_comparison": "Performance improves when in-context examples are present (SP+CPF or CP+SPF) compared to single-type baselines (SP or CP); exact per-model/per-condition numbers are reported in the paper's tables (see paper). For GPT-4o the FS EMs above can be compared to FS+CoT (see separate entry).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "In-context examples (few-shot) steer LLMs toward strategies demonstrated in examples, increasing use of the shown (possibly more efficient) strategy; few-shot prompting typically leads to fewer reasoning steps for problems the model can solve directly, indicating models can mimic the demonstrated solution style.",
            "null_or_negative_result": false,
            "experimental_details": "Dataset adapted from DeCaro (2016): 6 complex and 6 shortcut problems with identical numbers across types. Metrics: Exact Match (EM) averaged over six input problems per condition, and Steps averaged over inputs where the model reached the correct solution. Models were prompted in few-shot mode with the two orderings (complex-first and shortcut-first) and baselines of SP-only and CP-only.",
            "uuid": "e9311.0",
            "source_info": {
                "paper_title": "Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "FS+CoT",
            "name_full": "Few-shot prompting plus zero-shot Chain-of-Thought (FS+CoT)",
            "brief_description": "A hybrid prompting format where few-shot in-context examples are given and the model is also prompted to produce chain-of-thought style reasoning (here implemented as few-shot examples plus zero-shot CoT style prompting).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct; Llama-3.1-70B-Instruct; GPT-4o",
            "model_size": null,
            "task_name": "Mathematical equivalence problems (complex vs. shortcut)",
            "task_description": "Same 12-problem mathematical equivalence dataset (6 complex, 6 shortcut) probing strategy choice and procedural flexibility.",
            "presentation_format": "Few-shot examples combined with zero-shot Chain-of-Thought prompting (instructing or eliciting intermediate reasoning steps). The paper refers to this as few-shot + zero-shot CoT (citing Wei et al., 2022).",
            "comparison_format": "Compared against plain Few-shot prompting (FS) and baseline conditions SP and CP (single-type problem presentations).",
            "performance": "FS+CoT generally increases Exact Match at the cost of more reasoning steps. Example (reported in paper): GPT-4o EMs under FS+CoT: SP: 0.66, SP+CPF: 0.66, CP: 0.83, CP+SPF: 0.83 with Steps averaging ~3 for successful problems (contrast to ~1 step under FS).",
            "performance_comparison": "For GPT-4o, FS -&gt; FS+CoT comparison on SP condition: EM increased from 0.33 (FS) to 0.66 (FS+CoT), and Steps increased from 1 to 3, indicating an accuracy gain accompanied by longer, more complex reasoning traces. Similar patterns (increased steps with higher accuracy) are reported for other evaluated models in aggregate.",
            "format_effect_size": "+33 percentage points EM on GPT-4o SP condition (0.33 -&gt; 0.66); corresponding increase in Steps from ~1 to ~3 (reported).",
            "explanation_or_hypothesis": "CoT elicits more elaborate intermediate reasoning from LLMs, which can boost final-answer accuracy but also induces more complex (multi-step) solution traces; authors interpret this as a mental-set shift—CoT can encourage different internal solution strategies, sometimes helping escape simple but incorrect strategies but increasing step counts.",
            "null_or_negative_result": false,
            "experimental_details": "Prompting variants included few-shot examples plus a chain-of-thought elicitation (zero-shot CoT). Metrics: Exact Match averaged over 6 problems per condition and Steps averaged over successful cases. The paper reports aggregated EM and Steps per model and per condition (SP, SP+CPF, CP, CP+SPF).",
            "uuid": "e9311.1",
            "source_info": {
                "paper_title": "Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Problem-order effect (CPF vs SPF)",
            "name_full": "Presentation order: Complex-first (CPF) vs Shortcut-first (SPF)",
            "brief_description": "Manipulating the order of problem presentation/examples (solving complex problems first then shortcut problems, or vice versa) to test whether prior exposure induces a mental set and affects subsequent strategy choice.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct; Llama-3.1-70B-Instruct; GPT-4o",
            "model_size": null,
            "task_name": "Mathematical equivalence problems (complex vs. shortcut)",
            "task_description": "Same dataset of 6 complex and 6 shortcut problems; conditions differ only in the ordering of problem types presented to the model.",
            "presentation_format": "Two ordering conditions: Complex-first (solve 6 complex problems then 6 shortcut problems) and Shortcut-first (solve 6 shortcut problems then 6 complex problems). These orderings are applied within the few-shot or FS+CoT prompting setups.",
            "comparison_format": "Compared to baselines presenting only shortcut problems (SP) or only complex problems (CP) and across FS vs FS+CoT prompting methods.",
            "performance": "The paper reports that in-context examples in either order (CPF or SPF) enhance model performance (higher Exact Match) relative to baselines; however, the order (CPF vs SPF) did not systematically change the average number of steps to solution across conditions.",
            "performance_comparison": "Qualitative: both CPF and SPF improved EM compared to SP or CP baselines; no consistent change in Steps due to order alone. Numeric EM improvements are reported in the paper's tables per model/condition.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors conclude that exposure to in-context examples (regardless of order) makes models more likely to adopt the demonstrated strategies (improving accuracy), but presentation order by itself did not alter the number of steps required; this suggests that order affects strategy adoption (accuracy) but not the step-count metric in their setup.",
            "null_or_negative_result": true,
            "experimental_details": "Each ordering condition used the same six problems per type; evaluations averaged EM over six problems per condition and Steps averaged over successful runs. Both FS and FS+CoT were evaluated under each ordering condition.",
            "uuid": "e9311.2",
            "source_info": {
                "paper_title": "Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Inducing mental set constrains procedural flexibility and conceptual understanding in mathematics",
            "rating": 2,
            "sanitized_title": "inducing_mental_set_constrains_procedural_flexibility_and_conceptual_understanding_in_mathematics"
        },
        {
            "paper_title": "Investigating the effect of mental set on insight problem solving",
            "rating": 2,
            "sanitized_title": "investigating_the_effect_of_mental_set_on_insight_problem_solving"
        },
        {
            "paper_title": "A survey on in-context learning",
            "rating": 1,
            "sanitized_title": "a_survey_on_incontext_learning"
        }
    ],
    "cost": 0.01165675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs
21 Jan 2025</p>
<p>Saiful Haq saifulhaq@cse.iitb.ac.in 
Niyati Chhaya 
Piyush Pandey 
Pushpak Bhattacharya 
Iit Bombay 
Hyperbots Inc 
Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs
21 Jan 2025369AAC2519EBC3EB47F12E9B9C22C19BarXiv:2501.11833v1[cs.CL]
In this paper, we present an investigative study on how Mental Sets influence the reasoning capabilities of LLMs.LLMs have excelled in diverse natural language processing (NLP) tasks, driven by advancements in parameter-efficient fine-tuning (PEFT) and emergent capabilities like in-context learning (ICL).For complex reasoning tasks, selecting the right model for PEFT or ICL is critical, often relying on scores on benchmarks such as MMLU, MATH, and GSM8K.However, current evaluation methods, based on metrics like F1 Score or reasoning chain assessments by larger models, overlook a key dimension: adaptability to unfamiliar situations and overcoming entrenched thinking patterns.In cognitive psychology, Mental Set refers to the tendency to persist with previously successful strategies, even when they become inefficient -a challenge for problem solving and reasoning.We compare the performance of LLM models like Llama-3.1-8B-Instruct,Llama-3.1-70B-Instruct and GPT-4o in the presence of mental sets.To the best of our knowledge, this is the first study to integrate cognitive psychology concepts into the evaluation of LLMs for complex reasoning tasks, providing deeper insights into their adaptability and problem-solving efficacy.</p>
<p>Introduction</p>
<p>Recent advancements in transformer architectures (Vaswani, 2017) have profoundly reshaped NLP and vision-language tasks.Models such as LLaMA-3 (Dubey et al., 2024), Phi-4 (Abdin et al., 2024), Mixtral (Jiang et al., 2024), Deepseek-v3 (Liu et al., 2024), InternVL-2.5 (Chen et al., 2024), and Pixtral (Agrawal et al., 2024) have set new benchmarks in reasoning and generalization capabilities.These open-source large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on challenging benchmarks like MMLU (Hendrycks et al., 2020), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021), solidifying their role in solving reasoning-intensive tasks.</p>
<p>One of the most significant developments driving these advancements is in-context learning (ICL) (Dong et al., 2022).Unlike traditional training paradigms that require fine-tuning on task-specific data, ICL enables models to adapt to new tasks through examples embedded directly in the input prompts.This paradigm not only enhances task adaptability but also reduces computational overhead.The flexibility and scalability of ICL have propelled its adoption in solving problems across diverse domains, from mathematical reasoning to commonsense understanding.</p>
<p>Despite their success, traditional evaluation metrics such as accuracy, F1 score, and reasoning chain assessments primarily capture task performance in well-defined or familiar scenarios.These benchmarks, however, fail to account for an essential aspect of reasoning: adaptability in the face of novelty and the ability to overcome entrenched problem-solving patterns.This dimension aligns with the concept of a Mental Set (Öllinger et al., 2008) in cognitive psychology-a phenomenon where reliance on previously successful strategies inhibits the adoption of more efficient solutions in novel situations.Mental set, often described as cognitive rigidity, emerges when repeated success with specific approaches reinforces their use, even in cases where such approaches are suboptimal or ineffective.While this tendency streamlines problem-solving in routine contexts, it presents a critical bottleneck when models encounter unfamiliar challenges that demand innovative solutions.</p>
<p>In contrast to mental set, Insight represents the ability to overcome these entrenched patterns through a reorganization of mental representations, enabling novel solutions to emerge.Insightful problem-solving is characterized by sudden breakthroughs, often accompanied by the well-documented "aha!" moment.This process is inherently different from systematic problem-solving as it involves either decomposing familiar cognitive chunks or relaxing implicit constraints that limit how a problem is conceptualized.For instance, Duncker's candle problem (Duncker and Lees, 1945) and Luchins' water jug experiments (Luchins and Luchins, 1959) demonstrate how initial problem representations can either obstruct or facilitate insight, depending on whether solvers can reframe their approach.</p>
<p>The interplay between mental set and insight is particularly relevant when assessing reasoning capabilities, as models that exhibit high adaptability must strike a balance between leveraging prior knowledge and restructuring it when necessary.Understanding how these cognitive phenomena interact provides deeper insight into the flexibility and robustness of AI systems when faced with complex, unfamiliar tasks.Our contribution is:</p>
<p>• An investigative study on how Mental Sets influence the reasoning capabilities of LLMs (Llama-3.1-8B-Instruct,Llama-3.1-70B-Instruct and GPT-4o).To the best of our knowledge, this is the first study to integrate cognitive psychology concepts into the evaluation of LLMs for complex reasoning tasks, providing deeper insights into their adaptability and problem-solving efficacy.</p>
<p>Related work</p>
<p>Mental set (Öllinger et al., 2008) occurs when individuals continue to use a familiar strategy, even when it is no longer the most efficient approach.This is evident in problem-solving contexts like the Water-Jug problem (Luchins and Luchins, 1959), where participants persist in applying a complex method despite the availability of a simpler solution.Such entrenched thinking patterns can limit problem-solving effectiveness.For example, students might repeatedly use the "add-subtract" method in solving problems like 5 + 2 + 3 = ?+ 3, even when the "grouping" method would be faster and less cognitively demanding.This rigidity in thinking is a clear example of mental set interfering with problem-solving flexibility.On the other hand, procedural flexibility (DeCaro, 2016) refers to the ability to shift between various strategies or methods when solving problems, particularly in mathematics, where students often learn multiple approaches to tackle different types of prob-lems.This flexibility enables them to select the most efficient method depending on the problem's context.Research has shown that procedural flexibility is linked to a deeper conceptual understanding.For instance, when students are exposed to different problem-solving strategies, they become better equipped to adapt their approach to novel or unfamiliar challenges.Educational psychology studies suggest that students with greater procedural flexibility tend to experience reduced cognitive load and improved accuracy in their problemsolving.For example, in mathematics, students may be taught multiple strategies, such as the "addsubtract" method and the "grouping" method.The latter, which simplifies cognitive demands, may be more efficient in some cases.Flexible students are more likely to switch between methods depending on the problem, enhancing their understanding of key concepts like mathematical equivalence.While procedural flexibility is a valuable skill, mental set can hinder its development.</p>
<p>Dataset</p>
<p>The dataset used in this study is based on the problems outlined in (DeCaro, 2016), as summarized in Table 3.It consists of two types of mathematical equivalence problems: complex problems and shortcut problems.Each type includes six problems.</p>
<p>• Complex problems require multi-step strategies to solve.These problems involve various addition operations where multiple numbers must be added before solving for the missing value.</p>
<p>• Shortcut problems are designed for quicker resolution by leveraging repeated addends on both sides of the equation, enabling simpler and more efficient solutions.</p>
<p>The numbers used in both problem sets are consistent to ensure that any differences in problemsolving difficulty arise solely from the required strategy, rather than the numerical values themselves.The dataset is structured to evaluate how the order of problem presentation influences participants' (or LLMs') use of efficient problem-solving strategies.</p>
<p>Experiment Setup</p>
<p>The experimental setup is designed to evaluate the ability of large language models (LLMs) to Table 2: Performance comparison of models on shortcut problems (Sh) and complex problems (CP) across different conditions with fewshot prompting + zeroshot chain of thought.The conditions include SP+CPF (shortcut problems with complex problems first) and CP+SPF (complex problems with shortcut problems first), along with their respective baselines (SP and CP).The metrics reported are Exact Match (EM) scores, averaged over six input problems per condition, and the number of steps required to reach the solution, averaged over the input problems where the model successfully arrives at the correct solution.</p>
<p>solve mathematical equivalence problems, with a focus on the efficiency of strategy use (complex vs. shortcut strategies) based on the order of problem presentation.The experiment is adapted from the methodology proposed by (DeCaro, 2016) and consists of two conditions:</p>
<p>• Complex-first condition: LLMs solve six complex problems first, followed by six shortcut problems.</p>
<p>• Shortcut-first condition: LLMs solve six shortcut problems first, followed by six complex problems.</p>
<p>Each LLM is presented with the same set of problems, encompassing both complex and shortcut problems.For each problem, the LLMs are tasked with solving the equation by filling in the missing number, demonstrating their reasoning, and applying the most efficient strategy.The LLMs evaluated in this experiment include: Llama-3.1-8B-Instruct,Llama-3.1-70B-Instruct and GPT-4o.We employ both few-shot prompting and few-shot prompting with zero-shot chain-of-thought reasoning (Wei et al., 2022).</p>
<p>The evaluation metrics used are as follows: Exact Match, which measures whether the model produces the correct final answer and Steps, which tracks the number of steps taken to arrive at the correct solution.</p>
<p>Problem</p>
<p>Summary, Conclusion and Future Work</p>
<p>We present an investigative study on how mental sets influence the reasoning capabilities of LLMs.Inspired by cognitive psychology, we set up an experiment to compare the problem-solving abilities of three LLM models-Llama-3.1-8B-Instruct,Llama-3.1-70B-Instruct, and GPT-4o-on a mathematical equivalence dataset.Our findings show that in-context examples, whether under the CPF or SPF conditions, enhance performance but do not affect the number of steps taken to reach the final solution.Interestingly, while Few-shot (FS) prompting typically requires fewer steps, Few-shot+Zero-shot Chain-of-Thought (FS+CoT) prompting leads to more complex reasoning, as indicated by the increase in the number of steps required to solve problems.These results suggest that while humans can solve simpler problems in a single step, LLMs require more than one step to achieve higher accuracy, reflecting presence of mental sets.Future work includes expanding this dataset to test problem solving in VLMs.</p>
<p>Table 1 :
1
Performance comparison of models on shortcut problems (Sh) and complex problems (CP) across different conditions with few-shot prompting.The conditions include SP+CPF (shortcut problems with complex problems first) and CP+SPF (complex problems with shortcut problems first), along with their respective baselines (SP and CP).The metrics reported are Exact Match (EM) scores, averaged over six input problems per condition, and the number of steps required to reach the solution, averaged over the input problems where the model successfully arrives at the correct solution.
ModelMetric SP SP+CPF CP CP+SPFLlama-3.1-8b-instructEM0.16000Steps1xxxLlama-3.1-70b-instruct EM00.1600.66Stepsx1x1GPT-4oEM0.330.660.660.83Steps1111ModelMetric SP SP+CPF CP CP+SPFLlama-3.1-8b-instructEM0.50.660.330.83Steps3333Llama-3.1-70b-instruct EM0.50.660.660.83Steps3333GPT-4oEM0.660.660.830.83Steps3333</p>
<p>Table 3 :
3
Complex and Shortcut ProblemsHumans typically require just one step to solve problems in SP and SP+CPF, and LLMs demonstrate this capability in a limited number of successful cases, as shown in Table1.However, when using CoT prompting to improve performance, LLMs tend to take more than one step to solve problems, as illustrated in Table2, reflecting a mental set shift induced by CoT reasoning.
Type InputOutputcomplex7+5+9=3+?18complex4+14+8=6+?20complex15+3+9=13+?11complex9+7+6=?+319complex14+5+3=?+715complex6+3+12=?+156shortcut7+5+9=7+?14shortcut4+14+8=4+?22shortcut15+3+9=15+?12shortcut9+7+6=?+615shortcut14+5+3=?+315shortcut6+3+12=?+129</p>
<p>Jyoti Marah Abdin, Harkirat Aneja, Sébastien Behl, Ronen Bubeck, Suriya Eldan, Michael Gunasekar, Russell J Harrison, Mojan Hewett, Piero Javaheripi, Kauffmann, arXiv:2412.08905Phi-4 technical report. 2024arXiv preprint</p>
<p>Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, arXiv:2410.07073Pixtral 12b. 2024arXiv preprint</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Zhaoyang Hao Tian, Liu, arXiv:2412.052712024arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Inducing mental set constrains procedural flexibility and conceptual understanding in mathematics. Marci S Decaro, Memory &amp; cognition. 442016</p>
<p>. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, arXiv:2301.00234et al. 2022. A survey on in-context learning. arXiv preprint</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>On problemsolving. Karl Duncker, Lynne S Lees, Psychological monographs. 5851945</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Rigidity of behavior: A variational approach to the effect of einstellung. S Abraham, Edith Luchins, Luchins Hirsch, 1959</p>
<p>Investigating the effect of mental set on insight problem solving. Michael Öllinger, Gary Jones, Günther Knoblich, Experimental psychology. 5542008</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>            </div>
        </div>

    </div>
</body>
</html>
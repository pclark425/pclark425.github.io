<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3227 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3227</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3227</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-277dd00ab02f122133bf56b485dfb7c730acdcde</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/277dd00ab02f122133bf56b485dfb7c730acdcde" target="_blank">Retrieval-based Language Models and Applications</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This tutorial will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs, focusing on their model architectures and learning approaches, and use an exercise to showcase the effectiveness of retrieval- based LMs.</p>
                <p><strong>Paper Abstract:</strong> Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3227.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3227.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest Neighbor Language Model (kNN-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language model augmented with a token-level nonparametric datastore that performs nearest-neighbor lookups at generation time to improve factual recall and long-tail generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalization through Memorization: Nearest Neighbor Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A decoder LM augmented with a token-level datastore of (context, next-token) entries; at each decoding step it retrieves nearest neighbor contexts and uses aggregated next-token statistics to interpolate with the model's predicted distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (token-level nearest neighbors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores representations keyed by context tokens; during generation the system performs nearest-neighbor search over the datastore, aggregates neighbor next-token counts/probabilities, and interpolates that distribution with the model softmax to produce the final next-token distribution; datastore can be updated independently of model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling / next-token prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predicting the next token given preceding context, with emphasis on factual recall, rare-token prediction, and long-tail knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Nearest-neighbor retrieval over a token-level memory improves recall and generalization compared to pure parametric LMs, especially on rare or long-tail tokens and factual retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dependence on retrieval quality, computational cost and scalability of nearest-neighbor search, integration/hyperparameter choices for interpolation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3227.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3227.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RETRO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Enhanced Transformer / Improving language models by retrieving from trillions of tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive transformer that retrieves chunk-level passages from a very large external datastore and conditions generation on the retrieved chunks to reduce required model parameters and improve factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving language models by retrieving from trillions of tokens</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RETRO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Autoregressive transformer that, for each context, retrieves nearest neighbor text chunks from a massive external corpus and integrates them into the transformer's computation (e.g., via cross-attention or concatenation) to inform generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (chunk-level external datastore)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Performs chunk-level retrieval from a very large corpus (potentially trillions of tokens) for the current context and feeds retrieved chunks into the model's layers so the LM can attend to external content during decoding; datastore is external and updatable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling and knowledge-intensive generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-token generation and knowledge-intensive generation tasks where external factual content improves accuracy and reduces reliance on parametric capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / knowledge-intensive generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chunk-level retrieval from very large corpora allows models to rival or surpass larger parametric-only LMs while using fewer parameters and enables knowledge updates by changing the datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Practical retrieval scalability at extreme corpus sizes, sensitivity to retrieval quality, and potential privacy or leakage concerns from the datastore.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3227.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3227.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atlas: Few-shot Learning with Retrieval Augmented Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented few-shot learning approach where the model conditions on retrieved examples or passages to improve few-shot generalization across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Few-shot Learning with Retrieval Augmented Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-augmented language model designed for few-shot learning that retrieves relevant examples or passages at inference to provide task-specific context for the model.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (example/passage datastore)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Retrieves task-relevant examples or passages from a datastore and concatenates or otherwise integrates them into the model input, enabling the LM to use retrieved context as implicit demonstrations without altering its parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot learning / knowledge-intensive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Performing new tasks from only a few examples by leveraging retrieved similar examples or contextual passages to guide generation and predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>few-shot learning / knowledge-intensive generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval augmentation substantially helps few-shot performance by providing relevant context and examples, improving generalization with fewer parametric resources.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires effective retrieval and management of the example datastore; context-window limits for concatenating retrieved content.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3227.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3227.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mention Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An entity-centric memory mechanism that stores textual mentions keyed by entities and enables transformer models to attend to entity-specific stored text via mention-aware attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mention Memory</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer augmented with an entity-mention memory that provides dedicated attention to stored textual knowledge for specific entity mentions encountered in input.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>entity-mention memory (textual memory keyed by entities)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores textual snippets keyed by entity mentions; when an entity is mentioned the transformer can attend to the corresponding memory entries via specialized mention-attention mechanisms to incorporate entity-specific knowledge into representations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Entity-centric knowledge incorporation / knowledge-intensive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require access to entity-specific textual knowledge, improving accuracy where entity-level facts are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge-enhanced representation / language tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Entity-mention based memory provides a principled way to inject textual knowledge into transformers and improves the model's ability to use entity-specific facts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires reliable entity linking/mention identification and scalable memory access mechanisms; potential mismatch between mentions and stored entries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3227.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3227.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nonparametric MLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonparametric Masked Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A masked language modeling approach augmented with a nonparametric datastore that retrieves token-level evidence to improve masked token prediction and factual recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nonparametric Masked Language Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Nonparametric Masked Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A masked LM that uses an external nonparametric datastore to retrieve relevant contexts/tokens at prediction time to assist in masked token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>nonparametric retrieval (token-level datastore)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>During masked token prediction the model queries a datastore of contexts/tokens, retrieves nearest neighbors, and conditions predictions on aggregated retrieved information to improve accuracy and enable knowledge updates external to model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Masked language modeling / pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Masked token prediction tasks during pretraining or fine-tuning, emphasizing incorporation of external textual evidence to improve knowledge encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Augmenting masked LMs with nonparametric retrieval improves masked token prediction and allows knowledge to be updated without changing model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Datastore size and retrieval efficiency; integration strategy for retrieved context into masked prediction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3227.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3227.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory-Augmented Training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training Language Models with Memory Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods and training regimes for integrating external memory/datastores with LMs, including in-batch approximations and asynchronous datastore/index updates to make joint training feasible at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training Language Models with Memory Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-Augmented LM (training methods)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A set of training approaches that jointly or semi-jointly train retrieval components and LMs, using approximations (in-batch negatives) or asynchronous updates to handle very large datastores.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval datastore + training-time augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Describes scalable training recipes: train retrieval and LM separately (pipelined), use in-batch approximations as a proxy for full datastore during joint training, or update a full-corpus index asynchronously while training the model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language model pretraining and downstream adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improving LM capabilities by incorporating retrieval during training to better utilize external knowledge for downstream tasks like QA and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>training methodology / language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Jointly training retrieval and LMs is feasible using approximations or asynchronous index updates; careful design of negatives and update schedules affects effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Engineering complexity, computational cost, stale index issues, and difficulty scaling exact joint training to very large corpora.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3227.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3227.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest Neighbor Zero-Shot Inference (kNN-Prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot inference method that builds prompts from nearest-neighbor retrieved labeled examples, enabling LMs to perform tasks without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nearest Neighbor Zero-Shot Inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A technique that retrieves labeled examples nearest to an input and constructs a prompt from them so a pre-trained LM can perform the task in a zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented prompting (example datastore)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Queries a datastore of labeled examples, retrieves nearest neighbors for an input, and assembles them into the prompt provided to the LM; retrieval acts as an external memory to supply task-specific examples without model updates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot inference / diverse NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Performing tasks without fine-tuning by using retrieved examples as demonstration prompts to the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>zero-shot/few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using nearest-neighbor retrieved examples to form prompts improves zero-shot performance relative to naive prompting, enabling better task transfer without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires curated labeled datastore, sensitive to retrieval quality and prompt length constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3227.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3227.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive Semiparametric LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Semiparametric Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid language models that adaptively combine parametric predictions with nonparametric memory to allow efficient adaptation and improved recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaptive Semiparametric Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive Semiparametric LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Models that combine a parametric LM with a nonparametric memory component and adaptively weight or gate contributions from memory versus parameters depending on context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semiparametric memory (adaptive retrieval/interpolation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Interpolates or adaptively combines the model's parametric output with distributions or signals derived from a nonparametric datastore, potentially learning when to rely more on memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling and domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predictive modeling tasks requiring adaptation to new domains or data without full retraining, benefiting from memory that can be updated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semiparametric approaches allow better adaptation and recall in some settings compared to purely parametric models, offering parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Balancing contributions of parametric and nonparametric parts, and engineering efficient retrieval and integration.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3227.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3227.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Empirical Comparison Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical study analyzing when retrieval/nonparametric memory methods outperform parametric LMs and documenting limitations of both approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Parametric vs Nonparametric Memory comparison</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A comparative analysis exploring tradeoffs between large parametric LMs and retrieval-augmented nonparametric memories across tasks, focusing on effectiveness, parameter efficiency, and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>comparative: parametric vs retrieval-augmented nonparametric</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Analyzes models that store knowledge in parameters versus those that rely on an external datastore and retrieval, studying how each handles long-tail knowledge, updatability, and parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of tasks probing factual knowledge, long-tail recall, and adaptability to changing information to determine when retrieval-based memory helps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>empirical study / multiple tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-based (nonparametric) LMs can outperform parametric-only LMs with fewer parameters and enable knowledge updates, but both approaches have regimes where they fail or are suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval errors, domain mismatch between datastore and task, privacy/data leakage concerns, and tasks where parametric memorization remains competitive.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generalization through Memorization: Nearest Neighbor Language Models <em>(Rating: 2)</em></li>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Few-shot Learning with Retrieval Augmented Language Models <em>(Rating: 2)</em></li>
                <li>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention <em>(Rating: 2)</em></li>
                <li>Nonparametric Masked Language Modeling <em>(Rating: 2)</em></li>
                <li>Training Language Models with Memory Augmentation <em>(Rating: 2)</em></li>
                <li>Nearest Neighbor Zero-Shot Inference <em>(Rating: 2)</em></li>
                <li>Adaptive Semiparametric Language Models <em>(Rating: 2)</em></li>
                <li>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories <em>(Rating: 2)</em></li>
                <li>Retrieval augmented language model pre-training <em>(Rating: 1)</em></li>
                <li>Unsupervised Dense Information Retrieval with Contrastive Learning <em>(Rating: 1)</em></li>
                <li>kNN-Prompt: Nearest Neighbor Zero-Shot Inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3227",
    "paper_id": "paper-277dd00ab02f122133bf56b485dfb7c730acdcde",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "kNN-LM",
            "name_full": "Nearest Neighbor Language Model (kNN-LM)",
            "brief_description": "A language model augmented with a token-level nonparametric datastore that performs nearest-neighbor lookups at generation time to improve factual recall and long-tail generalization.",
            "citation_title": "Generalization through Memorization: Nearest Neighbor Language Models",
            "mention_or_use": "mention",
            "agent_name": "kNN-LM",
            "agent_description": "A decoder LM augmented with a token-level datastore of (context, next-token) entries; at each decoding step it retrieves nearest neighbor contexts and uses aggregated next-token statistics to interpolate with the model's predicted distribution.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (token-level nearest neighbors)",
            "memory_mechanism_description": "Stores representations keyed by context tokens; during generation the system performs nearest-neighbor search over the datastore, aggregates neighbor next-token counts/probabilities, and interpolates that distribution with the model softmax to produce the final next-token distribution; datastore can be updated independently of model weights.",
            "task_name": "Language modeling / next-token prediction",
            "task_description": "Predicting the next token given preceding context, with emphasis on factual recall, rare-token prediction, and long-tail knowledge.",
            "task_type": "language modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Nearest-neighbor retrieval over a token-level memory improves recall and generalization compared to pure parametric LMs, especially on rare or long-tail tokens and factual retrieval.",
            "limitations_or_challenges": "Dependence on retrieval quality, computational cost and scalability of nearest-neighbor search, integration/hyperparameter choices for interpolation.",
            "uuid": "e3227.0"
        },
        {
            "name_short": "RETRO",
            "name_full": "Retrieval-Enhanced Transformer / Improving language models by retrieving from trillions of tokens",
            "brief_description": "An autoregressive transformer that retrieves chunk-level passages from a very large external datastore and conditions generation on the retrieved chunks to reduce required model parameters and improve factuality.",
            "citation_title": "Improving language models by retrieving from trillions of tokens",
            "mention_or_use": "mention",
            "agent_name": "RETRO",
            "agent_description": "Autoregressive transformer that, for each context, retrieves nearest neighbor text chunks from a massive external corpus and integrates them into the transformer's computation (e.g., via cross-attention or concatenation) to inform generation.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (chunk-level external datastore)",
            "memory_mechanism_description": "Performs chunk-level retrieval from a very large corpus (potentially trillions of tokens) for the current context and feeds retrieved chunks into the model's layers so the LM can attend to external content during decoding; datastore is external and updatable.",
            "task_name": "Language modeling and knowledge-intensive generation",
            "task_description": "Next-token generation and knowledge-intensive generation tasks where external factual content improves accuracy and reduces reliance on parametric capacity.",
            "task_type": "language modeling / knowledge-intensive generation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Chunk-level retrieval from very large corpora allows models to rival or surpass larger parametric-only LMs while using fewer parameters and enables knowledge updates by changing the datastore.",
            "limitations_or_challenges": "Practical retrieval scalability at extreme corpus sizes, sensitivity to retrieval quality, and potential privacy or leakage concerns from the datastore.",
            "uuid": "e3227.1"
        },
        {
            "name_short": "Atlas",
            "name_full": "Atlas: Few-shot Learning with Retrieval Augmented Language Models",
            "brief_description": "A retrieval-augmented few-shot learning approach where the model conditions on retrieved examples or passages to improve few-shot generalization across tasks.",
            "citation_title": "Few-shot Learning with Retrieval Augmented Language Models",
            "mention_or_use": "mention",
            "agent_name": "Atlas",
            "agent_description": "A retrieval-augmented language model designed for few-shot learning that retrieves relevant examples or passages at inference to provide task-specific context for the model.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (example/passage datastore)",
            "memory_mechanism_description": "Retrieves task-relevant examples or passages from a datastore and concatenates or otherwise integrates them into the model input, enabling the LM to use retrieved context as implicit demonstrations without altering its parameters.",
            "task_name": "Few-shot learning / knowledge-intensive tasks",
            "task_description": "Performing new tasks from only a few examples by leveraging retrieved similar examples or contextual passages to guide generation and predictions.",
            "task_type": "few-shot learning / knowledge-intensive generation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Retrieval augmentation substantially helps few-shot performance by providing relevant context and examples, improving generalization with fewer parametric resources.",
            "limitations_or_challenges": "Requires effective retrieval and management of the example datastore; context-window limits for concatenating retrieved content.",
            "uuid": "e3227.2"
        },
        {
            "name_short": "Mention Memory",
            "name_full": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
            "brief_description": "An entity-centric memory mechanism that stores textual mentions keyed by entities and enables transformer models to attend to entity-specific stored text via mention-aware attention.",
            "citation_title": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
            "mention_or_use": "mention",
            "agent_name": "Mention Memory",
            "agent_description": "A transformer augmented with an entity-mention memory that provides dedicated attention to stored textual knowledge for specific entity mentions encountered in input.",
            "memory_used": true,
            "memory_type": "entity-mention memory (textual memory keyed by entities)",
            "memory_mechanism_description": "Stores textual snippets keyed by entity mentions; when an entity is mentioned the transformer can attend to the corresponding memory entries via specialized mention-attention mechanisms to incorporate entity-specific knowledge into representations.",
            "task_name": "Entity-centric knowledge incorporation / knowledge-intensive tasks",
            "task_description": "Tasks that require access to entity-specific textual knowledge, improving accuracy where entity-level facts are needed.",
            "task_type": "knowledge-enhanced representation / language tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Entity-mention based memory provides a principled way to inject textual knowledge into transformers and improves the model's ability to use entity-specific facts.",
            "limitations_or_challenges": "Requires reliable entity linking/mention identification and scalable memory access mechanisms; potential mismatch between mentions and stored entries.",
            "uuid": "e3227.3"
        },
        {
            "name_short": "Nonparametric MLM",
            "name_full": "Nonparametric Masked Language Model",
            "brief_description": "A masked language modeling approach augmented with a nonparametric datastore that retrieves token-level evidence to improve masked token prediction and factual recall.",
            "citation_title": "Nonparametric Masked Language Modeling",
            "mention_or_use": "mention",
            "agent_name": "Nonparametric Masked Language Model",
            "agent_description": "A masked LM that uses an external nonparametric datastore to retrieve relevant contexts/tokens at prediction time to assist in masked token prediction.",
            "memory_used": true,
            "memory_type": "nonparametric retrieval (token-level datastore)",
            "memory_mechanism_description": "During masked token prediction the model queries a datastore of contexts/tokens, retrieves nearest neighbors, and conditions predictions on aggregated retrieved information to improve accuracy and enable knowledge updates external to model parameters.",
            "task_name": "Masked language modeling / pretraining",
            "task_description": "Masked token prediction tasks during pretraining or fine-tuning, emphasizing incorporation of external textual evidence to improve knowledge encoding.",
            "task_type": "language modeling / pretraining",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Augmenting masked LMs with nonparametric retrieval improves masked token prediction and allows knowledge to be updated without changing model weights.",
            "limitations_or_challenges": "Datastore size and retrieval efficiency; integration strategy for retrieved context into masked prediction.",
            "uuid": "e3227.4"
        },
        {
            "name_short": "Memory-Augmented Training",
            "name_full": "Training Language Models with Memory Augmentation",
            "brief_description": "Methods and training regimes for integrating external memory/datastores with LMs, including in-batch approximations and asynchronous datastore/index updates to make joint training feasible at scale.",
            "citation_title": "Training Language Models with Memory Augmentation",
            "mention_or_use": "mention",
            "agent_name": "Memory-Augmented LM (training methods)",
            "agent_description": "A set of training approaches that jointly or semi-jointly train retrieval components and LMs, using approximations (in-batch negatives) or asynchronous updates to handle very large datastores.",
            "memory_used": true,
            "memory_type": "retrieval datastore + training-time augmentation",
            "memory_mechanism_description": "Describes scalable training recipes: train retrieval and LM separately (pipelined), use in-batch approximations as a proxy for full datastore during joint training, or update a full-corpus index asynchronously while training the model parameters.",
            "task_name": "Language model pretraining and downstream adaptation",
            "task_description": "Improving LM capabilities by incorporating retrieval during training to better utilize external knowledge for downstream tasks like QA and generation.",
            "task_type": "training methodology / language modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Jointly training retrieval and LMs is feasible using approximations or asynchronous index updates; careful design of negatives and update schedules affects effectiveness.",
            "limitations_or_challenges": "Engineering complexity, computational cost, stale index issues, and difficulty scaling exact joint training to very large corpora.",
            "uuid": "e3227.5"
        },
        {
            "name_short": "kNN-Prompt",
            "name_full": "Nearest Neighbor Zero-Shot Inference (kNN-Prompt)",
            "brief_description": "A zero-shot inference method that builds prompts from nearest-neighbor retrieved labeled examples, enabling LMs to perform tasks without fine-tuning.",
            "citation_title": "Nearest Neighbor Zero-Shot Inference",
            "mention_or_use": "mention",
            "agent_name": "kNN-Prompt",
            "agent_description": "A technique that retrieves labeled examples nearest to an input and constructs a prompt from them so a pre-trained LM can perform the task in a zero-shot manner.",
            "memory_used": true,
            "memory_type": "retrieval-augmented prompting (example datastore)",
            "memory_mechanism_description": "Queries a datastore of labeled examples, retrieves nearest neighbors for an input, and assembles them into the prompt provided to the LM; retrieval acts as an external memory to supply task-specific examples without model updates.",
            "task_name": "Zero-shot inference / diverse NLP tasks",
            "task_description": "Performing tasks without fine-tuning by using retrieved examples as demonstration prompts to the LM.",
            "task_type": "zero-shot/few-shot learning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Using nearest-neighbor retrieved examples to form prompts improves zero-shot performance relative to naive prompting, enabling better task transfer without parameter updates.",
            "limitations_or_challenges": "Requires curated labeled datastore, sensitive to retrieval quality and prompt length constraints.",
            "uuid": "e3227.6"
        },
        {
            "name_short": "Adaptive Semiparametric LM",
            "name_full": "Adaptive Semiparametric Language Models",
            "brief_description": "Hybrid language models that adaptively combine parametric predictions with nonparametric memory to allow efficient adaptation and improved recall.",
            "citation_title": "Adaptive Semiparametric Language Models",
            "mention_or_use": "mention",
            "agent_name": "Adaptive Semiparametric LM",
            "agent_description": "Models that combine a parametric LM with a nonparametric memory component and adaptively weight or gate contributions from memory versus parameters depending on context.",
            "memory_used": true,
            "memory_type": "semiparametric memory (adaptive retrieval/interpolation)",
            "memory_mechanism_description": "Interpolates or adaptively combines the model's parametric output with distributions or signals derived from a nonparametric datastore, potentially learning when to rely more on memory.",
            "task_name": "Language modeling and domain adaptation",
            "task_description": "Predictive modeling tasks requiring adaptation to new domains or data without full retraining, benefiting from memory that can be updated.",
            "task_type": "language modeling / adaptation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Semiparametric approaches allow better adaptation and recall in some settings compared to purely parametric models, offering parameter efficiency.",
            "limitations_or_challenges": "Balancing contributions of parametric and nonparametric parts, and engineering efficient retrieval and integration.",
            "uuid": "e3227.7"
        },
        {
            "name_short": "Empirical Comparison Study",
            "name_full": "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
            "brief_description": "An empirical study analyzing when retrieval/nonparametric memory methods outperform parametric LMs and documenting limitations of both approaches.",
            "citation_title": "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
            "mention_or_use": "mention",
            "agent_name": "Parametric vs Nonparametric Memory comparison",
            "agent_description": "A comparative analysis exploring tradeoffs between large parametric LMs and retrieval-augmented nonparametric memories across tasks, focusing on effectiveness, parameter efficiency, and limitations.",
            "memory_used": true,
            "memory_type": "comparative: parametric vs retrieval-augmented nonparametric",
            "memory_mechanism_description": "Analyzes models that store knowledge in parameters versus those that rely on an external datastore and retrieval, studying how each handles long-tail knowledge, updatability, and parameter efficiency.",
            "task_name": "Various knowledge-intensive NLP tasks",
            "task_description": "A suite of tasks probing factual knowledge, long-tail recall, and adaptability to changing information to determine when retrieval-based memory helps.",
            "task_type": "empirical study / multiple tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Retrieval-based (nonparametric) LMs can outperform parametric-only LMs with fewer parameters and enable knowledge updates, but both approaches have regimes where they fail or are suboptimal.",
            "limitations_or_challenges": "Retrieval errors, domain mismatch between datastore and task, privacy/data leakage concerns, and tasks where parametric memorization remains competitive.",
            "uuid": "e3227.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generalization through Memorization: Nearest Neighbor Language Models",
            "rating": 2
        },
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2
        },
        {
            "paper_title": "Few-shot Learning with Retrieval Augmented Language Models",
            "rating": 2
        },
        {
            "paper_title": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
            "rating": 2
        },
        {
            "paper_title": "Nonparametric Masked Language Modeling",
            "rating": 2
        },
        {
            "paper_title": "Training Language Models with Memory Augmentation",
            "rating": 2
        },
        {
            "paper_title": "Nearest Neighbor Zero-Shot Inference",
            "rating": 2
        },
        {
            "paper_title": "Adaptive Semiparametric Language Models",
            "rating": 2
        },
        {
            "paper_title": "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
            "rating": 2
        },
        {
            "paper_title": "Retrieval augmented language model pre-training",
            "rating": 1
        },
        {
            "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
            "rating": 1
        },
        {
            "paper_title": "kNN-Prompt: Nearest Neighbor Zero-Shot Inference",
            "rating": 1
        }
    ],
    "cost": 0.019621499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Tutorial Proposal: <br> Retrieval-based Language Models and Applications</h1>
<p>Akari Asai ${ }^{\dagger}$ Sewon Min ${ }^{\dagger}$ Zexuan Zhong ${ }^{\ddagger}$ Danqi Chen ${ }^{\ddagger}$<br>${ }^{\dagger}$ University of Washington ${ }^{\ddagger}$ Princeton University<br>{akari, sewon}@cs.washington.edu<br>{zzhong, danqic}@cs.princeton.edu</p>
<h2>1 Description</h2>
<p>Language models (LMs) such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) have shown impressive abilities in a range of natural language processing (NLP) tasks. However, relying solely on their parameters to encode a wealth of world knowledge requires a prohibitively large number of parameters and hence massive compute, and they often struggle to learn long-rail knowledge (Roberts et al., 2020; Kandpal et al., 2022; Mallen et al., 2022). Moreover, these parametric LMs are fundamentally incapable of adapting over time (De Cao et al., 2021; Lazaridou et al., 2021; Kasai et al., 2022), often hallucinate (Shuster et al., 2021), and may leak private data from the training corpus (Carlini et al., 2021). To overcome these limitations, there has been growing interest in retrieval-based LMs (Guu et al., 2020; Khandelwal et al., 2020; Borgeaud et al., 2022; Zhong et al., 2022; Izacard et al., 2022b; Min et al., 2022), which incorporate a non-parametric datastore (e.g., text chunks from an external corpus) with their parametric counterparts. Retrieval-based LMs can outperform LMs without retrieval by a large margin with much fewer parameters (Mallen et al., 2022), can update their knowledge by replacing their retrieval corpora (Izacard et al., 2022b), and provide citations for users to easily verify and evaluate the predictions (Menick et al., 2022; Bohnet et al., 2022).</p>
<p>Previously, retrieval and LMs have been studied mostly separately, and only recently researchers have integrated them and built systems in which retrieval and LMs interact more organically, and a number of retrieval-based LMs have been proposed due to growing interest. They differ in their neural architectures (e.g., the granularity of retrieval units, how to integrate retrieved information), learning algorithms, and different uses in downstream applications. In this tutorial, we aim to provide a
comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by first providing preliminaries covering the foundations of LM (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search methods widely used in neural retrieval systems; Karpukhin et al. 2020). We will then focus on recent progress in architectures, learning approaches, and applications of retrieval-based LMs.</p>
<p>A taxonomy of architectures We introduce a taxonomy of architectures of retrieval-based LMs based on a variety of dimensions. Retrieval-based LMs can be categorized by the granularity of retrieved units stored in the datastore: either 1) a chunk of text (Borgeaud et al., 2022; Izacard et al., 2022b), or 2) a token (Khandelwal et al., 2020; Zhong et al., 2022; Min et al., 2022), or 3) an entity mention (Fvry et al., 2020; de Jong et al., 2022). We also plan to cover techniques for refining data stores and improving similarity search (He et al., 2021; Alon et al., 2022). At the same time, retrieval-base LMs can be categorized based on how the retrieved information is integrated with the parametric encoder: 1) whether retrieved components are concatenated with the original input text (Lewis et al., 2020; Guu et al., 2020; Izacard et al., 2022b), 2) whether the retrieved components are latent and integrated into the intermediate layers of Transformers (de Jong et al., 2022; Fvry et al., 2020; Borgeaud et al., 2022), or 3) distribution of tokens from the retrieved components and the LMs are interpolated (Khandelwal et al., 2020; Zhong et al., 2022; Yogatama et al., 2021).</p>
<p>Scalable learning algorithms Then, we discuss the training approaches of retrieval-based LMs. Since a retrieval datastore is typically very large, how to train retrieval-based LMs effectively and efficiently remains challenging. We first discuss pipelined approaches that train retrieval components and LMs separately, either through large-</p>
<p>scale pre-training (Izacard et al., 2022a) or multitask instruction tuning (Asai et al., 2022). Several other works train retrieval-based LMs with a fixed retrieval module (Borgeaud et al., 2022; Yogatama et al., 2021). We then discuss joint training under reasonable resource requirements: either through in-batch approximations to a full datastore, or updating the datastore with updated parameters asynchronously. The former uses fractions of the full corpus that are carefully designed during joint training (Zhong et al., 2022; de Jong et al., 2022; Min et al., 2022). The latter, on the other hand, aims to use full corpus during training with asynchronous index update for every certain time steps (Izacard et al., 2022b; Guu et al., 2020).</p>
<p>Adaption to downstream tasks After discussing the basic building blocks of retrieval-based LMs, we show how retrieval-based LMs are adapted to downstream applications. We first briefly summarize the two approaches to adapt a model to a new task: zero-shot or few-shot prompting without any parameter updates (Shi et al., 2022; Wang et al., 2022), and fine-tuning on target task data (Lewis et al., 2020). We then discuss methods designed to build more powerful retrieval-based LMs for certain downstream tasks, such as dialogue (Shuster et al., 2021), semantic parsing (Pasupat et al., 2021), and machine translation (Khandelwal et al., 2021; Zheng et al., 2021).</p>
<p>Up to this point, our tutorial has mainly focused on retrieving and integrating English plain text. At this end, we will cover recent extensions of retrieval-based LMs beyond English text, including multilingual (Asai et al., 2021), multimodal (Chen et al., 2022; Yasunaga et al., 2022) and code (Parvez et al., 2021) retrieval. These works often extend dense retrieval models to enable retrieval between heterogeneous input spaces (e.g., cross-lingual, cross-modal) and have shown that referring retrieved knowledge leads to knowledgeintensive generation.</p>
<p>Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs. We conclude our tutorial by discussing several important questions and future directions, including (1) how we can further improve the scalability of retrievalbased LMs without sacrificing performance, (2) when retrieval-based LMs are particularly useful in the era of rapidly evolving LMs, and (3) what is necessary to enable applications of retrieval-based LMs for more diverse domains.</p>
<h2>2 Tutorial Outline</h2>
<ol>
<li>
<p>Introduction (15 minutes)</p>
</li>
<li>
<p>An overview of the tutorial</p>
</li>
<li>
<p>Why retrieval-based LMs?</p>
</li>
<li>
<p>Preliminaries (15 minutes)</p>
</li>
<li>
<p>Language models: Auto-regressive LMs vs. masked LMs</p>
</li>
<li>Dense retrieval methods</li>
<li>
<p>Approximate nearest neighbor search</p>
</li>
<li>
<p>Retrieval-based LMs: A taxonomy of architectures (40 minutes)</p>
</li>
<li>
<p>Granularity of datastore: tokens, entity mentions, and chunks of text</p>
</li>
<li>
<p>How retrieved information is integrated: incorporation in the input layer, intermediate layers, and the output layer</p>
</li>
<li>
<p>Retrieval-based LMs: Scalable learning algorithms (40 minutes)</p>
</li>
<li>
<p>Pipelined training</p>
</li>
<li>Training with In-batch approximations</li>
<li>
<p>Joint training of retrieval and LMs with asynchronous updates of corpus</p>
</li>
<li>
<p>Retrieval-based LMs: Downstream adaptations (40 minutes)</p>
</li>
<li>
<p>Adaptation methods: zero-shot/few-shot prompting and fine-tuning on downstream tasks</p>
</li>
<li>
<p>Downstream applications and task-specific modifications (e.g., dialogue, semantic parsing)</p>
</li>
<li>
<p>Extensions beyond English text (10 minutes)</p>
</li>
<li>
<p>Multilingual retrieval-based LMs</p>
</li>
<li>Multimodal retrieval-based LMs</li>
<li>
<p>Code generation</p>
</li>
<li>
<p>Demostration: An exercise to show retrievalaugmented LMs (10 minutes)</p>
</li>
<li>Conclusions and future directions (10 minutes)</li>
</ol>
<h2>3 Tutorial Information</h2>
<p>Type of the tutorial Cutting-edge.
Length This is a 3-hour tutorial.
Target audience The tutorial will be accessible to anyone who has a basic knowledge of machine learning and natural language processing. We think the topic will be of interest to both NLP researchers/students in academia and NLP practitioners in the industry.</p>
<p>Breadth We estimate that $20 \%$ of the work covered in this tutorial will be by the presenters and the remaining $80 \%$ by others. The papers we will cover are from both academia and industry.</p>
<p>Diversity considerations. The speakers are from two academic institutions with an affiliation with an industry research group, including both a professor and Ph.D. students. Three out of four speakers are female. The methods covered by our tutorials can scale up to various languages or domains, and we also briefly cover several papers focusing on multilingual and expert-domain extensions of the core frameworks. We will reach out to academic communities such as WiNLP ${ }^{1}$ and Masakhane ${ }^{2}$ to encourage them to attend our tutorial for participation of diverse audiences. Since retrieval-based LMs are alternatives to LMs with a significantly large number of parameters, we expect this tutorial to be especially useful to researchers with modest resources who do no have access to very large models.</p>
<p>An estimate of the audience size Given that language models are now used in a range of NLP tasks and retrieval-based approaches have been applied to diverse domains, we estimate that the number of audiences will be around $150+$.</p>
<p>Venues. We prefer ACL due to the growing interest in the area and the travel constraints of some of the speakers. EMNLP is our second preferred choice, and we currently do not consider EACL.</p>
<p>Technical equipment. We would like to have Internet access to show online demos.</p>
<p>Open access We plan to make all teaching material available online and agree to allow the publication of slides and video recordings in the ACL anthology.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Ethical considerations Retrieval-based LMs are often more powerful and parameter-efficient than LMs, and do not require full re-training to update world knowledge, which makes it more energyefficient and can reduce carbon footprints. Prior work also shows that referring to external world knowledge can reduce harmful biases and hallucinations, although retrieval-based LMs can still be plausible sounding but incorrect or non-sensical outputs. We note that, as retrieval-based LMs may retrieve raw data from a corpus, which can leak privacy-sensitive information, especially when they are built on top of a private corpus. We acknowledge this to caution those who manage to apply retrieval-based LMs to privacy-sensitive domains.</p>
<p>Pedagogical material We plan to do some short hands-on exercises to let the audience try different retrieval-based LMs with few-shot prompting using Colab.</p>
<h2>Past tutorials.</h2>
<ul>
<li>ACL 2020 tutorial on Open-domain QA (Chen and Yih, 2020): This tutorial provides comprehensive reviews of open-domain question answering, some of which consist of a retriever and a generative model, while we focus on the recent progress of architectures and learning algorithms of retrieval-based LMs for diverse NLP tasks, not limiting its focus to open-domain QA. Most of the papers will be discussed in this tutorial have been published since the Open-domain QA tutorial three years ago. Moreover, one of the instructors, Danqi was an instructor of this ACL 2020 tutorial.</li>
<li>SIGIR 2022 tutorial on Recent Advances in Retrieval-Augmented Text Generation (Cai et al., 2022): This tutorial focuses mainly on recent retrieval-augmented text generation approaches with a focus on two applications: dialogue and machine translation. Our tutorial puts more emphasis on the architecture and learning methods of retrieval-based LMs that can be applicable to diverse NLP tasks.</li>
</ul>
<h2>4 Presenters</h2>
<p>Akari Asai Akari Asai is a Ph.D. student in the Paul G. Allen School of Computer Science \&amp; Engineering at the University of Washington, advised by Prof. Hannaneh Hajishirzi. Her research lies</p>
<p>in natural language processing and machine learning. Her recent research focuses on question answering, retrieval-based LMs, multilingual NLP, and entity-aware representations. She received the IBM Fellowship in 2022. She is a lead organizer of the Workshop on Multilingual Information Access (NAACL 2022) and serves as an area chair in question answering at EACL 2023.</p>
<p>Sewon Min Sewon Min is a Ph.D. student in the Paul G. Allen School of Computer Science \&amp; Engineering at the University of Washington, and a visiting researcher at Meta AI. Her research spans question answering, representation and retrieval of factoid knowledge, and language modeling. She was a co-instructor and a co-organizer of multiple tutorials and workshops at ACL, NAACL-HLT, EMNLP, NeurIPS and AKBC, including a tutorial on Few-Shot NLP with Pretrained Language Models (ACL 2022), a tutorial on NLP for Long Sequences (NAACL-HLT 2021), and the Workshop on Semiparametric Methods in NLP (ACL 2022).</p>
<p>Zexuan Zhong Zexuan Zhong is a Ph.D. student in the Department of Computer Science at Princeton University, advised by Prof. Danqi Chen. His research interests lie in natural language processing and machine learning. His recent research focuses on retrieval-based LMs, generalization of retrieval models, and efficient models in NLP. He received a J.P. Morgan PhD Fellowship in 2022.</p>
<p>Danqi Chen Danqi Chen is an Assistant Professor of Computer Science at Princeton University and co-leads the Princeton NLP Group. Her recent research focuses on training, adapting, and understanding large LMs, and developing scalable and generalizable NLP systems for question answering, information extraction, and conversational agents. Danqi is a recipient of a Sloan Fellowship, a Samsung AI Researcher of the Year award, outstanding paper awards from ACL 2016, EMNLP 2017 and ACL 2022, and multiple industry faculty awards. Danqi served as the program chair for AKBC 2021 and (senior) area chairs for many *ACL conferences. She taught a tutorial on "Opendomain Question Answering" at ACL 2020.</p>
<h2>5 Reading List</h2>
<ul>
<li>Unsupervised Dense Information Retrieval with Contrastive Learning (Izacard et al., 2022a)</li>
<li>Task-aware Retrieval with Instructions (Asai et al., 2022)</li>
<li>Atlas: Few-shot Learning with Retrieval Augmented Language Models (Izacard et al., 2022b)</li>
<li>Improving language models by retrieving from trillions of tokens (Borgeaud et al., 2022)</li>
<li>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention (de Jong et al., 2022)</li>
<li>Generalization through Memorization: Nearest Neighbor Language Models (Khandelwal et al., 2020)</li>
<li>Nonparametric Masked Language Model (Min et al., 2022)</li>
<li>Training Language Models with Memory Augmentation (Zhong et al., 2022)</li>
<li>kNN-Prompt: Nearest Neighbor Zero-Shot Inference (Shi et al., 2022)</li>
<li>Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval (Alon et al., 2022)</li>
</ul>
<h2>References</h2>
<p>Uri Alon, Frank F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022. Neuro-symbolic language modeling with automatonaugmented retrieval. In International Conference on Machine Learning (ICML), Baltimore, USA.</p>
<p>Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with instructions. arXiv preprint arXiv:2211.09260.</p>
<p>Akari Asai, Xinyan Yu, Jungo Kasai, and Hanna Hajishirzi. 2021. One question answering model for many languages with cross-lingual dense passage retrieval. In Advances in Neural Information Processing Systems.</p>
<p>Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in neural information processing systems.</p>
<p>Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi. 2022. Recent advances in retrieval-augmented text generation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval.</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21).</p>
<p>Danqi Chen and Wen-tau Yih. 2020. Open-domain question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 34-37, Online. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W Cohen. 2022. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William W. Cohen. 2022. Mention memory: incorporating textual knowledge into transformers through entity mention attention. In International Conference on Learning Representations.</p>
<p>Thibault Fvry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International Conference on Machine Learning.</p>
<p>Junxian He, Graham Neubig, and Taylor BergKirkpatrick. 2021. Efficient nearest neighbor language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Punta Cana, Dominican Republic.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022b. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.</p>
<p>Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. arXiv preprint arXiv:2211.08411.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, and Kentaro Inui. 2022. Realtime qa: What's the answer right now? arXiv preprint arXiv:2207.13332.</p>
<p>Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest neighbor machine translation. In International Conference on Learning Representations.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations.</p>
<p>Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson dAutume, Tomas Kocisky, Sebastian Ruder, et al. 2021. Mind the gap: Assessing temporal generalization in neural language</p>
<p>models. Advances in Neural Information Processing Systems.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511.</p>
<p>Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, et al. 2022. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147.</p>
<p>Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Nonparametric masked language modeling. arXiv preprint arXiv:2212.01349.</p>
<p>Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2719-2734, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021. Controllable semantic parsing via retrieval augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. 2022. Nearest neighbor zero-shot inference. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021.</p>
<p>Zhenhailong Wang, Xiaoman Pan, Dian Yu, Dong Yu, Jianshu Chen, and Heng Ji. 2022. Zemi: Learning zero-shot semi-parametric language models from multiple tasks. arXiv preprint arXiv:2210.00185.</p>
<p>Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2022. Retrievalaugmented multimodal language modeling. arXiv preprint arXiv:2211.12561.</p>
<p>Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. 2021. Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics, 9:362-373.</p>
<p>Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing Chen, Weihua Luo, and Jiajun Chen. 2021. Adaptive nearest neighbor machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ http://www.winlp.org/
${ }^{2}$ https://www.masakhane.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
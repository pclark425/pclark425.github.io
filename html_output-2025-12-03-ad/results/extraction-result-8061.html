<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8061 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8061</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8061</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-53d212ea368e75cc152c3cb287343da22849915e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/53d212ea368e75cc152c3cb287343da22849915e" target="_blank">Large Language Models are Inconsistent and Biased Evaluators</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that LLMs are biased evaluators as they exhibit familiarity bias-a preference for text with lower perplexity, show skewed and biased distributions of ratings, and experience anchoring effects for multi-attribute judgments.</p>
                <p><strong>Paper Abstract:</strong> The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias-a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi-attribute judgments. We also found that LLMs are inconsistent evaluators, showing low"inter-sample"agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8061.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8061.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FamiliarityBias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Familiarity (perplexity) bias of LLM evaluators vs human experts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing that GPT-4 assigns higher scores to summaries with lower model perplexity relative to human expert ratings, indicating a preference for 'familiar' (low-perplexity) text that is stronger than that exhibited by human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models are Inconsistent and Biased Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization evaluation (per-dimension: Coherence, Consistency, Fluency, Relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (evaluations reported also for GPT-3.5 for some analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4 (chat-based models); perplexity computed with text-davinci-003 to measure familiarity</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (SummEval experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>mean perplexity per assigned rating (comparison of means between LLM-assigned rating groups and human-assigned rating groups)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>preference for low-perplexity text (familiarity bias); biased by source-document perplexity (source-driven bias)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4's average summary perplexities for high-rated summaries are lower than experts' averages and for low-rated summaries are higher, indicating an amplified preference for summaries that the model finds familiar; LLM ratings are also negatively correlated with source-document perplexity while human ratings show no such correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not emphasized for this comparison; familiarity bias is a limitation rather than an advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Summaries grouped by ratings (as assigned by GPT-4 or experts); perplexity computed with GPT-3 (text-davinci-003); Table 1 reports mean perplexities per rating (1-5) across four dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Inconsistent and Biased Evaluators', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8061.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8061.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granularity_vs_HumanCorr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of scoring granularity on correlation with human judgements</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative comparison of multiple LLM scoring scales (1-5, 1-5+modifiers, 1-10, 1-100 and sample-averaging variants) showing differing Kendall's τ correlations with human expert scores; 1-10 integer scoring gave the highest average agreement in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models are Inconsistent and Biased Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4 (chat-based); temperature tuned per method (some runs at 0, others averaged over samples at temperature 1.0)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (SummEval experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's τ correlation between LLM scores and human expert scores</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.428</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>score bias; underutilization of full numeric range; round-number/token bias (overprediction of round/round-like scores); sampling/temperature sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Increasing numeric granularity generally helps low-granularity settings, but extremely high granularity (1-100) leads to tokenization/round-number biases and sparse use of the range; 1-10 integer scoring produced the best average Kendall's τ (0.428) across dimensions in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Flexible zero-shot scoring, ability to produce fine-grained numeric outputs when configured appropriately; potential to increase effective granularity using sampling and averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multiple prompt instructions tested (1-5 integer, 1-5 with word modifiers, 1-5 with float modifiers, 1-10 integer, 1-100 integer); some methods used single-shot low-temperature decoding, others averaged N=10 samples at temperature=1.0 to approximate expected value; Kendall's τ computed per dimension and averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Inconsistent and Biased Evaluators', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8061.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8061.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnchoringEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anchoring (contextual carry-over) bias in multi-attribute LLM judgments vs human independence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When GPT-4 predicts multiple attributes in the same generation (Coherence, Consistency, Fluency, Relevance), earlier predicted scores strongly bias later scores (anchoring), with much higher inter-attribute correlation compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models are Inconsistent and Biased Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization multi-attribute evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4 (chat-based); single-generation multi-attribute prompting (Coherence->Consistency->Fluency->Relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (SummEval experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation between Coherence and Consistency ratings conditioned on prior score</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.979</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>excessive dependence of one predicted attribute on previously predicted attributes (anchoring); inflated inter-attribute correlation relative to humans</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Human Coherence-Consistency correlation reported as r=0.315, whereas GPT-4's corresponding correlation is r=0.979 when attributes are predicted in the same context — indicating strong anchoring caused by autoregressive generation that harms performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Ability to produce multi-attribute outputs in one pass (computationally cheaper), but this is shown to create bias.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompted GPT-4 to produce all four attribute scores in one generation; analyzed conditional distributions P(Consistency | Coherence) and computed Pearson r for human and GPT-4 score-pairs; appendix includes full pairwise plots.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Inconsistent and Biased Evaluators', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8061.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8061.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfInconsistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM self-consistency and inter-sample agreement vs human inter-annotator agreement (Krippendorff's α)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Krippendorff's α computed for human inter-annotator agreement and several GPT-4 consistency measures shows that GPT-4's inter-sample agreement is lower than human inter-annotator agreement on average, and GPT-4's judgments vary substantially across different prompting/settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models are Inconsistent and Biased Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization evaluation (Coherence, Consistency, Fluency, Relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4 (chat-based); multiple prompt variations (single-attribute vs multi-attribute, different score scales)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (SummEval experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's α (inter-annotator / inter-sample agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.587</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>lower inter-sample agreement than human annotators on average; sensitivity of judgments to prompt format and scoring scale (self-inconsistency across configurations)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Human inter-annotator α (average) = 0.659; GPT-4 inter-sample α (average) = 0.587. Additional self-consistency checks (single- vs multi-attribute prompt, different score scales) show substantial disagreement (e.g., α for 'Single vs Multi-attribute' average = 0.513; for 1-5 vs 1-100 comparison average = 0.439), indicating LLM judgments change with incidental configuration choices.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Noted advantages (speed, repeatability) tempered by demonstrated instability; paper suggests mitigation recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Krippendorff's α computed on SummEval labels; comparisons include: human inter-annotator agreement, GPT-4 inter-sample agreement (multiple sampled generations treated as separate 'annotators'), and pairwise agreement across prompting settings (single vs multi-attribute; different score scales).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Inconsistent and Biased Evaluators', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8061.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8061.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SourceDocSensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to presence/absence of source document (GPT-3.5-Turbo) compared to human expectations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation comparing GPT-3.5-Turbo performance with and without the source document shows substantial drops in correlation with human judgements when the source is removed, including for attributes that should be source-independent (e.g., Fluency).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models are Inconsistent and Biased Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-3.5-Turbo; ablation removing source document from prompt</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>expert annotators (SummEval experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's τ correlation with human judgments (reported per attribute)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.291</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>spuriously correlated feature use; decreased performance on relevance and other attributes when source removed; picking up correlated but spurious signals</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>With source included, average Kendall's τ across attributes = 0.291; excluding source reduced avg to 0.213 (Δ = -0.078, relative -26.7%); largest relative drops for Relevance (-44.6%) and Consistency (-33.2%). Fluency (which should be source-independent) also dropped, suggesting reliance on source-text cues even when irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>LLMs can use source context to improve judgments when it is appropriate, but they may over-rely on it leading to spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-3.5-Turbo evaluated with same prompts but either including or excluding the source document; Kendall's τ with human labels computed per attribute and aggregated; Table 5 reports values and percent changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Inconsistent and Biased Evaluators', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8061.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8061.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoSE_CaseStudy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoSE case study comparing improved recipe LLM evaluator to G-Eval and Chiang & Lee (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation on RoSE shows the authors' configured GPT-4-Turbo evaluator achieves statistically significant improvements over prior LLM-evaluator methods on certain partitions (CNNDM, SAMSum) measured by Kendall's τ against RoSE ACU labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models are Inconsistent and Biased Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization evaluation (ACU salience scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>RoSE (CNNDM, SAMSum, XSum partitions)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4-Turbo; non-Chain-of-Thought prompting, temperature=0, single output, 1-10 scoring, include ACU definition and evaluation steps</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>high-quality multi-stage annotators used by RoSE (expert-like, multi-stage annotation to derive ACU recall labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's τ between LLM scores and RoSE ACU labels</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.22</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>remaining gap to human-level agreement; variability across domains (in-domain CNNDM vs out-of-domain partitions); sensitivity to prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Their 'recipe' (1-10 scale, no CoT, temperature 0, include ACU instructions and evaluation steps) yields τ=0.220 on CNNDM (in-domain) and τ=0.308 on SAMSum; these improvements over G-Eval and Chiang & Lee are statistically significant on specific partitions but absolute τ values remain modest, indicating room for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Outperforms prior LLM-based evaluators on some partitions; fast, reproducible, and tunable via prompt/decoding choices.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Re-implementation of G-Eval and Chiang & Lee adapted to ChatCompletion format; compared all methods on GPT-4-Turbo without RoSE tuning; single-output, non-CoT, temperature 0 for authors' method; Kendall's τ computed per partition with bootstrap confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Inconsistent and Biased Evaluators', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>A closer look into using large language models for automatic evaluation <em>(Rating: 2)</em></li>
                <li>Large language models are not yet human-level evaluators for abstractive summarization <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8061",
    "paper_id": "paper-53d212ea368e75cc152c3cb287343da22849915e",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "FamiliarityBias",
            "name_full": "Familiarity (perplexity) bias of LLM evaluators vs human experts",
            "brief_description": "Comparison showing that GPT-4 assigns higher scores to summaries with lower model perplexity relative to human expert ratings, indicating a preference for 'familiar' (low-perplexity) text that is stronger than that exhibited by human experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
            "evaluation_task": "summarization evaluation (per-dimension: Coherence, Consistency, Fluency, Relevance)",
            "dataset_name": "SummEval",
            "judge_model_name": "GPT-4 (evaluations reported also for GPT-3.5 for some analyses)",
            "judge_model_details": "OpenAI GPT-4 (chat-based models); perplexity computed with text-davinci-003 to measure familiarity",
            "human_evaluator_type": "expert annotators (SummEval experts)",
            "agreement_metric": "mean perplexity per assigned rating (comparison of means between LLM-assigned rating groups and human-assigned rating groups)",
            "agreement_score": null,
            "reported_loss_aspects": "preference for low-perplexity text (familiarity bias); biased by source-document perplexity (source-driven bias)",
            "qualitative_findings": "GPT-4's average summary perplexities for high-rated summaries are lower than experts' averages and for low-rated summaries are higher, indicating an amplified preference for summaries that the model finds familiar; LLM ratings are also negatively correlated with source-document perplexity while human ratings show no such correlation.",
            "advantages_of_llm_judge": "Not emphasized for this comparison; familiarity bias is a limitation rather than an advantage.",
            "experimental_setting": "Summaries grouped by ratings (as assigned by GPT-4 or experts); perplexity computed with GPT-3 (text-davinci-003); Table 1 reports mean perplexities per rating (1-5) across four dimensions.",
            "uuid": "e8061.0",
            "source_info": {
                "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Granularity_vs_HumanCorr",
            "name_full": "Effect of scoring granularity on correlation with human judgements",
            "brief_description": "Quantitative comparison of multiple LLM scoring scales (1-5, 1-5+modifiers, 1-10, 1-100 and sample-averaging variants) showing differing Kendall's τ correlations with human expert scores; 1-10 integer scoring gave the highest average agreement in their experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
            "evaluation_task": "summarization evaluation",
            "dataset_name": "SummEval",
            "judge_model_name": "GPT-4",
            "judge_model_details": "OpenAI GPT-4 (chat-based); temperature tuned per method (some runs at 0, others averaged over samples at temperature 1.0)",
            "human_evaluator_type": "expert annotators (SummEval experts)",
            "agreement_metric": "Kendall's τ correlation between LLM scores and human expert scores",
            "agreement_score": 0.428,
            "reported_loss_aspects": "score bias; underutilization of full numeric range; round-number/token bias (overprediction of round/round-like scores); sampling/temperature sensitivity",
            "qualitative_findings": "Increasing numeric granularity generally helps low-granularity settings, but extremely high granularity (1-100) leads to tokenization/round-number biases and sparse use of the range; 1-10 integer scoring produced the best average Kendall's τ (0.428) across dimensions in Table 2.",
            "advantages_of_llm_judge": "Flexible zero-shot scoring, ability to produce fine-grained numeric outputs when configured appropriately; potential to increase effective granularity using sampling and averaging.",
            "experimental_setting": "Multiple prompt instructions tested (1-5 integer, 1-5 with word modifiers, 1-5 with float modifiers, 1-10 integer, 1-100 integer); some methods used single-shot low-temperature decoding, others averaged N=10 samples at temperature=1.0 to approximate expected value; Kendall's τ computed per dimension and averaged.",
            "uuid": "e8061.1",
            "source_info": {
                "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "AnchoringEffect",
            "name_full": "Anchoring (contextual carry-over) bias in multi-attribute LLM judgments vs human independence",
            "brief_description": "When GPT-4 predicts multiple attributes in the same generation (Coherence, Consistency, Fluency, Relevance), earlier predicted scores strongly bias later scores (anchoring), with much higher inter-attribute correlation compared to humans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
            "evaluation_task": "summarization multi-attribute evaluation",
            "dataset_name": "SummEval",
            "judge_model_name": "GPT-4",
            "judge_model_details": "OpenAI GPT-4 (chat-based); single-generation multi-attribute prompting (Coherence-&gt;Consistency-&gt;Fluency-&gt;Relevance)",
            "human_evaluator_type": "expert annotators (SummEval experts)",
            "agreement_metric": "Pearson correlation between Coherence and Consistency ratings conditioned on prior score",
            "agreement_score": 0.979,
            "reported_loss_aspects": "excessive dependence of one predicted attribute on previously predicted attributes (anchoring); inflated inter-attribute correlation relative to humans",
            "qualitative_findings": "Human Coherence-Consistency correlation reported as r=0.315, whereas GPT-4's corresponding correlation is r=0.979 when attributes are predicted in the same context — indicating strong anchoring caused by autoregressive generation that harms performance.",
            "advantages_of_llm_judge": "Ability to produce multi-attribute outputs in one pass (computationally cheaper), but this is shown to create bias.",
            "experimental_setting": "Prompted GPT-4 to produce all four attribute scores in one generation; analyzed conditional distributions P(Consistency | Coherence) and computed Pearson r for human and GPT-4 score-pairs; appendix includes full pairwise plots.",
            "uuid": "e8061.2",
            "source_info": {
                "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SelfInconsistency",
            "name_full": "LLM self-consistency and inter-sample agreement vs human inter-annotator agreement (Krippendorff's α)",
            "brief_description": "Krippendorff's α computed for human inter-annotator agreement and several GPT-4 consistency measures shows that GPT-4's inter-sample agreement is lower than human inter-annotator agreement on average, and GPT-4's judgments vary substantially across different prompting/settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
            "evaluation_task": "summarization evaluation (Coherence, Consistency, Fluency, Relevance)",
            "dataset_name": "SummEval",
            "judge_model_name": "GPT-4",
            "judge_model_details": "OpenAI GPT-4 (chat-based); multiple prompt variations (single-attribute vs multi-attribute, different score scales)",
            "human_evaluator_type": "expert annotators (SummEval experts)",
            "agreement_metric": "Krippendorff's α (inter-annotator / inter-sample agreement)",
            "agreement_score": 0.587,
            "reported_loss_aspects": "lower inter-sample agreement than human annotators on average; sensitivity of judgments to prompt format and scoring scale (self-inconsistency across configurations)",
            "qualitative_findings": "Human inter-annotator α (average) = 0.659; GPT-4 inter-sample α (average) = 0.587. Additional self-consistency checks (single- vs multi-attribute prompt, different score scales) show substantial disagreement (e.g., α for 'Single vs Multi-attribute' average = 0.513; for 1-5 vs 1-100 comparison average = 0.439), indicating LLM judgments change with incidental configuration choices.",
            "advantages_of_llm_judge": "Noted advantages (speed, repeatability) tempered by demonstrated instability; paper suggests mitigation recipes.",
            "experimental_setting": "Krippendorff's α computed on SummEval labels; comparisons include: human inter-annotator agreement, GPT-4 inter-sample agreement (multiple sampled generations treated as separate 'annotators'), and pairwise agreement across prompting settings (single vs multi-attribute; different score scales).",
            "uuid": "e8061.3",
            "source_info": {
                "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SourceDocSensitivity",
            "name_full": "Sensitivity to presence/absence of source document (GPT-3.5-Turbo) compared to human expectations",
            "brief_description": "Ablation comparing GPT-3.5-Turbo performance with and without the source document shows substantial drops in correlation with human judgements when the source is removed, including for attributes that should be source-independent (e.g., Fluency).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
            "evaluation_task": "summarization evaluation",
            "dataset_name": "SummEval",
            "judge_model_name": "GPT-3.5-Turbo",
            "judge_model_details": "OpenAI GPT-3.5-Turbo; ablation removing source document from prompt",
            "human_evaluator_type": "expert annotators (SummEval experts)",
            "agreement_metric": "Kendall's τ correlation with human judgments (reported per attribute)",
            "agreement_score": 0.291,
            "reported_loss_aspects": "spuriously correlated feature use; decreased performance on relevance and other attributes when source removed; picking up correlated but spurious signals",
            "qualitative_findings": "With source included, average Kendall's τ across attributes = 0.291; excluding source reduced avg to 0.213 (Δ = -0.078, relative -26.7%); largest relative drops for Relevance (-44.6%) and Consistency (-33.2%). Fluency (which should be source-independent) also dropped, suggesting reliance on source-text cues even when irrelevant.",
            "advantages_of_llm_judge": "LLMs can use source context to improve judgments when it is appropriate, but they may over-rely on it leading to spurious correlations.",
            "experimental_setting": "GPT-3.5-Turbo evaluated with same prompts but either including or excluding the source document; Kendall's τ with human labels computed per attribute and aggregated; Table 5 reports values and percent changes.",
            "uuid": "e8061.4",
            "source_info": {
                "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "RoSE_CaseStudy",
            "name_full": "RoSE case study comparing improved recipe LLM evaluator to G-Eval and Chiang & Lee (2023)",
            "brief_description": "Evaluation on RoSE shows the authors' configured GPT-4-Turbo evaluator achieves statistically significant improvements over prior LLM-evaluator methods on certain partitions (CNNDM, SAMSum) measured by Kendall's τ against RoSE ACU labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
            "evaluation_task": "summarization evaluation (ACU salience scoring)",
            "dataset_name": "RoSE (CNNDM, SAMSum, XSum partitions)",
            "judge_model_name": "GPT-4-Turbo",
            "judge_model_details": "OpenAI GPT-4-Turbo; non-Chain-of-Thought prompting, temperature=0, single output, 1-10 scoring, include ACU definition and evaluation steps",
            "human_evaluator_type": "high-quality multi-stage annotators used by RoSE (expert-like, multi-stage annotation to derive ACU recall labels)",
            "agreement_metric": "Kendall's τ between LLM scores and RoSE ACU labels",
            "agreement_score": 0.22,
            "reported_loss_aspects": "remaining gap to human-level agreement; variability across domains (in-domain CNNDM vs out-of-domain partitions); sensitivity to prompt engineering",
            "qualitative_findings": "Their 'recipe' (1-10 scale, no CoT, temperature 0, include ACU instructions and evaluation steps) yields τ=0.220 on CNNDM (in-domain) and τ=0.308 on SAMSum; these improvements over G-Eval and Chiang & Lee are statistically significant on specific partitions but absolute τ values remain modest, indicating room for improvement.",
            "advantages_of_llm_judge": "Outperforms prior LLM-based evaluators on some partitions; fast, reproducible, and tunable via prompt/decoding choices.",
            "experimental_setting": "Re-implementation of G-Eval and Chiang & Lee adapted to ChatCompletion format; compared all methods on GPT-4-Turbo without RoSE tuning; single-output, non-CoT, temperature 0 for authors' method; Kendall's τ computed per partition with bootstrap confidence intervals.",
            "uuid": "e8061.5",
            "source_info": {
                "paper_title": "Large Language Models are Inconsistent and Biased Evaluators",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "A closer look into using large language models for automatic evaluation",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not yet human-level evaluators for abstractive summarization",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 1
        }
    ],
    "cost": 0.01238125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Inconsistent and Biased Evaluators</h1>
<p>Rickard Stureborg ${ }^{1,2}$ Dimitris Alikaniotis ${ }^{1}$ Yoshi Suhara ${ }^{3, *}$<br>${ }^{1}$ Grammarly ${ }^{2}$ Duke University ${ }^{3}$ NVIDIA<br>rickard.stureborg@duke.edu<br>dimitrios.alikaniotis@grammarly.com<br>ysuhara@nvidia.com</p>
<h4>Abstract</h4>
<p>The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias-a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi-attribute judgments. We also found that LLMs are inconsistent evaluators, showing low "inter-sample" agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.</p>
<h2>1 Introduction</h2>
<p>The advancement of NLP research has relied much on automatic evaluation to conduct quantitative analysis by comparing proposed and existing solutions for shared problems. The use cases for automatic evaluation are extensive, but most famously text generation tasks such as text summarization and machine translation, with classic evaluation metrics, including the family of ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) scores, still widely in use today.</p>
<p>A core limitation of automatic evaluation is in developing new metrics and scaling them beyond limited benchmark datasets, primarily due to their common reliance on reference outputs. While there</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>is a line of work in reference-free automatic evaluation metrics, it is known that it is less reliable than the current reference-based metrics (Fabbri et al., 2021; Deutsch et al., 2022). Large Language Models (LLMs) have proven useful in this domain due to their demonstrated high natural language understanding abilities and performance at adhering to instructions. Furthermore, with the powerful zero-shot capability, LLMs do not require reference texts and can generate scores directly from the system output. This has led to great interest in developing LLM-based automatic evaluation metrics (Zheng et al., 2023b; Fu et al., 2023; Lin and Chen, 2023; Chiang and Lee, 2023; Chen et al., 2023; Wang et al., 2023a; Liu et al., 2023a; Gao et al., 2023; Shen et al., 2023; Luo et al., 2023; Chan et al., 2023); LLM evaluators (also known as LLM-as-a-judge) have become part of automatic evaluation for commonly used benchmarks for a variety of NLP tasks (Li et al., 2024; Huang et al., 2024) including LLM benchmarks such as MTBench (Zheng et al., 2023b).</p>
<p>However, little is known about the robustness of these LLM evaluators. A few studies have looked deeper into this point (Wang et al., 2023b; Zheng et al., 2023b; Liu et al., 2023c; Li et al., 2024); there is a need for further analysis into potential risks and failure points when using them, especially if used in sensitive applications. Therefore, in this paper, we aim to study two important characteristics of the LLM evaluator, namely bias and consistency, in order to understand and share the limitations of LLM evaluators. To this end, we conduct extensive experiments using GPT-3.5 and GPT-4, which are commonly used as LLM evaluators, with various prompts and generation configurations on the summarization evaluation benchmarks SummEval and RoSE datasets.</p>
<p>In this paper, we quantitatively analyze biases in LLM evaluators, while linking the biased behaviors with those of humans.</p>
<p>First, we use the perplexity as a familiarity metric and analyze the relationship between the average perplexity and each rating returned by the LLM evaluator. We show that the average perplexity shows a descending trend as the score increases. The results support that LLM evaluators have $f a$ miliarity bias (Zajonc, 1968)—LLM evaluators tend to develop a preference for texts simply because they are familiar with them. Second, we explore scoring granularity and report that LLM evaluators exhibit score biases, including round number bias Thomas and Morwitz (2009), assigning some scores more frequently than others. Third, we report that LLM evaluators experience anchoring effects (Tversky and Kahneman, 1974) when multiple labels are predicted in one output.</p>
<p>Then, we analyze the consistency of the LLM evaluator and show that LLM evaluators significantly change their judgments for different samples, demonstrating significantly lower inter-sample agreement than human experts' inter-annotator agreement. We also analyze LLM evaluators' inconsistent behaviors by changing the prompt configuration that should not affect the judgment.</p>
<p>Throughout analyzing these issues, we compiled findings into a set of recipes for LLM evaluators. We used the recipes to develop our new LLM evaluator and compared it with two existing LLM evaluators for text summarization. Experiment results on the RoSE dataset (Liu et al., 2023d) show that our new LLM evaluator statistically significantly improves upon the state-of-the-art.</p>
<h2>2 Methodology</h2>
<p>Analysis and results in this paper are the result of more than 560,000 generated outputs by LLMs.</p>
<h3>2.1 Datasets</h3>
<p>To investigate the performance of LLM-based evaluators, we test predictions on two main datasets. We use SummEval (Fabbri et al., 2021) as our development set, perform extensive analyses of LLM-based evaluators on this set, and then use RoSE (Liu et al., 2023d) as an evaluation set for our case study comparing our system with the current SOTA LLM evaluator for summarization.</p>
<h3>2.1.1 SummEval</h3>
<p>Introduced by Fabbri et al. (2021), SummEval is a dataset of human annotated evaluations for automatically produced summaries for CNN/Daily Mail news articles. The dataset annotates summaries on
four dimensions: Coherence (collective quality of sentences in the summary), Consistency (factual alignment with the source), Fluency (quality of the individual sentences), and Relevance (well-selected content). The dataset includes expert human judgments for 16 summaries produced by varying models on 100 articles over these four dimensions.</p>
<h3>2.1.2 RoSE</h3>
<p>RoSE (Liu et al., 2023d) is a benchmark of three datasets covering common summarization datasets: CNN/Daily Mail News articles (Nallapati et al., 2016), SAMSum dataset on chat dialogues (Gliwa et al., 2019), and XSum containing extremely short abstractive summaries of text documents (Narayan et al., 2018). Annotations for RoSE are done to record recall of "Atomic Content Units (ACU)", which is a recall-like metric measuring how many of the atomic facts displayed within an article were captured by the summary. We choose this benchmark due to its target labels very unlikely inclusion in any OpenAI model training given the time of its release, the high quality labels they achieve through a novel method for multi-stage annotation, and three domains to stress test our system on.</p>
<h3>2.1.3 Models</h3>
<p>We run our experiments in the analysis on a mix of GPT-3.5 (gpt-3.5-turbo-0301) and GPT-4 (gpt-4-0613). GPT-4 consistently outperforms GPT-3.5Turbo. For the eventual test evaluation reported in Section 4 on RoSE, we run previous work and our own approach using GPT-4-Turbo. Perplexity calculations are done using text-davinci-003 to match the LLM evaluator models as close as possible. We report our values against our own implementation of G-Eval to limit any potential differences in performance due to changes by OpenAI.</p>
<h3>2.1.4 Prompts</h3>
<p>Following Stureborg et al. (2024), we use a slight variations on a prompt derived from Liu et al. (2023b) to prompt LLMs for scores. The full prompt we use is shown in Figure 1 and Figure 8. This prompt takes five input strings: metric, metric_definition, aspects, article, and summary. We replace metric with a name describing what dimension of analysis to focus on. For SummEval, this is replaced with the string 'Coherence' to investigate the first label, for example. Further, metric_definition is replaced with a written explanation of what the metric is</p>
<table>
<thead>
<tr>
<th style="text-align: left;">You are the automatic summary evaluator</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">of a writing editor:</td>
</tr>
<tr>
<td style="text-align: left;">You consider an input document and a</td>
</tr>
<tr>
<td style="text-align: left;">corresponding summary</td>
</tr>
<tr>
<td style="text-align: left;">You evaluate the summary according to</td>
</tr>
<tr>
<td style="text-align: left;">one important quality:</td>
</tr>
<tr>
<td style="text-align: left;">1. {{metric}} (1-10) -</td>
</tr>
<tr>
<td style="text-align: left;">{{metric_definition}}</td>
</tr>
<tr>
<td style="text-align: left;">All ratings are between 1-10 where 1</td>
</tr>
<tr>
<td style="text-align: left;">is very poor and 10 is very good.</td>
</tr>
<tr>
<td style="text-align: left;">Your evaluation should be critical</td>
</tr>
<tr>
<td style="text-align: left;">and careful, and should closely</td>
</tr>
<tr>
<td style="text-align: left;">match the ratings of experts. This</td>
</tr>
<tr>
<td style="text-align: left;">evaluation is very important.</td>
</tr>
<tr>
<td style="text-align: left;">Consider these aspects when</td>
</tr>
<tr>
<td style="text-align: left;">evaluating:</td>
</tr>
<tr>
<td style="text-align: left;">{{aspects}}</td>
</tr>
<tr>
<td style="text-align: left;">The user will give you both the article</td>
</tr>
<tr>
<td style="text-align: left;">(document) and summary, and prompt</td>
</tr>
<tr>
<td style="text-align: left;">you to provide an evaluation.</td>
</tr>
<tr>
<td style="text-align: left;">Respond with your integer 1-10</td>
</tr>
<tr>
<td style="text-align: left;">score first, then a rationale.</td>
</tr>
<tr>
<td style="text-align: left;">Example:</td>
</tr>
</tbody>
</table>
<p>Figure 1: System text input for prompting chat-based LLMs to generate automatic evaluation scores in text summarization. This prompting strategy is generalized to allow for use of evaluating any metric(s) of interest, whether multiple or just one.
meant to indicate, while aspects explains some broader considerations that are helpful in assessing the quality of a summary on this dimension. Finally, article, and summary are replaced with the source document and summary for the models to make a prediction on.</p>
<h3>2.2 Evaluation Metrics</h3>
<p>The goal of automatic evaluation is to provide scores highly correlated with human judgments on the task at hand. In our work, we primarily measure this through Kendall’s $\tau$ correlation on scores produced for each label in SummEval (Coherence, Consistency, Fluency, Relevance), following the convention in other work on automatic evaluation of text summarization.</p>
<h2>3 Results and Analysis</h2>
<p>In this section, we perform extensive analysis into the performance of LLM evaluators, we uncover several issues of bias and inconsistency with these systems, and propose potential solutions.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Average perplexity for each rating by GPT- 4 and Experts. Summaries are grouped by evaluation scores (as assigned either by Experts or by GPT-4). GPT4 exhibits a disproportionate bias toward low perplexity summaries compared to expert annotators, demonstrating a familiarity bias.</p>
<h3>3.1 Familiarity Bias</h3>
<p>We investigate the bias models have toward low perplexity examples. Summaries are first grouped by evaluation scores (as assigned either by Experts or an LLM evaluator). This group of summaries is held separate for each dimension of analysis in SummEval. Perplexities are then computed with GPT-3 on the summary text, and a mean score is calculated for each group of summaries. Figure 2 shows that GPT-4 is disproportionately biased towards low perplexity summaries as compared with expert annotators. The mean perplexities of summaries assigned high scores (5s) are lower than that for expert raters, while mean perplexities of low assigned scores (1-3) are higher than expert raters.</p>
<p>Full results are reported in Table 1. We would like to note that LLM evaluators are even biased by the source document, as LLM evaluators' ratings are still negatively correlated with the average perplexity of source documents, for which human experts' ratings show no correlation. As system summaries in the SummEval are genearted by various summarization models and the perplexity of the summaries negatively correlates with the LLM evaluator's rating, we confirm that we can expand the notion of self-enhancement bias into familiarity bias.</p>
<h3>3.2 Scoring Granularity and Score Biases</h3>
<p>A common scale for scoring is 1-5 (Nemoto and Beglar, 2014). However, when producing scores for automatic evaluation, ties between candidate</p>
<table>
<thead>
<tr>
<th></th>
<th>Avg. perplexity of summary</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Avg. perplexity of source document</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT-4</td>
<td></td>
<td></td>
<td></td>
<td>Human experts</td>
<td></td>
<td></td>
<td></td>
<td>GPT-4</td>
<td></td>
<td></td>
<td></td>
<td>Human experts</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Rating</td>
<td>Coh</td>
<td>Con</td>
<td>Flu</td>
<td>Rel</td>
<td>Coh</td>
<td>Con</td>
<td>Flu</td>
<td>Rel</td>
<td>Coh</td>
<td>Con</td>
<td>Flu</td>
<td>Rel</td>
<td>Coh</td>
<td>Con</td>
<td>Flu</td>
<td>Rel</td>
</tr>
<tr>
<td>1</td>
<td>$-$</td>
<td>7.05</td>
<td>$-$</td>
<td>8.42</td>
<td>7.03</td>
<td>7.47</td>
<td>7.66</td>
<td>7.53</td>
<td>$-$</td>
<td>7.76</td>
<td>$-$</td>
<td>8.51</td>
<td>7.21</td>
<td>7.66</td>
<td>7.68</td>
<td>7.94</td>
</tr>
<tr>
<td>2</td>
<td>8.15</td>
<td>7.61</td>
<td>7.45</td>
<td>7.53</td>
<td>6.80</td>
<td>7.42</td>
<td>7.71</td>
<td>7.14</td>
<td>8.32</td>
<td>7.86</td>
<td>7.77</td>
<td>7.79</td>
<td>7.67</td>
<td>7.61</td>
<td>8.01</td>
<td>7.53</td>
</tr>
<tr>
<td>3</td>
<td>7.60</td>
<td>7.46</td>
<td>7.92</td>
<td>7.33</td>
<td>6.58</td>
<td>7.07</td>
<td>6.96</td>
<td>6.73</td>
<td>8.09</td>
<td>7.69</td>
<td>8.48</td>
<td>7.90</td>
<td>7.73</td>
<td>7.52</td>
<td>7.55</td>
<td>7.67</td>
</tr>
<tr>
<td>4</td>
<td>6.44</td>
<td>6.83</td>
<td>6.48</td>
<td>6.44</td>
<td>6.37</td>
<td>6.67</td>
<td>6.96</td>
<td>6.42</td>
<td>7.72</td>
<td>8.00</td>
<td>7.74</td>
<td>7.75</td>
<td>7.81</td>
<td>7.29</td>
<td>7.99</td>
<td>7.81</td>
</tr>
<tr>
<td>5</td>
<td>5.34</td>
<td>6.06</td>
<td>6.01</td>
<td>5.51</td>
<td>6.36</td>
<td>6.43</td>
<td>6.39</td>
<td>6.26</td>
<td>6.51</td>
<td>7.44</td>
<td>7.06</td>
<td>6.84</td>
<td>7.63</td>
<td>7.75</td>
<td>7.69</td>
<td>7.58</td>
</tr>
</tbody>
</table>
<p>Table 1: Average Perplexity of Summary and Source documents for each rating by GPT-4/Human experts.
examples are often undesirable. To reduce ties, we aim to increase scoring granularity: the distinct number of possible scores for candidate responses. We explore the following methods for increasing granularity:</p>
<ul>
<li>1-5 star: Resulting prediction when a model is instructed to provide an integer rating between 1-5 (inclusive).</li>
<li>1-5 + word modifier: Model is instructed to provide an integer rating between 1-5 along with a single word modifier indicating if it is 'strong' or 'weak'. For example, a summary may be rated as a " 3 ", "weak 5", or "strong 4". To map these ratings to a numerical value, we convert the 'strong' modifier to add 0.33 to the base rating, and 'weak' subtracts 0.33 (similar to grading scales).</li>
<li>1-5 + float modifier: this score is directly predicting the resulting numerical value from the word modifier. We instruct the model to predict values on a GPA scale (1.0, 1.33, 1.67, 2...).</li>
<li>1-10 score: instruct model to provide integer ratings between 1-10.</li>
<li>1-100 score: instruct model to provide integer ratings between 1-100.</li>
</ul>
<p>For each of these cases, we also consider methods of taking a sample average. In this approach, we produce N model responses ${ }^{1}$ and average the resulting scores to provide a final float value with a greater granularity without changing the prompt. This approach is similar to the approach outlined in G-Eval (Liu et al., 2023b), where each potential score is multiplied by its token probability to get an expected value score. Since OpenAI does not allow access to log probabilities of their top-end models we instead sample several times at a temperature of 1.0 , which approximates the expected value score and maintains an increased granularity.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Frequencies of each possible score as found in 64,000 predictions using the 1-100 scale. Models sparsely predict scores within the range. Frequencies of some scores, such as 90 and 95 , are far higher than 'odd' scores such as 92 or 19 , and much of the range is almost entirely ignored (1-60). Interestingly, 1-60 is a range often largely ignored in academic grading scales. This indicates an issue within instruction-following specific to automatic evaluation.</p>
<p>Figure 3 shows the distribution of scores produced when instructing GPT-3.5-Turbo and GPT-4 to rate summaries on a 1-100 scale. The scores in this distribution are not respected as intended, and the model assigns outsized probabilities to certain scores such as 90 and 95 . This reaffirms results by Zheng et al. (2023a) which found that multiplechoice selections by LLMs suffered from similar token biases, deteriorating performance. The full range is also not utilized, with predicted scores largely occurring between 70 and 100.</p>
<p>Figure 3 also shows that the score distribution has several peaks for round numbers such as 60 , 70, 80, 90 (Similarly for 75, 85, and 95), indicating that LLM evaluators also have round number bias like human. ${ }^{2}$</p>
<p>To verify which rating scales produce higher quality responses by LLM evaluator frameworks, we run a comparative analysis of the cases men-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>G</th>
<th>Coh</th>
<th>Con</th>
<th>Flu</th>
<th>Rel</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-5 star</td>
<td>5</td>
<td>.332</td>
<td>.362</td>
<td>.325</td>
<td>.337</td>
<td>.339</td>
</tr>
<tr>
<td>1-5 avg</td>
<td>41</td>
<td>.422</td>
<td>.370</td>
<td>.356</td>
<td>.439</td>
<td>.397</td>
</tr>
<tr>
<td>5 +word mod.</td>
<td>13</td>
<td>.361</td>
<td>.408</td>
<td>.345</td>
<td>.363</td>
<td>.369</td>
</tr>
<tr>
<td>5 +word (avg)</td>
<td>121</td>
<td>.394</td>
<td>.364</td>
<td>.316</td>
<td>.419</td>
<td>.373</td>
</tr>
<tr>
<td>5 +float mod.</td>
<td>13</td>
<td>.425</td>
<td>.453</td>
<td>.380</td>
<td>.395</td>
<td>.413</td>
</tr>
<tr>
<td>5 +float (avg)</td>
<td>121</td>
<td>.416</td>
<td>.378</td>
<td>.334</td>
<td>.438</td>
<td>.392</td>
</tr>
<tr>
<td>1-10 score</td>
<td>10</td>
<td>.450</td>
<td>.433</td>
<td>.366</td>
<td>.462</td>
<td>.428</td>
</tr>
<tr>
<td>1-10 avg</td>
<td>91</td>
<td>.424</td>
<td>.366</td>
<td>.332</td>
<td>.435</td>
<td>.389</td>
</tr>
<tr>
<td>1-100 score</td>
<td>100</td>
<td>.463</td>
<td>.423</td>
<td>.308</td>
<td>.339</td>
<td>.383</td>
</tr>
<tr>
<td>1-100 avg</td>
<td>991</td>
<td>.406</td>
<td>.351</td>
<td>.343</td>
<td>.414</td>
<td>.379</td>
</tr>
</tbody>
</table>
<p>Table 2: Correlation with human judgement for GPT-4 by method for increased granularity. “G” is the effective granularity (number of unique scores) possible within the given scale. Methods denoted “avg” are a 10-sample average run with temperature 1.0, while all other methods benefited from reducing temperature to 0. It seems that increasing granularity generally helps low-granularity methods, while high-granularity methods are harmed by increasing granularity. This may be due to the increase in temperature setting. Our results indicate that there may be diminishing returns of increasing scoring granularity.</p>
<p>tioned in §3.2. Table 2 shows performance of GPT-4 based evaluators on SummEval under the mentioned rating scales. The performance of 1-10 score performs best on average, with an average score of 0.428 Kendall’s $\tau$ across the labels in SummEval. This method also performs the best on relevance, at 0.462 Kendall’s $\tau$, while 1-100 scoring performs better on Coherence and the float modification method performs best on both Consistency and Fluency. Ultimately, increasing scoring granularity is shown to improve performance in our experiments, which should be carefully conducted for the risk of score bias and round number bias.</p>
<h3>3.3 Anchoring Effect in Multiple Judgments</h3>
<p>During evaluation of text, it is often helpful to describe several attributes regarding the text at the same time. For some tasks (such as hierarchical classification <em>Zhu et al. (2024)</em> or N-ary relation extraction <em>Cheung et al. (2023)</em>), the large set of target labels and long required contexts make separating annotation into independent generations infeasible; it is cheaper to predict all labels within the same output <em>Gao et al. (2023)</em>. We explore whether doing so is beneficial for the performance of the model, since it could be argued that this is similar to a multi-task setting where scores of one feature may help determine the correlation of others. However, conditioning on previously generated scores may bias generation on previous predictions in the context, thereby worsening performance.</p>
<p>We prompt GPT-4 to produce scores for Coherence, Consistency, Fluency and Relevance in a single generation (in that order). We then look at the distributions of, for example, Consistency given each predicted score on Coherence. Formally, we are interested in using our predictions to estimate the conditional probability:
$P(\textsc{Consistency}=X \mid \textsc{Coherence}=Y)$
We then plot the frequency of evaluated scores when the previous score was above or below 5 out of 10. Figure 4 shows one such plot, and the remainder of pairings are shown in Appendix C. We find that there is a disproportionate biasing effect from the model, where the mean score assigned to samples with a previous assigned score above 5 is substantially greater than the mean score assigned to samples with previous scores of 5, while these scores should not be so strongly correlated. In other words, LLM evaluators tend to overrely on this adjustment of its priors—experiencing an anchoring effect. This is unsurprising due to LLM’s auto-regressive generation, but points out the need to correct for such biases if utilizing multi-attribute predictions.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: (Top) Score distribution for consistency, conditioned on the previously assigned score for coherence when predicting both within the same context. (Bottom) Human-determined scores for consistency conditioned on what range the score fell into for coherence. Human scores are correlated by Pearson’s $r=0.315$, while GPT-4 scores are correlated by $r=0.979$. The above figures clearly show how previous scores bias the distribution of future scores in the generation. While such biasing is natural (and in part valid), the effect here is so large it harms performance.</p>
<p><sup>1</sup>It is unclear why the results should differ across each dimension, indicating another potential issue with LLM evaluation: hyper-parameters may not be stable across different labels.</p>
<p>| As seen in Figure 4, one source of poor performance for GPT-4 is that humans mostly rate summaries as highly consistent (4-5) while GPT-4 questions consistency very often, assigning relatively low scores. | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Scatter-plots of evaluated score versus expert judgements reveal that while many papers claim 0.40 τ is strong performance, the correlation with human judgements still needs substantial improvements. Even with correlation of over 0.40 Kendall's τ, we notice that any individual evaluation may lie within a very wide range as compared to the ground-truth labeled by experts. Note that the full range of 1-10 is underutilized again.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Performance of CoT and non-CoT prompting at varying Temperatures. Each prediction is computed by the average of 10 generations. Low temperatures are beneficial when making simple predictions, but higher temperatures (to a point) help improve performance when using Chain-of-Thought (CoT) prompting. This could be because of a more diverse set of explanations, leading to more unique features for prediction.</p>
<p>and simpler than the weighted average approach from Liu et al. (2023b). When using CoT, our results motivate drawing multiple samples while tuning temperature appropriately to maximize performance.</p>
<h3>3.6 Sensitivity to Source Document</h3>
<p>While the long-context abilities of LLMs allow predictions over more complex documents, we find that the model's use of the provided source document (the article being summarized) is questionable during automatic evaluation. The presence of this source document substantially affects ratings on fluency, which should be independent of the article text. The table below shows performance drops of LLM-based evaluation using GPT-3.5-Turbo when removing the Source document, although many of the categories which surely require the document to render a sensible judgement remain relatively high-performing. The LLM-evaluator may be picking up on spuriously correlated features when predicting its judgement, indicating a potentially problematic bias.</p>
<table>
<thead>
<tr>
<th>Source Doc</th>
<th>Coh</th>
<th>Con</th>
<th>Flu</th>
<th>Rel</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>Included</td>
<td>.346</td>
<td>.250</td>
<td>.237</td>
<td>.330</td>
<td>.291</td>
</tr>
<tr>
<td>Excluded</td>
<td>.291</td>
<td>.167</td>
<td>.212</td>
<td>.183</td>
<td>.213</td>
</tr>
<tr>
<td>Δ</td>
<td>-.055</td>
<td>-.083</td>
<td>-.025</td>
<td>-.147</td>
<td>-.078</td>
</tr>
<tr>
<td>%Δ</td>
<td>-15.9</td>
<td>-33.2</td>
<td>-10.6</td>
<td>-44.6</td>
<td>-26.7</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of GPT-3.5-Turbo with and without Source Document. Removing the source document (unsurprisingly) substantially reduces the performance of the automatic evaluator. However, this is also true for attributes that should not be dependent on the source document in the first place, such as Fluency. For categories such as relevance, making a prediction on the summary quality without the article should be impossible.</p>
<p>Overall performance drops by 27% (relative), heavily driven by a drop in performance on relevance. While relevance is a dimension of evaluation that depends entirely on the source documents match with the summary, GPT-3.5-Turbo is able to find features that may be correlated with the expert scoring.</p>
<h2>4 Case Study</h2>
<p>Using the lessons learned from SummEval in Section 3, we determine a few simple guidelines to significantly improve automatic evaluation with LLMs (see Table 6). We evaluate whether these guidelines improve performance by comparing to two previous works: G-Eval (Liu et al., 2023b) and a follow-up work by Chiang and Lee (2023). Chiang and Lee (2023) establish SOTA performance on SummEval, beating G-Eval's correlation with human</p>
<p>judgements on the dataset. However, some <em>Bhandari et al. (2020); Liu et al. (2023d)</em> have pointed out issues in these style of datasets, including that (1) expert ratings themselves include a lot of disagreement, (2) closed-source LLMs may have been trained on these well-established datasets, and (3) conclusions on these datasets don’t always hold for new systems.</p>
<p>For these reasons, we evaluate our system on RoSE, a summary evaluation dataset built carefully in a multi-stage process to maximize label quality and is unlikely to be included in GPT training data. RoSE’s target label is the metric <em>Atomic Content Units</em> (ACU) which is a normalized metric ranging from 0 to 1. Note that the CNNDM partition of the dataset is shared with SummEval, meaning that performance on this data is an in-domain test, while the other two partitions of RoSE serve as out-of-domain tests.</p>
<p>Implementation of Previous Work <em>Chiang and Lee (2023)</em> point out issues in replicating the reported correlation values from the G-Eval paper. Therefore, we compare with these works by re-implementing their systems using the descriptions in their methods and released code, and compute all correlation values from scratch. Both <em>Chiang and Lee (2023)</em> and G-Eval were approaches designed for OpenAI’s Completions API endpoint, as opposed to a ChatCompletion end-point, which is more limited in formatting and has no access to token probabilities. We map the prompts into a Chat format by simply placing them into the user prompt. For G-Eval, we sample 10 times and average the score to approximate their expected value calculation (which was done by multiplying token probabilities extracted from the model). We use auto-CoT as specified, but notice that this causes a higher proportion of “failed” generations which give texts but omit any final, parseable score. <em>Chiang and Lee (2023)</em> suggest not including auto-CoT or any evaluation steps in their approach. For our method, we include the evaluation steps undergone by annotators for the ACU metric. This text is taken directly from <em>Liu et al. (2023d)</em> with edits only for grammar and conciseness. Finally, we use the rate-explain setting they describe since it is one of their two best settings. They state rate-explain and analyze-rate are “do not see rate-explain to be significantly better (or worse) than analyze-rate”. While the authors don’t point this out, rate-explain is much cheaper and faster for generation given you can safely stop generation after the rating has been produced.</p>
<p>We compare all methods on GPT-4-Turbo. Our method, as determined by insights from Section 3, relies on a 1-10 scoring granularity and includes both evaluation steps and a definition of ACU (which is copy pasted from <em>Liu et al. (2023d)</em> and also added to other two approaches). We use non-CoT prompting at a temperature of 0, and generate a single output. Table 6 summarizes these approaches. None of these parameters are tuned on RoSE. While each solution in Table 6 might look commonly used techniques, to the best of our knowledge, none of existing work has combined them into a single recipe and conduct an empirical study to verify the effectiveness of the techniques.</p>
<p>Results Our method outperforms both <em>G-Eval</em> and <em>rate-explain</em> on the CNNDM and SAMSum partitions. The performance of our method achieves Kendall’s $\tau=0.220$ on the in-domain test set, and $\tau=0.308$ on SAMSum, indicating this partition may be easier to evaluate. While we outperform <em>Chiang and Lee (2023)</em> on SAMSum, the difference is not statistically significant. This significant variation in performance is due to prompting strategies, indicating a lot of room for performance improvements by closer studies in prompt engineering.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Performance Comparison on the RoSE benchmark. Our approach performs statistically significantly better than the SOTA LLM-evaluator for summarization <em>Chiang and Lee (2023)</em> on the CNNDM dataset partition, and significantly better than G-Eval on both CNNDM and SAMSum. Confidence intervals are computed through bootstrap sampling.</p>
<table>
<thead>
<tr>
<th>Issue w/ LLM evaluators</th>
<th>Reasonable Approach to Mitigate</th>
</tr>
</thead>
<tbody>
<tr>
<td>Low granularity for distinguishing summaries</td>
<td>Widen scores to 1-10 star scale</td>
</tr>
<tr>
<td>CoT prompting requires tuning temperature</td>
<td>Remove CoT and set temperature to 0</td>
</tr>
<tr>
<td>Removing source document impacts performance</td>
<td>Keep source even for attributes which don’t require it</td>
</tr>
<tr>
<td>Multi-attribute labels are highly correlated</td>
<td>Predict only one attribute per generation</td>
</tr>
</tbody>
</table>
<p>Table 6: Identified issues have immediate and actionable mitigations</p>
<h2>5 Related Work</h2>
<p>Automatic evaluation has been dependent on human annotations. Traditional automatic evaluation metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) consider token-level n-gram matching between system outputs and reference texts. Later, embedding-based automatic evaluation such as BERTScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020), and MoverScore (Zhao et al., 2019) were developed to take the semantic similarity into account. Extensive efforts to remove the reliance on manually written reference texts have been attempted by creating reference-free automatic evaluation metrics (Louis and Nenkova, 2013; Fonseca et al., 2019; Scialom et al., 2019, 2021; Vasilyev et al., 2020; Rei et al., 2021). However, Deutsch et al. (2022) have pointed out the current limitations as the measures of how well models perform a task.</p>
<p>Following the line of work, recent studies in LLM evaluators have shown that LLMs can be high-quality evaluators for various NLP tasks (Fu et al., 2023) including Summarization (Chen et al., 2023; Wang et al., 2023a; Liu et al., 2023a; Gao et al., 2023; Shen et al., 2023; Wu et al., 2023), Machine Translation (Kocmi and Federmann, 2023), Factual Consistency Evaluation (Luo et al., 2023), and other text generation tasks (Chen et al., 2023; Wang et al., 2023a; Chan et al., 2023; Kasner and Dušek, 2024).</p>
<p>However, they have primarily focused on improvements through prompt engineering. Among them, only a few studies have tried to reveal the limitations of LLM evaluators. They have reported that LLM evaluators have position bias-a preference for the first example of a pairwise comparison (Wang et al., 2023b; Zheng et al., 2023b); verbosity bias-preference for longer texts (Zheng et al., 2023b; Wu and Aji, 2023); and self-enhancement bias-LLM evaluators prefer text generated by themselves (Zheng et al., 2023b; Panickssery et al., 2024). Koo et al. (2023) have re-
ported cognitive biases in LLM evaluators. Following the studies, our paper aims to dig deeper to share quantitative analysis on these points and beyond. Our work partially overlaps with the recent work by Ohi et al. (2024), who studies likelihood bias in LLM evaluators across data-to-text and grammatical error correction tasks. However, our work differs in that we use a different metric (i.e., perplexity) to assess the bias and focus on a different target task (i.e., summarization), providing a new perspective on this issue.</p>
<h2>6 Conclusion</h2>
<p>We have provided a series of analyses into biased and inconsistent behaviors exhibited by LLM evaluators for the task of text summarization. Our findings show that (1) LLM evaluators are disproportionately biased towards low perplexity summaries than is helpful (familiarity bias), (2) they fail to respect scoring scales given to them when attempting to increase the granularity of scores (score bias), (3) they show degradation in multi-attribute judgment, being influenced by their previous ratings (anchoring effect). They are inconsistent their own judgements depending on settings such as inclusion of source documents.</p>
<p>In attempts to solve some of these issues, we share a recipe to mitigate these issues and show that we are able to significantly outperform the current SOTA method for LLM-based summary evaluation on the CNNDM partition of RoSE 90\% confidence. Our work suggests that more effort should be allocated towards understanding and remedying the issues exhibited by LLM evaluators.</p>
<h2>Limitations</h2>
<p>Reliance on GPT-based models. We experiment primarily on GPT-based, proprietary models from OpenAI due to their SOTA performance on automatic evaluation of text summarization. However, this means it is unclear how well our results generalize to other LLMs such as Llama-2, Vicuna, Alpaca, etc. Do to constraints in time and budget,</p>
<p>extending the analysis to investigate other LLMs was not possible during the time this work was carried out. This project involved generating more than 560,000 outputs from OpenAI models; repeating the experiments on several models amounts to substantial effort and resources. Future work could aim to replicate and extend our analysis to further models.</p>
<p>Reliance on SummEval for analysis. Our analysis section primarily investigates issues by measuring performance of various model and prompt configurations against SummEval. There is a risk that our results to do generalize well beyond For this reason, we also sought to measure performance on the RoSE benchmark, which is comprised of three datasets in different domains. We find that addressing the issuess seen in SummEval significantly improves performance on one of the domains, and has insignificant but positive results on the other domains.</p>
<p>Limited solutions. Although we investigate solutions to some of the identified issues in this paper, many remain to be studied and may provide the research community with directions for future research efforts. LLM's inconsistencies and biases as automatic evaluators is tough to build solutions around. There is ample opportunity for creative solutions, and while our work offers some, its main focus is in identifying the existing issues in the first place.</p>
<h2>Ethics Statement</h2>
<p>As this study focuses on text summarization and uses publicly available datasets, we do not see any clear ethical implications or considerations. We adhere to ethical research practices.</p>
<h2>References</h2>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online. Association for Computational Linguistics.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate.</p>
<p>Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study.</p>
<p>Jerry Junyang Cheung, Yuchen Zhuang, Yinghao Li, Pranav Shetty, Wantian Zhao, Sanjeev Grampurohit, Rampi Ramprasad, and Chao Zhang. 2023. Polyie: A dataset of information extraction from polymer material scientific literature.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. A closer look into using large language models for automatic evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 89288942, Singapore. Association for Computational Linguistics.</p>
<p>Nikolas Coupland. 2011. How frequent are numbers? Language \&amp; Communication, 31(1):27-37.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. 2022. On the limitations of reference-free evaluations of generated text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10960-10977, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Erick Fonseca, Lisa Yankovskaya, André F. T. Martins, Mark Fishel, and Christian Federmann. 2019. Findings of the WMT 2019 shared tasks on quality estimation. In Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), pages 1-10, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.</p>
<p>Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt.</p>
<p>Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. Association for Computational Linguistics.</p>
<p>Hidehito Honda, Rina Kagawa, and Masaru Shirasuna. 2022. On the round number bias and wisdom of crowds in different response formats for numerical estimation. Scientific Reports, 12(1):8167.</p>
<p>Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024. An empirical study of llm-as-ajudge for llm evaluation: Fine-tuned judge models are task-specific classifiers.</p>
<p>Zdeněk Kasner and Ondřej Dušek. 2024. Beyond reference-based metrics: Analyzing behaviors of open llms on data-to-text generation.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality.</p>
<p>Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023. Benchmarking cognitive biases in large language models as evaluators.</p>
<p>Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, and Chongyang Tao. 2024. Leveraging large language models for nlg evaluation: A survey.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yen-Ting Lin and Yun-Nung Chen. 2023. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. 2023c. LLMs as narcissistic evaluators: When ego inflates evaluation scores.</p>
<p>Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2023d. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4140-4170, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard. Computational Linguistics, 39(2):267300 .</p>
<p>Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for text summarization.</p>
<p>Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-tosequence rnns and beyond.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.</p>
<p>Tomoko Nemoto and David Beglar. 2014. Likert-scale questionnaires. In JALT 2013 conference proceedings, pages 1-8.</p>
<p>Masanari Ohi, Masahiro Kaneko, Ryuto Koike, Mengsay Loem, and Naoaki Okazaki. 2024. Likelihood-based mitigation of evaluation bias in large language models.</p>
<p>Arjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. 2021. Are references really needed? unbabel-IST 2021 submission for the metrics shared task. In Proceedings of the Sixth Conference on Machine Translation, pages 1030-1040, Online. Association for Computational Linguistics.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3246-3256, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. 2023. Large language models are not yet human-level evaluators for abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4215-4233, Singapore. Association for Computational Linguistics.</p>
<p>Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. 2024. Characterizing the confidence of large language model-based automatic evaluation metrics. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 76-89, St. Julian's, Malta. Association for Computational Linguistics.</p>
<p>Manoj Thomas and Vicki Morwitz. 2009. Heuristics in numerical cognition: Implications for pricing. In Handbook of pricing research in marketing, pages 132-149. Edward Elgar Publishing.</p>
<p>Amos Tversky and Daniel Kahneman. 1974. Judgment under uncertainty: Heuristics and biases. Science, 185(4157):1124-1131.</p>
<p>Oleg Vasilyev, Vedant Dharnidharka, and John Bohannon. 2020. Fill in the BLANC: Human-free quality estimation of document summaries. In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 11-20, Online. Association for Computational Linguistics.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.</p>
<p>Minghao Wu and Alham Fikri Aji. 2023. Style over substance: Evaluation biases for large language models.</p>
<p>Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. 2023. Less is more for long document summary evaluation by llms.</p>
<p>Robert B Zajonc. 1968. Attitudinal effects of mere exposure. Journal of personality and social psychology, 9(2p2):1.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2020. BertScore: Evaluating text generation with bert.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023a. Large language models are not robust multiple choice selectors.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena.</p>
<p>Chloe Qinyu Zhu, Rickard Stureborg, and Bhuwan Dhingra. 2024. Hierarchical multi-label classification of online vaccine concerns.</p>
<h1>A Our Method for Prompting LLM Evaluation</h1>
<div class="codehilite"><pre><span></span><code><span class="x">Document:</span>
<span class="cp">{{</span><span class="nv">article</span><span class="cp">}}</span>
<span class="x">Summary:</span>
<span class="cp">{{</span><span class="nv">summary</span><span class="cp">}}</span>
<span class="x">Evaluation Form (Scores ONLY):</span>
<span class="cp">{{</span><span class="nv">metric</span><span class="cp">}}</span><span class="x">:</span>
</code></pre></div>

<p>Figure 8: User text input, used in conjunction with the system prompt in Figure 1. The next immediate token is expected to be within the range of 1 to 10 , but oftentimes the models will output restatements of the metric name or other content first. In general, we find it is safe to stop the model generation after 10-20 tokens and parse this output using regex to find the first digit.</p>
<h2>System Prompt:</h2>
<div class="codehilite"><pre><span></span><code><span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">automatic</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">evaluator</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">writing</span><span class="w"> </span><span class="nv">editor</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">You</span><span class="w"> </span><span class="nv">consider</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">input</span><span class="w"> </span><span class="nv">document</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">corresponding</span><span class="w"> </span><span class="nv">summary</span>
<span class="o">-</span><span class="w"> </span><span class="nv">You</span><span class="w"> </span><span class="nv">evaluate</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">according</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">one</span><span class="w"> </span><span class="nv">important</span><span class="w"> </span><span class="nv">quality</span>:
<span class="w">    </span><span class="mi">1</span>.<span class="w"> </span><span class="nv">ACU</span><span class="w"> </span><span class="nv">Salience</span><span class="w"> </span><span class="ss">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">10</span><span class="ss">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">desired</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">quality</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">requires</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">summary</span>
<span class="w">        </span><span class="nv">to</span><span class="w"> </span><span class="k">include</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">only</span><span class="w"> </span><span class="nv">important</span><span class="w"> </span><span class="nv">information</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">input</span><span class="w"> </span><span class="nv">article</span>.
<span class="w">        </span><span class="nv">Salience</span><span class="w"> </span><span class="nv">can</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">determined</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">dissecting</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">summaries</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">fine</span><span class="o">-</span><span class="nv">grained</span>
<span class="w">        </span><span class="nv">content</span><span class="w"> </span><span class="nv">units</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">defining</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">annotation</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nv">based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">those</span><span class="w"> </span><span class="nv">units</span>.
<span class="w">        </span><span class="nv">Specifically</span>,<span class="w"> </span><span class="nv">we</span><span class="w"> </span><span class="nv">introduce</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">Atomic</span><span class="w"> </span><span class="nv">Content</span><span class="w"> </span><span class="nv">Unit</span><span class="w"> </span><span class="ss">(</span><span class="nv">ACU</span><span class="ss">)</span>,<span class="w"> </span><span class="nv">elementary</span>
<span class="w">        </span><span class="nv">information</span><span class="w"> </span><span class="nv">units</span><span class="w"> </span><span class="nv">which</span><span class="w"> </span><span class="nv">no</span><span class="w"> </span><span class="nv">longer</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">further</span><span class="w"> </span><span class="nv">split</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span>
<span class="w">        </span><span class="nv">purpose</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">reducing</span><span class="w"> </span><span class="nv">ambiguity</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">human</span><span class="w"> </span><span class="nv">evaluation</span>.<span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">evaluation</span><span class="w"> </span><span class="nv">process</span>
<span class="w">        </span><span class="nv">is</span><span class="w"> </span><span class="nv">decomposed</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">extracting</span><span class="w"> </span><span class="nv">facts</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">one</span><span class="w"> </span><span class="nv">text</span><span class="w"> </span><span class="nv">sequence</span>,<span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">checking</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">presence</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">extracted</span><span class="w"> </span><span class="nv">facts</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">another</span><span class="w"> </span><span class="nv">sequence</span>.
<span class="o">-</span><span class="w"> </span><span class="nv">All</span><span class="w"> </span><span class="nv">ratings</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">between</span><span class="w"> </span><span class="mi">1</span><span class="o">-</span><span class="mi">10</span><span class="w"> </span><span class="nv">where</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">very</span><span class="w"> </span><span class="nv">poor</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">very</span><span class="w"> </span><span class="nv">good</span>.
<span class="o">-</span><span class="w"> </span><span class="nv">Your</span><span class="w"> </span><span class="nv">evaluation</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">critical</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">careful</span>,<span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">closely</span><span class="w"> </span><span class="nv">match</span><span class="w"> </span><span class="nv">the</span>
<span class="w">    </span><span class="nv">ratings</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">experts</span>.<span class="w"> </span><span class="nv">This</span><span class="w"> </span><span class="nv">evaluation</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">very</span><span class="w"> </span><span class="nv">important</span>.
<span class="o">-</span><span class="w"> </span><span class="nv">Consider</span><span class="w"> </span><span class="nv">these</span><span class="w"> </span><span class="nv">aspects</span><span class="w"> </span><span class="nv">when</span><span class="w"> </span><span class="nv">evaluating</span>:
<span class="w">    </span><span class="mi">1</span>.<span class="w"> </span><span class="nv">ACU</span><span class="w"> </span><span class="nv">Writing</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">Read</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">document</span><span class="w"> </span><span class="nv">carefully</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">identify</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">Atomic</span><span class="w"> </span><span class="nv">Content</span>
<span class="w">        </span><span class="nv">Units</span><span class="w"> </span><span class="ss">(</span><span class="nv">ACUs</span><span class="ss">)</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">facts</span>.
<span class="w">    </span><span class="mi">2</span>.<span class="w"> </span><span class="nv">ACU</span><span class="w"> </span><span class="nv">Matching</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">Read</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">compare</span><span class="w"> </span><span class="nv">it</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">list</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">ACUs</span>.<span class="w"> </span><span class="nv">Check</span>
<span class="w">        </span><span class="nv">what</span><span class="w"> </span><span class="nv">proportion</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">extracted</span><span class="w"> </span><span class="nv">ACUs</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">correctly</span><span class="w"> </span><span class="nv">covers</span>.
<span class="w">    </span><span class="mi">3</span>.<span class="w"> </span><span class="nv">Assign</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">score</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">ACU</span><span class="w"> </span><span class="nv">Salience</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">scale</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="mi">10</span>,<span class="w"> </span><span class="nv">where</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span>
<span class="w">        </span><span class="nv">lowest</span><span class="w"> </span><span class="ss">(</span><span class="nv">covers</span><span class="w"> </span><span class="nv">very</span><span class="w"> </span><span class="nv">few</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">ACUs</span><span class="ss">)</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">highest</span><span class="w"> </span><span class="ss">(</span><span class="nv">covers</span><span class="w"> </span><span class="nv">all</span>
<span class="w">        </span><span class="nv">important</span><span class="w"> </span><span class="nv">ACUs</span><span class="ss">)</span><span class="w"> </span><span class="nv">based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">Evaluation</span><span class="w"> </span><span class="nv">Criteria</span>.
<span class="nv">The</span><span class="w"> </span><span class="nv">user</span><span class="w"> </span><span class="nv">will</span><span class="w"> </span><span class="nv">give</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">both</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">article</span><span class="w"> </span><span class="ss">(</span><span class="nv">document</span><span class="ss">)</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">summary</span>,<span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">prompt</span><span class="w"> </span><span class="nv">you</span>
<span class="w">    </span><span class="nv">to</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">evaluation</span>.<span class="w"> </span><span class="nv">Respond</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">integer</span><span class="w"> </span><span class="mi">1</span><span class="o">-</span><span class="mi">10</span><span class="w"> </span><span class="nv">score</span><span class="w"> </span><span class="nv">first</span>,<span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nv">a</span>
<span class="w">    </span><span class="nv">rationale</span>.
</code></pre></div>

<p>Example:</p>
<h2>User Prompt:</h2>
<div class="codehilite"><pre><span></span><code><span class="x">Document:</span>
<span class="cp">{{</span><span class="nv">article</span><span class="cp">}}</span>
<span class="x">Summary:</span>
<span class="cp">{{</span><span class="nv">summary</span><span class="cp">}}</span>
<span class="x">Evaluation Form (Scores ONLY):</span>
<span class="x">ACU Salience:</span>
</code></pre></div>

<h1>B Familiarity Bias</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: Average Perplexity associated with Automatic Evaluation Score, for each Attribute. While GPT-4 shows a preference for low perplexity samples, GPT-3.5-Turbo seems to show a dis-preference for high perplexity examples.</p>
<h2>C Anchoring Effect in Multiple Judgments</h2>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: Distribution of scores conditioned on the values of previous scores in the same generation.</p>
<h1>D Performance of CoT and non-CoT prompting for each attribute</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11: Performance of CoT and non-CoT prompting at varying Temperatures. Trends shown in Figure 6 hold up on individual target dimensions in SummEval. In general, non-CoT predictions are harmed by higher temperatures, while CoT predictions are improved (though with diminishing returns).</p>
<h2>E Self-inconsistency</h2>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coh</th>
<th style="text-align: center;">Con</th>
<th style="text-align: center;">Flu</th>
<th style="text-align: center;">Rel</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Inter-annotator agreement</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.659</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Inter-sample agreement</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.589</td>
<td style="text-align: center;">0.587</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Single- vs Multi-attribute</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.513</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-5 star vs 1-10 score</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.597</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-5 star vs 1-100 score</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.439</td>
</tr>
</tbody>
</table>
<p>Table 7: Full results for Krippendorff's $\alpha$ values for inter-annotator agreement and self-consistency evaluation. For GPT-4's "inter-sample" agreement, multiple samples are considered annotations made by different annotators. For the remaining Krippendorff's $\alpha$ values were calculated for the agreement between judgments obtained for different settings (e.g., using single-attribute template and multi-attribute template.) For 1-10 and 1-100 score judgments, the judgements were converted into the same scale (1-5) by binning the numbers.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For all experiments in this work, we set $\mathrm{N}=10$ to balance reducing variance with avoiding prohibitive cost increases&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ It is indeed an interesting open question that how LLMs inherit round number bias from human through text written by human. Existing work reported round number bias for human judgments (Coupland, 2011; Honda et al., 2022).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-536 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-536</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-536</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-69ee9b3a915951cc84b74599a3a2699a66d4004f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/69ee9b3a915951cc84b74599a3a2699a66d4004f" target="_blank">CLIPort: What and Where Pathways for Robotic Manipulation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> CLIPort is presented, a language-conditioned imitation-learning agent that combines the broad semantic understanding of CLIP with the spatial precision of Transporter and is capable of solving a variety of language-specified tabletop tasks without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures.</p>
                <p><strong>Paper Abstract:</strong> How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e536.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e536.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPORT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPORT: What and Where Pathways for Robotic Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stream, language-conditioned imitation-learning agent that combines a frozen CLIP-based semantic pathway with a translationally-equivariant spatial Transporter pathway to predict dense pick-and-place affordance maps for tabletop manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPORT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stream architecture: (1) semantic stream uses a frozen CLIP ResNet50 image encoder + CLIP sentence encoder to produce tiled language-conditioned spatial feature maps (decoder layers are trained), (2) spatial stream is a Transporter-style RGB-D ResNet-like FCN trained from scratch to yield translationally-equivariant dense features; three FCNs (f_pick, Phi_query, Phi_key) produce dense pick Q-maps and place Q-maps via cross-correlation between a query crop and the full-scene key map. Trained end-to-end (decoder & spatial nets) with imitation learning from demonstrations; CLIP encoders are frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ravens language-conditioned manipulation benchmark (extended) / tabletop pick-and-place tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Top-down orthographic RGB-D heightmap observations of tabletop scenes; English instructions specifying goals as either stepwise instructions or single goal descriptions. Actions are 2-step motion primitives (pick pose, place pose) parameterized in SE(2). Tasks include packing, precise placement (kits), stacking, rope alignment, separating piles, Towers-of-Hanoi, and more; evaluation uses success scores (0-100) across 10 simulated tasks and 9 real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; instruction following; multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained CLIP (image-language contrastive pretraining on internet image-caption pairs) as semantic prior + imitation learning on task demonstrations (fine-tuned decoder & spatial FCNs); language instructions provided at inference</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>imitation learning (supervised) from demonstrations; conditioning at inference via natural language (goal or step), pretrained CLIP encodings frozen and used to condition decoder layers</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>dense pixelwise action-value maps (Q-maps) representing affordances; semantic knowledge represented as upsampled/tiled CLIP image features and tiled CLIP sentence embeddings combined via element-wise product to produce spatially-distributed semantic feature maps; spatial knowledge represented as translationally-equivariant FCN feature maps and cross-correlation (query * key) to compute relative placement transforms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success score (0-100 % success), averaged over 100 episodes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CLIPORT (single-task) typically achieves >90% on many tasks with 1000 demonstrations and ~86% on most tasks with 100 demonstrations; CLIPORT (multi-task) often matches or exceeds single-task models on many tasks (e.g., outperforms single-task in 57% of evaluated settings). On the real robot, a multi-task CLIPORT trained with 179 image-action pairs achieved ~70% on simple block tasks. (Paper reports many per-task success rates in Table 1.)</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully grounds object attributes (colors, shapes, categories) from language into pixelwise affordance predictions; transfers semantic attributes across tasks (multi-task models can reuse seen attributes in different task contexts); precise spatial placements are solved when semantic conditioning is fused with spatial equivariant features; procedural instructions (step-by-step language) are grounded to produce correct multi-step action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Purely semantic models without depth (CLIP-only) cannot achieve fine-grained spatial precision; CLIP pretraining alone does not guarantee correct grounding of unseen color/attribute words in the physical simulator without a few in-domain examples; multi-task models struggle on longer-horizon tasks (e.g., align-rope) likely due to reduced coverage of input-action pairs per task; the system cannot handle full 6-DOF dexterous manipulation, partial observability beyond top-down heightmaps, or continuous multi-finger control.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Transporter-only (spatial-only) saturates around ~50% on semantic tasks (cannot ground language); CLIP-only (semantic-only, RGB + language) saturates ~76% because it lacks depth/precise spatial reasoning; RN50-BERT two-stream is an alternative semantic stream (ImageNet ResNet50 + DistilBERT) that performs worse than CLIP-based semantic stream in few-shot settings. (See Table 1 for per-task numbers.)</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key ablations: (1) One-stream Language-Conditioned Transporter (conditioning low-level bottleneck with CLIP language) performs nearly zero — showing naive insertion of language into spatial bottleneck corrupts spatial features. (2) Two-stream without CLIP skip connections degrades performance substantially — skip connections from CLIP encoder to decoder are important. (3) Using an untrained semantic stream or ImageNet-pretrained RN50+DistilBERT semantic stream helps less than frozen CLIP (CLIP is essential for few-shot generalization). (4) Data augmentation with SE(2) transforms is important for spatial equivariance. Overall the two-stream CLIP + Transporter fusion yields the best tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Separating semantic (what) and spatial (where) pathways is critical: CLIP provides rich object-relational and attribute priors while a translationally-equivariant spatial FCN provides precise location and orientation reasoning. 2) Semantic knowledge from a pretrained vision-language model can be spatialized by upsampling CLIP features and tiling sentence embeddings (Hadamard product) to create pixelwise semantic maps that condition affordance prediction. 3) Spatial relations and relative transforms are encoded via translational equivariance and cross-correlation matching (query/key) which naturally represents relative placement transforms without explicit object poses or symbolic states. 4) Naive fusion of high-level language into low-level spatial bottlenecks fails, so architectural separation and lateral fusions are necessary to preserve spatial reasoning while injecting semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>This entry synthesizes how CLIPORT explicitly encodes and fuses semantic (object-relational) knowledge from CLIP and spatial/procedural knowledge via Transporter FCNs and language-conditioned decoding; the model operates with direct sensory input (RGB-D), but the paper also analyzes semantic-only (RGB+language) and spatial-only baselines to illustrate the roles of each modality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIPort: What and Where Pathways for Robotic Manipulation', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e536.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e536.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastively pre-trained vision-language model (image encoder + sentence encoder) trained on large-scale image-caption pairs to align visual and textual representations; used here as a frozen semantic prior that provides image features and sentence embeddings to condition manipulation policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning Transferable Visual Models From Natural Language Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP (ResNet50 image encoder + Transformer sentence encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive pretraining on millions of image-caption pairs producing aligned image and text embeddings; in CLIPORT the ResNet50 image encoder is used up to penultimate layer (7x7x2048 features), and the CLIP sentence encoder produces a 1024-d goal encoding. The CLIP encoders are frozen; the downstream semantic decoder layers are trained and conditioned with tiled sentence embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as semantic encoder for language-conditioned robotic manipulation tasks in Ravens benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides semantic priors (categories, attributes, parts, text) by encoding RGB observations and language goals; encodings are upsampled/tiled and fused with spatial stream to produce pixelwise affordance predictions for pick-and-place.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; semantic grounding for instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (categories, attributes, parts) and semantic (text-image alignment); limited explicit spatial knowledge from pooled image features but no depth</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on internet-scale image-caption pairs (contrastive alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>frozen pretrained encoders produce embeddings which are tiled and used to condition decoder layers via element-wise product (Hadamard) in the semantic stream; not fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>spatialized semantic maps created by upsampling the ResNet50 penultimate features and tiling the sentence embedding to match spatial dimensions; combined via element-wise product to produce pixelwise semantic activations that preserve localization information</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>component-level contribution evaluated via CLIP-only baseline and ablations; overall system measured with success rate (%) on tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>As a CLIP-only baseline (semantic stream alone, RGB + language, no depth/spatial FCN), the model achieves substantial semantic grounding (high success on many semantic tasks) but lacks fine-grained spatial precision; paper reports CLIP-only 'saturates at ~76%' (text) and shows varying per-task scores (see Table 1), generally underperforming CLIPORT on precise placement tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong at grounding object categories, colors, textures, parts and transferring attributes across contexts; enables few-shot semantic generalization when fused appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Insufficient for millimeter-to-centimeter-level placement precision because no depth channel and no translationally-equivariant spatial processing; cannot by itself plan precise pick-and-place actions.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CLIP-only outperforms Transporter-only on semantic grounding tasks but underperforms CLIPORT which fuses CLIP with a spatial FCN. RN50+DistilBERT alternative is less effective for few-shot semantic grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Two-stream variants that use CLIP as semantic stream perform best; removing CLIP skips or using CLIP language in the wrong place (one-stream Language Transporter) degrades performance. CLIP is essential for few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CLIP provides a transferable object-relational and attribute prior usable for robotic instruction grounding; converting CLIP's pooled features into spatially-distributed maps (upsample + tile sentence embedding + Hadamard product) is an effective way to inject semantic knowledge into pixelwise affordance prediction while preserving localization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIPort: What and Where Pathways for Robotic Manipulation', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e536.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e536.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transporter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transporter Networks (spatial equivariant FCN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A translationally-equivariant fully-convolutional architecture for pick-and-place which predicts dense pick-value maps and uses cross-correlation between a query crop and full-scene key features to compute placement transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transporter networks: Rearranging the visual world for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transporter</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Spatial architecture built with FCNs designed to be translationally equivariant: predicts a dense pixelwise pick Q-map (argmax gives pick location) and computes place Q-values by cross-correlating a query embedding (crop around pick) with a key embedding of the full scene, optionally stacking rotated crops to handle discrete rotations (k rotations). Trained from scratch via imitation learning on RGB-D heightmaps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tabletop pick-and-place manipulation tasks in Ravens benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict where to pick and where to place in top-down RGB-D heightmaps; used as spatial-only baseline and as spatial pathway in CLIPORT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; precise placement</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (locations, translations, rotations, relative transforms)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learned from demonstrations (imitation learning) on the manipulation tasks; no external semantic pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised imitation learning with dense pixelwise cross-entropy targets for pick/place pixels and discrete rotations</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>dense spatial action-value maps (Q-maps) that are translationally equivariant; place is represented via cross-correlation matching between local query features and global key features yielding a map over relative transforms (Δτ ∈ SE(2)).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success score (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Transporter-only baseline often achieves reasonable spatial precision but fails on semantic tasks requiring language grounding; paper reports Transporter-only 'saturates at ~50%' on language-specified tasks (because it has no language input). Per-task scores reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good at precise pick-and-place spatial reasoning, generalizes over translations and rotations due to equivariance, effective for purely geometric rearrangement tasks when the goal is given as images or fixed rules.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Lacks object-relational and semantic grounding — cannot generalize to new semantic goals (e.g., new colors or categories) without task-specific demonstrations; performs poorly when language specification is required.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to CLIP-only, Transporter-only is worse on semantic grounding but better at raw spatial precision; CLIPORT (fusion) outperforms Transporter-only by combining semantics + spatial equivariance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>When used alone (one-stream Transporter-only) it underperforms the two-stream CLIPORT; when language features were naively injected at the bottleneck (One-Stream Language Transporter) performance collapsed, indicating careful fusion is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transporter encodes spatial knowledge explicitly as translationally-equivariant dense maps and cross-correlation matching, which naturally models relative transforms and placement relations for pick-and-place. However, it needs semantic priors to select task-relevant objects/attributes when language goals are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIPort: What and Where Pathways for Robotic Manipulation', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e536.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e536.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>One-Stream Language-Conditioned Transporter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-Stream Language-Conditioned Transporter (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablated variant where CLIP language features are injected into the bottleneck of a single-stream Transporter; this naive conditioning corrupted low-level spatial features and produced near-zero task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>One-Stream Language-Conditioned Transporter (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-stream Transporter architecture where CLIP-derived language features are applied at the bottleneck of the spatial FCN (instead of using a separate semantic stream and lateral fusions). The approach intended to let language directly modulate spatial features but was found to be detrimental.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ravens language-conditioned manipulation tasks (ablation study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same pick-and-place tasks; evaluation intended to test whether injecting high-level language at low-level spatial bottlenecks suffices to ground language for precise manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; ablation for instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>attempted mix of spatial+object-relational via single-path fusion</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained CLIP sentence features + spatial FCN trained on demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>imitation learning with language features injected into spatial bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>single dense FCN where language-modulated bottleneck features are expected to encode both semantics and spatial structure</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success score (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Performed very poorly — reported as near 0% on many tasks (Table 5 and text), demonstrating the failure of naive single-path conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>None significant; model fails to preserve the spatial structure required for precise placements when high-level language corrupts low-level spatial features.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>High-level language features injected at low spatial levels disrupt the translationally-equivariant spatial representations, leading to collapse of pick/place performance; demonstrates that semantic conditioning must be architecturally separated and fused carefully (e.g., via semantic decoder + lateral fusions).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Contrasts with the two-stream CLIPORT which successfully fuses CLIP semantics and Transporter spatial processing; highlights why CLIPORT's two-stream design is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>This ablation itself is the result: near-zero performance compared to Transporter-only (~50% in some tasks) and CLIPORT (>90% in best settings).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows that language must be integrated in a way that preserves spatial equivariance — naive conditioning of low-level spatial features with high-level language corrupts spatial reasoning and destroys manipulation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIPort: What and Where Pathways for Robotic Manipulation', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e536.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e536.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RN50-BERT baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RN50-BERT two-stream baseline (ImageNet ResNet50 + DistilBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stream variant that replaces the CLIP ResNet50 and CLIP sentence encoder with an ImageNet-pretrained ResNet50 and a DistilBERT sentence encoder, demonstrating the importance of vision-language pretraining for few-shot semantic grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RN50-BERT (ResNet50 image encoder + DistilBERT sentence encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Semantic stream uses ImageNet-pretrained ResNet50 image features and DistilBERT-derived language features, tiled and fused similar to CLIPORT; spatial stream remains the Transporter FCN; decoder layers trained similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ravens language-conditioned manipulation tasks (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of pick-and-place language-conditioned tasks used to compare semantic priors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; semantic grounding baseline</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (categories via ImageNet) and procedural (from language), but with weaker image-language alignment</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>ImageNet classification pretraining for the image encoder + standard language model pretraining for DistilBERT + imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuned decoder + lateral fusion; two-stream architecture identical to CLIPORT except different pretrained semantic encoders</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>semantic maps built from ResNet50 features with DistilBERT sentence embeddings tiled and fused at decoder layers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success score (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>RN50-BERT two-stream can achieve good performance on some tasks (see Table 1/5) but is generally less data-efficient than CLIP-based semantic stream; CLIP is described as essential for few-shot learning (n >= 10) whereas RN50-BERT requires more data to match performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Can learn to ground some semantic concepts when trained sufficiently, sometimes achieving high performance on tasks that rely more on appearance than language-image alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Less effective for few-shot generalization and attribute transfer than CLIP-based semantic stream because ImageNet pretraining lacks direct image-text alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to CLIP-based two-stream CLIPORT: RN50-BERT is typically worse in few-shot regimes; CLIPORT performs better on language-grounding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Using RN50+DistilBERT instead of CLIP degrades few-shot performance; supports finding that image-text aligned pretraining (CLIP) is important for semantic conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates that not all pretrained visual encoders are equally effective for semantic conditioning in language-conditioned manipulation: explicit image-text alignment (CLIP) yields better few-shot semantic grounding than ImageNet-only pretraining with a separate language model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIPort: What and Where Pathways for Robotic Manipulation', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transporter networks: Rearranging the visual world for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Learning Transferable Visual Models From Natural Language Supervision <em>(Rating: 2)</em></li>
                <li>Learning to rearrange deformable cables, fabrics, and bags with goal-conditioned transporter networks <em>(Rating: 1)</em></li>
                <li>Mapping instructions to actions in 3d environments with visual goal prediction <em>(Rating: 1)</em></li>
                <li>LingUNet: Language-conditioned semantic segmentation for mapping instructions to spatial predictions (Misra et al. reference) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-536",
    "paper_id": "paper-69ee9b3a915951cc84b74599a3a2699a66d4004f",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "CLIPORT",
            "name_full": "CLIPORT: What and Where Pathways for Robotic Manipulation",
            "brief_description": "A two-stream, language-conditioned imitation-learning agent that combines a frozen CLIP-based semantic pathway with a translationally-equivariant spatial Transporter pathway to predict dense pick-and-place affordance maps for tabletop manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CLIPORT",
            "model_size": null,
            "model_description": "Two-stream architecture: (1) semantic stream uses a frozen CLIP ResNet50 image encoder + CLIP sentence encoder to produce tiled language-conditioned spatial feature maps (decoder layers are trained), (2) spatial stream is a Transporter-style RGB-D ResNet-like FCN trained from scratch to yield translationally-equivariant dense features; three FCNs (f_pick, Phi_query, Phi_key) produce dense pick Q-maps and place Q-maps via cross-correlation between a query crop and the full-scene key map. Trained end-to-end (decoder & spatial nets) with imitation learning from demonstrations; CLIP encoders are frozen.",
            "task_name": "Ravens language-conditioned manipulation benchmark (extended) / tabletop pick-and-place tasks",
            "task_description": "Top-down orthographic RGB-D heightmap observations of tabletop scenes; English instructions specifying goals as either stepwise instructions or single goal descriptions. Actions are 2-step motion primitives (pick pose, place pose) parameterized in SE(2). Tasks include packing, precise placement (kits), stacking, rope alignment, separating piles, Towers-of-Hanoi, and more; evaluation uses success scores (0-100) across 10 simulated tasks and 9 real-world tasks.",
            "task_type": "object manipulation; instruction following; multi-step planning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pretrained CLIP (image-language contrastive pretraining on internet image-caption pairs) as semantic prior + imitation learning on task demonstrations (fine-tuned decoder & spatial FCNs); language instructions provided at inference",
            "has_direct_sensory_input": true,
            "elicitation_method": "imitation learning (supervised) from demonstrations; conditioning at inference via natural language (goal or step), pretrained CLIP encodings frozen and used to condition decoder layers",
            "knowledge_representation": "dense pixelwise action-value maps (Q-maps) representing affordances; semantic knowledge represented as upsampled/tiled CLIP image features and tiled CLIP sentence embeddings combined via element-wise product to produce spatially-distributed semantic feature maps; spatial knowledge represented as translationally-equivariant FCN feature maps and cross-correlation (query * key) to compute relative placement transforms",
            "performance_metric": "task success score (0-100 % success), averaged over 100 episodes",
            "performance_result": "CLIPORT (single-task) typically achieves &gt;90% on many tasks with 1000 demonstrations and ~86% on most tasks with 100 demonstrations; CLIPORT (multi-task) often matches or exceeds single-task models on many tasks (e.g., outperforms single-task in 57% of evaluated settings). On the real robot, a multi-task CLIPORT trained with 179 image-action pairs achieved ~70% on simple block tasks. (Paper reports many per-task success rates in Table 1.)",
            "success_patterns": "Successfully grounds object attributes (colors, shapes, categories) from language into pixelwise affordance predictions; transfers semantic attributes across tasks (multi-task models can reuse seen attributes in different task contexts); precise spatial placements are solved when semantic conditioning is fused with spatial equivariant features; procedural instructions (step-by-step language) are grounded to produce correct multi-step action sequences.",
            "failure_patterns": "Purely semantic models without depth (CLIP-only) cannot achieve fine-grained spatial precision; CLIP pretraining alone does not guarantee correct grounding of unseen color/attribute words in the physical simulator without a few in-domain examples; multi-task models struggle on longer-horizon tasks (e.g., align-rope) likely due to reduced coverage of input-action pairs per task; the system cannot handle full 6-DOF dexterous manipulation, partial observability beyond top-down heightmaps, or continuous multi-finger control.",
            "baseline_comparison": "Transporter-only (spatial-only) saturates around ~50% on semantic tasks (cannot ground language); CLIP-only (semantic-only, RGB + language) saturates ~76% because it lacks depth/precise spatial reasoning; RN50-BERT two-stream is an alternative semantic stream (ImageNet ResNet50 + DistilBERT) that performs worse than CLIP-based semantic stream in few-shot settings. (See Table 1 for per-task numbers.)",
            "ablation_results": "Key ablations: (1) One-stream Language-Conditioned Transporter (conditioning low-level bottleneck with CLIP language) performs nearly zero — showing naive insertion of language into spatial bottleneck corrupts spatial features. (2) Two-stream without CLIP skip connections degrades performance substantially — skip connections from CLIP encoder to decoder are important. (3) Using an untrained semantic stream or ImageNet-pretrained RN50+DistilBERT semantic stream helps less than frozen CLIP (CLIP is essential for few-shot generalization). (4) Data augmentation with SE(2) transforms is important for spatial equivariance. Overall the two-stream CLIP + Transporter fusion yields the best tradeoff.",
            "key_findings": "1) Separating semantic (what) and spatial (where) pathways is critical: CLIP provides rich object-relational and attribute priors while a translationally-equivariant spatial FCN provides precise location and orientation reasoning. 2) Semantic knowledge from a pretrained vision-language model can be spatialized by upsampling CLIP features and tiling sentence embeddings (Hadamard product) to create pixelwise semantic maps that condition affordance prediction. 3) Spatial relations and relative transforms are encoded via translational equivariance and cross-correlation matching (query/key) which naturally represents relative placement transforms without explicit object poses or symbolic states. 4) Naive fusion of high-level language into low-level spatial bottlenecks fails, so architectural separation and lateral fusions are necessary to preserve spatial reasoning while injecting semantics.",
            "notes": "This entry synthesizes how CLIPORT explicitly encodes and fuses semantic (object-relational) knowledge from CLIP and spatial/procedural knowledge via Transporter FCNs and language-conditioned decoding; the model operates with direct sensory input (RGB-D), but the paper also analyzes semantic-only (RGB+language) and spatial-only baselines to illustrate the roles of each modality.",
            "uuid": "e536.0",
            "source_info": {
                "paper_title": "CLIPort: What and Where Pathways for Robotic Manipulation",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pre-training)",
            "brief_description": "A contrastively pre-trained vision-language model (image encoder + sentence encoder) trained on large-scale image-caption pairs to align visual and textual representations; used here as a frozen semantic prior that provides image features and sentence embeddings to condition manipulation policies.",
            "citation_title": "Learning Transferable Visual Models From Natural Language Supervision",
            "mention_or_use": "use",
            "model_name": "CLIP (ResNet50 image encoder + Transformer sentence encoder)",
            "model_size": null,
            "model_description": "Contrastive pretraining on millions of image-caption pairs producing aligned image and text embeddings; in CLIPORT the ResNet50 image encoder is used up to penultimate layer (7x7x2048 features), and the CLIP sentence encoder produces a 1024-d goal encoding. The CLIP encoders are frozen; the downstream semantic decoder layers are trained and conditioned with tiled sentence embeddings.",
            "task_name": "Used as semantic encoder for language-conditioned robotic manipulation tasks in Ravens benchmark",
            "task_description": "Provides semantic priors (categories, attributes, parts, text) by encoding RGB observations and language goals; encodings are upsampled/tiled and fused with spatial stream to produce pixelwise affordance predictions for pick-and-place.",
            "task_type": "object manipulation; semantic grounding for instruction following",
            "knowledge_type": "object-relational (categories, attributes, parts) and semantic (text-image alignment); limited explicit spatial knowledge from pooled image features but no depth",
            "knowledge_source": "pretraining on internet-scale image-caption pairs (contrastive alignment)",
            "has_direct_sensory_input": true,
            "elicitation_method": "frozen pretrained encoders produce embeddings which are tiled and used to condition decoder layers via element-wise product (Hadamard) in the semantic stream; not fine-tuned",
            "knowledge_representation": "spatialized semantic maps created by upsampling the ResNet50 penultimate features and tiling the sentence embedding to match spatial dimensions; combined via element-wise product to produce pixelwise semantic activations that preserve localization information",
            "performance_metric": "component-level contribution evaluated via CLIP-only baseline and ablations; overall system measured with success rate (%) on tasks",
            "performance_result": "As a CLIP-only baseline (semantic stream alone, RGB + language, no depth/spatial FCN), the model achieves substantial semantic grounding (high success on many semantic tasks) but lacks fine-grained spatial precision; paper reports CLIP-only 'saturates at ~76%' (text) and shows varying per-task scores (see Table 1), generally underperforming CLIPORT on precise placement tasks.",
            "success_patterns": "Strong at grounding object categories, colors, textures, parts and transferring attributes across contexts; enables few-shot semantic generalization when fused appropriately.",
            "failure_patterns": "Insufficient for millimeter-to-centimeter-level placement precision because no depth channel and no translationally-equivariant spatial processing; cannot by itself plan precise pick-and-place actions.",
            "baseline_comparison": "CLIP-only outperforms Transporter-only on semantic grounding tasks but underperforms CLIPORT which fuses CLIP with a spatial FCN. RN50+DistilBERT alternative is less effective for few-shot semantic grounding.",
            "ablation_results": "Two-stream variants that use CLIP as semantic stream perform best; removing CLIP skips or using CLIP language in the wrong place (one-stream Language Transporter) degrades performance. CLIP is essential for few-shot performance.",
            "key_findings": "CLIP provides a transferable object-relational and attribute prior usable for robotic instruction grounding; converting CLIP's pooled features into spatially-distributed maps (upsample + tile sentence embedding + Hadamard product) is an effective way to inject semantic knowledge into pixelwise affordance prediction while preserving localization.",
            "uuid": "e536.1",
            "source_info": {
                "paper_title": "CLIPort: What and Where Pathways for Robotic Manipulation",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Transporter",
            "name_full": "Transporter Networks (spatial equivariant FCN)",
            "brief_description": "A translationally-equivariant fully-convolutional architecture for pick-and-place which predicts dense pick-value maps and uses cross-correlation between a query crop and full-scene key features to compute placement transforms.",
            "citation_title": "Transporter networks: Rearranging the visual world for robotic manipulation",
            "mention_or_use": "use",
            "model_name": "Transporter",
            "model_size": null,
            "model_description": "Spatial architecture built with FCNs designed to be translationally equivariant: predicts a dense pixelwise pick Q-map (argmax gives pick location) and computes place Q-values by cross-correlating a query embedding (crop around pick) with a key embedding of the full scene, optionally stacking rotated crops to handle discrete rotations (k rotations). Trained from scratch via imitation learning on RGB-D heightmaps.",
            "task_name": "Tabletop pick-and-place manipulation tasks in Ravens benchmark",
            "task_description": "Predict where to pick and where to place in top-down RGB-D heightmaps; used as spatial-only baseline and as spatial pathway in CLIPORT.",
            "task_type": "object manipulation; precise placement",
            "knowledge_type": "spatial (locations, translations, rotations, relative transforms)",
            "knowledge_source": "learned from demonstrations (imitation learning) on the manipulation tasks; no external semantic pretraining",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised imitation learning with dense pixelwise cross-entropy targets for pick/place pixels and discrete rotations",
            "knowledge_representation": "dense spatial action-value maps (Q-maps) that are translationally equivariant; place is represented via cross-correlation matching between local query features and global key features yielding a map over relative transforms (Δτ ∈ SE(2)).",
            "performance_metric": "task success score (%)",
            "performance_result": "Transporter-only baseline often achieves reasonable spatial precision but fails on semantic tasks requiring language grounding; paper reports Transporter-only 'saturates at ~50%' on language-specified tasks (because it has no language input). Per-task scores reported in Table 1.",
            "success_patterns": "Good at precise pick-and-place spatial reasoning, generalizes over translations and rotations due to equivariance, effective for purely geometric rearrangement tasks when the goal is given as images or fixed rules.",
            "failure_patterns": "Lacks object-relational and semantic grounding — cannot generalize to new semantic goals (e.g., new colors or categories) without task-specific demonstrations; performs poorly when language specification is required.",
            "baseline_comparison": "Compared to CLIP-only, Transporter-only is worse on semantic grounding but better at raw spatial precision; CLIPORT (fusion) outperforms Transporter-only by combining semantics + spatial equivariance.",
            "ablation_results": "When used alone (one-stream Transporter-only) it underperforms the two-stream CLIPORT; when language features were naively injected at the bottleneck (One-Stream Language Transporter) performance collapsed, indicating careful fusion is needed.",
            "key_findings": "Transporter encodes spatial knowledge explicitly as translationally-equivariant dense maps and cross-correlation matching, which naturally models relative transforms and placement relations for pick-and-place. However, it needs semantic priors to select task-relevant objects/attributes when language goals are provided.",
            "uuid": "e536.2",
            "source_info": {
                "paper_title": "CLIPort: What and Where Pathways for Robotic Manipulation",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "One-Stream Language-Conditioned Transporter",
            "name_full": "One-Stream Language-Conditioned Transporter (ablation)",
            "brief_description": "An ablated variant where CLIP language features are injected into the bottleneck of a single-stream Transporter; this naive conditioning corrupted low-level spatial features and produced near-zero task performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "One-Stream Language-Conditioned Transporter (ablation)",
            "model_size": null,
            "model_description": "Single-stream Transporter architecture where CLIP-derived language features are applied at the bottleneck of the spatial FCN (instead of using a separate semantic stream and lateral fusions). The approach intended to let language directly modulate spatial features but was found to be detrimental.",
            "task_name": "Ravens language-conditioned manipulation tasks (ablation study)",
            "task_description": "Same pick-and-place tasks; evaluation intended to test whether injecting high-level language at low-level spatial bottlenecks suffices to ground language for precise manipulation.",
            "task_type": "object manipulation; ablation for instruction following",
            "knowledge_type": "attempted mix of spatial+object-relational via single-path fusion",
            "knowledge_source": "pretrained CLIP sentence features + spatial FCN trained on demonstrations",
            "has_direct_sensory_input": true,
            "elicitation_method": "imitation learning with language features injected into spatial bottleneck",
            "knowledge_representation": "single dense FCN where language-modulated bottleneck features are expected to encode both semantics and spatial structure",
            "performance_metric": "task success score (%)",
            "performance_result": "Performed very poorly — reported as near 0% on many tasks (Table 5 and text), demonstrating the failure of naive single-path conditioning.",
            "success_patterns": "None significant; model fails to preserve the spatial structure required for precise placements when high-level language corrupts low-level spatial features.",
            "failure_patterns": "High-level language features injected at low spatial levels disrupt the translationally-equivariant spatial representations, leading to collapse of pick/place performance; demonstrates that semantic conditioning must be architecturally separated and fused carefully (e.g., via semantic decoder + lateral fusions).",
            "baseline_comparison": "Contrasts with the two-stream CLIPORT which successfully fuses CLIP semantics and Transporter spatial processing; highlights why CLIPORT's two-stream design is necessary.",
            "ablation_results": "This ablation itself is the result: near-zero performance compared to Transporter-only (~50% in some tasks) and CLIPORT (&gt;90% in best settings).",
            "key_findings": "Shows that language must be integrated in a way that preserves spatial equivariance — naive conditioning of low-level spatial features with high-level language corrupts spatial reasoning and destroys manipulation performance.",
            "uuid": "e536.3",
            "source_info": {
                "paper_title": "CLIPort: What and Where Pathways for Robotic Manipulation",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "RN50-BERT baseline",
            "name_full": "RN50-BERT two-stream baseline (ImageNet ResNet50 + DistilBERT)",
            "brief_description": "A two-stream variant that replaces the CLIP ResNet50 and CLIP sentence encoder with an ImageNet-pretrained ResNet50 and a DistilBERT sentence encoder, demonstrating the importance of vision-language pretraining for few-shot semantic grounding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RN50-BERT (ResNet50 image encoder + DistilBERT sentence encoder)",
            "model_size": null,
            "model_description": "Semantic stream uses ImageNet-pretrained ResNet50 image features and DistilBERT-derived language features, tiled and fused similar to CLIPORT; spatial stream remains the Transporter FCN; decoder layers trained similarly.",
            "task_name": "Ravens language-conditioned manipulation tasks (baseline)",
            "task_description": "Same set of pick-and-place language-conditioned tasks used to compare semantic priors.",
            "task_type": "object manipulation; semantic grounding baseline",
            "knowledge_type": "object-relational (categories via ImageNet) and procedural (from language), but with weaker image-language alignment",
            "knowledge_source": "ImageNet classification pretraining for the image encoder + standard language model pretraining for DistilBERT + imitation learning",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuned decoder + lateral fusion; two-stream architecture identical to CLIPORT except different pretrained semantic encoders",
            "knowledge_representation": "semantic maps built from ResNet50 features with DistilBERT sentence embeddings tiled and fused at decoder layers",
            "performance_metric": "task success score (%)",
            "performance_result": "RN50-BERT two-stream can achieve good performance on some tasks (see Table 1/5) but is generally less data-efficient than CLIP-based semantic stream; CLIP is described as essential for few-shot learning (n &gt;= 10) whereas RN50-BERT requires more data to match performance.",
            "success_patterns": "Can learn to ground some semantic concepts when trained sufficiently, sometimes achieving high performance on tasks that rely more on appearance than language-image alignment.",
            "failure_patterns": "Less effective for few-shot generalization and attribute transfer than CLIP-based semantic stream because ImageNet pretraining lacks direct image-text alignment.",
            "baseline_comparison": "Compared directly to CLIP-based two-stream CLIPORT: RN50-BERT is typically worse in few-shot regimes; CLIPORT performs better on language-grounding tasks.",
            "ablation_results": "Using RN50+DistilBERT instead of CLIP degrades few-shot performance; supports finding that image-text aligned pretraining (CLIP) is important for semantic conditioning.",
            "key_findings": "Demonstrates that not all pretrained visual encoders are equally effective for semantic conditioning in language-conditioned manipulation: explicit image-text alignment (CLIP) yields better few-shot semantic grounding than ImageNet-only pretraining with a separate language model.",
            "uuid": "e536.4",
            "source_info": {
                "paper_title": "CLIPort: What and Where Pathways for Robotic Manipulation",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transporter networks: Rearranging the visual world for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Learning Transferable Visual Models From Natural Language Supervision",
            "rating": 2
        },
        {
            "paper_title": "Learning to rearrange deformable cables, fabrics, and bags with goal-conditioned transporter networks",
            "rating": 1
        },
        {
            "paper_title": "Mapping instructions to actions in 3d environments with visual goal prediction",
            "rating": 1
        },
        {
            "paper_title": "LingUNet: Language-conditioned semantic segmentation for mapping instructions to spatial predictions (Misra et al. reference)",
            "rating": 1
        }
    ],
    "cost": 0.022254,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CLIPORT: What and Where Pathways for Robotic Manipulation</h1>
<p>Mohit Shridhar ${ }^{1, \dagger}$ Lucas Manuelli ${ }^{2}$ Dieter Fox ${ }^{1,2}$<br>${ }^{1}$ University of Washington ${ }^{2}$ NVIDIA<br>mshr@cs.washington.edu lmanuelli@nvidia.com fox@cs.washington.edu<br>cliport.github.io</p>
<h4>Abstract</h4>
<p>How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPORT, a language-conditioned imitationlearning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.</p>
<h2>1 Introduction</h2>
<p>Ask a person to "get a scoop of coffee beans" or "fold the cloth in half" and they can naturally take concepts like scoop or fold and ground them in concrete physical actions within an accuracy of a few centimeters. We humans do this intuitively, without explicit geometric or kinematic models of coffee beans or cloths. Moreover, we can generalize to a broad range of tasks and concepts from a minimal set of examples on what needs to be achieved. How can we imbue robots with this ability to efficiently ground abstract semantic concepts in precise spatial reasoning?
Recently, a number of end-to-end frameworks have been proposed for vision-based manipulation [2, 3, 4, 5]. While these methods do not use any explicit representations of object poses, instance segmentations, or symbolic states, they can only replicate demonstrations with a narrow range of variability and have no notion of the semantics underlying the tasks. Switching from packing red pens to blue pens involves collecting a new training set [2], or if using goal-conditioned policies, involves the user providing a goal-image from the scene [5, 6]. In realistic human-robot interaction settings, collecting additional demonstrations or providing goal-images is often infeasible and unscalable. A natural solution to both these problems is to condition policies with natural language. Language provides an intuitive interface for specifying goals and also for implicitly transferring concepts across tasks. While language-grounding for manipulation has been explored in the past [7, 8, 9, 10], these pipelines are limited by object-centric representations that cannot handle granular or deformable objects and often do not reason about perception and action in an integrated manner. In parallel, there has been great progress in learning models for visual representations [11, 12] and aligning representations of vision and language [13, 14, 15] by training on large-scale internet data. However, these models lack a fine-grained understanding on how to manipulate objects, i.e. physical affordances.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Language-Conditioned Manipulation Tasks: CLIPORT is a broad framework applicable to a wide range of language-conditioned manipulation tasks in tabletop settings. We conduct large-scale experiments in Ravens [2] on 10 simulated tasks (a-j) with 1000s of unique instances per task. See Appendix A for challenges pertaining to each task. CLIPORT can even learn one multi-task model for all 10 tasks that achieves better or comparable performance to single-task models. Similarly, we demonstrate our approach on a Franka Panda manipulator with one multi-task model for 9 real-world tasks (k-o; only 5 shown) trained with just 179 image-action pairs.</p>
<p>To this end, we propose the first framework that combines the best of both worlds: end-to-end learning for fine-grained manipulation with the multi-goal and multi-task generalization capabilities of vision-language grounding systems. We introduce a two-stream architecture for manipulation with semantic and spatial pathways broadly inspired by (or vaguely analogous to) the two-stream hypothesis in cognitive psychology [16, 17, 18]. Specifically, we present CLIPORT, a languageconditioned imitation-learning agent that integrates the semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Transporter has been applied to a wide range of rearrangement tasks from industrial packing [2] to manipulating deformable objects [6]. The key insight of the approach is formulating tabletop manipulation as a series of pick-and-place affordance predictions, where the objective is to detect actions rather than detect objects and then learn a policy. This action-centric approach to perception [19] is data efficient and effective at circumventing the need for explicit "objectness" in learnt representations. However, Transporter is a tabula rasa system that learns all visual representations from scratch and so every new goal or task requires collecting a new set of demonstrations. To address this problem, we bake in a strong semantic prior while learning policies. We condition our semantic stream with visual and language-goal features from a pre-trained CLIP model [1]. Since CLIP is pre-trained to align image and language features from millions of image-caption pairs from the internet, it provides a powerful prior for grounding semantic concepts that are common across tasks like categories, parts, shapes, colors, texts, and other visual attributes, all without a top-down pipeline that requires bounding boxes or instance segmentations [13, 14, 15, 20]. This allows us to formulate tabletop rearrangement as a series of language-conditioned affordance predictions, a predominantly vision-based inference problem, and thus benefit from the strengths of data-driven paradigms like scale and generalization.</p>
<p>To study these benefits, we conduct large-scale experiments in the Ravens [2] framework with a simulated suction-gripper robot. We propose 10 language-conditioned tasks with 1000s of unique instances per task that require both semantic and spatial reasoning (see Figure 1 a-j). CLIPORT is not only effective at solving these tasks, but surprisingly, it can even learn a multi-task model for all 10 tasks that achieves better or comparable performance to single-task models. Further, our evaluations indicate that our multi-task model can effectively transfer attributes like "pink block" across tasks, having never seen pink blocks or the word 'pink' in the context of the evaluation task. We also demonstrate our approach on a Franka Panda manipulator with one multi-task model for 9 real-world tasks trained with just 179 image-action pairs (see Figure 1 k-o).</p>
<p>In summary, our contributions are as follows:</p>
<ul>
<li>An extended benchmark of language-grounding tasks for manipulation in Ravens [2].</li>
<li>Two-stream architecture for using internet pre-trained vision-language models for conditioning precise manipulation policies with language goals.</li>
<li>Empirical results on a broad range of manipulation tasks, including multi-task models, validated with real-robot experiments.</li>
</ul>
<p>The benchmark, code, and pre-trained models are available at: cliport.github.io.</p>
<h1>2 Related Work</h1>
<p>Vision-based Manipulation. Traditionally, perception for manipulation has centered around object detectors, segmentors, and pose estimators [21, 22, 23, 24, 25, 26]. These methods cannot handle deformable objects, granular media, or generalize to unseen objects without object-specific training data. Alternatively, dense descriptors [27, 28, 29] and keypoint representations [30, 31, 32] forgo segmentation and pose representations, but do not reason about sequential actions and struggle to represent scenes with variable numbers of objects. On the other hand, end-to-end perception-toaction models can learn precise sequential policies [2, 4, 6, 33, 34, 35], but these methods have limited understanding of semantic concepts and rely on goal-images to condition policies. In contrast, Yen-Chen et. al [36] showed that pre-training on semantic tasks like classification and segmentation helps in improving efficiency and generalization of grasping predictions.</p>
<p>Semantic Models. With the advent of large-scale models [37, 38, 39], a number of methods for learning joint vision and language representations have been proposed [13, 14, 15, 20, 40]. However, these methods are restricted to bounding boxes or instance segmentations, which make them inapplicable for detecting things like piles of coffee beans or squares on a chessboard. Alternatively, works in contrastive learning forgo top-down object-detection and learn continuous representations by pre-training on unlabeled data [11, 12]. Recently, CLIP [1] applied a similar approach to align vision and language representations by training on millions of image-caption pairs from the internet.</p>
<p>Language Grounding for Robotics. Several works have proposed systems for instructing robots with natural language [7, 8, 9, 10, 41, 42, 43, 44, 45, 46, 47]. However, these methods use disentangled pipelines for perception and action with the language primarily being used to guide the perception. As such, these pipelines lack the spatial precision necessary for tasks like folding cloths. Recently, Lynch et. al [48] proposed an end-to-end system for grounding language in continuous control, but it requires several hours of human teleoperation data for a single simulated desk setting.</p>
<p>Two-Stream Architectures are prevalent in action-recognition networks [49, 50, 51] and audiorecognition systems [52, 53]. In robotics, Zeng et. al [54] and Jang et. al [55] have proposed twostream pipelines for affordance predictions of novel objects. The former requires goal-images and the latter is restricted to one-step grasps with single-category goals. In contrast, our framework provides a rich and intuitive interface with composable language commands for sequential tasks.</p>
<h2>3 CLIPORT</h2>
<p>CLIPORT is an imitation-learning agent based on four key principles: (1) Manipulation through a two-step primitive where each action involves a start and final end-effector pose. (2) Visual representations of actions that are equivariant to translations and rotations [56, 57]. (3) Two separate pathways for semantic and spatial information. (4) Language-conditioned policies for specifying goals and also transferring concepts across tasks. Combining (1) and (2) from Transporter with (3) and (4) allows us to achieve generalizable policies that go beyond just imitating demonstrations.</p>
<p>Section 3.1 describes the problem formulation, gives an overview of Transporter [2], and presents our language-conditioned model. Section 3.2 provides details on the training approach.</p>
<h3>3.1 Language-Conditioned Manipulation</h3>
<p>We consider the problem of learning a goal-conditioned policy $\pi$ that outputs actions $\mathbf{a}<em t="t">{t}$ given input $\gamma</em>}=\left(\mathbf{o<em t="t">{t}, \mathbf{l}</em>}\right)$ consisting of a visual observation $\mathbf{o<em t="t">{t}$ and an English language instruction $\mathbf{l}</em>$ :</p>
<p>$$
\pi\left(\gamma_{t}\right)=\pi\left(\mathbf{o}<em t="t">{t}, \mathbf{l}</em>}\right) \rightarrow \mathbf{a<em _pick="{pick" _text="\text">{t}=\left(\mathcal{T}</em>
$$}}, \mathcal{T}_{\text {place }}\right) \in \mathcal{A</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: CLIPORT Two-Stream Architecture. An overview of the semantic and spatial streams. The semantic stream uses a frozen CLIP ResNet50 [1] to encode RGB input, and its decoder layers are conditioned with tiled language features from the CLIP sentence encoder. The spatial stream encodes RGB-D input, and its decoder layers are laterally fused with the semantic stream. The final output is a map of dense pixelwise features that is used for pick or place affordance predictions. This same two-stream architecture is used in all 3 Fully-Convolutional-Networks $f_{\text {pick }}, \Phi_{\text {query }}$, and $\Phi_{\text {key }}$ with $f_{\text {pick }}$ is used to predict pick actions, and $\Phi_{\text {query }}$ and $\Phi_{\text {key }}$ are used to predict place actions. See Appendix C for the exact architecture.</p>
<p>The actions $\mathbf{a}=\left(\mathcal{T}<em _place="{place" _text="\text">{\text {pick }}, \mathcal{T}</em>}}\right)$ specify the end-effector pose for picking and placing, respectively. We consider tabletop tasks where $\mathcal{T<em _place="{place" _text="\text">{\text {pick }}, \mathcal{T}</em>}} \in \mathbf{S E}(2)$. The visual observation $\mathbf{o<em t="t">{t}$ is a top-down orthographic RGB-D reconstruction of the scene where each pixel corresponds to a point in 3D space. The language instruction $\mathbf{l}</em>$ either specifies step-by-step instructions e.g. "pack the scissors" $\rightarrow$ "pack the purple tape" $\rightarrow$ etc., or a single goal description for the whole task e.g "pack all the blue and yellow boxes in the brown box". See Figure 4 for specific examples.
We assume access to a dataset $\mathcal{D}=\left{\zeta_{1}, \zeta_{2}, \ldots, \zeta_{n}\right}$ of $n$ expert demonstrations with associated discrete-time input-action pairs $\zeta_{i}=\left{\left(\mathbf{o}<em 1="1">{1}, \mathbf{l}</em>}, \mathbf{a<em 2="2">{1}\right),\left(\mathbf{o}</em>}, \mathbf{l<em 2="2">{2}, \mathbf{a}</em>}\right), \ldots\right}$ where $\mathbf{a<em _pick="{pick" _text="\text">{t}=\left(\mathcal{T}</em>}}, \mathcal{T<em _pick="{pick" _text="\text">{\text {place }}\right)$ corresponds to expert pick-and-place coordinates at timestep $t$. These expert demonstrations are used to supervise the policy $\pi$.
Transporter for Pick-and-Place. The policy $\pi$ is trained with Transporter [2] to perform spatial manipulation. The model first (i) attends to a local region to decide where to pick, then (ii) computes a placement location by finding the best match through cross-correlation of deep visual features.
Following Transporter [2, 6], the policy $\pi$ is composed of two action-value modules (Q-functions): The pick module $\mathcal{Q}</em>}}$ decides where to pick, and conditioned on this pick action the place module $\mathcal{Q<em _pick="{pick" _text="\text">{\text {place }}$ decides where to place. These modules are implemented as Fully-Convolutional-Networks (FCNs) that are translationally equivariant by design. As we will describe in more detail below, we extend these networks to two-stream architectures that can handle language input. The pick $\operatorname{FCN} f</em>}}$ takes input $\gamma_{t}=\left(\mathbf{o<em t="t">{t}, \mathbf{l}</em>}\right)$ and outputs a dense pixelwise prediction $\mathcal{Q<em _pick="{pick" _text="\text">{\text {pick }} \in \mathbb{R}^{H \times W}$ of action-values, where are used to predict the pick action $\mathcal{T}</em>$ :}</p>
<p>$$
\mathcal{T}<em _pick="{pick" _text="\text">{\text {pick }}=\underset{(u, v)}{\operatorname{argmax}} \mathcal{Q}</em>\right)
$$}}\left((u, v) \mid \gamma_{t</p>
<p>Since $\mathbf{o}<em _pick="{pick" _text="\text">{t}$ is an orthographic heightmap, each pixel location $(u, v)$ can be mapped to a 3D picking location using the known camera calibration. $f</em>}}$ is trained in a supervised manner to predict the pick action $\mathcal{T<em _query="{query" _text="\text">{\text {pick }}$ that imitates the expert demonstration with the specified language instruction at timestep $t$.
The second $\operatorname{FCN} \Phi</em>}}$ takes in $\gamma_{t}\left[\mathcal{T<em t="t">{\text {pick }}\right]$, which is a $c \times c$ crop of $\mathbf{o}</em>}$ centered at $\mathcal{T<em t="t">{\text {pick }}$ along with the language instruction $\mathbf{l}</em>$ are then computed by cross-correlating the query and key features:}$, and outputs a query feature embedding of shape $\mathbb{R}^{c \times c \times d}$. The third FCN $\Phi_{\text {key }}$ consumes the full input $\gamma_{t}$ and outputs a key feature embedding of shape $\mathbb{R}^{H \times W \times d}$. The place action-values $\mathcal{Q}_{\text {place }</p>
<p>$$
\mathcal{Q}<em t="t">{\text {place }}\left(\Delta \tau \mid \gamma</em>}, \mathcal{T<em _query="{query" _text="\text">{\text {pick }}\right)=\left(\Phi</em>}}\left(\gamma_{t}\left[\mathcal{T<em _key="{key" _text="\text">{\text {pick }}\right]\right) * \Phi</em>\right)\right)[\Delta \tau]
$$}}\left(\gamma_{t</p>
<p>where $\Delta \tau \in \boldsymbol{S} \boldsymbol{E}(2)$ represents a potential placement pose. Since $\mathbf{o}<em _query="{query" _text="\text">{t}$ is an orthographic heightmap, rotations in the placement pose $\Delta \tau$ can be captured by stacking $k$ discrete angle rotations of the crop before passing it through the query network $\Phi</em>}}$. Then $\mathcal{T<em _Delta="\Delta" _tau="\tau">{\text {place }}=\operatorname{argmax}</em>} \mathcal{Q<em t="t">{\text {place }}\left(\Delta \tau \mid \gamma</em>}, \mathcal{T<em _pick="{pick" _text="\text">{\text {pick }}\right)$, where the place module is trained to imitate the placements in the expert demonstrations. For all models, we use $c=64, k=36$ and $d=3$. As in Transporter [2, 6], our framework can be extended to handle any motion primitive like pushing, sliding, etc. that can be parameterized by two end-effector poses at each timestep. For more details, we refer the reader to the original paper [2].
Two-Stream Architecture. In CLIPORT, we extend the network architecture of all three FCNs $f</em>$ from Transporter [2] to allow for language input and reasoning about high-level semantic concepts. We extend the FCNs to two-pathways: semantic (ventral) and spatial (dorsal). The semantic stream is conditioned with language features at the bottleneck and fused with intermediate features from the spatial stream. See Figure 2 for an overview of the architecture.}}, \Phi_{\text {query }}$ and $\Phi_{\text {key }</p>
<p>The spatial stream is identical to the ResNet architecture in Transporter - a tabula rasa network that takes in RGB-D input $\mathbf{o}<em t="t">{t}$ and outputs dense features through an hourglass encoder-decoder model. The semantic stream uses a frozen pre-trained CLIP ResNet50 [1] to encode the RGB input ${ }^{2} \tilde{\mathbf{o}}</em>}$ up until the penultimate layer $\tilde{\mathbf{o}<em t="t">{t} \rightarrow \mathbf{v}</em>}^{(0)}: \mathbb{R}^{7 \times 7 \times 2048}$, and then introduces decoding layers that upsample the feature tensors to mimic the spatial stream $\mathbf{v<em t="t">{t}^{(l-1)} \rightarrow \mathbf{v}</em>$ at each layer $l$.
The language instruction $\mathbf{l}}^{(l)}: \mathbb{R}^{h \times w \times C<em t="t">{t}$ is encoded with CLIP's Transformer-based sentence encoder to produce a goal encoding $\mathbf{l}</em>} \rightarrow \mathbf{g<em t="t">{t}: \mathbb{R}^{1024}$. This goal encoding $\mathbf{g}</em>}$ is downsampled with fully-connected layers to match the channel dimension $C$ and tiled to match the spatial dimensions of the decoder features such that $\mathbf{g<em t="t">{t} \rightarrow \mathbf{g}</em>}^{(l)}: \mathbb{R}^{h \times w \times C}$. The decoder features are then conditioned with the tiled goal features through an element-wise product $\mathbf{v<em t="t">{t}^{(l)} \odot \mathbf{g}</em>}^{(l)}$ (Hadamard product). Since CLIP was trained with contrastive loss on the dot-product alignment between pooled image features and language encodings, the element-wise product allows us to use this alignment while the tiling preserves the spatial dimensions of the visual features. This language conditioning is repeated for three subsequent layers after the bottleneck inspired by LingUNet [58]. We also add skip connections to these layers from the CLIP ResNet50 encoder to utilize different levels of semantic information from shapes to parts to object-level concepts [59]. Finally, following existing two-stream architectures in videoaction recognition [51], we add lateral connections from the spatial stream to the semantic stream. These connections involve concatenating two feature tensors and applying $1 \times 1$ conv to reduce the channel dimension $\left[\mathbf{v<em t="t">{t}^{(l)} \odot \mathbf{g}</em>}^{(l)} ; \mathbf{d<em _mathbf_v="\mathbf{v">{t}^{(l)}\right]: \mathbb{R}^{h \times w \times C</em>}}+C_{\mathbf{d}}} \rightarrow \mathbb{R}^{h \times w \times C_{\mathbf{v}}}$, where $\mathbf{v<em t="t">{t}^{(l)}$ and $\mathbf{d}</em>$ worked the best empirically. See Appendix C for details on the exact architecture.}^{(l)}$ are the semantic and spatial tensors at layer $l$, respectively. For the final fusion of dense features, addition for $f_{\text {pick }}$ and $1 \times 1$ conv fusion for $\Phi_{\text {query }}$ and $\Phi_{\text {key }</p>
<h1>3.2 Implementation Details</h1>
<p>Training from demonstrations. Similar to Transporter [2] we train CLIPORT through imitation learning from a set of expert demonstrations $\mathcal{D}=\left{\zeta_{1}, \zeta_{2}, \ldots, \zeta_{n}\right}$ consisting of discrete-time inputaction pairs $\zeta_{i}=\left{\left(\mathbf{o}<em 1="1">{1}, \mathbf{l}</em>}, \mathbf{a<em 2="2">{1}\right),\left(\mathbf{o}</em>}, \mathbf{l<em 2="2">{2}, \mathbf{a}</em>}\right), \ldots\right}$. During training, we randomly sample an inputaction pair from the dataset and supervise the model end-to-end with one-hot pixel encodings of demonstration actions $Y_{\text {pick }}: \mathbb{R}^{H \times W \times k}$ and $Y_{\text {place }}: \mathbb{R}^{H \times W \times k}$ with $k$ discrete rotations. In simulated experiments with the suction-gripper, we use $k=1$ for pick actions and $k=36$ for place actions. The model is trained with cross-entropy loss: $\mathcal{L}=-\mathbb{E<em _pick="{pick" _text="\text">{Y</em>}}}\left[\log \mathcal{V<em Y__text="Y_{\text" _place="{place">{\text {pick }}\right]-\mathbb{E}</em>}}}\left[\log \mathcal{V<em _pick="{pick" _text="\text">{\text {place }}\right]$ where $\mathcal{V}</em>}}=\operatorname{softmax}\left(\mathcal{Q<em t="t">{\text {pick }}\left((u, v) \mid \gamma</em>}\right)\right)$ and $\mathcal{V<em _place="{place" _text="\text">{\text {place }}=\operatorname{softmax}\left(\mathcal{Q}</em>\right)\right)$. Compared to the original Transporter models that were trained for 40 K iterations, we train our models for 200 K iterations (with data augmentation; see Appendix E) to account for additional semantic variation in tasks - randomized colors, shapes, objects. All models are trained on a single commodity GPU for 2 days with a batch size of 1.
Training multi-task models. Multi-task training is nearly identical to single-task training except for the sampling of training data. First, we randomly sample a task, and then select a random inputaction pair from that task in the dataset. Using this strategy, all tasks are equally likely to be sampled but longer horizon tasks are less likely to reach full coverage of input-action pairs available in the dataset. To compensate for this, we train all multi-task models $3 \times$ longer for 600 K iterations or 6 GPU days.}}\left(\left(u^{\prime}, v^{\prime}, \omega^{\prime}\right) \mid \gamma_{t}, \mathcal{T}_{\text {pick }</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 Results</h1>
<p>We perform experiments both in simulation and hardware aimed at answering the following questions: 1) How effective is the language-conditioned two-stream architecture for fine-grained manipulation compared to one-stream alternatives and other simpler baselines? 2) Is it possible to train a multi-task model for all tasks, and how well does it perform and generalize? 3) How well do these models generalize to seen and unseen semantic attributes like colors, shapes, and object categories?</p>
<h3>4.1 Simulation Setup</h3>
<p>Environment. All simulated experiments are based on a Universal Robot UR5e with a suction gripper. The setup provides a systematic and reproducible environment for evaluation, especially for benchmarking the ability to ground semantic concepts like colors and object categories. The input observation is a top-down RGB-D reconstruction from 3 cameras positioned around a rectangular table: one in the front, one on the left shoulder, and one on the right shoulder, all pointing towards the center. Each camera has a resolution of $640 \times 480$ and is noiseless.</p>
<p>Language-Conditioned Manipulation Tasks. We extend the Ravens benchmark [2] set in PyBullet [60] with 10 language-conditioned manipulation tasks. See Figure 1 for examples and Table 3 for challenges associated with each task. Each task instance is constructed by sampling a set of objects and attributes: poses, colors, sizes, and object categories. 8 of the 10 tasks have two variants, denoted by seen and unseen, depending on whether the task has unseen attributes (e.g. color) at test time. For colors: $\mathbb{T}<em _text="\text" _unseen="{unseen" colors="colors">{\text {seen colors }}={$ yellow, brown, gray, cyan $}$ and $\mathbb{T}</em>={$ red, green, blue $}$ used in both the seen and unseen spilts. For packing objects, we use 56 tabletop objects from the Google Scanned Objects dataset [61] and split them into 37 seen and 19 unseen objects. The language instructions are constructed from templates for simulated experiments, and human-annotated for real-world experiments. For more details about individual tasks, see Appendix A.}}=$ {orange, purple, pink, white} with 3 overlapping colors $\mathbb{T}_{\text {all }</p>
<p>Evaluation Metric. We adopt the 0 (fail) to 100 (success) scores proposed in the Ravens benchmark [2]. The score assigns partial credit based on the task, e.g. $3 / 5 \Rightarrow 60.0$ for packing 3 out of 5 objects specified in the instructions, or $30 / 56 \Rightarrow 53.6$ for pushing 30 out of 56 particles into the correct zone. See Appendix A for the specific evaluation metric used in each task. During an evaluation episode, an agent keeps interacting with the scene until an oracle indicates task-completion. We report scores on 100 evaluation runs for agents trained with $n=1,10,100,1000$ demonstrations.</p>
<h3>4.2 Simulation Results</h3>
<p>Table 1 presents results from our large-scale experiments in Ravens [2] and Figure 3 summarizes these results with average scores across seen and unseen splits.</p>
<p>Baseline Methods. To study
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Average scores across seen and unseen splits for all tasks in Table 1.
the effectiveness of our two-stream architecture, we broadly compare against two baselines: Transporter-only and CLIP-only. Transporter-only is the original Transporter [2], or equivalently, the spatial stream of CLIPORT with RGB-D input. Although Transporter-only does not receive any language goals, it shows what can be achieved through chance by exploiting the most likely actions seen during training. On the other hand, CLIP-only is just the semantic stream of CLIPORT with RGB and language input. CLIP-only shows what can be achieved by fine-tuning a pre-trained semantic model for manipulation without spatial information, particularly depth.</p>
<p>Two-Stream Performance. Figure 3 (seen) captures the essence of our main claims. The performance of Transporter-only saturates at $50 \%$ since it doesn't use the language instruction to ground the desired goal. CLIP-only does have a goal, but lacks the spatial precision to go the last mile and thus saturates at $76 \%$. Only CLIPORT (single) achieves more than $90 \%$, which indicates that both the semantic and spatial streams are crucial for fine-grained manipulation. Further, CLIPORT (single) achieves $86 \%$ on most tasks with just 100 demonstrations, showcasing its efficiency.
In addition to these baselines, we present various ablations and alternative one-stream and twostream models in Appendix F. To briefly summarize these results, CLIP is essential for few-shot</p>
<p>|  | packing-box-pairs <br> seen-colors | packing-box-pairs <br> unseen-colors | packing-box-pairs <br> unseen-colors | packing-seen-google <br> objects-seq | packing-unseen-google <br> objects-seq | packing-seen-google <br> objects-group | packing-unseen-google <br> objects-group |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Method | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 |
| Transporter-only [2] | 44.2 | 55.2 | 54.2 | 52.4 | 34.6 | 48.7 | 47.2 | 54.1 | 26.2 | 39.7 | 45.4 | 46.3 | 19.9 | 29.8 | 28.7 | 37.3 | 60.0 | 54.3 | 61.5 | 59.9 | 46.2 | 54.7 | 49.8 | 52.0 |
| CLIP-only | 38.6 | 69.7 | 88.5 | 87.1 | 33.0 | 65.5 | 68.8 | 61.2 | 29.1 | 67.9 | 89.3 | 95.8 | 37.1 | 49.4 | 60.4 | 57.8 | 52.5 | 62.0 | 89.6 | 92.7 | 43.4 | 65.9 | 73.1 | 70.0 |
| RN50-BERT | 36.2 | 64.0 | 94.7 | 90.3 | 31.4 | 52.7 | 65.6 | 72.1 | 32.9 | 48.4 | 87.9 | 94.0 | 29.3 | 48.5 | 48.3 | 56.1 | 46.4 | 52.9 | 76.5 | 86.4 | 43.2 | 52.0 | 66.3 | 73.7 |
| CLIPORT (single) | 51.6 | 82.9 | 92.7 | 98.2 | 45.6 | 65.3 | 68.6 | 71.5 | 14.8 | 59.5 | 86.8 | 96.2 | 27.2 | 50.0 | 65.5 | 71.9 | 52.7 | 67.0 | 84.1 | 94.0 | 61.5 | 66.2 | 78.4 | 81.5 |
| CLIPORT (multi) | 66.8 | 88.6 | 94.1 | 96.6 | 59.0 | 69.7 | 76.2 | 71.4 | 41.6 | 78.4 | 85.0 | 84.4 | 40.7 | 51.1 | 65.8 | 70.3 | 71.3 | 84.6 | 89.6 | 88.3 | 68.4 | 69.6 | 78.4 | 80.5 |
| CLIPORT (multi-attr) | - | - | - | - | 46.2 | 72.0 | 86.2 | 80.3 | - | - | - | - | 35.4 | 45.1 | 78.9 | 87.4 | - | - | - | - | 48.6 | 69.3 | 84.8 | 89.1 |
|  | stack-block-pyramid <br> seq-seen-colors | stack-block-pyramid <br> seq-unseen-colors | separating-piles <br> seen-colors | separating-piles <br> unseen-colors | towers-of-hanoi <br> seq-seen-colors | towers-of-hanoi <br> seq-unseen-colors |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 |
| Transporter-only [2] | 4.5 | 2.3 | 5.2 | 4.5 | 3.0 | 4.0 | 2.3 | 5.8 | 42.7 | 52.3 | 42.0 | 48.4 | 41.2 | 49.2 | 44.7 | 52.3 | 25.4 | 67.9 | 98.0 | 99.9 | 24.3 | 44.6 | 71.7 | 80.7 |
| CLIP-only | 6.3 | 28.7 | 55.7 | 54.8 | 2.0 | 12.2 | 18.3 | 19.5 | 43.5 | 55.0 | 84.9 | 90.2 | 59.9 | 49.6 | 73.0 | 71.0 | 9.4 | 52.6 | 88.6 | 45.3 | 24.7 | 47.0 | 67.0 | 58.0 |
| RN50-BERT | 5.3 | 35.0 | 89.0 | 97.5 | 6.2 | 12.2 | 21.5 | 30.7 | 31.8 | 47.8 | 46.5 | 46.5 | 33.4 | 44.4 | 41.3 | 44.9 | 28.0 | 66.1 | 91.3 | 92.1 | 17.4 | 75.1 | 85.3 | 89.3 |
| CLIPORT (single) | 28.3 | 64.7 | 93.3 | 98.8 | 13.7 | 24.3 | 31.2 | 41.3 | 54.5 | 59.5 | 93.1 | 98.0 | 47.2 | 51.0 | 76.6 | 75.2 | 59.4 | 92.9 | 97.4 | 100 | 56.1 | 89.7 | 95.9 | 99.4 |
| CLIPORT (multi) | 33.5 | 75.3 | 96.8 | 96.5 | 23.3 | 26.8 | 31.7 | 22.2 | 48.9 | 72.4 | 90.3 | 89.0 | 56.6 | 62.6 | 64.9 | 62.8 | 61.6 | 96.3 | 98.7 | 98.1 | 60.1 | 65.6 | 76.7 | 68.7 |
| CLIPORT (multi-attr) | - | - | - | - | 15.5 | 51.5 | 59.3 | 79.8 | - | - | - | - | 49.9 | 51.8 | 48.2 | 59.8 | - | - | - | - | 56.7 | 78.0 | 88.3 | 96.9 |
|  | align-rope | packing-unseen-shapes |  |  |  |  |  |  | assembling-kits-seq <br> seen-colors |  |  |  | assembling-kits-seq <br> unseen-colors |  |  |  | put-blocks-in-bowls <br> seen-colors |  |  |  | put-blocks-in-bowls <br> unseen-colors |  |  |  |
|  | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 | 1 | 10 | 100 | 1000 |
| Transporter-only [2] | 6.9 | 30.6 | 33.1 | 51.5 | 16.0 | 20.0 | 22.0 | 22.0 | 5.8 | 11.6 | 28.6 | 29.6 | 7.8 | 17.6 | 25.6 | 28.4 | 16.8 | 33.3 | 62.7 | 64.7 | 11.7 | 17.2 | 14.8 | 18.7 |
| CLIP-only | 13.4 | 48.7 | 70.4 | 70.7 | 13.0 | 28.0 | 44.0 | 50.0 | 0.8 | 9.2 | 19.8 | 23.0 | 2.0 | 4.6 | 10.8 | 19.8 | 23.5 | 60.2 | 93.5 | 97.7 | 11.2 | 34.2 | 33.2 | 44.5 |
| RN50-BERT | 3.1 | 25.0 | 63.8 | 57.1 | 19.0 | 25.0 | 32.0 | 44.0 | 2.2 | 5.6 | 11.6 | 21.8 | 1.6 | 6.4 | 10.4 | 18.4 | 13.8 | 44.5 | 81.2 | 91.8 | 16.2 | 23.0 | 30.3 | 23.8 |
| CLIPORT (single) | 20.1 | 77.4 | 85.6 | 95.4 | 21.0 | 26.0 | 40.0 | 37.0 | 12.2 | 17.8 | 47.0 | 66.6 | 16.2 | 18.0 | 35.4 | 34.8 | 23.5 | 68.3 | 92.5 | 100 | 18.0 | 35.3 | 37.3 | 25.0 |
| CLIPORT (multi) | 19.6 | 49.3 | 82.4 | 74.9 | 25.0 | 35.0 | 37.0 | 31.0 | 11.4 | 34.8 | 46.2 | 52.4 | 7.8 | 21.6 | 29.0 | 25.4 | 54.0 | 90.2 | 99.5 | 100 | 32.0 | 48.8 | 55.3 | 45.8 |
| CLIPORT (multi-attr) | - | - | - | - | - | - | - | - | - | - | - | - | 7.6 | 10.4 | 43.8 | 34.6 | - | - | - | 23.0 | 41.8 | 66.5 | 75.7 |  |</p>
<p>Table 1. Language-Conditioned Test Results. Task success scores (mean $\%$ ) from 100 evaluation instances vs. # of training demonstrations ( $1,10,100$, or 1000 ). The challenges pertaining to each task are described in Appendix A. CLIPORT (single) models are trained on seen splits, and evaluated on both seen and unseen splits. CLIPORT (multi) models are trained on seen splits of all 10 tasks with $1 \overline{17}, 10 \overline{7}, 100 \overline{72}$, and 1000 $\overline{7}$ demonstrations where $\overline{1}=10$. CLIPORT (multi-attr) indicate CLIPORT (multi) models trained on seen-and-unseen splits from all tasks except for that one particular heldout task, for which it is trained only the seen split. See Figure 3 for an overview with average scores.
learning (i.e. $n \geq 10$ ) in lieu of semantic stream alternatives like ImageNet-trained ResNet50 [62] with BERT [38]. Image-goal models outperform CLIPORT (single) in packing Google objects, but this is only because they do not have to solve the language-grounding problem.
Multi-Task Performance. In realistic scenarios, we want the robot to be capable of any task, not just one task. We investigate this through CLIPORT (multi) in Table 1 with one multi-task model trained on all 10 tasks. CLIPORT (multi) models are trained only on seen-splits of tasks, so an unseen attribute like 'pink' is consistent throughout single and multi-task settings. Surprisingly, CLIPORT (multi) outperforms single-task CLIPORT (single) models in $41 / 72=57 \%$ of the evaluations in Table 1. This trend is also evident in Figure 3 (seen), especially in instances with 100 demonstrations or less. Although CLIPORT (multi) is trained on more diverse data from other tasks, both CLIPORT (multi) and CLIPORT (single) have access to the same amount of data per task. This supports our premise that language is a strong conditioning mechanism for reusing concepts from other tasks without learning them from scratch. It also validates a trait of data-driven approaches where training on lots of diverse data leads to more robust and generalizable representations [1, 63]. However, CLIPORT (multi) performs worse on longer-horizon tasks like align-rope. We hypothesize that this is because longer-horizon tasks get less coverage of input-action pairs in the dataset. Future works could use better sampling methods that balance tasks according to their average time horizon.
Generalizing to Unseen Attributes. Tasks that require generalizing to novel colors, shapes, and objects are more difficult and all our agents achieve relatively lower performance on these tasks, as shown in Figure 3 (unseen). However, CLIPORT (single) models do substantially better than chance, i.e., Transporter-only. The lower performances are due to the difficulty of grounding unseen attributes such as 'pink' and 'orange' in the language instruction "put the pink block on the orange bowl", when the agent has never encountered words 'orange', 'pink' or their corresponding visual characteristics in the context of the physical environment. Although pre-trained CLIP has been exposed to the attribute 'pink', it could correspond to different concepts in the physical setting depending on factors like lighting condition, and thus requires at least few examples to condition the trainable semantic decoder layers. Additionally, we notice that CLIPORT (single) is also less prone to overfitting compared to Transporter-only. As evidenced in towers-of-hanoi-seq-unseen-colors task in Table 1, Transporter-only suffers from a performance drop because of rings with unseen colors despite the fact that Tower of Hanoi can be solved without attending to the colors and simply focusing on the ring size. We hypothesize that since CLIP was trained on diverse internet data, it enables our agent to focus on task-relevant concepts while ignoring irrelevant aspects of the task.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Affordance predictions from CLIPORT (multi) models in sim (left two) and real settings (right three). More examples in Appendix H.
Transferring Attributes across Tasks. One solution for dealing with unseen attributes is to explicitly learn these attributes from other tasks. We study this with CLIPORT (multi-attr) in Table 1 and Figure 3 (unseen). For these models, CLIPORT (multi) is trained on both seen-and-unseen splits from all tasks except for the task being evaluated on, for which it was only trained on the seen split. As such, this evaluation measures whether having seen pink blocks in put-blocks-in-bowl-unseen-colors helps solve "pack all the pink and cyan boxes" in packing-box-pairs-unseen-colors. Results indicate that such explicit transfers result in significant improvements. For instance, on the put-blocks-in-bowls-unseen-colors task for $n=1000$, CLIPORT (multi)'s performance increases from 45.8 to 75.7 .</p>
<h1>4.3 Real-Robot Experiments</h1>
<p>We validated our results in hardware with a Franka Panda manipulator. See Appendix D for setup details. Table 2 reports success rates for a multi-task model trained and evaluated on 9 real-world tasks. Due to COVID restrictions, we could not conduct largescale user-studies, so we report on small train (5-10 demos) and test sets (5-10 runs) per task. Overall, CLIPORT (multi) is effective at few-shot learning with just 179 samples, and the performances roughly correspond to those in simulated experiments, with simple block manipulation tasks achieving $\sim 70 \%$. We estimate that for more robust real-world performance at least 50 to 100 training demonstrations are necessary, as evident in Figure 3. Interestingly, we observed that the model sometimes exploits biases in the training data instead of learning to ground instructions. For instance, in Put Blocks in Bowl, the training set consisted of only one datapoint on "yellow blocks" being placed inside a "blue bowl". This made it difficult to condition the model to place "yellow blocks" in non-blue bowls. But instances with just one or two examples where a colored block went to different colored bowls was sufficient to make the model pay attention to the language. In summary, unbiased datasets containing both a good coverage of expected skills and invariances, and a decent number of training demonstrations, are crucial for good real-world performance.</p>
<h2>5 Conclusion</h2>
<p>We introduced CLIPORT, an end-to-end framework for language-conditioned fine-grained manipulation. Our experiments, specifically with multi-task models, indicate that data-driven approaches to generalization have yet to be fully-exploited in robotics. Coupled with the right action abstraction and spatio-semantic priors, end-to-end methods can quickly learn new skills without requiring top-down pipelines that need task-specific engineering.
While CLIPORT can solve a range of tabletop tasks, extending it to dexterous 6-DOF manipulation that goes beyond the two-step primitive remains a challenge. As such, it cannot handle complex partially-observable scenes, or output continuous control for multi-fingered hands, or predict task-completion (see Appendix I for an extended discussion). But overall, we are excited by the confluence of data and structural priors for building scalable and generalizable robotic systems.</p>
<h1>Acknowledgments</h1>
<p>All simulated experiments were facilitated through the Hyak computing cluster funded by the STF at the University of Washington. We thank Mohak Bhardwaj for help with the Franka setup at UW. We are also grateful to our colleagues Chris Xie, Jesse Thomason, and Valts Blukis for providing feedback on the initial draft. This work was funded in part by ONR under award #1140209-405780.</p>
<h2>References</h2>
<p>[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020 [cs], Feb. 2021.
[2] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, and J. Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.
[3] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.
[4] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for visionbased robotic manipulation. Conference on Robot Learning (CoRL), 2018.
[5] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.
[6] D. Seita, P. Florence, J. Tompson, E. Coumans, V. Sindhwani, K. Goldberg, and A. Zeng. Learning to rearrange deformable cables, fabrics, and bags with goal-conditioned transporter networks. In IEEE International Conference on Robotics and Automation (ICRA), 2021.
[7] M. Shridhar and D. Hsu. Interactive visual grounding of referring expressions for human-robot interaction. In Proceedings of Robotics: Science and Systems (RSS), 2018.
[8] C. Matuszek, L. Bo, L. Zettlemoyer, and D. Fox. Learning from unscripted deictic gesture and language for human-robot interactions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 28, 2014.
[9] M. Bollini, S. Tellex, T. Thompson, N. Roy, and D. Rus. Interpreting and executing recipes with a cooking robot. In Experimental Robotics, pages 481-495. Springer, 2013.
[10] D. K. Misra, J. Sung, K. Lee, and A. Saxena. Tell me dave: Context-sensitive grounding of natural language to manipulation instructions. The International Journal of Robotics Research (IJRR), 35(1-3):281-300, 2016.
[11] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597-1607. PMLR, 2020.
[12] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9729-9738, 2020.
[13] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems (NeuRIPS), 2019.
[14] Y. C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu. Uniter: Universal image-text representation learning. In European Conference on Computer Vision, pages 104-120. Springer, 2020.</p>
<p>[15] H. Tan and M. Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.
[16] D. H. Hubel and T. N. Wiesel. Receptive fields and functional architecture in two nonstriate visual areas (18 and 19) of the cat. Journal of neurophysiology, 28(2):229-289, 1965.
[17] M. Livingstone and D. Hubel. Segregation of form, color, movement, and depth: anatomy, physiology, and perception. Science, 240(4853):740-749, 1988.
[18] A. Derrington and P. Lennie. Spatial and temporal contrast sensitivities of neurones in lateral geniculate nucleus of macaque. The Journal of physiology, 357(1):219-240, 1984.
[19] J. J. Gibson. The ecological approach to visual perception: classic edition. Psychology Press, 2014.
[20] A. Kamath, M. Singh, Y. LeCun, I. Misra, G. Synnaeve, and N. Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. arXiv preprint arXiv:2104.12763, 2021.
[21] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
[22] Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. In Proceedings of Robotics: Science and Systems (RSS), 2018.
[23] M. Zhu, K. G. Derpanis, Y. Yang, S. Brahmbhatt, M. Zhang, C. Phillips, M. Lecce, and K. Daniilidis. Single image 3d object detection and pose estimation for grasping. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pages 3936-3943. IEEE, 2014.
[24] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, and J. Xiao. Multi-view selfsupervised deep learning for 6d pose estimation in the amazon picking challenge. In 2017 IEEE international conference on robotics and automation (ICRA), pages 1386-1383. IEEE, 2017.
[25] X. Deng, Y. Xiang, A. Mousavian, C. Eppner, T. Bretl, and D. Fox. Self-supervised 6d object pose estimation for robot manipulation. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 3665-3671. IEEE, 2020.
[26] C. Xie, Y. Xiang, A. Mousavian, and D. Fox. The best of both modes: Separately leveraging rgb and depth for unseen object instance segmentation. In Conference on Robot Learning (CoRL), pages 1369-1378. PMLR, 2020.
[27] P. R. Florence, L. Manuelli, and R. Tedrake. Dense object nets: Learning dense visual object descriptors by and for robotic manipulation. In Conference on Robot Learning (CoRL), 2018.
[28] P. Florence, L. Manuelli, and R. Tedrake. Self-supervised correspondence in visuomotor policy learning. IEEE Robotics and Automation Letters, 5(2):492-499, 2019.
[29] P. Sundaresan, J. Grannen, B. Thananjeyan, A. Balakrishna, M. Laskey, K. Stone, J. E. Gonzalez, and K. Goldberg. Learning rope manipulation policies using dense object descriptors trained on synthetic depth data. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 9411-9418. IEEE, 2020.
[30] L. Manuelli, W. Gao, P. Florence, and R. Tedrake. kpam: Keypoint affordances for categorylevel robotic manipulation. In International Symposium on Robotics Research (ISRR), 2019.
[31] T. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman, and V. Mnih. Unsupervised learning of object keypoints for perception and control. Advances in neural information processing systems (NeuRIPS), 32:10724-10734, 2019.
[32] X. Liu, R. Jonschkowski, A. Angelova, and K. Konolige. Keypose: Multi-view 3d labeling and keypoint estimation for transparent objects. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11602-11610, 2020.</p>
<p>[33] K. Zakka, A. Zeng, J. Lee, and S. Song. Form2fit: Learning shape priors for generalizable assembly from disassembly. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 9404-9410. IEEE, 2020.
[34] S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closedloop grasping from low-cost demonstrations. IEEE Robotics and Automation Letters, 5(3): 4978-4985, 2020.
[35] Y. Wu, W. Yan, T. Kurutach, L. Pinto, and P. Abbeel. Learning to Manipulate Deformable Objects without Demonstrations. In Proceedings of Robotics: Science and Systems (RSS), 2020.
[36] L. Yen-Chen, A. Zeng, S. Song, P. Isola, and T.-Y. Lin. Learning to see before learning to act: Visual pre-training for manipulation. In IEEE International Conference on Robotics and Automation (ICRA), 2020.
[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeuRIPS), 2017.
[38] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2018.
[39] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.
[40] F. Yu, J. Tang, W. Yin, Y. Sun, H. Tian, H. Wu, and H. Wang. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. arXiv preprint arXiv:2006.16934, 2020.
[41] Y. Bisk, D. Yuret, and D. Marcu. Natural language communication with robots. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), pages $751-761,2016$.
[42] J. Thomason, S. Zhang, R. J. Mooney, and P. Stone. Learning to interpret natural language commands through human-robot dialog. In Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI), 2015.
[43] J. Hatori, Y. Kikuchi, S. Kobayashi, K. Takahashi, Y. Tsuboi, Y. Unno, W. Ko, and J. Tan. Interactively picking real-world objects with unconstrained spoken language instructions. In Proceedings of International Conference on Robotics and Automation (ICRA), 2018.
[44] Y. Chen, R. Xu, Y. Lin, and P. A. Vela. A Joint Network for Grasp Detection Conditioned on Natural Language Commands. arXiv:2104.00492 [cs], Apr. 2021.
[45] V. Blukis, R. A. Knepper, and Y. Artzi. Few-shot object grounding for mapping natural language instructions to robot control. In Conference on Robot Learning (CoRL), 2020.
[46] C. Paxton, Y. Bisk, J. Thomason, A. Byravan, and D. Fox. Prospection: Interpretable plans from language by predicting the future. In International Conference on Robotics and Automation (ICRA), pages 6942-6948. IEEE, 2019.
[47] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2011.
[48] C. Lynch and P. Sermanet. Grounding language in play. arXiv preprint arXiv:2005.07648, 2020.
[49] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. arXiv preprint arXiv:1406.2199, 2014.</p>
<p>[50] C. Feichtenhofer, A. Pinz, and A. Zisserman. Convolutional two-stream network fusion for video action recognition. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1933-1941, 2016.
[51] C. Feichtenhofer, H. Fan, J. Malik, and K. He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), pages 6202-6211, 2019.
[52] E. Kazakos, A. Nagrani, A. Zisserman, and D. Damen. Slow-fast auditory streams for audio recognition. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 855-859. IEEE, 2021.
[53] F. Xiao, Y. J. Lee, K. Grauman, J. Malik, and C. Feichtenhofer. Audiovisual slowfast networks for video recognition. arXiv preprint arXiv:2001.08740, 2020.
[54] A. Zeng, S. Song, K.-T. Yu, E. Donlon, F. R. Hogan, M. Bauza, D. Ma, O. Taylor, M. Liu, E. Romo, et al. Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching. In 2018 IEEE international conference on robotics and automation (ICRA), pages 3750-3757. IEEE, 2018.
[55] E. Jang, S. Vijayanarasimhan, P. Pastor, J. Ibarz, and S. Levine. End-to-end learning of semantic grasping. In Conference on Robot Learning (CoRL), Proceedings of Machine Learning Research. PMLR, 2017.
[56] R. Kondor and S. Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International Conference on Machine Learning (ICML), 2018.
[57] T. Cohen and M. Welling. Group equivariant convolutional networks. In International conference on machine learning (ICML), 2016.
[58] D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkhin, and Y. Artzi. Mapping instructions to actions in 3d environments with visual goal prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.
[59] G. Goh, N. C. †, C. V. †, S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah. Multimodal neurons in artificial neural networks. Distill, 2021. doi:10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.
[60] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 2016.
[61] Google scanned objects dataset, 2020. URL https://app.ignitionrobotics.org/ GoogleResearch/fuel/collections/Google\ Scanned\ Objects.
[62] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016.
[63] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee. 12-in-1: Multi-task vision and language representation learning. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
[64] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
[65] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
[66] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.</p>
<p>[67] J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. arXiv preprint arXiv:1904.12584, 2019.
[68] D. Ding, F. Hill, A. Santoro, and M. Botvinick. Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures. arXiv preprint arXiv:2012.08508, 2020.
[69] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623, 2021.</p>
<h1>A Task Details</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">precise placing</th>
<th style="text-align: center;">multimodal placing</th>
<th style="text-align: center;">multi-step sequencing</th>
<th style="text-align: center;">unseen <br> poses</th>
<th style="text-align: center;">unseen colors</th>
<th style="text-align: center;">unseen objects</th>
<th style="text-align: center;">language instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">put-blocks-in-bowls-seen-colors*</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">put-blocks-in-bowls-unseen-colors*</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">assembling-kits-seq-seen-colors</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">step</td>
</tr>
<tr>
<td style="text-align: center;">assembling-kits-seq-unseen-colors</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">step</td>
</tr>
<tr>
<td style="text-align: center;">packing-unseen-shapes</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">stack-block-pyramid-seq-seen-colors</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">step</td>
</tr>
<tr>
<td style="text-align: center;">stack-block-pyramid-seq-unseen-colors</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">step</td>
</tr>
<tr>
<td style="text-align: center;">towers-of-hanoi-seq-seen-colors</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">step</td>
</tr>
<tr>
<td style="text-align: center;">towers-of-hanoi-seq-unseen-colors</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">step</td>
</tr>
<tr>
<td style="text-align: center;">packing-box-pairs-seen-colors*§</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">packing-box-pairs-unseen-colors*§</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">packing-seen-google-objects-seq§</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">step</td>
</tr>
<tr>
<td style="text-align: center;">packing-unseen-google-objects-seq§</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">step</td>
</tr>
<tr>
<td style="text-align: center;">packing-seen-google-objects-group*§</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">packing-unseen-google-objects-group*§</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">align-rope* $\dagger$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">separating-piles-seen-colors* $\dagger$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">goal</td>
</tr>
<tr>
<td style="text-align: center;">separating-piles-unseen-colors* $\dagger$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">goal</td>
</tr>
</tbody>
</table>
<p>${ }^{\text {§ }}$ tasks that are commonly found in industry.
*tasks that have more than one correct sequence of actions.
${ }^{\dagger}$ tasks that require manipulating deformable objects and granular media.
Table 3. Language-conditioned tasks in Ravens [2] with their associated challenges.
We extend the Ravens benchmark [2] to 10 language-conditioned. 8 out of 10 tasks have two evaluation variants, denoted by seen and unseen in their names. See Table A for an overview of the challenges associated with each task and split. Figure 5 presents the full list of attributes, shapes, and objects across seen and unseen splits. All tasks use hand-coded experts to generate expert demonstrations. These experts use privileged state information from the simulator along with pre-specified heuristics to complete the tasks. We refer the reader to the original Transporter paper [2] for details regarding these experts. The following is a description of each language-conditioned task:</p>
<h2>A. 1 Align Rope</h2>
<p>Example: Figure 1(a).
Task: Manipulate a deformable rope to connect its end-points between two corners of a 3-sided square. There are four possible combinations for aligning the rope: "front left tip to front right tip". "front right tip to back right corner", "front left tip to back left corner", and "back right corner to back left corner". Here 'front' and 'back' refer to canonical positions on the 3 -sided square. The poses of both the rope and 3 -sided square are randomized for each task instance.
Objects: All align-rope instances contain a rope with 20 articulated beads and a 3 -sided square.
Success Metric: The poses of all beads match the line segments between the two correct sides.</p>
<h2>A. 2 Packing Unseen Shapes</h2>
<p>Example: Figure 1(b).
Task: Place a specified shape in the brown box. Each task instance contains 1 shape to be picked along with 4 distractor shapes. The shape colors are randomized but have no relevance to the task. This task does not require precise placements and is mostly a test of the agent's semantic understanding of arbitrary shapes.
Objects: packing-unseen-shapes is trained with seen shapes but evaluated on unseen shapes from Figure 5.</p>
<p>Success Metric: The correct shape is inside the bounds of the brown box.</p>
<h2>A. 3 Assembling Kits Seq</h2>
<p>Example: Figure 1(c).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Attributes and Objects: Attributes and objects across seen and unseen splits. Shapes objects are from Transporter [2]. Other tabletop objects are from the Google Scanned Objects dataset [61]</p>
<p>Task: Precisely place each specified shape in the specified hole following the order prescribed in the language instruction at each timestep. This is one of the hardest tasks in the benchmark requiring precise placements of unseen shapes of unseen colors and grounding spatial relationships like "the middle square hole" or "the bottom letter R hole". Each task instance contains 5 shapes and a kit with randomized poses.</p>
<p>Objects: Both assembling-kits-seq-seen-colors and assembling-kits-seq-unseen-colors are trained on seen shapes but evaluated on unseen shapes from Figure 5. However for color randomization, assembling-kits-seq-seen-colors is trained and evaluated on seen colors, and assembling-kits-seq-unseen-colors is trained with seen colors but evaluated on unseen colors from Figure 5.
Success Metric: The pose of each shape matches the specified hole at the correct timestep. The final score is the total number of shapes that were placed in the correct pose at the correct timestep, divided by the total number of shapes in the scene (always 5).</p>
<h1>A. 4 Put Blocks in Bowl</h1>
<p>Example: Figure 1(d).
Task: Place all blocks of a specified color in a bowl of specified color. Each bowl fits just one block and all scenes contain enough bowls achieve the goal. Each task instance contains several distractor blocks and bowls with randomized colors. The solutions to this task are multi-modal in that there could be several ways to place the blocks specified in the language goal. This task does not require precise placements and mostly tests an agent's ability to ground color attributes.
Objects: put-blocks-in-bowl-seen-colors is trained and evaluated on seen colors from Figure 5 for both blocks and bowls. put-blocks-in-bowl-unseen-colors is trained on seen colors but evaluated on unseen colors from Figure 5 for both blocks and bowls.
Success Metric: All blocks of the specified color are within the bounds a bowl of the specified color. The final score is the total number of correct blocks in the correct bowls, divided by the total number of relevant color blocks in the scene.</p>
<h2>A. 5 Packing Box Pairs</h2>
<p>Example: Figure 1(e).
Task: Tightly pack all the boxes of two specified colors inside the brown box. All scenes contain the exact number of relevant color blocks to fill the box completely, but also contain some distractor boxes of irrelevant colors. The sizes of the boxes and the brown box are randomized. The distractor objects have equivalent sizes to the relevant objects to make the task more difficult. Sometimes the scene only contains one of the two specified specified colors and the agent has to actively ignore the missing color. Overall, this task requires both semantic understanding of colors and precise spatial reasoning for tightly packing boxes of unknown sizes.
Objects: Boxes with randomized widths and lengths and a brown box. packing-box-pairs-seen-colors is trained and evaluated on seen color boxes from Figure 5. packing-box-pairs-unseen-colors is trained on seen color boxes but evaluated on unseen color boxes from Figure 5.
Success Metric: All blocks of the two specified colors are tightly packed inside the bounds of the brown box. The final score is the total volume of the correct color blocks inside the box, divided by the total volume of the relevant color blocks in the scene.</p>
<h2>A. 6 Packing Google Objects Seq</h2>
<h2>Example: Figure 1(f).</h2>
<p>Task: Place the specified objects in the brown box following the order prescribed in the language instruction at each timestep. This task does not require precise placements and mostly evaluates an agent's ability to ground semantic object descriptions. All objects in a scene are unique without any duplicates. The poses of the objects and the box are randomized for each scene.
Objects: packing-seen-google-objects-seq is trained and evaluated on all 56 objects in Figure 5. packing-unseen-google-objects-seq is trained on 37 seen objects but evaluated on 19 unseen objects in Figure 5.
Success Metric: Each specified object is within the bounds of the brown box at the correct timestep. The final score is the total volume of the correct objects placed inside the box at the correct timestep, divided by the total volume of the relevant objects.</p>
<h1>A. 7 Packing Google Objects Group</h1>
<h2>Example: Figure 1(g).</h2>
<p>Task: Place all objects of the specified category in the brown box. This task does not require precise placements or following a specific action sequence. Each scene contains objects of multiple categories with each category containing at least 2 duplicates. The task cannot be solved by counting the number of objects since there are distractor objects, each with 2 or more duplicates.
Objects: packing-seen-google-objects-group is trained and evaluated on all 56 objects in Figure 5. packing-unseen-google-objects-group is trained on 37 seen objects but evaluated on 19 unseen objects in Figure 5.</p>
<p>Success Metric: All specified objects of a category are within the bounds of the brown box. The final score is the total volume of the correct objects in the box, divided by the total volume of the relevant objects of the specified category in the scene.</p>
<h2>A. 8 Stack Block Pyramid</h2>
<h2>Example: Figure 1(h).</h2>
<p>Task: Build a pyramid of colored blocks in a color sequence specified through the step-by-step language instructions. Each task contains 6 blocks with randomized colors and 1 rectangular base, all initially placed at random poses.</p>
<p>Objects: 6 blocks and 1 rectangular base. stack-block-pyramid-seq-seen-colors is trained and evaluated on seen color blocks from Figure 5. stack-block-pyramid-seq-unseen-colors is trained on seen color blocks but evaluated on unseen color blocks from Figure 5.</p>
<p>Success Metric: The pose of each block at the corresponding timestep matches the specified location. The final score is the total number of blocks in the correct pose at the correct timestep, divided by the total number of blocks (always 6).</p>
<h2>A. 9 Separating Piles</h2>
<h2>Example: Figure 1(i).</h2>
<p>Task: Sweep the pile of blocks into the specified zone. Each scene contains two square zones: one relevant to the task, another as a distractor. The pile and zones are placed at random poses on the table.</p>
<p>Objects: A pile of colored blocks and two squares. separating-piles-seen-colors is trained and evaluated on seen colors from Figure 5 for all blocks and squares. separating-piles-unseen-colors is trained on seen colors but evaluated on unseen colors from Figure 5 for all blocks and squares.</p>
<p>Success Metric: All blocks are inside the bounds of the specified zone. The final score is the total number of blocks inside the correct zone, divided by the total number of blocks in the scene.</p>
<h2>A. 10 Towers of Hanoi Seq</h2>
<h2>Example: Figure 1(j).</h2>
<p>Task: Move the ring to the specified peg in the language instruction at each timestep. The sequence of ring placements is always the same, i.e. the perfect solution to three-ring Towers of Hanoi. This task can be solved without using colors by just observing the ring sizes. However, it tests the agent's ability to ignore irrelevant concepts to the task (color in this case). The task involves precise pick and place actions for moving the rings from peg to peg.
Objects: 1 peg base and 3 rings (small, medium, and big). towers-of-hanoi-seen-colors is trained and evaluated on seen ring colors from Figure 5. towers-of-hanoi-unseen-colors is trained on seen ring colors but evaluated on unseen ring colors from Figure 5.
Success Metric: The pose of each ring at the corresponding timestep matches the specified peg location. The final score is the total number of correct ring placements, divided by total steps in the perfect solution ( 7 for three-ring Towers of Hanoi).</p>
<h1>B Evaluation Workflow and Validation Results</h1>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">packing-box-pairs <br> seen-colors</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">packing-box-pairs <br> unseen-colors</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">packing-seen-google <br> objects-seq</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">packing-unseen-google <br> objects-seq</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">packing-seen-google <br> objects-group</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">packing-unseen-google <br> objects-group</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">Transporter-only [2]</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">56.0</td>
</tr>
<tr>
<td style="text-align: center;">CLIP-only</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">78.1</td>
</tr>
<tr>
<td style="text-align: center;">RN50-BERT</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">77.9</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (single)</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">73.3</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (multi)</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">81.7</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (multi-attr)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">89.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">stack-block-pyramid <br> seq-seen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">stack-block-pyramid <br> seq-unseen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">separating-piles <br> seen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">separating-piles <br> unseen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">towers-of-hanoi <br> seq-seen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">towers-of-hanoi <br> seq-unseen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">Transporter-only [2]</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">81.7</td>
</tr>
<tr>
<td style="text-align: center;">CLIP-only</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">58.1</td>
</tr>
<tr>
<td style="text-align: center;">RN50-BERT</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">84.3</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (single)</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (multi)</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (multi-attr)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">96.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">align-rope</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">packing-unseen-shapes</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">assembling-kits-seq <br> seen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">assembling-kits-seq <br> unseen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">put-blocks-in-bowls <br> seen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">put-blocks-in-bowls <br> unseen-colors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">Transporter-only [2]</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: center;">CLIP-only</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">24.7</td>
</tr>
<tr>
<td style="text-align: center;">RN50-BERT</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">27.7</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (single)</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">48.3</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (multi)</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">43.3</td>
</tr>
<tr>
<td style="text-align: center;">CLIPORT (multi-attr)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">75.7</td>
</tr>
</tbody>
</table>
<p>Table 4. Validation Results. Task success scores (mean \%) from 100 evaluation instances vs. # of training demonstrations ( $1,10,100$, or 1000). The challenges pertaining to each task can be found in Appendix A. CLIPORT (single) models are trained on seen splits, and evaluated on both seen and unseen splits. CLIPORT (multi) models are trained on seen splits of all 10 tasks with $1 \mathrm{~T}, 10 \mathrm{~T}, 100 \mathrm{~T}$, and $1000 \mathrm{~T}$ demonstrations where $\mathrm{T}=10$. CLIPORT (multi-attr) indicate CLIPORT (multi) models trained on seen-and-unseen splits from all tasks except for that one particular heldout task, for which it is trained only the seen split. See Figure 6 for an overview with average scores.</p>
<p>Evaluation Workflow. All simulated experiments in Section 4.1 follow a four-phase workflow: (1) generate train, validation, and test sets, (2) train agents on the train set, (3) optimize on the validation set to find the best checkpoint, (4) evaluate the best checkpoint on the test set. Both validation and test sets consist of 100 evaluation instances each. We found that validation loss is a poor metric for determining the best checkpoint as actions are often multi-modal. In a task like "put the yellow blocks in the red bowl" where there are three possible yellow blocks to choose from, the validation loss is high if the agent chooses a different yellow block to the expert, but in fact choosing any yellow block would suffice in achieving the goal. This issue is addressed by determining the best checkpoint through task execution performance on the validation set.</p>
<p>Validation Performances. During validation, we evaluate a trained agent across fixed checkpoints between 1K-200K iterations for single-task settings and 1K-600K iterations for multi-task settings. We then choose the best-performing checkpoint for each task. Table 4 presents validation results for all tests in Section 4.1. Following Transporter [2], we use a learning rate of $1 \mathrm{e}-4$ with no additional hyperparameter tuning. We note that better learning rate schedules and other hyperparameter optimizations could possibly improve the performance of agents, especially in multi-task settings.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Average validation scores across seen and unseen splits for all tasks in Table 4.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. CLIPORT Two-Stream Architecture: A detailed architecture diagram of the semantic and spatial pathways.</p>
<h1>C Two Stream Architecture Details</h1>
<p>Figure 7 provides a detailed architecture diagram of CLIPORT's two-stream design. We use ReLU activations after each conv and identity blocks without any Batch Normalization. Note that we repeat the depth input to match the dimensions of the RGB image $\mathbb{R}^{H \times W \times 1} \rightarrow \mathbb{R}^{H \times W \times 3}$ following Transporter [2]. All models were implemented in PyTorch [64]. For CLIP, we use the implementation and pre-trained checkpoint released by the authors ${ }^{3}$.</p>
<h2>D Robot Setup</h2>
<p>Hardware Setup. All real-robot experiments were conducted on a Franka Panda robot with a parallelgripper. For perception, we use a Kinect-2 RGB-D camera mounted on a tripod, tilted down looking at the table. Although the Kinect-2 provides images at a resolution of $1280 \times 720$, we use downsampled $960 \times 540$ images for a faster user-interface. The extrinsic calibration between the camera and the robot base-frame is computed with an AR Marker through ARUCO ROS ${ }^{4}$. See Figure 8 for an overview of the setup.
Demonstrations and Execution. For collecting demonstrations with the Franka Panda, we developed a 2D interactive tool that uses the top-down RGB view from the Kinect-2 to specify pick-and-place locations. The user first selects a 2D bounding box on the live RGB feed, and then picks a discrete rotation angle by clicking around the bounding box. For grasping, we use a simple heuristic to determine the height at which to close the fingers. First we segment the pointcloud encapsulated by the bounding box, then we vertically crop the pointcloud up to the height of the gripper fingers, and then compute a 3D cen-
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Real-Robot Experimental Setup.
troid of the selected points by taking an average. This 3D centroid is used to plan a path for the end-effector with an RRT* motion-planner to execute a predefined sequence - go down, open/close the gripper, raise up. For executing a trained CLIPORT model, a similar grasping approach is used, but instead of the user-specified bounding box, we take $32 \times 32$ crops centered around the pick and place predictions (i.e. affordance argmax) to compute 3D centroids from the pointcloud. Only the sweeping and folding actions are different in that the end-effector does not raise up after grasping.
Pick Rotations for Parallel Grippers. The suction gripper used in simulation does not require a pick rotation since the grasps are specified as pin-point locations. However, with the Franka Panda, the parallel gripper requires a specific yaw rotation at which to grasp an object. To handle this, we separate the pick module $\mathcal{Q}_{\text {pick }}$ into two components: locator and rotator. The locator predicts a pixel location $(u, v)$ given the full observation and language input. The rotator takes a $64 \times 64$ crop of the observation at $(u, v)$ along with the language input and predicts a discrete rotation angle by selecting from one of $k$ rotated crops. We use $k=36$ in all our hardware experiments. While it's possible to predict both the location and rotation with a single module, this decoupled approach allows us to fit the model on a single GPU (NVIDIA P100) with reduced memory usage from cropped rotations.</p>
<h2>E Data Augmentation</h2>
<p>Following common practice and the original Transporter implementation [2], we augment the training samples by applying random $\mathbf{S E}(2)$ transformations. Augmentations where $\mathcal{T}<em _place="{place" _text="\text">{\text {pick }}$ or $\mathcal{T}</em>$ are out of frame after the transformation are discarded. These augmentations are particular important for learning spatially-equivariant representations with FCNs without overfitting to images from limited training demonstrations.}</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Data Augmentation: $\mathbf{S E}(2)$ transform applied to RGB-D input. The left image shows the original input, and the right image shows the transformed input along with expert $T_{\text {pick }}$ (red) and $T_{\text {place }}$ (green) actions.</p>
<h1>F Ablations and Baselines</h1>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">stack-block-pyramid seq-seen-colors</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">stack-block-pyramid seq-unseen-colors</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">packing-seen-google object-seq</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">packing-unseen-google object-seq</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">One-Stream Transporter-only</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">37.3</td>
</tr>
<tr>
<td style="text-align: center;">One-Stream CLIP-only</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">One-Stream Language Transporter</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">One-Stream Image-Goal Transporter</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">83.3</td>
</tr>
<tr>
<td style="text-align: center;">Two-Stream CLIP-Transporter w/o skips</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;">Two-Stream Untrained-Sem-Transporter</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">34.8</td>
</tr>
<tr>
<td style="text-align: center;">Two-Stream RN50-BERT-Transporter</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">56.1</td>
</tr>
<tr>
<td style="text-align: center;">Two-Stream CLIP-Transporter (ours)</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">71.9</td>
</tr>
</tbody>
</table>
<p>Table 5. Ablations and Baselines. Evaluation scores (mean \%) for stack-block-pyramid-seq and packing-google-objects-seq tasks from 100 evaluation runs. Stacking block pyramids involves both semantic and precise spatial reasoning, whereas packing objects mostly involves semantic grounding without requiring any precise placements.</p>
<p>Table 5 presents various baselines and ablations from our simulated experiments. The following is a description of each model:
One-Stream Transporter-only is the original Transporter [2] with RGB-D input, or equivalently, the spatial stream of CLIPORT. For all experiments, we implemented our own version of Transporter in PyTorch and did not use the modeling code provided with the original paper. Our Transporter models are also trained for 200 K iterations instead of 40 k iterations.
One-Stream CLIP-only is the semantic stream of CLIPORT with RGB and language input.
One-Stream Language Transporter is Transporter [2], but the bottleneck features are conditioned with CLIP language features in a similar fashion to the semantic stream in CLIPORT. This model performs very poorly because the high-level language features corrupt the low-level spatial features necessary for precise pick-and-place actions.
One-Stream Image-Goal Transporter is a goal-conditioned version of Transporter [6] which receives a goal-image as input. For sequential tasks with a specific order (indicated with seq in their name), we provide the goal-image from the next timestep, and for non-sequential tasks we provide the goal-image from the final timestep. The implementation follows the goal-conditioned Transporter proposed in [6], except we found that element-wise addition worked better than element-wise product for combining goal-image features with $\mathcal{Q}_{\text {place }}$ features.
Two-Stream CLIP-Transporter w/o skips is a variant of the CLIPORT model without skip connections from the CLIP-ResNet encoder to the decoder layers. The results in Table 5 show that these skip connections are particularly important for good performance. We hypothesize that utilizing different levels of semantic information from the visual encoder - patterns, shapes, parts, objects, and high-level concepts, is crucial for conditioning the semantic stream decoders.
Two-Stream RN50-BERT-Transporter is the same two-stream architecture as CLIPORT, except instead of the CLIP ResNet50, we use a standard ResNet50 [62] pre-trained on ImageNet classification. And instead of the CLIP sentence encoder, we use a pretrained DistilBERT model [65] to extract language embeddings. CLIP offers the benefit of multi-modal alignment between vision and language features while not being restricted to instance segmentation or bounding box detection pipelines.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/openai/CLIP
${ }^{2}$ https://github.com/pal-robotics/aruco_ros&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>